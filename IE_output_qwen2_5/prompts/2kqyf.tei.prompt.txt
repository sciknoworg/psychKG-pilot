You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
An apprentice pastry chef is not confident that the cake they just made matches the quality standards of their workplace. Unexpectedly, they receive compliments from their mentor telling them the cake is excellent. The low confidence of the apprentice and the positive feedback received from their mentor are in conflict, as they provide opposite opinions about the probability that the cake is of sufficient quality. This mismatch between the internal estimate of that probability 
(i.e., confidence)
 and the external information about that probability (i.e., feedback) generates a prediction error, which likely will result in increased confidence the next time the apprentice is faced with a similar situation.
With a more well-calibrated sense of confidence, the apprentice pastry chef can now rely more heavily on their sense of confidence as an internal feedback signal to judge the quality of their work, and decide, for example, to start anew (if confidence is low) or put it out for sale (if confidence is high). As this example illustrates, one does not merely have to learn the task at hand (i.e., how to bake cakes).
Its accompanying confidence needs to be learned in parallel, as it is also relevant for future action (in the example, judging the quality of the cake).
Remarkably, although there exist several mechanistic explanations regarding the computations underlying decision confidence, its learning process has been neglected in virtually all prior models 
(Adler & Ma, 2018;
Calder-Travis et al., 2024;
Drugowitsch et al., 2012;
Fleming & Daw, 2017;
Khalvati et al., 2021;
Kiani & Shadlen, 2009;
Le Denmat et al., 2024;
Shekhar & Rahnev, 2024)
.
Instead, focus resided primarily on the definition of decision confidence, which converged on the posterior probability of making a correct choice conditional on relevant information 
(Kiani & Shadlen, 2009;
Meyniel et al., 2015;
Sanders et al., 2016)
. Computing that quantity is useful, because confidence is informative about one's accuracy and thus can be used to optimize future actions 
(Balsdon et al., 2020;
Desender et al., 2018
Desender et al., , 2019
Drugowitsch et al., 2019;
Frömer et al., 2021;
Lee & Daunizeau, 2021;
Pescetelli & Yeung, 2021)
.
What remains unclear, however, is how agents can learn and dynamically calibrate their feeling of confidence to these probabilities. In this work, we aim to provide a first account of how the mapping from decision-relevant information to confidence is dynamically updated to efficiently track the probability of being correct. We propose that the computation of confidence is learned from environmental feedback, as it represents an external source of information about one's accuracy.
Similar to how agents use feedback to learn how to perform a task 
(Skinner, 1938;
Sutton & Barto, 2018)
, here, we explore how confidence itself can be learned from feedback. Specifically, we hypothesize that confidence is updated by minimizing the prediction error between external feedback and the current estimate of confidence.
A first key prediction of this hypothesis is that individuals should dynamically adapt their reporting of confidence in response to environmental feedback. For example, if feedback is overly positive or overly negative for a given decision, this should affect the computation of confidence for the next decision within the same task. A second key prediction is that this dynamic confidence computation can be independent of changes in objective performance. Indeed, whereas performance and confidence should naturally be linked, biased feedback signals (e.g., as generated in our experiment), can in principle cause dissociations between the two quantities. We tested these predictions in a perceptual decision-making experiment where two feedback regimes (positive vs negative) alternate every 126 trials. We expected that confidence gradually increases over the course of positive feedback phases, whereas it should gradually decrease over the course of negative feedback phases.
To model these dynamics, we build on the recently proposed low-dimensional confidence (LDC) model (Le . Like many models of confidence 
(Calder-Travis et al., 2024;
Kiani & Shadlen, 2009;
Moran et al., 2015;
Moreno-Bote, 2010;
Pleskac & Busemeyer, 2010)
, the LDC model is built on top of the influential drift diffusion model (DDM) 
(Ratcliff, 1978)
, and describes how the amount of accumulated evidence and the associated reaction times are used to compute confidence.


4
This model offers a tractable starting point for a learning model, because it explains confidence and its deviations from optimality using only two confidence-specific scaling parameters. The first parameter is called the subjective evidence reliability (α) and weighs the importance of accumulated evidence in the computation of confidence. The second parameter is a confidence bias (β) and accounts for stimulus-independent over-(β > 0) and underconfidence (β < 0).
Here, we extend the LDC model to a dynamic model (further referred to as DLDC) where confidence is learned by updating its parameters in a trial-by-trial manner. Specifically, we implemented the DLDC model as a neural network comprising a single layer of units, which takes as input the readout of the decision process as modeled by the DDM (i.e., total accumulated evidence and elapsed time, 
Figure 1
) and returns a confidence estimate. The two parameters of LDC, α and β, are now weights on the evidence and bias inputs of our neural network, respectively. They can therefore be updated via gradient descent to minimize the prediction error between the external feedback provided and the model-predicted confidence. In the next sections, we first report our behavioral experiment, followed by computational modelling results. Confidence is considered to represent the Bayesian probability of a choice being correct. Under DDM assumption, this probability is calculated conditional on the total accumulated evidence, elapsed time and choice (represented by the color on the heatmap). B. The DLDC model is a learning model of confidence implemented as a single layer neural network model. The network takes the readout of the decision process (i.e. accumulated evidence and elapsed time) as well as a bias as input. These are then combined and passed through a logistic activation function, resulting in a confidence estimate between 0 and 1. To minimize the prediction error between estimated confidence and feedback, the computation of confidence dynamically evolves by updating its weights, α and β, via gradient descent. 5


Methods


Participants
Sixty-eight participants (10 men, age: M = 18.81, SD = 1.71, range = 17-30) took part in the experiment in return for course credit. All participants read and signed a written informed consent form at the start of the experiment. All procedures were approved by the KU Leuven ethics committee.
Nine participants were excluded from analysis due to chance level performance on at least one repetition of any feedback condition. Two participants with more than 90% of their confidence ratings on the same level were additionally removed from data analysis, as they did not follow the instruction of using the whole confidence scale.


Procedure
Each trial started with a fixation cross for 500 ms. Next, 80 colored dots (a mix of red and blue) appeared for 200 ms within a white squared frame in the middle of a computer screen. Participants were tasked to judge as quickly and accurately as possibly which color of dots was the most numerous by pressing C or N with one of their thumbs. Trial difficulty was determined by the number of dots belonging to the prominent category: 44 for hard trials, 47 for medium trials and 53 for easy trials.
There was no response deadline for this decision. Next, participants reported the confidence in their decision on a 6-point scale with the following labels: "sure error", "probably error", "guess error", "guess correct", "probably correct" and "certainly correct" (reversed order for half of the participants), using the keys 1, 2, 3, 8, 9 and 0 with the ring, middle and index finger of both hands. There was no response deadline for the confidence report. Finally, feedback about the correctness of their primary decision was given by a continuous value ranging from 0 (certainly wrong) to 100 (certainly correct) presented centrally on the screen for 1000 ms. The feedback value was additionally visually represented by means of a filled horizontal scale with labels "sure error", "probably error", "random chance", "probably correct" and "sure correct" at the 0, 25, 50, 75 and 100 tick marks respectively.
Participants were told that this number represented the probability that their response was correct.
In reality, the feedback was model-generated. To compute the model-based feedback, single trial values for the inputs (evidence and time) of our model were required. While time is directly available by summing decision and confidence RT, accumulated evidence is a latent variable. We therefore estimated the latent evidence accumulation process of single trials using average drift rate and decision boundary estimates obtained from previous experiments with the same task and difficulty levels (this estimation procedure is described in full detail in the model fitting section). Feedback for a
given trial corresponded to model-predicted confidence from pre-determined values of α and β.
Critically, unknown to participants every 3 blocks of 42 trials the value of β that was used to generate the feedback switched between 2 possible values (β =1, high vs β = -1, low feedback condition), while the value for α was fixed to 18 throughout the experiment (see supplementary material for the behavioral results from an experiment where feedback was manipulated according to α instead). In total, there were 3 repetitions of both feedback conditions. The feedback condition order was counterbalanced across participants.
At the end of the experiment, participants were asked a series of four questions to assess their perception and understanding of feedback, as well as their awareness of the feedback manipulation.
The first question was an open-ended question inviting participants to share anything unusual they noticed about the experiment. We considered that participants were aware of the feedback manipulation if they at least reported that feedback values were sometimes high, and sometimes low.
The next two questions prompted how much participants felt feedback helped them assess how well they performed, and how much they felt feedback closely tracked their performance. Answers for these two questions were made on a 5-point Likert scale. Lastly, we checked the level of understanding of feedback by showing an example feedback of 81%. Participants were instructed to select all true propositions within a list. We considered that participants correctly understood feedback if they acknowledged that the response associated to the example feedback is potentially correct, although the true accuracy of the decision is uncertain. presented patch of colored dots was mostly composed of red or blue dots. They then rated their confidence on a six-point scale ranging from "sure error" to "sure correct". Finally, model-generated feedback was presented, framed as the probability that the decision was correct. B. Feedback alternated between two conditions (high vs low) throughout the experiment.
Feedback values were generally higher in the high condition, independently of trial accuracy and difficulty. Error bars are SEM, although most of them are too small to be visible.


Statistical analysis
Outlier trials with reaction times (RTs) below 200 ms or above 5000 ms were removed from the analysis. Trials with confidence RTs above 5000 ms were filtered out as well. Confidence ratings and RT analyses were performed using linear mixed models, for which we report type-III F statistics using Satterthwaite's approximation for degrees of freedom. Since accuracy is a binary outcome variable, generalized mixed models were used for its analysis, for which we report Х 2 statistics. All models comprised feedback condition (high vs low), trial difficulty (easy, medium, hard) and trial counts within a feedback phase (1-126) as fixed effects as well as all interactions. Trial accuracy (correct vs incorrect) and its interaction with other factors were also included as fixed effects when analyzing confidence ratings. To determine the random effect structure for each model, we first fitted a model with random intercepts for participants only. We then added random slopes for each fixed effect if the model fits were significantly improved until reaching the most complex random effect structure possible for this dataset. All mixed models were fitted in R using the "lme4" package 
(Bates et al., 2015)
, p-values were obtained by using Satterthwaite's method for approximating degrees of freedom, as implemented in the "lmerTest" package 
(Kuznetsova et al., 2017)
.


Results


Objective performance
We first examined the overall effect of feedback on objective performance, excluding the factor of time, to determine if this experiment replicates the selective influence of feedback on confidence found in previous studies (Le 
Van Marcke et al., 2024)
. Consistent with prior findings, the feedback manipulation did not affect accuracy, χ²(1) = 0.16, p = .688 ( 
Figure 3A
, left panel).
However, a small effect on RTs was observed, with slower RTs in the low feedback condition compared to the high feedback condition, F(1,57) = 6.03, p = .017 ( 
Figure 3B
, left panel). Trial difficulty significantly influenced both RTs, F(2, 34860) = 343.44, p < .001, and accuracy, χ²(2) = 2074.08, p < .001.
No interaction was found between trial difficulty and feedback conditions for RTs (F(2,34861) = 0.79, p = .453) or accuracy (χ²(2) = 1.80, p = .407).
Next, we examined the dynamics of the effect of feedback on objective performance by adding the within-phase trial number as a fixed effect in the mixed models ( 
Figure 3A
 and 3B, right panel). As anticipated, we found no interaction between feedback condition and within-phase trial in RTs, F(1, 34862) = 0.52, p = .472, nor in accuracy, χ² = 1.40, p = .236. remained constant over the course of a feedback phase. C. Confidence was higher overall for high feedback phases. In contrast with the effect on RTs, confidence ratings gradually increased (resp. decreased) after 10 switching into a high (resp. low) feedback phase, for both correct and incorrect trials. This finding was successfully captured by our learning model. For visualization purposes, the x-axis on the right panel of A-C shows time as means over successive (non-overlapping) groups of 7 trials. D. The influence of feedback on confidence over the course of the experiment can also be appreciated on the unaggregated confidence ratings (correct trials only).
For visualization purposes, the data was smoothed by applying a rolling mean with a window size of 25 trials. On this plot, participants were split according to which feedback condition they experienced first. Grey dashed lines represent the feedback condition switches. The two curves crossed after each switch, highlighting the fact that confidence gradually increased for participants who entered a high feedback phase, and respectively decreased for participants who entered a low feedback phase. 


Confidence
We next turned to the effect of feedback on confidence ratings. Similar to previous section, we initially fitted a model without the factor of time to determine if the current experiment replicates previous findings. Overall, confidence was higher in the high feedback condition than in the low feedback condition, F(1, 60) = 9.39, p = .003 ( 
Figure 2B)
. No Feedback*Trial accuracy interaction was found, F(1,66) = 0.38, p = .540, indicating that the effect of feedback on confidence was similar for correct and incorrect trials. An interaction between trial difficulty and trial accuracy was also found in confidence ratings, F(2, 40561) = 537.40, p < .001, showing higher confidence in correct trials and lower confidence in incorrect trials with easier difficulty levels. There was no interaction between trial difficulty and feedback conditions for confidence, F(2,39512) = 0.19, p = .824.
Next, we looked at the evolution of confidence over time (trials). If participants learned how to compute confidence from the feedback provided, then we should observe a gradual increase in confidence from the start to the end of a high feedback phase. Similarly, a gradual decrease in confidence is expected in a low feedback phase. To examine this, we added the within-phase trial number as a fixed effect in our mixed models. As predicted, we observed a significant (within-phase)
Trial*Feedback condition interaction on confidence, F(1, 42699) = 12.27, p < .001. As can be seen in 
Figure 3C
, confidence for both correct and incorrect decisions gradually increased over time during high feedback phases, and similarly decreased during low feedback phases. The same finding can be appreciated in the unaggregated data in 
Figure 3D
. The fact that this gradual change in the computation of confidence initiated by feedback was not mediated by a change in objective performance suggests that participants selectively updated their confidence computation based on trial-by-trial feedback.


Computational modelling


Dynamical low-dimensional confidence model (DLDC)
The DLDC model is an extension of the LDC model (Le , which describes how the outcomes of the DDM process (i.e. evidence and time) are mapped to a confidence estimate. More specifically, confidence in the LDC model is computed according to the following equation:
= 1 1 + exp (− 1 √ ( + ))
(1)
where is time, ∈ {−1; 1} is the choice and is total accumulated evidence. The computation of confidence is thus under control of and , which are free but static parameters. In the current dynamic extension, and are dynamically updated weights in a network. These are updated on a trial-by-trial basis such that they minimize the cross-entropy between confidence and feedback, resulting in the following update rule:
+1 = + ( − ) * √ (2) +1 = + ( − ) * 1 √ (3)
where is the value of at trial , is the learning rate of (and similarly for ), is feedback
given at trial , is model-predicted confidence at trial , is total accumulated evidence at trial , and is total elapsed time (e.g. RT + confidence RT) at trial . The model starts with initial weights 0 and which represent prior beliefs before the start of the learning process. This model thus has a total of four free parameters: initial weights ( 0 , ) and learning rates ( , ).


Model fitting procedure
Model fitting of accuracy, RT and confidence data was performed in two subsequent steps. In a first step, all accuracy and RT data were fitted separately per participant using the standard DDM with 5 free parameters: decision bound, non-decision time, and a separate drift rate for each of the three difficulty levels. Best-fitting parameters were obtained using quantile optimization 
(Ratcliff & Tuerlinckx, 2002)
, by minimizing the mean squared error (MSE) between the proportion of observed and model-predicted RTs within each quantile group:
= ∑ ∑( , − , )² (4)
where , is the proportion of observed trials with accuracy ∈ {correct, error} within the RT quantile and , represents the proportion of predicted trials within the same RT quantiles. The RT distributions were divided in 6 groups formed by quantiles .1, .3, .5, .7 and .9. This optimization step was performed via a differential evolution algorithm implemented in the DEoptim R package 
(Mullen et al., 2009)
. The algorithm stopped once no improvement of the objective function was obtained for the last 100 generations or until reaching 1000 generations (this latter criterion was never reached).
The population size was set to 10 times the number of parameters to estimate.
In a second step, the initial states ( 0 , 0 ) and learning rates ( , ) of our confidence learning model were fit to observed confidence ratings. Since α and β are updated every trial according to the feedback received, single trial values for the inputs of our model (evidence and time) are required. The time input in the model was determined as the sum of decision and confidence RT. As a latent variable, the true amount of accumulated evidence is unknown. We estimated it using the decision bound and drift rates obtained from fitting the DDM to accuracy and RT data in the previous step. By definition, the amount of accumulated evidence at the time of the decision is equal to the decision bound. The total amount of accumulated evidence for a given trial can thus be decomposed as:
= + (5)
, where ∈ {−1; 1} is the choice, is the decision bound and is the post-decisional accumulated evidence. Note that in the current implementation of DDM, evidence accumulation starts at 0 and continues until reaching boundary or − , hence total evidence at the time of the decision is equal to . Assuming that individuals kept on accumulating evidence until they reported their confidence with no change in drift rate, we obtain 
(Drugowitsch et al., 2012;
Moreno-Bote, 2010;
Ratcliff, 1978)
:
~ ( , 2 )
(5)
where is the drift rate, is confidence RT and is the DDM accumulator noise (fixed to .1 here).
Total evidence for each trial is thus estimated by randomly sampling from the normal distribution described in Eq. 
5
and adding that to the boundary hit on that trial.
The meta-parameters (learning rates and starting points) of our model are estimated for each subject by minimizing the MSE between observed and model-predicted confidence ratings of all trials:
= ∑( − )² (6)
where is the observed confidence judgment at trial and is the model-predicted confidence judgment at trial . Similar to the DDM fitting described in step 1, the fitting of our confidence learning was performed via differential evolution. A new value of for every trial was drawn on every generation to prevent the fitting procedure from depending too much on a specific estimate of .


Modelling results
If the behavioral findings are indeed caused by participants learning how to compute confidence from feedback, then these should be well accounted for by the DLDC model which implements this learning process. To verify this, we fitted the learning rates and starting weights of the DLDC model to confidence ratings, separately for each participant. As can be visually appreciated in 
Figure 3C
, model fits closely tracked observed confidence ratings. Specifically, our model successfully captured the gradual increase (and respective decrease) in confidence after entering a high (respectively low) feedback block. We tested our learning model's ability to capture the behavioral data by performing the same analysis on model predictions. We generated confidence ratings for each subject using the estimated parameters from their best-fitting model obtained in the model comparison. We first looked at the overall effect of feedback on confidence in both experiments. Model fits fully reproduced the pattern observed in behavioral data, with a main effect of feedback on predicted confidence, F(1,60) = 6.27, p = .015, and no feedback*accuracy interaction, F(1,61) = 1.05, p = .310. We next turned to the dynamics of the effect of feedback on predicted confidence within switch blocks by adding trial to the model. Similarly to behavioral data, there was a significant feedback*trial interaction, F(1,42774) = 103.92, p < .001. Overall, predictions from our learning model reproduced the main behavioral findings, showing that the computation of confidence was dynamically adapted to track feedback.


Model comparison
If participants learned how to compute confidence from feedback, then a learning model such as the DLDC model should provide a better fit than a non-learning model. We tested this prediction by comparing the empirical fit of several variants of the DLDC model, including a non-learning model. Given that our model allows for the updating of confidence computation according to two distinct parameters (α and β), there are three possible ways for individuals to change their confidence computation: update α, update β, or update both. Initial values 0 and 0 are estimated in all models, but when α and/or β is updated, we additionally estimated the learning rate of α and/or β (otherwise, learning rates were fixed to 0). In a model recovery analysis, we found that these three dynamic variants are hard to distinguish (due to evidence being an unknown, latent variable; see supplementary results for full details). Nonetheless, learning vs no-learning models can be reliably recovered (despite a general bias towards the no-learning model). For this reason, we grouped all three learning models (α, β, and both) together for the sake of model comparison, and compared learning models versus a non-learning model. In the no learning model, the learning rate for α and β was fixed to 0 (i.e. making it equivalent to the LDC model, (Le Denmat et al., 2024)).
When looking at the mean Bayesian information criterion (BIC) over participants ( 
Figure 4A)
, the no learning model performed considerably worse than the learning models (ΔBIC between no learning and each learning model ranged between 10.29 and 14.64). However, when examining per participant which model provided the best fit ( 
Figure 4B
), most participants' data were best explained by the no learning model (34 participants, i.e. 60%). These divergent results between mean BIC and best model per participant suggest that when the learning models won over the non-learning model, they did so by a significantly larger margin than when the non-learning model won. Given the greater complexity of learning models (i.e. more free parameters), the non-learning model would win over the learning models in BIC for equivalently good empirical fits. In contrast, a substantial win by the learning models over the non-learning model can only mean that they better captured some aspects of the data. We thus reasoned that there were two qualitatively different groups of participants, namely those who were best explained by the nolearning model (the static group), and those who were best explained by the one of the learning models (the dynamic group). We hypothesized that participants in the static group showed little to no dynamic change in confidence according to feedback, whereas the confidence of participants in the dynamic group closely tracked feedback. To test this, we reanalyzed the confidence ratings by adding a three-way fixed effect interaction between feedback condition, trial within a feedback block and best-fitting model (learn vs. no learn model) in the mixed model. This three-way interaction was significant, F(2, 1634) = 22.79, p < .001, suggesting that the learning effect of feedback on confidence differed according to the best-fitting model. Further analysis by splitting the dataset according to the best-fitting model revealed no Trial*Feedback condition interaction within the static group, F(1, 25559) = 0.36, p = .549, meaning that these participants did not dynamically adapt their computation of confidence according to feedback. Conversely, we found a highly significant Trial*Feedback condition interaction within the dynamic group, F(1, 16888) = 19.64, p < .001. The disparity between both groups as well as the predictions from both model categories are visually represented in 
Figure 4C
-F. The dynamic group showed a strong trend in confidence ratings, uniquely captured by the learning models ( 
Figure 4C
), while the static group did not learn from feedback, making the non-learning model sufficient to explain their behavior ( 
Figure 4F
).
To understand better why some participants adapted their computation of confidence based on the feedback while others did not, we analyzed the responses to the post-experiment questionnaire.
Four questions were asked at the end of the experiment, assessing: (1) how aware participants were of the feedback manipulation, (2) how much they felt feedback helped them assess how well they performed, (3) how much they felt feedback closely tracked their performance and (4) whether they properly understood the feedback they received. We found no difference between the static and dynamic groups in awareness of the manipulation, X²(1) = 0.01, p = .943. However, participants in the dynamic group reported that feedback helped them better assess how well they performed X²(1) = 9.33, p = .002, and reported that feedback better tracked their performance, X²(1) = 9.62, p = .002.
Finally, more participants in the static group did not properly understand the feedback they received, X²(1) = 464.94, p < .001. 354


Discussion
An overlooked aspect about the computation of confidence is that it needs to be learned. In the current work, we reasoned that individuals calibrate their sense of confidence in a statistically efficient manner, by minimizing the prediction error between external feedback and confidence. We tested this hypothesis in a behavioral experiment where two manipulated feedback conditions repeatedly alternated throughout the experiment. As predicted, we found that participants' confidence ratings dynamically tracked feedback, with participants gradually becoming more confident when faced with positive feedback and less confident with negative feedback. To capture these results, we proposed a learning model of confidence in which the parameters underlying the computation of confidence were dynamically updated based on the mismatch (prediction error) between feedback and confidence.
Model comparison between learning and non-learning models revealed that roughly half of participants dynamically updated their computations of confidence based on the feedback while the other did not. These individual differences were linked to the general understanding of the feedback as well as the perceived pertinence and helpfulness of feedback.


Prediction errors shape confidence
The current behavioral findings align with previous studies on the effect of feedback on confidence 
(Baranski & Petrusic, 1994;
Haddara & Rahnev, 2022;
Katyal et al., 2023;
Le Denmat et al., 2024;
Soto et al., 2024;
Van Marcke et al., 2024)
. In our study, we alternated between more positive and more negative feedback blocks to reveal the rapid time scale of confidence updating. Earlier research instead manipulated feedback through a single training phase (where feedback is presented) followed by a single testing phase (where feedback is no longer presented) 
(Katyal et al., 2023;
Le Denmat et al., 2024;
Van Marcke et al., 2024)
. These studies found that feedback during training selectively influenced confidence during testing without affecting objective performance. Our experiment largely replicates these findings, showing that feedback uniquely impacts confidence. Although RTs were overall slightly longer in the low feedback condition, this was not due to a dynamic adaptation to 20 feedback. Instead, confidence gradually increased during high feedback phases and gradually decreased during low feedback phases. This suggests that the influence of feedback on confidence is not due to a change in response strategy but rather results from continuous learning from the feedback received.
Since our model-generated feedback reflected a model-based estimate of the probability of being correct (rather than a binary label), we essentially investigated the influence of feedback on confidence by inducing confidence biases in different directions. A different approach to studying the effect of feedback on confidence is to manipulate the probability of presenting (always accurate) feedback 
(Baranski & Petrusic, 1994;
Haddara & Rahnev, 2022;
Soto et al., 2024)
. This research has generally shown that metacognitive biases can be reduced by providing feedback 
(Baranski & Petrusic, 1994;
Haddara & Rahnev, 2022;
but see Soto et al., 2024
 for contradicting evidence in children). This result aligns with the current findings and supports the hypothesis that confidence is learned from prediction errors with feedback. Just as we demonstrated that humans progressively bias their confidence in a direction consistent with the biased feedback they receive, our confidence learning hypothesis similarly predicts that they become less biased when learning from unbiased feedback.


From static to dynamic models of confidence
Our behavioral findings were successfully accounted for by the DLDC model, a learning model of confidence framed as a single-layer neural network. The dynamic nature of the model was key to capturing the observed patterns in confidence over the course of feedback phases, indicating that the computation of confidence should be treated as a dynamically evolving process. Understanding the computation of confidence in increasingly realistic situations is one of the main goals identified in the field 
(Rahnev et al., 2022)
. Realistic environments are often volatile (i.e., contingencies change) or completely new. As a result, adaptive agents (including humans) need to constantly question and adjust their confidence to keep it well-calibrated to their performance.
Although we have to our knowledge provided the first account of trial-by-trial learning of confidence, we are not the first to describe a model of confidence in which learning plays a pertinent role. 
(Webb et al., 2023)
 proposed a deep learning model where the computation of confidence is learned separately from the decision. Specifically, their model takes as input naturalistic images which is passed to a deep neural network encoder. The output of this encoder is then sent to two separate output layers: one designed to produce a decision and the other to predict the probability of that decision being correct (i.e. confidence). By doing so, Webb and colleagues found that their model displayed the same confidence biases as humans, thus pinpointing natural statistics of stimuli as the origin of these biases (as opposed to biased computations of confidence). However, given the complexity of the model and the large number of parameters to optimize, the learning of confidence took place over thousands of trials, which is a very different timescale than the dynamic adjustments of confidence revealed in the current experiment. Future work shall investigate whether we can also scale up our modeling approach to realistic inputs.


Extension for the model and future directions
With the DLDC model, we formalized how individuals learn the posterior probability of making a correct choice by minimizing the prediction error between external feedback and their current confidence. However, individuals do not always have access to feedback. One might thus wonder whether and how that probability can be updated in the absence of feedback. A potential solution is to consider a kind of temporal difference learning rule where instead of feedback, the target would be average past confidence ratings. Such learning mechanism, initially proposed by 
(Rahnev et al., 2015)
 to explain the confidence leak effect (whereby confidence on a given decision influences confidence on the following decision), would allow agents to detect and adapt to changes in the environment. For instance, a context shift from an easy to a hard decision difficulty context would create a mismatch between previously experienced confidence, generally high in an easy context, and confidence in the current decision, which is likely to be lower in a hard context. Individuals would then adapt and learn that they should be less confident in that harder context. Another question that can be explored by our model is the nature of the effective goal function used to learn confidence. It is commonly assumed that the probability of making the correct decision is the optimal computation for confidence, because it is maximally informative about the accuracy of decisions 
(Calder-Travis et al., 2024;
Rahnev & Denison, 2018)
, and can thus be optimally used to adapt behavior 
(Drugowitsch et al., 2019)
. However, this is not the only functional role of confidence. For instance, in social settings, confidence is also used to enforce one's opinion and push the group towards deciding one's chosen option 
(Zarnoth & Sniezek, 1997)
. As a result, in a collaborative context it is optimal to match the level of confidence with that of a (potentially biased) collaborator 
(Bang et al., 2017)
, whereas in a competitive context there can be value in being overconfident 
(Johnson & Fowler, 2011)
. One could thus consider a more instrumental approach to the learning of confidence, where confidence is dynamically updated to maximize reward instead of matching the probability of making the correct choice. Our model, while still anchored on the probability of making the correct decision, takes a step in that direction by updating its parameters with a computational approach that can be used to optimize any potential confidence function.
Figure 1 :
1
Learning model of confidence. A.


Figure 2 :
2
Experimental procedure and manipulation. A. Time course of a trial. Participants judged whether a briefly


Figure 3 :
3
Effect of feedback manipulation on behavior. A. Accuracy was unaffected by our feedback manipulation. B. Participants showed overall faster RTs when receiving feedback of higher value, this effect


Note: Error bars reflect SEM. Shaded bars on the right panel of C represent SEM of model predictions. Shaded areas of D reflect SEM of behavioral data.


Figure 4 :
4
Model comparison. A. Difference in mean BIC over participants with the best-fitting model (lower ∆BIC means more evidence for that model). For learning models, the best ∆BIC over all three models is represented. B. Number of participants best explained by each model in BIC. C-F. Aggregated confidence ratings over the course of a feedback phase using the same conventions asFigure 3C. Behavioral data is split according to the best-fitting model groups (rows). The full learning model (C, E) accurately accounted for the data of both groups, while the non-learning model (D, F) failed to capture the gradual updating of confidence according to feedback in the dynamic group.








Acknowledgments
The research was supported by a starting grant from the KU Leuven (STG/20/006, https://www.kuleuven.be/kuleuven/), a Francqui Start-Up Grant from the Francqui


Foundation
(PXF-D8830, https://www.francquifoundation.be/) and two grants from the Research Foundation Flanders, Belgium (FWO-Vlaanderen, https://www.fwo.be/) (G0B0521N and G010419N).


Declaration of generative AI and AI-assisted technologies in the writing process






During the preparation of this work the authors used Microsoft Copilot in order to improve the readability and language of the manuscript. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.


Code and data availability
All raw data and code used for this manuscript are openly available here: https://osf.io/cjgz3/?view_only=f6fbf227fd57420abb591498fc8a414c.
 










Comparing Bayesian and non-Bayesian accounts of human confidence reports




W
T
Adler






W
J
Ma




10.1371/journal.pcbi.1006572








PLOS Computational Biology




14


11














Confidence controls perceptual evidence accumulation




T
Balsdon






V
Wyart






P
Mamassian




10.1038/s41467-020-15561-w








Nature Communications




11


1


1753
















D
Bang






L
Aitchison






R
Moran






S
Herce Castanon






B
Rafiee






A
Mahmoodi






J
Y F
Lau






P
E
Latham






B
Bahrami






C
Summerfield




Confidence matching in group decision-making


















10.1038/s41562-017-0117








Nature Human Behaviour




1


6














The calibration and resolution of confidence in perceptual judgments




J
V
Baranski






W
M
Petrusic








Perception & Psychophysics




55


4


















10.3758/BF03205299














Fitting Linear Mixed-Effects Models Using lme4




D
Bates






M
Mächler






B
Bolker






S
Walker




10.18637/jss.v067.i01








Journal of Statistical Software




67
















Bayesian confidence in optimal decisions




J
Calder-Travis






L
Charles






R
Bogacz






N
Yeung










Psychological Review
















Confidence predicts speed-accuracy tradeoff for subsequent decisions. eLife, 8, e43499




K
Desender






A
Boldt






T
Verguts






T
H
Donner




10.7554/eLife.43499


















Subjective Confidence Predicts Information Seeking in Decision Making




K
Desender






A
Boldt






N
Yeung








Psychological Science




29


5


















10.1177/0956797617744771














Learning optimal decisions with confidence




J
Drugowitsch






A
G
Mendonça






Z
F
Mainen






A
Pouget








Proceedings of the National Academy of Sciences


the National Academy of Sciences






116
















10.1073/pnas.1906787116














The Cost of Accumulating Evidence in Perceptual Decision Making




J
Drugowitsch






R
Moreno-Bote






A
K
Churchland






M
N
Shadlen






A
Pouget




10.1523/JNEUROSCI.4010-11.2012








Journal of Neuroscience




32


11
















Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation




S
M
Fleming






N
D
Daw








Psychological Review




124


















10.1037/rev0000045














Responsebased outcome predictions and confidence regulate feedback processing and learning. eLife, 10, e62825




R
Frömer






M
R
Nassar






R
Bruckner






B
Stürmer






W
Sommer






N
Yeung




10.7554/eLife.62825


















The Impact of Feedback on Perceptual Decision-Making and Metacognition: Reduction in Bias but No Change in Sensitivity




N
Haddara






D
Rahnev




10.1177/09567976211032887








Psychological Science




9567976211032887














The Evolution of Overconfidence




D
Johnson






J
Fowler




10.1038/nature10384








Nature




477
















Distorted learning from local metacognition supports transdiagnostic underconfidence




S
Katyal






Q
Huys






R
J
Dolan






S
Fleming




10.31234/osf.io/qcg92


















Bayesian inference with incomplete knowledge explains perceptual confidence and its deviations from accuracy




K
Khalvati






R
Kiani






R
P N
Rao




10.1038/s41467-021-25419-4








Nature Communications




12


1














Representation of Confidence Associated with a Decision by Neurons in the Parietal Cortex




R
Kiani






M
N
Shadlen








Science




324


5928


















10.1126/science.1169405














lmerTest Package: Tests in Linear Mixed Effects Models




A
Kuznetsova






P
B
Brockhoff






R
H B
Christensen








Journal of Statistical Software




82


















10.18637/jss.v082.i13














A low-dimensional approximation of optimal confidence




Le
Denmat






P
Verguts






T
Desender






K




10.1371/journal.pcbi.1012273








PLOS Computational Biology




20


7














Trading mental effort for confidence in the metacognitive control of value-based decision-making. eLife, 10, e63282




D
G
Lee






J
Daunizeau




10.7554/eLife.63282




















F
Meyniel






M
Sigman






Z
F
Mainen




10.1016/j.neuron.2015.09.039








Confidence as Bayesian Probability: From Neural Origins to Behavior






88














Post choice information integration as a causal determinant of confidence: Novel data and a computational account




R
Moran






A
R
Teodorescu






M
Usher




10.1016/j.cogpsych.2015.01.002








Cognitive Psychology




78
















Decision Confidence and Uncertainty in Diffusion Models with Partially Correlated Neuronal Integrators




R
Moreno-Bote








Neural Computation




22


7


















10.1162/neco.2010.12-08-930














DEoptim: An R Package for Global Optimization by Differential Evolution




K
Mullen






D
Ardia






D
L
Gil






D
Windover






J
Cline












SSRN Scholarly Paper 1526466








The role of decision confidence in advice-taking and trust formation




N
Pescetelli






N
Yeung








Journal of Experimental Psychology: General




150


3


















10.1037/xge0000960














Two-stage dynamic signal detection: A theory of choice, decision time, and confidence




T
J
Pleskac






J
R
Busemeyer




10.1037/a0019737








Psychological Review




117


3














Suboptimality in perceptual decision making




D
Rahnev






R
N
Denison




10.1017/S0140525X18000936








Behavioral and Brain Sciences




41














Confidence Leak in Perceptual Decision Making




D
Rahnev






A
Koizumi






L
Y
Mccurdy






M
Esposito






H
Lau








Psychological Science




26


11


















10.1177/0956797615595037














A theory of memory retrieval




R
Ratcliff




10.1037/0033-295X.85.2.59








Psychological Review




85


2
















Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability




R
Ratcliff






F
Tuerlinckx




10.3758/BF03196302








Psychonomic Bulletin & Review




9


3
















Signatures of a Statistical Computation in the Human Sense of Confidence




J
I
Sanders






B
Hangya






A
Kepecs




10.1016/j.neuron.2016.03.025








Neuron




90


3
















How do humans give confidence? A comprehensive comparison of process models of perceptual metacognition




M
Shekhar






D
Rahnev




10.1037/xge0001524








Journal of Experimental Psychology: General




153


3
















The Behavior of Organisms: An Experimental Analysis




B
F
Skinner








Appleton-Century-Crofts












Performance Feedback Triggers Liberal Detection and Perceptual Confidence Biases in Early Childhood: Implications for metacognitive training




D
Soto






M
Lallier






K
Desender






P
Elosegi




10.31234/osf.io/8u7b5


















Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








526






2nd ed (pp. xxii








Manipulating Prior Beliefs Causally Induces Under-and Overconfidence




H
Van Marcke






P
L
Denmat






T
Verguts






K
Desender








Psychological Science




35


4


















10.1177/09567976241231572














Natural statistics support a rational account of confidence biases




T
W
Webb






K
Miyoshi






T
Y
So






S
Rajananda






H
Lau




10.1038/s41467-023-39737-2








Nature Communications




14


1


3992














The Social Influence of Confidence in Group Decision Making




P
Zarnoth






J
A
Sniezek








Journal of Experimental Social Psychology




33


4


















10.1006/jesp.1997.1326















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]