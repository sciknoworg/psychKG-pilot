You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



speed with which algorithms generate predictions (fast versus slow) impacts people's willingness to trust these predictions. We contrast this with how the prediction speed of others affect an observer's willingness to trust their prediction. This provides us with insights into how the same cue (i.e., response time) can be interpreted differently as a function of different prediction providers (i.e., algorithmic-vs. human-generated predictions).
The article is organized as follows. We start by examining the recent literature in psychology and economics on how people interpret human response times in social interactions.
We subsequently discuss how different response times may influence trust in algorithmic predictions. We describe our experimental tests in the third section and conclude with a broader discussion of the results.


Prediction accuracy and response time as information
In recent years, researchers in psychology and economics have looked at how observing others' response times influences various interpersonal judgments and behaviors 
(Critcher, Inbar, & Pizarro, 2013;
Evans & van de Calseyde, 2017;
Konovalov & Krajbich, 2017;
Mata & Almeida, 2014;
van de Calseyde, Keren, & Zeelenberg, 2014)
. For decisions based on preferences, people believe that others' response times are associated with feelings of doubt or conflict. For example, 
Critcher and colleagues (2013)
 asked participants to evaluate the moral character of two persons who found wallets filled with cash. Both decided to keep the wallet, but one made the decision relatively quickly, whereas the other made the same decision slowly. In turn, the person who was slower to decide to keep the wallet was judged as less dishonest than the one who immediately chose to keep it (see Van de Calseyde et al., 2014 for how others' response times affect interpersonal choices).
In explaining these effects, the above-mentioned research found that people use observed response times as information. That is, slow decisions signaled feelings of conflict and doubt to observers (whereas fast decisions signaled confidence), explaining why people evaluated the person who was relatively slow in choosing to keep the wallet as less dishonest. However, slow response times are perceived differently for tasks that people presume require effort (e.g., making difficult predictions). In such cases, observing slower response times indicates that the person exerted the necessary effort to complete the task, whereas faster times reveal a lack of effort or commitment 
(Jago & Laurin, 2018;
Kupor, Tormala, Norton, & Rucker, 2014)
. Importantly, the more effort people believe others invest in completing relatively difficult tasks, whether in the form of time, physical exertion, pain, or money, the more positive the outcome of that effort is evaluated 
(Festinger, 1957;
Kruger, Wirtz, Van Boven, & Altermatt, 2004;
Labroo & Kim, 2009;
Norton, Mochon, & Ariely, 2012)
.
In testing this 'effort heuristic', 
Kruger and colleagues (2004)
 asked participants to evaluate the quality of two paintings made by the same artist. In one condition, participants were told that the artist finished the first painting in 18 hours, whereas it took her 4 hours to finish the second painting. In the second condition, this information was reversed (i.e., 4 hours to finish the first, 18 hours to finish the second painting). Consistent with the conjecture that people use time spent on completing a task as a heuristic for quality, paintings that took longer to finish were judged as being of higher quality (regardless of the order in which they were made). Here, we argue that the speed with which predictions are generated similarly influences how observers evaluate the quality of predictions. More precisely, given that slow response times and actions lead to perceptions of effort and commitment when completing difficult tasks, observers are expected to perceive others' predictions as being of higher quality when they are generated slowly (versus quickly).
Although slow response times are expected to increase the perceived quality of humangenerated predictions, it remains unclear how people would perceive slow algorithmic predictions. We propose that people have different expectations of how difficult prediction tasks are for algorithms, compared to humans. Some tasks, like image recognition for instance, are extremely easy for humans, but (currently) difficult for algorithms 
(Krizhevsky, Sutskever, & Hinton, 2012)
. Conversely, people may think that making a prediction is a relatively easy task for an algorithm, as it is an objective task involving the integration of multiple pieces of information or complex calculations 
(Castelo, Bos, & Lehmann, 2019)
. This view leads us to predict that slower response times will lead to lower quality evaluations of algorithm-generated predictions, as they will signal more effort being exerted for an ostensibly easy task.
This proposition is based on the notion that people perceive the quality of advice differently depending on whether the advice provider has engaged in the right amount of thinking required by the situation (i.e., when their level of thoughtfulness matches the apparent difficulty of the task). For example, 
Kupor and colleagues (2014)
 found that more thoughtful decisions (varied by describing how much effort was devoted) were seen as higher in quality, but only for difficult decisions. For easy decisions, the findings were less clear: more thoughtful decisions were generally seen as lower in quality, but the amount of thinking did not always have a statistically significant impact. This work suggests that the relationship between effort and perceived quality thus depends on observers' beliefs about task difficulty.
If people think that prediction tasks are easy for algorithms, then longer responses ought to lead to decreased prediction quality evaluations because the algorithm's level of effort would not match the apparent difficulty of the task.2 Conversely, one can predict that for tasks which people consider to be difficult for an algorithm, longer response times ought to lead to increased quality evaluations. However, we maintain that people will consider most prediction tasks to be easy for algorithms. Therefore, we should observe that longer response times generally lead to lower quality evaluations for algorithmic predictions.


Present research
2 More conservatively, the findings from 
Kupor et al. (2014)
 suggest that there should be no positive relationship between effort and perceived quality for easy tasks (if not a significant negative relationship).
We conducted seven studies (see 
Table for
 an overview) to test how people judge the quality of algorithm-and human-generated predictions. Using a variety of different prediction contexts and methodologies, we find that slow human predictions are judged as being of higher quality than fast human predictions. However, the opposite occurs for algorithms: fast algorithmic predictions are judged as superior to slow algorithmic predictions. While speed impacts perceptions of effort similarly for both algorithms and humans (i.e., slower speeds lead to perceptions of more effort being exerted), the relationship between perceived effort and prediction quality differs for humans and algorithms because people perceive prediction tasks to be easy for algorithms, but difficult for humans.
At the same time, we also observe that response time is a more evaluable attribute for humans than for algorithms as it has an impact both in joint (within-subject) and single (between-subject) evaluation conditions. While the effect of response time can appear in singleevaluation conditions for algorithms, this is moderated by the user's previous experience with the algorithm (i.e., slower predictions were judged as increasingly worse over time). Finally, we find that these inferences have behavioral consequences: people are more likely to choose a humangenerated prediction over a slowly generated algorithmic prediction. Additionally, in an incentivized study using sports predictions, we find that people are more willing to rely on quick (as opposed to slow) algorithmic predictions.
For all studies, we report how we determined the sample size, all data exclusions (if any), all manipulations, and all measures. All studies but one (Study 5) were pre-registered. The links to the registrations are provided in the appendix, where we also provide a link to the projects' OSF page with access to data, materials, and analysis code.
Data were analyzed using multi-level models with random estimates for participants and varying different prediction scenarios and response times across participants 
(Westfall, Kenny, & Judd, 2014)
. We relied on the lme4 
(Bates, Mächler, Bolker, & Walker, 2015)
 and the lmerTest 
(Kuznetsova, Brockhoff, & Christensen, 2017)
 packages in R to construct the models and extract p-values. Since there are currently no widely accepted effect size estimates for multi-level models we report standard Cohen's dz.
[Insert 
Table 1
 around here]


Studies 1 and 2
In Studies and 2 we investigated the impact of fast-versus slow response times on the perceived accuracy of human-vs. algorithmic predictions. We hypothesized that slow human predictions would be evaluated as more accurate than fast human predictions, whereas slow algorithmic predictions would be evaluated as less accurate than fast algorithmic predictions.
Both studies followed a similar procedure so we describe them together.


Methods


Participants
Both studies were conducted on MTurk. Participants were assigned to a 2 (Prediction provider: Human vs. Algorithm; between-subjects) x 2 (Response time: Fast vs. Slow; withinsubjects) mixed-design experiment. After excluding participants who did not pass the initial attention check and those who did not complete the entire study, there were 304 participants (46% female; MAge = 36.45, SDAge = 11.28) in Study 1 and 302 participants (47% female; MAge = 38.79, SDAge = 12.15) in Study 2.


Procedure
The two studies differed in the task scenarios used and whether an actual prediction, ostensibly made by a human or an algorithm, was shown. In Study 1, participants were told to imagine that they were an admissions officer working at a public university where they had to predict the academic success of potential students. They were then told that admission officers receive various pieces of information about each student and that this information is used to make predictions about the student's success. In Study 2, participants were told that they were sales officers working for a large consumer goods company and that their task was to predict the future sales of various products.
Participants were told that because of university (S1) or company (S2) regulations, as a quality assurance measure, one always needs to consult a colleague [an algorithm] when making a prediction. Additionally, they were told that they would know how much time the colleague [algorithm] took to generate the prediction. In Study 2, participants were also told that the company uses "boxes" to represent sales units and that a sales officer might predict future sales of an X number of boxes of a specific product. So, for each product, we provided participants with a prediction of boxes, ostensibly made by a human colleague 
[algorithm]
. The predictions could vary randomly from 10 to 90 boxes, in increments of ten.
Participants went through six randomly presented vignette scenarios, each representing an individual student (S1) or product (S2). Three of the predictions were described as provided quickly and three as provided slowly. The response time descriptions varied. For the fast predictions we used: "after only a couple of seconds", "immediately", and "straight away". For the slow predictions we used: "after a long pause", "after some time", and "after an extended period of time". No additional information about the colleague was provided. In the algorithm condition, the participants were told that the statistical algorithm is called "StatCast" and that it was designed by the university/company to predict the success of students (S1) or future sales (S2).
Participants evaluated what they thought the accuracy of the prediction was on a scale from -3 (very inaccurate) to 3 (very accurate).3 In addition, after providing all six of the accuracy estimates, each participant responded to two questions (one for fast and one for slow speedspresented randomly) on how likely they would have been to use the prediction as their own (-3 very unlikely to 3 very likely).


Results4
Perceived accuracy. A 2 (human = -0.5; algorithm = +0.5) x (fast = -0.5; slow = +0.5) analysis found a significant effect of the prediction provider in S1, F(1, 303) = 3.97, p = .05, dz = 0.11 and in S2, F(1, 300) = 23.77, p < .001, dz = 0.28. Algorithms were considered more accurate overall, compared to humans. In S1, there was also a main effect of response time, F(1, 303) = 4.59, p = .03, dz = 0.12. Slow predictions were considered as more accurate compared to fast predictions. In S2, there was no main effect of response time (F < 1). Most importantly, there was a two-way interaction in both S1, F(1, 303) = 25.03, p < .001, dz = -0.29 and S2, (1, 300) = 13.36, p < .001, dz = -0.21 (see 
Figure 1
, Study 1-A and Study 2-C subplot).
Next, we compared the simple effect of response time for human-and algorithmic predictions. Both in S1, F(1, 156) = 18.82, p < .001, dz = 0.25 and in S2, F(1, 154) = 6.84, p = .01, dz = 0.15, participants evaluated the accuracy of human-generated predictions as much higher when it was generated slowly, than when it was generated quickly. Similarly, both in S1, F(1, 147) = 4.07, p = .05, dz = -0.11 and in S2, F(1, 146) = 5.75, p = .02, dz = -0.14, participants evaluated the accuracy of algorithm-generated predictions as much lower when it was generated slowly, than when it was generated quickly.
Willingness to use predictions. Using the same analysis approach as above, we again found significant main effects of the prediction provider in S1, F(1, 303) = 4.05, p = .05, dz = 0.12 and in S2, F(1, 300) = 4.47, p = .04, dz = 0.12. There was again a main effect of response time in S1, F(1, 303) = 8.85, p = .003, dz = 0.29, but not in S2. Both effects were in the same direction as in the analysis above. Importantly, there was again a significant two-way interaction in both S1. F(1, 303) = 34.44, p < .001, dz = -0.57 and S2, F(1, 300) = 21.89, p < .001, dz = -0.27 (see 
Figure 1
, Study 1-B and Study 2-D subplots). Simple effects showed that for the human-generated predictions, participants were more willing to use those predictions that the human generated slowly in S1, F(1, 156) = 41.19, p < .001, dz = 0.37 and in S2, F(1, 154) = 13.61, p < .001, dz = 0.21. The reverse was true for algorithmic predictions in S1, F(1, 147) = 4.00, p = .05, dz = -0.11 and in S2, F(1, 146) = 8.71, p = .004, dz = -0.17; participants were more likely to use quickly generated predictions.
[Insert 
Figure 1
 about here]


Discussion
The first two studies demonstrate that the response time cue has differential effects on the perceived accuracy of human-versus algorithmic predictions. Specifically, slowly generated human predictions were seen as more accurate. However, this reversed for algorithms (i.e., slow predictions were seen as less accurate). Importantly, this result also extended to a person's willingness to use a prediction as their own (i.e., a greater willingness to use slowly generated human predictions, but a lower willingness to use slowly generated algorithmic predictions).
These effects replicated across two different task scenarios and when participants were provided with actual numeric predictions. Our next study investigates the mechanism underlying the different effects of response time on the perceived quality of human-vs. algorithmic predictions.


Study 3
The first two studies demonstrated that the relationship between response time and prediction quality differs for human vs. algorithmic predictions. Building on these results, we test a moderated mediation model where slower response times are seen as signaling more effort for both algorithms and humans. However, we predict that the relationship between effort and prediction quality evaluation is moderated by the prediction provider. This moderation is related to differences in perceived difficulty for humans vs. algorithms in making predictions. For human predictions, we expected that the prediction task should be seen as difficult; therefore, more effort should lead to higher quality evaluations 
(Kupor et al., 2014)
. For algorithms, the prediction task should be seen as easy. Therefore, more algorithmic effort should not be related to prediction quality, or more effort should lead to lower quality evaluations. To test this account, we conducted a study measuring perceived task difficulty for algorithms/humans, perceived effort, and prediction accuracy.


Method


Participants
Five hundred and four participants were recruited on MTurk. The study had the same design as Studies 1 and 2. We aimed to recruit 230 people per between-subject condition. After excluding people who failed the attention check or simply did not complete the full study, we had 486 participants (58% female; MAge = 38.39, SDAge = 11.04) in Study 3.


Procedure
The procedure was similar to Study 2 with three changes. First, we inserted a question asking people how difficult they thought making predictions was for humans/algorithms: "Fill in the blank: Predicting future sales is a task that is relatively ____ for an algorithm [human] to accomplish." Participants could either select "easy" or "difficult". We randomly varied whether this question was presented before or after participants were presented with any of the predictions. Second, after being presented with the speed of the prediction provider, participants were asked: "How much effort did your colleague 
[StatCast]
 exert to come to this prediction?".
They could answer on a 1 (Little effort) to 7 (Much effort) scale. Third, because the accuracy question was on a separate screen and after the effort question, we wanted to make sure that the participants were aware of the response time manipulation. We thus re-worded the question5 to: 


Results
As expected, most people (81.07%) thought making predictions is a difficult task for a human to accomplish, but an easy (78.60%) one for an algorithm, χ2 = 173.15, p < .001. Order in which the question was asked had no impact on the distribution of the answers. Next, we looked at the perceived accuracy. The same analysis approach as in Study 2 again found a significant effect of the prediction provider, F(1, 484) = 48.88, p < .001, dz = 0.32. Algorithms were considered more accurate overall (M = 4.80; SD = 1.39), compared to humans (M = 4.09; SD = 1.59). There was also a main effect of response time, F(1, 484) = 40.32, p < .001, dz = 0.29.
Slower predictions were considered more accurate overall (M = 4.77; SD = 1.28) than faster predictions (M = 4.13; SD = 1.69). More importantly, there was a significant two-way interaction, F(1, 300) = 98.98, p < .001, dz = -0.45. We compared the simple effects of response time on human-vs. algorithmic predictions. There was a significant effect of response time for humangenerated predictions, F(1, 242) = 165.95, p < .001, dz = 0.85. Participants believed that slowly generated human predictions were more accurate (M = 4.85; SD = 1.18), than quickly generated predictions (M = 3.34; SD = 1.59). There was also an effect of response time for algorithmgenerated predictions, F(1, 242) = 4.74, p = .03, dz = 0.14. In contrast to human predictions, slowly generated algorithmic predictions were seen as less accurate (M = 4.69; SD = 1.38) than quickly generated predictions (M = 4.91; SD = 1.39).
Moderated mediation model. We tested the model using STATA's GSEM builder. This was a 1-1-1 multilevel mediation model. Response time was set as the IV, effort was set as a mediator, and prediction quality evaluation was set as the DV. Crucially, prediction provider (human = -.5 vs. algorithm = +.5) was set as a moderator of the effort and prediction quality evaluation pathway. The overall indirect effect of perceived effort was significant, b = 1.43, SE = .06, z = 25.81, p < .001, 95% CI [1.32, 1.54]. However, prediction provider moderated the relationship between effort and prediction accuracy. The negative coefficient indicates a weaker relationship between effort and accuracy for algorithms, compared to humans (see upper-most section of 
Figure 2
).
[Insert 
Figure 2
 about here]
To better understand the pattern of moderated mediation, we conducted multi-level mediations for human and algorithmic predictions separately. For human predictions (see 
Figure   2
, lower left side), effort fully mediated the relationship between response time and prediction accuracy as slower response times led to the perception of more effort exerted which, in turn, led to higher prediction accuracy. For algorithms (see 
Figure 2
, lower right side), slower responses led to the perception of more effort exerted, but there was subsequently no relationship between effort and prediction accuracy6.


Discussion
As predicted, the asymmetric impact of different response times on the perceived accuracy of human-vs. algorithmic predictions can be explained by a mismatch in the expected difficulty of making predictions. Specifically, while making a prediction was considered to be an easy task for algorithms to accomplish, this task was seen as difficult for humans. This difference, in turn, had notable consequences in how observers responded to the inferred effort of slower response times. That is, while human effort (as inferred from slow responses) was positively correlated with the quality of another person's prediction, algorithmic effort was uncorrelated with the perceived quality of an algorithm's prediction. In the general discussion, we reflect in more detail on the implications of these findings for tasks other than predictions.


Study 4
In the previous study, we found that perceptions of task difficulty differed for human-vs. algorithmic predictions. In Study 4, we therefore explicitly manipulated task difficulty. Here, we expected that task difficulty would moderate the relationship between response time and perceived prediction quality. More specifically, when tasks are difficult, there should be a positive relationship between response time and quality, but when tasks are easy, there should be a negative relationship. Critically, task difficulty (rather than prediction provider) should be the primary factor that influences the relationship between response time and perceived prediction quality. In Study 4a, we use the same scenario as in Study 1, i.e., predicting the success of students, while in Study 4b we used a different scenario. Specifically, participants had to imagine being a human resource officer predicting how long employees will be absent from work.
Because the two studies had a similar procedure we again describe them together.


Method


Participants
Both studies were conducted on Mturk, both had 100 participants each (S4a: 39% female; MAge = 35.24, SDAge = 11.47; S4b: 42% female; MAge = 34.99, SDAge = 10.00), and the same mixed design: 2 (Prediction provider: Human vs. Algorithm; between-subject) x 2 (Response time: Fast vs. Slow; within-subject) x 2 (Task difficulty: Easy vs. Difficult; within-subject).


Procedure
The overall procedure was similar to Studies 1 and 2 with two differences. First, we directly manipulated the difficulty of the prediction. Participants in the easy task condition were presented with instructions which said that: "for a particular student (S4a) / employee (S4b), there were either nine or ten [one or two] valid pieces of information available, making the prediction easy [very difficult]". Second, we did not provide any numerical prediction in either of the studies.


Results


Perceived accuracy.
A 2 (human = -0.5; algorithm = +0.5) x 2 (fast = -0.5; slow = +0.5)
x (easy = -0.5; difficult = 0.5) analysis found that there was a main effect of difficulty both in S4a, F(1, 98) = 314.78, p < .001, dz = 1.80 and S4b, F(1, 98) = 223.07, p < .001, dz = 1.51. Accuracy evaluations were lower for difficult than easy predictions. There was a main effect of response time in S4b, F(1, 98) = 5.34, p = .02, dz = .23 with slowly generated predictions being judged as more accurate compared to faster predictions, but this effect did not appear in S4a.
In addition, there was a two-way interaction effect between response time and task difficulty both in S4a, F(1, 98) = 20.64, p < .001, dz = .45 and S4b, F(1, 98) = 6.50, p = .01, dz = .26. The interaction showed that there was a significant effect of response time for the difficult predictions both in S4a, F(1, 99) = 6.21, p = .01, dz = .25 and S4b, F(1, 99) = 11.99, p = .001, dz = .35. In S4a, there was an effect of response time for the easy predictions, F(1, 99) = 5.58, p = .02, dz = .24, but there was none in S4b. For difficult predictions, slower predictions were judged as more accurate compared to faster predictions. This reversed for the easy predictions. Slower predictions were judged as less accurate compared to faster predictions.
Finally, there was also a two-way interaction effect between prediction provider and response time both in S4a, F(1, 98) = 12.68, p = .001, dz = .36 and S4b, F(1, 98) = 13.38, p < .001, dz = .37 which showed that there was a significant effect of response time for human generated predictions both in S4a, F(1, 48) = 21.41, p < .001, dz = .67 and S4b, F(1, 47) = 8.55, p = .01, dz = .43. Just as in our previous studies, when the colleague took their time to generate the prediction, it was judged as more accurate, compared to when they were fast. However, the effect of response time was not significant for algorithmic predictions in S4a (F = 2.13) nor in S4b (F < 1), although it was in the same direction as our previous studies. Faster algorithmic predictions were judged as being of higher quality than slower ones. No other effects were significant (see 
Figure 3
).
[Insert 
Figure 3
 about here]
Willingness to use. There was a main effect of difficulty both in S4a, F(1, 98) = 168.98, p < .001, dz = 1.30 and S4b, F(1, 98) = 123.11, p < .001, dz = 1.11 with more difficult predictions being less likely to be used than easier predictions. In S4a, there was also a main effect of response time, F(1, 98) = 4.89, p = .03, dz = 0.22 with people being less willing to use predictions that were generated fast, compared to slow. There was no effect of response time in S4b.
In addition, there was also a two-way interaction effects between response time and difficulty both in S4a, F(1, 98) = 13.75, p < .001, dz = .37 and S4b, F(1, 98) = 5.71, p = .02, dz = .24 which showed that there was a significant effect of response time for the difficult predictions both in S4a, F(1, 99) = 13.82, p < .001, dz = .37 and S4b, F(1, 99) = 4.05, p = .05, dz = .20, but there was no effect for easy predictions in either study. For difficult predictions, people were more willing to use slower compared to faster generated predictions.
Finally, there was also a two-way interaction between prediction provider and response time both in S4a, F(1, 98) = 13.02, p < .001, dz = .36 and S4b, F(1, 98) = 15.19, p < .001, dz = .39 which showed that there was a significant effect of response time on human-generated predictions in S4a, F(1, 48) = 6.28, p = .02, dz = .39, and in S4b, F(1, 47) = 5.81, p = .02, dz = .35. Again, when the colleague took their time to generate a prediction, participants were more likely to use it than when they were fast. However, there was no significant effect of response time on algorithmic predictions in S4a (F < 1) nor in S4b (F = 1.74) although they were in the same direction as previous studies, with participants saying that they were more likely to use them for fast predictions than slow predictions. No other effects were significant.


Discussion
The results of both Study 4a and 4b show that once difficulty is explicitly manipulated, response time has a similar effect on the perceived accuracy of predictions for both algorithms and humans. Critically, task difficulty moderated the relationship between different response times and prediction quality: when the task was difficult, there was a positive relationship between response time and quality, but when the task was easy there was a negative relationship.


Study 5
In the previous studies, we relied on a within-subjects manipulation of response time. We focused on this approach because decision-makers often have repeated encounters with the same person or algorithmic support system. Nevertheless, it could be that response time is a much more easily evaluable attribute for humans as compared to algorithms 
(Hsee & Zhang, 2010)
.
Arguably, the average person has more prior experience with human predictions than algorithmic predictions, and this lack of experience with algorithms may make it more difficult to evaluate changes in an algorithm's response time. In Study 5, we therefore focus on algorithms and test the effect of response time on prediction quality evaluations in a single (between-subject) evaluation design. Crucially, we expected the effect of response time to become stronger once participants experienced multiple fast or slow predictions.


Method Participants
Two-hundred and forty-one participants were recruited on Prolific. The study had a single between-subject factor of response time (Fast vs. Slow). After excluding the people who failed an attention check presented at the end of the study, we were left with 236 participants (60% female; MAge = 35.44, SDAge = 11.91).


Procedure
We used a realistic task where participants were presented with English Championship League football predictions for an upcoming round of matches. We chose the Championship League, rather than the Premier League (which has some of the most famous teams in the world, e.g., Manchester United, Liverpool, etc.) to avoid our participants being too familiar with the taskin which case they may disregard algorithmic predictions entirely. The predictions presented to the participants were made by an actual algorithm from the "FiveThirtyEight" website.
Participants evaluated the quality of 12 predictions made by an algorithm called "StatCast". The league has 24 teams; hence 12 matches and 12 predictions were made for each weekly round of matches. Participants were told that the algorithm was developed at the Eindhoven University of Technology to predict the outcome of sporting matches. The presented matches were scheduled one week after we collected the data for this study. To expand on our main dependent variable, for each match, participants were asked: "How accurate do you think is StatCast's prediction?", and "How persuasive do you think is StatCast's prediction?" Ranging from -3 (Not at all) to 3 (Very much). To describe the predictions, we used the same wordings from previous studies. For fast predictions, we added: "Instantly", "Quite rapidly", and "With little or no delay". For slow predictions, we added: "With a substantial lag", "After a lengthy period", and "After an extensive delay". We had six response time wordings for both fast and slow speeds so the wordings were shown twice each, given that we had 12 trials. At the end, after going through all 12 trials, participants we asked if they were a fan of any particular club within the league (if they said yes, they were asked to type in the name of the club).


Results
The two measures of accuracy and persuasiveness were highly correlated, r = .76, p < .001 so we made one composite measure of perceived prediction quality (by averaging the answers). We first verified whether, taking into account all 12 trials, we would observe the same effect of response time as in previous studies. Note that now, participants were presented with the same response time descriptions, i.e., either just fast, or just slow. As expected, there was an effect of response time, F(1, 234) = 15.58, p < .001, dz = 0.26. Prediction quality in the slow condition was judged as lower (M = 4.06, SD = 1.44) than in the fast condition (M = 4.68, SD = 1.48)7.
Subsequently, we tested the effect of response time solely for first trials. We observed the same effect of response time, F(1, 234) = 6.03, p = .01, dz = 0.16 although considerably smaller than the overall effect (Mslow = 4.33; Mfast = 4.77). As expected, when we looked at the effect of response time solely for the last trials that participants experienced, the same effect was present, although much larger, F(1, 234) = 16.35, p < .001, dz = 0.26 (Mfast = 4.74; Mslow = 3.96). More experience with the same algorithm thus increased participants' sensitivity to algorithmic response times. Looking across all 12 trials, we see that predictions with slower responses were evaluated as worse over time (see 
Figure 4
).
[Insert 
Figure 4
 about here]


Discussion
Relying on sports predictions, we successfully replicated the same effect of algorithmic response times, but now in a between-subjects design. Specifically, participants who only experienced slowly generated predictions by an algorithm judged these predictions as worse than those who only experienced fast predictions. The effect increased as participants' experience with the algorithm increased8. Slow predictions were evaluated as much worse on the last trials, while the quality evaluations for fast predictions remained relatively stable over time. Experience with an algorithm is thus an important moderator of the effect of different algorithmic response times on people's quality evaluations.


Study 6
In the last two studies, we extend our findings to behavioral consequences of observing slow vs. fast algorithmic predictions. We focused solely on algorithms, as people are particularly 
8
 We also looked at how people evaluate prediction advice quality independent of seeing all other response time manipulations in all the other studies we use the within-subject manipulation of response time. We focused only on the first trial that participants saw (i.e., either a single fast or a single slow prediction). We found that, for humans, the same effect of response time can be observed. i.e., slower predictions were judged as being of higher quality. For algorithms, however, there was no difference, i.e., simply seeing either one fast or one slow prediction generated by an algorithm, did not have an effect on prediction advice quality. This is consistent with our proposition that response time is a more evaluable attribute for humans, than algorithms. For more detail about the analysis please see the supplementary material.
unwilling to use algorithm-generated advice, which is often better than advice generated by humans 
(Carroll et al., 1982;
Dietvorst et al., 2015;
Önkal et al., 2009)
. This means that not following algorithm-generated advice can have potentially negative consequences. In Study 6, we looked at the consequences of different algorithmic response times on seeking additional advice beyond the one provided by an algorithm. We expected that participants presented with slow (vs. fast) algorithmic predictions would be more likely to choose to use a human-generated prediction instead. In addition, we recruited a separate (smaller) sample of participants to gauge how willing people would be to go to another human prediction provider, where no information about the algorithm's response time was provided. We hoped that this would help us to position the effect more clearly (i.e., identify if the effects of different response times were driven more by fast-or slow algorithmic predictions).


Methods


Participants
Two hundred and twenty-six participants were recruited on MTurk. There was a single within-subject condition of response time. After excluding participants who did not pass the initial attention check and those who did not complete the full study, we had 200 participants (42% female; MAge = 35.89, SDAge = 11.28). Simultaneously, an additional 63 participants were recruited for the separate "no response time info" condition. After excluding those who did not pass the attention check and those who did not complete the full study, we were left with 50 participants in this condition (50% female; MAge = 34.24, SDAge = 9.16).


Procedure
The procedure was similar to Study 2. The only difference was the wording of the main dependent variable which now read: "Given StatCast's response time, how likely are you to disregard its prediction and consult a colleague instead"ranging from -3 (very unlikely) to 3 (very likely).


Results9
Willingness to disregard the algorithmic prediction. As expected, our analyses indicated that people were more likely to disregard the algorithm's prediction for a colleague's when it was generated slowly (M = 3.90, SD = 1.66) as opposed to quickly (M = 3.48, SD = 1.96), F(1, 199) = 7.15, p = .01, dz = 0.20.
No info about response time. When no information about the algorithm's response time was given, the average willingness to consult a colleague was similar to the fast condition (No information: M = 3.54, SD = 1.83; Fast prediction: M = 3.48; SD = 1.96; t(248) = 0.20, p = .84).
These results indicate that the effect of response time is most likely driven by situations when the algorithm took its time to generate the prediction.


Discussion
Results of Study 6 show that the effect of different algorithmic response times extends to situations where participants are given an opportunity to consult another person for a prediction.
People were more likely to disregard slow (vs. fast) algorithm-generated predictions.


Study 7
In our final study, we conducted an incentivized test of the behavioral consequences of observing algorithmic response times, relying on the sports prediction task as introduced in Study 5. Specifically, participants were given the opportunity to choose those sports predictions that would go towards a monetary bonus. That is, we paid an extra reward for each prediction that the participant chose and that turned out to be true (e.g., if the algorithm suggested Blackburn Rovers would win and they actually won, participants would get an extra £.05). Data 9 Participants were also asked to evaluate how much effort they thought the algorithm exerted. Slower predictions were again evaluated as the algorithm exerting more effort, F(1, 199) = 99.56, p < .001, dz = 0.71. In addition, at the end of the study, participants were also asked to evaluate StatCast's quality as an algorithm given the time it took to provide the predictions, evaluating all six different response time descriptions. The graphical representation of the answers essentially indicates that StatCast was judged as being of lower quality for slow speed descriptions. The analysis code allows the interested reader to generate the graph, but we do not consider it relevant to report it in the main text of the article.
were collected two days before the first match was scheduled. We hypothesized that people would be more likely to choose a sports prediction that the algorithm generated fast as opposed to slow. In addition, we also wanted to explore whether there would be any differences between a UK sample (which should be more familiar with the English Championship League) and a US sample (which should be less familiar with it) in how different response times would impact quality evaluations and behaviors.


Method Participants
Three hundred and forty-nine people took the survey on Prolific. After excluding people who failed the attention check or simply did not complete the full study, we were left with 20010 participants (60% female; MAge = 34.66, SDAge = 11.81). The sample had 100 participants from the UK (72% female; MAge = 35.48, SDAge = 11.68) and 100 participants from the US (48% female; MAge = 33.84, SDAge = 11.95). Response time (Fast vs. Slow) was the only within subject factor.


Procedure
The procedure was similar to Study 5 but for five differences. First, the matches were updated to select upcoming matches at the time that this study was conducted. Second, response time was provided in actual numbers to participants. Specifically, for each trial, a random number ranging from 4.9 to 6.9 was generated. In the fast conditions, 4 seconds were subtracted from this number while in the slow conditions, 6 seconds were added to illustrate the algorithm's response time. This way, we also knew which response time each participant saw. Third, after going through the 12 trials, participants were shown a list of all the predictions with the same 10 In our preregistration, we stated that we would exclude participants that spent, on average, more than 10 seconds on each trial as this might indicate that they have looked up information about the games. After verifying the average times, we realized we underestimated the necessary time as 98 participants would need to be excluded. We decided to void this aspect of our registration since it would mean discarding 50% of our sample resulting in a serious lack of statistical power to detect an effect. response times that they saw during the trials. They could then choose three of these predictions as "their own", meaning that they would receive an additional monetary reward of £.05 for each of the predictions that turned out to be true. There was no deception involved since we verified the results after the matches were played and paid out each participant dependent on their choices. Fourth, towards the end, we explored participants' familiarity with the English Championship League by presenting them with four statements for which they had to indicate their agreement from -3 (Completely disagree) to 3 (Completely agree). The statements were: "I am an avid fan of the English Championship League", "I consider myself an expert when it comes to the English Championship League", "I watch at least one of the English Championship League matches every week (during the season)", "I am familiar with the current standings in the English Championship League." Cronbach's alpha was very high at .94 so we made one composite measure by averaging the results of the four statements. Fifth, for each prediction (i.e., each match), it was randomly determined whether StatCast predicted the outcome of the match in a fast or slow way.


Results
The two measures of accuracy and persuasiveness were highly correlated, r = .80, p < .001 so we made one composite measure of perceived prediction quality (by averaging the answers). Consistent with previous studies, we found a significant effect of response time, F(1, 199) = 29.95, p < .001, dz = 0.39. Participants considered slow algorithmic predictions to be of a lower quality (M = 4.05; SD = 1.54), compared to fast predictions (M = 4.84; SD = 1.63).
To verify whether there were any differences in familiarity between UK and US participants, we compared our participants' scores on the familiarity measure. Indeed, participants in the UK said that they were more familiar with the English Championship League (M = 2.61; SD = 1.71) than participants in the US (M = 1.58; SD = 1.10), t(198) = 5.05, p < .001, dz = .72. Including country as a variable in our analysis, we again obtained an effect of response time, F(1, 198) = 30.54, p < .001, dz = 0.40, and a two-way interaction with country and response time, F(1, 198) = 5.30, p = .02, dz = 0.16. There was no main effect of country (F < 1).
In decomposing the interaction (see 
Figure 4
), we found a significant effect of response time for both the UK, F(1, 99) = 6.53, p = .01, dz = 0.26 and US participants, F(1, 99) = 24.76, p < .001, dz = 0.50, although it is clear that the difference in quality evaluations for predictions made quickly and predictions made slowly was much stronger for US participant as compared to UK participants.
[Insert 
Figure 5
 around here]
We also verified whether there would be an effect of response time if we did not use the categorical (Fast vs. Slow) conceptualization as the independent variable, but instead if we used the actual numerical values of response times shown to the participants. Again, there was a clear negative relationship b = -.14, SE = 0.054, t(1607.6) = -2.50, p = .01, indicating that the longer it took an algorithm to come to a prediction, the lower the perceived quality of its prediction.
Choice data. Each person could choose three predictions that would go towards their bonus, meaning 600 choices were made in total. Had people shown no preference for either fast or slow predictions, we would have observed something close to a 50/50 distribution. However, and in accordance with our expectations, the data showed that people actually chose 381, or 63.5% fast predictions overall. A binomial test indicated that this was significantly different than the expected 50/50 distribution, p < .001 (two-sided). Looking only at UK participants, 59.3% of their choices favored a fast prediction. A binomial test again indicated that this was significantly different from the 50/50 distribution, p = .001 (two-sided). As expected, for US participants, even more choices favored fast predictions (67.6%), p < .001 (two-sided).


Discussion
Using sports predictions, a more concrete response time manipulation (i.e., using numbers rather than textual descriptions), and an incentivized prediction task, we confirmed that slow response times had a detrimental impact on the perceived quality of algorithmic prediction.
People judged slower predictions as less accurate and less persuasive, and they were less likely to rely on them for their bonuses. This tendency was much more pronounced in a US sample, where familiarity with the English Champions League (the domain in which the predictions were made) was much lower. Thus, response time was a much more relied upon cue in situations that are unfamiliar, leading individuals to display an even stronger condemnation for slowly generated algorithmic predictions.


General discussion
When are people reluctant to trust algorithm-generated advice? Here, we demonstrate that it depends on the algorithm's response time. People judged slowly (vs. quickly) generated predictions by algorithms as being of lower quality. Further, people were less willing to use slowly generated algorithmic predictions. For human predictions, we found the opposite: people judged slow human-generated predictions as being of higher quality. Similarly, they were more likely to use slowly generated human predictions.
We find that the asymmetric effects of response time can be explained by different expectations of task difficulty for humans vs. algorithms. For humans, slower responses were congruent with expectations; the prediction task was presumably difficult so slower responses, and more effort, led people to conclude that the predictions were high quality. For algorithms, slower responses were incongruent with expectations; the prediction task was presumably easy so slower speeds, and more effort, were unrelated to prediction quality. In short, response times have a nuanced effect on advice quality evaluations. Indeed, for more difficult judgments, longer response times may lead to similar perception of quality for algorithms as for humans, namely: slower responses leading to higher quality evaluations.
Similarly, we find that the effect of algorithmic response times on prediction quality evaluations appeared both in a between-and within-subject setting, and that the effect of response time is moderated by a person's experience with an algorithm. Specifically, as people repeatedly experienced slow algorithms, the (detrimental) effect of slow algorithmic responses on prediction quality evaluations became stronger. Finally, focusing on algorithms specifically, we find that slow algorithmic predictions can lead people to seek out additional advice from other humans. Confirming the importance of response time as a cue, a subset of people who were unfamiliar with the prediction domain relied even more on the time algorithms needed to make predictions.
Previous research has identified response time as an important cue in social interactions 
(Critcher et al., 2013;
Evans & Van de Calseyde, 2017;
Mata & Almeida, 2014;
Van de Calseyde et al., 2014)
 and participants in our studies also used it as information to evaluate the quality of others' predictions. However, while most prior research indicates that observed response times are interpreted in terms of doubt 
(Critcher et al., 2013;
Evans & van de Calseyde, 2017;
Van de Calseyde et al., 2014)
, the current results demonstrate that response times can also be interpreted in terms of effort 
(Jago & Laurin, 2018;
Kupor et al., 2014)
. More specifically, if doubt (rather than effort) was the main information that response times signaled, we would have seen different results. That is, people would have perceived fast predictions by others as more accurate as faster response times have been shown to indicate more confidence (Van de Calseyde et al., 2014) and people generally prefer confident (over doubtful) predictions 
(Stavrova & Evans, 2018)
.
Interestingly, while people interpreted algorithmic response times in terms of effort (i.e., slow predictions indicate more effort exertion by an algorithm), people seem to see it as undiagnostic when evaluating the quality of predictions. We speculate that this is due to the fact that algorithms are judged more as tools that perform complicated tasks following closed and structured procedures 
(Simon & Neisser, 1992)
. Therefore, tasks that involve complex calculations are seen as easy for algorithms to accomplish, making the presence or absence of effort relatively meaningless. Nonetheless, while perceived effort did not serve as a suitable mechanism in explaining how algorithmic response times affect people's quality evaluations, there could be other possible mechanisms that govern this relationship. One potential avenue for future research is to investigate whether people have default assumptions about algorithms such that observing slowness might be indicative of an algorithm's "bugginess".
The model that relies on task difficulty as a moderator of response times allows for several predictions that are relevant for future research. For instance, following this model, we would predict that tasks that are seen as difficult (easy) for algorithms (humans) slower response times would lead to higher (lower) quality evaluations. This theorizing is also relevant to other domains such as moral judgments. Previous work suggests that increased deliberation on tragic trade-offs reaffirms the solemnity of the occasion (i.e., longer response times breed trust), while deliberation on taboo trade-offs undermines trust 
(Tetlock, Kristel, Elson, Green, & Lerner, 2000)
. Thus, in some cases, the longer one takes on contemplating indecent proposals, the more one's moral identity is compromised. It could be that moral judgments constitute a separate cognitive arithmetic and are thus differently amenable to response times than judgments (e.g., forecasting, recognition, calculation). It is worth pointing out that recent evidence suggests that people seem to be strongly averse to algorithms making any sort of moral decisions 
(Bigman & Gray, 2018)
, so a challenge for future research is to understand how response time might modulate trust in algorithmic advice when applied to the moral domain.
Response time also seems to be a more evaluable attribute for humans than for algorithms. We obtained several indications for this notion throughout our studies. First, effect sizes of response time for humans were consistently much larger than for algorithms. Second, the response time effect was reliably obtained for humans even when experiencing only a single indication of fast or slow response time (i.e., a between-subject designsee also supplementary material). Conversely, for algorithms, it appears that experience with the algorithm can play a crucial role as the results of Study 5 suggest. It is worth pointing out thought that Study 5 did not include a human prediction provider condition which would have allowed for a direct comparison of between-subject effects across both human and algorithm predictions providers.
Consistent with general evaluability theory 
(Hsee & Zhang, 2010)
, people might not have relevant reference information for different response times in algorithms. As it increasingly becomes more likely that people will interact with the same algorithms, sensitivity to the attribute of response time might play an important role in how we evaluate algorithm-generated advice in the future.
In our studies, people were generally trusting of algorithmspredictions provided by algorithms were judged to be better overall. These results are in line with the idea that algorithm aversion primarily arises when people observe an algorithm fail 
(Dietvorst et al., 2015;
Dietvorst, Simmons, & Massey, 2016)
. Similarly, other recent work has found that advice has a greater impact on people when they think it comes from algorithms 
(Logg, Minson, & Moore, 2019)
 and the reported findings in the current paper are consistent with this notion.
Practically, our results could have important implications: algorithmic response times can have a profound impact on the way people evaluate and use advice. This implies that people might be sensitive to imperfections, glitches, or delays, when advice by an algorithm is being provided, leading them to adversely (and perhaps erroneously) disregard the advicein particular when people have repeated experiences with an algorithm. As already argued, this could have various negative consequences such as leading people to solicit further advice or, if the advice situation is particularly unfamiliar, a larger reliance on response time as a cue.
Conversely, making fast response times salient may increase a person's reliance on algorithmic predictions. Future research could address this interesting question in more detail by testing whether and when response times can be used as a nudge to increase a person's trust in algorithmic advice.
In the supplementary material, we report an additional two studies that tackle the question whether prediction provider's expertise, and the direction of the prediction (i.e., whether an increase or a decrease was predicted) moderate the impact of different response times on humanvs. algorithmic predictions. Study 8 looked at the potential impact of advice provider expertise.
For average expertise, both human-and algorithmic predictions were considered more accurate when provided slowly, compared to predictions provided quickly. However, we observed no effects in the expert conditions, possibly due to a ceiling effect. Finally, Study 9 focused only on algorithmic predictions and looked at whether response time would have a different impact dependent on whether the prediction was of an increase compared to a decrease. Prediction direction did not have an effect. Another important direction for future research is to look at situations which are inherently riskier, more important in terms of their consequences, and more high-stakes. While general algorithm aversion could apply for these situations 
(Logg, 2017)
, and it seems rare that people still have misgivings on applying algorithms in such situations, important cues like response time (and others) could moderate algorithm advice evaluation.


Conclusion
Given the ubiquity of prediction algorithms, as well as their general superiority in providing high-quality advice, understanding how subtle cues may impact the way people evaluate algorithms is both timely and important. The present research is an initial step towards understanding this matter by demonstrating how different algorithmic response times affect people's evaluations and behaviors. A very simple cue such as response time, which at times can even be just a random fluctuation, can evidently lead individuals to disregard or adopt an algorithm's solution.      
"
Given your colleague's [algorithm's] delayed [quick] response time, how accurate do you think is his [its] prediction?"


Figure captions
Figure captions


Figure 1 .
1
The means and standard errors of Study 1 (upper row) and Study 2 (lower row) results on perceived accuracy of the generated prediction (A and C) and willingness to use generated prediction (B & D) as a function of prediction provider (Algorithm vs. Human) and response time (Fast vs. Slow).


Figure 2 .
2
Path models with corresponding coefficients for the moderated mediation model (upper section of figure), the mediation model for the human prediction provider only (lower left section of figure) and the mediation model for the algorithm prediction provider only (lower right section of the figure). ns p < .05; * p < .05; ** p < .01; *** p < .001. The reported coefficients are unstandardized.


Figure 3 .
3
The means and standard errors of Study 4a (upper row) and Study 4b (lower row) results on perceived accuracy of the generated prediction as a function of prediction provider (Algorithm vs. Human), response time (Fast vs. Slow), and task difficulty (Easy vs. Difficult).


Figure 4 .
4
The means and standard errors of Study 5 on advice quality as a function of response time (Fast vs. Slow) and experience with the algorithm (i.e., ranging from the first to the twelfth trial).


Figure 5 .
5
The means and standard errors of Study 7 results on advice quality as a function of participants' country of origin (UK vs. US) and response time (Fast vs. Slow).


For the purposes of this paper, we loosely define "algorithm" to include any evidence-based forecasting formulas and rules such as statistical models, decision aids, or other mechanical procedures
(Dietvorst, Simmons, & Massey, 2015)
.


The scale was re-coded to range from 1 to 7 in the analysis. This was the case in all studies that used these anchors.


In the preregistration we stated that we would perform mixed ANOVA's and regressions. We report the regressions to be in line with the other presented studies. However, the data analysis files (https://osf.io/efauv/) contain code for performing the ANOVA's which show the same results.


Although this text may raise the possibility of demand effects, we note that we obtained similar results in studies that did not include this text (e.g., Study 5).


We also tested the same model using perceived difficulty as the moderator instead of prediction provider. As perceived difficulty is closely related to prediction provider, we expected to obtain the same results. As expected, the results were replicated. The exact statistics are provided in the OSF materials (https://osf.io/ykamv/).


Twelve participants said that they were a fan of a specific club in the league. Excluding those participants, the effect remained significant and was slightly stronger at dz = .27.














Fitting Linear Mixed-Effects Models Using lme4




D
Bates






M
Mächler






B
Bolker






S
Walker




10.18637/jss.v067.i01








Journal of Statistical Software




67


1


















A
H
Beck






A
R
Sangoi






S
Leung






R
J
Marinelli






T
O
Nielsen






M
J
Vijver






Van De














Systematic Analysis of Breast Cancer Morphology Uncovers Stromal Features Associated with Survival




D
Koller




10.1126/scitranslmed.3002564








Science Translational Medicine




3


108
















People are averse to machines making moral decisions




Y
E
Bigman






K
Gray




















10.1016/j.cognition.2018.08.003








Cognition




181














Evaulation, Diagnosis, and Prediction in Parole Decision Making




J
S
Carroll






R
L
Wiener






D
Coates






J
Galegher








Law & Society Review




17














Task-Dependent Algorithm Aversion




N
Castelo






M
W
Bos






D
R
Lehmann








Journal of Marketing Research




56


5


















10.1177/0022243719851788














How Quick Decisions Illuminate Moral Character




C
R
Critcher






Y
Inbar






D
A
Pizarro








Social Psychological and Personality Science




4


3


















10.1177/1948550612457688














A case study of graduate admissions: Application of three principles of human decision making




R
M
Dawes








American Psychologist




26


2


















10.1037/h0030868














Lay Perceptions of Selection Decision Aids in US and Non-US Samples




D
L
Diab






S.-Y
Pui






M
Yankelevich






S
Highhouse




10.1111/j.1468-2389.2011.00548.x








International Journal of Selection and Assessment




19


2
















Algorithm aversion: People erroneously avoid algorithms after seeing them err




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1037/xge0000033








Journal of Experimental Psychology: General




144


1
















Overcoming Algorithm Aversion: People Will Use Imperfect Algorithms If They Can (Even Slightly) Modify Them. Management Science, mnsc




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1287/mnsc.2016.2643


















The effects of observed decision time on expectations of extremity and cooperation




A
M
Evans






P
P F
Van De Calseyde




10.1016/j.jesp.2016.05.009








Journal of Experimental Social Psychology




68
















A Theory of Cognitive Dissonance




L
Festinger








Stanford University Press












Against Your Better Judgment? How Organizations Can Improve Their Use of Management Judgment in Forecasting. Interfaces




R
Fildes






P
Goodwin




10.1287/inte.1070.0309








37


















C
K
Hsee






J
Zhang




10.1177/1745691610374586








General Evaluability Theory. Perspectives on Psychological Science




5


4
















Inferring Commitment from Rates of Organizational Transition




A
S
Jago






K
Laurin




10.1287/mnsc.2017.2980








Management Science
















Revealed Indifference: Using Response Times to Infer Preferences (SSRN Scholarly Paper No. ID 3024233). Retrieved from Social Science Research Network website




A
Konovalov






I
Krajbich




















ImageNet Classification with Deep Convolutional Neural Networks




A
Krizhevsky






I
Sutskever






G
E
Hinton




F. Pereira, C. J. C. Burges, L. Bottou, & K. Q


















Weinberger






Advances in Neural Information Processing Systems




25














The effort heuristic




J
Kruger






D
Wirtz






L
Van Boven






T
W
Altermatt




10.1016/S0022-1031








Journal of Experimental Social Psychology




40


1
















Thought Calibration: How Thinking Just the Right Amount Increases One's Influence and Appeal




D
M
Kupor






Z
L
Tormala






M
I
Norton






D
D
Rucker








Social Psychological and Personality Science




5


3


















10.1177/1948550613499940














lmerTest Package: Tests in Linear Mixed Effects Models




A
Kuznetsova






P
B
Brockhoff






R
H B
Christensen








Journal of Statistical Software




13


82
















10.18637/jss.v082.i13














The "Instrumentality" Heuristic: Why Metacognitive Difficulty Is Desirable During Goal Pursuit




A
A
Labroo






S
Kim








Psychological Science




20


1


















10.1111/j.1467-9280.2008.02264.x














Algorithm appreciation: People prefer algorithmic to human judgment




Jennifer
M
Logg






J
A
Minson






D
A
Moore




10.1016/j.obhdp.2018.12.005








Organizational Behavior and Human Decision Processes




151
















Theory of Machine: When Do People Rely on Algorithms? Harvard Business School Working Paper Series # 17-086




Jennifer
Logg






Marie




















Using metacognitive cues to infer others' thinking




A
Mata






T
Almeida








Judgment & Decision Making




9


4
















Clinical versus statistical prediction: A theoretical analysis and a review of the evidence




P
E
Meehl




10.1037/11281-000


















The IKEA effect: When labor leads to love




M
I
Norton






D
Mochon






D
Ariely








Journal of Consumer Psychology




22


3


















10.1016/j.jcps.2011.08.002














Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy




C
O'neil








Crown/Archetype












The relative influence of advice from human experts and statistical methods on forecast adjustments




D
Önkal






P
Goodwin






M
Thomson






S
Gönül






A
Pollock




10.1002/bdm.637








Journal of Behavioral Decision Making




22


4
















Robot lawyer DoNotPay now lets you 'sue anyone' via an app




J
Porter














Retrieved








Decision Support Systems: Concepts and Resources for Managers




D
J
Power








Greenwood Publishing Group












Can computers help us understand the human mind




H
Simon






U
Neisser








Taking sides: Clashing views on contraversial psychological issues


















Decision aids for people facing health treatment or screening decisions




D
Stacey






F
Légaré






K
Lewis






M
J
Barry






C
L
Bennett






K
B
Eden






L
Trevena




10.1002/14651858.CD001431.pub5








Cochrane Database of Systematic Reviews




4














Examining the trade-off between confidence and optimism in future forecasts




O
Stavrova






A
M
Evans








Journal of Behavioral Decision Making




32
















The psychology of the unthinkable: Taboo trade-offs, forbidden base rates, and heretical counterfactuals




P
E
Tetlock






O
V
Kristel






S
B
Elson






M
C
Green






J
S
Lerner








Journal of Personality and Social Psychology




78


5


















10.1037/0022-3514.78.5.853














Decision time as information in judgment and choice




P
P F M
Van De Calseyde






G
Keren






M
Zeelenberg




10.1016/j.obhdp.2014.07.001








Organizational Behavior and Human Decision Processes




125


2
















Technically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech




S
Wachter-Boettcher








W. W. Norton & Company












Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli




J
Westfall






D
A
Kenny






C
M
Judd








Journal of Experimental Psychology: General




143


5


















10.1037/xge0000014














Algorithms (and the) everyday. Information




M
Willson




10.1080/1369118X.2016.1200645








Communication & Society




20


1
















Computer-based personality judgments are more accurate than those made by humans




W
Youyou






M
Kosinski






D
Stillwell




10.1073/pnas.1418680112








Proceedings of the National Academy of Sciences




112


4
















Appendix OSF link to data, materials








Links for preregistrations of individual studies: Study




1













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]