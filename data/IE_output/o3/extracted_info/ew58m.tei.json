[
  {
    "measured_by": "phasic midbrain dopamine neuron activity",
    "justification": "“Phasic bursts from midbrain dopamine (DA) neurons have been proposed as a neural correlate of the reward prediction error (RPE) signal central to RL theories.”",
    "construct": "Reward prediction error"
  },
  {
    "measured_by": "striatal BOLD response",
    "justification": "“…the BOLD signal in the striatum was consistent with a policy-update signal, rather than an action-value update [47].”",
    "construct": "Policy-update signal"
  },
  {
    "measured_by": "rich-versus-lean context choice task",
    "justification": "“When given a choice between two options… learned in a rich environmental context… and… in a lean context… humans and other animals display a marked preference for the lean-context option.”",
    "construct": "Context-dependent preference learning"
  },
  {
    "measured_by": "sensorimotor adaptation task with mirror-reversed visual feedback",
    "justification": "“Hadjiosif et al. used mirror-reversed visual feedback in a sensorimotor adaptation task, and found that subjects’ patterns… were well explained by a model in which an implicit behavioral policy was updated…”",
    "construct": "Implicit behavioral policy updating"
  },
  {
    "measured_by": "reaction-time choice task modeled with a two-parameter gamma policy",
    "justification": "“…policy-gradient RL algorithms are straightforwardly applicable to continuous action spaces… (e.g., a two-parameter gamma distribution for reaction time choices; [32]).”",
    "construct": "Continuous-action learning"
  }
]