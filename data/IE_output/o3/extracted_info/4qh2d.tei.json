[
  {
    "measured_by": "REFERENCE model (computational model)",
    "justification": "“By following the intuition of AL and PT theories, we proposed a model that learns the value of a reference-point and uses it to dynamically center the outcomes… We refer to this model as the REFERENCE model.”",
    "construct": "Reference-point dependence in reinforcement learning"
  },
  {
    "measured_by": "learning + transfer phase gain/loss reinforcement-learning task",
    "justification": "“Our behavioral paradigm joins a learning phase with a transfer phase… The type of outcome defined the learning context: ‘gain’… or ‘loss’.”",
    "construct": "Reference-point dependence in reinforcement learning"
  },
  {
    "measured_by": "RANGE model (computational model)",
    "justification": "“In line with RF theory, we proposed a model that learns the range of possible outcomes and uses it to dynamically rescale the outcomes… This model, referred to as the RANGE model, satisfactorily captures the key behavioral effects.”",
    "construct": "Outcome range adaptation in reinforcement learning"
  },
  {
    "measured_by": "reinforcement-learning task manipulating outcome magnitudes",
    "justification": "“We built upon the previous behavioral paradigms to include systematic manipulation of outcome magnitudes, generating learning contexts with different outcome ranges.”",
    "construct": "Outcome range adaptation in reinforcement learning"
  },
  {
    "measured_by": "three-option reinforcement-learning paradigms designed to elicit IIA violations",
    "justification": "“The authors designed several behavioral paradigms, where participants choose between three options whose expected values were designed to elicit classic violations of IIA.”",
    "construct": "Dependence on irrelevant alternatives in reinforcement learning"
  }
]