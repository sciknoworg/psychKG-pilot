[
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "“The Go/No-Go task … captures the tendency to approach rewarding stimuli … The strengths of the ‘Go’ biases are controlled by two parameters: b for the unlearned (instrumental) bias and π for the learned (Pavlovian) bias.”",
    "construct": "Pavlovian bias"
  },
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "The same model estimates “b for the unlearned (instrumental) bias,” making the Go bias one of the fitted computational parameters of the Go/No-Go task.",
    "construct": "Go bias"
  },
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "“The instrumental and Pavlovian learning processes are jointly controlled by a learning rate parameter (ε) … We fit … ε … to individual participants.”",
    "construct": "Learning rate"
  },
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "“Noise in the action selection process is controlled by a lapse rate parameter (ξ) that captures the probability of random responses.”",
    "construct": "Lapse rate"
  },
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "“The instrumental and Pavlovian learning processes are jointly controlled by … the ‘effective size’ (ρ) of different outcome types … we fit a separate ρ parameter.”",
    "construct": "Effective reward/punishment size"
  },
  {
    "measured_by": "Go/No-Go task – Pavlovian reinforcement-learning model",
    "justification": "They “introduced an additional parameter … effective size of neutral outcomes” to capture how neutral feedback influences behavior in the Go/No-Go task.",
    "construct": "Effective neutral-outcome size"
  },
  {
    "measured_by": "Change Detection task – Absolute Difference (MAD) model",
    "justification": "In the Change Detection section: “The noise in the image representation is accounted for by a sensitivity parameter σ, and the decision threshold is controlled by the detection threshold θ. … we focus on the dynamics of σ and θ.”",
    "construct": "Detection threshold"
  },
  {
    "measured_by": "Change Detection task – Absolute Difference (MAD) model",
    "justification": "Same passage states that σ is the “sensitivity parameter” capturing noise in representations and is estimated from Change Detection trials.",
    "construct": "Sensitivity (noise parameter)"
  },
  {
    "measured_by": "Random Dot Motion task – Drift Diffusion Model",
    "justification": "“In the drift diffusion model, evidence accumulation is controlled by the drift rate parameter δ … we fit a single drift rate for each session.”",
    "construct": "Drift rate"
  },
  {
    "measured_by": "Random Dot Motion task – Drift Diffusion Model",
    "justification": "“The threshold for decision is controlled by the decision boundary α.”",
    "construct": "Decision boundary (boundary separation)"
  },
  {
    "measured_by": "Random Dot Motion task – Drift Diffusion Model",
    "justification": "Reaction times “are modeled as the sum of the evidence accumulation time and the non-decision time τ, which captures processes such as stimulus encoding and motor response.”",
    "construct": "Non-decision time"
  },
  {
    "measured_by": "Lottery Ticket task – Risk-sensitive utility model",
    "justification": "“We used a risk-sensitive utility function … governed by the risk attitude parameter (ρ). Higher ρ indicates a higher tendency to prefer risky choices.”",
    "construct": "Risk attitude"
  },
  {
    "measured_by": "Lottery Ticket task – Softmax decision rule",
    "justification": "“We used a softmax probability function to model participants' choices, with the inverse temperature parameter (β) as a free parameter.”",
    "construct": "Inverse temperature (choice stochasticity) – Lottery"
  },
  {
    "measured_by": "Intertemporal Choice task – Hyperbolic discounting model",
    "justification": "“The tendency to prefer delayed rewards is represented by the discount rate parameter (k). Larger k values indicate greater impulsivity and preference to receive immediate rewards.”",
    "construct": "Discount rate"
  },
  {
    "measured_by": "Intertemporal Choice task – Softmax decision rule",
    "justification": "The hyperbolic model was “adapted … with a softmax probability function … governed by the inverse temperature parameter (β).”",
    "construct": "Inverse temperature (choice stochasticity) – Intertemporal"
  },
  {
    "measured_by": "Two-Armed Bandit task – Probit exploration model",
    "justification": "“Exploitative choices are promoted by the weight of the estimated value difference of the two machines (w_V).”",
    "construct": "Weight on estimated value difference"
  },
  {
    "measured_by": "Two-Armed Bandit task – Probit exploration model",
    "justification": "“Directed exploration is governed by the weight of the relative uncertainty between the two machines (w_RU).”",
    "construct": "Weight on relative uncertainty (directed exploration)"
  },
  {
    "measured_by": "Two-Armed Bandit task – Probit exploration model",
    "justification": "“Random exploration is controlled by the weight of the signed total uncertainty (w_sign(V)/TU).”",
    "construct": "Weight on signed total uncertainty (random exploration)"
  },
  {
    "measured_by": "Numerosity Comparison task – Noisy magnitude-encoding model",
    "justification": "“The parameter w is the Weber fraction, which controls the magnitude-dependence of noise” in encoding numerosities during the Numerosity Comparison task.",
    "construct": "Weber fraction"
  }
]