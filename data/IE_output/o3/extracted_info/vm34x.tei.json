[
  {
    "measured_by": "calibration graph plotting predicted probability bins against observed outcome frequencies",
    "justification": "The article states, “The calibration of forecasters can be depicted with calibration graphs … anything they attached a probability of n percent to was true approximately n percent of the time,” indicating that calibration is assessed via these graphs.",
    "construct": "Calibration of probabilistic forecasts"
  },
  {
    "measured_by": "discrepancy between assigned probability levels and the proportion of propositions that are actually true",
    "justification": "The text explains that “overconfidence… is when our probabilities for a set of propositions exceed the proportion of those propositions that are true,” showing that overconfidence is gauged by comparing stated probabilities to actual truth rates.",
    "construct": "Overconfidence"
  }
]