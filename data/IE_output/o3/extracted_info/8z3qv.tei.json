[
  {
    "measured_by": "model-based weighting parameter w (dual-system RL model)",
    "justification": "“The model-based weighting parameter w in our dual-system RL model reflects the degree to which participants used the task's transition structure to plan towards an island.”",
    "construct": "Model-based planning / Model-based control"
  },
  {
    "measured_by": "inverse temperature parameter β (soft-max choice rule)",
    "justification": "“β is the inverse temperature parameter which controls the exploitation-exploration trade-off between two choice options given their difference in value.”",
    "construct": "Explore–exploit balance / Choice stochasticity"
  },
  {
    "measured_by": "learning rate α in the dual-system RL model",
    "justification": "“…the prediction error is then used to update the value… where α is a learning rate used to scale the prediction error…”",
    "construct": "Reward learning rate"
  },
  {
    "measured_by": "eligibility-trace decay parameter λ",
    "justification": "“…updated with δ PE where λ represents an eligibility trace decay parameter representing how much a prediction error is used to update previous actions.”",
    "construct": "Eligibility trace influence"
  }
]