[
  {
    "measured_by": "weighting parameter w from the dual-system reinforcement-learning model",
    "justification": "\"Model-based control was indexed by weights closer to 1\" – explicitly linking the psychological construct (model-based control) to the model’s weighting parameter w.",
    "construct": "Model-based control"
  },
  {
    "measured_by": "weighting parameter w from the dual-system reinforcement-learning model",
    "justification": "The same sentence continues \"whereas model-free control was indexed by weights closer to 0,\" showing that lower values of the w parameter serve as the measure of model-free control.",
    "construct": "Model-free control"
  },
  {
    "measured_by": "choice stickiness parameter π",
    "justification": "The authors state that they estimated \"a choice (π)… capturing persistence in which spaceship participants choose,\" directly tying the construct of choice persistence to the π parameter.",
    "construct": "Choice persistence"
  },
  {
    "measured_by": "response stickiness parameter ρ",
    "justification": "The same sentence notes a \"response stickiness parameter (ρ), capturing persistence in … which response key they press,\" making ρ the quantitative measure of response persistence.",
    "construct": "Response persistence"
  },
  {
    "measured_by": "inverse temperature parameter β",
    "justification": "They estimated \"an inverse temperature parameter (β), which determined the exploitation/exploration trade-off between two choice options,\" linking this construct to β.",
    "construct": "Exploration–exploitation trade-off"
  },
  {
    "measured_by": "deterministic two-step task (Kool et al., 2017)",
    "justification": "The methods section explains that \"this task allows us to distinguish between model-based and model-free strategies,\" identifying the deterministic two-step task as the instrument used to measure the preference between the two strategies.",
    "construct": "Model-based vs. model-free strategy preference"
  }
]