[
  {
    "measured_by": "two-phase learning/transfer reinforcement-learning task / range-normalization paradigm",
    "justification": "“A typical experimental demonstration of outcome context-dependence employs a two-phase design, comprising a learning phase and a transfer phase … the preferences expressed during the transfer phase will differ depending on whether option values are encoded in an objective manner or in a context-dependent manner.”",
    "construct": "Outcome context-dependence / relative value learning"
  },
  {
    "measured_by": "different learning rates for positive vs. negative prediction errors / bandit reinforcement-learning tasks",
    "justification": "“Studies have shown that option values are updated more following positive prediction errors than negative ones. Computationally, this bias is modeled by assuming different learning rates for positive and negative prediction errors.”",
    "construct": "Positivity bias / asymmetric learning rates"
  },
  {
    "measured_by": "mismatch between calculated option values and chosen actions / perseveration parameter in RL choice models",
    "justification": "“A common example of a praxic bias in human reinforcement learning is choice repetition or perseveration, where actions are repeated regardless of the rewards received, causing actions to deviate from the calculated option values.”",
    "construct": "Choice perseveration / praxic bias"
  }
]