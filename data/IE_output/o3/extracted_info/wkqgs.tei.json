[
  {
    "measured_by": "self-reported confidence ratings on a 50%–100% probability scale",
    "justification": "“participants were also instructed to report how confident they were that they selected the correct response on a scale ranging from 50% (guessing) to 100% (certain correct)”",
    "construct": "Metacognitive confidence"
  },
  {
    "measured_by": "percentage of images where the same participant’s two classifications differ",
    "justification": "“Since participants made two responses on each image, we calculated the rate at which both of these responses were different. The intra-rater disagreement rate was …”",
    "construct": "Intra-rater disagreement"
  },
  {
    "measured_by": "percentage of disagreement between first responses of randomly paired participants",
    "justification": "“we calculated the inter-rater disagreement by pairing the participants randomly and calculating the inter-rater disagreement with their first responses”",
    "construct": "Inter-rater disagreement"
  },
  {
    "measured_by": "Brier score and Murphy’s Brier decomposition",
    "justification": "“We calculated the Brier score and Murphy's Brier decomposition for each experiment. The Brier score is the mean squared error between the confidence rating and the outcome.”",
    "construct": "Confidence calibration"
  }
]