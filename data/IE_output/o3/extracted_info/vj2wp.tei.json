[
  {
    "measured_by": "learning rate parameter (α) in model-free reinforcement learning",
    "justification": "“...the parameters learning rate, inverse temperature, and perseverance reflect the extent of value update in response to new information, the degree of noise, and the effect of choice history, respectively.”",
    "construct": "extent of value update in response to new information"
  },
  {
    "measured_by": "inverse temperature parameter (β) in the soft-max choice rule",
    "justification": "The same sentence notes that the “inverse temperature… reflect[s] the degree of noise” in decisions, linking this construct to the β parameter used in the model’s soft-max function.",
    "construct": "decision-making noise"
  },
  {
    "measured_by": "perseverance parameter (g) in choice-trace reinforcement learning models",
    "justification": "The cited passage also states that the “perseverance [parameter] reflect[s] the effect of choice history,” indicating that g operationalises behavioral perseverance.",
    "construct": "effect of choice history / behavioral perseverance"
  },
  {
    "measured_by": "separate learning rate parameters for positive (α⁺) and negative (α⁻) prediction errors",
    "justification": "In the section “RL with asymmetric learning rates” the chapter explains that “differential learning rates control for the speed of learning from positive and negative reward prediction errors,” explicitly tying this construct to distinct α⁺ and α⁻ parameters.",
    "construct": "asymmetric learning from positive vs. negative reward prediction errors"
  }
]