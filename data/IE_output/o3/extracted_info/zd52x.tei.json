[
  {
    "measured_by": "α parameter in Q-learning / FQ-learning models",
    "justification": "“where α ∈ [0,1] is the learning rate, which determines the extent to which the prediction error affects the updated value” (section describing Eqs. 1–2).",
    "construct": "Learning rate"
  },
  {
    "measured_by": "β parameter in the soft-max choice rule",
    "justification": "“The parameter β is the inverse temperature, which indicates how sensitively the choice probability changes with the value difference between options.”",
    "construct": "Inverse temperature"
  },
  {
    "measured_by": "α_F parameter in Forgetting Q-learning (FQ-learning) model",
    "justification": "“Q t+1(ā t)= (1−α_F)Q t(ā t), where α_F is the forgetting rate, which determines the rate at which the value of the unchosen option decays.”",
    "construct": "Forgetting rate"
  },
  {
    "measured_by": "φ and τ parameters in the choice-trace component of Q+C model",
    "justification": "“The parameter τ∈[0,1] is the decay rate of the choice trace… The choice trace weight φ… controls the tendency to repeat (φ>0) or avoid (φ<0) recently chosen options.”",
    "construct": "Choice hysteresis (choice trace)"
  },
  {
    "measured_by": "IDT property captured via latent state of a trained RNN assessed with the on-policy IDT check",
    "justification": "“This demonstrates that the RNN effectively captures individual differences… We also propose a method to quantify the degree of IDT in a trained RNN, which we term the on-policy IDT check.”",
    "construct": "Individual differences in learning behaviour"
  }
]