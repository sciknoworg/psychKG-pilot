[
  {
    "measured_by": "α_W parameter of the reinforcement-learning model fitted to the Information Bias Learning Task",
    "justification": "“In these equations, α_W∈[0,1] … express learning rates for Q_W …” – explicitly defining α_W as the measure of the win-side learning rate.",
    "construct": "learning rate (win outcomes)"
  },
  {
    "measured_by": "α_L parameter of the reinforcement-learning model fitted to the Information Bias Learning Task",
    "justification": "“α_L∈[0,1] express learning rates for Q_L …” – identifying α_L as the quantitative indicator of the loss-side learning rate.",
    "construct": "learning rate (loss outcomes)"
  },
  {
    "measured_by": "β parameter (inverse temperature) of the reinforcement-learning model",
    "justification": "“the inverse temperature, β∈(0,20], adjusts the sharpness of the value difference between the options in the choice probability” – showing β is the metric used to index exploration versus exploitation.",
    "construct": "exploration-exploitation tendency (inverse temperature)"
  },
  {
    "measured_by": "φ parameter (perseveration magnitude) in the RL model with a perseveration term (M1s/M1m)",
    "justification": "“These models have a new parameter φ that represents the perseveration magnitude and biases actions toward repeated selection of a previously chosen stimulus, independent of outcome history.”",
    "construct": "choice perseveration tendency"
  }
]