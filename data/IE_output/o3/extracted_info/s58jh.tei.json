[
  {
    "measured_by": "scaling parameter (in RL models)",
    "justification": "“Huys and colleagues 63 … reward sensitivity was operationalized through a scaling parameter reflecting how strongly individuals valued the rewards they received.”",
    "construct": "reward sensitivity"
  },
  {
    "measured_by": "learning-rate parameter (α)",
    "justification": "“The learning rate quantified the extent to which individuals updated their expectations based on new feedback.”",
    "construct": "learning rate"
  },
  {
    "measured_by": "inverse temperature parameter",
    "justification": "“This trade-off is often quantified using the inverse temperature parameter, where lower values indicate greater exploration and higher values lead to deterministic exploitation of high-value options.”",
    "construct": "exploration–exploitation trade-off"
  },
  {
    "measured_by": "risk-aversion parameter",
    "justification": "“Mathematical models incorporate parameters such as a delay discounting rate and risk aversion to quantify these effects 36.”",
    "construct": "risk aversion"
  },
  {
    "measured_by": "delay-discounting rate",
    "justification": "“Mathematical models incorporate parameters such as a delay discounting rate and risk aversion to quantify these effects 36.”",
    "construct": "delay discounting / temporal impulsivity"
  },
  {
    "measured_by": "weighting parameter between model-based and model-free systems",
    "justification": "“Hybrid reinforcement learning models capture this interplay … and weighting parameters that quantify their relative contributions 37,38.”",
    "construct": "model-based vs. model-free control balance"
  },
  {
    "measured_by": "weighting parameter between working memory and reinforcement learning",
    "justification": "“In the model … the weighting parameter is adapted to determine how much decision-making is influenced by working memory.”",
    "construct": "working-memory influence on choice"
  },
  {
    "measured_by": "drift-diffusion model parameters",
    "justification": "“Sequential sampling models such as drift-diffusion models … provide a mechanistic account of choice accuracy and reaction time.”",
    "construct": "evidence-accumulation dynamics"
  },
  {
    "measured_by": "recency-bias parameter",
    "justification": "“Computational modeling identified an elevated recency bias in opioid-addicted individuals, meaning they are more likely to repeat previous responses regardless of reinforcement history.”",
    "construct": "recency bias"
  }
]