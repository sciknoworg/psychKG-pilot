[
  {
    "measured_by": "Working-Memory module parameters (capacity C, weighting w) of the RLWM computational model",
    "justification": "“The RLWM model captures the parallel recruitment of working memory … The parameter is determined by two free parameters, the working memory capacity (i.e., resource limit) C, and the initial WM weighting w.”",
    "construct": "working memory / short-term memory processes"
  },
  {
    "measured_by": "Reinforcement-Learning module in the RLWM model (learning rate α, prediction-error update)",
    "justification": "“The RL module is characterized by Equations 1-2… action-value Q(s,a) is updated on each trial using the delta rule … capturing the gradual RL process.”",
    "construct": "reinforcement learning / stimulus-action value learning"
  },
  {
    "measured_by": "Linear Ballistic Accumulator (LBA) model parameters (drift rate v, boundary b, non-decision time t0)",
    "justification": "“A similar evidence accumulation model, the Linear Ballistic Accumulator, or LBA, can easily accommodate any number of actions … RT is modeled by the accumulation of evidence where parameters A, b, and t0 determine the reaction time.”",
    "construct": "decision-making reaction time / evidence accumulation speed"
  },
  {
    "measured_by": "Shannon entropy of the average action policy (Equation 13)",
    "justification": "“This prior uncertainty term was modelled by … the Shannon entropy is computed on this vector… we incorporate the uncertainty quantity from Equation 13 into the evidence accumulation rate.”",
    "construct": "action uncertainty / prior uncertainty over actions"
  }
]