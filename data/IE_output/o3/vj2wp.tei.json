[
  {
    "topic_or_construct": [
      "extent of value update in response to new information"
    ],
    "measured_by": [
      "learning rate parameter (\u03b1) in model-free reinforcement learning"
    ],
    "justification": "\u201c...the parameters learning rate, inverse temperature, and perseverance reflect the extent of value update in response to new information, the degree of noise, and the effect of choice history, respectively.\u201d"
  },
  {
    "topic_or_construct": [
      "decision-making noise"
    ],
    "measured_by": [
      "inverse temperature parameter (\u03b2) in the soft-max choice rule"
    ],
    "justification": "The same sentence notes that the \u201cinverse temperature\u2026 reflect[s] the degree of noise\u201d in decisions, linking this construct to the \u03b2 parameter used in the model\u2019s soft-max function."
  },
  {
    "topic_or_construct": [
      "effect of choice history / behavioral perseverance"
    ],
    "measured_by": [
      "perseverance parameter (g) in choice-trace reinforcement learning models"
    ],
    "justification": "The cited passage also states that the \u201cperseverance [parameter] reflect[s] the effect of choice history,\u201d indicating that g operationalises behavioral perseverance."
  },
  {
    "topic_or_construct": [
      "asymmetric learning from positive vs. negative reward prediction errors"
    ],
    "measured_by": [
      "separate learning rate parameters for positive (\u03b1\u207a) and negative (\u03b1\u207b) prediction errors"
    ],
    "justification": "In the section \u201cRL with asymmetric learning rates\u201d the chapter explains that \u201cdifferential learning rates control for the speed of learning from positive and negative reward prediction errors,\u201d explicitly tying this construct to distinct \u03b1\u207a and \u03b1\u207b parameters."
  }
]