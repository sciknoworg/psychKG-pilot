[
  {
    "topic_or_construct": [
      "Calibration of probabilistic forecasts"
    ],
    "measured_by": [
      "calibration graph plotting predicted probability bins against observed outcome frequencies"
    ],
    "justification": "The article states, \u201cThe calibration of forecasters can be depicted with calibration graphs \u2026 anything they attached a probability of n percent to was true approximately n percent of the time,\u201d indicating that calibration is assessed via these graphs."
  },
  {
    "topic_or_construct": [
      "Overconfidence"
    ],
    "measured_by": [
      "discrepancy between assigned probability levels and the proportion of propositions that are actually true"
    ],
    "justification": "The text explains that \u201coverconfidence\u2026 is when our probabilities for a set of propositions exceed the proportion of those propositions that are true,\u201d showing that overconfidence is gauged by comparing stated probabilities to actual truth rates."
  }
]