[
  {
    "topic_or_construct": [
      "Learning rate"
    ],
    "measured_by": [
      "\u03b1 parameter in Q-learning / FQ-learning models"
    ],
    "justification": "\u201cwhere \u03b1 \u2208 [0,1] is the learning rate, which determines the extent to which the prediction error affects the updated value\u201d (section describing Eqs. 1\u20132)."
  },
  {
    "topic_or_construct": [
      "Inverse temperature"
    ],
    "measured_by": [
      "\u03b2 parameter in the soft-max choice rule"
    ],
    "justification": "\u201cThe parameter \u03b2 is the inverse temperature, which indicates how sensitively the choice probability changes with the value difference between options.\u201d"
  },
  {
    "topic_or_construct": [
      "Forgetting rate"
    ],
    "measured_by": [
      "\u03b1_F parameter in Forgetting Q-learning (FQ-learning) model"
    ],
    "justification": "\u201cQ t+1(\u0101 t)= (1\u2212\u03b1_F)Q t(\u0101 t), where \u03b1_F is the forgetting rate, which determines the rate at which the value of the unchosen option decays.\u201d"
  },
  {
    "topic_or_construct": [
      "Choice hysteresis (choice trace)"
    ],
    "measured_by": [
      "\u03c6 and \u03c4 parameters in the choice-trace component of Q+C model"
    ],
    "justification": "\u201cThe parameter \u03c4\u2208[0,1] is the decay rate of the choice trace\u2026 The choice trace weight \u03c6\u2026 controls the tendency to repeat (\u03c6>0) or avoid (\u03c6<0) recently chosen options.\u201d"
  },
  {
    "topic_or_construct": [
      "Individual differences in learning behaviour"
    ],
    "measured_by": [
      "IDT property captured via latent state of a trained RNN assessed with the on-policy IDT check"
    ],
    "justification": "\u201cThis demonstrates that the RNN effectively captures individual differences\u2026 We also propose a method to quantify the degree of IDT in a trained RNN, which we term the on-policy IDT check.\u201d"
  }
]