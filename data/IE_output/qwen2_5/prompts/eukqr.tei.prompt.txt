You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Verbal Reports as Data Revisited: Using Natural Language Models to Validate


Cognitive Models
We propose a novel technique that uses individuals' verbal reports to validate psychological processes assumed by computational cognitive models. We capitalize on recent advances in Natural Language Processing (NLP) models, especially their context-sensitivity, and recommend using them to classify participants' unstructured verbal descriptions of the strategies they use to perform tasks. This framework emphasizes that verbal descriptions are a valuable and under-utilized source of data for ensuring cognitive models align with their psychological assumptions. We argue that this framework can encompass a broad range of cognitive tasks, including problem solving, reasoning, memory, categorisation, and decision-making.
Computational approaches to understanding cognition are commonplace. Advances in, and the availability of, resources for modelling have led to a huge increase in the number and variety of cognitive models. Many of these models are able to accurately describe people's behaviour across a wide range of tasks and contexts. Despite their accuracy, a debate remains over the extent to which such models should be considered as capturing something fundamental about underlying psychological processes (e.g., 
Szollosi, Donkin, & Newell, 2023)
. Do such models directly mimic these processes or simply provide mathematical tools for examining them? This debate has yielded several fruitful avenues of investigation but to date has largely neglected one potentially important source of data that we believe could help bridge the gap between using models as tools versus explanations of behaviour. This additional data source is participants' verbal descriptions of how they perform tasks.
The idea that participants can explain their own behaviour reliably has received short-shrift in some corners of the literature (most famously by 
Nisbett and Wilson (1977)
). However, more recent re-evaluations of this perspective have made a case for how and why careful elicitation of participants' verbal descriptions can inform our understanding of psychological processes (e.g., 
Newell & Shanks, 2014
Szollosi & Newell, 2020)
. Seminal research by Ericsson, Simon and others explored how self-report questionnaires and unstructured verbal reports provide potentially rich sources of data that can be used to build psychological explanations of behaviour 
(Ericsson & Simon, 1980;
Fox, Ericsson, & Best, 2011;
Ranyard & Svenson, 2011;
Svenson, 1979
Svenson, , 1989
. The innovation in our work is to show how these data can also be used to help validate the psychological assumptions inherent in our cognitive and computational models of behaviour.
In essence, we suggest that verbal report data should be used in the way that other sources of data are often used to inform and constrain cognitive models such as data from neuroimaging 
(Carter, Meyer, & Huettel, 2010)
, process-tracing, and eye-tracking 
(Gluth, Kern, Kortmann, & Vitali, 2020;
Krajbich, 2019;
Krajbich & Rangel, 2011;
Vanunu, Hotaling, Le Pelley, & Newell, 2021)
.
Our proposed framework enables the validation of assumptions inherent in cognitive models using open-end, unstructured text-based responses. This approach uses the power of Natural Language Processing (henceforth NLP) models to evaluate participants' unstructured self-reported strategies, and to classify them according to their content. This classification represents a new source of information that allows a more refined insight into two important aspects of experimental work: i) individuals' knowledge and understanding of the experimental task and ii) the strategies participants think they used to solve or perform a particular task. The rapid development of NLP models and their outstanding performance across a wide range of text-based tasks allows this source of data to provide stronger support (or challenge) to the formal descriptions of human thought and behavior made by cognitive models. Hence it serves in helping to hold cognitive models accountable for their psychological assumptions.
Prior to outlining our framework, it is important to highlight how our approach sets itself apart from the existing literature linking NLPs with human psychology. One focus of this literature is on the similarities or differences between human reasoning and that of NLPs 
Bhatia (2022)
; 
Bhatia and Richie (2022)
; 
Bhatia and Stewart (2018)
; 
Schulz (2022, 2023)
. A second focus is to use NLP models to help generate experimental stimuli, enabling researchers to acquire naturalistic stimuli while retaining the necessary level of experimental control during lab-based tasks. 
(Stimuli, which, in
 turn, can be used to construct and inform computational models of cognition (e.g., 
Bhatia (2022)
; 
Bhatia and Richie (2022)
) .
Our objective is different to both of these applications. We do not seek to compare the 'cognitive abilities' of NLPs with those of humans, nor evaluate the significance of their similarities; instead, we aim to revitalize the elicitation of verbal reports and their direct comparison with the constructs (i.e., parameters) defined in our cognitive models.
By leveraging the automated capabilities of NLPs to efficiently handle text-based data, we aim to contrast these analyses with the parameters of cognitive models that are interpreted as holding psychological significance.


Theoretical Framework
The basic principle of cognitive models is the necessity to formally describe human reasoning and behavior using abstract mathematical specifications (e.g. 
Navarro, 2021;
Proulx & Morey, 2021;
Singmann et al., 2022)
. The objective is to achieve high levels of precision while eliminating any subjective interpretation about the underlying processes. Although using ordinary language to describe cognitive processes is clearly a challenge to this principle, mathematical specifications are yet just another, very precise verbal description of those processes.
To illustrate this idea consider the standard class of evidence accumulation models 
(Brown & Heathcote, 2008;
Ratcliff, 1978;
Ratcliff, Smith, Brown, & McKoon, 2016)
. In these models noisy evidence starts accumulating at a point between two response boundaries (also known as a "decision threshold") and terminates when one of the boundaries is reached. Let us consider a very simple lab-based task, a choice between two food items, an apple and a pear. Let us further consider Person A, who has a clear preference for apples over pears. An evidence accumulation model will be able to capture a bias towards an apple based on Person A's reaction time distributions and will use either a biased starting point or a lower boundary for the preferred option 
(Ratcliff et al., 2016)
. Now, imagine asking Person A to engage in describing their behaviour while making their decision. Person A might describe her preferences for the apple using the features important to her (e.g., taste, texture and other nutritional facts). We can further expect that when explicitly asked, Person A will provide her reasoning for her strong preference for apples describing this choice as clear, obvious or quick due to her weak interest in pears. Evidence accumulation models help capturing an initial bias towards the apple with a precise numerical value that best describes the empirical data provided by Person A. However, it is also clear that if this parameter indeed captures the process of an underlying behaviour precisely (and simply), it can also be explained by a lay person using natural language in a similar way to those described in the 'biased' starting point (or by a lower decision boundary).
Following the example, derived from their text response, Person A indeed has an initial bias towards one of the options, which can validate an introduction of a bias to a computational model. This might contrast with a situation in which a participant states that they did not notice or recognize the pear and so chose the apple. In this case a model with a biased starting point seems inappropriate for capturing the thought process engaged in by the participant.
This simple example helps reveal how verbal reports can be beneficial for validating and supporting the assumptions underlying our models, particularly in terms of their parameters. However, this process is not straightforward and requires careful planning.
Earlier work by 
Ericsson and Simon (1980)
, sheds light on the challenges in this area and remains relevant even with the advancements of contemporary NLP models.


The Legacy of Ericsson & Simon
The seminal work by 
Ericsson and Simon (1980)
 explores in detail the utility of verbal reports as data in experimental settings. We review some of the key aspects here to illustrate their relevance for our framework.  Choose between concurrent or retrospective collection of verbal reports. Verbal Reporting:
Choose between providing structured and directed guidance on the focus areas for the verbal reports, and permitting participants to freely express their thoughts in their accounts, Instruction Formulation: Determine how to strategically craft instructions that prompt participants to articulate their actual strategies. Elicitation Cadence: Decide on whether to gather reports consistently after each task trial or intermittently at predetermined intervals.
The first, and perhaps self-evident, step is to consider whether the primary task of interest is amenable to the elicitation of verbal reports. This criterion of "verbalizability" (Step 1, 
Figure 1
) refers to the requirement that information regarding the utilized strategy must already exist in a verbal form for participants to effectively report it, and that the reporting of that information does not negatively disrupt primary task performance. Not all experimental designs will accommodate this criterion. For instance, studies investigating perceptual-motor functions, have shown that verbalization can exert an influence on how tasks are executed (e.g., 
Beilock & Carr, 2005)
. Similarly, some speed-accuracy tasks may be less suitable for this method due to the challenges associated with direct articulation in time-restricted environments (e.g., 
Nisbett & Wilson, 1977)
. In each case it is important to consider the potential consequences of asking participants to articulate information that is not naturally processed verbally or is challenging to express in words. Care must be taken to ensure that this process in itself does not alter the way in which participants approach a task.
One way this can be done is by comparing performance on a primary task either with or without concurrent verbalisation/strategy questioning (e.g., 
Lagnado, Newell, Kahan, & Shanks, 2006)
.
If amenability to verbalization is established, the next step is to consider the candidate cognitive or computational models that can be applied to the primary task (Step 2, 
Figure 1
). Here, the important aspect is that the psychological assumptions underpinning particular parameters in a given model have the potential to be captured in verbal reports via natural language. As our earlier example with fruit choice illustrated, the idea is to be able to map what people say on to psychologically significant parameters in the cognitive model (we expand further on this process -which is core to our approach -in the next section). If this kind of mapping is achievable, then researchers can proceed to the third step.
Step 3 requires detailed consideration of the exact method for eliciting verbal reports. Here, researchers need to decide whether to elicit reports via Concurrent Probing, which involves questioning subjects during a task about their underlying hypotheses or Retrospective Verbalization, in which participants are queried post-task, either immediately or after several trials. Additional considerations are whether elicitation is directed or undirected. Directed probes present specific queries to participants, aiming to discern whether they employed a particular strategy or approach. Conversely, undirected probes grant participants the latitude to freely elucidate their thought processes. 
Figure 1
 provides additional information on other methodological choices.


Linking Verbal Reports with Cognitive Models via Language Models.
Successful navigation of the steps outlined in 
Figure 1
, brings a researcher to the point of having a candidate task that is amenable to verbalisation, a set of candidate cognitive models for explaining performance on that task, and consensus on the best methods for eliciting verbal reports. 
Figure 2
 presents the conceptual framework underlying the next stages in linking the content of the verbal reports to the psychological assumptions underlying the parameters in a cognitive model, via NLP models.
At this stage the researcher conducts the experiment to examine a specific prediction (to test a theory) and collects both behavioural and text-based data to test it. At the conclusion of the experiment, the researcher will have three sources of information for analysis: primary task behaviour (e.g., trial by trial responses), verbal reports (whether obtained concurrently with the experiment, immediately afterwards, or at intermittent intervals throughout the study), and model estimates obtained from using empirical data to fit cognitive models (at both individual and group levels, potentially) (See 
Figure 2
, circular nodes).
The crucial next stage is the Text Analysis (blue rectangle, 
Figure 2
) in which verbal reports are used as input to the Natural Language Models. The benefit here is that Natural Language Models are designed to scan and analyse human-generated (non-constrained) text. They are based on supervised machine learning algorithms that have been trained on vast amounts of text data in order to learn particular functions. A recent influential development was the introduction of an "attention mechanism" which allows the embedding of contextual meaning into words based on their location in a sentence 
(Vaswani et al., 2017)
. 1 . This improvement was made possible due to the 1 These models start by processing every word sequentially -similar to earlier models of word embeddings (i.e., vector representation for words, where similar words have similar embeddings/vector representations). In this class of models, each word has a unique embedding. It then assigns a weight to existence of a class of neural network models called "transformers" 
(Devlin, Chang, Lee, & Toutanova, 2018)
. With these new mechanisms, NLP models improved their ability to make sense of human-generated text and were able to outperform the old models across a large number of text-based tasks 
(Devlin et al., 2018;
Wolf et al., 2020)
. This feature is particularly useful in the analysis of verbal report data in which descriptions of strategies may remain relatively impoverished despite attempts to guide participants' For example, one way to classify participants' verbal descriptions, is to allow language models to test the relation between a set of candidate labels and then select the one that best describes a given report. These candidate labels are provided to the model by the experimenter based on assumptions about the different kinds of behaviours participants might exhibit in performing the task. In practice, the language model takes a sentence from a verbal report and a list of candidate labels as input and returns accuracy score(s) to each tested label(s) 
(Sanh et al., 2022;
Yin, Hay, & Roth, 2019
). This 'accuracy' score ranges between 0 and 1, which can be understood as the 'confidence' the model assigns to the ability of a label to describe the sentences provided by participants in their verbal reports.
Ideally, participant-generated verbal reports will consist of two important elements: i) a description of how the problem was solved or choice was made; these descriptions represent the 'algorithms' that generated the target behaviour and ii) the reason (why) each word embedding that represents the position of that word in the text (e.g., in a sentence). In a last step a weight is generated for each word that represents how much its meaning or grammatical form (e.g., nouns, verbs, adjectives, adverbs, prepositions etc.) is dependent on other words in the sentence.
For instance, the sentence 'I took my dog for a walk', the subject of the sentence are given more power relative to "my" (which is possessive pronoun.) or "walked" which is the verb of the sentence. a specific algorithm was used. In reality, the reports might not capture these aspects clearly, requiring steps of refinement to what we term 'good verbal descriptions' in our framework. The main goal in this step of the analysis is to correlate these label scores with the individual cognitive model fits (that offer the computational 'description' of how the task was performed) to assess the validity, or psychological plausibility of the cognitive models' core assumptions. To assess the quality of the verbal descriptions requires comparing the selected (i.e., most suitable) label of an individual's strategy to their actual behaviour in the task. If there is a good match, then these reports are amenable to the final step of correlating the outputs of the language models (i.e., the confidence assigned to the different labels) with the overal fit or the parameter of interest in the cognitive model.
Here, we have provided only a conceptual illustration of this technique. In the Appendix we include a practical example of how to apply this framework to an experiment examining risky choice. This case study primarily demonstrates the rich potential of verbal descriptions for enhancing and verifying cognitive models. As such, it serves as a proof of concept, with certain decisions tailored to our specific task and data set. It also highlights how the failure to elicit sufficiently explicit verbal reports from participants limits the validation process. With improvements in both elicitation techniques and the sophistication of NLP models, we envisage that future attempts to apply our framework will yield even greater insights.
It is important to note that the contents of verbal reports can be used to refine experimental designs and to develop and test new (or alternative) models and theories (see 
Figure 2
: orange node.). The revelation of new, untested models unfolds naturally by employing this method. As participants articulate their thought processes, unforeseen patterns, perspectives, or nuances may emerge. These insights can be pivotal in highlighting gaps in current models or suggesting entirely new avenues of exploration.
Moreover, the iterative process of refining experimental designs based on these verbal reports ensures that subsequent studies are better aligned with the intricacies of human cognition and behavior.  Overall, such data can contribute to understanding what might otherwise be considered unexplained noise 
(Davis-Stober & Regenwetter, 2019;
Regenwetter & Robinson, 2017;
Regenwetter, Robinson, & Wang, 2022)
. Individuals may exhibit variability in behaviour because they discovered novel solutions to the problems they are trying to solve or because they shifted their attention to unexplored details in their environment 
(Szollosi et al., 2023)
. In other words, the unexplained noise could be the outcome of deliberate, intentional, and well-planned behavior, rather than the "trembling hand" that is often invoked to explain variability.


Discussion
We propose a framework that connects text-based data with the cognitive processes assumed by computational models. The framework assumes that individuals are often aware of their decision strategies, and with the right questions, can provide valuable input about them in form of verbal reports. The responses can be classified by NLP models and can later validate the decision processes the cognitive models assume.
In the remainder of the paper, we consider two challenges for implementing our conceptual framework.


Challenges in Eliciting Verbal Reports
Perhaps the main challenge in the application of our framework is the need for high quality data, in which people articulate their strategies and reasoning in a way that can be related to cognitive models. Researchers must think about appropriate questions and innovative ways to encourage participants to describe their reasoning in detail.
Overcoming this challenge requires that verbal reports are elicited in a timely fashion and that questions, or elicitation prompts are sensitive to the information participants used to perform a task (e.g., 
Newell & Shanks, 2014
Shanks & John, 1994)
. Timely collection could imply concurrent elicitation -a method for minimizing information loss from memory or recall biases -but this could extend the duration of the task and prove unsuitable for tasks that prioritize speed and accuracy.
Conversely, retrospective elicitation may lead to reconstructed hypothesized accounts of how a task was approached since the timing at which the verbal reports are elicited allows for additional processing, or post-hoc inference. These considerations underscore the essential role the nature of the task plays in determining the appropriateness of the method and its potential effects on both performance and the reliability of verbal reports.
Earlier studies suggest some remedies to these problems 
(Ranyard & Svenson, 2011;
Williamson, Ranyard, & Cuthbert, 2000)
. For example, 
Williamson et al. (2000)
 propose asking participants to provide verbal descriptions pre-and post-task completion. In the first stage participants are asked to prepare a list of the information they think they will need to know in order to reach their decision. After the completion of the task (or trial), participants are asked to summarize how they made each of their decisions 
(Ericsson & Simon, 1980
, 1984
. A related technique is to ask participants to describe their behaviour to a hypothetical future participant in such a way that the future participant will maximize earnings or accuracy in a task.
Other approaches to reduce the measurement errors or imperfections associated with using verbal reports as data are to increase the frequency with which data is collected, to vary prompts or frames for instructions 
(Gonzalez, Dana, Koshino, & Just, 2005
) and to automate the collection of reports. We developed a tool that addresses these challenges by enabling real-time speech recognition, where spoken reports are automatically converted into text and analysed using large-language models, greatly reducing the typical burden of analysing self-report data 
(Ostrovsky, Ungermann, Newell, & Donkin, 2024)
. This last approach offers a practical solution which can assist in synchronizing verbal reports with other behavioral records. This enables the verification of report consistency with both primary behavioral data, and the aspects of the cognitive model used to explain that behaviour. Together this can potentially enhance the overall reliability of the information gathered. Such automatic methods can reduce the verbalisation-effort experienced by participants and leave cognitive processes engaged by the primary task relatively unaffected 
(Ericsson & Simon, 1980
). An even more direct method for assessing the accuracy/validity of verbal reports is to compare the performance of separate groups of participants who are directed to behave in different ways on the same task. Consider a simple risk-elicitation task (akin to the one described in our case study in the Appendix) in which participants could be instructed to behave in a 'risk-averse' or 'risk-seeking' manner. Such a design would enable a comparative analysis of the reports generated under these varied instructions.
The hope is that the instructions would lead to identifiable signatures in the behavioural data that would be linked to both elements of the verbal reports and the parameters used to index those behaviours in the computational models. In a sense, this approach 'reverse-engineers' the elicitation problem permitting validity and robustness checks across the three sources of input for the analysis.


Challenges in Selecting and Applying NLPs
Just as eliciting accurate verbal reports can be challenging, the selection of the appropriate NLP model for analyzing those reports is highly dependent on the specific nature of the experimental task, and the needs of the experimenter. The rapidly increasing progress in the field of NLP models also makes any advice in this area quickly redundant; nontheless, here, we provide a few pointers to help understand this evolving landscape. One popular current model is BERT-MNLI is adept at discerning logical connections within verbal communication, and offers detailed understanding of complex language nuances 
(Bhargava, Drozd, & Rogers, 2021)
. This makes it suitable for analyses requiring deep linguistic insights. Conversely, DistilBERT is noted for its operational efficiency and broad applicability, which is advantageous in diverse experimental settings, especially where there are computational limitations. The choice between these models should be tailored to the unique requirements and constraints of each research project.
The zero-shot classification technique for tasks such as categorizing participants' verbal descriptions stands as the premier choice for classifying new texts without the need for model training specific to a task 
(Chen & Li, 2021)
. Therefore, it is well-suited for experimental tasks that produce unique and infrequently encountered data types, as well as for situations where data availability is limited. In the process of classifying participants' verbal reports, as detailed in our case study (see Appendix), we adopted the zero-shot classification technique. This approach was chosen to bypass the need for task-specific model training, a challenge compounded by the scarcity of our data.
A key leverage point of NLP models is in scenarios where numerical data is sparse or unavailable. Word and sentence embeddings present a valuable alternative in these settings as NLP models can convert textual information into a numerical format that computational models can process. An illustration of of how NLP models can be leveraged in the context of naturalistic decision-making is provided by 
Bhatia and Stewart (2018)
. In their experiments, participants were presented with realistic choices between an array of movies and various food dishes, the details of which were meticulously scraped from online sources.
The principal objective of the study was to observe and analyze the cognitive processes employed by participants when confronted with real selections of movie options and culinary dishes. By applying models like those discussed by 
Bhatia and
 Stewart 
2018
 


Concluding Remarks
Analyzing verbal reports can arguably be done by human raters. However, even if raters' subjectivity can be overcome by asking them to provide an independent opinion on each text-based response, once verbal description becomes a standard source of data with multiple data points per individual, rating becomes a tedious exercise. This is particularly true when dealing with large data sets or when attempting to analyze the data over time, as the sheer volume of data can quickly overwhelm even the most diligent human rater.
Our novel framework offers significant advantages over these standard approaches in terms of scalability, efficiency, and more objectivity. As such, we believe that automated algorithms for analyzing verbal reports represent an important step forward This Appendix details of our case study (which serves as an illustration of our theoretical framework). It includes the description of our experimental design, our behavioural data, the procedure of exclusion criteria applied in our analysis, of fitting the Bayesian Updating model and of the information about the NLP model (and pipeline) used to analyse the verbal results, the candidate keywords used as the input for the NLP model as well as the results of this analysis. It also contains the raw verbal descriptions participants provided at the end of the two experiments.


A case study
Before describing our case study in detail, we note that we anticipate a significant portion of the methodological steps we outline here might not be required once NLP models have been trained and fine tuned to categorize verbal descriptions of participants' strategies. In other words, the primary goal of this case study is to show that verbal descriptions represent a valuable reservoir of data, which can be both enlightening and beneficial for verifying cognitive models.
Our proposed technique was assessed based on existing data comprising verbal descriptions of choice strategies provided by participants in two experiments conducted as part of the first author's PhD dissertation. In essence, the current analysis attempts to map these descriptions onto the psychological assumptions made by one of the two computational models -the Distance Model -that we proposed and tested in that work 
(Ostrovsky, Liew, & Newell, 2023, manuscript in preparation)
. We describe the methods and results of this work below. In the full project, we also considered an additional Bayesian Updating Model that incorporated different underlying assumptions. For brevity we focus here on the validation of the Distance Model but all details about the application of our technique to the Bayesian Updating Model can be found in the Appendix. 


Methods & Procedure


Behavioural Data
As an initial step, we evaluate the appropriateness of the experimental task for eliciting verbal reports, as outlined in Figure 1. Our task was not subject to time constraints and appears to facilitate the accessibility of information regarding the strategies used by individuals. In the subsequent section, we detail the task undertaken by participants to illustrate the aspects we anticipated would be verbalizable within this task.
Participants were presented with the same risky-choice problem in every trial:
There were two jars: Jar A consisted of 300 black balls, each worth £14 and 700 white balls, each worth £0. Jar B consisted of a flipped distribution of balls with 700 black balls, each worth £6 and 300 white balls, each worth £0. Hence, the two jars had equal expected value (henceforth EV) but differed in their riskiness. 2 Participants were told that one of two jars will be randomly selected by the computer on each trial. As participants were not told which of the two was selected, they were presented with a 100-ball sample (from the selected jar) to help them work out which jar was selected.
They were then asked to choose from which jar they wish to draw their outcome-determining ball. They could either choose to select from the one indicated by the contents of the 100-ball sample (by pressing the button 'stick') or to choose to draw from the unselected jar (by pressing the button 'switch').
Choosing to draw from Jar A indicates an appetite for risk and choosing Jar B
indicates an aversion to risk. Risk seeking individuals are those, who chose Jar A (either by choosing to stick to this jar or by actively switching to draw from it) on the majority of trials. Risk averse individuals are those, who chose Jar B (either by choosing to stick to this jar or by actively switching to draw from it) on the majority of trials.
Both experiments used a within-subjects design with two conditions (Solo and Social) each presented in their respective block of trials. In the first block, participants made a series of risky choices alone (i.e., Solo condition; see 
Figure A1
 panel A). In the second block, participants completed a series of the same risky decisions as in the Solo condition, however, this time they made their decisions after observing choices made by other (hypothetical) people (i.e., Social condition; see 
Figure A1
 panel B). Trials differed in terms of the composition of the 100-ball sample shown to participants, and the number of other players choosing to take or avoid risks. For brevity, here we focus on how participants' choices changed between the solo and social phases of the experiment.


Modelling Data
As our second step, detailed in 
Figure 1
, we begin this section with the delineation of the models that were defined and fitted to the data. Furthermore, we explicate the psychological interpretation of their parameters, setting the stage for the subsequent analysis of the verbal reports.
To explain how participants integrate and update their private information (i.e., in the form of the 100-ball sample) given social information (i.e., the choices made by the hypothetical others) we analyzed two primary models. The model that consistently outperformed in various experiments is referred to as the Distance Model which posits that people have a sense of how different they are from others and they update their choices based on how much weight they put on the distance between themselves and others.
P (C) = [β × P (S − n N )] + P (S)
(1)
Equation 1 describes the Distance model. n N presents the proportion of social evidence. n are the number of choices others made in favour of one of the choice options and N is the total number of choices made. P (S) is proportion of choices for that option made in the absence of social information. β is a free parameter representing the weight people assign to the relative distance between their own choices and the choices made by others (i.e., social information). It has a lower boundary of β = 0 representing ignoring others and accounting only for one's own preferences and an upper boundary β = 1 indicating a full integration of others' choices into one's own.
In a nutshell, our results can be summarised as the following: 1) people pay attention to others' choices (to different degrees) and 2) information integration is consistently inline with the relative updating principle. That is, the Distance model provided a better account of the data for the majority of participants than the alternative model (the Bayesian Updating Model), details of which can be found in the Appendix.
Crucially, at the end of the experiment, participants were asked to describe any choice strategies they used in the first block (i.e., when making risky decisions alone) and those they used in the second block (i.e., when making decisions after observing the choices made by the other hypothetical players). Participants inserted their verbal descriptions into an empty text box that was not restricted by a maximum length. It is noteworthy that the verbal descriptions elicited at the end of the two experiments were not collected for the purpose of this NLP analyses described here but were rather included, as in many psychological experiments, as additional source of information that could be of use.
There were two main consequences of this design decision. First, since our questions were not specified for this task, not all participants provided descriptions that could be classified as a choice description. For instance, consider the description given by one participant about a strategy used in the Solo condition: "i relied on how many black or white balls it looked like there were". This individual is clearly describing a strategy. However, this description does not allow for a any inference about their intentions to chose Jar A or Jar B. It does not, for example, specify which colour of balls (i.e., black or white), or if any at all, determined their choice of jar. Unfortunately, in these occasions, there was no meaningful label that could be matched to a choice behaviour (i.e., to the number of risky choices) and we had to exclude those relatively early in our analysis procedure. Secondly, as our participants were not directly asked to, they rarely differentiated between a strategy description (i.e, a how question) and their reasoning (i.e., a why question). In the Appendix, we provide a full summary of the verbal descriptions as well as the best suitable label assigned for each individual (see Appendix, 
Table A10 and Table A11
).
Despite these challenges, our experimental task and analysis provide three important sources of information: i) behavioural data about participants' preferences for risk when making decisions alone (from the Solo condition) and after observing others' choices (from the Social condition), ii) individuals' model fits and iii) text-based descriptions of strategies provided by our participants. All sources of information are crucial for the mapping between NLP model output and our individual model fits, making the current data set a suitable, though by no means perfect, 'toy' example for our framework.


Current NLP Model
In this section we explain our methodological steps for this case study. As mentioned above, some of these steps are unique for this case study and are outlined in detail to allow the reader to follow our work flow. Later we discuss which steps should become unnecessary once NLP models are trained to learn to classify verbal descriptions. 
Figure 1
 illustrates, once again, how our case study serves as an exemplar for the application of this framework. Given that our analysis was conducted retrospectively, the considerations regarding the elicitation point, reporting structuring type, the formulation of instructions, and the cadence of elicitation have been predetermined. However, as highlighted in the lilac right rectangle of the figure, we advise that these critical questions be addressed in advance when determining the specific task and the precise methods for eliciting verbal reports.
We used a modified version of the Bidirectional Encoder Representations Model (henceforth: BERT); a new type of neural network transformer model for natural language processing that was pre-trained on billions of words 
(Devlin et al., 2018)
. For our specific purpose of labeling we used the DistilBERT model, which is a compacted and quicker version of the full BERT model. Using this version increases the efficiency and reduces the compute-time needed to run these types of models .
As our data set was not large enough to fine-tune the BERT model from scratch we used a pre-existing fine-tuned version of BERT provided by the Huggingface company, which is a curated collection of pre-trained models 
(Wolf et al., 2019)
. The fine-tuned model we selected for our labeling task is called "zero-shot-classification" pipeline. This pipeline allows classification of novel text that was not present in the training data and the association of a label (or a set of candidate labels) with a piece of text, irrespective of its domain or its aspect (e.g., emotion, name, event, etc.). In practice, the model takes a sentence and a list of candidate labels as input and returns accuracy score(s) to each tested label(s) 
(Sanh et al., 2022;
Yin et al., 2019)
. This 'accuracy' score ranges between 0 and 1, which can be understood as the 'confidence' the model assigns to the ability of a label to describe the sentence (see 
Figure A2
).
In zero-shot classification, the input text is encoded using a pre-trained BERT model, which tokenizes and generates contextual embeddings from the text. Each label category is also transformed into an embedding vector using techniques like averaging word embeddings or the Universal Sentence Encoder. The process then involves computing similarity scores between the text's embedding and the label embeddings, using metrics such as cosine similarity or Euclidean distance. These scores indicate how well the input aligns with each label's context. The label with the highest similarity score is chosen as the input text's predicted class, with each label receiving a score representing this alignment 
(Chen & Li, 2021)
. C an did ate La be ls 
Figure A2
 . A graphical representation of the process, in which BERT classifies text using 'zero-shot-classification' pipeline with an illustrative example. The left yellow box represents the first step, in which text is fed into the model alongside a set of candidate labels. In our example, the model is given the text "still used my own judgement mostly but checked out other peoples and went with them a few times" with candidate labels "influenced sometime" and "not influenced". The middle box represents the fine-tuned model that processes the association between every label with the input text.
The teal-coloured box on the right represents the output stage, in which end-users obtain the scores the model assigned to each, from best to worst. In this example, the model assigned a high accuracy score to the label "influenced sometimes" and low accuracy to the label "not influenced".


Labeling -Solo and Social Condition
To classify participants' verbal descriptions into 'description strategy' groups, BERT tests the relation between a set of candidate labels and selects the one that best describes this description. Our main goal for this analysis was to correlate these label scores with the individual model fits (that describe the way people integrate information) to assess the validity, or psychological plausibility of the models' core assumptions. To do that, we first needed to identify the quality of the verbal descriptions. We did so by comparing the selected (i.e., most suitable) label of individuals' strategy to their actual behaviour in the task. For example, if a person's best label on their verbal description indicates that they followed others when making their decisions in the second experimental block (see 
Figure A1b
), we could check if they, indeed, changed their choices, on the majority of the trials, in the Social condition relative to the trials in the Solo condition (see 
Figure A1a
).
Strategy Classification. For the first step in the current analysis we used the NLP model to classify strategies described in each experimental condition. As our model could not be trained (and fine-tuned) on data to classify decision making strategies, we defined the labels using terms that describe choice strategies in these conditions. The labels were allocated into a set of 'strategy-description' classes.
Strategy Classification vs. Actual Behaviour. The second step was to compare the way in which participants described their behaviour with what they actually did. We conducted this comparison in three steps. First, we selected the labels that the model assigned the highest confidence to (i.e., had the highest score) (see 
Tables A8 and A9
, A10, and A11). In a second step, we applied an exclusion criteria to exclude participants, who clearly described a strategy but failed to describe it in a way that revealed their choice/preference for one of the scoring strategies. Last, to identify 'good' describers we matched the labels' scores with actual behaviour.
Strategy Classification vs. Model Fits. Following these steps we then matched the weighting parameter (i.e., β) to the NLP labels' scores. To do so we rank-ordered the NLP model's score of the strategies in the Social condition in the following way: if the individuals' best label indicated that they did not rely on others' choices (e.g., "did not influence me"), the score should be low when matched with the weight-on-distance parameter. This is because a low β estimates indicates not relying on the choices of others. Therefore, as scores on labels were around .9 (indicating a high confidence that the label described the text well), we subtracted the score from 1 when compared with the weighting parameter (i.e., they put low weight on others). If the individuals' best label indicated that they incorporated others' choices into their own, we left them as is when comparing with the weight-on-distance parameter (i.e., they put high weight on others and thus have a high β estimate). We also coded our labels' scores to individuals who suggested that they were occasionally influenced by others' choices by converting a high score into a mid-weight by subtracting .5 from it. If the labels indicated that others had a frequent influence on choice, we ranked the score to align with a relatively high weight by subtracting .2 from their label accuracy score (i.e., interpreting "often" as .8 of the time).
Note that we asked participants only once about their strategy (at the end of the experiment) and hence relied on a single description per participant. Behaviour, on the other hand, was based on the proportion of choice across 60 trials. To address this issue, we allowed up to 30% discrepancy between the labels' scores for the verbal descriptions and the observed behaviour. That is, if the distance between actual behaviour and the confidence in the label's score was less than .3, participants were included for the analysis. If it was greater than .3 we excluded participants. The full description of the exclusion criteria are summarized in the Appendix.
In applying our discrepancy criteria to verbal reports from both solo and social settings, we observed a substantial decrease of the data of around 25%, which is less than ideal. This reduction might be attributed to the fact that our analysis was conducted post-hoc and reflects that our study was not initially designed to focus on verbal reports. Adhering more closely to 
Ericsson and Simon (1980)
's methodology from the outset might have mitigated this loss of data.
Tables A8 and A9 and 
Figure A5
 summarize these steps both graphically and descriptively. We discuss how to minimize this difference in measures of behaviour for future projects in the Discussion section.


Results
We used three sources of data: empirical data, individual model fits, and the verbal descriptions on decision strategies provided by participants. We report the results on the link between the NLP outputs and the Distance model outputs. The results of the same analysis using the Bayesian Updating model output are provided in the Appendix (see 
, Table A3
 and 
Figure A4
).


Verbal Descriptions and the Distance Model
We analysed whether individual fits of the Distance model (see These figures show three key patterns. First, we observe a greater density around the values 0 and .5 on the x axis relative to the values around 1.0. This can be explained by participants' conservative descriptions about the reliance on others. They do not often describe their choice as completely affected by others' behaviour. However, note that those individuals were removed after applying our exclusion criterion (i.e., applied to those who did not provide a verbal description that was in line with their behaviour in both the solo and social phases of the experiment).
We tested our visual impression with a linear model for each data set separately to learn if the verbal descriptions, were, indeed predictive of the psychological assumptions in the Distance model (i.e., that people integrate others choices depending on the weighted distance between others and oneself). Our results are summarised in 
Table   A1
 and suggest that the scores of the verbal descriptions are a significant predictor of the weighting Distance model parameter only after applying our exclusion criteria.
Notably, this leaves us with only a quarter of our sample, an issue that we return to in the Discussion.  .


Exclusion Criteria
Exclusion Criterion 1. Labeling the text participants generated allows us to compare the way in which they described their behaviour with what they actually did.
We conducted this comparison in three steps. First, we selected the labels that the model assigned the highest confidence to (i.e., had the highest score) (see 
Tables A1   and A3
). In a second step, we applied an exclusion criteria to exclude participants, who clearly described a strategy but failed to describe it such that it revealed their choice/preference for one of three strategies: i) a preference for risk aversion, ii) a preference for risk seeking, or iii) no clear preference (i.e., a description of a rather random strategy such as "I went with my gut feeling"). To identify 'good' describers we matched the labels' scores with actual behaviour. That is, a high confidence in a preference for risk seeking (risk aversion) was compared to the number of risky (risk averse) choices individuals made during the Solo condition. For example, the description: "I tried to maximize my chance to get the highest award" was classified as risk seeking since the highest reward could have been obtained by choosing the riskier jar (i.e., choosing to draw from jar A). If the participant indeed chose the riskier option in the majority of the trials (i.e., in more than 50%), she was identified as a "good (risk seeking) describer". For those, who were best described by a 'random' labels, we matched with a rather indifference to risk (i.e., about 50% risky choices). Clearly, these non-classifiable participants perhaps have tried to describe their behaviour but failed to do so in a way that captured their behavioural patterns. This is likely a result of the way, in which we formulated our questions. We return to address this issue in the Discussion section below. We acknowledge that verbal descriptions, just as behaviour, can be noisy. We, however, asked participants only once and hence relied on a single description. Behaviour, on the other hand, was based on the proportion of choice across many trials. To address this issue, we allowed up to 30% discrepancy between verbal descriptions labels scores and behaviour. That is, if the distance between actual behaviour and the confidence in the label's score was less than .3, participants were included for the analysis and excluded otherwise. Tables A1 and A3 and 
Figure A5
 summarize these steps both graphically and descriptively. We discuss how to minimize this error in future projects in the Discussion section.


Exclusion Criterion 2.
As in the Solo condition, we defined 'good describers'
as participants whose descriptions could be validated by their actual behavioural data.
This validation was computed as the (absolute) difference between the risk preferences revealed in the Solo condition and those revealed in the Social condition. Finally, as before, we allowed some error to exist between verbal descriptions and actual behaviour (i.e., error margin of up to 30%). If participants' reports were consistent with a change in behaviour between the Solo condition and in the Social condition that was within the determined error margin 3 , they were classified as 'good describers' and were included in the last step for this analysis and were excluded otherwise. To summarise, in the second and last exclusion step in the current analysis, only 'good describers' identified from both the Solo and the Social condition (see results, 
Figure A3c and ??)
 were included.
We further broke down each main class into sub-classes, that represented a specific task feature that was used to describe a decision process. Recall that participants could use many cues to make their decisions. They could rely on the distribution of the balls, on their outcomes or a combination of the two. We first added a subclass that simply described an explicit expression of a preference for a specific jar (e.g., "chose Jar A"/"chose Jar B"). We then added terms that were related to choices based on the distribution of the balls (e.g., "more black balls", "jar with the most white"), and those based on outcomes (i.e., "get 6", "get higher value"). For random/indifference descriptions we included labels such as "gut feeling" or "intuition". For example, while some people may focus on their chance to receive a/any reward, others might focus on the chance to obtain a specific reward. Two illustrative examples are: "I chose the jar with the majority of black balls" or by another participants as: "I tried to maximize my chance to win". These descriptions suggest that these people used different sources of information to indicate a preference for the same Jar (Jar B). That is while the first description focused on the distribution of the (black) balls, the second focused on the chance to win a reward (which is maximized in Jar B). In contrast, we obtained other descriptions in line with a preference for the riskier Jar A; examples include: "I chose the jar with the majority of white balls" and "I tried to maximize my chance to get the highest award". 
Table A5
 contains the full list of all the main and sub-classes of the labels we tested in this analysis.    
Figure ?
? seems almost identical to A3a, the statistical analysis provides evidence that there is a (weak) correlation between the labels' scores and the individual fits from the Bayesian Updating model. This was also true when we test this relationship on the data that consists of 'good' describers after the exclusion criteria was applied to the data (see 
Figure ??
). The results of the linear models are summarised in 
Table A1
. The findings suggest that the language model output is a significant predictor in the full data set and after applying the exclusion criterion.


Bayesian Model


Discussion
While our case-study data does not provide the ideal test-bed for these ideas, we think they provide a useful proof of concept for the application of NLP models in other tasks.
We observe a consistent relationship between models estimates' and verbal descriptions only when the relatively small sub-sample of 'good' describers are included in the analyses (see 
Figure A5
 and 
Table A1
). These findings indicate that mapping verbal descriptions onto either empirical behaviour or the psychological assumptions made by our models is not a straightforward task.
There are two ways to interpret our mixed results. The first takes our results as a challenge to the underlying psychological process assumed by our model. That is, people do not assign a mental weight on the distance between themselves and others but rather act according to a model that was not yet considered. As can be seen in the Appendix, attempts to fit the Bayesian model suggest that people do not reliably follow the absolute updating principle either. This is a valid interpretation and our framework provides suggestions to next possible steps (see 
Figure 2
). One possible step is to use individuals' descriptions (especially those, who included their reasoning) as a guide in thinking about future models or possible modifications to the experimental design. For example, in the current case study, some individuals wrote that after trying to follow the choices made by the hypothetical other participants, they experienced losses or gains indicating that they were trying to learn about (spurious) dependencies between features of the experiment.
However, there is a valid reason to assume that our inconclusive findings are a direct consequence of the formulation of our questions. Judging by the results of the NLP models' text classification, we could see that although many of our participants indeed describe their choice processes, their responses did not contain the necessary information to align exactly with our models' parameters. That is, we did not ask questions that could provide sufficiently rich descriptions. First, the verbal descriptions frequently described behaviour alone but did not disambiguate the reasoning behind it.
Just like any other scientific theory, a model's aim is to provide not only a description of some (behavioural) regularities but also an explanation for them 
(Navarro, 2021;
Proulx & Morey, 2021;
Singmann et al., 2022)
. In order to test the validity of our cognitive models we would need a description of both (i.e., the description of behaviour and the reasons underlying it). Despite our inconclusive results, we believe that posing appropriate questions to our participants that are defined with the theory and models in mind and that include both the question of how and the question of why will be able to bring new insights.


Labels
Labels Used for the NLP model's Analysis.


Table A4 Keywords Used in Social Condition
Meta Class always sometimes Often never Influence "influenced by" "sometimes influenced me" "often influenced me" "did not influence me"
Use "used the others" "sometimes used the others" "often used the others" "did not use the others"
Care & Majority "care about majority" "sometimes cared about majority" "often cared about majority" "did not care about the majority"
Care & Others "care about others" "sometimes cared about others" "often cared about others" "did not care about the others"
Note. The Keywords used for labelling the strategies description given as free-text by participants "jar with the most black"
"higher chance to get black ball"
Chance to win "high p(win)" "higher odds of success" Reward "get 6" Jar A Chose Jar A "chose jar A" "stick to jar A" Balls "more white balls" "higher chance to get white ball"
"jar with the most white"
Chance to win "high p(reward) " "riskier higher reward" "higher value"
"higher chance to get reward" "greater bonus"
Reward "go for £14 bonus" "get 14"
Random "gut feeling"
"intuition" "just stick" "just switch"
Unclassified "how many black and white balls"
"correct"
Note. The Keywords used for labelling the strategies description given as free-text by participants in the Solo condition  
Figure A5
 . Our conceptual framework for the link between verbal description analysis and cognitive models applied to the current case study. The model starts with input data consisting of 3 (correlated) sources of information (green circles). To identify good verbal descriptions, the verbal reports are evaluated via Text Analysis models and are matched with behaviour (blue rectangles). Good Describers are participants, whose verbal descriptions could be successfully classified based on the NLP model output. Once text analysis is performed and good describers are selected (here a total of 21 out of 81 participants), both validation on participants' understanding of the experimental manipulation and new ideas for potential models could be generated (orange rectangle). Finally, the link between the individual NLP model outputs and the individual model fits are correlated (red rectangle) "sometimes influenced me" "often influenced me" "did not influence me"
Use "used the others" "sometimes used the others" "often used the others" "did not use the others"
Care & Majority "care about majority" "sometimes cared about majority" "often cared about majority" "did not care about the majority"
Care & Others "care about others" "sometimes cared about others" "often cared about others" "did not care about the others"
Note. The Keywords used for labelling the strategies description given as free-text by participants in the Social condition Chose Jar B "chose jar B" "stick to jar B" Balls "more black balls" "jar with the most black"
"higher chance to get black ball"
Chance to win "high p(win)" "higher odds of success"
Reward "get 6" Jar A Chose Jar A "chose jar A" "stick to jar A" Balls "more white balls" "higher chance to get white ball"
"jar with the most white"
Chance to win "high p(reward) " "riskier higher reward" "higher value" "higher chance to get reward" "greater bonus"
Reward "go for £14 bonus" "get 14"
Random "gut feeling" "intuition" "just stick" "just switch"
Unclassified "how many black and white balls"
"correct"
Note. The Keywords used for labelling the strategies description given as free-text by participants in the Solo condition     Note. Verbal descriptions provided by participants on their strategies used to make decisions in the Solo condition in Experiment 3. Pid represents the participants ID. Best Label represents the label that obtained the highest accuracy score by the BERT model. Score represents the score associated with the best label obtained by the BERT model. Cog. Model Fits represents the individual fit of the Distance model, and
Behaviour Distance represents the discrepancy between the NLP model's score and actual behaviour.
Figure
on the exact elicitation methods of verbal reports: -Elicitation Point: Simultaneous or Retrospective? -Verbal Reporting: Structured or Spontaneous? -Instruction Formulation: Tailoring effective instruction -Elicitation Cadence: Continuous or Sporadic?


Figure 1 .
1
Flowchart illustrating the steps for eliciting verbal reports. Identify the Experimental Task: Start by assessing the experimental task's suitability for eliciting verbal reports. Analyze factors such as verbalizability and time constraints Identify Candidate Models: Once the task is defined, select cognitive models for comparison and testing. Describe their psychological assumptions, akin to verbal reports and determine if these assumptions can be expressed verbally. Decide on Elicitation Methods: Make crucial decisions about collecting verbal reports during the experiment. Consider: Elicitation Point:


of verbal reports can take advantage of NLP techniques, including methods spanning from vectorization and labeling, to clustering and keyword extraction. By transforming qualitative verbal data into quantitative, structured information, researchers can attempt to effectively discern the underlying cognitive mechanisms at play during experimental tasks.


Figure 2 .
2
Our conceptual framework for the link between verbal description analysis and cognitive models. The process starts with input data consisting of 3 (correlated) sources of information (green circles). To identify good verbal descriptions, the verbal reports are evaluated via Text Analysis models and are matched with behaviour (blue rectangles). Once text analysis is performed it provides both validation on participants' understanding of the experimental manipulation and new ideas for potential models (orange rectangle). Finally, the link between the individual NLP model outputs and the individual model fits are correlated (red rectangle)


, they were able to approximate how individuals evaluate options and make choices in natural contexts. Such models analyze the semantic content of language used by individuals when discussing their preferences, thereby revealing underlying patterns and factors that influence their decisions.


Figure
A1 . Trial Layout for the Experiments. Panel A shows steps from the Solo Condition in which participants indicated their risk preferences (choice of a jar to draw from) in the absence of any information about what other players chose. Panel B shows steps from the social condition in which participants indicated their risk preferences after seeing what 10 hypothetical other players had chosen to do.


Equation 1) are validated by the descriptions individuals provided about their choice strategies. Recall that this model assumes that information is integrated in relative terms. The free parameter β represents the weight on the distance between risk preferences exhibited alone and the proportion of other hypothetical players who chose the risky option in the social phase. It has a lower boundary of β = 0 representing ignoring others and paying attention only to one's own preferences and an upper boundary β = 1 representing full integration of others' choices into one's own. Figure A3 displays the individual fits of the Distance model as a function of the NLP model classification of their verbal descriptions. Figure A3a shows the results of the full data set consisting of all individuals' verbal reports and their model fits. Figures A3b and A3c displays the results from the data after applying the exclusion criteria (see Methods).


Figure A3 .
A3
The relationship between NLP Model's output (rank-ordered labels' scores) and the Distance model's individuals fits. (a) The relationship computed based on a full data set (no excluded describers). (b) The relationship computed based on a subset of the full data set (excluded describers based on the quality of description in the Solo condition). (c) excluded describers based on the quality of description in the Solo condition & Social condition. Note that the Distance model's free parameter is the weighing parameter β with 0 and 1 for the lower and upper boundaries respectively. Note that higher values of β (weighting in Distance model) indicate greater influence of social information, whereas higher values of r (Social Resistance in Bayesian Updating model) indicate lesser influence of social information.


Figure A4 .
A4
The relationship between Language Model's output and the Bayesian Updating model individuals fits. (a) The relationship computed based on a full data set (no excluded describers). (b) The relationship computed based on a subset of the full data set (excluded describers based on the quality of description in the Solo condition). (c) excluded describers based on the quality of description in the Solo condition & Social condition. Note that the Distance model's free parameter is the weighing parameter β with 0 and 1 for the lower and upper boundaries respectively. Note that higher values of β (weighting in Distance model) indicate greater influence of social information., whereas higher values of r (Social Resistance in Bayesian Updating model) indicate lesser influence of social information. P (C) ∝ P (S) × [


the Bayesian Updating model. P (C) is the posterior probability of choosing the riskier option in the social information condition. P (S) is the prior preference (i.e., when choosing without exposure to any social information) for one of the choice options. B(.) is the binomial probability mass function for observing n j successes or pieces of evidence out of a total of N . 4 r is a freely varying resistance parameter that measures the extent to which people inhibit the updating of their prior preference. If r = 0, no resistance is present and the individual maximally updates their behaviour based on social information. However, if r > 0, the individual is increasingly resistant towards updating such that their posterior choices are minimally affected by social information.Verbal Descriptions and the Bayesian Updating Model. We analysed whether individual fits of the Bayesian Updating model (see Equation 2) are validated by individuals' descriptions of their decision strategies. Recall that this model assumes that information is integrated on an absolute scale. The free parameter r represents the resistance towards updating social information with lower boundary of r = 0 representing no resistance (and choices that are fully explained by the observation of others' choices) and upper boundary r = 10 representing high resistance (and choices that are explained fully by one's own preferences for risk). In our original analysis of the data this model was outperformed by the Distance model both on the group and on the individual level. Interestingly, we find similar results to those observed in the Distance model. Although


Table A1
A1
Regression Model. Text-Analysis And the Cognitive Model's Individual Fits.
Data Set
Intercept β Estimate p-value R 2 N
Full
.37
.16
.157
.025 81
Exclusion Criterion 1
.33
.09
.560
.009 39
Exclusion Criterion 2
-.05
.28
.005
.345 21
Note. Regression estimates from the text analysis and Distance model individual fits. N
refers to the number of individuals included in the analysis.


Table A3
A3
Regression estimates from the text analysis and Bayesian Updating model individual fits.
Data Set
Intercept β Estimate p-value
R 2
N
Full
.54
-.02
.04
.053 81
Exclusion Criterion 1
.46
-.02
.19
.046 39
Exclusion Criterion 2
.25
-.03
.002
0.401 21
Data Set refers to the data set applied that generated the respective sub-figure.β Es- timate refers to the estimated slope examining the relation between the ranked-score of Language Label and the β parameter in the Distance model. R 2 refers to the percentage of the variance in the β parameter that is explained by the language label score. N refers to the number of individuals included in the analysis


Table A5
A5
V
IV
III
Labels
II
"stick to jar B"
I
"chose jar B"
"more black balls"
Keywords Used in Solo Condition
Class Sub-Class
Jar B
Chose Jar B
Balls


Table A6
A6
never
Often
sometimes
Keywords Used in Social Condition
Meta Class always
Influence "influenced by"


Table A7
A7
Keywords Used in Solo Condition
Labels
Class
Sub-Class
I
II
III
IV
V
Jar B


Table A8
A8
Individual Raw Verbal Descriptions, NLP output and Exclusion Criteria Included? looked at how many blacks there were in grid and aimed for the jar with the most black.
Pid Verbal Description
Risk Pref. Solo Best Label Stage 1 -Included? Stage 2 -.08 Label Score jar with the most black .92 Yes YES
3
estimated the number of black balls.
.25
correct
.44
No
NO
4
based on the average number of black balls. from 0-49. jar a.
.0
how many black and white balls
.23
No
No
from 50-100. jar b.
5
what looked correct
.58
correct
.96
No
No
6
i played it safe to begin with choosing the outcome with higher
.68
higher chance to get reward
.98
Yes
YES
odds of success but then if i went on an unsuccessful streak i
went for the riskier higher reward for a while.
7
confident in my choices
.07
correct
.84
No
No
8
likelihood of the white balls
.31
higher chance to get white ball
.99
Yes
No
9
looked at the grid
.41
correct
.33
No
No
10 i tried to stick if i thought it was jar b
.0
stick to jar B
.97
Yes
Yes
11 tended to choose jar b for the higher percentage chance of
.45
higher chance to get black ball
.99
Yes
Yes
pulling a black ball.
12 i usually decided to stick with my initial choice of whichever
.6
greater bonus
.66
Yes
Yes
jar i thought the ball came from. there were pros and cons
either way. for example jar a paid a bigger potential bonus,
but with a much lower probability.
13 i chose the one that would give me the better chance of a
.43
higher chance to get reward
.99
Yes
N0
reward
14 generally tried to go for £14 bonus
.73
higher chance to get reward
.95
Yes
Yes
15 i tried to go with the higher value
.65
higher value
.99
Yes
Yes
16 i guessed which jar it came from by looking at the number of
.0
intuition
.93
Yes
No
coloured balls
17 honestly they were random or gut feeling
.33
gut feeling
.98
Yes
No
18 i chose based on hoe many black and white balls in the sample.
.55
how many black and white balls
.97
No
No
then i mostly stuck because i thought that was the better
option.
19 tried to see which jar it might have come from and then the
.82
riskier higher reward
.59
Yes
YES
odds of it being £14 or £6
20 based on how many white balls
.34
higher chance to get white ball
.78
Yes
No
21 by looking at the split of balls on the board and thinking
.48
higher chance to get reward
.93
Yes
No
what one would be most likely to come out next to secure the
biggest bonus
22 assessment of what's come out of the jar and which one gives
.0
higher chance to get black ball
.96
Yes
Yes
the best chance of black ball in next draw
23 determined which urn by higher % of colour, e.g. if higher
.82
higher chance to get white ball
.88
Yes
Yes
% white then thought it was urn 1. if i thought it was urn 1
but the % laid out were close (e.g. say a 60-40 split) i would
switch. if i thought it was urn 1 but not many black had been
drawn i would stick. i always tried to draw a ball from urn 1.
24 intuition
.53
gut feeling
.98
Yes
Yes
25 i chose jar b if it looked like more balls were black. i eventually
.08
higher chance to get reward
.99
Yes
No
decided to start sticking even on jar a because the potential
prize was bigger.
26 i thought it was probably better to aim to choose from jar b.
.0
stick to jar B
.99
Yes
Yes
although the reward was less, it was 70%.
27 i chose with the potential to get the £14 ball in all cases. i
1.0
go for £14 bonus
.96
Yes
Yes
chose it based on the chance of this in each case.
28 i always wanted 14
.57
higher value
.97
Yes
YES
29 i weighed up the % of each colour ball
.44
how many black and white balls
.72
No
No
30 gut feelings, didn't overthink anything.
.48
gut feeling
.99
Yes
Yes
31 how many white / black balls had been pulled in the bottom
.73
how many black and white balls
.99
No
No
row
32 generally aimed for a greater chance of obtaining a reward i.e.
.16
higher chance to get reward
.99
Yes
NO
the £6 (7 in 10) over the riskier £14 (3 in 10). the jar choice
was an assumption on the majority colour i saw before me.
33 tried to choose the jar by the number of white or black ball
.13
stick
.97
Yes
No
percentage picked out and the stuck to the choice.
34 by seeing how many black balls were out and directly before
.53
higher chance to get black ball
.46
Yes
No
it and trying to work out the chance of another.
35 based on the amount of balls in each urn
.3
higher chance to get reward
.67
Yes
No
36 tried to estimate likelihood that the sample was from each jar
.37
get 6
.32
Yes
Yes
37 i compared how many black ball came out.
.44
jar with the most black
.45
Yes
Yes
38 looking at the picks
.42
higher chance to get reward
.54
Yes
No
39 i went by how many black balls i believe i saw and worked on
.42
higher chance to get black ball
.98
Yes
Yes
the greater chance of the black ball coming from pot b
40 i chose whichever one had the best odds.
.25
higher odds of success
.94
Yes
Yes
Note. Verbal descriptions provided by participants on their strategies used to make decisions in the Solo
condition in Experiment 2. Pid represents the participants ID. Question Solo Condition are verbal descriptions participants provided about how decisions were made in the Solo condition. Risk Pref. represents the proportion of risky choices across all trials in the Solo condition. Best Label represent the label that obtained the highest accuracy score by the BERT model. Label Score represent the score associated with the best label obtained by the BERT model. Stage 1 -Included? represents whether the individual was included after the first stage of exclusion (in which none classifiable labels were identified). Stage 2 -Included? represents whether the individual was included after the second stage of exclusion, in which labels that were unsuccessfully matched with choice behaviour were excluded.


Table A9
A9
Individual Raw Verbal Descriptions, NLP output and Exclusion CriteriaVerbal descriptions provided by participants on their strategies used to make decisions in the Solo
Pid Verbal Description
Risk Pref. Solo Best Label
Label Score Stage 1 -Included? Stage 2 -Included?
1
I made decisions based on the likelihood of getting the £6.
.2
get 6
.98
Yes
Yes
2
I thought about probabilities THROUGHOUT
.09
higher odds of success
.44
Yes
Yes
I tried to make sure the final pick came from jar B which
.0
stick to jar B
.99
Yes
Yes
would give me the greater chance of a nonzero number.
4
I tried to choose from jar b the whole time.
.4
chose jar B
.89
Yes
Yes
5
Mostly chose to stick
.8
chose to stick
0.97
Yes
No
6
I tried to choose the 14£ reward
.79
go for £14 bonus
.98
Yes
Yes
7
Based on what the jar with the highest number of black balls.
.2
jar with the most black
.98
Yes
Yes
8
I guessed the ones with more black were from jar b
.28
odds of black
.96
Yes
Yes
9
I just tried to figure the odds if it was black I would try to
.45
odds of black
.96
Yes
Yes
stick and if it was white then I figure most likely will be 0
anyway so I may switch or keep it the same.
10 I went for the risk and high reward Jar A
.84
higher odds of success
.99
Yes
No
11 I looked at the probability of choosing a black ball for each
.23
likelihood to get black ball
.98
Yes
Yes
jar.
12 I chose to stick if I was reasonably sure it was jar B since jar
.0
higher chance to get black ball
.99
Yes
Yes
B had a higher chance of getting a black ball.
13 by instinct
.42
gut feeling
.99
Yes
No
14 I tried to ensure I was picking Jar B each time
.0
stick to jar B
.99
Yes
Yes
15 I almost always switched to B, because it had the higher per-
.06
higher chance to get black ball
.99
Yes
Yes
centage of black balls.
16 By how likely I felt I would get a black ball
.46
likelihood to get black ball
.98
Yes
Yes
17 Based on the sample
.44
chose to stick
.86
Yes
No
18 whether more balls were black or white
.06
how many black and white balls
.94
No
No
19 by looking at the likelihood of a black ball being pulled out
.05
likelihood to get black ball
.98
Yes
Yes
20 When I chose alone, I looked at the balls in the grid and tried
.05
chose to switch
.7
Yes
No
to see how many white vs black balls there were; if I felt that
there were a lot of white balls, I chose A and then when I saw
that there was a lot of black balls, I chose B. I usually stayed
with the jar that I felt was which ever one had the most balls,
I don't really think I switched much.
21 First, I tried to choose the jar with the most expensive black
.4
chose to switch
.99
Yes
No
balls. When I realized that wasn't working and that I should
go with the lower cost black balls of which there were many I
did that.
22 Going for the more likely 6 euro.
.15
higher value
.99
Yes
No
23 By looking for what colour the majority of the balls were
.3
higher chance to get reward
.99
Yes
No
and then when it came to sticking or switching I'd make that
choice to pick B majority of the times as it would yield a
higher probability of a reward
24 I wanted Jar B
.0
chose jar B
.99
Yes
Yes
25 i chose stick or switch depending on which answer i thought
.25
higher chance to get reward
.99
Yes
No
would let me pick from jar b. i tried to choose from jar b as
much as possible since it was more likely to reward me.
26 The choices were made based on probability that there was
.18
chose jar B
.99
Yes
Yes
going to be a black ball, almost exclusively choosing out of
Jar B
27 Based upon what I thought the probability would be that a
.84
likelihood to get black ball
.98
Yes
No
black ball would be picked and, if possible, from jar A
28 I always went for Jar B as it had the higher chance of picking
.0
higher chance to get black ball
. 99
Yes
Yes
a winning black ball
29 Probability of getting a prize was higher in box B so more
.35
higher chance to get reward
.99
Yes
No
likely to choose that
30 I wanted the biggest payout, regardless of the chance of get-
1.0
riskier higher reward
.99
Yes
Yes
ting 0 -if I have the option to get the higher payout, I risked
it.
31 I tried to choose B because even though the expected values
.0
chose jar B
.98
Yes
No
of Jars A & B were the same (4.20 pounds), I would rather
have the greater chance of getting the lesser versus the greater
chance of getting nothing.
32 i figured out what would be the most logical outcome and
.46
correct
.96
No
No
what was best for me.
33 I prefer the larger chance at a smaller reward so I tried to
.0
higher chance to get reward
.98
Yes
No
make sure the draw came from B
34 Chance
.64
higher odds of success
.95
Yes
No
35 by what i felt was the correct answer
.45
correct
.96
No
No
36 I calculated the probability of the Jar using the 10x10 grid
.08
higher chance to get black ball
.99
Yes
Yes
and then made a decision to go for Jar B that had a higher
probability of hitting a black ball albeit at a lower payoff.
37 I made choices based on what balls had been drawn. If it
.33
likelihood to get white ball
.97
Yes
No
looked like there were a lot of white balls in the draw, I'd
switch. If the last few balls drawn were white, I'd vacillate
between switching or sticking.
38 Random
.6
gut feeling
.91
Yes
No
39 I tried to go for the safest option. If I believed the ball was
.09
stick to jar B
.98
Yes
Yes
from jar B, I would always stick with it.
40 With math.
.76
correct
.66
No
No
41 based off intuition/counter intuition
.2
gut feeling
.99
Yes
No
Note.
condition in Experiment 3. Pid represents the participants ID. Question Solo Condition are verbal descriptions participants provided about how decisions were made in the Solo condition. Risk Pref. represents the proportion of risky choices across all trials in the Solo condition. Best Label represent the label that obtained the highest accuracy score by the BERT model. Label Score represent the score associated with the best label obtained by the BERT model. Stage 1 -Included? represents whether the individual was included after the first stage of exclusion (in which none classifiable labels were identified). Stage 2 -Included? represents whether the individual was included after the second stage of exclusion, in which labels that were unsuccessfully matched with choice behaviour were excluded.


Table A10
A10
Experiment 1. Individual Raw Verbal Descriptions, NLP output and Exclusion CriteriaNote. Verbal descriptions provided by participants on their strategies used to make decisions in the Solo condition in Experiment 2. Pid represents the participants ID. Best Label represents the label that obtained the highest accuracy score by the BERT model. Score represents the score associated with the best label obtained by the BERT model. Cog. Model Fits represents the individual fit of the Distance model, and Behaviour Distance represents the discrepancy between the NLP model's score and actual behaviour.
Pid
Verbal Description
Best Label
Score
Cog. Model Fits
Behaviour Distance
1
by looking how many black and white balls there were and some-
sometimes influenced me
.99
0.0
.12
times taking advice from others
2
i didn't find this information useful but did still look at it
did not influence me
.99
.76
.46
still used my own judgement mostly but checked out other peoples
sometimes influenced me
.99
.0
.25
and went with them a few times.
4
i paid no attention to other peoples information
did not influence me
.99
.04
.05
5
i went with what looked like the majority went with
influenced by
.99
.48
.005
6
i checked the other people's answers about half of the time and the
sometimes influenced me
.99
.0
.04
other half i went with trying to be cautious but then going for the
riskier option if the cautious answers were not paying off.
7
i found them confusing. sometimes i changed my mind based on
sometimes influenced me
.99
.61
.37
them and felt like i lost because of it.
8
i still went with what i thought rather than the majority
did not care about the
.99
.0
.21
majority
9
usually followed the majority
often cared about majority
.99
.76
.116
10
i kept my choices the same as the first experi-
sometimes influenced me
.94
.0
.0
ment.
trying to stick when i thought it was jar b
becauseithadmorechanceof money, eventhoughitwasalesseramount
11
sometimes followed the crowd opinion but mostly favoured jar b
sometimes influenced me
.99
.0
.11
still.
12
i typically took other people's information into considered and of-
sometimes influenced me
.99
1.0
.29
ten switched my choice to correspond with the majority. during
this round, i often preferred to pick jar b as that had a higher
probability of paying out a bonus overall.
13
i tried to find patterns
sometimes influenced me
.98
.78
.008
14
mostly went for the £14 bonus, but did occasionally try doing the
sometimes influenced me
.99
.16
.15
opposite to the majority as well as the same
15
i tried to go with the higher value
influenced by
.99
.0
.34
16
i guessed which jar it came from based on the colour of the balls,
did not influence me
.99
.0
.0
the other people's information didn't really influence me
17
random and gut feeling
influenced by
.99
.37
.16
18
the same, i didn't find the other people's information persuasive,
did not influence me
.99
.07
.11
they have no more data that i do.
19
tried to guess what jar it came from and then went with the ma-
sometimes influenced me
.98
1.0
.29
jority decision to stick or twist most times
20
looked at how many people stuck or switched to see if my pick was
influenced by
.99
.34
.11
wrong
21
i considered the majority choice of others but sometime went with
sometimes influenced me
.99
.28
.1
my own intuition
22
used the same basic process as when choosing alone but did a cross
sometimes used the others
.96
1.0
.50
check to see what other people were doing for additional support
23
exactly the same strategy, but i looked at the other choices and it
influenced by
.97
.0
.11
seemed as if most were trying to draw from urn 2??
24
influenced by them
influenced by
.99
1.0
.05
25
i wasn't interested in other people's guesses as they had no more
did not care about others
.99
.9
.45
information than i did. i used the same strategy as the solo mode.
26
often i found going with the majority seemed to lead to me getting
sometimes influenced me
.99
.99
.48
rewards more often, so i often used their choices to make mine
27
i did exactly the same as i did previously. i did not take any notice
did not influence me
.99
.0
.0
of the other people's information.
28
same as without advice
did not influence me
.99
.6
.32
29
i did the same as in the choosing alone section, then looked for
influenced by
.98
.96
.21
confirmation bias
30
sometimes i went with the information but again gut lead the way.
sometimes influenced me
.99
.75
.077
31
i used mostly my original choice but did on occasion take the others
sometimes influenced me
.99
.92
.18
into consideration
32
didn't change my view greatly and still went with my assessment
did not influence me
.99
.14
.045
above. i backed my view over the crowd.
33
i got confused on the second part and at one point started to make
sometimes influenced me
.99
.83
.38
a different choice to what seemed the logical one.
34
part way through i switched to mainly clicking the same one as the
sometimes influenced me
.99
.01
.086
least people picked as this seemed right most of the time unless i
really was adamant my choice was right.
35
based on the amount of stick or switches
influenced by
.99
.48
.02
36
same as when i chose alone
did not use the others
.99
.91
.061
37
i mostly went with the overall percentage
influenced by
.99
1.0
.024
38
still looking at the picks
influenced by
.99
1.0
.48
39
same as when i was on my own.
influenced by
.97
.51
.33
40
i generally went with what others chose because i noticed that
sometimes used the others
.99
1.0
.2
when i played the odds it seemed off so i kind of lost interest in
focusing on that.


Table A11
A11
Experiment 3. Individual Raw Verbal Descriptions, NLP models' Output and ExclusionCriteria strategy as alone, but once seen that was not really working just tried to go with what the majority of 10 people did. If it was tied at 5, I usually just stuck to the stick option.Note. Verbal descriptions provided by participants on their strategies used to make decisions in the Solo condition in Experiment 2. Pid represents the participants ID. Best Label represents the label that obtained the highest accuracy score by the BERT model. Score represents the score associated with the best label
Pid
Verbal Description
Best Label
Score
Cog. Model Fits
Behaviour Distance
1
I continued to make choices which I thought
.99
sometimes influenced
.09
.111
would secure me the £6, however, there were
me
occasions where I was swayed to go with Jar ie
chance of £14 based on others switching to it.
2
U tried not to take notice of others, it made
.98
sometimes influenced
.32
.118
me lose confidence in my own judgement when I
me
paid attention. I eventually decided I was better
off ignoring others and I think it went slightly in
my favour when I did
3
I tried to choose the jar that gave me the largest
.99
influenced by
.13
.069
chance of a nonzero value
4
It did not affect my choice
.99
did not influence me
.0
.333
5
Chose based on the majority but still went
.99
influenced by
.48
.233
mostly with stick
6
mostly ignored others information, I had my
.99
did not influence me
.97
.258
goal in mind
7
I was influenced by what others had done.
.99
sometimes influenced
.61
.127
me
8
I continued using my previous strategy and ig-
.99
did not influence me
.0
.189
nored what other people choosed.
9
I tried same .99
sometimes influenced
.52
.018
me
10 I didn't let the choices of others sway my opin-
.99
did not influence me
.48
.091
ion, I went with a mixed based on what I fancied
11 I looked at the probability of choosing a black
.99
did not influence me
1.0
.319
ball for each jar. The other people's information
did not sway me very much.
12 My method of choosing didn't change once I had
.99
did not influence me
.32
.164
information about other people's decisions.
13 I mostly followed my instinct, but on occasion I
.99
sometimes influenced
.0
.017
weighed the answers of others
me
14 Other peoples information had little effect and I
.99
did not influence me
.33
.176
still tried to choose Jar B each time
15 It did not change how I chose, I ignored what
.99
did not influence me
.2
.053
the others chose.
16 The same as without others choices
.97
did not influence me
.31
.068
17 Based on the sample but sometimes I checked
.99
sometimes influenced
.79
.358
what other people did
me
18 if I thought it was B, I stayed put. If I thought
.76
sometimes cares
.13
.044
it was A, I switched
about others
19 It didn't really influence my strategy.
.99
did not influence me
.86
.455
obtained by the BERT model. Cog. Model Fits represents the individual fit of the Distance model, and Behaviour Distance represents the discrepancy between the NLP model's score and actual behaviour.


Table A12
A12
Experiment 3. Individual Raw Verbal Descriptions, NLP models' output and ExclusionCriteria -Continued When I was making choices with the other people's information, I would switch sometimes, when I felt that there was maybe it was a different jar. Sometimes, I saw that only 3 people would switch, so I ended up taking a chance and switching and actually ended up getting the black ball in Jar A at least 5 times.. which is a lot more than I thought I'd get and this happened every time I did this strategy. Usually stayed with the one that I picked if I felt like more people switched than stuck with the Jar. But it was only when I felt like.. maybe there was a chance to risk, or when I Felt like I didn't want to risk it and
Pid
Verbal Description
Score
Best Label
Cog. Model Fits
Behaviour Distance
20
"stick with my jar."
.99
me sometimes influenced
.68
.306
21
Other people's choices didn't matter to me. I trusted
.99
did not influence me
.0
.378
my own decision making skills.
22
Still going for the more likely 6 euro.
.99
influenced by
.16
.059
23
The same as when I was alone but on the odd occa-
.99
sometimes influenced
.33
.191
sion I was uncertain I would look at the other peoples
me
information
24
I wanted Jar B
.99
influenced by
.57
.305
25
"other people's information did not affect my choices,
.99
did not influence me
.0
.236
i still tried to choose from jar b as much as possible"
26
"Mostly based out of probability, but sometimes when
.99
sometimes influenced
.28
.055
there was a significant amount of people choosing
me
from Jar A, I thought it might be a good idea and
it worked out a few times."
27
"As above, based upon what I thought might be the
.98
sometimes influenced
1.0
.47
probability for a black ball from jar A (best choice)
me
would be picked -if not high probability then from
jar B. Was helped a little by others' choices at first
but saw they were pretty useless :("
28
I ignored the additional information and stuck with
.99
did not influence me
.14
.067
my original strategy
29
Sometimes looked to see how others decided adn
.99
sometimes influenced
.15
.157
sometimes went with majority
me
30
The same as before. The responses were obviously
.99
did not influence me
.034
.018
auto-generated and NOT real people because they
didn't make any sort of sense. I ignored them com-
pletely.
31
"I tried to ignore the other people, but it sure seemed
.990
sometimes influenced
.078
.036
like the experimenters were toying with me in this
me
section. It seemed like I got a lot more 0's than I
would have by the rules as presented. "
32
i took into account what people did and made my
.990
sometimes influenced
.27
.079
decision partly based on that.
me
33
Exactly the same way. I tried to get a draw from B.
.98
influenced by
.0
.0
34
Follow the crowd (mostly)
.99
often used the others
.26
.202
35
by sticking to my guns
.92
did not influence me
.14
.176
36
"Well, most of the additional data was 50-50 so that
.99
did not influence me
.25
.215
did not help much. If it was a borderline A or B, I
went with what the majority wanted to do."
37
I used the exact same tactics. What other people
.99
did not influence me
.075
.127
chose didn't concern me at all.
38
I was influenced by others choices
.99
sometimes influenced
1.0
.055
me
39
I didn't pay too much attention to their choices to be
.99
did not influence me
.0
.018
honest. I was always stuck with my strategy.
40
With math
.98
influenced by
.47
.07
41
same way as alone
.99
did not use the
.70
.254
others


in learning about how human articulation of strategies can be used to validate, refine and inspire novel computational models of cognition.


EV JarA = EV JarB = 4.2; V ariance JarA = 41.16, V ariance JarB = 7.56


This model takes as its likelihood the probability of observing n j pieces of information out of a total of N .








Appendix


Raw Data
 










When high-powered people fail: Working memory and "choking under pressure" in math




S
L
Beilock






T
H
Carr








Psychological science




16


2


















P
Bhargava






A
Drozd






A
Rogers




arXiv:2110.01518


Generalization in nli: Ways (not) to go beyond simple heuristics










arXiv preprint








Inductive reasoning in minds and machines




S
Bhatia




10.31234/osf.io/hkpm3


















Transformer networks of human conceptual knowledge




S
Bhatia






R
Richie








Psychological Review
















Naturalistic multiattribute choice




S
Bhatia






N
Stewart








Cognition




179
















Using cognitive psychology to understand GPT-3




M
Binz






E
Schulz




10.1073/pnas.2218523120








Proceedings of the National Academy of Sciences


the National Academy of Sciences






120












Turning large language models into cognitive models




M
Binz






E
Schulz


















The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive Psychology




57


3
















Functional neuroimaging of intertemporal choice models: A review




R
M
Carter






J
R
Meyer






S
A
Huettel








Psychology, and Economics




3


1










Journal of Neuroscience








Zs-bert: Towards zero-shot relation extraction with attribute representation learning




C.-Y
Chen






C.-T
Li




arXiv:2104.04697










arXiv preprint








The 'paradox' of converging evidence




C
P
Davis-Stober






M
Regenwetter








Psychological Review




126


6


















J
Devlin






M.-W
Chang






K
Lee






K
Toutanova




arXiv:1810.04805


Bert: Pre-training of deep bidirectional transformers for language understanding










arXiv preprint








Protocol analysis: Verbal reports as data




K
A
Ericsson






H
A
Simon








Psychological Review


Ericsson, K. A., & Simon, H. A.






The MIT Press






Verbal reports as data








Do procedures for verbal reporting of thinking have to be reactive? a meta-analysis and recommendations for best reporting methods




M
C
Fox






K
A
Ericsson






R
Best








Psychological Bulletin




137


2
















Value-based attention but not divisive normalization influences decisions with multiple alternatives




S
Gluth






N
Kern






M
Kortmann






C
L
Vitali








Nature Human Behaviour




4


6
















The framing effect and risky decisions: Examining cognitive functions with fmri




C
Gonzalez






J
Dana






H
Koshino






M
Just








Journal of Economic Psychology




26


1
















Accounting for attention in sequential sampling models of decision making




I
Krajbich








Current Opinion in Psychology




29
















Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions




I
Krajbich






A
Rangel








Proceedings of the National Academy of Sciences




108


33
















Insight and strategy in multiple-cue learning




D
A
Lagnado






B
R
Newell






S
Kahan






D
R
Shanks








Journal of Experimental Psychology: General




135


2


162














If mathematical psychology did not exist we might need to invent it: A comment on theory building in psychology




D
J
Navarro








Perspectives on Psychological Science




16


4
















Unconscious influences on decision making: A critical review




B
R
Newell






D
R
Shanks








Behavioral and Brain Sciences




37


1
















Open minded: Searching for truth about the unconscious mind




B
R
Newell






D
R
Shanks








The MIT Press


Cambridge, Massachusetts












Telling more than we can know: Verbal reports on mental processes




R
E
Nisbett






T
D
Wilson








Psychological Review




84


3
















How do people integrate private and social information when making risky decisions?




T
Ostrovsky






S
X
Liew






B
R
Newell












manuscript in preparation








From verbal reports to model validation: Theoretical framework and application




T
Ostrovsky






P
Ungermann






B
R
Newell






C
Donkin








Tagung experimentell arbeitender psycholog:innen










conference of experimental psychologists








Beyond statistical ritual: Theory in psychological science




T
Proulx






R
D
Morey








Perspectives on Psychological Science




16


4
















Verbal data and decision process analysis




R
Ranyard






O
Svenson








Psychology Press Oxford


UK.












A theory of memory retrieval




R
Ratcliff








Psychological Review




85


2
















Diffusion decision model: Current issues and history




R
Ratcliff






P
L
Smith






S
D
Brown






G
Mckoon








Trends in Cognitive Sciences




20


4
















The construct-behavior gap in behavioral decision research: A challenge beyond replicability




M
Regenwetter






M
M
Robinson








Psychological Review




124


5
















Four internal inconsistencies in tversky and kahneman's (1992) cumulative prospect theory article: A case study in ambiguous theoretical scope and ambiguous parsimony




M
Regenwetter






M
M
Robinson






C
Wang








Advances in Methods and Practices in Psychological Science




5


1
















Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter




V
Sanh






L
Debut






J
Chaumond






T
Wolf




10.48550/arXiv.2110.08207


arXiv:1910.01108












arXiv preprint








Multitask prompted training enables zero-shot task generalization




V
Sanh






A
Webson






C
Raffel






S
H
Bach






L
Sutawika






Z
Alyafeai






.
.
Rush






A
M




10.48550/arXiv.2110.08207


















Characteristics of dissociable human learning systems




D
R
Shanks






M
F S
John








Behavioral and brain sciences




17


3
















Statistics in the service of science: Don't let the tail wag the dog




H
Singmann






D
Kellen






G
E
Cox






S
H
Chandramouli






C
P
Davis-Stober






J
C
Dunn








Computational Brain & Behavior










others (2022)








Process descriptions of decision making




O
Svenson








Organizational Behavior and Human Performance




23


1
















Eliciting and analyzing verbal protocols in process studies of judgment and decision making. Process and structure in human decision making




O
Svenson




















Toward nonprobabilistic explanations of learning and decision-making




A
Szollosi






C
Donkin






B
R
Newell








Psychological Review
















People as intuitive scientists: Reconsidering statistical explanations of decision making




A
Szollosi






B
R
Newell








Trends in Cognitive Sciences




24


12
















How top-down and bottom-up attention modulate risky choice




Y
Vanunu






J
M
Hotaling






M
E
Le Pelley






B
R
Newell








Proceedings of the National Academy of Sciences




118


39


2025646118


















A
Vaswani






N
Shazeer






N
Parmar






J
Uszkoreit






L
Jones






A
N
Gomez














Attention is all you need




I
Polosukhin








Advances in Neural Information Processing Systems






30














A conversation-based process tracing method for use with naturalistic decisions: An evaluation study




J
Williamson






R
Ranyard






L
Cuthbert








British Journal of Psychology




91


2




















T
Wolf






L
Debut






V
Sanh






J
Chaumond






C
Delangue






A
Moi


















Huggingface's transformers: State-of-the-art natural language processing


10.48550/arXiv.1910.03771


arXiv:1910.03771








arXiv preprint












T
Wolf






L
Debut






V
Sanh






J
Chaumond






C
Delangue






A
Moi


















Transformers: State-of-the-art natural language processing






Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations


the 2020 conference on empirical methods in natural language processing: system demonstrations
















Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach




W
Yin






J
Hay






D
Roth




arXiv:1909.00161










arXiv preprint









"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]