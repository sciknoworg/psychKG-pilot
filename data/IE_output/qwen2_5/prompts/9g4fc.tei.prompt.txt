You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



The odd case when cognitive biases are more prevalent in researchers than in samples 
Walmsley and Gilbey (2016;
 based on Walmsley, 2016 1 ) reported on the impact of cognitive biases on pilots' decision-making, concluding that their findings "provide strong evidence" of such impact and that the effects were "likely to place pilots in greater danger" (p. 542). The research was picked up by news outlets (e.g., 
Staufenberg , 2016)
 and social media (e.g., 
Fradera, 2016
) with a spin-off into the irrationality of pilots and flight safety. Despite all said, however, the original research seems more informative about the cognitive biases of the researchers than of their sample, as subsequently discussed.
In Study 1, on the anchoring and adjustment effect, the authors declared that pilots exposed to the high anchor "reported a higher assessment of cloud height…(M = 2468…), visibility…(M = 15.50 km…), 
[and]
 likelihood of deciding to assess conditions as safe enough to continue the flight…(M = 6.62…) compared to when they had been exposed to the low anchor", thus "failing to adjust their perceptions in the face of evidence to the contrary…[and] more likely to feel that it was safe to continue the flight", particularly experienced pilots 
(p. 535-536)
.
And yet, all scenarios were designed with a cloud base at 2500 feet and 16 km of visibility, adequate for VFR (visual flight rules) flight. Therefore, participants exposed to the high anchor perceived the experimental conditions accurately and their higher likelihood to continue flying into objectively safe VFR conditions was not irrational. Only the participants exposed to the low anchor showed bias, heavily discounting the observable evidence and,
1 Authorship conventions dictate that authors are solely responsible for the contents they publish, irrespective of sources or influences. This said, I was co-supervisor of Walmsley's thesis and, therefore, may have had some influence-or lack of, thereof-on Walmsley's work. As far as I may bear some responsibility as a 'ghost' author of Walmsley's thesis, the criticisms laid here may equally apply to me retrospectively, and shall serve as a partial correction of the research literature cited (also BetterScience, 2016, 
Perezgonzalez, 2016
.
worryingly, preferring to continue their flight (M = 6.17 on a scale running from '1' = strongly disagree to '9' = strongly agree) than to divert.
In Study 2, on the confirmation bias, the authors concluded that there was "no evidence that participants valued disconfirmatory evidence over confirmatory [one]" (p. 539). However, the simple tests of significance they used are not appropriate for ascertaining null hypotheses 
(Fisher, 1954)
, making such conclusion unwarranted. They need 
Neyman-Pearson's approach (1933)
 or Bayes factors 
(Jeffreys, 1961)
 for such purpose. More simply, though, had they used confidence intervals (CIs, 
Neyman, 1935;
Cumming, 2012;
Perezgonzalez, 2015a
) they would have observed that some evidence seems apparent but that the tests were not sensitive enough to capture it as a statistically significant result ( 
Figure 1
).


FIGURE 1 | Confidence intervals for measures of confirmation bias in pilots
Notes: Overall sample. Novice versus expert pilots. Pilots without versus with experience entering instrument meteorological conditions (IMC). The horizontal line at point 1.66 represents the number of disconfirming choices expected by chance.
Furthermore, Study 2's methods are suspect. The disconfirmatory items were largely ambiguous (phrased as 'almost certainly', 'may') for three of the five scenarios while the confirmatory items were certain (phrased as 'can see', 'safely flew'), and adjudication to a confirmation bias was self-serving, as "inappropriate use of the positive testing strategy may put the pilot at risk of VFR flight into IMC [instrument meteorological conditions] and was interpreted as evidence of confirmation bias" (Walsmley, 2016, p. 92). The latter case was compounded by scenarios urging prompt landing either because of developing emergencies (e.g., engine making strange noises) or because of their "it is important" phraseology (e.g., very important meeting but no other aerodromes in the immediate area), with heavy costs in time and opportunity if diverting. Such design may partly account for pilots discounting the ambiguous disconfirming items, while not fencing off alternative rationalizations which expert pilots and those with experience entering inadvertently into IMC may have entertained (such as their confidence in having navigated IMC successfully, or deciding to wait for the situation to become less ambiguous before diverting, or planning an emergency landing as last resort).
Study 3, on outcome bias-which, ironically, the authors showed when designing scenarios "favourable for conducting a VFR flight" yet "based on actual weather reports at the time of real VFR flight into IMC accidents" 
(Walmsley, 2016, p. 111-113)
-was based on group comparisons and found that participants exposed to negative outcomes assessed the situations as being worse than participants exposed to positive outcomes, thus concluding that the latter "were more likely to conduct the same flight themselves…[which] might increase the risks faced…with no guarantee that the outcome will be good" (p. 541).
However, the authors also failed to account for the methodology used: When compared against the standard scale (running from '1', a favourable assessment, to '9', an unfavourable assessment), the effect of negative outcomes is apparent both for vicarious judgement (M = 6.02) and vicarious risk assessment (M = 5.96), yet positive outcomes show no effect either way (M = 5.01, M = 5.03, respectively), meaning a neutral assessment or indecision ( 
Figure   2
). Thus, while negative outcomes made participants more conservative regarding their vicarious assessments, a positive outcome did not lead to liberal assessments, as implied.
More importantly, all groups reported their decision to fly to be safer than that of others (by 0.66 points for the negative outcome participants, by 0.85 for the positive outcome participants, and by 0.57 points for the control group). This illusion of control seems understandable for the positive outcome pilots, as they just showed a neutral assessment of vicarious experience. What is at odds is that pilots with the hindsight of negative vicarious information still showed a similar degree of illusion of control, remaining uncertain regarding to whether they will fly in similar conditions or not (one would expect them to score higher in aversion). The simple use of CIs would also have helped the authors locate the results in the overall context of their own methodology, leading to different conclusions.
Why did the authors go astray in their conclusions? I do not believe the authors were tempting 
Huff (1954)
 or 
Goldacre (2008)
, nor simply scaremongering. Instead, their methodology was not completely free of biases (confirmation and outcome biases) and, more importantly, they relied far too mindlessly on statistical significance as the only standard for result interpretation, thus becoming blind to their own methods and observed data, a cognitive bias of anchoring on a misunderstood approach to testing 
(Gigerenzer, 2004;
Perezgonzalez, 2015b)
. Consequently, while the tests results may have been technically correct, their divorce from the underlying methodological context made them factually wrong, as so were the conclusions attained.


FIGURE 2 | Confidence intervals for measures of outcome bias in pilots
Notes: n-, negative outcomes condition; ctrl, control group; p+, positive outcomes condition. Judgement, vicarious judgement of risk; Perception, vicarious assessment of risk; Decision, own assessment of willingness to flight under similar conditions. Horizontal line represents the neutral point on a nine-anchor scale.
 












Re: Pilots 'very likely' to misjudge flying conditions due to irrational decisions




Betterscience




10.6084/m9.figshare.4490789












Online comment to news article








Understanding the new statistics. Effect sizes, confidence intervals, and meta-analysis




G
Cumming








Routledge


New York, NY














R
A
Fisher




Statistical methods for research workers


Edinburgh, UK




Oliver and Boyd








12th edn








Sorry to say, but your pilot's decisions are likely just as irrational as yours and mine




A
Fradera














Web log post








Mindless statistics




G
Gigerenzer




10.1016/j.socec.2004.09.033






The Journal of Socio-Economics




33
















Bad science




B
Goldacre








Fourth Estate


London, UK












How to lie with statistics




D
Huff








W.W. Norton


New York, NY














H
Jeffreys




Theory of probability


Oxford, UK




Oxford University Press








3rd edn








On the problem of confidence intervals




J
Neyman




10.1214/aoms/1177732585






The Annals of Mathematical Statistics




6


3
















On the problem of the most efficient tests of statistical hypotheses




J
Neyman






E
S
Pearson




10.1098/rsta.1933.0009






Philosophical Transactions of the Royal Society of London. Series A




231
















Confidence intervals and tests are two sides of the same research question




J
D
Perezgonzalez




10.3389/fpsyg.2015.00034






Frontiers in Psychology




6


34














Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing




J
D
Perezgonzalez




10.3389/fpsyg.2015.00223






Frontiers in Psychology




6


223














Re: Sorry to say, but your pilot's decisions are likely just as irrational as yours and mine




J
D
Perezgonzalez




10.6084/m9.figshare.4460078












Web log comment. All comments removed by BPS Research Digest








The luxury of living in the Ivory Tower




J
D
Perezgonzalez








New Zealand Aviation News




20














Flights into deteriorating weather conditions: Investigating cognitive biases in weather-related decision making (Doctoral thesis




S
Walmsley










Manawatu, New Zealand






Massey University












Cognitive biases in visual pilots' weather-related decision making




S
Walmsley






A
Gilbey




10.1002/acp.3225






Applied Cognitive Psychology




30


4
















Pilots 'very likely' to misjudge flying conditions due to irrational decisions. The Independent




J
Staufenberg





















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]