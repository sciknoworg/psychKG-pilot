You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Standard models of economic decision-making generally assume a two-step decision process, where individuals identify and assign values to available options, and ultimately pick the option with the highest subjective value 
(1)
(2)
(3)
. The values attributed to individual options can derive from different sources. On the one hand, a priori neutral stimuli acquire positive or negative experiential values after association with past outcomes (rewards and punishments) 
(4)
(5)
(6)
. On the other hand, the explicit description of an option's possible outcomes and their probabilities are combined to form a subjective expected value 
(7)
(8)
(9)
(10)
. Such explicit descriptions may take many different forms, including written language (from simple vignettes to fully specified numerical variables), a symbolic code communicating the decision variables (payoffs and probability) in an unambiguous manner, or a combination of the two 
(11)
.
In the standard two-step model, the way option values are built (via experience or description) is only peripheral to the decision process itself, meaning that experiential and symbolic values converge to a central valuation and decision-making system 
(3,
(12)
(13)
(14)
(15)
(16)
.
Thereby, choices between experiential and symbolic options should present no particular challenge, because their values are translated into an internal common currency, allowing an unbiased comparison between these differently generated option values. This normative point of view is indirectly supported by the fact that the neural correlates of experiential and symbolic values largely overlap in the so-called brain valuation system 
(17)
(18)
(19)
(20)
.
However, several lines of evidence in behavioral decision-making research question the idea of a central valuation system. In fact, it is now a very well established that, when studied separately, experience-based and description-based choices display different properties: a phenomenon referred to as the description-experience gap 
(21)
(22)
(23)
(24)
. This difference in the subjective valuation of experiential and symbolic options poses a direct, theoretical challenge to the idea of a central valuation system 
(25)
. This rather suggests the existence of modality-specific valuation systems, relying on distinct cognitive representations, which would hinder, if not impede, the comparison between experiential and symbolic options.
Strikingly, this key prediction has not been directly assessed, because studies usually consider separate sets of decision problems for experiential and symbolic options 
(26,
23)
. Thereby, to date, very little experimental evidence has formally assessed the commensurability of experiential and symbolic option values, nor their mapping into a central or different valuation systems 
(27,
28)
. This is particularly problematic considering that hybrid choices seem to be the norm rather than the exception in our modern societies where descriptive information is omnipresent. For example, everyday situations like choosing between our favorite restaurant (experience) and a new one with good review (description) is a prototypical example of such a hybrid decision.
To fill this gap and challenge the commensurability of experiential and symbolic values, we designed a new behavioral protocol. The experiment started with a learning phase during which human participants repeatedly faced abstract cues paired with probabilistic outcomes, thereby learned to associate experiential expected-values to the originally neutral symbols. After this phase, participants were asked to make hybrid choices between the experienced symbols and described lotteries visualized as colored pie-charts (a standard way to represent value symbolically) 
(11)
. When making hybrid choices, participants treated the two kinds of options asymmetrically and, specifically, were neglecting experiential values. This asymmetry was robust across seven experiments, where we controlled for many possible alternative explanations, such as, insufficient learning, generalization issues or lack of incentives. Overall, the relative neglect of an option's value conditional on its source is consistent with the idea that different types of valuessuch as experiential and symbolicmay involve different representational systems, resulting in their incommensurability. The rightmost panel displays successive screens of a typical trials in the Experiential-Symbolic choice phase (ES). The ES-phase consists in binary choices between a lottery (standardly materialized as a piechart) and a symbol previously presented in LE-phase. In most experiments, the EE phase lasted 88 trials (8 E-options x 11 S-options). Durations are given in milliseconds. (B) The panels illustrate three possible hypotheses on how participants could make choices in the ES-phase. In each panel the probability of chosen the E-option is plotted against the value of the S-option (expressed as probability of winning a point). The insets represent the indifference points (where the curves cross 50%; of not unbiased indifference points should lay on the diagonal). The color of the curves indicates the value of the E-option (lowest: light orange; highest: dark orange). The leftmost panel illustrate the default hypotheses according to which E-options and S-options are fully commensurable and therefore the curves cross 50% (indifference point) at exactly the value of the E-option. The central panel illustrates experiential value neglect scenario according to which ES-choices are determined (almost) uniquely by the value of the S-options. Finally, the rightmost panel illustrates the symbolic value neglect scenario, accordingly to which ES-choices are determined (almost) uniquely the value of the E-options. (C) The panel displays the options values. The topmost part shows how E-option were organized in learning contexts (in all experiment except Exp. 4 and 7; of note, the attribution of the value to the symbols was randomized across participants). The bottommost part shows the lotteries used in the ES phase (in all experiment except Exp. 7). (D) The experiments were structured as follows: they all started with a learning phase (LE), where participants made choices between abstract symbols and received feedback information. After the LE phase, participants were asked to make repeated choices between each E-option and several lotteries (see 
Fig. 1A
 and 
Fig. 1C
). From Experiment 5 on, participants were also asked to make choice between E-options that were not necessarily presented together. Finally, we assessed the stated probability (SP) of winning for each symbol by asking participants to explicitly rate each E-option, following a probability matching procedure 
(29)
.


Results
We conducted a series of experiments structured in two main phases, one allowing the formation of subjective values from the experience of past outcomes, and a second where these experiential options (E-options) were presented against options whose subjective values were described by symbolic means (S-options) ( 
Fig. 1A)
. During the first (or learning: LE) phase, E-options were materialized by abstract shapes that provided no explicit information concerning the expected value (EV) of the option. During the LE choices, E-option values could therefore only be inferred from the history of gains (+1 point) and losses (-1 point) associated to a specific cue. E-options were presented in four fixed pairs, each featuring an EV-maximizing and an EV-minimizing option.
Subsequently, in the Experiential-Symbolic (ES) phase, participants were asked to make choices between the very same E-options of the previous phase and pie-charts explicitly describing the associated probabilities of gain and loss. As these ES, "hybrid" choices are the main focus of this paper, we thereafter delineate three plausible hypotheses concerning the behavioral output of this phase.
First, assuming that the subjective values of the E-and S-options are mapped into a common scale (common currency hypothesis), participants should make unbiased decisions in the ES phase. Accordingly, the probability of choosing, say, the E-option, will be jointly determined by the EV of the E-and the S-option ( 
Fig. 1B: left)
. In other terms, for a given E-option the inferred indifference point will precisely correspond to S-options with equal EV.
Alternatively, the possibility that subjective values are constructed and represented in a modality-specific way (representational gap hypothesis) entails that E-and S-options are not readily commensurable. This situation could lead to two possible scenarios. In one of them, participants make random choices in the ES-phase. In the other scenario participants could prioritize one of the two sources of information. Within this scenario, participants could resolve the tension between E-and S-options basing their choices primarily on the explicit symbolic values provided by the lotteries. In other terms, participants would pick the lottery, when positive, and reject it when negative, as if the Eoption values were neglected and regressed to zero (experiential value neglect; 
Fig. 1B:
 mid). In the other case, participants would present an over-reliance on experiential values and would display the opposite pattern: accept or reject an E-option without considering the S-option value (symbolic value neglect; 
Fig. 1B: right)
. Crucially, the ES phase of our experiments allows to tease apart these different scenarios by analyzing the probability of choosing an E-option as a function of the S-option being presented. More precisely, taking each E-option separately and uncovering the S-option (value) at which a preference shifts from the former to the latter provides us with an estimate of how much a participant values an E-option. Quantifying the relation between E-options and Soptions boils down to inferring indifference points (i.e., when the probability of choosing one option over the other is 50%) which acts as proxies of participant E-option values ( 
Fig. 1B: insets)
. dark orange). Dots represent the empirical indifference points, the value of a lottery that corresponds to a probability of choosing the symbol 50% of the times. (C) The panels represent for each symbol the inferred value (as expressed by the probability of winning; p(win)) as a function of the actual value. ES estimates are represented in orange, LE estimates in blue and SP estimates in pink. In the data-boxes, the dark tone line represents the mean, mid-dark tone the standard mean error, light tone a 95% confidence interval. The lines represent linear regression (dark tone), and the average standard mean error (light tone). (D)
Comparison of individual inferred slopes obtained from linear fit (see 
Fig. 2C
) in the three modalities (LE, ES and SP in blue, orange and pink, respectively). The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area probability represents density functions. ***p<0.001 paired sample t-tests.


First evidence for the experiential value neglect scenario
In the LE phase of the first experiment (N=76), we presented pairs of E-options in an interleaved manner (i.e., E-option pairs are distributed randomly in the sequence of trials) and we displayed only the outcome of the chosen option (partial feedback) ( 
Fig
 Regarding analysis of the ES phase, the probability of choosing an E-option in an ES decision was largely determined by the S-option EV-value and the preference shift abruptly occurred around S-option EV equal to zero (i.e., P(+1) = P(-1) = 0.5). Despite clear proofs of successful value learning and encoding during the LE phase, ES phasechoice pattern was clearly consistent with the experiential value neglect scenario. 
(Fig.   2B
: left).
To quantify and statistically compare the differences in preferences observed in the LE and the ES phase, we first estimated the theoretical subjective value of each E-option separately for the two choice types, proxied by its probability of winning a point: p(win) (remind that the outcomes are fixed, so the expected value of different options only depend on their probabilities to win). Concerning the LE phase, we leveraged on a classical associative learning approach, where we assumed p(win) to be iteratively updated as a function of a prediction error-minimizing learning rule 
(30,
31,
6)
. We were able to infer p(win) attributed to each E-option at the end of the learning process by fitting this, rather parsimonious and standard, model. Concerning the ES phase, subjective p(win) estimates were inferred using the following method: the probability of choosing a specific E-option over a S-option of various expected values was assumed to take the form of a logistic sigmoid function. We fitted those logistic functions to each E-option and individual, and used them to extrapolate the indifference points indexing E-options' subjective p(win).
Finally, to compare the overall valuation of the E-options in the LE and the ES phases, we computed a measure of how well the subjective p(win) estimates from each phase matched the objective underlying probabilities, using slopes estimates from linear regressions.
At this aggregate level, a slope equal to 1 corresponds to an unbiased representation of E-options' p(win), whereas a slope equal to 0 corresponds to random representations. In our data, the slopes estimated from the LE phase were significantly higher and closer to 1 compared to those estimated from ES-choices (T(75)=6.53, P < .001) ( We ruled out a first trivial interpretation for this result, by only including in the analyses participants that performed at 100% of correct response in catch trials (i.e. trials involving choices between two S-options; see Supplementary Materials), disseminated across the ES phase to ensure the participants' capacity to understand the symbolic representation of the probabilities.
In the following sections of the paper, we provide additional evidence in favor of the experiential neglect hypothesis by progressively ruling out alternative interpretations via additional measures and experiments.


Ruling out insufficient learning and forgetting
While the experiential neglect pattern observed in the ES phase is consistent with the idea that E-options and S-options are not equally considered in the decision process, it is also consistent with a much more trivial hypothesis: insufficient learning. Despite reinforcement learning model fitting suggesting otherwise (see 
Fig. 2C
: left), it is indeed possible that the neglect of E-option in the decision is caused by an imperfect and noisy E-option value representations at the end of the learning phase. To rule out this alternative interpretation, we devised a series of experiments where we changed the LE phase in order to improve learning, while keeping the (average) option values the same. In a second experiment (Exp. 2; N=71), we therefore presented decision problems as blocks (rather than interleaved as in Exp. 1), so as to improve performance and option identification by preventing the saturation of working memory 
(32)
. In a third experiment (Exp. 3; N=83), we additionally provided the outcome information concerning the unchosen optiona manipulation known for increasing accuracy 
(33,
34)
. Finally, on top of these variations, in a fourth experiment (Exp. 4; N=88) we also reduced the number of decision problems of the LE phase to two, such that each decision problem was presented for twice as many trials as in experiments 1-3, thereby reducing the uncertainty about the options' outcomes. These manipulations were successful in significantly increasing decision accuracy in the LE phase (Exp. 1: 0.66±0.01; Exp. 2: 0.71±0.01, β=0.05, T(314)=2.28, P < 0.05; Exp. 3: 0.82±0.01, β=0.16, T(314)=7.17, P < 0.001; Exp. 4: 0.79±0.01; β=0.13, T(314)=5.8, P < 0.001), while avoiding ceiling performance issues.
Indeed, even in the easiest experiments, accuracy was still significantly modulated by the decision values; for instance, the accuracy in the more difficult decision problem 
(60/40)
 was always lower compared to the easiest one ('90/10') (T=5.81, P<0.001; T=8.81, P<0.001).
Crucially, the remarkable increase in the LE phase accuracy of the new experiments (107% -124% of Exp. 1) was not paralleled by detectable qualitative differences in ES phase choice patterns 
(Fig 2B)
. In other terms, the experiential value neglect persists despite the uncertainty concerning the E-options' values being considerably reduced (via blocked design, complete feedback and increasing the number of trials per decision problem).
To quantitatively characterize this claim, we estimated the subjective p(win) for each Eoption separately for the LE and the ES phases and fitted a linear regression between the estimated subjective p(win) and their true values (as described above). Confirming the efficiency of our manipulations in increasing learning performance, the LE-inferred slopes The comparison between the first four experiments suggests that experiential value neglect is not a mere effect of insufficient learning. We indeed observe that an improved performance in the learning phase does not translate into a similar decrease of the experiential value neglect effect. However, independently of the quality of learning, it is also theoretically possible that participants forgot the E-option values when entering the ES hybrid choice phase, although the fact that the ES phase directly succeeded the LE phases within a matter of seconds makes it improbable. To rule out this possibility, in Exp. 1-4, we asked participants to evaluate the E-options' p(win) just after the ES phase, by implementing a fully incentivized stated probability (SP) procedure 
(35)
. More precisely, participants were explicitly asked to rate the probability of winning a point they attribute to an E-option, by means of a numerical rating scale 
(Fig. 1D)
.
We then evaluated the quality of the E-option memory retention by regression these stated probabilities against their true values. Note that because this elicitation happens after the ES phase, this SP-inferred slopes constitutes a lower bound of how well E-option values are learned and could be recovered during the ES phase. Yet, the SP-inferred slopes were systematically higher than the ES-inferred slopes and significantly so in Exp. 


Ruling out generalization issues and assessing the robustness to practice
The above-reported results from 4 experiments and 3 preference elicitation methods indicate that the experiential value neglect phenomenon cannot be accounted for by insufficient learning nor by mere forgetting. In the present section we rule out two additional alternative explanations. First, it should be noted that the ES phase involves a generalization process, because the E-options are extrapolated from the decision context where their subjective values are originally learned. It is therefore conceivable that the apparent experiential value neglect is spuriously created by a generalization problem.
Second, in the previously reported experiments, participants went through the different phases (LE, ES and SP) only once: perhaps participants were somehow taken by surprise by the ES phase. In that case, presenting them different phases of the experiment twice will possibly allow them to improve their decisions by anticipating the ES-phase 
(36)
.
To control for generalization and practice, we run two additional experiments. In Exp. 5
and Exp. 6 (N=71 and N=66), after the learning phase, we interleaved the ES-choices with choices involving E-options presented in all possible combinations (referred to as EE-choices). Thus, in all cases except one, EE-choices required being able to generalize their value to new decision problems. As in ES-choices, we plotted the probability of choosing a given E-option as a function of the alternative E-option ( 
Fig. 3B)
. To check whether experiential value neglect disappears if participants are given the opportunity to learn how to make ES decisions, Exp. 6 included a second session where we repeated all phases (LE, ES, ES and SP). Of note, E-options in the second sessions were materialized by a new set of symbols. presented the same characteristics as that of Exp. 3: complete feedback and block design) 
(Fig. 3A)
.
To formally assess the difference between EE-and ES-choices, we calculated for each participant their option-specific indifference points, following the same procedure used for ES-choices and we compared the inferred slopes across decision modalities. EE-inferred slopes were consistently significantly higher than ES slopes in both Exp. 5 and Exp. 6 (Exp. 5: T(70)=4.5, P < 0.001; Exp. 6.1: T(65)=4.08, P < 0.001).
Being presented with the whole experiment a second time had no detectable effect in choice behavior in neither the EE-or the ES-phase. Indeed, we observe no significant increase in the slopes in neither ES-(β=0.04, T(260)=0.84, P=0.4) nor EE-choices (β=0.1, T(260)=1.59, P=0.11) and the ES-inferred slopes were still significantly smaller compared to EE-ones (Exp. 6.2: T(65)=5, P < 0.001). This suggests that being exposed with the whole experiment one time and, by doing so giving participants the possibility to adjust the decision strategy does not affect the main results. information is considered a hallmark of normative behavior 
(37,
38)
. However, if E-option information processing (e.g. memory access/retrieval) is costly or if neglecting E-options does not hinders decision performance dramatically, it may become rational to do so 
(39)
(40)
(41)
.
To evaluate this possibility, we simulated choices based on an extreme version of the experiential neglect rule: if an S-option has positive expected value, choose it, otherwise choose the E-option. These simulations show that, applied to the decision problems of the ES phase from experiments 1-to-6, extreme experiential neglect still generates 77% of expected-value maximizing choices. This result is actually not as counterintuitive as it initially appears: by design, a positive lottery is the most advantageous option in ≥50% of the decision problems in which it appears, and the converse is true for the negative expected value lotteries. These considerations suggest that, instead of representing an intrinsic cognitive limitation of value-based decision-making, the experiential value neglect is a rational heuristic strategy deployed by efficient (or lazy) decision-makers maximizing an accuracy-effort trade-off 
(42)
(43)
(44)
(45)
.
In order to test this new interpretation of the results, we designed a new experiment (Exp.


7)
where we reorganized E-and S-options probabilities in a way that makes neglecting experiential values economically disadvantageous 
(Fig. 4A)
. In this new configuration, the narrower range of S-option values are nested within the broader E-option values, so that any given S-option has a higher expected value compared to the 4 negative Eoptions, and a lower expected value compared to the 4 positive E-options. Such configuration guarantees that participants neglecting E-option values in the ES-phase will exhibit a chance-level choice accuracy (50% of expected value maximizing choices).
Except for the modification of the lotteries, Exp. 6 present the exact number of trials.
Despite this stronger economic incentive, the behavioral pattern in ES-phase remained consistent with the experiential value neglect scenario 
(Fig. 4B)
. The significant difference between ES and EE slopes persisted in Exp. 7 (T(70)=5.12, P<0.001), suggesting that despite the reorganization of probabilities, we were still able to elicit more accurate E-option values from EE-choices 
(Fig.4F, Fig. 4G
). As a consequence, compared to Exp. 6, the accuracy in the ES-choices significantly dropped in Exp. 7 by approximately 20% (T(94.97)=11.01, P < .001, 
Fig 4C)
. Of note, the accuracy in the EEchoices remained the same 
(Fig. 4D, Fig. 4E)
, with no significant difference between the two experiments (T(131.77)=0.38, P=1, BF¹⁰=0.19).
These findings indicate that experience values are neglected even when it involves an (economic) cost. Therefore, the results are consistent with the idea that the experiential value neglect reflects a hard-coded feature of hybrid choices between experiential and symbolic option, rather than being strategically deployed by the relative lack of incentive in Exp. 1-6.


Controlling for ambiguity aversion
E-options may be deemed more ambiguous, because their outcome probability distributions are inferred from finite samples and cannot been known with absolute precision or certainty. Experiential value neglect cannot be accounted by a simple form of ambiguity aversion 
(46)
(47)
(48)
, because E-options are generally preferred compared to negative expected value S-options (i.e., there is no systematic bias against E-options).
Nonetheless, to assess whether the participant's attitude toward ambiguous lotteries differed between experiential and symbolic options in a final experiment we included choices with ambiguous lotteries (i.e., lotteries, whose value was hidden). The results (presented in the Supplementary Materials and 
Figure S1
) indicate that ambiguity aversion was not detectable in our set up and that it could therefore not contribute to explain the observed pattern of behavior. The results of Exp. 8 also replicate all previously reported findings. 


Reaction times analysis: a tale of two systems?
Choice behavior differ across the ES-and the EE-choices. In the ES-phase, participants neglect the experiential option value and to make choices only based on the symbolic option value, so that, if the S-option is positive, it is chosen, otherwise it is rejected 
(Fig.   5A
). On the other hand, EE-choices are based on the retrieval from memory of the experiential values of both options. Thus, one decision process (ES-choices) seems to involve the processing and representation of only one option value (the lottery), while the other process (EE-choices) seems to involve the processing and the representation of two option values. We hypothesized that these different processes translate into different reaction times between the two choice modalities. To test this prediction, we compared the reaction times in EE and ES-choices, while including only decisions with similar objective value difference 
(49)
. Indeed, we found that ES decisions were faster compared to EE decisions, both when the S-option is chosen -(ESs) and when the E-option is chosen -(ESe) (T(136)=6.02, P < 0.001; T(136)=3.98, P < 0.001; 
Fig. 5B and Fig. 5C
).
Of note, within ES decisions, ESe choices were also slightly but significantly slower the ESs choices (~50ms; T(136)=4.35, P < 0.001), which may indicate that choosing the Eoption requires additional processing to retrieve and represent the value of the E-option.
To confirm this intuition, we considered two categories of ES-choices: choices exclusively consistent with the participant choosing using the estimates inferred from the LE phase, on one side, and choices consistent with a full experiential value neglect, on the other side 
(Fig. S5)
. We observed that, in conformity with previous results, ES-choices that are consistent with a full experiential value neglect are significantly faster than choices that can only be explained taking into account the E-option values estimated from the LEphase (T(386)= 2.27, P<0.05) 
(Fig. 5D)
. Overall, the RT analyses support the idea that We suggest two not-mutually exclusive explanations. One possibility is that experiential value estimates are perceived as less precise. Note here that precision represents the uncertainty about the value estimate itself 
(48)
. Indeed, assuming imperfect memory storage and retrieval, it is conceivable that experiential values are less precise compared to symbolic ones that can be perfectly calculated 
(77)
. According to this interpretation, participants would quasi-systemically prioritize the more precise source of information for their choices 
(47,
48,
78)
. Another possibility is that participants prefer discarding experiential information not to incur the cost associated with the cost of memory retrieval 
(79,
80)
. Reaction times analysis was overall consistent with this idea, because choices involving the processing of the experiential values were generally slower compared to those involving symbolic ones, even if balanced in objective difficulty 
(49)
. This latter interpretation leaves open the possibility that if one makes memory retrieval less costly, the behavioral pattern could be reversed (i.e., we would witness symbolic value neglect). This could be possible for example after extensive training, once experience-based choices are routinized 
(81)
 or, conversely, by making symbolic information harder to decode. These are interesting possibilities to be explored by future studies.
Finally, we speculate on the possible cognitive mechanisms underlying the experiential value neglect phenomenon and we identify two plausible candidates. The first mechanism involves 'bottom-up' attentional processes. It is well-documented that attentional focus biases evidence accumulation in value based decision-making 
(82,
83)
. It is therefore conceivable that an attentional bias toward symbolic options may result in prioritizing described information and neglecting experiential one. The second possible mechanism involves a 'top-down' heuristic process, according to which the calculation of individual option values is hijacked by a deterministic decision rules 
(44)
. Of note, even if we managed to demonstrate experiential value neglect in situations where it is disadvantageous (experiment 7), it can nonetheless be argued that this decision rule is overall adaptive, because computationally cheap and satisfying in most situations (see experiments 1-6).
To conclude, our results add to the collection of behavioral anomalies showing that values representations are inherently dependent on the way they are built, as it is postulated by the 'construction of preference' framework 
(84,
14,
85)
 


Methods and supplementary results
In this document we present the methods, as well as some additional results, including those issued from an experiment (Exp. 8), which is only briefly mentioned in the main text.


Experimental participants
In total, we tested 787 participants (430 females; aged 31.09±10.  
Table 1
). Of note, none of the results presented in the main or supplemental text was affected by the exclusion of the participants. 
Table S1
. Experiments parameters. The 'Exp.' column refers to the experiment number. The 'Outcome (LE)' column refers to the outcomes displayed during a single LE phase trial. The column can take two values: partial (only obtained outcome) or complete (both obtained and forgone outcomes). The 'Structure (LE)' column refers to how the presentation of the options (or decision problems) was organized in the LE phase. 'Blocked' correspond to the case in which all trials belonging to a given option pair are presented in a row.
Otherwise, when options pairs are distributed randomly, the value is set to 'interleaved'. The 'Decision problems (LE)' column refers to the number of option pairs presented in the LE phase. The 'Phases' column, refers to the specific phases present in a particular experiment. 'LE' refers to the learning phase. 'ES' stands for Experiential-Symbolic phase. 'EE' stands for Experiential-Experiential phase (performed after learning with no feedback). 'SP' stands for Stated Probability phase. 'EA/SA' stands for Experiential-Ambiguous and Symbolic-Ambiguous. The 'Sessions' column provides the number of sessions, i.e., how many times we repeated the sequence of phases with a different set of E-options. The 'N' column refers to the number of participants included in the experiment after exclusion of those displaying >100% correct response rate in the ES catch trials.
The research was carried out following the principles and guidelines for experiments including human participants provided in the declaration of Helsinki (1964, revised in 2013). The INSERM Ethical Committee approved the study and participants provided written informed consent prior to their inclusion. To sustain motivation throughout the experiment, the tasks were economically incentivized. Specifically, in addition to a showup fee, participants were initially endowed with £2.5, and according to their choices, they could reach a maximum £5. The conversation rate was around 1pt = 1 cent and they were explained that all points won across the different phases were summed up. The average final bonus was £4.05 ± 0.72, which was significantly higher compared to what they would have got in average following random choices (T(615) = 52.58, P < 0.001).


Data availability
The analysis was performed using Matlab R2021a. The code and data for the analysis is available here: https://github.com/bsgarcia/RetrieveAndCompareAnalysis.


Behavioral protocol
The different experiments were conducted on a website programmed in javascript, html and css (code: https://github.com/bsgarcia/RetrieveAndCompare, testing: https://humanrl.scicog.fr/RandCTesting).


Initial learning phase (LE phase)
Participants first performed a probabilistic instrumental learning task (LE). Participants were provided with written instructions explaining that the aim of the task was to maximize Regarding feedback, there were two settings: partial and complete. A partial feedback setting implied that only the outcome of the chosen option (or cue) was displayed, while complete feedback means that both outcomes were displayed, regardless of the choice. 


Hybrid choices between experiential and symbolic values (ES phase)
This phase is present in all the experiments.
After the LE phase, E-options were presented against symbolic cues (S-options). Soptions were implemented as pie-charts, where the green part indicates the probability to win a point, and the red part indicates the probability to lose a point. Each E-option 
(8)
 involved in the LE phase was presented against 11 S-options (for a total of 88 trials), with probability of winning (and respectively loosing) a point ranging from 0% to 100%, with a 10% step. On each trial, one pair was randomly presented with one cue on right and left side of the screen. Participants were required to select, without time-limit, between the two cues by left-clicking. After the choice, the selected cue was highlighted with a black border and the transition to the next trial, lasted approximately 1000 ms. No feedback was presented during the ES phase. Participants were informed about their earnings only at the end.
Although the outcome was not displayed, participants were told that this phase was still incentivized, such that choice accuracy affected their bonus compensation.


Assessing generalization of experiential values (EE phase)
This phase is present in Experiment 5 to 8. After the LE phase, each E-option was presented against other E-options. With 8 cues presented in the LE phase, it follows that each E-option was presented against the other 7 E-options, so that this phase contained 56 trials. EE choices were presented in the same time as the ES choices, because we wanted to avoid having them differ in terms of time elapsed since the LE phase. Thus, technically the EE and the ES phases overlap.
For each trial, one pair was randomly presented with one cue on the right and left sides of the screen. Participants were required to select, without time-limit, between the two cues by left-clicking. After the choice, the selected cue was highlighted with a black border and the transition to the next trial, lasted approximately 1000 ms. The transition effect lasted approximately 1000 ms and leave place for the next trial. No feedback was presented.
Although the outcome was not displayed, participants were told that they could still win (and lose) points during this phase, this phase was still incentivized, such that choice accuracy affected their bonus compensation.


Stated Probability assessment (SP phase)
In all experiments, participants were asked, for each E-option previously faced in the LE phase, the following question «What are the odds this symbol gives a +1?». They had to provide their answer on rating-scale, going from 0% to 100% with a 5% step.
Answers were incentivized via a matching probability procedure that is based on the In other words, the higher the response (p) of the participant, the higher the chances were the outcome would be determined by the E-option. Conversely, the lower the response (p), the higher the chances were that the outcome would be determined by the random lottery number (r).


Ambiguity assessment
Preference towards ambiguous lotteries was assessed only in Experiment 8. After the LE phase, E-options as well as S-options were presented against an ambiguous cue. This ambiguous cue was represented by a greyed pie-chart, with a question mark on top.
Consequently, it was represented similarly to S-options, i.e., as a lottery, which was however 100% ambiguous in the sense that it conveys no a priori information regarding probabilities of gains or losses (see 
Figure S1)
. Each E-option 
(8)
 and S-options (8), were presented against this ambiguous cue two times, resulting in a total of 32 trials. For each trial, one pair was randomly presented with one cue on the right and left sides of the screen. Participants were required to select, without time-limit, between the two cues by left-clicking. After the choice, the selected cue was highlighted with a black border while a transition effect was activated. The transition effect lasted approximately 1000 ms and left room for the next trial. No feedback was presented.
Although the outcome was not displayed, participants were told that they still could win (and lose) points during this phase, and that correct choices (i.e., choices maximizing expected value) and wrong choices would thus affect their bonus compensation.


Statistical and computational modeling Inferential statistics
All t-tests were realized using Python 3.9 and the pairwise_ttests function from the pingouin library. Bonferroni's corrections were applied systematically. Linear regressions were realized using Matlab R2020a fitlm function.


E-option probabilities inference in ES and EE phases
To infer a probability estimate (or indifference point) for each E-option from EE and ES choices we proceeded as follows. In those phases, an E-option was assessed relatively to other cues (either S-options, either other E-options). In the ES phase 11 S-options were presented against each E-option. In the EE phase 7 E-options were presented against each E-option. Choosing the E-option that was currently assessed is always coded as 1,
whereas choosing the cue presented against (an S-option in ES, or an E-option in EE) is always coded as 0.
We note ∈ {0, 1} the choice of a participant at trial . Thus, for each E-option we obtain a vector of choices = ( 1 , 2 , 3 , … ) , with = 11 in the ES phase, and = 7
in the EE phase. We then fit the following logistic function (2, 3):
( ) = 1 1 + ( − )
With > 0 (which controls the slope of the function) being a free parameter unique to each individual , while ∈ [0, 1] (the function midpoint) is a free parameter that is estimated for each E-option and individual . The indifference point represents here the probability where a preference shift (from one cue to another) occurs, and is thus a subjective probability (or value) estimate for the E-option and participant . Both parameters were estimated through minimum negative log-likelihood estimation, using matlab's fmincon function.
With > 0 being the temperature parameter, that implements choice stochasticity. As decreases, the events of choosing or tend to become equi-probable. As increases, the difference between ( , ) and ( , ) is amplified, and the choice becomes more and more deterministic (until the function almost acts as an argmax policy).


Model fitting
Learning rate and temperature parameters (here denoted ) involved in the reinforcement learning model were estimated by finding values that minimized the negative logarithm of the posterior probability over the free parameters (− ( ( | )), which was computed as follows:
− ( ( | )) ∝ − ( ( | )) − ( ( ))
Where ( | ) is likelihood of the data (i.e., the observed choices during the LE phase)
given certain parameter values, and ( ) is the prior probability of those parameter values.
The prior probability distribution over the learning rates was assumed as beta distributed and quasi-uniform (betapdf(1.1, 1.1)). The softmax temperature was for its part assumed to be gamma distributed (gampdf(1.2, 5)).
The optimization procedure was again performed using Matlab's fmincon function and previously described in Lebreton et al., 2019.


Parameter and choice recovery in EE and ES phases
To quantify and statistically compare the differences in preferences observed in the ES and EE phase, we estimated for each E-option its theoretical subjective value (expressed in terms of probability of winning a point). This value is itself inferred in term of indifference points. For instance, in ES choices, one E-option with 80% chance of winning will be compared to range of S-options (going from 0% to 100% chance of winning a point). The indifference point for the E-option considered is then the S-option value at which a preference shift occurs between the two kinds of options (let's say, when the S-option is above 80%, considering the decision-maker is rational). To infer those indifference points, we fitted a logistic function to each subject choice history for each E-options in both EE and ES phases (see the methods section). We treated these indifference points as proxies for subjective values, i.e., E-option value estimates (or probability estimates, as in the numerical space considered they are equivalent).
To assert that this fitting procedure is robust, and that we do not elicit random subjective values, we followed a parameter recovery procedure 
(7)
.
We simulated EE and ES choices based on the (EE and ES) E-option value estimated from experiment 1 to 6.
More precisely, for each subject, we simulated an agent going through its choice history, and we used the 8 inferred estimates (one for each E-option, each subject having its own 8 indifferent points) to simulate new choices.
We generate these choices using an argmax decision rule, meaning that the agent systematically selects the option with the highest value. Of note, in the simulated ES phase, we do not suppose any subjective deformation regarding S-options, such that the agent is directly informed of the objective expected-value to make its decision.
At this point of the procedure, simulated choices can be compared to choices from behavioral data. By doing so, we can see how well they match, and therefore whether our value estimates allow us to correctly recover the choices actually made by our participants. EE choices are recovered up to 83%, whereas ES choices are almost perfectly recovered, with a score of 96%. The fact that EE choices are less recovered is not surprising, as the E-option estimates results from the comparison of one option against 7 others, when in the ES phase an E-option is presented against a wider range of alternative options (11), hence allowing better precision in the fitting of E-option value estimates.
We then generate new E-option value estimates, by applying our initial logistic fitting procedure (see methods section) to this newly simulated data. We observe that E-option value estimates are almost perfectly recovered, both in the ES 
(Fig. S2)
 and EE 
(Fig. S3)
 phase, with a spearman that is systematically higher than .97. .


Fig. S2 Recovery of E-option estimated probabilities in Experiments 1-to-6, ES phase.
We estimate 8 E-option value for each subject in the ES phase. Thereafter, going through each individual choice history, we simulate a new choice dataset using these value estimates as an input for an argmax decision rule. We apply our logistic fitting procedure again (see the methods section) on this simulated data, to generate new estimates. Then we run a spearman correlation to test the relationship between the estimates from the behavioral data and the estimates from the simulated data. The grey dotted line corresponds to a perfect recovery of E-option probability estimates. 
Fig S3.
 Recovery of E-option estimated probabilities in Experiments 1-to-6, EE phase. We estimate 8 E-option value for each subject in the EE phase. Thereafter, going through each individual choice history, we simulate a new choice dataset using these value estimates as input for an argmax decision rule. We apply our logistic fitting procedure again (see the methods section) on this simulated data, to generate new estimates. Then we run a spearman correlation to test the relationship between the estimates from the behavioral data and the estimates from the simulated data. The grey dotted line corresponds to a perfect recovery of E-option probability estimates.


The impassable gap between experiential and symbolic values
Garcia et al.


Supplementary results


Experiment 8
We devised Experiment 8 to test whether the behavioral pattern observed in the Thus, we presented each E-option ( 
Fig. S1A: top)
 and S-option 
(Fig. S1A: bottom)
 against one ambiguous option (A-option), represented by a greyed pie-chart which conveyed no a priori information. Interestingly, the indifference point inferred for the Aoption was close to 50% (both when the A-option is presented against E-and A-options).
It suggests that without a priori information, participants associate a 50% subjective probability of winning a point to the A-option. When presented against E-options ( 
Fig.   S1B: top)
, the A-option is preferred against E-option which probability of winning a point is inferior to 50%, which suggests that those options are remembered as giving a negative expected-value. The preference is reversed when the E-option probability is above 50%,
showing that participants associate those options to positive expected-values. When presented against S-options ( 
Fig. S1B: bottom)
 Of note, E-options cannot be conflated with A-options for two reasons. First, when presented against A-options, E-options choice frequency increases monotonically with their associated objective probabilities. This result suggests that E-options are robustly linked to past outcome information, when it comes to comparing them to ambiguous stimuli.
Second, the experiential neglect pattern cannot be the result of pure ambiguity aversion, as E-options are in average preferred against S-options when the latter has a negative expected-value, regardless of the E-option value. It suggests that this preference for known risks only holds in the gain domain, which excludes a pure preference toward known risks, i.e., a pure ambiguity aversion. Of note, introducing A-options among the other post-learning assessments did not affect the previously observed relation in inferred slopes 
(Fig. S1C)
. LE-inferred slopes were consistently significantly higher than ES slopes (Exp. 


Choice profiling among Experiment 1-to-8
We classified ES-choices in different categories as a function of being explained exclusively either by a full E-value neglect, by E-option estimates elicited in the LE phase, by both, or finally by none of them 
(Fig. S5)
.
In order to do so, we ran two simulations for each experiment.
In the first simulation, for each subject, we simulate an agent that is confronted with the history of decision problems that the real subject was facing. This artificial agent makes decisions according to the following experiential neglect decision rule: If the S-option is above 50% chance of winning a point, choose the S-option, otherwise choose the Eoption. This behavior is what we name experiential neglect, because the values of the Eoptions are not even considered by the decision-maker.
In the second simulation, we also simulate an agent that is confronted with the history of decision problems that the real subject was facing. However, this agent has access to the E-option value estimates (specific to the subject in question) that were inferred from the LE-phase through our Q-learning model fitting procedure (see methods). We do not assume any deformation regarding the perception of S-option probabilities and rewards.
Consequently, we assume that the agent uses an argmax rule (i.e., systematically choosing the highest value), to decide between the (subjective) E-option estimates and the S-option objective expected-value.
With this simulated choice dataset, we can compute the proportion of choices from our behavioral data that match with each simulation.
We observe that most of the choices can be both explained by LE estimates and the extreme experiential neglect decision rule (see main text). Yet, the number of choices
The impassable gap between experiential and symbolic values 
Garcia et al.
 exclusively explained (or predicted) by the experiential neglect rule is significantly higher than the number of choices explained by LE estimates (T(598)=13.87, P<0.001).
Of note, the Exp. 7, due to its particular configuration of probabilities among E-and Soptions, seemingly allows to discriminate better between the two decision models, as the number of choices explained by both decision rules. 
Fig S5.
 Choices prediction from Experiments 1-to-8. We run 2 simulations. In the first one, we assume that all participants make use of an experiential neglect decision rule, which basically consists in choosing the S-option as long as it is higher than 50% chance of winning a point, and otherwise choose the E-option.
The second one consists in simulating all choices while assuming that participants use experiential values from the LE phase (i.e., the ones we inferred through our Q-learning model), therefore "LE estimates".
Thereafter we compute the proportion of choices that are explained by each simulation, i.e., the proportion of behavioral choice that are identical to simulated choices. Choices explained by experiential neglect are in red. Choices explained by inferred experiential values from the LE phase are in dark blue. Choices explained by both experiential values and experiential neglect are in grey. Choices explained by none of them are in black.
Fig 1 .
1
Behavioral tasks, hypotheses, option values and experimental protocol. (A) The leftmost panel displays successive screens of a typical trials in the learning phase (LE). The LE-phase consists in a twoarmed bandit task with fixed (4 or 2in Exp. 4) pairs of abstract cues (E-options) and contained 120 trials.


Fig 2 .
2
Raw behavioral results and inferred option values in Experiments 1-to-4. (A) Correct choice rate grouped per learning context in the LE phase, where '40/60' designated the hardest decision problem, '10/90' the easiest decision problem. The dark blue line indicates the mean, the mid-dark blue indicates the standard mean error, and the light blue indicates a 95% confidence interval. The dotted line indicates chance (or random) responding (50%). (B) Average probability of choosing an E-option over a S-option during ES phase. The color of the curves indicates the value of the E-option (lowest: light orange; highest:


. 2A, Exp. 1). Apart from the most difficult learning context (60/40), choice accuracy was above chance level for all E-option pairs (T(75)=1.5, P>.05; T(75)=10.98, P<0.001), thus indicating that participants aimed at (and managed to) maximize expected value. Furthermore, accuracy was modulated by the difference in expected value (i.e., the decision value) of the E-option pair. Choice accuracy increased as a function of the decision value (β=0.077, T(300)=2.16, P < 0.05; β=0.08, T(300)=2.35, P < 0.05; β=0.21, T(300)=5.94, P < 0.001), thus indicating that participants' behavior was sensitive to the specific EV of E-options involved in a given pair.


Fig. 2C: left). Thus, ES decision problems feature a specific neglect of E-option values, as if hybrid choices prioritized the value of the symbolic options over an unbiased comparison of experiential and symbolic values, thereby confirming the experiential neglect hypothesis.


3 : 2 :
32
increased significantly across experiments (Exp. 2: β=0.11, T(942)=5.98, P=0.055; Exp. β=0.28, T(942)=6.5, P < 0.001; ; Exp. 4: β=0.31, T(942)=7.27, P < 0.001). Critically, the ES slopes were not modulated across experiments aside from Exp. 4 (Exp. 2: β=-0.1, T(942)=-1.76, P=0.07; Exp. 3: β=0.02, T(942)=6.5, P=0.67; ; Exp. 4: β=0.11, T(942)=2.06, P < 0.05) (Fig. 2D). Overall, LE-inferred slopes were significantly higher than the ES slopes in all experiments (Exp. 2: T(70)=11.74, P < 0.001; Exp. 3: T(82)=15.8, P < 0.001; Exp. 4: T(87)=11.64, P < 0.001; Fig. 2E), and the asymmetric effects of the manipulations on the LE versus ES phases translated into a significant interaction between the choice modality (ES and LE) and the experiment number (Exp. β=-0.21, T(942)=-2.58, P<0.05; Exp. 3: β=-0.26, T(942)=-3.29, P<0.01; ; Exp. 4: β=0.2, T(942)=2.57, P < 0.05).


2, 3 , 4 (
34
Exp. 1: T(75)=2.62, P>0.05; Exp. 2: T(70)=3.42, P<0.05; Exp. 3: T(82)=4.38, P<0.001, Exp. 4: T(87)=4.87, P<0.001). Therefore, E-options' values elicited during the SP phase were more accurate than those elicited in the preceding ES-phase. This observation rules out forgetting as a plausible interpretation of the apparent experiential value neglect pattern observed in the ES phase.


Fig 3 .
3
Raw behavioral results and inferred option values in Experiments 5-to-6. (A) Average probability of choosing an E-option over a S-option during ES phase. The color of the curves indicates the value of the E-option (lowest: light orange; highest: dark orange). Dots represent the empirical indifference points, the value of a lottery that correspond to a probability of choosing the symbol 50% of the times. Exp. 6.1 and Exp. 6.2 refers to the first and the second session, respectively. (B) Average probability of choosing an E-option over another E-option during EE phase. The color of the curves indicates the value of the Eoption (lowest: light green; highest: dark green). Dots represent the empirical indifference points, the value of a lottery that corresponds to a probability of choosing the symbol 50% of the times. (C) The panels represent for each symbol the inferred value (as expressed by the probability of winning; p(win)) as a function of the actual value. ES estimates are represented in orange and EE estimates in green. In the data-boxes, the dark tone line represents the mean, mid-dark tone the standard mean error, light tone a 95% confidence interval. The lines represent linear regression (dark tone), and the average standard mean error (light tone). (D) Comparison of individual inferred slopes obtained from linear fit (see Fig. 3C) in two modalities (ES and EE in orange and green, respectively). The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area represents probability density functions. ***p<0.001 two sample t-test. EE-choices curves revealed that participants were capable of successfully extrapolating the value of the E-options to new decision problems involving other E-options. On the other side, the ES-choices were consistent with experiential values neglect, thus replicating the previous experiments (of note, the LE-phase of Exp. 5 and Exp. 6


Fig 4 .
4
Option values and behavioral results in Experiment 7. (A) The panel shows and compare the options value in Exp .1-6 to that of Exp 7. In Exp. 7, we reorganized E-options and S-options values such that half of the E-options have higher expected-values than all S-options and, conversely the other half have lower expected-values. In such an arrangement, a participant fully neglecting the E-options values in the ES phase will end up with random choices in respect to utility maximization (B) Average probability of choosing an E-option over a S-option during ES phase. The color of the curves indicates the value of the E-option (lowest: light orange; highest: dark orange). Dots represent the empirical indifference points, the value of a lottery that correspond to a probability of choosing the symbol 50% of the times. (C) Expected value maximizing (i.e., correct) choices in the ES phase of Exp. 6 compared to Exp. 7. The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area probability represents density functions. ***p<0.001 two-sample t-test. (D) Average probability of choosing an E-option over another E-option during EE phase. The color of the curves indicates the value of the E-option (lowest: light green; highest: dark green). Dots represent the empirical indifference points, the value of a lottery that correspond to a probability of choosing the symbol 50% of the times. (E) Expected value maximizing (i.e., correct) choices in the EE phase of Exp. 6 compared to Exp. 7. The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area probability density functions. ***p<0.001 two-sample t-test. (F) The panel represents for each symbol the inferred value (as expressed by the probability of winning; p(win)) as a function of the actual value. ES estimates are represented in orange and EE estimates in green. In the data-boxes, the dark tone line represents the mean, mid-dark tone the standard mean error, light tone a 95% confidence interval. The lines represent linear regression (dark tone), and the average standard mean error (light tone). (G) Comparison of individual inferred slopes obtained from linear fit (see Fig. 4F) in two modalities (ES and EE; in orange and green, respectively). The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area probability represents density functions. ***p<0.001 paired two-sample t-test. Experiential value neglect persists even when it bears an economic cost Analysis of choice behavior in the ES show that learned values of the E-options are largely neglected, as if participants were deciding on the basis of the value of the S-options only, and this despite the fact performance in the LE, SP and EE-choices indicate that E-option values are well learned and memorized. Neglecting experiential values seems, at least prima facie, suboptimal for the decision process, as taking into account all relevant


Fig. 5
5
Hypothetical decision model and reaction times analyses (A)The panel presents a schematic representation of the decision process in the EE-and the ES-phases, respectively. The two processes differ in that in the former case (EE) the decision is based by retrieving the values of both options, while in the latter case (ES), under an extreme form of experiential value neglect, only the value of the lottery matters. (B) Median reaction times across modalities. EE decisions are significantly longer than ES decisions (regardless of the choice taken in ES). When comparing when an S-option is chosen (ESs) andwhen an E-option is chosen (ESe) we also observed a significant difference. The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area probability density functions. (C) Different in reaction times differences (ESe -ESs in orange; EE -ESs in green). In the data-boxes, the dark tone line represents the mean, mid-dark tone the standard mean error, light tone a 95% confidence interval. (D) Reaction times as a function of whether the ES-choices could be only explained by a total neglect of the experiential value (red) or whether they could only be explained by experiential values estimated from the learning phase (dark blue). In the data-boxes, the dark tone line represents the mean, mid-dark tone the standard mean error, light tone a 95% confidence interval. *p<0.05, **p<0.01, ***p<0.001 paired two-sample t-test.


choices based on the S-values of the lotteries required reduced cognitive processing compared to those involving the retrieving from memory. Thus, E-values inferred from ES-choices are consistent with the dual process model of Fig. 5A.DiscussionOur results clearly indicate that the experiential and symbolic option values are not treated symmetrically when making hybrid choices and speak against the idea of a central valuation system that encodes option values in a common currency, regardless of the way they are built(3,
12)
. The key finding supporting this claim is provided by the analysis of hybrid decision problems between experiential and symbolic cues, where choices appeared to be made by largely neglecting value information acquired during the learningphase. Crucially, by running several experiments and including multiple control measures, we ruled out several alternative explanations for the experiential value neglect: this decision-making pattern is not due to insufficient learning, forgetting, generalization issue, or a lack of incentive. Finally, reaction time analyses are consistent with different processing of experiential and symbolic values and with the idea of an additional cognitive cost associated with the memory retrieval of learned values. It seems that past experiences and symbolic descriptions of possible outcomes ultimately generate value representations different enough to make them largely incommensurable and that the tension between the two is resolved by overweighting (or prioritizing) symbolic information. In the following paragraphs we try to provide plausible reasons why these values representations radically differ, why symbolic information is favored in hybrid choices and which cognitive mechanisms could underlie the behavioral pattern observed. Symbolic descriptions of lotteries in our task (and in general) involve separate information about at least two different features of outcomes: payoffs (i.e., the amount of reward to be won or lost) and their probability (50). Models of decision-making designed to explain behavior in this kind of paradigms frequently assume that probability and payoffs are processed individually. For instance, in prospect theory and its extensions, different subjective weighting functions are supposed to apply to these variables (51-53, 14, 54). A separate representation of payoffs and probabilities is also assumed by models that do not suppose the calculation of a multiplicative expected utility (55) and by models supposing that decisions are underpinned by feature-by-feature comparisons (56-60). On the contrary, experience-based choices, as instantiated by simple reinforcement learning tasks, are usually modeled assuming that the decision-makers represents a unique numeric value for each state-action pair. The decision-maker can 'look-up' in this value matrix before making their choice and, once an outcome is obtained it partially overwrites the 'cached' values previously stored in memory, so that they approximate the average outcome (61). Option value representation is therefore structurally very different from that of description-based choices, because the relevant features (payoffs and probabilities) are never explicitly represented as separate attributes of the outcomes. Furthermore, some authors even suggest that reinforcement-based choices may bypass the calculation of reward-based option-specific values, and is underpinned by what is called direct policy learning (62-65). Our results seem to reject an extremely orthodox interpretation of direct policy learning (accuracy in the learning phase was sensitive to the value difference between options and experiential values were successfully generalized to new combinations). It is nonetheless plausible to conceive that -at least to some extent -reinforcement-based decisions involve a value-free (policy-based) component that can be hardly compared with the subjective extracted from explicit payoffs and probabilities. Functional neuroimaging investigations of experiential and symbolic decision-making may also shed light on the debate about value representation across modalities. While functional meta-analyses identified overlapping correlates of experiential and symbolic values (17-20), the putative neural mechanisms of reinforcement-based and descriptionbased decisions differ in many crucial respects. First of all, the most influential and consensual neural models of reinforcement-based leaning and decision-making give a preponderant role to dopamine-induced neural plasticity circuits (66-68). More specifically dopamine-dependent plasticity is supposed to drive action selection by shaping the strength of the synapses between the frontal cortex and the basal ganglia (69, 70). Current neural models do not attribute to dopamine-driven processes and the basal ganglia a prominent role in description-based choices. Rather, they suppose that the decision process is solved by cortical circuits (71-74), following an evidence accumulation process similar to that observed for perceptual decisions (75, 76). Thus, structural differences in the neural mechanisms of choices across modalities may represent a biologically grounded bases of the representational difference between experiential and symbolic values. The representational tension of hybrid choices is solved by participants by neglecting the experiential values and basing their choices on the symbolic value. Several control analyses allowed us to formally exclude the possibility that this effect merely arise from insufficient knowledge of the experiential values. Why is the symbolic information preferred?


.
More specifically, our findings pose serious challenges to the default assumption that values representations are shared across different decision-making modalities, traditionally referred to as experience-and description-based. The incommensurability between experiential and symbolic values results in behaving as if discarding acquired information and consequently entails suboptimal decisions. These findings are worth exploring outside the experimental setting because many real-life decisions involve a tension between an experiential and a symbolic component.


42 years) distributed across seven experiments. Participants were recruited via Prolific, a platform dedicated to online research participants recruitment (https://prolific.co/). To assess participants' engagement in the different tasks and their understanding of probability representation, we inserted catch trials consisting in choices between two lotteries (S-options), with one of the two cues being obviously better in terms of expected value maximization. In all analyses we only retained the participants displaying 100% of correct choices in these catch trials. In total 599 participants were included. Experiment 1 to 7 included the following numbers of participants: 76,71, 83, 88, 71, 66, 71, 73 (see


their payoff by seeking monetary rewards and avoiding monetary losses. From experiment 1 to 5, participants performed only one learning session. Experiment 6, 7 and 8 for their part include 2 learning sessions. From experiment 1 to 7, each learning session contained four pairs of experiential cues (E-options), apart from experiment 4, which contained 2 (but featured proportionally twice more trials). Each pair was fixed, so that a given cue was always presented against the same other cue. Thus, within learning sessions, pairs of cues represented stable choice contexts. Within each pair, the two cues were associated to two outcomes; either winning a point (+1) either losing one (-1). The four (two in experiment 4) cue pairs corresponded to four contexts of varying difficulty, indexed by the difference in the probability of winning a point between the two cues. On each trial, one pair was randomly presented with one cue on the right and the other on left side of the screen. Participants were required to select, without time-limit, between the two cues by left-clicking. After the choice, the selected cue was highlighted with a black border while a transition effect was activated. The transition effect lasted approximately 1000 ms and revealed the outcome of the choice. The outcome was then displayed during approximately 1500 ms. In experiments 1, 2, 3, 5, 6 and 7, the four pair of cues were presented 30 times each, for a total of 120 trials within sessions. In experiment 4, the two pairs were presented 60 times each, to maintain an identical number of trials. In experiment 1, pairs of cues were presented in an interleaved manner, meaning they were distributed randomly across the 120 trials. From experiment 2 to 7, pairs were presented in a blocked manner, meaning they were stacked in sequences of 30 choices.


Experiment 1 and 2
2
involved partial feedback. From experiment 3 on, feedback was set to complete.


Matching Probability Mechanism(1). More precisely, participant chose a probability (p) for the presented E-option. A number (r) is then randomly drawn in the interval [0 1]. If p > r, the outcome of the choice was obtained using the E-option probability of winning and losing a point (as-if the E-option was chosen in the LE phase for instance). Otherwise, if p < r, the participant has r (%) chance of winning a point, and respectively 1-r (%) chance of losing a point.


Experiential-
Symbolic phase was the result of ambiguity aversion (8), i.e. that the participant have a preference for options with known probability distributions over option with unknown probability distributions. In other words, participants would neglect experiential expected-values estimated during the LE phase because they are reluctant toward ambiguous options (E-options) and consequently mainly rely on options actually providing full probabilistic information (S-options).


Fig. S1
S1
Raw behavioral results and inferred option values in Experiments 8. (A) The topmost panel displays successive screens of a typical trial in the Experiential-Ambiguous (EA) phase. The bottommost panel displays successive screens of a typical trials in the Symbolic-Ambiguous (SA) phase. The EA-phase consists in binary choices between a symbol previous encountered in the LE-phase, and an ambiguous lottery (materialized as greyed pie-chart with a question mark on top). The SA-phase consists in binary choices between an explicit lottery (materialized as a pie-chart partly green for gain probabilities, and partly red for loss probabilities) and an ambiguous lottery (materialized as greyed pie-chart with a question mark on top). (B) Average probability of choosing an ambiguous option (A-option) over a E-option (top) or an Soption (bottom) during the ambiguity phase. Dots represent the empirical choice frequency of the A-option. The largest dot at the intersection of the grey dotted line represents the indifference point, i.e., when the subject chooses randomly between the two options. The error bars represent the standard error of the mean. (C) Comparison of individual inferred slopes obtained from linear fit in three modalities (LE, ES and EE in blue, orange and green, respectively). The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area represents the probability density function. ***p<0.001 paired sample t-tests.


8 . 1 : 8 .
818
T(72)=13.05, P < 0.001; Exp Fig S4. Inferred option values in Experiments 5-to-Comparison of individual inferred slopes obtained from linear fit in the 4 modalities (LE, ES, EE and SP in blue, orange, green and purple, respectively). The black lines represent mean and standard error of the mean. The colored boxes represent 95% confidence interval. The shaded area represents the probability density function. ***p<0.001 paired sample t-tests.


The impassable gap between experiential and symbolic values Garcia et al.4   


The impassable gap between experiential and symbolic valuesGarcia et al.   








Acknowledgements
The authors thank Aurélien Baillon for helpful comments. SP is supported by the Institut de Recherche en Santé Publique (IRESP, grant number : 20II138-00), the Agence The impassable gap between experiential and symbolic values Garcia et al.   






Inferring E-option value estimated in the LE phase
To infer E-option values in the learning (LE) phase, we fitted a reinforcement learning model (or Q-learning model) to our data 
(4,
5)
. 
 










A Note on the Pure Theory of Consumer's Behaviour




P
A
Samuelson








Economica




5


















J
Von Neumann






O
Morgenstern




Theory of games and economic behavior


Princeton, NJ, US




Princeton University Press








Theory of games and economic behavior








A framework for studying the neurobiology of value-based decision making




A
Rangel






C
Camerer






P
R
Montague








Nat. Rev. Neurosci




9
















Relative and absolute strength of response as a function of frequency of reinforcement




R
J
Herrnstein








J. Exp. Anal. Behav




4


267
















B
F
Skinner




Science and human behavior




Simon and Schuster














Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








MIT press












Exposition of a New Theory on the Measurement of Risk




D
Bernoulli








Econometrica




22
















Essai philosophique sur les probabilités (H. Remy, 1829)




P.-S
Laplace














An axiomatization of cumulative prospect theory




P
Wakker






A
Tversky








J. Risk Uncertain




7


















D
Kahneman






A
Tversky




Handbook of the fundamentals of financial decision making: Part I




World Scientific
















Frames, biases, and rational decision-making in the human brain




B
De
Martino






D
Kumaran






B
Seymour






R
J
Dolan








Science




313


















P
W
Glimcher




Foundations of neuroeconomic analysis


USA




OUP














A review essay about Foundations of Neuroeconomic Analysis by Paul Glimcher




C
F
Camerer








J. Econ. Lit




51
















Does the brain calculate value?




I
Vlaev






N
Chater






N
Stewart






G
D A
Brown








Trends Cogn. Sci




15
















EPS Prize Lecture: Decision by sampling: The role of the decision environment in risky choice




N
Stewart








Q. J. Exp. Psychol




62
















From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience




I
Erev






E
Ert






O
Plonsky






D
Cohen






O
Cohen








Psychol. Rev




124


369














The valuation system: a coordinate-based metaanalysis of BOLD fMRI experiments examining neural correlates of subjective value




O
Bartra






J
T
Mcguire






J
W
Kable








Neuroimage




76
















Prediction error in reinforcement learning: a metaanalysis of neuroimaging studies




J
Garrison






B
Erdeniz






J
Done








Neurosci. Biobehav. Rev




37
















Informatic parcellation of the network involved in the computation of subjective value




J
A
Clithero






A
Rangel








Soc. Cogn. Affect. Neurosci




9
















Separate neural representations of prediction error valence and surprise: Evidence from an fMRI meta-analysis




E
Fouragnan






C
Retzler






M
G
Philiastides








Hum. Brain Mapp




39
















The description-experience gap in risky choice




R
Hertwig






I
Erev








Trends Cogn. Sci




13
















The role of memory in distinguishing risky decisions from experience and description




C
R
Madan






E
A
Ludvig






M
L
Spetch








Q. J. Exp. Psychol




70
















A meta-analytic review of two modes of learning and the description-experience gap




D
U
Wulff






M
Mergenthaler-Canseco






R
Hertwig








Psychol. Bull




144


140














The description-experience gap: a challenge for the neuroeconomics of decision-making under uncertainty




B
Garcia






F
Cerrotti






S
Palminteri








Philos. Trans. R. Soc. B




376


20190665














How (in)variant are subjective representations of described and experienced risk and rewards? Cognition




D
Kellen






T
Pachur






R
Hertwig








157














A choice prediction competition: Choices from experience and from description




I
Erev






E
Ert






A
E
Roth






E
Haruvy






S
M
Herzog






R
Hau






R
Hertwig






T
Stewart






R
West






C
Lebiere








J. Behav. Decis. Mak




23
















Differentiable Neural Substrates for Learned and Described Value and Risk




T
H B
Fitzgerald






B
Seymour






D
R
Bach






R
J
Dolan








Curr. Biol




20
















The description-experience gap in risky choice in nonhuman primates




S
R
Heilbronner






B
Y
Hayden








Psychon. Bull. Rev




23
















Intrasubject comparison of four response modes for "subjective probability" assessment




W
M
Ducharme






M
L
Donnell








Organ. Behav. Hum. Perform




10
















A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement




R
A
Rescorla








Curr. Res. Theory


















Learning the value of information in an uncertain world




T
E J
Behrens






M
W
Woolrich






M
E
Walton






M
F S
Rushworth








Nat. Neurosci




10
















The Tortoise and the Hare: Interactions between Reinforcement Learning and Working Memory




A
G E
Collins








J. Cogn. Neurosci




30
















Contextual modulation of value signals in reward and punishment learning




S
Palminteri






M
Khamassi






M
Joffily






G
Coricelli








Nat. Commun




6
















Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning




S
Bavard






A
Rustichini






S
Palminteri








Sci. Adv




7


340












Measuring utility by a single-response sequential method




G
M
Becker






M
H
Degroot






J
Marschak








Behav. Sci




9
















SSL: A Theory of How People Learn to Select Strategies




J
Rieskamp






P
E
Otto








J. Exp. Psychol. Gen




135
















The foundations of statistics




L
J
Savage








Courier Corporation












Information Processing and Bounded Rationality: A Survey




B
L
Lipman








Can. J. Econ. Rev. Can. Econ




28


















V
M
Chase






R
Hertwig






G
Gigerenzer








Visions of rationality






2














Heuristic decision making




G
Gigerenzer






W
Gaissmaier








Annu. Rev. Psychol




62


















B
Mackowiak






F
Matejka






M
Wiederholt




Rational inattention: A review
















Theories of bounded rationality




H
A
Simon








Decis. Organ




1
















Human problem solving: The state of the theory in 1970




H
A
Simon






A
Newell








Am. Psychol




26


145
















G
E
Gigerenzer






R
E
Hertwig






T
E
Pachur




Heuristics: The foundations of adaptive behavior




Oxford University Press














Administrative behavior




H
A
Simon








Simon and Schuster












Risk, Ambiguity, and the Savage Axioms




D
Ellsberg








Q. J. Econ




75
















Ambiguity and rationality




D
Frisch






J
Baron








J. Behav. Decis. Mak




1
















Recent developments in modeling preferences: Uncertainty and ambiguity




C
Camerer






M
Weber








J. Risk Uncertain




5
















Rethinking fast and slow based on a critique of reaction-time reverse inference




I
Krajbich






B
Bartling






T
Hare






E
Fehr








Nat. Commun




6


7455














Risk aversion and incentive effects




C
A
Holt






S
K
Laury








Am. Econ. Rev




92
















Prospect theory: An analysis of decision under risk




A
Tversky






D
Kahneman








Econometrica




47
















Advances in prospect theory: Cumulative representation of uncertainty




A
Tversky






D
Kahneman








J. Risk Uncertain




5
















The Probability Weighting Function




D
Prelec








Econometrica




66


497














Generalized expected utility theory: The rank-dependent model




J
Quiggin








Springer Science & Business Media












Computational models of adaptive behavior and prefrontal cortex




A
Soltani






E
Koechlin








Neuropsychopharmacology


















Eye movements in reading and information processing: 20 years of research




K
Rayner








Psychol. Bull




124


372














An eye-tracking study on information processing in risky decisions: Evidence for compensatory strategies based on automatic processes




A
Glöckner






A.-K
Herbold








J. Behav. Decis. Mak




24
















The Dynamics of Decision Making in Risky Choice: An Eye-Tracking Analysis




S
Fiedler






A
Glöckner








Front. Psychol




3


335














An overall probability of winning heuristic for complex risky decisions: Choice and eye fixation evidence




V
Venkatraman






J
W
Payne






S
A
Huettel








Organ. Behav. Hum. Decis. Process




125
















It's not what you see but how you see it: Using eyetracking to study the risky decision-making process




J
A
Aimone






S
Ball






B
King-Casas








J. Neurosci. Psychol. Econ




9
















Learning to predict by the methods of temporal differences




R
S
Sutton








Mach. Learn




3


















P
Dayan






L
F
Abbott








Theoretical neuroscience: computational and mathematical modeling of neural systems
















Signals in Human Striatum Are Appropriate for Policy Update Rather than Value Prediction




J
Li






N
D
Daw








J. Neurosci




31
















The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)




B
Y
Hayden






Y
Niv








Behav. Neurosci




135


192














Value-free reinforcement learning: policy optimization as a minimal model of operant behavior




D
Bennett






Y
Niv






A
J
Langdon








Curr. Opin. Behav. Sci




41
















Cross-task individual differences in error processing: neural, electrophysiological, and genetic components




M
J
Frank






C
D'lauro






T
Curran








Cogn. Affect. Behav. Neurosci




7
















Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive




A
G
Collins






M
J
Frank








Psychol. Rev




121


337














Learning the payoffs and costs of actions




M
Möller






R
Bogacz








PLOS Comput. Biol




15


1006285














The basal ganglia: a vertebrate solution to the selection problem?




P
Redgrave






T
J
Prescott






K
Gurney








Neuroscience




89
















Stepping out of the box: information processing in the neural networks of the basal ganglia




I
Bar-Gad






H
Bergman








Curr. Opin. Neurobiol




11
















Mechanisms underlying cortical activity during value-guided choice




L
T
Hunt






N
Kolling






A
Soltani






M
W
Woolrich






M
F
Rushworth






T
E
Behrens








Nat. Neurosci




15
















A neuro-computational model of economic decisions




A
Rustichini






C
Padoa-Schioppa








J. Neurophysiol




114
















Orbitofrontal cortex: a neural circuit for economic decisions




C
Padoa-Schioppa






K
E
Conen








Neuron




96
















Metaplasticity as a Neural Substrate for Adaptive Learning and Choice under Uncertainty




S
Farashahi






C
H
Donahue






P
Khorsand






H
Seo






D
Lee






A
Soltani








Neuron




94


6














Neural Activity in Macaque Parietal Cortex Reflects Temporal Integration of Visual Motion Signals during Perceptual Decision Making




A
C
Huk






M
N
Shadlen








J. Neurosci




25
















A neurocomputational model of altruistic choice and its implications




C
A
Hutcherson






B
Bushong






A
Rangel








Neuron




87
















Imprecise neural computations as a source of adaptive behaviour in volatile environments




C
Findling






N
Chopin






E
Koechlin








Nat. Hum. Behav




5
















Preferences for information precision under ambiguity




R
Bricet












THEMA (THéorie Economique, Modélisation et Applications), Université de …












Costs of Memory: Ideas and Predictions




R
Dukas








J. Theor. Biol




197
















A Model of Costly Recall




H
Afrouzi






S
Kwon






Y
Ma












Columbia University






working paper








Habits without values




K
J
Miller






A
Shenhav






E
A
Ludvig








Psychol. Rev




126


292














Visual fixations and the computation and comparison of value in simple choice




I
Krajbich






C
Armel






A
Rangel








Nat. Neurosci




13
















Visual attention modulates the integration of goal-relevant evidence and not value. eLife




P
Sepulveda






M
Usher






N
Davies






A
A
Benson






P
Ortoleva






B
De
Martino








9


60705














S
Lichtenstein






P
Slovic




The construction of preference




Cambridge University Press














The case against economic values in the brain




B
Hayden






Y
Niv




10.31234/osf.io/7hgup
















The choice axiom after twenty years




R
D
Luce








J. Math. Psychol




15
















Individual choice behavior: A theoretical analysis




R
D
Luce








Courier Corporation












Contextual influence on confidence judgments in human reinforcement learning




M
Lebreton






K
Bacily






S
Palminteri






J
B
Engelmann








PLOS Comput. Biol




15


1006973














Ten simple rules for the computational modeling of behavioral data. eLife




R
C
Wilson






A
G
Collins








8


49547












EE-inferred slopes for their part were systematically higher than ES slopes


P < 0.001; Exp 8.2: T(72)=6.38, P < 0.001






) as well as EE slopes (Exp. 8.1: T(72)=5.9






Exp. 8.1: T(72)=5.78, P < 0.001; Exp 8.2: T(72)=4.61, P < 0.001








From experiments 5-to-8 (Fig. S4


LE slopes were consistently and significantly higher than ES slopes






Exp. 5: T(70)=12.94, P<0.001; Exp. 6.2: T(65)=10.59, P<0.001; Exp. 7.2: T(70)=14.4, P<0.001;Exp. 8.2: T(72)=10.41, P<0.001), EE slopes (Exp. 5








2: T(72)=6.38, P<0.001


as well as SP






Exp. 5: T(70)=8.98, P<0.001; Exp. 6.2: T(65)=6.18, P<0.001; Exp. 7.2: T(70)=10.88, P<0.001;Exp. 8.2: T(72)=7.71, P<0.001








SP slopes for their part, are systematically higher than ES slopes


2: T(72)=1.12






P=1). Exp. 5: T(70)=4.05, P<0.01; Exp. 6.2: T(65)=5.34, P<0.001; Exp. 7.2: T(70)=4.8, P<0.001;Exp. 8.2: T(72)=4.62, P<0.001), designating the ES values as the lowest








Intrasubject comparison of four response modes for "subjective probability" assessment




W
M
Ducharme






M
L
Donnell








Organ. Behav. Hum. Perform




10
















The choice axiom after twenty years




R
D
Luce








J. Math. Psychol




15
















Individual choice behavior: A theoretical analysis




R
D
Luce








Courier Corporation












Learning the value of information in an uncertain world




T
E J
Behrens






M
W
Woolrich






M
E
Walton






M
F S
Rushworth








Nat. Neurosci




10
















Imprecise neural computations as a source of adaptive behaviour in volatile environments




C
Findling






N
Chopin






E
Koechlin








Nat. Hum. Behav




5
















Contextual influence on confidence judgments in human reinforcement learning




M
Lebreton






K
Bacily






S
Palminteri






J
B
Engelmann








PLOS Comput. Biol




15


1006973














Ten simple rules for the computational modeling of behavioral data. eLife




R
C
Wilson






A
G
Collins








8


49547












Ambiguity and rationality




D
Frisch






J
Baron








J. Behav. Decis. Mak




1

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]