You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Decisions under uncertainty of outcomes (see 
Bach & Dolan, 2012;
LaValle, 2006)
 is widely studied in economics 
(Allais, 1953;
Güth et al., 1982;
Kahneman et al., 1982)
 and cognitive neuroscience 
(Bach & Dolan, 2012;
Hayden & Niv, 2021;
Klein-Flügge et al., 2022)
. In doing so, the majority of studies models decision-making with binary choice paradigms, where choice options are presented simultaneously. By valuating the choice options, an agent (human or animal) makes decisions and the values given to the available options can then be modeled based on the observed behavior (see 
Hayden & Niv, 2021)
. Foraging theory, however, models behavior from the perspective that choice options appear sequentially in the majority of cases. The crux with this kind of decisions is that future options depend on the outcome of the current choices. The decision-making agent must then decide to accept or reject the appearing option at a given instance in time (see 
Hayden & Moreno-Bote, 2018;
Yoo et al., 2021)
. Since most choices in nature follow this sequential framework, the underlying paradigm is arguably more ecologically valid than binary choice.
This reject-accept decision paradigm can be compared to the approach-avoidance conflict (AAC). The AAC represents a fundamental framework in biology and behavioral ecology to optimize choice and has been frequently studied in psychology to model anxiety in animal research (see 
La-Vu et al., 2020)
. There are a number of successful attempts to reverse-translate these paradigms to human experiments (see 
Aupperle et al., 2015)
. Notably, the threat-anticipatory freezing reaction and the related bradycardia are associated with information gathering and action preparation in animals and humans (see 
Livermore et al., 2021)
. Based on the ubiquity of the AAC in making survival relevant choices in nature, it is reasonable to assume that the underlying decision-making process is essential for the cognitive functioning underlying our behavior under naturalistic conditions. Therefore, this study aims to model human behavior under sequential AAC conditions. Since the underlying processes have been shaped and optimized over the course of evolution, this line of research fol-lows the call for more consideration of the evolutionary context of the emergence of behavior and the nervous system in systems neuroscience 
(Mobbs et al., 2018)
.
Sequential choice includes conflicts between interests (or trade-offs) at different time-points. This entails the problem of inconsistencies with changing preferences over time (see 
Hammond, 1976;
Lotito, 2022;
McClennen, 1990;
Strotz, 1956)
. For example, one may decide to eat a snack out of an urge, but would decide against when considering the time pressure due to an appointment in the near future. In this case, the temporal dimension of a sequential decision problem creates a trade-off. This, in turn, brings up the question of how people choose their decision rules (called policies) in order to make trade-off choices. The selection of choice rules (also called policies) concerns the topic of meta-decision making (see 
Boureau et al., 2015)
. The temporal context of future trade-offs in sequential choice can either be considered or not, which shapes the current policy of an agent. If dynamic changes in trade-offs are considered over multiple time-points, the choice strategy would be more sophisticated and so more likely to be optimal with respect to the agent's outcome.
If an agent would only focus on her preference of the current state, the behavior would be more myopic/naive (see 
Lotito, 2022)
. To date, it is little understood how people approach trade-offs in sequential choice paradigms on a meta-decision making level (choosing between different policies).
Furthermore, the AAC is an exemplary trade-off with widespread appearance in nature. Therefore, it is reasonable to assume that the AAC has an impact on the policy selection. Due to this, we are interested in how people change between different policies under approach and avoidance conditions in a sequential task. To study this, we developed a probabilistic foraging game (played on the computer) in which people are prompted to approach or avoid certain dangers in the environment in order to optimize their behavioral outcomes. Our task design allows us to identify different policies that are either more sophisticated or more naive. Via conventional model comparisons, we can identify eventual changes in the range of choice sophistication based on our task's conditions (approach or avoid danger).
Our task consists of a single-agent game allowing a unique optimal solution due to the Markov property, which states that future options only depend on the current choice, not the past (see 
Sutten & Barto, 2018)
. By formulating a sequential decision-making paradigm as a Markov Decision Process (MDP), the cumulative sum of the expected future reward can be computed via dynamic programming. By these means, we can test a choice model that accounts for all aspects of the entire temporal sequence of the decision process. 
Figure 1
 illustrates the MDP algorithm in detail.
A problem with MDP-based models is that these algorithms are generally domain specific, while humans often perform effectively in new tasks after just a few of observations 
(Lee et al., 2014;
Wu et al., 2018)
. This is striking, given the limited resource of computational power rendered by the human brain 
(Juechems et al., 2021)
. This limited cognitive capacity brings up an additional aspect about optimization, namely resource or computational rationality: meta-decision making under the constraint of lowering energetic costs for computing a policy 
(Gershman et al., 2015;
Lieder & Griffiths, 2020)
. In order to achieve this, a decision tree can be truncated into binarized or graded rules of thumb. Policies of this kind are called heuristics: a reduction of a complex task concerning the evaluation of outcome probabilities by predicting values of simpler judgments (Tversky & Kah-


Figure 1: Illustration of Markov Decision Process (MDP) computation with backwards induction (or dynamic programming). A) Transition graph
between energy states for two actions: w stands for action "waiting" and f stands for action "foraging". Since the goal of the task was to "stay alive", a reward of (-1) was given for transitioning to an energy state of 0 (in red), which is absorbing. Transitions to all other energy states were associated with a reward of 0. B) Illustration of backwards induction with corresponding transition matrix. Value updates take place from last to first time-points ("days") in order to compute the state-action values. Values are computed by the dot product of the transitions vecctor (probabilities to transit between energy states) and reward vector (containing the rewards of the states). 
neman, 1974)
. There is evidence suggesting heuristics-based (or evolutionary) algorithms offer a scalable alternative to MDP-based solutions with sufficient performance and lower computational costs 
(Salimans et al., 2017)
. An agent that employs this kind of strategy may under-perform in problems where the precise optimal solution is computable, but show advantages in complex realworld scenarios where the solution space becomes intractable. In other words, it may be beneficial to approximate a solution with a more suboptimal/myopic consideration than to overthink a problem when optimization is unfeasible.
In a sequential choice context, the optimal policy (computed via MDP) and heuristics incorporate different meta-decision making approaches. While MDP-based methods emphasize different decision variables in different states, heuristics rely on preferences regarding one aspect of a state (such as "is the weather good or bad"). The reduction to one situational aspect of a choice is more naive or myopic in the sense that it does not include any temporal variability of preferences (see 
Lotito, 2022)
. Our task is designed to compare choice models related to naive/heuristic or sophisticated/MDP-based meta-decision strategies.
In summary, we elaborated two main meta-decision strategies: An MDP-based and a heuristics-based approach. Both strategies have different advantages. While the former offers the maximal solution, the latter puts itself above with its scalability and data efficiency. Based on former work, we have reason to believe that humans take advantage of both choice strategies 
(Korn & Bach, 2015
, 2019
. In the present paper, we are interested in exploring the extent to which people rely more on the one or the other approach when faced with difficult environments. To do so, we adopt the virtual hunter-gatherer task from former work (see 
Korn & Bach, 2018
, 2019
, in which participants can influence their survival in a game-like set-up in the following way: Over the course of several time-points, participants have to decide whether they want to risk searching for food (also called foraging) or wait for better conditions. The expenses of energetic resources of the foraging option could be higher than for waiting, since foraging had a certain probability to fail. Additionally, participants could also encounter predators (virtual agents) when foraging, which would cause even Participants were in a forest with a good and a bad weather type (lower and higher chance of finding food). Foraging outcome is probabilistic with hunting success, hunting failure and predator encounter as possible outcomes. The number of food fields or predators in a weather type divided by the total number of fields results in the probability of gaining food and the risk of a predator encounter. The probability of success in a certain weather type results from the probability of food gain corrected for the joint risk of a predator encounter (p success = p gain × (1 -r predator)). The goal of the game was to "stay alive" (prevent discrete energy bar from depleting to zero) over 8 discrete time-points ("days") in order to get a monetary reward. Note: the days were not displayed, so participants had to track them by themselves. A total of 72 forests were played per participants. The probability of foraging gain and the risk of a predator encounter were selected carefully to create 2 experimental conditions (38 forests each): in one condition the probability of success suggests approaching gains (approach forests), while in the other condition the probability of success suggests avoiding predators (avoidance forests). Since higher probability to gain food coincides with higher risk of a predator encounter, the two policies were contradicting at all times. The figure depicts an example of an avoidance forest since the probability of success is higher in the weather type without predator.
higher energetic costs. To test whether choice strategies are influenced by principles of the AAC, we included different levels of competition threat by the predators. While in one experimental condition, participants should approach higher predator risks due to higher probabilities of gaining food, they should avoid higher predator risks in the other condition. Our task design allows scaling choice models with divergent signature behaviors differently and bit them against one another via quantitative model comparison. 
Figure 2
 illustrates the details of our experimental task with all relevant features.


Results


Data recording
We recorded a total of 29 subjects (14 male, 15 female; age = 23.93 ± 3.73) without any history of psychiatric conditions. The total number of possible trials per participant was 576 (72 forests times 8 days). The average probability of surviving a forest was 0.22 ± 2.25. Of all responses, 0.02 ± 2.69 were non-responses (due to exceeding the time limit), which were excluded from the data when conducting the model comparison. This left us with an average number of 361.45 ± 23.22 trials per participant (min = 292, max = 411), summing to a total of 10270 decisions. Participants had an mean probability to survive a forest until the end (after 8 days) of μ = 0.11 with a s = 2.7. There was no significant difference of the survival rates between the two experimental conditions (approach vs. avoidance forests; t = -0.89, p = 0.38).


Model comparisons
Our task allowed a number of heuristic models derived from the task variables. Heuristic models concern decision variables of the task with binary or graded judgmental value operations.
Simple examples of these models are the weather type, the probability of food gain, or the risk of a predator attack. There are numerous ways of recombining decision variables to models of higher complexity. The expected value, for example, is the probability-weighted average of all possible values. Heuristic policies of this kind represent models of higher judgmental operation, since they integrate multiple variables to produce one value. The probability of success also represents such an intermediate model, since its values account for the join risk of a predator encounter (p success = p gain × (1 -r predator)). Since heuristic preferences can change over the course of time in a myopic approach, we further created a so-called multi-heuristic policy that accounts for significant preference changes related to energy states. Since the cost for choosing to wait is one energy point, participants have to forage in order to have any chance of survival in the game (see 
Table 1
, binary energy state BES). Similarly, if the energy points exceed the number of trials left in a forest, the game can be won by simply waiting the rest of the time (see 
Table 1
, wait when safe WWS). Our multiheuristic policy accounts for these preferences in the respective states, while otherwise choosing according to the probability of success. All behavioral models including example values for the forest depicted in 
Figure 2
 are elaborated in 
Table 1
. Weather type In each forest, the grids representing weather types can be categorized into "good" weather with higher probability of foraging success and "bad" weather with lower probability of success.
Categorical: 0 "bad" or 1 "good" 0 for example with threat, 1 for example without threat (since corrected success probability is higher)
Binary energy state (BES)
Participants must forage in energy state 1. Therefore, this variable is binarized as 1 if the energy state is 1, and 0 for all the other energy states.
Binary: 1 or 0 0 (since waiting does not lead to starvation)
Wait when safe (WWS)
If energy state is higher than days left in a forest, participants can simply wait in order to win Binary: 1 or 0 0 if only 3 days left, otherwise 1 the game. Therefore, this variable is 0 if energy state is higher than days left, and 1 for all other energy states. The number of days in a forest was not displayed and had to be tracked by participants themselves.
Win-stay-lose-shift (WSLS)
Possible strategy that uses the outcome of the former trial to decide in the current trial. This policy chooses foraging if the energy state increased and waiting if it decreased in the former trial.
Categorical: 1 "energy state increased" or 0 "energy state decreased"
Not available since it depends on previous trial in the context.
Bayesian model inference suggests that the multi-heuristic policy performs the best (PEP = 0.98) over the entire data set ( 
Figure 3A)
. Our multi-heuristic model was composed of the three best performing heuristics (p success, binary energy state and wait when safe), which we validated via a decision tree. When comparing the multi-heuristic policy with the optimal policy values qualitatively via their model fits ( 
Figure 3B)
, we see that the optimal policy values underfit when participants are in energy state 1 and foraging becomes a forced choice. This can be seen in 
Figure 3C
 and D showing the actual responses and the predicted responses of the optimal policy value model for BES and WWS situations. Due to this, we were motivated to generate a capped optimal policy value model, in which we "capped" states where the BES and WWS models eliminate uncertainty due to the nature of the task (e.g. if you wait in energy state 1, you have no chance of surviving the current forest). Refer to 
Table 1
 (OP values + cap) for the specifics of our capping procedure. To prevent the multi-heuristic policy winning the model comparison for the trivial reason of capturing non-trade-off situations without ambiguity, we include this additional model in our further model comparisons. With respect to our entire data set, we can conclude that people generally seem to apply a more heuristics-based/myopic meta-decision strategy in our task.
To see if the meta-decision strategy changes in response to an approach-avoidance conflict, we conducted model comparisons for our experimental conditions (approach and avoidance forests)
separately. The results suggest that the capped optimal policy values significantly gains importance in the avoidance forests (see 
Figure 4A
 and B). The PEP results even show a slight preference of the capped optimal policy values (PEP = 0.47) compared to the multi-heuristic policy (PEP = 0.45) in avoidance forests. This means that there are inter-individual differences between participants, with some showing more sophisticated decision-making in this condition than others. In an additional step, we tested whether the beta coefficients (β 0 and β 1 ) from fitting our optimal policy values model via maximum likelihood estimation (see Methods section 4. 0.91, p = 0.36). This suggests that participants generally do behave more optimally in avoidance forests. 
Figure 4D
 shows the model fits of the two leading models for the entire data set.
Next, we tested whether the survival rate (No. of survived forests) in our task can be predicted by the slope (β 1 coefficient) of the OP values + cap model. This serves as confirmation that the stronger use of the optimal policy by participants indeed shows better choices reflected by the task's outcome (No. of survived forests). 
Figure 4C
 shows the results of this analysis. It can be seen that there is a positive correlation (r 2 = 0.67, t = 7.51, p = 0) between β 1 coefficients (slope of the logistic model) and the success rate in the task. Since the optimal policy fit seems predictive for participants' task performances, we can assume that the quality of choices generally improved in avoidance forests.


Validation of the multi-heuristic policy
As a proof of concept for the multi-heuristic policy, we ran a linear regression of the uncertainty of the model predictions for participants' log transformed response times (see Methods section 5.8 Response time analysis). If the respective model applies well for peoples' choices, re-  sponses should get slower when decisions are more uncertain/harder. We tested our multi-heuristic policy with a linear regression between uncertainty values and log-transformed response times and found a significant association (z = 26.86, p = 0). The results of this analysis are illustrated qualitatively in 
Figure 5A
 and B. Note that BES states (multi-heuristic policy bins = 1) generally show higher response times than WWS states (multi-heuristic policy bins = 1). This means that BES choices were generally harder to make for participants compared to WWS decisions.
To further validate our multi-heuristic policy, we extracted a decision tree based on features generated of our heuristic variables (see Methods section 5.9 Decision tree). Since, by default, a decision tree is generated from binary features, some decision variables had to be binarized via mean split in order to perform this analysis. A feature that underwent such a transformation would suggest foraging when the current environment's value would be higher than the mean, and waiting otherwise. The hereby engineered features were then used for our decision tree analysis, which calculates the relative importance of a feature to explain the data. In the process, features are reduced ("pruned") to a statistically meaningful amount to avoid overfitting of the generated decision tree model. The branches of the resulting tree hierarchically classify the importance of the features in the nodes (with the root node on top representing the most important feature). By these means, we gen- erated an interpretable proxy for the heuristic considerations underlying participants' choices when preferences change in different states. As depicted in 
Figure 5C
, the branches of the resulting decision tree support our multi-heuristic policy with the three relevant features BES, WWS and p success remaining after pruning.


Discussion
Overall, our findings suggest the multi-heuristic policy to be a good model in the present data set. This suggests that participants generally use a heuristics-based/myopic meta-decision strategy.
However, optimal behavior becomes more likely in avoidance forests. Therefore, predator avoidance in our task increases the chance for better choices. This interpretation is supported by the model comparisons of the two task conditions (approach and avoidance forests), as well as, the significant interaction in our logistic mixed effects model between the capped optimal policy values and our task conditions. Furthermore, we found a positive correlation between the slope of the optimal policy values (capped) and task performance, suggesting that the quality of choices increases with higher evidence for the optimal policy value model. We suspect that the salience of the avoidance policy prompts participants to integrate a transition model comparable to the optimal policy in their choices. Since this observation is mainly based on the PEP results, there must be inter-individual differences in the degree to which choices improve in avoidance forests.
It remains unclear why people tend to use simpler decision rules in approach forests. We hypothesize that this effect may be rooted in the internal trade-off between efficiency and accuracy of a choice model. The affordance competition hypothesis 
(Cisek, 2007)
 states that action scripts are processed directly from sensory inputs and filtered by simultaneously perceived biases for policies.
Concerning this, heuristics could be understood as some sort of inductive biases (see 
Goyal & Bengio, 2022)
 to increase efficiency in processing decision relevant information. In this sense, heuristics serve to behave according to computational/resource rationality (see 
Gershman et al., 2015;
 Lieder & Griffiths, 2020) by reducing energetic expenses of policy the computation if the prospect will not significantly increase by higher efforts. In our task, since the energetic cost of maximizing a sequential choice problem is relatively high, it may be beneficial for organisms to minimize this effort by finding an easy and sufficient (heuristic) solution.
When an external stimulus triggers the relative importance of aversive stimuli, like in our task's avoidance forests, paying the energetic cost for computing a better policy becomes highly relevant in order to survive. 
(Gershman et al., 2015;
Lieder & Griffiths, 2020)
 Regarding this, we suspect there to be a specialized neural system (like, e.g., the brain's salience network) for detecting salient risks in the environment that prompts up-scaling the complexity of a meta-decision strategy.
Neuroimaging experiments paired with computational modeling of behavior are needed to test this.
Furthermore, the inter-individual differences in the degree of sophistication evoked by our task's avoidance forests could be explained by a latent factor such as intelligence. It is a promising path for the continuation of this work to test for correlations between beta coefficients of the optimal policy values and measurements for cognitive capacities like, e.g., fluid intelligence scores.
Another aspect of our task concerns the differentiation between so-called model-based vs.
model-free systems for learning and decision-making (see 
Drummond & Niv, 2020)
. In this regard, we suggest that the avoidance forests in our task cause an increased usage of model-based decisionmaking. For such deliberate/model-based behavior to take place, known aspects of the environment have to be exploited in order to generate a transition model of the environment (see 
Collins & Shenhav, 2022)
. From our data it is unclear when the transition model is established, but people tend to include related considerations more in avoidance forests. We consider this as evidence for the specialization of human decision-making functions to the demands of sequential choice. Concerning this, it can be hypothesized that this specialization is responsible for seemingly paradoxical signature choices observed in behavioral economics, such as the Allais paradox (see 
Allais, 1953)
 or loss aversion (see 
(Tversky & Kahneman, 1992)
. Further research is required to bridge our behavioral observations with algorithmic and physiological implementations of decision-making to test this idea.
Based on the here presented results, we assume that heuristic approximation and the generation of meta-strategic protocols based on these heuristics play important roles in sequential choice.
Such meta-strategic protocols can be imagined as more or less complex decision trees, where the number of nodes (or the hierarchical structure) defines the sophistication of behavior. The complexity of a meta-decision strategy may rely on the external level of demand. While strategy efficiency is generally preferred to save energy, more complex and sophisticated choice strategies rise under increased environmental risks. Evidence suggests that the way we approach and resolve sequential decision-making conflicts may strongly relate to aspects of mental functioning and psychological health (see 
Bavolar & Bacikova-Sleskova, 2020)
. Therefore, the investigation of policy selection, implementation and switching in ecologically valid, sequential set-ups with competing interests is of fundamental interest to advance the scientific understanding of mental health, as well as, improve psychopathology prevention and treatment.


Methods


Participants
We tested 29 healthy participants (14 male, 15 female; age = 23.93 ± 3.73) in the lab. Recruitment was done via fliers and online tenders. Before recording, participants were asked whether they had any history of psychiatric or neurological diagnoses, which would have represented an exclusion criterion. None of the invited subjects were excluded. Ethics approval was given by the local ethics committee of the medical faculty of the University of Heidelberg.


The hunter-gatherer task with predators
The here presented task was based on the so-called "Hunter-Gatherer Game" from 
Korn and Bach (2018)
. In its original form, the task constituted different environments (called forests) with two weather types each. Participants had to "survive" (prevent discrete energy levels/life points from depleting to zero) over a number of time-points (called days) in a forest in order to gain a monetary incentive. Every trial (called day) in the game, one of the weather types was selected to represent the current weather. The weather conditions of the day entailed information about the probability to find food. Participants then had to decide whether they would like to hunt/search for food (also called "foraging") or wait for better conditions. The outcome of the foraging option was probabilistic. The probabilities (to successfully find food or not) were fully derivable from the visual representation of the weather types. A trade-off was included by surely losing a low amount of energy (one energy points) when waiting, but potentially losing a higher amount (two energy points) when foraging without success. If the foraging outcome was successful, participants could gain one or two energy points.
Similar to Korn and Bach 2019, the crucial additional feature in the current version of the experiment was that we included some predators/threats. The probability of encountering a predator was also random and could be derived from the visual representation of the weather types. If subjects foraged and encountered a predator, they would lose a significant amount of energy (three lifepoints) potentially being fatal in the game. This should mimic a situation where energy expenses are increased due to a fight or flight reaction of the subject.
In all forests, the weather type with a higher probability of finding food also had a higher risk of encountering a predator. This created an approach-avoidance conflict. In some forests, it was better to maximize the probability of success, while in other forests it was better to minimize the risk of a threat encounter. 
Figure 2
 illustrates the task's visual representation. In the following, mathematical details about the task design that are relevant for the behavioral modeling will be addressed.


Mathematical details of the task
In a game-like environment, Participants had to go out and find food in order to survive in foraging environments called "forest". Each forest had two so-called "weather types". These were abstract representations of the probability of gaining food when foraging p, the probability of foraging failure q = 1-p and the risk of encountering a predator r. A weather type consisted of a grid with a number of fields (ranging from 2 to maximum fields). Each field had an equal probability to be the location after the hunt. If the field contained food, the hunt was successful. The ratio between the food containing fields and the total number of fields, thus, corresponded to p. Hereby, we set the minimum p to 0.2 to avoid trials with close to zero chance of finding food. The values for p were drawn from the closed interval [0.2, 0.67].
A predator had the same condition to end up on any of the grid fields as the participants.
Therefore, r represented the joint probability of landing on the same field, which was sampled from the interval [0, 0.67]. Similarly to p, the values for r were the result of the design choice of our virtual environment: the ration between the number of threats in a forest (0, 1 or 2) and the number of fields (min. 2, max. 10) determined the probability to encounter a predator in the current weather.
Since r ≠ 1, the maximum risk of threat encounter was 0.67 (3 fields with 2 predators). In order to balance p and r, we limited the probability to gain food to maximum 0.67. Due to the joint probability of finding food and encountering a predator, the true (or safe) probability of foraging success was p s = p*(1-r). Correspondingly, the true probability of unsuccessful foraging without encountering a predator was q s = (1-p)*(1-r) = q*(1-r).
In total, we had 72 forests. The forests were split into 4 experimental blocks with 18 forests each. Before the actual task, participants went through a training session with 10 forests. Participants played inside the same forest for 8 consecutive time-points t (called "days"). On each day, one of the weather types was randomly drawn (0.5 probability). If the final location after foraging contained food, the participant would yield an energy gain g (+1 or +2 energy points depicted as green dots). If a participant ended on a field without food, the foraging cost c f of -2 energy points was paid. Some weather types (not all) contained predators. If a predator was encountered, the predator cost c p of -3 energy points was deducted.
An essential aspect of the experiment was to test meta-decision strategies in two types of environments: approach and avoidance forests (experimental conditions with 36 forests each). In approach forests, the number of predators in each weather type would range from 0 to 2, with six forests having zero predators in one of the weather types. These forests were characterized with a higher probability of success p s in the weather containing a higher number of predators, for which participants should "approach" competitors to maximize success. In avoidance forests, one weather type contained 2, while the other weather type contained 0 predators. Participants should generally avoid foraging in the weather condition containing predators in order to maximize the probability of success p s in these forests.
To "stay alive" in the present forest, participants had to keep their discrete energy level from depleting to 0. An incentive of 0.50 € was provided, which participants could win if they survived until the end of a forest. If an energy state of 0 was reached, the current trial was stopped and the participant was prompted to the next forest prematurely without completing all possible days. There was no incentive for higher energy states, the only goal was to "stay alive" (prevent energy from depleting to 0). The initial energy level on the first day of a forest was randomly drawn between 4 and 5. The energy levels were capped at a maximum of 6 discrete points. This gives us a total of 7 possible energy levels (0 to 6). Since there were 2 weather types with equal probability of occurring, the total number of possible states inside a forest was 14 (7 energy states × 2 weather types). This finite set of states with their corresponding transition probabilities fulfills all conditions for a fully observable Markov Decision Process. This allows to numerically derive the state-action values by dynamic programming, where the highest action value among the available options represents the best choice. In other words, the optimal policy dictates when it is best to forage and when to wait. The values underlying this optimal policy can be used as a model to fit responses via a logistic link function that captures the uncertainty of the model in a given environment. This allows a more finegrained model fit regarding the noise term of the link function than with a hard optimal policy. For a more detailed description of this model, see section 5.4 Computation of the optimal policy values.
Our experimental paradigm is built around the idea of trade-off choices. If the transition probabilities in the weather types of a forest are too similar to each other, the trade-off disappears and the preference becomes indifferent/random. In order to avoid indifferent choices, we required that the (hard) optimal policy should suggest foraging more in one of the weather types. Therefore, we took the absolute difference of foraging preferences of the optimal policy d f over all states and timepoints in the two weather types and filtered out forests with d f < 10.


Computation of the optimal policy values
The experimental set-up allowed to derive the optimal policy values from a fully observable MDP. Since the transition probabilities between states were fully overt, the state-action values could be computed iteratively via dynamic programming (or backwards induction). 
Figure 1
 illustrates the requirements (state-action transitions and transition matrix) for computing of the optimal policy values with the MDP algorithm.
Backwards induction was performed by iteratively solving the following Bellman equation for all state-action pairs and time-points:
V π =E π [ G t ∨ s=s t ] =E π [ ∑ j=0 T γ j R t + j +1 ∨ s=s t ] ,
(1)
where is the expected value of the total return G when starting from state s at time t and following policy Л. We only considered the discount factor ɣ = 1, since the game mechanics of our task did not necessitate nor motivate discounted rewards over time. The policy Л can, thus, be computed from the argmax of the average future rewards of the action set a as follows:
π (s )=argmax a ∑ s ' ,r
p 
(s ' , r ∨ s , a)
 [r+γV 
(s ' )]
 (2) Accordingly, the action values for foraging V f and waiting V w can be transformed into a single foraging value f in the following manner:
f =V f − V w (3)
If f is positive, the optimal policy suggests foraging. If f is negative, the optimal policy suggests waiting. If f is zero, the optimal policy is indifferent. Taking the optimal policy values (not the policy itself) as a model to fit to the data entails the advantage that the derivative of the logistic link function captures the uncertainty of the current environment, which would be disregarded by a hard optimal policy. The code for calculatin the optimal policy values was made publicly available via github (see https://github.com/SAEG64/fora02/tree/main).


Task presentation
At the beginning of a forest, participants were shown both weather types in the so-called forest phase (3.5s), followed by 8 consecutive decision and feedback phases (the so-called "days") until completion of a forest. The feedback was displayed for 2s. Before each decision phase, a random fixation time between [0.5, 3.8] appeared in the form of a blank screen. Participants had to give a response within 3s, otherwise the option 'waiting' was chosen by default. In case of no response, these decisions were excluded from the data when conducting model comparisons. 
Figure 2
 illustrates the details of the task presentation. Before doing the task, subjects received written instruction about the set-up. The full instructions sheet can be found in S1 Task instructions.


Behavioral modeling
The main objective of this experiment was to test whether participants would implement a sophisticated/MDP-based or rather naive/heuristics-based meta-decision making approach in different experimental conditions. To test this, our experiment allowed model comparisons between a number of single or multi-heuristic models and the state-action value difference of the optimal policy (called optimal policy values). A detailed description of our models can be found in 
Table 1
.
Our task design is composed of the following number of variables that can be directly derived from the task and represent simple heuristic policies: probability of gaining food, risk of threat encounter, weather type (good and bad weather type), the internal (binary) energy state and the wait when safe policy (if sufficient energy points are collected, participants can simply wait to win the game). We also included the win-stay-lose-shift heuristic, which binarizes the evaluation of the last event in the choice history. These models represent directly observable and univariate features of our task, which do not require any further feature engineering. In addition to this, we included what we call "intermediate" models, a set of policies that require some recombination of variables to represent slightly more complex decision rules. Models of this kind are the probability of success (denoted as conditional probability p (success | risk) ), the expected energy gain (E[X] = ∑x i p i ; x i : energy gain/ loss magnitude, p i : respective probability to gain or lose energy), as well as, the marginal value model. The success probability accounts for the joint probability of a predator encounter and, thus, requires some additional computation compared to the simpler probability to gain food. The expected gain scales the gains and losses with their respective probabilities and sums up the resulting values. Note that we only include the "naive" expected gain, which takes the raw probability of gaining food as a scale without correcting for the joint probability of encountering a predator. This is due to the model performing better than mathematically more correct versions of the expected gain, e.g., accounting for the joint probability of a predator encounter. The marginal value model is a binary variable that uses the trial by trial mean expected gain as a threshold for accepting a weather type as good enough to forage. This idea stems from the marginal value theorem (MVT),
where patch foraging behavior is explained with a threshold value computed from the mean expectancy of sequentially experienced environments (see 
Gabay & Apps, 2021;
Mobbs et al., 2018)
.
Ultimately, we created a multi-heuristic policy that considers the situational context of the validity of the three best performing heuristics, namely the probability of success, the binary energy state (BES) and the wait when safe (WWS) strategy. We created our multi-heuristic policy by modifying the model values for p s in states that obviate the task's trade-off in the following manner:
 If energy is 1: p s ← 1 (participant has to forage; binary energy state BES)  If energy is higher than time-points left: p s ← 0 (wait-when-safe state WWS)
When fitting our models, we realized that the optimal policy value model suffers from a disadvantage since it contains a certain noise in BES and WWS situations not included in the multiheuristic policy. This arises from our MDP algorithm, where f is calculated relative to transition probabilities of the choice options in an environment. In this regard, optimal policy values are lower in if the transition probabilities are more similar between the weather types, regardless of a state obviating this ambiguity due to the BES or WWS policy dictating a uniform action preference across all forests. To cope with this, we "capped" optimal policy values in an additional model to the max/min of all calculated values for f equivalently to the value transformation of p s when creating the multi-heuristic policy. Simply put, we entered the maximum value of all calculated values for f (0.625) if the multi-heuristic policy had a value of 1 and the minimum value of all calculated values for f (-0.84) if the multi-heuristic policy had a value of 0. This capping ensured that the multiheuristic policy was not falsely favored over the optimal policy value model due to a better fit in states without behavioral noise.
The optimal policy values (capped and uncapped) and the multi-heuristic policy are based on a multivariate evaluation of the environment, for which they represent decision rules of the highest complexity in our task. It should be noted that we could have generated multivariate logistic regressions instead of recombining model values to account for several variables. However, due to the BIC penalizing model complexity, multivariate logistic regression models would have a disadvan-tage, whereas the optimal policy considers multiple aspects of the decision sequence in a single model value. In order to assure a fair model comparison between the optimal policy values with a multi-heuristic policy, we created a single model based on multivariate considerations and used only univariate logistic regression for our Bayesian model inference.


Model comparison
The total amount of possible data points was reduced due to forests being exited prematurely with energy state 0 ("game-over") and none response trials. For the remaining trials, logistic regressions were performed for all models using the following form:
P forage = 1 1+ exp (− DV ) ,
(4)
where DV is the decision variable as defined by:
DV =β 0 + β 1 * policy
(5)
We compared the models using the Bayesian information criterion (BIC), which is a metric for the negative log likelihood of a model penalized for model complexity. The BIC was obtained by fitting each model to participants responses separately via logistic regression. To determine which model explained participants' behavior the best, we computed the log-group Bayes factor (BF) assuming a fixed intercept and the protected exceedance probability (PEP) assuming random intercepts. The BF refers to the posterior odds for the null hypothesis (see 
Kass & Raftery, 1995)
.
The PEP is a measurement for the likeliness of a given model to be more frequent in a population (see 
Rigoux et al., 2014)
. To assure the quality of our modeling, we performed parameter recoveries (see S2 
Figure 2
) and calculated the confusion matrix (see S2 
Figure 3
) for the most correlating models (see S2 
Figure 1
). The entire codebase for our modeling analyses was made publicly available (see https://github.com/SAEG64/fora02/tree/main).


Response time analysis
The sigmoid function generated from the logistic regression allows deriving a metric for a respective model's predictive choice uncertainty u Л . The assumption of a policy's choice uncertainty was, that higher uncertainty values would linearly predict longer response times if the underlying model would indeed be a valid description of the decision process. A model's choice uncertainty is mathematically defined by the derivative of the respective model fit as follows:
u π = 1 exp ( DV ) * (1+exp (− DV )) 2
(6)
Therefore, we ran a linear regression of the response times with the winning model's uncertainty values. Since response times usually follow a logarithmic distribution due to time pressure, we log transformed the dependent variable. The code for running this analysis can be found online (see https://github.com/SAEG64/fora02/tree/main).


Decision tree
To validate our multi-heuristic policy, we extracted a decision tree containing the most important heuristic features. In simple terms, the decision tree calculates the proportion of data across subjects that is consistent with a respective feature. The root node represents the feature having the highest conformity with the data. The further down, the nodes represent features with lower data conformity. In order to assure interpretability, we computed a hard decision tree, for which we had to binarize model values (if they were graded or continuous) e.g. by defining mean cutoffs. If a model had to be binarized like, for example, the p success model, the respective binarized feature would suggest foraging if an environment contained a value above the mean, and waiting otherwise.
We made sure that the features we selected were not highly correlated with each other (refer to S2 
Figure 1)
. To preserve the trajectories of the choice sequences we used all 10482 state-action pairs including none response trials. For validation purposes, 20% of the pairs were reserved as a test set.
To train the decision tree, we used a maximum depth of 5 selected nodes based on Shannon's en -tropy as a measure for impurity. The mathematical formulation of our impurity measure was as follows:
H ( X )=− Σᵢ ₌ ₁ ₋ pᵢlog ₙ ₂ pᵢ (7)
where p i is the probability of randomly selecting an example in class i.
Then, to ensure that the tree is interpretable and has no redundancy in features, we pruned the tree by defining the minimum weighted fraction of the total sum of weights of leaf nodes to be 5%
and set the limit for inducing a split to be based on the impurity decrease as 0.02 (refer to the hyperparameters "min_weight_fraction_leaf" and "min_impurity_decrease" of scikit-learn's Decision
Tree Classifier in Python). The codebase for calculating the deicision tree I publicly available on github (see https://github.com/faizankshaikh/ForaGym/blob/main/notebooks/DataAnalysis.ipynb).
Figure 2 :
2
Sequential foraging task with predators.


Figure 4 :
4
Effects of experimental condition (approach vs. avoidance forests


Figure 5 :
5
Validation of multi-heuristic policy. Left panel shows Response time as a function of model uncertainty. A) Uncertainty is the derivative of the model fit (of the multi-heuristic policy). B) Linear association of uncertainty values with response time. C) Simplified representation of the multiheuristic policy computed via decision tree analysis. Out of all possible features entered, the three branches remained after pruning the decision tree. Note: the success probability was transformed to a binary variable in order to compute hard deci -sion tree.


Table 1 : Overview of predictive variables representing behavioral models
1
Variable name
Explanation
Range of
Example value of
possible values
respective task
in the task
variables depicted in
Figure 2
Optimal policy values Difference between state-action
-0.84 -0.625
Cannot be depicted
values computed with dynamic
since it depends on
programming for all possible
the context (current
future states s.
state and time-point)
of all relevant
variables
Optimal policy values
Equivalently to the multi-
-0.84 -0.625
Not available since it
"capped" to max/min
heuristic policy, model values
depends on the
values in non-trade-off
were modified in states that
context (current state
states
obviate trade-offs (see multi-
and time-point) of
(OP values + cap)
heuristic policy below). Hard-
all relevant variables
coded max of 0.625 if binary
energy state suggests foraging
and min of -0.84 entered if wait
when safe can be applied. In all
other states, the model is
identical with the optimal policy
values.


7
Model comparison) changed depending on the experimental conditions. This could hint to switches of focus between environmental features that, in turn, improve participant performance. To test this, we ran a mixed-effects model to see whether the experimental conditions interact with the two best models (namely OP values + cap and multi-heuristic policy). The capped optimal policy values show a significant inter-Empirical responses in BES and WWS states. D) Optimal policy value model fitted to BES and WWS states. Note that there is an underfit of the optimal policy values for these states due to the interdependent of the MDP on the current environment.
Figure 3: Model comparisons for entire data set (no condition effect). A) Bayesian model inference with information criterion (BIC) for fixed and
protected exceedance probability (PEP) for random intercepts (accounting for subject variance). B) Model fits of the two best models plotted for the
respective model's binned values (left: optimal policy value, right: multi-heuristic policy). Bubble sizes are proportional to the sampling of actual re -
sponses at the respective data bin. C)
action with our experimental conditions (z = -2.52, p < 0.05), but not the multi-heuristic policy (z =
10














A Primer on Foraging and the Explore/Exploit Trade-Off for Psychiatry Research




M
A
Addicott






J
M
Pearson






M
M
Sweitzer






D
L
Barack






M
L
Platt




















10.1038/npp.2017.108








Neuropsychopharmacology




42


10














Le Comportement de l'Homme Rationnel devant le Risque: Critique des Postulats et Axiomes de l'Ecole Americaine




M
Allais








Econometrica




21


4


















10.2307/1907921














Neural substrates of approach-avoidance conflict decision-making




R
L
Aupperle






A
J
Melrose






A
Francisco






M
P
Paulus






M
B
Stein




10.1002/hbm.22639








Human Brain Mapping




36


2
















Knowing how much you don't know: A neural organization of uncertainty estimates




D
R
Bach






R
J
Dolan








Nature Reviews Neuroscience




13


8


















10.1038/nrn3289














Decision-making styles and mental health-A person-oriented approach through clustering




J
Bavolar






M
Bacikova-Sleskova




10.1002/bdm.2183








Journal of Behavioral Decision Making




33


5
















Deciding How To Decide: Self-Control and Meta-Decision Making




Y.-L
Boureau






P
Sokol-Hessner






N
D
Daw








Trends in Cognitive Sciences




19


11


















10.1016/j.tics.2015.08.013














Cortical mechanisms of action selection: The affordance competition hypothesis




P
Cisek




10.1098/rstb.2007.2054








Philosophical Transactions of the Royal Society B: Biological Sciences




362
















Advances in modeling learning and decision-making in neuroscience




A
G E
Collins






A
Shenhav




10.1038/s41386-021-01126-y








Neuropsychopharmacology




47


1
















Model-based decision making and model-free learning




N
Drummond






Y
Niv




10.1016/j.cub.2020.06.051


R860-R865








Current Biology




30


15














A hypothalamic-thalamostriatal circuit that controls approach-avoidance conflict in rats




D
S
Engelke






X
O
Zhang






J
J
O'malley






J
A
Fernandez-Leon






S
Li






G
J
Kirouac






M
Beierlein






F
H
Do-Monte




10.1038/s41467-021-22730-y








Nature Communications




12


1














Computational rationality: A converging paradigm for intelligence in brains, minds, and machines




S
J
Gershman






E
J
Horvitz






J
B
Tenenbaum




10.1126/science.aac6076








Science




349


6245
















Inductive biases for deep learning of higher-level cognition




A
Goyal






Y
Bengio




10.1098/rspa.2021.0068








Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences




478


20210068














An experimental analysis of ultimatum bargaining




W
Güth






R
Schmittberger






B
Schwarze








Journal of Economic Behavior & Organization




3


4


















10.1016/0167-2681


















Changing Tastes and Coherent Dynamic Choice. The Review of Economic Studies




P
J
Hammond




10.2307/2296609








43














A neuronal theory of sequential economic choice




B
Y
Hayden






R
Moreno-Bote




10.1177/2398212818766675








Brain and Neuroscience Advances




2














The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)




B
Y
Hayden






Y
Niv








Behavioral Neuroscience




135


2


















10.1037/bne0000448














The Paraventricular Thalamus as a Critical Node of Motivated Behavior via the Hypothalamic-Thalamic-Striatal Circuit. Frontiers in Integrative Neuroscience




A
G
Iglesias






S
B
Flagel




10.3389/fnint.2021.706713








15












Optimal utility and probability functions for agents with finite computational precision




K
Juechems






J
Balaguer






B
Spitzer






C
Summerfield




10.1073/pnas.2002232118








Proceedings of the National Academy of Sciences




118


2
















D
Kahneman






P
Slovic






Tversky




10.1017/CBO9780511809477




Judgment under Uncertainty: Heuristics and Biases


A.




Cambridge University Press














Medial and orbital frontal cortex in decision-making and flexible behavior




M
C
Klein-Flügge






A
Bongioanni






M
F S
Rushworth








Neuron




110


17


















10.1016/j.neuron.2022.05.022














Maintaining Homeostasis by Decision-Making




C
W
Korn






D
R
Bach




10.1371/journal.pcbi.1004301








PLOS Computational Biology




11


5














Heuristic and optimal policy computations in the human brain during sequential decision-making




C
W
Korn






D
R
Bach




10.1038/s41467-017-02750-3








Nature Communications




9


1














Minimizing threat via heuristic and optimal policies recruits hippocampus and medial prefrontal cortex




C
W
Korn






D
R
Bach








Nature Human Behaviour




3


7


















10.1038/s41562-019-0603-9














Planning Algorithms




S
M
Lavalle








Cambridge University Press












To Approach or Avoid: An Introductory Overview of the Study of Anxiety Using Rodent Assays




M
La-Vu






B
C
Tobias






P
J
Schuette






A
Adhikari




10.3389/fnbeh.2020.00145








Frontiers in Behavioral Neuroscience




14














Neural computations underlying arbitration between model-based and model-free learning




S
W
Lee






S
Shimojo






J
P
Doherty








Neuron




81


3


















10.1016/j.neuron.2013.11.028














Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources




F
Lieder






T
L
Griffiths




10.1017/S0140525X1900061X








Behavioral and Brain Sciences




43














Approach-Avoidance Decisions Under Threat: The Role of Autonomic Psychophysiological States




J
J A
Livermore






F
H
Klaassen






B
Bramson






A
M
Hulsman






S
W
Meijer






L
Held






F
Klumpers






L
D
De Voogd






K
Roelofs




10.3389/fnins.2021.621517








Frontiers in Neuroscience




15














Dynamic Inconsistency in Choice and Different Models of Dynamic Choice -A Review




G
Lotito




10.2139/ssrn.4314092








SSRN Electronic Journal
















Rationality and Dynamic Choice: Foundational Explorations




E
F
Mcclennen




10.1017/CBO9780511983979








Cambridge University Press












Foraging for foundations in decision neuroscience: Insights from ethology




D
Mobbs






P
C
Trimmer






D
T
Blumstein






P
Dayan




10.1038/s41583-018-0010-7








Nature Reviews Neuroscience




19


7
















T
Salimans






J
Ho






X
Chen






S
Sidor






I
Sutskever




10.48550/arXiv.1703.03864


arXiv:1703.03864




Evolution Strategies as a Scalable Alternative to Reinforcement Learning
















Myopia and Inconsistency in Dynamic Utility Maximization




R
H
Strotz




10.2307/2295722








The Review of Economic Studies




23


3


















R
S
Sutten






A
G
Barto






Reinforcement Learning




MIT Press








2nd ed.








Exploration in neo-Hebbian reinforcement learning: Computational approaches to the exploration-exploitation balance with bio-inspired neural networks




A
Triche






A
S
Maida






A
Kumar




10.1016/j.neunet.2022.03.021








Neural Networks




151
















Judgment under Uncertainty: Heuristics and Biases




A
Tversky






D
Kahneman








Science




185


4157
















Advances in prospect theory: Cumulative representation of uncertainty




A
Tversky






D
Kahneman








5














Balancing exploration and exploitation with information and randomization. Current Opinion in Behavioral Sciences




R
C
Wilson






E
Bonawitz






V
D
Costa






R
B
Ebitz




10.1016/j.cobeha.2020.10.001








38














Generalization guides human exploration in vast decision spaces




C
M
Wu






E
Schulz






M
Speekenbrink






J
D
Nelson






B
Meder








Nature Human Behaviour




2


12


















10.1038/s41562-018-0467-4














Continuous decisions




S
B M
Yoo






B
Y
Hayden






J
M
Pearson




10.1098/rstb.2019.0664








Philosophical Transactions of the Royal Society B: Biological Sciences




376















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]