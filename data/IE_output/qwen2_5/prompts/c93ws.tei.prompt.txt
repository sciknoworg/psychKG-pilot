You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
The mind is very hard to check.


And swift it falls on what it wants;
The training of the mind is good, A mind so tamed brings happiness.
-Dhammapada, third century BCE 
(Buddhaghosa, 1996)
 As the more than 2,300 year old quote above shows, the importance of self-regulation for wellbeing has been known to humans for a long time. Comparatively more recent scientific research corroborates this ancient wisdom linking self-regulation to a wide array of outcomes including relationship satisfaction, beneficial financial decision making and even health 
(Moffitt et al., 2011)
.
It is widely believed that executive functions, a group of low-level cognitive functions responsible for the conscious (rather than habitual) allocation of attention, are the fundamental building blocks of self-regulation 
(Inzlicht et al., 2021)
. The most widely discussed executive functions are updating, shifting and inhibition. "Updating" refers to the active process of observing which new information enters one's consciousness and replacing information that has become irrelevant. "Shifting" refers to the process of switching between two mental tasks. Finally, "inhibition" refers to the deliberate process of deciding against engaging with a dominant or automatic impulse 
(Webb et al., 2018)
. Executive functions are closely related to working memory 
(Unsworth et al., 2020)
, which is the cognitive resource supporting the ability to "encode, maintain and manipulate information in one's immediate awareness" 
(Webb et al., 2018)
. The two concepts are related in two ways: 1) the processes of inhibition, shifting and updating are involved in managing the content and focus of attention within working memory and 2) are themselves dependent on working memory for temporarily holding contextual information on i.e. the tasks to shift between 
(Miyake et al., 2000;
Shipstead et al., 2016)
. In addition to this functional relationship, a large body of research also shows that the neural correlates of executive functions and working memory can both be traced back to a highly overlapping group of brain networks responsible for conscious goal-directed behaviour 
(Wu et al., 2020)
.
Given the core role executive functions play in regulating behaviour, the idea of improving them either directly or through working memoryhas spurred a wide interest from both scholars and the general public 
(Redick, 2019)
. By now, cognitive training is a USD 3.2 billion market promising benefits like higher self-confidence and overall improved mood (BrainHQ, no date; Cognitive 
Assessment and
Training Market -Global Forecast to 2025, 2020)
. The core idea behind the various cognitive trainings commercially available is that repeated practice of tasks that challenge executive functions and/or working memory strengthen their underlying neural circuits so that subjects not only improve on the individual task itself but gain stronger executive functions overall 
(Reuter-Lorenz & Park, 2014)
.
A key challenge to research on cognitive training is the so called "curse of specificity" which refers to the difficulty of disentangling "mere" improvements on a training task itself from more general improvements in the cognitive function the task trains 
(Schmiedek, 2016)
. Consequently, to demonstrate more general cognitive improvements subjects not only need to show increased performance on a particular task but also on other tasks involving the same or a related cognitive function.
While there is broad consensus in the literature on the possibility to improve performance on training tasks themselves, the evidence for training-induced improvements in other untrained tasks is mixed -both on the level of primary studies and on the level of meta-analyses 
(Nguyen et al., 2019;
Soveri et al., 2017)
. Results are mixed in two ways. Firstly, different primary studies and meta-analyses thereof come to different conclusions on the mean transfer effects of executive functions training. Secondly, there seems to be a high level of heterogeneity of effects in the cognitive training literature more generally. In fact, its level of heterogeneity seems to be unusually high when compared to the other fields of psychology 
(Moreau, 2021)
.
What might be driving this mixed state 1 of the literature?
This umbrella review investigates the role that the methodological quality of meta-analyses and conceptual inconsistencies in the wider literature play.
There are a range of methodological issues that need to be considered when synthesizing cognitive training research and it has not been systematically assessed so far how well previous meta-analyses address them. Commonly cited issues include 1) passive control group designs that cannot rule out placebo effects 
(Boot et al., 2013)
, 2) accounting for baseline cognitive performance differences between training and control group 
(Melby-Lervåg & Hulme, 2016)
 and 3), appropriate handling of the statistical dependence among multiple effect sizes drawn from the same study 
(Sala et al., 2019)
.
Two types of conceptual inconsistency affect the cognitive training literature.
Firstly, there is inconsistency in how scholars have reported results on transfer effects. While there is broad agreement on the idea to differentiate between near transfer (transfer to tasks that require cognitive abilities that are closely related to those the training task requires) and far transfer (transfer to tasks that are less closely related to the training task), scholars have operationalised these terms differently 
(Nguyen et al., 2019)
. Within the realm of working memory trainings, for example, some scholars have defined near transfer as transfer between different types of working memory 
(Goghari & Lawlor-Savage, 2017)
 while others have included transfer to short-term memory tasks in their definition of near transfer 
(Borella et al., 2010)
. While this inconsistency does not affect the level of heterogeneity of a meta-analyses, it may lead to different conclusions between meta-analyses on the mean transfer effects of cognitive training as they actually summarise different outcomes.
Secondly, there is inconsistency in the cognitive training literature in terms of how tasks are mapped to the cognitive outcomes they are meant to train and/or measure 
(Birney, 2015)
. The lack of standardisation presents a challenge to research synthesis as it creates uncertainty on whether results on outcomes that are called the same, in fact, relate to the same outcome. As a consequence, a meta-analysis author might unknowingly include a wider range of outcomes than intended. If cognitive training impacts these outcomes differently, this might be reflected as a high level of heterogeneity in the meta-analysis or even bias its summary effect size. 
Webb et al. (2018)
 propose to standardise task-outcome mapping by combining the Cattell-Horn-Carroll (CHC) taxonomy of cognitive abilities 
(Carroll, 1993;
 and the three-factor model of executive functions developed by 
Miyake et al. (2000)
. Both models are supported by substantial bodies of factor-analytic research -in the case of the CHC taxonomy by a bit more than 100 years of scholarly work.
The primary objective of this umbrella review was to assess how methodological differences and conceptual inconsistencies have contributed to the mixed results in previous meta-analyses. In particular, it assessed how well meta-analyses have considered the issues of appropriate control group design, accounting for baseline performance differences between treatment and control group and addressing statistical dependence of effect sizes. Additionally, it addressed the issues of different transfer distance definitions and mappings between cognitive tasks and the cognitive abilities they exercise in combination. Instead of relying on the transfer distance definition provided by a meta-analysis, it directly extracted training and transfer tasks from the included studies. After mapping all extracted tasks against the taxonomy of cognitive abilities developed by 
Webb et al. (2018)
 it clustered the effect sizes of the meta-analyses based on the transfers that emerged from this exercise. By "disassembling" and "standardising" transfer effects this way, this umbrella review aimed to provide a more transparent overview of the evidence base. Based on this, the secondary objective of this umbrella review was to assess what could be learned about transfer effects from this new overview when taking into account the methodological quality of the meta-analyses.


Methods


Search strategy and study selection
A database search of PsychINFO for the period from January 2013 to March 2021 was performed to identify meta analyses focused on the effects of executive functions and working memory training. This time frame was chosen because the first meta-analysis on this topic was published in 2013 
(Katz et al., 2018)
. Both subject headings and uncontrolled keywords were used. The searches followed the general scheme of "'Synonyms for included outcomes' AND 'Synonyms for included interventions' AND 'Synonyms for systematic reviews and meta-analyses'". Synonyms were generated through three parallel work streams: 1) gaining theoretical understanding of the subject area from conceptual reviews, such as or 
Webb et al. (2018)
, 2) an iterative process of extracting keywords from potentially relevant reviews and refining the search strategy accordingly and 3) searching for subject headings in PsycINFO and PubMed. The complete search strategy can be found in appendix 1. Search results were filtered by type of publication to only include systematic reviews and meta-analyses. To make sure search results were comprehensive, pilot searches were checked for inclusion of any of nine reviews that were previously identified to be potentially relevant based on their title and abstract. The full list of these "indicator reviews" can be found in appendix 2. In addition to the database search, articles that were included in the fulltext screening were reviewed for further potentially relevant meta-analyses.


Inclusion criteria Population
This umbrella review focuses on healthy adults who are 20 years old or older. Children and adolescents were excluded as they are in a different stage of brain developments than adults 
(Tau & Peterson, 2010)
. While different estimates exist for the end of adolescence, all fall between 20 and 25 years 
(Arain et al., 2013;
Johnson et al., 2009)
. To be more comprehensive, this umbrella review erred towards the lower bound of this range and included meta-analyses that contained studies with populations as young as 18 years old as long as older subjects represented the majority of the overall review population. Meta-analyses that include a mix of healthy and unhealthy adults were included when separate analyses were reported for the healthy population. A population was considered unhealthy when it had a neurological condition, such as multiple sclerosis, or was otherwise medically diagnosed, i.e. with cancer, due to the possibility of diseaserelated changes to cognitive functioning 
(Palmer, 2020)
. Populations with subjects up to 29 years old were coded as "young", those with subjects between 30 to 59 years old were coded as "midage" and those with subject older than 60 years as "old'.


Interventions
This umbrella review focuses on single-domain training tasks that target either working memory or executive functions. While complex executive functions tasks, such as the "Tower of Hanoi" task or the Wisconsin Card Sorting test, appear to lead to stronger transfer effects 
(Nguyen et al., 2019)
, they are excluded because of the task impurity problem: complex tasks require a variety of cognitive abilities 
(Miyake et al., 2000)
. When transfer effects are observed after training on complex tasks, it can, thus, not be involved which of the trained abilities lead to the transfer. While they were excluded as training tasks, complex executive functions tasks were included as transfer tasks gauge general improvements in executive functioning.
To maintain a coherent mapping between tasks and outcomes, the taxonomy of cognitive tasks by 
Webb et al. (2018)
 was applied to all meta-analyses (see table 1 for a brief overview of it). With research on cognitive training stretching across different psychological disciplines maintaining such coherence has been identified as a challenge to cross-disciplinary research synthesis 
(Birney, 2015)
. The mapping was done based on task names provided by the meta-analyses and the author calibrated his understanding of the taxonomy based on the remapping of tasks 
Webb et al. (2018)
 performed for the meta-analysis by 
Lampit et al. (2014)
. Cases in which subjects were trained or tested on a group of tasks that belong to different outcomes were coded as "multiple (outcome 1, outcome 2, etc.)". When a meta-analysis did not provide study-level information on training and transfer tasks it was excluded from this umbrella review. 


Comparators
Meta-analyses synthesizing studies with either active or passive groups were included. Following best practice guidelines, to be considered as active control group, control group participants needed to engage in activities that were as similar as possible to those of the treatment group e.g. in terms of cognitive stimulation, enjoyment, etc. but did not predominantly engage the cognitive function being trained, i.e. inhibition 
(Boot et al., 2013;
Simons et al., 2016)
. Control groups watching videos, doing physical activities or filling in questionnaires were not considered as active, for example. Assessing whether the control and treatment group activities are similar enough necessarily depends on the specific tasks implemented by a study and, therefore, needs to be assessed on a study level. This umbrella review, thus, relied on the meta-analyses to provide sufficient details. When this was not the case, the control group classification of the meta-analysis was adopted but marked as "not verified".


Outcomes
This umbrella review focuses on transfer effects between tasks described as relating to working memory or executive functions. Unlike previous meta-analyses, it did not label transfers as either near or far. This choice is based on the judgement that currently no suitable approach to standardise transfer definitions is available for studies focused on working memory and executive functions training. The classification of transfer differences proposed by Noack et al. 
2009
is a commonly cited approach to define transfer distance. It was judged to not be applicable to this umbrella review, however. The main reason for this judgement was a conservative interpretation of ongoing research on the integration of executive functions into the Cattel-Horn-Carroll (CHC) taxonomy of cognitive abilities. This integration matters as the CHC taxonomy is the foundation of the classification by 
Noack et al. (2009)
. In other words: the applicability of the latter to this umbrella review, hinged on the valid integration of executive functions in the former. More detailed arguments supporting this choice can be found in appendix 3.
As an alternative to labelling transfers as near or far, this umbrella review employed a clustering approach. Based on the results of the task remapping, it formed clusters representing the most common types of transfers across the included meta-analyses following a hierarchical approach. First, it grouped tasks by the broad cognitive ability they belong to, such as executive functions. Effect sizes for which transfers between the same pairs of broad abilities were most common were grouped together (i.e. two effect sizes for which transfer between executive functions and general short-term memory were most common would be grouped into the same cluster). In a second step, it grouped tasks by the narrow ability they belong to, further refining the clusters that emerged from the first step.
Given the inconsistent mapping of tasks to outcomes in the cognitive training literature mentioned earlier, an effect size reported to reflect transfer between working memory tasks might, in fact, also contain tasks relating to other cognitive domains, such as executive functions. When such an effect size is then assigned to a cluster focused on transfer between working memory tasks, its assignment is not 100% "pure". To give an indication of well an effect size fits the cluster it was assigned to, a "purity" level was calculated for each effect size:
(%) = ℎ ( ) ℎ
Note that the number of experimental effect sizes contributing to a meta-analytic effect size might be larger than the number of studies it summarises, i.e. when a several effect sizes were extracted from a single study.


Study design
This umbrella review focuses on random effects meta-analyses with a systematic search strategy that synthesize randomised control trials or studies using other non-random designs, such as quasi-experimental studies. A search strategy was considered systematic when it contained 1) a clearly formulated question and 2) systematic and explicit methods to identify, select and critically appraise relevant primary research 
(Higgins et al., 2020)
.


Data extraction
Type of effect size measures This umbrella review reports all effect sizes as hedges' g, which is the standardised mean difference between treatment and control group 
(Lakens, 2013)
. To be considered for this umbrella review, effect size estimates had to be based on at least two primary studies.


Time of data collection
Only data relating to measurements taken directly after the end of the training were considered in this umbrella review.


Measures of heterogeneity
To transparently report the level of heterogeneity across studies, typically used measures were extracted: the Q-statistic, a statistical significance test for heterogeneity, the I 2 statistic, which indicates how much percent of the variance between studies is true, so not due to chance and the 2 statistic, which reflects the magnitude of the true variance between studies 
(Harrer et al., 2019)
. As the interpretation of both the Q-statistic and I 2 is highly sensitive to the context of a meta-analysis, two-sided 95% predictions intervals were calculated to aid the judgement on the level of heterogeneity, wherever possible, using the formula provided by IntHout et al. 
2016
:
% = ± . , − * √ +
where denotes the two-sided critical t-value, denotes the number of studies contributing to the effect size and denotes the standard error of the mean effect. When was not available, it was approximated by dividing the difference between the upper and lower bound of the 95% confidence interval of the mean effect by 3.92 as suggested by IntHout et al. (2016).


Handling of statistically dependent effect sizes
An important challenge to synthesizing research on cognitive trainings is that of statistically dependent effect sizes. Cognitive training studies often measure performance changes in a cognitive ability with multiple cognitive tasks. Since these measures are drawn from the same sample, they are not statistically independent, however. Including results for multiple measures from the same study into a meta-analyses without accounting for their statistical dependence, artificially reduces the variance of a meta-analytic effect size 
(Cheung, 2019)
. To be included in this umbrella review, meta-analyses, therefore, needed to report a way to handle these cases.
A common approach to handling this issue is to create an average of all measures for a certain cognitive ability and only include the average in the meta-analysis. This approach has been shown to artificially increase the variance of the corresponding meta-analytic effect size, however 
(Moeyaert et al., 2017)
. Two alternative approaches -multilevel meta-analysis and robust variance estimation -typically lead to unbiased variance estimates 
(Moeyaert et al., 2017)
. Therefore, meta-analyses deploying either one of them were considered as following best practice.


Publication bias
A binary judgement on the presence of publication bias was extracted for each effect size provided by the meta-analyses. Where available, information on the magnitude of the publication bias was extracted, too.


Strength of evidence
When a meta-analysis evaluated the strength of its evidence base, the evaluation tool, its scoring method and the overall judgement on the strength of the evidence were extracted.


Synthesis
As the clusters that emerged after the remapping of tasks differed substantially in their size and overall level of purity, an empirical synthesis (i.e. in the form of a second-order meta-analysis) was not feasible. Hence, results were synthesized narratively.


Results


Search results
197 records were identified through searching PsycINFO. As only one database was searched, no duplicates had to be removed prior screening. 183 records were removed based on screening titles and abstract. A further 9 records were excluded after full-text screening. 7 records were identified through screening the citations of the records considered for full-text screening from the database search. Out of these, 6 were excluded after full-text screening. The most common reason for exclusion across all identified records was missing data. 
Figure 1
 provides an overview of the search and screening process following current PRISMA reporting guidelines 
(Page et al., 2020)
. Characteristics of included meta-analyses 
Table 2
 describes the main characteristics of the included meta-analyses. Note that metaanalyses may include populations beyond the scope of this umbrella review. In these cases only the information for populations relevant to this review are described.
Among the included meta-analyses most subjects were of mid-age (30 to 59 years old) or old (60+ years old). While randomised control trials (RCTs) have the largest share (76%) of the studies contained in the meta-analyses, non-random designs such as quasi-experimental designs are also common. Only one meta-analysis pre-registered a review protocol. Two meta-analyses provided summary judgements on the strength of the evidence of their evidence base. While the results for the two are not directly comparable, it is noteworthy that for both meta-analyses a lack of adequate blinding procedures was the most common methodological shortcoming.


Re-mapping of tasks to outcomes & clustering of effect sizes
A total of 837 tasks or groups of tasks were extracted from studies and mapped against the taxonomy of 
Webb et al. (2018)
. As the meta-analysts have not consistently provided information on the cognitive outcome they have categorised a task to, the extent to which this mapping changed the categorisation of tasks could not be assessed directly, however. Nevertheless, comparing the transfer distance definitions given by the meta-analyses with the clusters this umbrella review determined, provides several clues for the impact of the mapping. 
As table 3
 shows, for example, both 
(Nguyen et al., 2019)
 [2] and 
(Soveri et al., 2017)
 [3] described their near transfer results as relating to working memory tasks only. The mapping, however, revealed that their results also included a substantial amount of executive functions tasks 2 , so that they were allocated to the cluster "Transfer between EF and GSM (and vice versa)". Next to this concrete example, the high share of purity levels below or around 60% in five out of the six clusters supports the notions that most meta-analyses include tasks from more domains than they report to. Two meta-analyses reporting to examine transfer between working memory tasks, for example, might, thus, in fact, examine different combinations of transfers (in addition to working memory). This indicates that at least some of the inconsistencies between meta-analyses can be traced back to inconsistencies in terms of mapping tasks to cognitive outcomes.
Given the large number of tasks to be mapped and sometimes sparsity of detailed task information, committing some mistakes in the mapping process was unavoidable. To give other researchers the chance to critique the mapping decisions, supplementary file 1 provides a complete overview of all decisions. More details on the type of tasks included in each cluster can be found in supplementary file 2.


Synthesis of meta-analyses


Results by cluster
The following sections describe the results presented in table 4.


Transfer between EF and GSM (or vice versa)
Significant effect sizes hover between g = 0.18 and g = 0.41 for this cluster with a simple mean 3 of g = 0.34 which excludes 
(Soveri et al., 2017)
 [5] and [6] to avoid double counting effect sizes. Given the low purity indexes by 
Weicker et al. (2016)
 and 
Nguyen et al. (2019)
, this average is a noisy estimate of the cluster's overall level of effectiveness. Due to its substantially higher level of purity compared to the other meta-analyses, 
Soveri et al. (2017)
 [4] (g = 0.24) is considered the least wrong summary effect size for this cluster -before considering the impact of other moderators. As the moderator analysis of 
Soveri et al. (2017)
 shows, however, this estimate is an overstatement: when limiting the scope of the effect size to experiments including active control groups it is only gActive = 0.18. While 
Soveri et al. (2017)
 [4] has a high level of purity, it is worthwhile to investigate which of the transfers it includes fall outside of the definition of this cluster. With a share of 18%, transfer between updating tasks is the second largest type of transfer. As will be seen in the last cluster, the effect size for transfer between updating tasks seems to be around g = 0.59. The inclusion of this type of transfer in 
Soveri et al. (2017)
 [4] (gActive = 0.18) has, therefore, somewhat inflated the effect size so that it likely is lower than 0.18 in reality. As the remaining 10% of other transfer types contained in the effect size are different from any of the common transfers found in the other clusters, it could not be judged if gActive should be adjusted. 
Soveri et al. (2017)
 report to have found no evidence for publication bias. Finally, as no information on heterogeneity is available for any of the effect sizes in this cluster, the available data does not allow judgements on their applicability across all subjects in the included populations.


Transfer between multiple GSM and EF narrow abilities and GL
This cluster consists of just one effect size broadly representing the strength of transfer between tasks related to either GSM or EF and tasks related to GL with a roughly equal split between GSM and EF tasks. With g = 0.19 the effect size is considered small. The purity level of 64% indicates a moderate to high level of fit of the cluster. The second most common type of transfer is transfer between updating tasks and tasks that could not be classified (17%). Consequently, the influence of this transfer type on the overall effect size could not be assessed. Moreover, no information on publication bias is available. Taken together, these factors indicate that a conservative estimate of the effect size likely is: g ≤ 0.19. Finally, no information on the level of heterogeneity was provided.


Transfer within GSM and EF respectively
This cluster represents both transfer among GSM tasks and transfer among EF tasks. The summary effect sizes in this cluster overlap: 
Sala et al. (2019)
 [9] is a subgroup effect of 
Sala et al. (2019)
 [8]. To avoid double counting studies that are included in both, 
Sala et al. (2019)
 [8] is the only effect size considered for this cluster. It has been chosen because of its richer dataset and despite its comparatively low purity level of 55% (of which 34 percentage points relate to transfer between GSM tasks and 21 percentage points to transfer between EF tasks). With g = 0.27 the effect size for this cluster is small. Nevertheless, due to publication bias this effect size is still an overstatement: depending on the publication bias analysis employed the true adjusted effect size ranges between g = 0.16 and g = 25. At 25%, transfer between G SM and EF (and vice versa) is the second most common type of transfer contributing to the effect size. As the results of the cluster on this transfer type have shown, its effect is of a similar size (g ≤ 0.18). Therefore, it is assumed that the inclusion of this type of transfer does not substantially distort 
(Sala et al., 2019) [8]
.
The level of between-study heterogeneity is 2 ≅ 0.033. Calculating a 95% prediction interval for the mean effect makes it easier to interpret the level of heterogeneity: Quite clearly, this demonstrates that the level of heterogeneity is large. Unlike previous evidence suggests, the type of control group does not seem to moderate the size of the effect. The standardised mean difference in pre-treatment performance on the trained task between treatment and control group was a significant moderator, however: b = -0.540 (p<0.001). In fact, when accounting for this moderator and removing outliers from the dataset, the between-study heterogeneity is reduced to 2 ≅ 0.000 (between-study heterogeneity after exclusion of outliers but before moderation analysis is 2 ≅ 0.017). As 30 out of the 39 studies included in 
Sala et al. (2019) [8]
 are RCTs it is surprising that pre-treatment differences influence the effect size -one would expect that they are nil (or at least that random imbalances between RCTs cancel each other out). Ergo, it is the 9 quasi-experimental studies they include that likely drive this observation.
= − 0.


Transfer between updating and other narrow EFs
For this cluster all effect sizes relate to the same meta-analysis: 
Soveri et al. (2017)
. It is also the only cluster that specifically focuses on one narrow ability as a training task. With g = 0.16 the transfer effect between updating and other executive functions tasks is small. As the moderator analysis on control group designs shows, the true effect is likely even smaller than that. When limiting the scope of the effect size to experiments including active control groups, the effect becomes g = 0.08. Given that the authors report to not have found evidence for publication bias and only include RCTs, it is tempting to view this effect size as an unbiased estimate of the mean transfer effect between updating and other narrow executive functions tasks. As the purity level of 63% shows, however, the effect size contains more than just this transfer. In particular, the second most common transfer type is transfer between updating and perceptual speed accounting for 25%. As this transfer type is not represented through any of the other clusters, the impact it has on the effect size of Soveri et al. (2017) could not be assessed directly. Taken together, these factors indicate that a conservative estimate of the effect size likely is: g ≤ 0.08. Finally, no information on the level of heterogeneity is provided.


Transfer between different GSM narrow abilities
As in the previous cluster, only one meta-analysis is contributing to this cluster -this time, effect sizes for two different age groups: young/ mid-aged adults and old adults. The size of the transfer effect is medium for the latter (g = 0.36) and large for the former (g = 0.6) -a pattern that resembles results from other meta-analyses that provide separate effect sizes for different age groups 
(Toril et al., 2014)
. As they relate to different populations, the two effect sizes are not merged into an overall effect size for the cluster. The level of purity for both is 52% indicating that about half of the transfer distances contributing to the overall effect sizes do not relate to transfer between narrow GSM abilities. Of those only one other group of transfers could be identified that is simple enough to allow for interpretation: transfer between GSM and EF and vice versa. The other transfers contained tasks from several different broad abilities, making interpretation difficult. The proportion of GSM-EF transfers was 15%. As the cluster on GSM-EF transfers has indicated, the effect size for this type of transfer is on the order of g = 0.18 or smaller, and, therefore, substantially smaller than the overall effect sizes of this cluster. The presence of GSM-EF transfers in the calculation of the overall effect of this cluster, thus, had a downward biasing influence. Given that most of the remaining 33% of transfers were transfers between either individual GSM or EF tasks and groups of GSM and EF tasks, it is assumed that they would either bias the overall effect sizes of this cluster downwards as well or do not induce additional bias. Taken together, these considerations indicate that the effect sizes in this cluster are, in fact, somewhat understated. No information on publication bias or level of heterogeneity was provided limiting further interpretation.


Transfer within same narrow ability
Effect sizes in this cluster relate to transfer between tasks of the same narrow ability for either GSM or EF, such as between different types of updating tasks, and all have a high level of purity ranging from 69% to 100%. Overall, statistically significant effect sizes span from g = 0.35 to g = 0.70. While both 
Sala et al. (2019)
 and 
Pappa et al. (2020)
 contain non-randomised study designs, neither report analyses on the potentially confounding influence of baseline differences in cognitive performance between treatment and control group 4 . As shown by 
Sala et al. (2019)
, this can bias their results. This is especially problematic for 
Pappa et al. (2020)
 as only 2 of the 7 studies they include are RCTs. For 
Sala et al. (2019)
, this problem is arguably less severe as 13 of the 15 studies they include are RCTs. To reduce the risk of bias, only the effect sizes from 
Soveri et al. (2017)
 and 
Sala et al. (2019)
 are considered in the further analysis.
The effect size from Soveri et al. (2017) (g = 0.62) relates to the transfer between different versions of n-back, an updating task, and has a purity level of 100%. The effect size from Sala et al. (2019) (g = 0.35) reflects both transfer between updating tasks and between different high working memory, low working memory and short-term memory tasks respectively. It has a purity level of 69%. The second most common transfer type for 
Sala et al. (2019)
 is transfer between GSM and EF (vice versa) with a share of 27%. As the effect size for this transfer is small (g ≤ 0.18), 
Sala et al. (2019)
 [20] is likely understated. Due to their difference in scope and level of purity the two effect sizes are not pooled to form an overall cluster effect size but are left to stand on their own. 
Soveri et al. (2017)
 provide subgroup effect sizes for experiments with active or passive control groups only. Their results confirm the pattern observed in previous clusters: experiments with active control groups report smaller effect sizes -in this case g = 0.59 as compared to g = 0.62 for the experiments with a passive control group. While 
Soveri et al. (2017)
 report to have found no evidence for publication bias, no such information is available for Sala et al. (2019) 5 . Finally, no information on the level of heterogeneity is provided for either one of them. 
4
 As mentioned earlier, 
Sala et al. (2019)
 do report a moderator analysis that accounts for baseline performance differences for their overall near transfer effect. Since the effect size they contribute to this cluster is related to a non-random subset of the studies they included in the overall near transfer -namely to those providing effect sizes for "nearer" transfer -the results of the moderator were deemed inapplicable to the effect size Sala et al. (2019) are contributing to this cluster. 
5
 Like for the moderator analysis on baseline performance differences, 
Sala et al. (2019)
 do report publication bias analyses for their overall near transfer effect. For the same reason the results of the moderator analyses were not applied to their effect size in this cluster, their publication bias analyses are not applied, too.
Overall results 
Table 5
 summarises the most representative effect sizes from all clusters. It shows that the effect sizes for transfers between broad abilities are smaller than those between different narrow abilities or within narrow abilities (with the exception of transfer between updating and other narrow EF tasks). 


Metaanalysis
Transfer between EF and GSM (or vice versa) gActive  0.18 
Soveri et al. (2017)
 Transfer between multiple GSM and EF narrow abilties and GL g  0.19 Sala et al.
Transfer within GSM and EF respectively 0.16  g  0.25 
Sala et al. (2019)
 Transfer between updating and other narrow EFs 
g  0.08 Soveri et al.
(2017)
General observations


Lack of heterogeneity and publication bias information
One observation that immediately jumps out of table 4 is the overall lack of information on the level of heterogeneity and publication bias. Heterogeneity information is available for only 2 effect sizes and publication bias information is available for 5. Most effect sizes that miss this information stem from subgroup analyses. 
Sala et al. (2019)
, for example, provide effect sizes for one overall near transfer effect as well as for three different subtypes: nearer, near & less near. While they provide both heterogeneity and publication bias information for the overall near transfer effect, they do not do so for the subtypes. A similar pattern can be observed for 
Nguyen et al. (2019)
 and 
Weicker et al. (2016)
.


Methodological quality of meta-analyses
The included meta-analyses have addressed the issue of statistical dependence between effect sizes in different ways. While 
Soveri et al. (2017)
 and 
Sala et al. (2019)
 have employed best practice approaches (robust variance estimation and multi-level meta-analysis), 
Weicker et al. (2016)
, 
Nguyen et al. (2019)
 and 
Pappa et al. (2020)
 used a simple averaging approach.
Regarding the issue of adequate control group designs, all meta-analyses label control groups as either active or passive according to the best practice definition provided earlier: to be considered as an active control group, control group participants needed to engage in activities that were as similar as possible to those of the treatment group e.g. in terms of cognitive stimulation, enjoyment, etc. but did not predominantly engage the cognitive function being trained 
(Boot et al., 2013;
Simons et al., 2016)
. Nevertheless, subgroup effect sizes limited to studies with active (or passive) control groups are only available for three effect sizes. For the other 12 effect sizes only an overall effect size containing studies with both control group designs are available. As the meta-analyses provide no information on the proportion of experiments employing active and passive control groups, the degree to which a particular effect is distorted cannot be assessed. Nevertheless, the subgroup effects provided by 
Soveri et al. (2017)
 indicate that the upper bound of this bias is likely on the order of gActive-Passive = 0.1 regardless of the type of transfer.
Finally, out of the 21 effect sizes available overall (including subgroup effect sizes for active and passive control groups), many effect sizes (16) did not control for the issue of baseline performance differences. Out of these, 8 contain studies with non-random designs and are, therefore, at risk of being biased due to imbalances in baseline cognitive performance between treatment and control group.


Discussion
The primary objective of this umbrella review was to assess the extent to which low methodological quality and conceptual inconsistencies contributed to the mixed results on transfer effects of executive functions and working memory trainings reported in previous meta-analyses. In particular, methodological quality was evaluated on the basis of control group design, consideration of baseline performance differences in cognitive performance between treatment and control groups and the approach taken to account for statistical dependence of multiple effect sizes from the same study. Conceptual inconsistencies related to differences in the definition of transfer distance and the mapping between task and cognitive outcomes.
A secondary objective was to evaluate what could be learned about transfer effects when these factors are taken into consideration.


Summary of main results


Prevalence and impact of conceptual inconsistencies
After remapping all tasks included in the meta-analyses using the taxonomy proposed by 
Webb et al. (2018)
, six clusters of transfer types emerged. An important result of the remapping process is that many effect sizes contain more types of transfer than the meta-analyses describe. In fact, for 9 out of the 15 effect sizes only 60% or less of all experimental effect sizes they contain correspond to the transfer type of the cluster they have been assigned to. To estimate the impact of this remapping on the conclusions of the meta-analyses, one would have to reanalyse the data collected by the meta-analyses in light of the new mapping. Such analysis was beyond the scope of this umbrella review, however. Nevertheless, the reanalysis that 
Webb et al. (2018)
 have conducted of the meta-analysis by 
Lampit et al. (2014)
 suggests that such reanalysis could yield meaningful differences -especially for executive functions. While the original analysis of 
Lampit et al. (2014)
 resulted in an insignificant effect of cognitive training on executive functions 6 (g = 0.09, p = 0.096), the reanalysis by 
Webb et al. (2018)
 yielded a small but significant effect (g = 0.17, p = 0.003).


Prevalence and impact of methodological shortcomings
In addition to these conceptual factors, a majority of effect sizes were affected by at least one methodological shortcoming.
Firstly, while all meta-analyses followed best practice for defining active controls, subgroup effect sizes limited to active control groups were only available for 3 out of 15 effect sizes. Given that studies employing passive control groups tend to yield higher effect size estimates, effect sizes containing both studies with active and passive control groups are biased upwards. As the included meta-analyses did not provide information on the proportion of studies employing either control group design, the magnitude of this bias could not be evaluated in detail for each metaanalysis. Still, the subgroup analyses done by 
Soveri et al. (2017)
 indicate that the upper bound of this bias is likely on the order of gActive-Passive = 0.1 regardless of the type of transfer. An overall lack of data on heterogeneity hindered an evaluation of the amount of heterogeneity that is explained by the different control group designs.
Secondly, out of the 12 effect sizes that contained studies with non-random designs, the impact of baseline performance differences between treatment and control group was only accounted for by 4. The remaining 8 effect sizes were, therefore, at risk of being biased by imbalances in baseline performance in the studies using non-random designs. In addition to raising the risk of bias for these effect sizes, missing to account for baseline differences can also inflate their level of heterogeneity: when the studies they include are imbalanced in different ways, the variance of effect sizes, and therefore, the level of heterogeneity, is artificially increased. Due to the general lack of data on heterogeneity mentioned previously as well as due to a low number of effect sizes per cluster overall, the impact of this issue on both the level of heterogeneity and the mean effect size could not be estimated.
Thirdly, 2 out of the 5 included meta-analyses used best practice approaches (i.e. robust variance estimation and multilevel meta-analysis) to account for statistical dependence between multiple effect sizes drawn from the same study. The remaining 3 addressed this issue by only including an average of the different effect sizes into their summary effect. While this approach solves the issue of statistical dependence, it has been shown to artificially increase the variance of the metaanalytic effect size and, therefore, the level of heterogeneity 
(Moeyaert et al., 2017)
. Consequently, some of the high level of heterogeneity among previous meta-analyses can be attributed to the approach meta-analyses took towards statistical dependence of effect sizes. As for the issue of baseline performance differences, the overall lack of data on heterogeneity prevented an evaluation of the impact of this issue.
Next to these three issues identified a priori, the assessments of the quality of evidence conducted by 
Pappa et al. (2020)
 and 
Nguyen et al. (2019)
, also revealed that double blind allocation of treatment is a common methodological shortcoming in cognitive training studies. Only 
Nguyen et al. (2019)
, have assessed how adherence to a rigorous blinding procedure influences the results of studies, however. They found that it had no effect.
Taken together, these issues illustrate that methodological shortcomings have frequently affected the conclusions of previous meta-analyses -both in terms of their summary effect size and in terms of their level of heterogeneity. Due to a lack of data, the impact of these issues could not be assessed for either, however.


Publication bias
In addition to methodological quality and conceptual inconsistencies, the prevalence of publication bias was assessed across the included meta-analyses. Out of the 5 effect sizes for which publication bias information was available, 4 reported to have found no evidence for publication bias. Given the overall lack of publication bias information, however, it is unclear if this pattern is representative of the wider literature.


Overall conclusions on transfer effects
After consideration of the above factors, the following conclusions can be drawn about the transfer effects of executive functions and working memory training:
1. Small, statistically significant results can be observed for transfers between executive functions and general short-term memory tasks. The upper bound of the transfer effects is g = 0.18 (p<0.05). No lower bound could be determined. 2. Results on transfer effects between different executive functions or general short-term memory narrow abilities vary widely, ranging from g = 0.08 (p < 0.001) or smaller to g = 0.6 (p < 0.05) or higher. The magnitude of transfer effects seems to vary with age and type of transfer with stronger effects for older adults and transfer between tasks of different narrow abilities of general short-term memory. 3. Results on transfer effects between tasks from the same narrow ability range from moderate (g = 0.35, p < 0.05; or higher) to high (g = 0.62, p < 0.001).
These results suggest that while some generalisation of cognitive improvement can be achieved its scope is narrow with the strongest and most consistent improvements being observed for transfer between tasks of the same narrow ability.


Limitations
A key limitation that affects both this umbrella review and the wider literature is a lack of outcomes focused on real-life behaviour 
(Schmiedek, 2016)
. Ultimately, only effects on these outcomes can demonstrate the merit of cognitive trainings.
Another important point that this umbrella review does not address is the question on the duration of effects. As fade-out effects are a common issue in training interventions in general 
(Bailey et al., 2020)
, they should be expected for cognitive trainings as well.


Implications for further research
While, the remapping of tasks to cognitive outcomes provided substantiation to the notion that inconsistent task outcome mapping is a common problem across the cognitive training literature, further research is needed to determine the impact this inconsistency has on the results of previous meta-analyses. The dataset of remapped tasks that was created for this umbrella review could serve as the foundation of this research. Moreover, to simplify the process of research synthesis going forward, scholars are encouraged to align on a common taxonomy to map task to cognitive outcomes. The taxonomy developed by 
Webb et al. (2018)
 seems to provide a good starting point for that.
Similarly, further research is necessary to evaluate the amount of heterogeneity that can be explained by the inclusion of active and passive control group designs, failure to account for performance differences at baseline and difference in approaches to handle statistical dependence between previous meta-analyses. A key next step, in this regard, is contacting the authors of the included meta-analyses for the missing heterogeneity information.
Finally, given the varied findings on transfer effects between different narrow abilities for both executive functions and general short-term memory, future research should prioritise investigating the moderators of these type of transfer effects.


Conclusion
This umbrella review showed that methodological shortcomings and conceptual inconsistencies are common among meta-analyses of single-domain executive functions or working memory training focused on adults. A lack of on heterogeneity data and small number of effect sizes that are comparable to each other in terms of the type of transfer they relate to, precluded a comprehensive assessment of the impact of these factors on the results of the meta-analyses. Nevertheless, when only considering the meta-analyses with the highest methodological quality, this umbrella review finds that transfer effects between executive functions and working memory are small, transfer effects between different narrow abilities vary widely from small to large and, finally, transfer effects between tasks from the same narrow ability vary from moderate to large. 
Figure 1 -
1
PRISMA 2020 flowchart


Table 1 -Overview of taxonomy by Webb et al. (2018) Broad ability Narrow ability
1
Fluid reasoning (GF)
Abstract reasoning (AR)
Verbal reasoning (VR)
Long-term memory and
Learning/ Encoding efficiency (GL)
retrieval (GLR)
Retrieval fluency (GR)
General short-term memory
High working memory (High WM)
(GSM)
Low working memory (Low WM)
Short-term memory (STM)
Executive functions (EF)
Updating
Shifting
Inhibition
Processing speed (GS)
Perceptual speed
Visual processing (GV)
Sensory perception
Visualisation (VZ)


Table 5 -Overview of most representative results per cluster
5
Cluster
Bounds of
representative effect
size(s)


Unless explicitly stated otherwise, "mixed" from now on refers to the different means between meta-analyses and general high level of heterogeneity together.


Percent executive functions tasks of all tasks contributing to effect size: Soveri et al. (2019) -20%, Nguyen et al. (2019) -59%


Weighted mean could not be calculated due to lack of information on the number of effect sizes for Nguyen et al. (2019) [3].


Lampit et al. (2014)
 did not differentiate between near and far transfer.








Appendix
Appendix 1 -Search strategy Cluster Concept (level 1)


Concept (level 2)
Appendix 3 -Arguments for not classifying transfers as "near" or "far" 
Noack et al. (2009)
 propose to classify learning transfer between two tasks as either far, near or nearer depending on their positions in the Cattell-Horn-Carroll model of cognitive abilities by 
Carroll (1993)
 depicted conceptually in figure 2. Learning transfer between two tasks was coded as "far" when two tasks stem from different broad abilities, "near" when both tasks are related to the same broad ability, and "nearer" when both tasks related to the same narrow ability. Working memory has long been considered as a broad ability (P. . The integration of the three-factor model of executive functions proposed by 
Miyake et al. (2000)
 into the Cattell-Horn-Carroll model is an active area of research, however (P. A. 
Webb et al., 2018)
. While 
Webb et al. (2018)
 propose to integrate executive functions as a separate broad ability that includes updating, inhibition and cognitive flexibility as narrow abilities,  suggest that the three executive functions are redundant concepts -at least within the framework of the CHC taxonomy -and can be merged with general processing speed (Gs) and general short-term memory (Gsm). Neither of these proposals was considered convincing enough to serve as a guideline. In the case of 
Webb et al. (2018)
, the authors provide no empirical analysis to back up their proposal. In the case of , the authors factor-analytically examined the datasets of seven influential psychological studies containing both tasks typically associated with CHC abilities and executive functions. Their analysis does not address the assessment of the divergent and convergent validity of narrow CHC abilities and executive functions, however, and therefore can neither reject nor confirm the possibility of a separate broad executive function ability 
(Webb et al., 2018)
.
With the integration of executive function in the CHC taxonomy being a matter of ongoing research, the model by 
Noack et al. (2009)
 could not be used to guide the classification of transfer distances.  
 














M
Arain






M
Haque






L
Johal






P
Mathur






W
Nel






A
Rais






R
Sandhu






S
Sharma


















Maturation of the adolescent brain


10.2147/NDT.S39776








Neuropsychiatric Disease and Treatment




9














Persistence and Fade-Out of Educational-Intervention Effects: Mechanisms and Potential Solutions




D
H
Bailey






G
J
Duncan






F
Cunha






B
R
Foorman






D
S
Yeager








Psychological Science in the Public Interest




21


2


















10.1177/1529100620915848














Challenges for an Interdisciplinary Consideration of Cognitive Training. New Directions for Child and Adolescent Development




D
P
Birney






















10.1002/cad.20087














The Pervasive Problem With Placebos in Psychology: Why Active Control Groups Are Not Sufficient to Rule Out Placebo Effects




W
R
Boot






D
J
Simons






C
Stothart






C
Stutts








Perspectives on Psychological Science




8


4


















10.1177/1745691613491271














Working memory training in older adults: Evidence of transfer and maintenance effects




E
Borella






B
Carretti






F
Riboldi






R
De
Beni




10.1037/a0020683








Psychology and Aging
















A Treasury of Buddhist Stories: From the Dhammapada Commentary




Brainhq










Buddhist Publication Society






BrainHQ from Posit Science








A Theory of Cognitive Abilities: The Three-Stratum Theory




J
B
Carroll








Human Cognitive Abilities: A Survey of Factor-Analytic Studies




















10.1017/CBO9780511571312.018






Cambridge University Press












Meta-analyzing dependent correlations: An SPSS macro and an R script




S
F
Cheung






D
K
Chan






.-S








Behavior Research Methods




46


2


















10.3758/s13428-013-0386-2














A Guide to Conducting a Meta-Analysis with Non-Independent Effect Sizes




M
W
Cheung






.-L




10.1007/s11065-019-09415-6








Neuropsychology Review




29


4
















Cognitive Assessment and Training Market-Global Forecast to 2025








#:~:text=%5B256%20Pages%20Report%5D%20The%20cognitive,29.3%25% 20during%20the%20forecast%20period










Markets and Markets








Comparison of cognitive change after working memory training and logic and planning training in healthy older adults




V
M
Goghari






L
Lawlor-Savage








Frontiers in Aging Neuroscience
















Doing Meta-Analysis in R




M
Harrer






P
Cuijpers






T
A
Furukawa






D
D
Ebert




















Cochrane Handbook for Systematic Reviews of Interventions




Thomas
Higgins






Chandler






Li
Cumpston






Welch




















Plea for routinely presenting prediction intervals in meta-analysis




J
Inthout






J
P A
Ioannidis






M
M
Rovers






J
J
Goeman




10.1136/bmjopen-2015-010247








BMJ Open




6


7














Integrating Models of Self-Regulation




M
Inzlicht






K
M
Werner






J
L
Briskin






B
W
Roberts








Annual Review of Psychology




72


















10.1146/annurev-psych-061020-105721














Integrating the switching, inhibition, and updating model of executive function with the Cattell-Horn-Carroll model




P
A
Jewsbury






S
C
Bowden






M
E
Strauss








Journal of Experimental Psychology. General




145


2


















10.1037/xge0000119














The Cattell-Horn-Carroll Model of Cognition for Clinical Assessment




P
Jewsbury






S
Bowden






K
Duff




10.1177/0734282916651360








Journal of Psychoeducational Assessment




35














Adolescent Maturity and the Brain: The Promise and Pitfalls of Neuroscience Research in Adolescent Health Policy




S
B
Johnson






R
W
Blum






J
N
Giedd




10.1016/j.jadohealth.2009.05.016








The Journal of Adolescent Health : Official Publication of the Society for Adolescent Medicine




45


3
















How to play 20 questions with nature and lose: Reflections on 100 years of brain-training research




B
Katz






P
Shah






D
E
Meyer








PNAS Proceedings of the National Academy of Sciences of the United States of America




115


40
















Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs




D
Lakens




10.3389/fpsyg.2013.00863








Frontiers in Psychology
















Computerized cognitive training in cognitively healthy older adults: A systematic review and meta-analysis of effect modifiers




A
Lampit






H
Hallock






M
Valenzuela




10.1371/journal.pmed.1001756








PLoS Medicine




11


11














There is no convincing evidence that working memory training is effective: A reply to




M
Melby-Lervåg






C
Hulme




Karbach and Verhaeghen


















10.3758/s13423-015-0862-z








Psychonomic Bulletin & Review




23


1


















A
Miyake






N
P
Friedman






M
J
Emerson






A
H
Witzki






A
Howerter






T
D
Wager


















The Unity and Diversity of Executive Functions and Their Contributions to Complex "Frontal Lobe" Tasks: A Latent Variable Analysis






Cognitive Psychology




41


1
















10.1006/cogp.1999.0734














Methods for dealing with multiple outcomes in meta-analysis: A comparison between averaging effect sizes, robust variance estimation and multilevel meta-analysis




M
Moeyaert






M
Ugille






S
N
Beretvas






J
Ferron






R
Bunuan






W
V
Noortgate








International Journal of Social Research Methodology




20


6


















10.1080/13645579.2016.1252189














A gradient of childhood self-control predicts health, wealth, and public safety




T
E
Moffitt






L
Arseneault






D
Belsky






N
Dickson






R
J
Hancox






H
Harrington






R
Houts






R
Poulton






B
W
Roberts






S
Ross






M
R
Sears






W
M
Thomson






A
Caspi








Proceedings of the National Academy of Sciences




108


7


















10.1073/pnas.1010076108














Shifting minds: A quantitative reappraisal of cognitive-intervention research




D
Moreau








Perspectives on Psychological Science




16


1
















Immediate and long-term efficacy of executive functions cognitive training in older adults: A systematic review and meta-analysis




L
Nguyen






K
Murphy






G
Andrews




10.1037/bul0000196








Psychological Bulletin




145


7
















Cognitive plasticity in adulthood and old age: Gauging the generality of cognitive intervention effects




H
Noack






M
Lovden






F
Schmiedek






U
Lindenberger








Restorative Neurology and Neuroscience




27


5
















The PRISMA 2020 statement: An updated guideline for reporting systematic reviews




M
J
Page






J
Mckenzie






P
Bossuyt






I
Boutron






T
Hoffmann






C
Mulrow






L
Shamseer






J
Tetzlaff






E
Akl






S
E
Brennan






R
Chou






J
Glanville






J
Grimshaw






A
Hróbjartsson






M
Lalu






T
Li






E
Loder






E
Mayo-Wilson






S
Mcdonald






D
Moher






















Metaarxiv




10.31222/osf.io/v7gm2














Cognition and cancer treatment. Monitor on




C
Palmer








Psychology




52


2














Working memory updating training promotes plasticity & behavioural gains: A systematic review & metaanalysis




K
Pappa






V
Biswas






K
E
Flegal






J
J
Evans






S
Baylan








Neuroscience and Biobehavioral Reviews




118


















10.1016/j.neubiorev.2020.07.027


















V
Pergher






M
A
Shalchy






A
Pahor






M
M
Van Hulle






S
M
Jaeggi






A
R
Seitz


















Divergent Research Methods Limit Understanding of Working Memory Training


10.1007/s41465-019-00134-7








Journal of Cognitive Enhancement




4


1














The Hype Cycle of Working Memory Training




T
S
Redick




10.1177/0963721419848668








Current Directions in Psychological Science




28


5
















How Does it STAC Up? Revisiting the Scaffolding Theory of Aging and Cognition




P
A
Reuter-Lorenz






D
C
Park








Neuropsychology Review




24


3


















10.1007/s11065-014-9270-9














Working memory training does not enhance older adults' cognitive skills: A comprehensive meta-analysis




G
Sala






N
D
Aksayli






K
S
Tatlidil






Y
Gondo






F
Gobet




















10.1016/j.intell.2019.101386








Intelligence




77














Methods and Designs




F
Schmiedek




10.1007/978-3-319-42662-4_2








Cognitive Training: An Overview of Features and Applications


T. Strobach & J. Karbach




Springer International Publishing
















Integrating Hot and Cool Intelligences: Thinking Broadly about Broad Abilities




W
J
Schneider






J
D
Mayer






D
A
Newman




10.3390/jintelligence4010001








Journal of Intelligence




4


1














Working Memory Capacity and Fluid Intelligence: Maintenance and Disengagement




Z
Shipstead






T
L
Harrison






R
W
Engle




10.1177/1745691616650647








Perspectives on Psychological Science




11


6
















Do "brain-training" programs work?




D
J
Simons






W
R
Boot






N
Charness






S
E
Gathercole






C
F
Chabris






D
Z
Hambrick






E
A L
Stine-Morrow








Psychological Science in the Public Interest




17


3
















Working memory training revisited: A multi-level meta-analysis of n-back training studies




A
Soveri






J
Antfolk






L
Karlsson






B
Salo






M
Laine




10.3758/s13423-016-1217-0








Psychonomic Bulletin & Review




24


4
















Normal Development of Brain Circuits




G
Z
Tau






B
S
Peterson




10.1038/npp.2009.115








Neuropsychopharmacology




35


1
















Video game training enhances cognition of older adults: A meta-analytic study




P
Toril






J
M
Reales






S
Ballesteros








Psychology and Aging




29


3


















10.1037/a0037507














Are individual differences in attention control related to working memory capacity? A latent variable mega-analysis




N
Unsworth






A
L
Miller






M
K
Robison




10.1037/xge0001000








Journal of Experimental Psychology: General, No Pagination Specified-No Pagination Specified
















Meta-Analysis of the Effects of Computerized Cognitive Training on Executive Functions: A Cross-Disciplinary Taxonomy for Classifying Outcome Cognitive Factors




S
L
Webb






V
Loh






A
Lampit






J
E
Bateman






D
P
Birney




10.1007/s11065-018-9374-8








Neuropsychology Review




28


2
















Can Impaired Working Memory Functioning Be Improved By Training? A Meta-Analysis With a Special Focus on Brain Injured Patients




J
Weicker






A
Villringer






A
Thoene-Otto




10.1037/neu0000227








Neuropsychology




30


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]