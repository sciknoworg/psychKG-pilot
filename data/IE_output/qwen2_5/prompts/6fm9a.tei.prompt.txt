You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
In spite of bounds on space, time, and data, people are able to make good decisions in complex scenarios. What enables us to do so? And what might equip artificial systems to do the same? One essential ingredient for making complex problem solving tractable is a capacity for exploiting abstract representation.
To illustrate why abstractions are needed for adaptive decision making, consider hiking in a forest 
(Figure 1)
. A river runs from mountains in the north down south through the forest, and you have set up camp in a clearing just east of the river. As the sun begins to set, you find yourself west of the river and want to return to camp. How do you accomplish this task? You might reason, "Since I am on the west side of the river, I probably want to cross the river to get to the east side of the river and then walk towards my campsite." Such a compact thought is possible because you have abstract representations for that support decision making: These include the general notion of being in a location relative to the axis of the river, or the rough ideas of crossing the river and moving towards the campsite.
Without abstractions, decision making would be confined to specific, low-level states and actions-e.g., "At the current state, rotate your left leg 25 • , place it down beside the rock on the path, then swing your arm forward...". In a huge and complex scenario like navigating through a forest, this approach is both cognitively unwieldy and, in a fundamental sense, computationally intractable 
(Bellman, 1957;
Bylander, 1994
). The difficulty is due to decision-making costs that arise from two interrelated sources. First, agents need to learn about their environment, which costs time and experience (e.g., via exploration) 
(Kakade, 2003)
. Second, agents need to compute decision-theoretic quantities, which costs memory and thought (e.g., via planning) 
(Littman, Dean, & Kaelbling, 1995)
. As problems grow, these costs grow exponentially. Abstractions are essential because they enable decision makers to manage the growth of these costs.
Within psychology and neuroscience, abstraction plays a key role in the hierarchical learning and organization of motor sequences 
(Lashley, 1951;
Rosenbaum, Kenny, & Derr, 1983;
Rosenbaum, Inhoff, & Gordon, 1984)
, habits 
(Botvinick, 2008;
Dezfouli & Balleine, 2013;
Ribas-Fernandes et al., 2011)
, and planning 
(Solway et al., 2014;
Botvinick & Weinstein, 2014)
. For this reason, researchers in artificial intelligence (AI) and reinforcement learning (RL) have long been interested in how to leverage abstraction to provide human-like solutions to human-level problems 
(Dayan & Hinton, 1993;
Sacerdoti, 1974;
Dietterich, 2000;
Givan, Dean, & Greig, 2003;
Andre & Russell, 2002;
Barto & Mahadevan, 2003)
. In this paper, we discuss work in AI and RL that helps elucidate how abstractions support efficient decision making in both people and machines.
Research in AI and RL focuses on two broad types of abstraction. First, state abstractions treat certain configurations of the environment as similar by aggregating them or assigning them shared features. For instance, in the hiking example, being west of the river is not a single concrete configuration of the environment, but rather an abstract representation that covers many concrete states (e.g., being different distances west of the river). Second, temporal abstractions (often called "options") are temporally extended macro-actions that describe a general course of action. For example, the idea of crossing the bridge is not a single specific action, but rather captures many possible courses of action.
State and temporal abstractions enable compact representation of a domain. For instance, 
Figure 2a
 represents the hiking example as a graph where nodes are specific configurations of the environment (i.e., ground states) and edges are transitions to new configurations resulting from taking actions. The appropriate abstractions could induce the simpler problem representation in 
Figure 2b
. This new model ignores irrelevant details like the color of specific trees while capturing useful distinctions such as your location relative to the river. As a result, it supports efficient learning and requires little thought to reason about compared to the original version.
The foregoing discussion sketches out how abstractions can support efficient decision making. But how could it work in practice? Here, we dive into recent work in AI and RL that provides some answers. Specifically, we focus on three ways in which abstractions have been shown to facilitate efficient and scalable decision making. First, abstractions guide exploration and generalization by systematically modifying the distribution of learning experiences. In particular, abstractions can guide how agents explore and generalize based on environmental structure or representational simplicity. Second, abstractions facilitate efficient tradeoffs in learning. For example, they enable learning time and optimality to be exchanged as well as support optimal transfer between tasks. Finally, abstraction is essential for simplifying computation. For instance, abstractions influence the cost of computing a good plan by inducing simpler or more complex planning models. In the following sections, we discuss each of these benefits of abstraction. 


Abstractions Guide Exploration and Generalization
Suppose during your camping trip you want to learn to fish. At first, you try a few spots along the river at random. But soon, you notice a pattern: Certain areas have more vegetation, others have less. This information provides a new and more efficient way for you to organize your fishing attempts. For example, you could fish areas with high and low vegetation to gain a range of experiences about how it affects your catch. Or, on your first trip you may learn that your best catch was in high vegetation areas, so that on your second trip to a different river, you seek out similar areas. Here, the abstract concept of river vegetation guides your exploration in the current fishing task and tracks a generalizable feature relevant to future tasks, which both allow you to make better use of your limited time and experience.
In RL, abstractions similarly facilitate efficient learning by guiding exploration and generalization. But what is the basis of this guidance? Put another way, the concept of vegetation is useful, but what determines the identification of such a concept in general? Below, we discuss two ways in which abstractions can guide learning: domain structure and representational simplicity.


Domain Structure
Many domains have meaningful structure based on the topology of state connections. In the hiking example, being west of the river is an intuitive abstract state because all the locations west of the river are more connected to one another and less connected to locations east of the river. Early research in RL identified a number strategies for learning abstractions based on clustering or graph-theoretic notions like "bottleneck" states (Şimşek & 
Barto, 2004)
. More recent methods generalize such approaches to learn abstractions based on the state connectivity induced by a domain's structure. For instance, the successor representation 
(Dayan, 1993;
Gershman, Moore, Todd, Norman, & Sederberg, 2012;
Momennejad et al., 2017)
 and successor features 
(Barreto et al., 2017;
Kulkarni, Saeedi, Gautam, & Gershman, 2016)
 are state abstractions based on how likely an agent is to visit other states or features. A similar idea underpins eigenoptions 
(Machado, Bellemare, & Bowling, 2017;
Machado, Rosenbaum, et al., 2017;
Mahadevan, 2005)
. Specifically, both successor representations and eigen-options involve decomposing an a domain's structure into a set of "principal components" that compactly represent how certain regions of a domain are connected or will be visited.
Thus, being west of the river and being east of the river could emerge as successor-based state abstractions due to the connectivity and visitation of their respective ground states. A corresponding eigen-option would express the temporally abstract action of going from the west side to the east side since action sequences falling in this category capture a principal source of variance in connectivity. Notably, abstractions based on state connectivity encode information about the dynamics of a domain separate from its reward structure. These reward-agnostic representations can be learned either offline or during task learning, but, in either case, they provide an especially powerful and efficient way to adapt when rewards change or when transferring to tasks with related dynamics.


Representational Simplicity
Abstractions also guide exploration and generalization based on representational simplicity. Simplicity plays a key role in understanding representations in humans 
(Chater & Vitányi, 2003;
Sanborn, Bourgin, Chang, & Griffiths, 2018)
 and machines 
(Li & Vitányi, 2008)
. For finite decision makers, simplicity is important because it enables compression, the representation of information using less physical space (e.g., memory).
In RL, a number of methods have been developed that leverage a bias towards representational simplicity. For example, curiosity-driven learning 
(Schmidhuber, 1991;
Oudeyer, Kaplan, & Hafner, 2007;
Bellemare et al., 2016;
Pathak, Agrawal, Efros, & Darrell, 2017;
Ostrovski, Bellemare, Oord, & Munos, 2017;
Kulkarni, Narasimhan, Saeedi, & Tenenbaum, 2016
) pits a motivation to learn a simple, compressed representation of the world against a desire for new experiences that do not easily fit within that representation. Placing these two processes in competition with one another prevents the drive for simplicity from compressing everything into a single undifferentiated representation (the ultimate abstraction) that cannot support high quality decisions and instead pushes the system to adaptively explore a range of qualitatively different parts of a task. Doing so ultimately allows the architecture to both represent the world compactly and explore it efficiently.
Simplicity also plays a role in the Option-Critic framework 
(Bacon, Harb, & Precup, 2017;
Harb, Bacon, Klissarov, & Precup, 2018;
), a neural network architecture that simultaneously learns abstract macro-actions and the value of low-level actions that constitute them. By concurrently learning at multiple levels of abstraction, the agent acquires a representation that leads to better transfer to new tasks. The process at play is closely related to the notion of a "blessing of abstraction" in causal learning 
(Goodman, Ullman, & Tenenbaum, 2011)
 and regularization in supervised learning 
(Bishop, 2006)
: A bias towards simple, unifying representations not only saves on representational space, but also prevents overfitting to a particular task. An important consequence of this bias is that the agent is able to transfer to new tasks more efficiently.


Abstractions Facilitate Efficient Tradeoffs
Decision making and learning involve tradeoffs when space, time, and data are limited. But, it is important that these tradeoffs be made efficiently (i.e., without unnecessary costs). Here, we discuss how abstractions can efficiently trade off learning time and optimality, as well as minimize negative transfer between tasks.


Learning Time and Optimality
Imagine that every morning you wake up at your campsite and need to navigate to the bridge crossing the river. What kind of abstract states would be useful? It depends on how much time you have to spend learning. One option is to learn every tree, rock, and shortcut through the forest in great detail. Doing so has the benefit of ensuring you learn a highly efficient route to the bridge. An alternative strategy is to learn a less detailed representation of the forest that takes you to an obvious landmark-e.g., the river-and then take a simple path to the bridge. This second approach may be more costly in terms of the physical actions taken or time navigating, but is simpler and faster to learn. Depending on how much time and data are available, it may be better for the agent to pursue the second strategy than the first.
Learning the simpler but suboptimal policy depends on the resolution of the state abstractions used-i.e., the amount of detail one is willing to keep or ignore about the environment. In RL, a number of approaches use abstraction in this manner to efficiently balance optimality and learning time 
(Jiang, Kulesza, & Singh, 2015;
Ortner, 2013)
. For instance, if abstractions are represented by aggregating ground states into clusters, adaptively modulating the size of the clusters and granularity of the state representation controls this tradeoff 
(Taiga, Courville, & Bellemare, 2018;
Bellemare et al., 2016;
Mandel, Liu, Brunskill, & Popovic, 2016)
.


Optimizing Transfer between Tasks
Abstractions also facilitate effective transfer from one task to another. Transfer can be positive 
(Sutton, Precup, & Singh, 1999;
Konidaris & Barto, 2007;
Bacon et al., 2017;
Topin et al., 2015)
, but it can also be negative 
(Jong, Hester, & Stone, 2008)
. Ideally, an agent would learn abstractions that avoid negative transfer as much as possible while increasing the possibility of positive transfer. One setting in which this approach is feasible is in lifelong learning, in which an agent is assumed to be continuously receiving tasks from a distribution of tasks. Given separate phases of discovering and using temporal abstractions, it is possible to ensure that any learned options only help later learning and decision making 
(Brunskill & Li, 2014)
. Used properly, temporal abstractions can also guarantee that the amount of data needed for learning is reduced even within a single task 
Fruit, Pirotta, Lazaric, & Brunskill, 2017)
. For agents with limited time, such abstractions can play a critical role in ensuring efficient and effective use of data.


Abstractions Simplify Computation
Computation is costly for people 
(Griffiths, Lieder, & Goodman, 2015)
 as well as artificial agents 
(Russell, 1997)
. For example, if you were to plan a hike back to your campsite, you could simulate possible routes from your current state or "work backwards" from your final destination. These cognitive operations require time and mental effort. However, the total cost of computation depends directly on the model such operations occur over 
(Givan et al., 2003)
. Planning can be easier given a good abstract representation (e.g., compare 
Figure 2a
 and 
Figure 2b)
. Recent work in RL leverages this intuition to efficiently scale planning to complex domains. Here, we focus on two broad approaches: algorithmspecific abstractions, in which an abstraction scheme is tailored to take advantage of particular features of a planning algorithm, and end-to-end approaches, in which learning ab-stractions for planning occurs in the context of a more general learning architecture.


Algorithm-Specific Abstractions
Algorithms such as Monte Carlo Tree Search 
(Abramson, 1987;
Coulom, 2006;
Browne et al., 2012)
 are based on the notion of forward simulation: From an initial state, candidate plans are generated, evaluated, and the best is chosen. The major problem for these algorithms is branching: Each additional timestep into the future increases the number of plans to evaluate multiplicatively. To keep branching manageable, state-aggregation techniques simplify models by only storing relevant information. For instance, by treating all states with the same optimal action as identical, branching due to stochasticity in action outcomes can be reduced 
(Hostetler, Fern, & Dietterich, 2014;
Anand, Grover, Mausam, & Singla, 2015;
Anand, Noothigattu, Mausam, & Singla, 2016)
. Along similar lines, the model used for planning can be simplified by aggregating states that lead to the same possible outcomes within a few time steps 
(Jiang, Singh, & Lewis, 2014)
.
An alternative to forward simulation is backwards induction, which involves starting from possible future states and propagating information backwards to possible precursor states. Abstractions can also assist computation in backward induction. For instance, value iteration 
(Bellman, 1957)
 repeatedly "sweeps" over a set of states and backs up information to each precursor state. Since each iteration involves looking at every state in the planning model, it is extremely sensitive to the number of possible states. However, by incorporating temporal abstractions into the planning model, more information can be propagated further backwards on each sweep 
(Mann & Mannor, 2014)
 and it becomes possible to more efficiently trade off error in the approximation of future rewards with computation 
(Mann, Mannor, & Precup, 2015)
.
Finally, relaxing the goal of planning provides opportunities to leverage powerful abstractions. For instance, plan feasibility seeks to determine whether a plan can be constructed that satisfies some set of constraints. Since the goal is no longer optimality but simply satisfiability, relevant aspects of the task can be translated into symbolic logic and efficiently tested 
(Konidaris, Kaelbling, & Lozano-Perez, 2014
McDermott et al., 1998)
.


End-to-end Efficient Planning
Another approach is to learn planning abstractions while also learning everything else about a task in an end-to-end manner with neural networks. Using abstractions for planning is especially important because planning in a ground state space (e.g., the pixels of a video game) is only possible for short time horizons 
(Oh, Guo, Lee, Lewis, & Singh, 2015)
. To address these limitations, architectures have been developed that learn and use abstract representations for planning. In particular, the abstract representation needs to be trained to predict future rewards, thus preserving the most relevant information for decision making in the planning model 
(Oh, Singh, & Lee, 2017;
Silver et al., 2017)
.


Conclusion
Abstractions are key for efficiently scaling decision making. In particular, recent research in RL demonstrates how state and temporal abstractions guide exploration and generalization, facilitate efficient tradeoffs, and simplify computations. In the context of learning and decision making with limited resources, abstractions enable agents to strategically manage finite space, time, and data, which is necessary for scaling problem solving to complex tasks.
For psychologists, this analysis raises questions about why biological agents like humans have certain types of abstract representations. For instance, humans and other organisms may have certain abstractions not only because they facilitate effective exploration and generalization but because they support fast planning or modulate critical tradeoffs during learning. Future work in both computer science and psychology will need to identify other pressures that can shape abstraction learning-such as the need to communicate and coordinate with others-to offer a clearer understanding of the value of abstraction.


Conflict of Interest Statement
The authors declare no conflict of interest.


Annotated References
• (**) (Bacon et al., 2017) -Proposes option-critic architecture that demonstrates the effectiveness of jointly learning abstract and low-level policies.
• (*) 
(Jiang et al., 2015)
 -Analyzes how state aggregation learning schemes can be used to efficiently trade off data and solution quality, which is key when data is costly or finite.
• (*) 
(Machado, Rosenbaum, et al., 2017
) -Combines learning eigen-options with learning successor representations, which both leverage the transition structure of a domain for learning abstract representations.
• (**) 
(Pathak et al., 2017)
 -Implements curiosity as an intrinsic reward signal based on an agent's error when predicting the outcome of actions to enable effective representation learning even in the absence of external rewards.
• (*) 
(Oh et al., 2017)
 -Proposes the Value Prediction Network architecture that learns an internal model composed of abstract states for efficient model-based planning.
• (*) 
(Barreto et al., 2017)
 -Introduces a generalization of successor representations 
(Dayan, 1993)
 that enables efficient transfer across tasks with shared causal structure.
Figure 1 .
1
An environment, an agent, and the agent's abstract representation of a domain.


Figure 2 .
2
(a) A domain in all of its complexity, and (b) the representation induced by an effective set of state and temporal abstractions for this domain. The representation in (b) facilitates efficient learning and computation, unlike the one in (a).














The Expected-Outcome Model of Two-Player Games (Unpublished doctoral dissertation)




B
D
Abramson








New York, NY, USA






Columbia University














A
Anand






A
Grover






Mausam






P
Singla




ASAP-UCT: Abstraction of State-Action Pairs in UCT. Twenty-Fourth International Joint Conference on Artificial Intelligence


















A
Anand






R
Noothigattu






Mausam






P
Singla




OGA-UCT: On-the-Go Abstractions in UCT. Twenty-Sixth International Conference on Automated Planning and Scheduling
















State Abstraction for Programmable Reinforcement Learning Agents




D
Andre






S
J
Russell








Eighteenth national conference on artificial intelligence


Menlo Park, CA, USA




American Association for Artificial Intelligence
















The option-critic architecture




P.-L
Bacon






J
Harb






D
Precup








Proceedings of the conference aaai


the conference aaai


















Successor features for transfer in reinforcement learning




A
Barreto






W
Dabney






R
Munos






J
J
Hunt






T
Schaul






H
P
Van Hasselt






D
Silver








Advances in Neural Information Processing Systems


















Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems




A
G
Barto






S
Mahadevan








13














Unifying count-based exploration and intrinsic motivation




M
Bellemare






S
Srinivasan






G
Ostrovski






T
Schaul






D
Saxton






R
Munos








Advances in Neural Information Processing Systems


















Dynamic programming




R
Bellman








Princeton University Press














C
M
Bishop




Pattern Recognition and Machine Learning




Springer














Hierarchical models of behavior and prefrontal function




M
M
Botvinick




10.1016/J.TICS.2008.02.009






Trends in Cognitive Sciences




12


5
















Model-based hierarchical reinforcement learning and human action control




M
M
Botvinick






A
Weinstein




10.1098/rstb.2013.0480






Philosophical Transactions of the Royal Society B: Biological Sciences




369




















C
B
Browne






E
Powley






D
Whitehouse






S
M
Lucas






P
I
Cowling






P
Rohlfshagen














A survey of monte carlo tree search methods




S
Colton








IEEE Transactions on Computational Intelligence and AI in games




4


1
















PAC-inspired option discovery in lifelong reinforcement learning




E
Brunskill






L
Li








International conference on machine learning


















The computational complexity of propositional STRIPS planning




T
Bylander








Artificial Intelligence




69
















Simplicity: a unifying principle in cognitive science?




N
Chater






P
Vitányi




10.1016/S1364-6613(02)00005-0






Trends in Cognitive Sciences




7


1
















Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search




R
Coulom








International conference on computers and games


















Improving generalization for temporal difference learning: The successor representation




P
Dayan








Neural Computation




5


4
















Feudal reinforcement learning




P
Dayan






G
E
Hinton








Advances in Neural Information Processing Systems


















Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized




A
Dezfouli






B
W
Balleine




10.1371/journal.pcbi.1003364






PLoS Computational Biology




9


12














Hierarchical reinforcement learning with the maxq value function decomposition




T
G
Dietterich








Journal of Artificial Intelligence Research




13
















Exploration-exploitation in MDPs with options




R
Fruit






A
Lazaric




arXiv:1703.08667










arXiv preprint








Regret minimization in MDPs with options without prior knowledge




R
Fruit






M
Pirotta






A
Lazaric






E
Brunskill








Advances in Neural Information Processing Systems


















The successor representation and temporal context




S
J
Gershman






C
D
Moore






M
T
Todd






K
A
Norman






P
B
Sederberg








Neural Computation




24


6
















Equivalence notions and model minimization in markov decision processes




R
Givan






T
Dean






M
Greig








Artificial Intelligence




147


1-2
















Learning a theory of causality




N
D
Goodman






T
D
Ullman






J
B
Tenenbaum








Psychological review




118


1


110














Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic




T
L
Griffiths






F
Lieder






N
D
Goodman




10.1111/tops.12142






Topics in Cognitive Science




7


2
















When Waiting Is Not an Option: Learning Options With a Deliberation Cost




J
Harb






P.-L
Bacon






M
Klissarov






D
Precup








Thirty-Second AAAI Conference on Artificial Intelligence
















State Aggregation in Monte Carlo Tree Search




J
Hostetler






A
Fern






T
Dietterich








7












Abstraction selection in model-based reinforcement learning




N
Jiang






A
Kulesza






S
Singh








Proceedings of the 32nd international conference on machine learning


F. Bach & D. Blei


the 32nd international conference on machine learning
Lille, France




PMLR




37














Improving UCT planning via approximate homomorphisms




N
Jiang






S
Singh






R
Lewis








Proceedings of the 2014 international conference on autonomous agents and multi-agent systems


the 2014 international conference on autonomous agents and multi-agent systems


















The utility of temporal abstraction in reinforcement learning




N
K
Jong






T
Hester






P
Stone








Proceedings of the Conference on Autonomous Agents and Multiagent Systems


the Conference on Autonomous Agents and Multiagent Systems


















On the sample complexity of reinforcement learning (Unpublished doctoral dissertation)




S
M
Kakade












University of London, England












Building portable options: Skill transfer in reinforcement learning




G
Konidaris






A
G
Barto








Proceedings of the International Joint Conference on Artificial Intelligence


the International Joint Conference on Artificial Intelligence






7














Constructing symbolic representations for high-level planning




G
Konidaris






L
P
Kaelbling






T
Lozano-Perez








Aaai




















G
Konidaris






L
P
Kaelbling






T
Lozano-Perez








From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning






61














Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation




T
D
Kulkarni






K
Narasimhan






A
Saeedi






J
Tenenbaum








Advances in Neural Information Processing Systems




















T
D
Kulkarni






A
Saeedi






S
Gautam






S
J
Gershman




arXiv:1606.02396


Deep successor reinforcement learning










arXiv preprint








The problem of serial order in behavior




K
S
Lashley








Cerebral mechanisms in behavior; the hixon symposium


Oxford, England




Wiley
















An Introduction to Kolmogorov Complexity and Its Applications




M
Li






P
Vitányi




10.1007/978-0-387-49820-1






Springer


New York, NY; New York












On the complexity of solving markov decision problems




M
L
Littman






T
L
Dean






L
P
Kaelbling








Proceedings of the eleventh conference on uncertainty in artificial intelligence


the eleventh conference on uncertainty in artificial intelligence




















M
Liu






M
C
Machado






G
Tesauro






M
Campbell




arXiv:1712.04065


The eigenoption-critic framework










arXiv preprint








A laplacian framework for option discovery in reinforcement learning




M
C
Machado






M
G
Bellemare






M
Bowling




arXiv:1703.00956










arXiv preprint










M
C
Machado






C
Rosenbaum






X
Guo






M
Liu






G
Tesauro






M
Campbell




arXiv:1710.11089


Eigenoption discovery through the deep successor representation










arXiv preprint








Proto-value functions: Developmental reinforcement learning




S
Mahadevan








Proceedings of the 22nd international conference on machine learning


the 22nd international conference on machine learning


















Efficient bayesian clustering for reinforcement learning




T
Mandel






Y.-E
Liu






E
Brunskill






Z
Popovic








Proceedings of the International Joint Conference on Artificial Intelligence


the International Joint Conference on Artificial Intelligence
















Scaling up approximate value iteration with options: Better policies with fewer iterations




T
Mann






S
Mannor








International conference on machine learning


















Approximate value iteration with temporally extended actions




T
Mann






S
Mannor






D
Precup








Journal of Artificial Intelligence Research




53
















Pddl-the planning domain definition language




D
Mcdermott






M
Ghallab






A
Howe






C
Knoblock






A
Ram






M
Veloso






.
.
Wilkins






D


















The successor representation in human reinforcement learning




I
Momennejad






E
M
Russek






J
H
Cheong






M
M
Botvinick






N
D
Daw






S
J
Gershman




10.1038/s41562-017-0180-8






Nature Human Behaviour




1


9




















J
Oh






X
Guo






H
Lee






R
L
Lewis






S
Singh


















Action-conditional video prediction using deep networks in atari games






Advances in Neural Information Processing Systems
















Value prediction network




J
Oh






S
Singh






H
Lee








Advances in Neural Information Processing Systems


















Adaptive aggregation for reinforcement learning in average reward Markov decision processes




R
Ortner








Annals of Operations Research




208


1
















Count-based exploration with neural density models




G
Ostrovski






M
G
Bellemare






A
Oord






R
Munos








International conference on machine learning


















Intrinsic motivation systems for autonomous mental development




P.-Y
Oudeyer






F
Kaplan






V
V
Hafner








IEEE transactions on evolutionary computation




11


2
















Curiosity-driven Exploration by Self-supervised Prediction




D
Pathak






P
Agrawal






A
A
Efros






T
Darrell








Proceedings of the 34th international conference on machine learning


the 34th international conference on machine learning




JMLR




70














A Neural Signature of Hierarchical Reinforcement Learning




J
Ribas-Fernandes






A
Solway






C
Diuk






J
Mcguire






A
Barto






Y
Niv






M
Botvinick




10.1016/J.NEURON.2011.05.042






Neuron




71


2
















Choosing between movement sequences: A hierarchical editor model




D
A
Rosenbaum






A
W
Inhoff






A
M
Gordon




10.1037/0096-3445.113.3.372






Journal of Experimental Psychology: General




113


3
















Hierarchical control of rapid movement sequences




D
A
Rosenbaum






S
B
Kenny






M
A
Derr




10.1037/0096-1523.9.1.86






American Psychological Association


9












Planning in a hierarchy of abstraction spaces




S
J
Russell






E
D
Sacerdoti




10.1016/S0004-3702(97)00026








Artificial Intelligence




94


1-2










Artificial Intelligence








Representational efficiency outweighs action efficiency in human program induction




S
Sanborn






D
D
Bourgin






M
Chang






T
L
Griffiths








Proceedings of the 40th annual meeting of the cognitive science society


T. Rogers, M. Rau, X. Zhu, & C. Kalish


the 40th annual meeting of the cognitive science society
Austin, TX




Cognitive Science Society
















A possibility for implementing curiosity and boredom in model-building neural controllers




J
Schmidhuber








Proc. of the international conference on simulation of adaptive behavior: From animals to animats


of the international conference on simulation of adaptive behavior: From animals to animats


















The predictron: end-to-end learning and planning




D
Silver






H
Van Hasselt






M
Hessel






T
Schaul






A
Guez






T
Harley








JMLR






Degris, T








Using relative novelty to identify useful temporal abstractions in reinforcement learning




Ö
Simşek






A
G
Barto




10.1145/1015330.1015353






Twenty-first international conference on machine learningicml '04


New York, New York, USA




ACM Press




95












Optimal Behavioral Hierarchy




A
Solway






C
Diuk






N
Córdova






D
Yee






A
G
Barto






Y
Niv






M
M
Botvinick




10.1371/jour-nal.pcbi.1003779






PLoS Computational Biology




10


8


1003779














Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning




R
S
Sutton






D
Precup






S
Singh








Artificial Intelligence




112


1
















Approximate exploration through state abstraction




A
A
Taiga






A
Courville






M
Bellemare












ICML Workshop on Exploration in RL








Portable option discovery for automated learning transfer in object-oriented Markov decision processes




N
Topin






N
Haltmeyer






S
Squire






J
Winder






M
Desjardins






J
Macglashan








Proceedings of the International Joint Conference on Artificial Intelligence


the International Joint Conference on Artificial Intelligence



















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]