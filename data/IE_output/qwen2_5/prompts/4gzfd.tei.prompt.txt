You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
From applying for a research grant to giving one's all to earn a bonus at work, numerous human activities can either be crowned by dramatic success or thwarted by failure. In these "make-or-break" endeavors, people are either handsomely rewarded upon success or gain nothing upon failure. Although there is almost always a monotonic relationship between the invested time (or effort) in an activity and the expected levels of performance, there is considerable uncertainty about the outcomes (e.g., success or failure). Thus, some of the most important decisions in life may be defined by how one chooses to allocate limited resources between make-or-break tasks-with potentially life changing outcomes-and "safe" alternatives, where outcomes are a more predictable function of performance (e.g., the linear relationship a wage worker experiences between hours worked and income). How should you divide your time between chasing a challenging goal and investing in safer occupations? What is the optimal allocation strategy, and how does it compare to simpler, but computationally less expensive strategies? What behaviors do the different strategies produce across decision-making settings?


Background
As early as the 1930s, Kurt Lewin and colleagues identified the importance of success-or-failure reward structures for understanding human behavior 
(Hoppe, 1931;
Lewin, 1936)
. They investigated how people with different abilities set their aspiration levels 
(Festinger, 1942;
Rotter, 1942)
, calibrated based on the chances of success and the potential rewards to be gained. Lewin and colleagues postulated that people experience joy when they achieve their aspirations and feel acute loss when they fail, and that they choose the appropriate aspiration level simply by balancing these two motivating forces and the probability of success 
(Lewin, Dembo, Festinger, & Sears, 1944;
Siegel, 1957)
. In their work, they assumed that people strictly commit themselves to a single aspiration level selected from a continuum of possible choices (i.e., a single task) and that there are no constraints on the amount of time or effort that could be invested (i.e., an infinite horizon). This early work provided valuable insights into how the likelihood of success influences how people set aspiration levels. Yet how do these insights transfer to problems where there is a finite horizon (i.e., limited time to be allocated) and multiple tasks with different payoff functions?
Since then, there have been repeated attempts in the behavioral sciences-independent of or inspired by Lewin's framework-to address how people reason about their chances of success in meeting their goals (e.g., 
Bandura, 1977;
Heider, 1958;
Weiner & Kukla, 1970)
, how aspiration levels affect people's choices 
(Diecidue & Van De Ven, 2008
; J. W. 
Payne, Laughhunn, & Crum, 1980;
Simon, 1955)
, and how people choose among variable levels of aspiration 
(Atkinson, 1957
). Yet there has been no rigorous mathematical formalization of the problem people grapple with when they have to allocate scarce resources between a make-or-break activity, where investment of additional effort can alter their chances of success, and a safe alternative 1 . The closest attempt to formally ground the problem was a static effort allocation framework developed by 
Kukla (1972)
, who greatly simplified the problem by assuming away any uncertainty in the relation between effort and performance. Kukla revealed a distinct discontinuous relationship between task difficulty and the optimal allocation of effort; people should refrain from engaging in very difficult tasks, but also invest more effort when success is marginally within reach. Despite its simplicity, the setting Kukla described is of broad interest, because it exposes the challenging nature of a common class of effort alloca- 
1
 Before the introduction of formal decision theory 
(von Neumann & Morgenstern, 1944)
 to psychology by 
Edwards (1953;
1954a)
, there were several attempts by psychologists to introduce a formal theory of valence 
(Meehl & MacCorquodale, 1951)
; many of these attempts were inspired by the study of decision problems characterized by success-failure reward structures. This body of research, commonly referred to as expectancy theory, turned out to be quasi-equivalent to subjective expected utility theory 
(Feather, 1959;
Siegel, 1957)
, but lacked the theoretical coherence and elegance of the latter, and waned in influence over time. tion problems encountered in daily life. Similar discontinuities between the parameters that characterize the decision-making problem (i.e., rewards, abilities, constraints) and the normatively prescribed or observed behavior (i.e., time allocation) have been uncovered in various domains, such as allocating time among learning tasks 
(Son & Sethi, 2006)
, effort in writing prestigious grants 
(Bol, de Vaan, & van de Rijt, 2018)
 and allocating attention in perceptual decisions 
(Morvan & Maloney, 2012)
, potentially hinting at a common underlying problem structure.
The notion that optimal solutions for even simple problems may harbor drastic discontinuities as a function of the task parameters contrasts with the main time allocation paradigms in economics and psychology, which assumes that different activities, often defined as effort and leisure, complement each other 
(Becker, 1965;
Borjas & Van Ours, 2000;
Kurzban, Duckworth, Kable, & Myers, 2013;
Varian, 1978)
. Such resource allocation problems (called "convex" in the language of mathematics) are easy to solve using standard optimization techniques 
(Boyd & Vandenberghe, 2004)
, and the changes in the optimal solutions are smooth as we gradually shift the parameters of the problem. In a similar vein, optimal foraging theory (the most prominent time allocation paradigm in behavioral biology) is also a convex optimization problem where the foraging time allocated across different resource patches depends on immediate rewards and the rate at which these rewards diminish 
(Charnov, 1976)
. Although these modeling frameworks have been productively used to describe behavior and prescribe how to allocate resources across a wide range of domains, ranging from mental effort allocation 
(Kool & Botvinick, 2014)
 to cognitive search (S. J. 
Payne, Duggan, & Neth, 2007;
Pirolli & Card, 1999)
, they cannot account for the behavioral dynamics produced by success-failure reward structures. In many real-life problems returns directly depend on how additional efforts translate into improved chances of success in achieving a make-or-break goal.
Make-or-break tasks also provide a new and valuable perspective for studying risk-taking behavior in dynamic environments. Alternative theories of risk-taking are empirically tested using binary experiments asking people to make consequential or hypothetical choices among a number of discrete risky options 
(Edwards, 1954b;
Kahneman & Tversky, 1979;
O'Donoghue & Somerville, 2018)
. Over the years, the models explaining empirical irregularities became increasingly more complex, adding further parameters and specifications to account for the largest number of choices. In real-life however, people take risks dynamically, and the decision to keep on investing in a risky challenge can be daunting. Can risk-taking theories from the static domain be easily transferred to risky dynamic goal-pursuit problems? Accordingly, how could one implement an artificial agent to be risk averse in the dynamic decision-making setting 
(Feldman & Sproull, 1977)
? In challenging real-world environments, where calculating the optimal strategy is computationally excessive, existing expected and non-expected utility theories need to be cognitively simplified to become psychologically plausible (e.g., 
Ebert & Strack, 2015)
. Here we explore one promising avenue. Risk-averse behavior, and other distinctive behavioral patterns (e.g., temporal discounting and perseverance) can be produced by leveraging boundedly rational principles, such as myopia or simple heuristics.


Contributions and Scope
The first contribution of this article is to develop a formal framework that accounts for the role of uncertainty and information in resource allocation problems involving a make-or-break activity and a safe alternative.
Our framework draws inspiration from Lewin's early work on aspiration levels (e.g., 
Lewin et al., 1944
) and 
Kukla's (1972)
 static model of effort allocation, but with a radical departure in how uncertainty accumulates with larger investments of effort or time. We show that manipulating critical factors in the task, such as the rewards or constraints, can produce abrupt and discontinuous changes in the normative allocation strategy, with uncertainty playing a crucial role in determining the optimal solution. We describe two variants of the problem. In the one-shot allocation problem, the decision maker receives feedback about performance only at the very end of the task, after all available time has been allocated. In this formulation of the problem, performance is unobservable during the allocation phase, as is the case when studying for a pass/fail exam or preparing a grant application. In the dynamic allocation problem, the decision maker receives immediate feedback on their current output and-like a PhD candidate toiling to fulfill graduation requirements before funding runs out-can dynamically adapt their allocation strategies at any point in time, either continuing to pursue the make-or-break goal or dropping out in favor of the safe alternative.
The second contribution of this article is to introduce the optimal solution for the dynamic version of the problem. In line with previous theoretical work on dynamic decision-making across disciplines in the behavioral and social sciences 
(Dixit & Pindyck, 1994;
Malhotra, Leslie, Ludwig, & Bogacz, 2017;
McCardle, Tsetlin, & Winkler, 2016)
 we show that the solution can be expressed by optimal decision thresholds. We mathematically prove that optimally solving the dynamic allocation problem implies prioritizing investment in the make-or-break task over the safe rewards task (whenever investing in the make-or-break task is considered profitable) and switching unidirectionally to the safe alternative only when performance falls below the giving-up threshold or when success has been reached. This allows us to use well-known results from optimal stopping theory in stochastic processes 
(Shiryaev, 2007)
 as well as standard numerical methods to calculate the optimal giving-up strategy. Following previous work on optimal stopping, we discretize time and derive the solution by backwards induction. We show that acting optimally implies using a more tolerant giving-up threshold as uncertainty in the environment increases, echoing results from the dynamic investing literature in finance and economics 
(McDonald & Siegel, 1986)
. The optimal strategy provides insights to the nature of the decision-making environment. Yet it is inaccessible to humans, due to its exorbitant computational costs 
(Simon, 1990)
. This raises the question of how to constrain the space of computational level theories in order to define cognitively plausible strategies that are available to human decision-makers 
(Marr, 1982;
Van Rooij, 2008)
.


The third contribution of this article is to analyze how different boundedly rational strategies perform
relative to the optimal solution across decision-making environments. First, we define a myopic giving-up strategy, which is based on the optimal solution to the one-shot allocation problem. This myopic solution implies giving up earlier than the fully optimal strategy, because it does not consider the possibility of dynamically reassessing one's policy based on new information acquired during the task (i.e., direct feedback about performance). Myopic giving-up becomes more conservative relative to the optimal strategy as uncertainty increases, yielding a distinct pattern of risk-aversion. Second, we examine the play-to-win heuristic, a simple heuristic strategy that only decides whether or not to invest in the make-or-break task, and then either abandons the task entirely or stubbornly perseveres until success or failure occurs. When contrasted with optimal giving-up, this strategy produces risk-seeking behavior that is consistent with the sunk cost fallacy 
(Arkes & Blumer, 1985)
. Finally, we define the control points strategy, a generalization of the above two strategies, which considers giving-up at a fixed number of time points. The strategy allows for a more flexible trade-off of computational complexity and dynamic responsiveness. Holding all other factors constant, we find that an increase of uncertainty in the environment improves the relative performance of the play-towin and control points strategies, which despite disregarding most information, almost always outperform the myopic giving-up strategy and can approximate the performance of the optimal solution.
The structure of this paper is as follows: we begin by developing the mathematical framework for the allocation problem and describing how different specifications of the decision environment can change the reward structure of the make-or-break task. We then describe the optimal solution to the one-shot allocation problem before moving on to the dynamic version of the problem. We present both optimal and myopic solutions to the dynamic problem, and we compare them to simpler heuristic strategies. In the Discussion, we draw connections with the dynamic decision-making literature from mathematical psychology and economics, with expected and non-expected utility theories, and with theories of effort allocation, motivation, and learning. These links highlight a rich set of opportunities for further research and contributions.


A Formal Framework
Let us consider an agent who can allocate time (or any another resource) t m and t s between two different reward-generating activities {m, s}, where m is an instance of a make-or-break task with a binary successfailure outcome, and s is an instance of a safe task, where rewards are a more predictable function of the allocated time. We assume that the total available time to be allocated T is fixed, with T = t m + t s .
We introduce a function describing how invested time maps onto performance quality (Equation 1) and a function mapping performance quality onto rewards for each reward-generating activity (Equation 2 and 3).
Intuitively, the quality (or quantity) of performance q(t) in any given task is a continuous function of the allocated time t and depends on the abilities of the agent, starting from q(0) = 0. Working on a task, however, is often accompanied by uncertainty about future performance. For instance, people may commit unintended and random errors (e.g., incorrect retrieval of information from memory during an exam), where these types of errors can described using a stochastic function. It is reasonable to assume that the change in performance ∆ q over an interval of time ∆t is a Gaussian random variable, independent of past performance, and that this random change ∆ q depends only on the magnitude of ∆t (i.e., time homogeneity). The unique function that satisfies all of the above assumptions is a diffusion process given by:
q(t) = λt + σW t
(1)
The first term in Equation 1 is a deterministic term and is proportional to time, where λ is the skill level of the agent. Larger values of λ lead to better performance for the same amount of time. The second term is a random term, where σ is the performance uncertainty defining the variance per unit of time, and W t is the Wiener process. A Wiener process is the analog of a random walk for continuous time, where for each t, it yields a Gaussian random variable W t ∼ N (0,t), with a mean of 0 and variance equal to t. In the same way that the variance of a random walk increases with the number of steps, the variance of a Wiener process also increases with time t. As shown in 
Figure 1
, the expectation of performance quality (dotted line) is affected only by the skill level λ and the time t, whereas the underlying uncertainty (confidence band) is determined by the performance uncertainty σ and grows with larger investments of time t. 


Rewards as a Function of Performance
Given a specific reward-generating activity (e.g., writing a grant application or working for an hourly wage), we can describe a function mapping performance onto rewards. Here, we consider two different types of functions, g m (x) for the make-or-break task and g s (x) for the safe task, where x = q(t) is the performance function described in Equation 1.
2.1.1 Make-or-break.
In the make-or-break setting, the agent either receives a considerable reward B upon success, or nothing upon failure, depending on whether their performance x = q(t) reaches a precise and known success threshold ∆:
g m (x) =    B, if x ≥ ∆ 0, otherwise
(2)
The performance threshold ∆ captures the strict binary outcomes of success or failure and controls the difficulty of the make-or-break task. The reward B may consist, for example, in receiving a monetary bonus at work, winning an award, receiving a grant, or the joy of reaching an aspiration 2 . Note that it is not necessary for B to be a known value, but could also be sampled from some known distribution, in which case B would denote the expected value of the reward.


Safe reward.
In the safe-reward setting, rewards are a linear function of performance quality, with reward rate v corresponding to an hourly wage or a fixed rate of reward as a function of performance quality x = q(t):
g s (x) = v • x (3)


Rewards as a Function of Resource Allocation
We can now describe the stochastic processes of earning rewards as functions mapping time allocations t m and t s onto rewards r m and r s :
r m (t m ) = g m (q m (t m )) = g m (λ m t m + σ m W m t m ) =    B, if λ m t m + σ m W m t m ≥ ∆ 0, otherwise (4) r s (t s ) = g s (q s (t s )) = v • q s (t s ) = v • (λ s t s + σ s W s t s )
(5)
The expectation of rewards for the make-or-break task can be written as:
E[r m (t m )] = B • P(λ m t m + σ m W m t m ≥ ∆) = B • P 1 √ t m W m t m ≥ ∆ − λ m t m σ m √ t m = B • 1 − Φ ∆ − λ m t m σ m √ t m
(6)
where E[•] denotes expectation, P(•) denotes probability and Φ denotes the cumulative density function (CDF) of a standard normal distribution. Regardless of the exact parameters of the problem, the above equation is always a sigmoid function (see Supplementary Material Section 6.1 for the proof). In contrast, the expectation of rewards for the safe alternative is:
E[r s (t s )] = v • λ s t s
(7)
Although uncertainty is a crucial component in calculating expected reward for the make-or-break task (Equation 6), the same does not apply to the safe-reward task (Equation 7), because the expected value of the Wiener process W s t s ∼ N (0,t s ) is always zero. Thus, the performance uncertainty σ s does not influence the expectation, but only the variance of the reward. To give a better idea of these reward functions, 
Figure   payoffs
 (e.g., feeling embarrassment or disappointment). Similarly, expected utility theory suggests that people may discount rewards B. Even if we allow for subjective transformations of the experienced outcomes, the main insights of our framework hold. Figure 2: Expected reward as a function of time invested in the make-or-break task (left column) and the safe-reward task (right column). The top row shows the influence of different skill levels (λ) and the middle row shows different levels of performance variance (σ). The bottom row shows the influence of the task-specific parameters: the bonus (B) and success threshold (∆) bottom left, and the reward rate (v) bottom right. When not specified, we use the default parameter values of T = 10, λ m = λ s = 10, σ m = σ s = 5, B = 1000, ∆ = 50, and v = 10. These parameters are chosen such that the make-or-break reward is equal to the expected reward for the safe-reward task when all available time T is invested, that is,
B = v • λ s T .
2.2.1 Sensitivity of expected rewards to environmental parameters.
The expected rewards for the make-or-break task are a sigmoid function of time allocation (left column of 
Fig. 2)
, where the inflection point and the shape of the expected returns curve are jointly determined by the performance uncertainty σ, the skill level of the agent λ m , the success threshold ∆, and the bonus B. First, note that the agent's skill level λ m and the difficulty of the task (defined by the success threshold ∆) are directly related; increasing the former has a similar effect to decreasing the latter. Additionally, the performance uncertainty σ determines the shape of the expected returns in the make-or-break task where, in the degenerate case of σ = 0, expected reward becomes a step function of time allocation (middle left panel of 
Fig. 2
). As uncertainty increases, the curvature of the sigmoid reward function becomes smoother.
Finally, the bonus B in the make-or-break task and the skill level λ s and reward rate v in the safe-reward task control the relative payoffs between the two tasks. Multiplying or dividing both B and v • λ s by the same amount does not alter the relative payoff in the tasks.


Marginal returns.
Investing a small amount of time in the make-or-break task typically yields very small expected returns;
the resulting performance is almost certain to fall below the success threshold. Initially, increasing time allocation in the make-or-break task corresponds to only small increases in expected reward, with returns being smaller when uncertainty is low. However, as one continues to invest more time in the make-or-break task, the rate of increase in expected reward accelerates and rises rapidly until it reaches an inflection point (at t m = 5, middle left panel of 
Fig. 2
). The rate at which expected reward increases around the inflection point is determined by performance uncertainty σ, with higher uncertainty creating a more gradual transition (smoother sigmoid curves; see middle left panel of 
Fig. 2
). Eventually, as performance quality surpasses the threshold ∆, the marginal increases in expected reward become smaller and smaller, and the expected reward curve draws near the upper bound of B. Thus, after a certain expected performance level, additional effort has diminishing marginal returns


Results
Having established the formal framework for the problem, we now present solutions to two versions of the problem and discuss their implications. First, we address the one-shot version of the problem, where the agent makes a single decision about how to allocate resources between the two tasks, without any feedback on performance or reward. Second, we address the dynamic version of the problem, where performance q(t)
is observable and switching between activities can occur at any point in time t < T (i.e., after any amount of investment prior to exhausting all available time). We present a fully rational and a boundedly rational solution to the dynamic problem, both relying on the idea of a "giving-up" threshold; we then compare these solutions with simple heuristic strategies that achieve competitive performance at much lower computational costs.  . The top right panel shows the effect of uncertainty (in the make-or-break task, σ = σ m ) on the curve shape. The bottom panels show how the returns curve changes as we vary the bonus or the threshold (left) or the reward rate in the safe task (right). When not specified, we use the following default parameter values: T = 10, λ m = λ s = 10, σ = 5, B = 1000, ∆ = 50, v = 10.


The One-Shot Allocation Problem
If an agent has a fixed amount of time T to allocate between the two tasks, and needs to commit to this allocation at the beginning of the task, how can they maximize the sum of expected rewards? This problem is identical to a case where the agent has to commit to a plan upfront, or to a scenario where there is no feedback on performance. Therefore, the optimization problem is reduced to a single decision about how to divide the total available time T between the two activities t m and t s , where the overall reward function can be written as:
h(t m ,t s ) = E[r m (t m )] + E[r s (t s )] = B 1 − Φ ∆ − λ m t m σ m √ t m + v • λ s t s .
(8)
An agent should then optimize this function (Equation 8) under resource constraints, which is written formally as:
maximize t m ,t s h(t m ,t s ) subject to T = t m + t s (9)
The above equations give rise to a simple, yet commonly encountered, non-convex optimization problem. Below, we describe and offer intuitions about how the optimal allocation policy depends on the parameters of the problem (see 
Fig. 3
).
3.1.1 Playing safe or going all in.
For some sets of parameters, one option completely dominates the other, and the decision maker should simply invest all available time resources T in just one of the tasks. In some cases, for instance, the rewards of the make-or-break task simply do not justify the risks, and the clear dominant option is to "play it safe" by investing all available time resources in the safe-reward task. This is the case when (i) there is only a slim chance of success in the make-or-break task (i.e., due to scarcity of total available time T , a very high threshold ∆, or low skill level λ m ; see top left of 
Fig. 3
 where λ m = 1) or (ii) when the expected returns from the safe-reward task are simply much higher (i.e., when v • λ s T is much larger than B).
As we vary the parameters making the make-or-break task more attractive, it becomes rational for agents to allocate at least some-possibly all-of their time to the make-or-break task. There is a discontinuous switch in the optimal policy when a critical point in the parameter space is crossed as we increase the total available time, the bonus of the make-or-break task, or the skill level of the agent (see Section 7.2 in the Supplementary Materials). This is illustrated in 
Fig. 4
, where the optimal policy switches from "playing it safe" (t s = T ) to "going all in" (t m = T ) at the critical value of λ m satisfying the equation
B • P(λ m T + σ m W m T > ∆) = v • λ s T
(10)
Equation 10 corresponds to the point where the two extreme policies of "playing it safe" and "going all in" produce equal expected rewards. Transitions from one extreme to the other always occur when the make-orbreak task is sufficiently rewarding (see Supplementary Materials for proof), and we vary the total available time resources T . In the Supplementary Material we specify the exact conditions under which all-or-nothing transitions should occur as we vary the skill level or the reward of the make-or-break task.
3.1.2 Slacking off for the highly skilled.
In environments where success in the make-or-break task is highly likely without requiring the investment of all available time, the optimal policy entails allocation of resources to both activities. As expected performance in the make-or-break task increases beyond the critical point described in Equation 10, the decision maker can potentially err on the side of investing too much time in the make-or-break task. This error comes with the opportunity cost of losing potential rewards from investing surplus time in the safe-reward task.
This is the case when the marginal expected gain of investing more time in the make-or-break task is less than the gain of investing more time in the safe-reward task, that is:
[E[r m (t m )]] t m =T < v • λ s ,
(11)
where the derivative is taken with respect to the allocation t m . In 
Figure 4
, this corresponds to the point at which the optimal allocation of time to the make-or-break task begins to decrease, as the likelihood of success increases as a function of skill level (λ m ). Everything else being equal, a lower threshold and more total available resources make it more likely that an agent should allocate time to both activities. People with higher skill levels can quickly secure a higher probability of success and invest residual time resources in the safe-reward task. Whereas 
Kukla (1972)
  Optimal Allocation 
Figure 4
: Optimal allocation as a function of skill level and under different levels of performance uncertainty (line type). Unless otherwise specified, we use the following default parameter values: T = 10, B = 1000, ∆ = 50, v = 10. To illustrate the crucial role of uncertainty, we set λ s = 3 and vary the value of λ m ∈ [0, 10]. For this set of parameters, it is already profitable to switch from allocating everything to the safe-reward task to allocating everything to the make-or-break task when an agent has a 30% chance of success. At higher uncertainty levels, this critical point occurs at an even lower level of skill (compare the optimal allocation policy for σ = 0.1 with σ = 5 and σ = 10). On the other hand, people with higher skill levels should allocate time to the make-or-break task until the point that marginal returns from increasing the chances of success in the task are equal to the linear opportunity cost of the safe-reward task (see Equation 11). At higher uncertainty levels, this critical point occurs at higher skill levels.


The motivating effect of uncertainty.
Everything else being equal, higher performance uncertainty implies that it is optimal to allocate all time to the make-or-break task for a larger subset of parameters in the parametric space. People with lower skill should dedicate themselves completely to the make-or-break goal in hope of eventual success. It also makes sense for people with higher skill to put all their effort in the make-or-break task, in order to minimize the risk of failing (see also 
Figure 8
 in the Supplementary Material). Higher uncertainty leads to an increase in the expected rewards of the make-or-break task for small amounts of time allocation, and it also implies that the deceleration of marginal returns occurs more slowly (see the middle left panel of 
Figure 2
 and the upper right panel of 
Figure 3
). As a result, in more uncertain environments (i.e., larger σ), the critical point described by Equation 10 occurs for lower levels of skill or total available time, while the critical point described by Equation 11 occurs for larger values. 
Figure 4
 demonstrates this effect when comparing a low uncertainty environment (σ = 0.1) with intermediate (σ = 5) or high uncertainty environments (σ = 10). The motivating effect of uncertainty is particularly pronounced when comparatively large rewards are at stake in the make-or-break task (i.e., a high B/(v • λ s T ) ratio) and gradually attenuates as the relative rewards from the safe-reward task increase.


Discussion.
In everyday experience, people often need to decide how to allocate their time or effort between challenging make-or-break goals and safe alternatives. We have shown that this type of resource allocation problem is non-convex in nature, leading to striking discontinuities in how people should allocate their time as the parameters of the problem vary. Small differences in people's skill levels, resources, or the rewards available may change the optimal policy from allocating all of one's time to the safe-reward task to investing everything in the make-or-break task. Crucially, we have shown how the degree of uncertainty in the environment moderates the optimal allocation policy and determines when these discontinuities are expected to occur.


The Dynamic Allocation Problem
So far we have assumed that the agent makes a single allocation decision, dividing the available time between two activities without any feedback on performance. In many problems, however, people dynamically obtain information about their performance, which they can use to reassess their prospects of success and alter their behavior on the fly. How should agents dynamically invest their time when they can directly observe their performance and switch dynamically between tasks? In the analyses that follow, we assume that the agent has full knowledge of all the critical parameters of the problem and his current performance but is uncertain only about his future performance.


Optimal allocation policy.
Order of tasks and switching between tasks. Intuitively, people above a certain skill level in the makeor-break task should start by investing time towards it; they can switch to the safe-reward task in case of an early success or if success seems unattainable. Because future rewards in the safe-reward task do not depend on past performance (i.e., there is no useful feedback from the safe task), the dominant strategy is to begin with the make-or-break task and to switch unidirectionally to the safe-reward task (see Supplementary Material Section 7.3 for a proof). This leads to a significant simplification of our problem: an optimal solution to the allocation problem involves at most one switch between tasks and only from the make-orbreak to the safe-reward task. The question then is when to make this single switch.  
Figure 5
: Visualization of the myopic (dashed line) and optimal (solid line) giving-up thresholds for three samples drawn from the stochastic performance process q(t). The green sample reaches the success threshold (dotted line) and is free to invest the remaining time in the safe-reward task. The orange sample has hit the myopic giving-up threshold (dashed line) and therefore gives up on the make-or-break task. However, an optimal decision maker has a more tolerant giving-up threshold (solid line), thus the blue sample and the green sample continue even after hitting the myopic threshold, resulting in either a later giving-up point (blue) or eventually reaching the success threshold (green). Any time resources remaining after the agent reaches either a giving-up threshold or the success threshold are allocated to the safe-reward task. We use the following default parameter values:
T = 10, σ m = σ s = 5, B = 1000, ∆ = 50, v = 10, but use λ m = λ s = 5 for illustrative purposes.
Giving-up threshold. At any given point in the make-or-break task, the agent has to decide whether to switch immediately to the safe-reward task or to further pursue the make-or-break goal, based solely on the current performance. At very poor performance levels, success is highly unlikely; therefore, a rational agent should switch to the safe-reward task. On the other hand, if the agent is relatively close to the success threshold (and assuming the reward is large enough), it would be sensible to continue in pursuit of eventual success. For a specific intermediate level of performance, the two strategies will yield the same expected reward, such that the agent can be indifferent about which strategy to follow. We call this indifference point the giving-up threshold and at time t we denote it by c t . Intuitively, the agent should abandon the make-or-break task in favor of the safe-reward task if, and only if, performance is equal to or falls below this value after time t. So long as the agent starts with a performance above the giving-up threshold (i.e., q(t 0 ) > c 0 such that it is rational to invest in the make-or-break task at all), task switching should occur when performance q(t m ) falls below the giving-up threshold c t or upon reaching the success threshold ∆. More precisely, the point at which an agent should give up is given by
τ = min{t : q m (t) ≤ c t or q m (t) ≥ ∆}.
(12)
In the stochastic processes literature, this type of problem is considered a problem of optimal stopping. The existence of a giving-up threshold that leads to an optimal policy through Equation 12 is guaranteed by Theorem 2.2 in 
Peskir and Shiryaev (2006)
, with several methods for deriving this optimal policy given in Chapter 8 of the same book 3 . To the best of our knowledge, there is no analytic solution for finding the optimal policy and all approaches that can be used to find it are computationally expensive. Here, we follow the most widely used approach for deriving the optimal policy, which involves discretizing time and using backwards induction 
(Dixit & Pindyck, 1994;
Kushner & Dupuis, 2013)
. In this section, we rely use the expected value theory framework in order convey our results with the greatest clarity. Yet our analyses can also be generalized to the framework of expected utility theory, and it is possible to account for risk-aversion, risk-seeking or inter-temporal discounting (see Supplementary Material sections 7.4.1 and 7.4.2).
First, we discretize time by dividing T into n equally sized intervals. This turns the problem into a discrete optimal control problem whose solution involves solving the resulting Bellman Equation using backwards induction
R k (x) = max ∞ −∞ R k+1 (y) • φ x (y)dy, g m (x) + v • λ s • (n − k)T n ,
(13)
for k = 0, . . . , n − 1. R k (x)
is the expected total reward from time k to n, if at k the performance in the makeor-break task is x, and φ x is a suitable normal distribution. The initial conditions are given by R n (x) = g m (x) (see Supplementary Materials Section 7.4 for details). Thus, the giving-up threshold c k at time k is the unique solution of the equation:
∞ −∞ R k+1 (y) • φ c k (y)dy = v • λ s • (n − k)T n .
(14)
The optimal policy is to switch to the safe reward task at the first step k such that q m (t k ) ≤ c k , due to falling below the threshold, or q m (t k ) ≥ ∆, due to success. Deriving the optimal threshold requires starting from the end of the task and reasoning backwards. Thus, calculating the giving-up threshold at t 0 requires knowledge of the threshold values at all subsequent steps. Finding the optimal solution is computationally demanding not only for humans but also for machines, as we explain in the following paragraph.
The computational burden of calculating the optimal threshold. Herbert Simon, one of the founding fathers of artificial intelligence, argued that theories of rationality that do not take into account the computational costs of problem solving are incomplete or even misleading 
(Simon, 1978)
. We build on his conception of rationality and apply computational complexity theory 
(Papadimitriou, 2003;
Van Rooij, 2008)
 to express the computational cost of the optimal stopping strategy. As shown in Equation 13, for each step k and each performance value x, we have to calculate an integral over all performance values y at the next step. Assuming that the integral is computed numerically and approximated by a sum, both its computational cost and the accuracy of the result depend on the number of summands we use. If we use m terms in the sum, then the computational complexity of one integration will be O(m). This will give us R k (x) for a specific k and x. But in order to later find R k−1 (x ), for any x , we need to have R k (x) for m different values of x. Thus, for each k, we need O(m 2 ) computations. This gives us R k (x) for a specific k and for m different values of x. The accuracy of the approximation also depends on the number of steps we use in the time axis, that is, the number of values of k. Thus, if we divide time into n steps, we need in total O(n • m 2 ) computations for the solution described above, which results in an algorithm of polynomial complexity. Although polynomial-time algorithms are considered tractable in computer science and artificial intelligence, as opposed to non-polynomial-time (for example exponential-time), in practice they can still be prohibitively expensive to compute 4 , even for exponents as low as 2, that is O(n 2 ). Even for relatively small n and m values (e.g., n = m = 100), the optimal policy would require a million computations (n • m 2 = 10 6 ). Regardless of the exact cost measure used to penalize computation, the demands of the optimal strategy are excessive (for different operationalizations of computational costs see 
Fechner, Schooler, & Pachur, 2018;
Johnson & Payne, 1985;
Lieder & Griffiths, 2017)
. The computational burden would be even larger for an optimally behaving risk-averse or risk-seeking agent, since additional parameters would need to be computed (see Supplementary Materials section 7.4.2). An important principle in cognitive science is to constrain the space of computational level theories to plausible strategies at the algorithmic level 
(Marr, 1982)
. Thus, it becomes clear that alternative strategies with lower computational complexity would provide valuable insights to how human decision makers grapple with these problems or even hint to how to program artificial agents coping with similar decision problems.


Boundedly rational alternatives.
Dynamic decision-making problems are both widely prevalent 
(Brehmer, 1992;
Friedman, Isaac, James, & Sunder, 2014;
Gonzalez, Fakhari, & Busemeyer, 2017;
March, 1996)
 and notoriously challenging from a computational perspective. While optimal solutions often depend on backwards induction, several studies have shown that even for shallow planning depth, humans do not use backwards induction (e.g., 
Hotaling & Busemeyer, 2012;
Huys et al., 2015;
Zhang & Yu, 2013)
. The same holds in game-theoretical contexts 
(Johnson, Camerer, Sen, & Rymon, 2002;
McKelvey & Palfrey, 1992)
. Instead of using computationally complex solutions, people tend to rely on simple and computationally inexpensive algorithms. While heuristics can lead to consistent deviations from optimality in some environments 
(Lieder & Griffiths, 2017;
Tversky & Kahneman, 1974)
, they can produce competitive performance in other environments 
(Analytis, Kothiyal, & Katsikopoulos, 2014;
Gaissmaier & Schooler, 2008;
Gigerenzer & Todd, 1999)
. In the following sections, we examine several boundedly rational strategies that circumvent the computational costs of full rationality, yet perform competitively in some environments.
The myopic giving-up strategy. An alternative to backward induction is to assume that the decision maker acts myopically and decides whether or not to pursue the make-or-break task further as if this decision were the last that they could make. Myopic strategies accurately describe human behavior in a wide array of settings, ranging from sequential hypothesis testing to sequential search and multi-armed bandit tasks (see also J. R. 
Busemeyer & Rapoport, 1988;
Gabaix, Laibson, Moloche, & Weinberg, 2006;
Stojic, Analytis, & Speekenbrink, 2015;
Wu, Schulz, Speekenbrink, Nelson, & Meder, 2018;
Zhang & Yu, 2013)
. In our case, as more time is allocated to the make-or-break task, some of the associated uncertainty becomes replaced by an actual outcome y = q m (t ) experienced up until time t ∈ [0, T ]. The agent can use this information to revise their allocation policy, making a myopic decision at any point during the allocation problem. Because future performance is independent of past performance (by the properties of the Wiener process), the problem of finding the optimal allocation is the same as before, but with total time T r = T − t and threshold ∆ − y. The expected reward for the make-or-break task, if additional time t m were invested in it, would be
E[r m (t m )] = B • P(λ m t m + σ m W m t m ≥ ∆ − y) = B • 1 − Φ ∆ − y − λ m t m σ m √ t m
(15)
The expected returns for the safe-reward option would still be
E[r s (t s )] = v • λ s t s .
A myopic agent seeks to optimize the sum of these rewards, under the constraint T r = t m + t s , and will continue to invest in the task for as long as it would myopically be profitable to do so. A giving-up threshold in terms of performance can be computed ( 
Fig. 5
, dashed line) as the point below which the myopic policy prescribes investing no further time in the make-or-break task, in other words when t m = 0. When performance drops below this threshold, the myopic agent will switch to the safe-reward task.
Finding the t m that maximizes the total expected reward is an easy optimization problem, which can be solved by finding the roots of the derivative of Equation 15. The computational complexity of this, using Newton's method, for example, is a constant multiple of the complexity of computing elementary functions 
(Brent, 1976)
. Hence, at any point, an agent can decide whether to continue with the make-or-break task by solving a computationally inexpensive problem. In total, O(n) calculations will be needed, but distributed equally over n intervals. This starkly contrasts with the optimal strategy, where the entire computational cost has to be paid upfront. The myopic strategy produces highly risk-averse behavior. In 
Figure 6
 we also compare the myopic strategy with an optimally behaving risk-averse agent whose preferences are described by a power function. As uncertainty in the environment increases, the myopic strategy aligns with progressively higher levels of risk-aversion.
The play-to-win strategy. Another way to cope with the computational complexity of dynamic decisionmaking is to employ heuristic strategies 
(Gigerenzer & Gaissmaier, 2011;
Tversky & Kahneman, 1974)
.
We first consider the play-to-win strategy, a heuristic strategy that stubbornly pursues success in the makeor-break task, switching to the safe-reward task only when the make-or-break threshold is reached. Thus, the strategy only needs to decide whether to start pursuing the make-or-break goal at all, eliminating the computational costs almost entirely. Although the strategy is suboptimal, it has an intuitive appeal. Most  
Figure 6
: Myopic, optimal, and risk-averse optimal (ρ = 0.75) giving-up thresholds, as a function of performance variance (σ m ) and ability (λ m ) in the make-or-break task. Other parameters are defaulted to T = 10, σ s = 5, B = 1000, ∆ = 50, v = 10, and λ s = 5. The gray confidence bound shows one standard deviation of performance quality. As σ m increases, the differences between the myopic and optimal thresholds increase. Note that the myopic giving-up strategy becomes progressively more risk-averse under higher uncertainty (i.e., more risk-averse than the ρ = 0.75 optimal risk-averse curve when σ m = 10). See Supplementary Materials section 7.4.2 for how to incorporate risk preferences in our framework. on this strategy is the first time point at which the stochastic process crosses the success threshold. This event is commonly referred to as the first passage time of the stochastic process. First passage time problems for a Wiener process with a single threshold have been studied extensively across scientific disciplines (see 
Redner, 2001
) and the results can be readily transferred to our setting. To calculate the expected total reward for this strategy, we need to bear in mind that there are two cases: either (i) the agent pursues the make-orbreak task, but never reaches the reward threshold ∆ and receives zero reward, or (ii) the agent reaches the reward threshold after some time τ ≤ T and receives the bonus B in addition to rewards from the safe-reward task, to which any surplus resources T − τ were allocated. The time τ can be defined mathematically as:
τ = min{t : q m (t) ≥ ∆} = min t : W m t + λ m σ m • t ≥ ∆ σ m
(16)
In the stochastic processes literature, the above is referred to as a hitting time or first passage time, at level ∆ σ m , of a Wiener process with drift λ m σ m . We provide the exact probability density function of τ (known as an inverse Gaussian distribution) and an analytical expression for the expected reward for this strategy in Supplementary Materials Section 7.5.
Intuitively, the play-to-win heuristic can be ruinous for agents who start when their chances of succeeding in the make-or-break task are negligible. To perform competitively across environments it has to be combined with a good starting rule. What is a good starting rule for the play-to-win strategy? For somebody in possession of a calculator and the analytical formula for the expected returns on the strategy (Supplementary Material Section 7.5), it would be trivial to compute whether the expected returns are higher for the make-or-break task than for playing it safe. Even if suboptimal, this rule ensures that the agent will reapwhenever possible-the higher expected returns procured from stubbornly pursuing the make-or-break goal.
However, it is unlikely that laypeople will be able to use the analytical formula. Alternatively, an individual could employ a myopic calculation (using Equation 15 only at t = 0) to gauge whether to start pursuing the make-or-break goal in the first place, and then never give up. Such a starting rule is more cognitively plausible and would protect people from starting in contexts where success is unlikely (e.g., where the decision maker has a low skill level and the uncertainty in the environment is relatively low), eliminating the costs of continuously monitoring their progress and calculating a giving-up threshold. Note that in order to illustrate both the adaptive and pathological use of the play-to-win heuristic, we simulate the strategy in 
Figure 7
 without a starting rule, but we also discuss the psychologically plausible myopic starting rule.
Control points strategy. Another plausible heuristic strategy is for the agent to consider the possibility of giving up at only a fixed number of time points (e.g., at the end of the academic or financial year). According to this strategy, there are two cases in which the agent may switch to the safe-reward task: if she passes the reward threshold ∆ or if at one of the "control points" her performance drops below the giving-up threshold c t . More precisely, the time τ that the agent will switch to the safe-reward task is
τ = min{t : q m (t) ≥ ∆ or (t = t k , t k is a control point and q m (t k ) ≤ c t k )}.
(17)
To some degree educational institutions and businesses already implement this strategy. They often require members to pass interim performance thresholds for progressing to the next level, rather than supervising them continuously. The strategy can be also seen as a self-control strategy (e.g., 
Ainslie, 1992;
Metcalfe & Mischel, 1999)
, since people only have to consider giving up at a few discrete points in time rather than face the constant temptation to quit (as is the case for the myopic threshold).
The play-to-win and myopic giving-up strategies can be seen as special cases of the control points strategy. The control points strategy considers giving-up at N points in time, which in the case of the playto-win strategy, the agent considers giving-up only at N = 1 point situated at the beginning of the task t = 0.
On the other extreme, if we let N → ∞, we recover the myopic strategy. Although there are many possible ways one can choose the points of control, we will consider only a simple one, where the control points are placed at equal intervals. For instance, if the total available time is 10 units and there are 4 control points, then these will be at t ∈ {2, 4, 6, 8}. Optimal Derived through backwards induction and implemented as a giving-up threshold calculated for each time point. If performance falls below the threshold, the optimal strategy dictates that the agent should give up and switch to the safe alternative.
O(n • m 2 )
Myopic Similar to the optimal strategy, but using only a single forward-looking myopic calculation to determine the giving-up threshold. This myopic calculation implies a more conservative threshold as uncertainty increases.


O(n)
Control Points Rather than computing a giving-up threshold at all possible time points, the control points strategy only considers giving up at a fixed number of k time points. In the limit of infinite control points, it becomes the myopic strategy.


O(k)
Play to Win This is a fixed strategy that ignores feedback about performance, and stubbornly pursues the make-or-break task until eventual success or failure. This strategy can also be combined with a number of different starting rules, that gauge whether the make-or-break task is worthwhile to pursue until an eventual success or failure.
O(0) without a starting rule and O(1) with a starting rule.
Note: n denotes the number of discrete time points, m are the discretized number of possible performance levels, and k is the number of control points considered, where k < n.


Strategy comparison.
For the play-to-win heuristic, the expected returns and the distribution of the time of success can be derived analytically. In contrast, for threshold-based strategies and the control points heuristic, the expected returns can be derived only numerically and the distribution of success and dropout times can only be determined using simulations. To obtain additional insights into how the strategies compare to one another, we therefore simulated the behavior of decision makers in the make-or-break task at three skill levels (λ m ∈ {4, 5, 6}) and two levels of uncertainty (σ m ∈ {5, 10}). These conditions are sufficient to capture the crucial factors influencing the performance of the strategies as well as the interaction between them. 
Figure 7
 compares the simulated behavior of the various strategies in order of complexity (decreasing from left to right), showing the distribution of attained rewards from which the timing distribution of successes and drop-outs can be derived. We implement the play-to-win heuristic without a starting rule (i.e., it always pursues the makeor-break task) for illustrative purposes, and implement the control points strategy with 4 control points at t ∈ {2, 4, 6, 8}. 
Table 1
 provides a short summary of each strategy.
Optimal vs. myopic giving-up strategies. Looking first at the optimal and myopic giving-up strategies, the differences in performance shown in 
Figure 7
 can be explained by a divergence in their respective thresholds 
(Fig. 6
). As uncertainty in the environment increases, so does the value of information that is received through feedback in the course of the task. Only the optimal giving-up strategy integrates the value of information that will be revealed in the future and thus prescribes persevering longer (before dropping out), whereas the myopic giving-up drops out more conservatively. At very low or very high skill levels, the optimal and myopic giving-up strategies lead to similar performance. At very low skill levels, both strategies play it safe; at very high skill levels, even the more conservative myopic giving-up threshold is rarely ever , where the colored dots indicate the individual outcomes over 10,000 simulations, while the diamonds indicate the mean earnings. We vary the performance variance σ m and skill level λ m , where the other parameters are defaulted to T = 10, σ s = 0, B = 1000, ∆ = 50, v = 10, and λ s = 5. Rewards above 1000 indicate that the agent succeeded in achieving the make-or-break goal and acquired additional rewards from the safe-reward task. Rewards equal to 500 indicate that the agent allocated all available time to the safe task. Rewards equal to 0 indicate that the agent pursued the make-or-break task unsuccessfully, without obtaining any reward. Rewards between 500 and 0 imply that the decision maker began investing in the make-or-break task, but gave up and switched to the safe-reward task. We implement the control points strategy with 4 control points at t ∈ {2, 4, 6, 8} and the play-to-win strategy without a starting rule to illustrate the pathological case where it pursues the make-or-break task even when success is unlikely.
triggered 
5
 . We now focus on discussing how skill and uncertainty influence the performance of the optimal or myopic giving-up strategies. Everything else being equal, the results will be more pronounced when a larger reward is at stake (for high B/(v • λ s T ) ratios), and the differences will attenuate when the relative rewards from the safe-reward task increase. Here we present results for B = 2v • λ s T . The analysis could be also conducted for inherently risk-averse or risk-seeking agents. The main difference then is that as we increase the risk-aversion in the utility function the relative performance of the myopic strategy as compared to the other strategies improves, making it more attractive for risk-averse decision makers.
The effect of skill level: The divergence between the myopic and optimal giving-up strategies is most pronounced at intermediate skill levels (i.e., σ = 5, λ m = 5). Under these conditions, it is only marginally profitable (from a myopic perspective) to allocate any amount of time to the make-or-break task. The myopic threshold is rather conservative, leading to quick dropouts in most simulation runs. Agents following the myopic strategy succeed in the make-or-break task in very few cases (3.1%). In contrast, simulated agents following the optimal strategy persevere for longer and succeed in approximately half the trials, with dropouts more evenly distributed across time (see the upper middle panel of 
Figure 7)
. At relatively low skill levels (e.g., σ m = 5, λ m = 4), an optimal decision maker would pursue the make-or-break task, whereas a myopic agent would play it safe. Even under these conditions, an unexpectedly good start could bring the success threshold within reach for an optimal decision maker. If performance falls below expectations, a decision maker can switch to the safe-reward alternative, thereby acquiring at least some rewards. Only a few of the simulated agents following the optimal strategy persevere until success (only 3.3 % for σ m = 5, λ m = 4), with most simulated agents dropping out early (upper left panel of 
Figure 7)
. Under this set of parameters, the optimal policy leads to a modest increase in terms of expected returns, where rare successes are largely balanced out by early drop outs. The difference between the two giving-up strategies is still notable-yet much smaller-at relatively high skill levels (see the upper right panel of 
Figure 7)
. The effect of uncertainty: The two threshold strategies diverge even more in terms of performance and observed behavior as uncertainty increases for two reasons. First, higher uncertainty implies a higher value of dynamically revealed information. The optimal strategy integrates this information and becomes more tolerant overall, investing in the make-or-break task at even lower skill levels. Second, higher uncertainty leads to larger fluctuations in performance, causing myopic agents to prematurely drop out more often, even at higher skill levels. Both these factors make the optimal threshold much more perseverant at high levels of uncertainty when compared to the conservative myopic threshold (contrast the upper and lower panels of When and why you should never give up. For moderate or high skill levels in the make-or-break task, the play-to-win heuristic does fairly well relative to the optimal strategy in terms of average rewards and can even outperform the myopic strategy. As uncertainty in the environment increases, the play-to-win heuristic approximates the expected rewards of the optimal strategy (see 
Figure 7
, middle and right columns for σ = 10 and λ m = 5, 6). How is this possible, given the stark difference in computational costs? In environments with high uncertainty, the optimal strategy is already more tolerant in order to accommodate the increased value of information. Yet even in cases where performance falls below the optimal givingup threshold, it is not unlikely that performance will rebound and ultimately reach the success threshold.
In environments with extreme uncertainty, the play-to-win heuristic approximates the expected rewards of the optimal giving-up strategy, rendering the computational costs for deriving the optimal threshold an unnecessary burden. Recall that for illustrative purposes, 
Figure 7
 shows the play-to-win strategy without a starting rule, such that it always pursues the challenging goal. This results in pathological cases, where play-to-win pursues the make-or-break task even when the prospects of success seem impossibly bleak at the outset (see upper left panel of 
Figure 7
 when σ m = 5 and λ m = 4, where the play-to-win strategy would succeed in only about 27% of the cases, and gain no rewards in other cases). This outcome can easily be prevented by adding a starting rule using a myopic calculation to assess whether it is worthwhile at all to invest in the make-or-break task, in which case effort is continually allocated until success or failure ensues. With a myopic starting rule, the play-to-win strategy outperforms the myopic threshold strategy in all examined environments where the make-or-break task is myopically perceived as attractive, despite disregarding feedback about performance.
The benefits of giving up full control The control points strategy largely outperforms the myopic strategy despite being computationally cheaper. It also slightly outperforms play-to-win, further closing the gap to the optimal strategy, but at a relatively lower computational costs. Recall that we implemented the control points strategy with N = 4 points where giving-up is considered, located at equally spaced time intervals (corresponding to the horizontal yellow lines in 
Figure 7)
. In comparison, the myopic threshold has N → ∞ points where giving up is considered and the play-to-win can be implemented with N = 1 point (at the very beginning). By channeling drop-outs to a few control points, there is a substantial reduction in the probability of early drop-outs. As seen in 
Figure 7
, the control points strategy not only approximates the optimal strategy in terms of rewards, but also in terms of the distribution of drop-outs. While drop-outs following the control points strategy only occur at distinct points, the relative mass resembles that of the optimal strategy much more than the other two boundedly rational strategies (e.g. the distributions for λ m = 5 and σ m = 5, 10). In high uncertainty environments where the decision about whether to begin the task is marginal, the control points strategy leads to substantial gains over the myopic and play-to-win strategies (see λ m = 4 and σ m = 10 bottom left of 
Figure 7
), suggesting that it may be a good heuristic in high uncertainty environments. This implementation is not always beneficial, for instance in environments with lower levels of uncertainty, although an additional control point at t = 0 would protect decision-makers from potential losses when the make-or-break task is too challenging (upper left panel λ m = 4 and σ m = 5).


Discussion
Our results suggest that even though there is a well defined optimal allocation strategy, the computational costs may be prohibitively expensive for both humans and artificial decision-makers. However, we show that simpler boundedly rational alternatives can achieve high expected rewards at low computational costs by disregarding information and exerting less control. The myopic strategy produces a distinct pattern of risk-averse behavior (compared to the optimal strategy) at a lower computational cost. Using even simpler models, we show that the play-to-win and control points strategies largely outperform the myopic threshold and can sometimes approximate the performance of the optimal strategy. The competitive performance of these heuristic strategies runs contrary to findings from other complex decision-making settings, where it has been shown that higher uncertainty necessitates more monitoring and control 
(Osman, 2010)
. Our results suggest that the right level of complexity and control may depend on the structure of the task.


General Discussion
How should people invest time in risky but highly rewarding, make-or-break tasks, forgoing other activities with safe rewards? How much effort should a scientist invest towards applying for a prestigious grant?
When should an entrepreneur give up on a high-risk yet potentially high-reward startup that is currently performing worse than expected? Researchers across disciplines in the social and behavioral sciences have alluded to the crucial factors involved in making these decisions (e.g., 
Eccles & Wigfield, 2002;
Vroom, 1964
), but the problem has previously evaded rigorous analysis. In this article, we present a formal model that addresses this conceptual gap and can be applied to allocation problems in psychology, economics and behavioral biology.


On the large impact of small changes
We found that the expected returns of the make-or-break tasks are sigmoid in shape, which implies that similar real world effort allocation problems may harbor discontinuities in the best possible time allocation policy. Small changes in the parameters of the problem can shift the prescribed allocation policy from one extreme (i.e., investing nothing) to the other (i.e., allocating all available resources; see 
Kukla, 1972;
Vancouver, More, & Yoder, 2008)
. For instance, consider two students with very similar skill levels studying in the same competitive program. The optimal strategy may prescribe one student to drop out entirely, while the other should double down and invest all available resources into their studies. Similarly, relaxing the deadline for a grant application or reducing the threshold required to achieve a bonus at work may mobilize people to dedicate themselves fully to a task, whereas they might have appeared indifferent before. Crucially, we showed that the optimal allocation policy is moderated by the degree of uncertainty in the relation between effort and performance. In environments with high uncertainty, people with a larger range of skill levels will be motivated to pursue challenging goals, either in hope of an outside chance of success or working hard to minimize the offhand chances of failure. These results are at odds with the prevailing theories of time allocation in economics, behavioral biology, and psychology, which typically assume diminishing returns or that different activities complement each other (e.g., 
Becker, 1965;
Borjas & Van Ours, 2000;
Charnov, 1976;
Kurzban et al., 2013)
. In these theories, the underlying optimization problems are convex, with only a single optimum that shifts gradually as the parameters of the problem are varied. Likewise, the predicted policy switches cannot be accounted for by theories of motivation in cognitive and organizational psychology 
(Bandura & Locke, 2003;
Heath, Larrick, & Wu, 1999;
Locke & Latham, 2002)
, which suggest that people tend to respond to marginal changes in the difficulty of achieving a goal with continuous (either monotonic or non-monotonic) changes in the amount of effort exerted.


The computational challenges of dynamic decision-making
There are two modes of discovery in the study of human decision-making. Psychologists and neuroscientists tend to start their inquiry by conducting experiments reflecting realistic decision-making settings, and assess the extent to which human behavior corresponds to plausible cognitive models (e.g., 
Brehmer, 1992;
J. Busemeyer, 2002;
Gonzalez, Lerch, & Lebiere, 2003)
. The descriptive, rather than the normative value of these models drives the inquiry. For example, mathematical psychologists and neuroscientists have extensively investigated dynamic decision-making in problems where people choose between two or more multi-attribute options, and where information about the value of the options is accessed and evaluated dynamically (e.g., by retrieving memories of similar items experienced in the past). Similarly to our setting, new information is assumed to unfold dynamically as a stochastic drift over time. Furthermore, these models assume that decision makers make a choice when the evidence accumulated in favor of an alternative has crossed a decision threshold (e.g., J. R. 
Busemeyer & Townsend, 1993;
Khodadadi, Fakhari, & Busemeyer, 2017;
Krajbich, Armel, & Rangel, 2010;
Ratcliff et al., 2016;
Usher & McClelland, 2001
).
One of the main advantages of these dynamic models is that they generate rich predictions, not only about eventual choices, but also about the timing of events, namely, when and how often different decision thresholds will be crossed at different parameterizations of the problem. At the same time, it has been increasingly valuable to leverage dynamic optimization techniques to obtain insights in the properties of optimal policies and to assess the conditions under which different action thresholds lead to good decisions (see 
Bogacz, Brown, Moehlis, Holmes, & Cohen, 2006;
Fudenberg, Strack, & Strzalecki, 2015;
Malhotra et al., 2017;
Tajima et al., 2016)
. These recent investigations provide the first steps towards a rational analysis 
(Anderson, 1991;
Chater & Oaksford, 1999)
 or what David 
Marr (1982)
 describes as a computational level analysis of the tasks in question, developing a better understanding of the reward structures and the environment within which cognition adapts. Are the optimal policies for this task also cognitively plausible strategies? Our computational complexity analysis suggests it is unlikely. Nevertheless, knowing the optimal policies can guide the search for computationally efficient and psychologically plausible algorithms that can produce competitive performance.
In contrast, economists and scientists in neighboring disciplines begin their inquiry from normative models. Researchers in economics and finance have extensively studied optimal policies in dynamic decisionmaking settings where the value of an asset-commonly described by a Wiener process-fluctuates over time (e.g., a stock, or the value of a factory) and agents are required to make high-impact and irreversible decisions (e.g., selling a factory; see 
McDonald & Siegel, 1986;
Pindyck, 1991)
. These problems have clearcut analogs in common problems encountered by laypeople, such as whether and when to sell a house or quit a stressful job. 
Dixit and Pindyck (1994)
 were well aware of the relevance of their modeling framework for everyday decisions and briefly discuss marriage as an irreversible dynamic decision that most people make at least (or at most) once. To date, only a handful of experimental studies have investigated how people make decisions in such dynamic settings, where the tasks are framed as investment decisions 
(Oprea, Friedman, & Anderson, 2009;
Strack & Viefers, 2013)
. How do people cope with the computational complexity of dynamic decision-making in these settings when the optimal strategy is computationally inaccessible and there are scant opportunities for learning? Defining and studying computationally simpler, boundedly rational strategies could lead to a rich set of predictions about behavior in dynamic decision environments (see, e.g., 
Oprea et al., 2009)
.
The dynamic task we present here has structural and technical similarities with both these strands of research. Thinking like economists or theoretical biologists, we were able to characterize the optimal givingup strategy for a family of common problems 6 . In the spirit of Herbert 
Simon (1978)
, we employed complexity theory-an essential methodological tool in computer science and a artificial intelligence-to gauge the computational requirements of the optimal strategy. We showed that it is computationally excessive for humans, and that it is demanding even for modern computers. Thinking like behavioral scientists, we then proposed several cognitively plausible alternatives, motivated by the principles of myopic and heuristic decision-making. These simpler strategies come with substantially cheaper computational costs. Deriving the optimal strategy enabled us to assess the prescriptive value of the boundedly rational strategies for different parameters of the decision-making problem. As in the dynamic decision-making problems investigated in mathematical psychology and neuroscience, our models generate a rich set of predictions about the timing of success and dropout at different parameterizations of the problem. Clearly, we have not yet exhausted the space of psychologically plausible stopping strategies (see J. R. 
Busemeyer & Rapoport, 1988)
. For instance, an agent may decide to stop when performance stagnates for a long period of time, constructing a local (in time) stopping rule. Formally analyzing additional strategies is beyond the scope of this paper, yet our framework paves the way for future experiments that will make it possible to assess the descriptive value of the proposed strategies and to identify other plausible psychological alternatives.


Risky choices, sunk costs, and bounded rationality
We started from the assumption that decision makers maximize expected value. This was a convenience assumption that allowed us to convey our results with the greatest clarity. How do our results connect to the literature on risky choice? When the marginal utility of rewards diminishes 
(Bernoulli, 1954)
, people may discount the expected rewards from the two activities, especially from the large make-or-break bonus (see Supplementary Material section 7.4.2) 7 . Going a step further, people may subjectively reassess the value of succeeding or dropping out in the make-or-break task 
(Atkinson, 1957;
Heath et al., 1999;
Lewin et al., 1944)
 or distort the probabilities of success and failure at each level of invested effort 
(Kahneman & Tversky, 1979)
. How do these findings transfer to the complex and dynamic decision-making settings encountered in the real-world? There is evidence that as the environment becomes more complex (e.g., the number of alternatives increases) people tend to rely more on computationally simple strategies (e.g., 
Venkatraman, Payne, & Huettel, 2014)
. Every variable transformation, such as those implied by expected and non-expected utility theories, requires computation. However, we have already shown that the optimal strategy is prohibitively expensive, regardless of the mode of computation. Thus, deriving an 'optimal' riskaverse or time-discounted policy using backwards induction may have normative value (e.g., 
Smith & Ulu, 2017)
 but little descriptive or prescriptive value. Plausible theories of risky choice in dynamic contexts have to address the severe computational challenges of deriving optimal and time-consistent risk-averse strategies (also see 
Hammond, 1976)
. A way to tackle this issue is to assume that some people are described by a nonexpected utility theory (e.g. prospect theory), while at the same time taking risks myopically, as if they disregarded future choices 
(Barberis, 2012;
Ebert & Strack, 2015)
.
In the real-world people may make risky choices by responding to the reward structure of the environment learning, for instance, from experienced rewards 
(Denrell, 2007;
March, 1996;
Stewart, Chater, & Brown, 2006)
. Importantly, computational limitations require that decision makers find simple and robust strategies that trade off effectively between complexity and expected returns. This argument has been advanced already for static risky choices (see 
Brandstätter, Gigerenzer, & Hertwig, 2006;
J. W. Payne, Bettman, & Johnson, 1993
), yet it is imperative in the dynamic case, where the costs of computation are high even for
computers. The psychologically plausible strategies we have proposed can produce different risk-taking patterns, highlighting the boundedly rational approach to the study of risky behavior in dynamic contexts. For instance, the myopic giving-up strategy produces risk-averse behavior when compared to the optimal strategy, and becomes increasingly risk-averse with higher levels of uncertainty (see 
Figure 6
). Risk-aversion could also be derived at much higher computational costs by passing rewards through a risk-averse utility function and then calculating the optimal strategy. However, here the pattern of risk-aversion produced by the myopic strategy is due to computational simplifications rather than some innate risk preference. The play-to-win strategy, by contrast, produces behavior that can be seen as a version of risk-seeking and is compatible with the sunk cost fallacy 
(Arkes & Blumer, 1985)
. Thus, instead of subjectively transforming rewards and probabilities, people might express their risk tendencies by committing to boundedly rational strategies 
(Friedman et al., 2014;
Pedroni et al., 2017)
. Subjective transformations of value and distortions of perceived probabilities might be important components of dynamic choices, but they have to operate in tandem with other behavioral assumptions-such as myopia-to be behaviorally plausible. We intend to investigate these issues in future experiments.


Self-efficacy and learning
When people embark on new, challenging projects (such as pursuing a PhD or founding a startup), they often only have a rough estimate of their own abilities. 
Bandura (1977)
 coined the term "self-efficacy" to describe the belief in one's ability to succeed, complete tasks, and reach goals. A mismatch between believed and actual skill levels can alter how people allocate their time to different tasks. Under-confidence can have the most dramatic effect, leading to self-fulfilling prophecies, where people who underestimate their abilities may never pursue a highly rewarding make-or-break goal (see 
Hogarth & Karelaia, 2012)
. Without direct experience of successful achievement of a challenging goal, the decision maker may never rectify their initial beliefs. In the one-shot version of the task, overconfidence can lead decision makers to under-invest or over-invest depending on their skill level. In the dynamic version of the task, however, overconfidence may encourage myopic individuals to persevere for longer, thereby counteracting the conservative effect of myopic giving up 
(Nozick, 1994)
.
How do people form beliefs about their own skill level and the relationship between effort, luck, and achievement in a task? They may use social comparisons or individual experiences in similar tasks to inform their beliefs about their skill levels 
(Bandura, 1977;
Bol et al., 2018;
Frank, 1935)
. Repeated interactions with a task or continuous feedback about performance may help them to hone their intuitions about how effort and luck jointly define the chances of achieving a goal 
(Weiner, 1985;
Weiner & Kukla, 1970)
. Even if people are certain about their skill level, they may leverage repeated interactions with the same task to refine their allocation strategies or choose among them 
(Erev & Barron, 2005;
Rieskamp & Otto, 2006)
. 
Oprea et al. (2009)
, for instance, studied how people behave in optimal stopping problems, where they have to choose when to sell an asset. Although the optimal strategy is similar in terms of complexity to the optimal threshold strategy described here, through repeat interactions with the same task, people were able to approximate optimality by leveraging a simple learning strategy.


The value and cognitive underpinnings of perseverance
A main insight from the optimal giving-up strategy we advanced is that optimally behaving agents should persevere before dropping out, and that the strategy should become more tolerant as the amount of uncertainty in the environment increases. This result echoes findings from the dynamic investing literature in economics and finance, where agents should wait longer before making irreversible decisions, thus harnessing the value of dynamically unfolding information 
(Dixit & Pindyck, 1994)
. 
Duckworth and collaborators
 have shown that the ability to persevere and maintain effort in the face of failures is at least as important as intelligence for succeeding in one's field 
(Duckworth, Peterson, Matthews, & Kelly, 2007)
. The unexpected outcomes of real-world problems can often lead to discouragement and make people doubt their chances of success. Perseverance when pursuing a high-stakes, yet challenging goals might be even more valuable in real-world settings, precisely because of the additional uncertainties involved in estimating one's abilities.
In such settings, simple strategies that persevere for longer than what seems reasonable under a myopic set of beliefs (e.g., the control points strategy that considers giving up at only a few discrete time points, or play-to-win as an extreme case of ignoring feedback) could in fact lead to better outcomes. In line with work by 
McGuire and Kable (2013)
, our approach shows that people's ability to persevere is not merely a character trait, but has important cognitive underpinnings.


Conclusion
A common movie plot involves a protagonist pursuing a challenging but rewarding make-or-break goal. After multiple frustrations and drawbacks, the protagonist manages to come back on top before time runs-out and marginally succeeds in meeting their goal. In real life, people may rationally abstain from investing in such risky make-or-break goals or give up early after initial setbacks, in order to pursue other safer activities.
Knowing when to pursue a challenging goal and when to give up are difficult decision-making problems, yet they are pervasive in everyday life. We have shown that simple strategies relying on myopic or heuris-tic principles perform competitively against the optimal strategy, yet sidestep much of the computational burden. What is more, these boundedly rational strategies can reproduce the distinctive behavioral patterns of risk-aversion, risk-seeking, and the sunk-cost fallacy, suggesting that these patterns arise not only from inherent preferences, but could also be the product of simplifying cognitive strategies. Thus, the persevering movie hero may appear unboundedly courageous, exactly because they are boundedly rational.


Acknowledgments
We 


Code Availability Statement
The code for producing the results reported in the paper is available at https://osf.io/p2vur/.
7 Supplementary Material 7.1 The expected rewards of the make-or-break task is a sigmoid function of time
Recall that the expected reward from investing time t m in the make-or-break task is
E[r m (t m )] = B • 1 − Φ ∆ − λ m t m σ m √ t m .
(18)
We are now going to show that this is a sigmoid function for t m > 0, meaning that E[r m (t m )] converges to a number c ∈ R as t m → ∞ and that it has a unique inflection point, with its second derivative being positive on the left and negative on the right of this point. The first statement is straightforward, because as
t m → ∞, ∆−λ m t m σ m √ t m → −∞, so E(r m (t m )) → B.
For the second statement, we write
E[r m (t m )] = B • 1 − Φ ∆ σ m t − 1 2 m − λ m σ m t 1 2 m = B • 1 − Φ at − 1 2 m − bt 1 2 m ,
(19)
where a = ∆ λ m and b = λ m σ m . Therefore, it is enough to show that the function
f (t) = Φ at − 1 2 − bt 1 2 , a, b > 0
(20)
has a unique inflection point for t > 0, with its second derivative passing from a negative to a positive value.
We have
f (t) = − 1 2 √ 2π e − 1 2 • at − 1 2 −bt 1 2 2 • at − 3 2 + bt − 1 2
(21)
and
f (t) = 1 4 √ 2π • t 7 2 • e − 1 2 • at − 1 2 −bt 1 2 2 • b 3 t 3 + ab 2 + b t 2 + 3a − a 2 b t − a 3
(22)
Because the first two factors in the expression for f (t) are always positive, its sign is determined by the third degree polynomial
z(t) = b 3 t 3 + ab 2 + b t 2 + 3a − a 2 b t − a 3 .
(23)
It is therefore enough to show that z(t) has exactly one root for t > 0 and its sign switches from negative to positive as t crosses that root from left to right.
By further noticing that z(0) < 0 and z(t) → ∞ as t → ∞, it is sufficient to show that z(t) has at most one local extremum for t > 0 (which will have to be a local minimum), or that z (t) has at most one root for t > 0. We calculate
z (t) = 3b 3 t 2 + 2(ab 2 + b)t + 3a − a 2 b.
(24)
Even if this equation has two roots, their sum has to be equal to − 2(ab 2 +b) 3b 3 , so that at least one has to be negative. This concludes the proof that E[r m (t m )] is a sigmoid function.


Optimal time allocation policy for the one-shot problem
In this section we study how the optimal policy for the one-shot allocation task varies with some of the parameters of the problem. We begin by studying some general properties of the total expected reward function, which is given by
h(t m ,t s ) = E[r m (t m )] + E[r s (t s )],
(25)
where t m is the time invested in the make-or-break task and t s is the time invested in the safe-reward task. If the total available time is T , then we may write t s = T − t m , hence the above simplifies to Lemma 7.2. When the local extrema of h(t m ) exist, their positions t min and t max vary continuously with the parameters B, σ m , λ m , λ s , v, ∆, and are independent of T .
Proof: Independence from T follows from the fact that h (t m ) is independent of T . For the other statement, note that by the MVT, the unique root of h (t m ) must occur in the interval (t min ,t max ). This implies that h (t min ) > 0 and h (t max ) < 0. Therefore, by continuity of h (t m ) and of the (partial) derivatives of h with respect to any of the parameters, we conclude that both local extrema positions t min and t max are stable, in the sense that they persist for small changes of the parameters and vary continuously with them.
Lemma 7.3. i) If h(t m ) ≥ h(0) for some t m ∈ (0, ∞), then h(t m ) has a unique local maximum at t max ∈
(0, ∞), which is also global.
ii) If t opt = 0, then the condition in (i) holds and, moreover, t opt = min{T,t max }.
Proof:
i) Since h (t m ) < 0 for large t m , h(t m )
attains a global maximum in [0, ∞). Even if that happens to be at 0, then by assumption it must also be attained somewhere in (0, ∞). This global maximum in (0, ∞) will also be a local maximum, which by Lemma 7.1 is unique.
ii) If t opt = 0, then by definition there exists some
t m ∈ (0, T ], such that h(t m ) > h(0). From part (i),
there exists a unique local maximum t max ∈ (0, ∞). Clearly, if t max ≤ T , then t opt = t max . If on the other hand t max > T , then since h(t m )'s unique local maximum is at t max , it has no local maximum in (0, T ), so its maximum in the interval [0, T ] is attained either at 0 or at T , which means that t opt = 0 or t opt = T . By assumption, the first possibility is excluded, therefore t opt = T . This concludes the proof that t opt = min{T,t max }.


Varying the total available time T
Suppose first that we vary the total available time T , while all other parameters are kept fixed. We want to look at how t opt varies. We distinguish two cases.
First suppose that h(0) ≥ h(t m ) for all t m ∈ [0, ∞). Note from Equation 28 that this condition is independent of T . If it is true, then investing zero time in the make-or-break task is always preferable to investing non-zero time in it. In other words, t opt = 0 no matter what the value of T , so this case is trivial.
For the other case, we have the following proposition.
Proposition 7.4. Suppose that h(t m ) > h(0) for some t m > 0. Then,
t opt =      0, if T ≤ T * T, if T * < T ≤ t max t max , if T > t max ,
(30)
where t max is the unique global maximum of h(t m ) and T * is given by Optimal time allocation . At very low amounts of total available time (i.e., a tight deadline), the agent allocates all their time to the safe alternative. At T * , the agent puts all their effort into the make-or-break task. As the total available time increases, the agent continues to invest all their time in the make-or-break task to increase the chances of achieving it, until the point t max , at which the benefits from improving the chances of success are offset by the opportunity cost. Beyond that point, the agent should invest all available time in the safe task, which entails that the time allocation problem has an internal solution. We illustrate these time allocation patterns for low and intermediate uncertainty (σ = 1 vs. σ = 5) and denote time allocated to the make-or-break task by a subscript. Right: The optimal amount of time allocated to the make-or-break task as a function of the skill level in the task λ m ∈ [0, 20]. At skill level λ * m , the agent should discontinuously change their strategy from allocating all their time to the safe-reward task to allocating all or at least some of their time to the make-or-break task. When the opportunity cost of time is relatively low (v = 10), the agent should invest all their time in the make-or-break task; when the opportunity cost (v = 40) is relatively high, some of their time. As the skill level continues to increase, the proportion of time the agent should optimally allocate to the make-or-break task decreases. Consistently with the main text, in both panels we use the default parameter values of T = 10, σ m = σ s = 5, B = 1000, ∆ = 50, v = 10, λ m = 10. We default λ s to 3 for illustrative purposes and to ensure consistency with 
Figure 4
 in the main text. and satisfies T * < t max .
T * = inf {t m ∈ (0, ∞) : h(t m ) > h(0)},
(31)
Proof: Let T * be as in Equation 31. By continuity, h(T * ) = h(0), so t opt = 0 if and only if T ≤ T * .
For T > T * , by Lemma 7.1 we have t opt = min{T,t max }. Finally, note that since h(t max ) > h(0), we have T * < t max .


Varying the skill level λ m
We now study how t opt changes as we vary λ m , assuming that all other parameters remain constant. In this section, we use the notation h(t m ; λ m ) and t opt (λ m ) in order to emphasize the dependence of these quantities on λ m . For t m = 0, h(0; λ m ) does not depend on λ m , so we will write just h(0).
To simplify the presentation of the result, we will assume that when the skill level for the make-orbreak task is 0, then the optimal strategy is always to invest all available time into the safe-reward task. 
9
 Mathematically this means that h(t m ; 0) < h(0) for all t m > 0.
We show that when λ m is smaller than some value λ * m , then the optimal policy is to allocate all time to the safe-reward task, that is t opt (λ m ) = 0. For λ m > λ * m , the optimal policy is to put at least some time into the make-or-break task. Moreover, the transition at λ * m is discontinuous, with t opt jumping from 0 to t * > 0. As λ m increases further, t opt (λ m ) will change continuously (but never increase).
The value of t * can be either T , in which case at λ * m there will be a transition from allocating all time to the safe-reward task to allocating all time to the make-or-break task, or it may be smaller than T , in which case the transition will be from allocating all time to the safe-reward task to allocating time to both tasks. In the following proposition, we prove all of the above and give some technical conditions that tell us whether t * = T or t * < T .
Proposition 7.5. Suppose that h(t m ; 0) < h(0) for all t m ∈ (0, T ]. We have the following:
i) t opt (λ m ) = 0, if λ m ≤ λ * m min{T,t max (λ m )}, if λ m > λ * m ,
(32)
where Note that for any given λ m > 0, h (0; λ m ) < 0. Therefore, by continuity of h (t m ; λ m ), there exists some ε = ε(λ m ) > 0, such that h(t m ; λ m ) < h(0; λ m ) = h(0) for any t m ∈ (0, ε). In particular, there exists some ε > 0, such that h(t m ; λ m (T )) < h(0), hence also λ m (t m ) > λ m (T ), for any t m < ε. This proves our claim that λ m (t m ) attains a minimum in (0, T ].
λ * m = min{λ m > 0 : ∃t m ∈ (0, T ], h(t m ; λ m ) = h(0)}. (33) ii) For λ m > λ * m , t opt (λ m )
We define
λ * m = min t m ∈(0,T ] {λ m (t m )} = min{λ m > 0 : ∃t m ∈ (0, T ], h(t m ; λ m ) = h(0)}.
(35)
Recalling that h(0) ≥ h(t m ; λ m ) if and only if λ m ≤ λ m (t m ), we obtain
t opt (λ m ) = 0 ⇔ h(0) ≥ sup 0<t m ≤T h(t m ; λ m ) ⇔ λ m ≤ min 0<t m ≤T λ m (t m ) = λ * m .
(36)
Combining this with Lemma 7.3, the result follows.
ii) Continuity of t opt (λ m ) for λ m > λ * m follows from part (i) and Lemma 7.2. From the definition of λ * m , it follows that there exists some t m ∈ (0, T ] such that h(t m ; λ * m ) = h(0). By Lemma 7.3, h(t m ; λ m ) has a unique local maximum t max (λ * m ), and by Lemma 7.2, lim
λ m →λ * m t max (λ m ) = t max (λ * m ).
(37)
Combining this with part (i), we obtain
lim λ m λ * m t opt (λ m ) = min lim λ m λ * m t max (λ m ), T = min{t max (λ * m ), T } > 0,
(38)
because both T and t max (λ * m ) are greater than zero. Finally, note that by Lemma 7.1 we have that h (T ; λ * m ) ≥ 0 if and only if
t min (λ * m ) ≤ T ≤ t max (λ * m ). But T < t min (λ * m )
is impossible anyway, because then h(t m ; λ * m ) would be strictly decreasing in [0, T ], contradicting the definition of λ * m . Therefore, h (T ; λ * m ) ≥ 0 if and only if T ≤ t max (λ * m ). Combining this with Equation 38, we get that h (T ; λ * m ) ≥ 0 if and only if lim
λ m λ * m t opt (λ m ) = T .


Varying the reward B
We now study how t opt changes as we vary B, assuming that all other parameters remain constant. In this section, we use the notation h(t m ; B) and t opt (B) in order to emphasize the dependence of these quantities on B. The results and the proof are very similar as for λ m , with one exception: here, the condition h(t m ; 0) < h(0) for all t m ∈ (0, T ] is automatically satisfied, as can be seen directly from Equation 28. We therefore have the following proposition.
Proposition 7.6. We have the following:
i. t opt (B) = 0, if B ≤ B * min{T,t max (B)}, if B > B * ,
(39)
where
B * = min{B > 0 : ∃t m ∈ (0, T ], h(t m ; B) = h(0)}.
(40)
ii. For B > B * , t opt (B) is a continuous function of B, and
lim B B * t opt (B) = t * ∈ (0, T ]
(41)
where t * = min{T,t max (B * )}. Moreover, t * = T if and only if h (T ; B * ) ≥ 0.
As when varying λ m , we see that as B increases, there is a discontinuous jump in the optimal amount invested in the make-or-break task at B * , from 0 to t * . The transition can be either to investing all of the available time in the make-or-break task (if t * = T ) or to investing time in both tasks (if t * < T ).
The proof is completely analogous to the case for λ m , so we omit it.
7.3 Switching tasks more than once in the dynamic allocation problem does not provide any benefit
In this section, we show that allowing agents to switch tasks more than once in the dynamic allocation problem does not lead to an improvement in the expected reward of the optimal policy. More precisely, we
show that by restricting ourselves to time allocation policies that either never switch tasks or start with the make-or-break task and switch only once, we can get equally high expected rewards as with unrestricted time allocation policies. Thus, given that an optimal policy exists, there will also exist an optimal policy with the specified properties (never switch or start from make-or-break and switch only once). But first, we need a rigorous definition of what a time allocation policy is. We use a rather general definition that requires the satisfaction of only a few intuitive properties.
This section relies on the theory of stochastic processes. A stochastic process is a random function of time. We also refer to the concepts of stopping time and filtration. We give a brief, intuitive description of these concepts and refer the interested reader to 
Bass (2011)
 and 
Karatzas and Shreve (2012)
 for more details.
A filtration is a technical way to describe the information known up to any specific point in time. We say that a stochastic process is "adapted to a filtration" if its value at any point in time relies only on the information known by that time, with respect to the filtration used.
A stopping time is a specific type of stochastic process which, as its name suggests, often describes the time that another process is stopped or, from another point of view, the time that an event occurs. Saying that the stopping time is adapted to the filtration means that whether or not the event occurs by some point in time follows from the information known so far, with respect to the filtration used. In our case, the event will be switching from one task to the other. Thus, the decision of whether to switch should strictly rely on information about the past performance.
We now proceed with the definition of a time allocation policy. In what follows we use superscripts m and s, instead of subscripts, to distinguish between the make-or-break and the safe-reward task.
Definition 7.7. A time allocation policy (for the dynamic allocation problem) is a pair of stochastic processes (τ m , τ s ), defined on [0, T ], with the following properties:
• τ m t + τ s t = t, for all t ∈ [0, T ] • 0 ≤ τ m t 2 − τ m t 1 ≤ t 2 − t 1 and 0 ≤ τ s t 2 − τ s t 1 ≤ t 2 − t 1 , for any t 1 ,t 2 ∈ [0, T ], t 1 ≤ t 2
• For each t ∈ [0, T ], τ m t and τ s t are stopping times adapted to the filtration generated by W m . 
Figure 9
: The process of backward induction that an agent has to follow to numerically calculate the returns from the optimal policy. Left: For any performance level at time t n−1 , the agent has to calculate the expected reward from performing the make-or-break task in the interval [t n−1 , T ], by looking at all possible terminal performance values at time T . For terminal performance values above the reward threshold ∆ (denoted by orange color), the reward from the make-or-break task will be B, while for terminal performance values below ∆ (blue), the reward will be 0. The expected reward of the task will be a weighted average of these two numbers. The optimal policy can be found by comparing the expected reward of the make-or-break task with that of the safe-reward task for the same interval. For large performance values at t n−1 , it will be optimal to invest in the make-or-break task; for smaller performance values, it will be better to invest in the safe-reward task. The optimal giving-up threshold c n−1 can be located by finding the break-even point where the two tasks give the same expected reward. Right: For any performance level at time t n−2 , we calculate the expected reward from performing the make-or-break task in the interval [t n−2 ,t n−1 ] and the optimal policy in [t n−1 , T ], which is known from the previous step (orange for make-or-break task and blue for safe-reward task). We compare this expected reward with that of the safe-reward task for the whole interval [t n−2 , T ]. The larger of the two will be the expected reward of the optimal policy for [t n−2 , T ]. As for c n−1 , the optimal giving-up threshold c n−2 can be located by finding the point at which the agent is indifferent between the two courses of action. We continue like this for t n−3 , t n−4 ,. . .,t 0 .
0 t T q m (t) ∆ x = q m (t n−1 ) t n−1 0 t t n−1 T q m (t) ∆ x = q m (t n−2 ) t n−2 c n−1
make-or-break task and at some point switching to the safe-reward task (Proposition 7.9). The switch may only happen at times t 0 = 0, t 1 , . . . , t n−1 , t n = T , with t 0 corresponding to starting off with the safe-reward task and t n corresponding to never switching. Accordingly, for any k = 0, . . . , n − 1, at time t k the agent has two possible strategies: perform the safe-reward task for the rest of the time remaining; or perform the make-or-break task for one time interval and re-evaluate whether to continue or to switch tasks at time t k+1
(when there will be new information). The optimal choice is the one that gives a higher expected payoff.
The expected payoff on investing the remaining time in the safe-reward task can be calculated immediately. However, the payoff for the make-or-break task in the interval [t k ,t k+1 ] depends not only on the performance outcome, but also on the choices made at later times. If we know the optimal strategy to follow from time t k+1 onwards, we may assume that the agent will follow it. In other words, if we have already found the optimal policy in the interval [t k+1 , T ], then we may use it to calculate the payoff from choosing to perform the make-or-break task in the interval [t k ,t k+1 ]. This suggests that we solve the problem with backwards induction.
The solution is illustrated in 
Fig. 9
. We start by considering the decision at time t n−1 . Since there cannot be any task switching after that time, we only have to compare two strategies: perform the makeor-break task or the safe-reward task for the interval [t n−1 , T ]. Recall that we are assuming that the reward threshold has not been reached, so in particular q m (t n−1 ) < ∆. The expected reward from performing the
safe-reward task is v • λ s • (T −t n−1 ) = v • λ s • T n .
The reward from performing the make-or-break task depends on the performance at time T , q m (T ). If the performance is y = q m (T ), the reward will be R = g m (y) (see Equation 2). But at time t n−1 , the agent does not have this information; their decision has to be based on their performance up to time t n−1 , that is q m (t n−1 ). Given a performance x = q m (t n−1 ) at time t n−1 , there is a probability distribution for the performance y = q m (T ) at time T . Therefore, to find the expected reward at time t n−1 , one has to integrate over all possible performances at time T , weighted by their likelihood. This is shown in 
Figure 9a
. In symbols, we write
R m n−1 (x) = E n [ R |q m (t n−1 ) = x] = ∞ −∞ E n [ R |q m (t n−1 ) = x, q m (T ) = y] • φ x (y)dy = ∞ −∞ E n [ R |q m (T ) = y] • φ x (y)dy,
(45)
where R is the total reward, φ x (y) is the probability density function for the performance at time T , given that at time t n−1 the performance was x, and E n [•] denotes expectation given that the make-or-break task was performed up to time t n = T . Therefore, R m n−1 (x) is the expected reward from performing the makeor-break task on the last interval (t n−1 ,t n ), given that this task has been performed up to time t n−1 , and the performance at time t n−1 was x. The expected value in the first integral of Equation 45 is conditioned on the performance at time t n−1 being x, and the performance at time t n = T being y. But if we know the performance at the last step, the performance at earlier steps is irrelevant for calculating the reward, so this justifies the last equality. Now, the quantity E n [ R |q m (T ) = y] is straightforward to calculate; it is the reward, given that only the make-or-break task has been performed and at time T the performance in this task is y. Recalling the definition of g m in Equation 2, we have that E n [ R |q m (T ) = y] = g m (y), so that Equation 45 can be rewritten as
R m n−1 (x) = ∞ −∞ g m (y) • φ x (y)dy.
(46)
To find φ x (y), note that the performance change for an interval of length t n−1 − t n−2 = T n is normally distributed, with mean λ m • T n and variance σ 2
m T 2 n 2 . That is, φ x (y) = n √ 2πσ m T • e − n 2 ( y−x−λm T n ) 2 2σ 2 m T 2
(47)
Equation 46 gives us the expected reward for performing the make-or-break task in the last time interval, for any observed performance x at time t n−1 . In order to decide which task to perform in the last interval, we compare this to the expected total reward of performing the safe-reward task in the last interval (assuming that the make-or-break task has been performed up to time t n−1 ), that is
R s n−1 (x) = g m (x) + v • λ s • T n ,
(48)
where the first term is the reward from the make-or-break task, which is equal to either 0 or B, depending on whether x exceeds the reward threshold ∆ or not, and the second term is the expected reward from the safe-reward task. Here x = q m (t n−1 ) again denotes the performance in the make-or-break task at time t n−1 .
The expected reward of the optimal policy for the last step will be
R n−1 (x) = max R m n−1 (x), R s n−1 (x) .
(49)
Clearly, if the reward threshold has already been reached by time t n−1 , so that x > ∆, then R s n−1 (x) will always exceed R m n−1 (x). If we focus on the more interesting case of x < ∆, then Equation 49 reduces to
R n−1 (x) = max R m n−1 (x), v • λ s • T n .
(50)
For small values of x, the safe-reward task will give a higher expected reward, so that
R n−1 (x) will equal v • λ s • T n .
Note that this term does not depend on x. For larger x (close to the reward threshold ∆), performing the make-or-break task will yield a higher expected reward, so that R n−1 (x) will equal R m n−1 (x), which does depend on x. We denote by c n−1 the break-even point, for which the expected rewards of the two tasks are equal. That is, c n−1 solves the equation
R m n−1 (c n−1 ) = v • λ s • T n .
(51)
Once c n−1 has been calculated, the optimal policy for the interval [t n−1 , T ] can be simply described as follows: If q m (t n−1 ) > c n−1 , continue performing the make-or-break task; otherwise, switch to the safereward task.
Now that we know the expected reward of the optimal policy for the last step for any performance value x, we can go one step back, to calculate the expected reward assuming only that the make-or-break task has been performed up to time t n−2 . Again, the expected reward of performing the safe-reward task for the rest of the time is straightforward to find:
R s n−2 (x) = g m (x) + v • λ s • (T − t n−2 ) = g m (x) + v • λ s • 2T n ,
(52)
where x = q m (t n−2 ) is the performance in the make-or-break task up to time t n−1 . We now consider the expected reward for performing the make-or-break task for the interval [t n−2 ,t n−1 ] and assuming that the optimal policy will be followed for the interval [t n−1 , T ], which is the case for a fully rational agent.
Suppose that at time t n−2 the performance is x = q m (t n−2 ). Then, there is a probability distribution for the performance y = q m (t n−1 ) at time t n−1 . We take this into account in calculating the expected reward for performing the make-or-break task. This is illustrated in the right part of 
Figure 9b
, and can be expressed as
R m n−2 (x) = E n−1 [ R | q m (t n−2 ) = x ] = ∞ −∞ E n−1 [ R | q m (t n−2 ) = x, q m (t n−1 ) = y ] • φ x (y)dy = ∞ −∞ E n−1 [ R | q m (t n−1 ) = y ] • φ x (y)dy = ∞ −∞ R n−1 (y) • φ x (y)dy,
(53)
where E n−1 denotes expectation, given that the make-or-break task is performed up to time t n−1 and the optimal policy is followed afterwards. Thus, R m n−2 (x) denotes the expected reward, given that the makeor-break task has been performed up to time t n−2 , the performance at the make-or-break task at time t n−2 is x, the make-or-break task is performed in the interval (t n−2 ,t n−1 ), and the optimal policy is followed afterwards. Note that the last equality in Equation 53 follows from the definition of R m n−2 (x) in Equation 45. The expected reward of the optimal policy is
R n−2 (x) = max R m n−2 (x), R s n−2 (x) = max ∞ −∞ R n−1 (y) • φ x (y)dy, g m (x) + v • λ s • 2T n .
(54)
The above procedure can be continued inductively, to get
R m k (x) = ∞ −∞ R k+1 (y) • φ x (y)dy,
(55)
R s k (x) = g m (x) + v • λ s • (n − k)T n .
(56)
and
R k (x) = max {R m k (x), R s k (x)} = max ∞ −∞ R k+1 (y) • φ x (y)dy, g m (x) + v • λ s • (n − k)T n ,
(57)
for any k = 0, . . . , n − 1, where, according to Equation 46, R n (y) = g m (y).
Equation 57 is an instance of the Wald-Bellman equations. A more rigorous proof that the above equations give the reward of the optimal policy can be found in Section 1.2 of 
Peskir and Shiryaev (2006)
.
For any k = 0, 1, . . . , n − 1, the performance break-even point c k can be calculated as in Equation 51: it is the unique solution of the equation
R m k (c k ) = v • λ s • (n − k)T n .
(58)
For q m (t k ) > c k , the agent should continue performing the make-or-break task in the interval [t k ,t k+1 ].
Otherwise, they should switch to the safe-reward task. For consistency, we set c N = ∆.
To summarize, the algorithm for the optimal policy is the following:
• For each k and each x, find R k (x) inductively from Equation 57, and c k from Equation 58. Initialize with R N (x) = g m (x), where g m is given by Equation 2, and c N = ∆.
• If c 0 ≥ 0, then perform the safe-reward task only.
• If c 0 < 0, start with the make-or-break task and switch to the safe-reward task at time τ = min{k : q m (t k ) ≤ c k or q m (t k ) ≥ ∆},
with the understanding that τ = t N implies that there is no switching.


Time-discounting
The discussion above applies if we assume a risk-neutral agent and no time-discounting. In this section we generalize the results to allow for time-discounting and in the next section we deal with non-risk-neutral agents.
If we want to take into account time-discounting, then the time that a reward is obtained matters. For the make-or-break task we assume that the reward is obtained upon crossing the reward threshold, while for the safe-reward task we assume that the reward is obtained continuously. We will convert everything to discounted value at time t = 0. The discounted value of the expected safe reward if the safe task is performed from time t k = kT n onwards will be
T kT n v • λ s • e −βt dt,
(60)
where β is the discount factor.
If the make-or-break reward threshold was reached at time t k , then we have to add the discounted value of the make-or-break reward, B • e − βkT n . Otherwise, given that the agent performs the safe-reward task for the rest of the time, they will not earn the make-or-break reward 10 . Therefore, the total expected reward from performing the safe-reward task starting at time t k is
R s k (x) = g m (x) • e −βkT n + T kT n v • λ s • e −βt dt
(56 )
Equation 55 for the expected reward of the make-or-break task, R m k (x), remains unchanged, so Equation 57 becomes
R k (x) = max {R m k (x), R s k (x)} = max ∞ −∞ R k+1 (y) • φ x (y)dy, g m (x) • e −βkT n + T kT n v • λ s • e −βt dt ,
(57 )
with initial conditions R n (x) = g m (x) • e −βT .
The performance break-even point c k , for any k = 0, 1, . . . , n − 1, will be the unique solution of the equation
R m k (c k ) = T kT n v • λ s • e −βt dt.
(58 )
Since the make-or-break reward will be obtained in the future, consistently and optimally behaving time-discounting agents should become more conservative and shift their giving-up threshold upwards.


Risk-aversion
We now consider risk who are not risk-neutral, and who seek to maximize a utility function u, rather than the expected value from the two activities. More specifically, the agent seeks to maximize the expected utility of the total reward obtained by time T . Our main results about the single transition from the make-or-break to the safe activity does not always hold in this cases, and the algorithm we described in section 3.2.1 in not guaranteed to be option. The problem arises from the fact that now the (stochastic) performance in the safe-reward task can affect the additional utility from rewards in the future, thus possibly changing the balance between the make-or-break and the safe-reward task. As a result, we may no longer assume that the optimum policy consists of performing the make-or-break task first, safe-reward task second, with only one switch from the first to the second, which was the result of Section 7.3. For example, a risk-averse agent may initially decide that the expected utility of the safe-reward task is higher than that of the make-or-break reward, but poor performance in the safe-reward task can make a high reward more attractive, justifying a switch to the make-or-break task.
To circumvent this problem, we are going to assume that the performance in the safe-reward task is not random, but deterministic. That is, we will assume that σ s = 0. Assuming that the opportunity cost is safe is a common assumption in work on optimal investment problems in economics, finance and operations research (e.g., 
Dixit & Pindyck, 1994)
. In this case performing the safe-reward task early indeed provides no benefit (no extra information) over performing it later, and the results of Section 7.3 still hold, so that an optimal policy will involve at most one switch from the make-or-break task to the safe-reward task.
It is now straightforward to modify Equations 55-58 to account for a utility-based maximization problem. First, we modify the definition of R k (x) to denote the expected utility, assuming that the make-or-break task was performed up to time t k , and the optimal policy was followed afterwards. We also modify the definitions of R m k (x) and R s k (x) accordingly, to refer to utility, rather than reward. In particular, we have
R s k (x) = u g m (x) + v • λ s • (n − k)T n ,
(56 )
Equation 55 for R m k (x) remains unchanged, so that Equation 57 becomes
R k (x) = max {R m k (x), R s k (x)} = max ∞ −∞ R k+1 (y) • φ x (y)dy, u g m (x) + v • λ s • (n − k)T n ,
(57 )
with initial conditions R n (x) = u(g m (x)).
The performance break-even point c k , for any k = 0, 1, . . . , n − 1, will be the unique solution of the equation
R m k (c k ) = u v • λ s • (n − k)T n .
(58 )
The above discussion applies to any utility function u. For our results in 
Figure 6
 we use a power utility function, u(x) = x 1−ρ 1−ρ , which is one of the most commonly implemented expected utility functions in the literature (see 
Holt & Laury, 2002;
O'Donoghue & Somerville, 2018)
. For ρ > 0 agents are risk-averse, while for ρ < 0 they are risk-seeking. An optimally and consistently behaving risk-averse agent would be more conservative compared to a risk-neutral agent, so that their giving-up threshold would be shifted upwards. By contrast, optimally and consistently behaving risk-seeking individuals will become relatively more perseverant and will shift their giving-up threshold downwards.


Analytic expressions of hitting times and expected returns for the play-to-win strategy
In this section we provide a formula for calculating the expected reward for the play-to-win strategy. Recall from Section 3.2.2 that the time τ at which an agent using the play-to-win strategy will switch to the makeor-break task is given by τ = min{t : q m (t) ≥ ∆} = min t :
W m t + λ m σ m • t ≥ ∆ σ m ,
(61)
as long as τ < T . The expected reward from the make-or-break task is then
E[r m (t m )] = B • P(τ ≤ T )
(62)
and the expected reward from the safe-reward task is
E[r s (t s )] = E[v • λ s • (T − τ) • 1 τ≤T ] (63) = E[v • λ s • T • 1 τ≤T ] − E[v • λ s • τ • 1 τ≤T ]
(64)
= v • λ s • T • P(τ ≤ T ) − v • λ s • E[τ • 1 τ≤T ],
(65)
where 1 τ≤T denotes the indicator function of the set {τ ≤ T }; it equals 1 if τ ≤ T and 0 otherwise. Therefore, the total expected reward for the play-to-win strategy is
E[r m (t m ) + r s (t s )] = (v • λ s • T + B) • P(τ ≤ T ) − v • λ s • E[τ • 1 τ≤T ].
(66)
To continue, we need an expression for the probability density function of τ. From Equation 61 we see that τ is the hitting time at level ∆ σ m of a Brownian motion with drift λ m σ m 
(Bass, 2011;
Karatzas & Shreve, 2012)
. Its probability density is an inverse Gaussian distribution (see Section 3.2 in 
Chhikara, 1989)
, given for any t > 0 by
f τ (t) = ∆ √ 2πt 3 • σ m • e − (∆−λm•t) 2 2tσm 2 .
(67)
Using this, we can write the total expected reward as
E[r m (t m ) + r s (t s )] = ∆ √ 2π • σ m • T 0 (v • λ s • T + B)t − 3 2 − v • λ s • t − 1 2 • e − (∆−λm•t) 2 2tσm 2 dt.
(68)
Figure 1 :
1
Performance quality as a function of time invested, with total available time T = 10. The panels show different levels of performance uncertainty σ; the colors represent different skill levels λ. Dotted lines indicate the expected performance quality and confidence bands show one standard deviation.


2
shows the influence of each parameter on expected reward for each activity type as a function of time allocation.


Figure 3 :
3
Total expected reward as a function of the proportion of time allocated to the make-or-break task, where remaining time is allocated to the safe-reward alternative. The top left panel shows how the shape of the curve varies for different skill levels (we set λ m = λ s = λ)


suggested a discontinuous relationship between the difficulty of a task and the effort people exert, here we have generalized it to any level of uncertainty and clarified the relations between the different parameters of the problem (seeFig 4,where skill and difficulty are inverse concepts).


people are familiar with real-world examples of individuals who, once they had taken the first step, would not give up on a goal, irrespective of their chances of success. The crucial event calibrating the expected returns λ m = 4 λ m = 5 λ m = 6


Figure 7 :
7
Strategy comparison. Reward distributions of each strategy (in decreasing order of complexity from left to right)


Figure
Figure 7).


Figure 8 :
8
Left: The optimal amount of time allocated to the make-or-break task as a function of the total available time T ∈ [0, 10]


is a continuous function of λ m , and lim λ m λ * m t opt (λ m ) = t * (34)where t * = min{T,t max (λ * m )}. Moreover, t * = T if and only if h (T ; λ * m ) ≥ 0.Proof:i) For any t m ∈ (0, T ], we have that h(t m ; 0) < h(0), h(t m ; λ)is strictly increasing in λ m and limλ m →∞ h(t m ; λ m ) > h(0). Therefore, the equation h(t m ; λ m ) = h(0) has a unique solution. We define λ m (t m ) to be this solution and note that h(t m ; λ m ) > h(0) if and only if λ m > λ m (t m ).Because the first partial derivatives of h(t m ; λ m ) with respect to t m and λ m are both continuous and the one with respect to λ m is non-zero, the Implicit Function Theorem implies that the solution λ m (t m ) of h(t m ; λ m ) = h(0) is a continuously differentiable function of t m . Therefore, λ m (t m ) attains a minimum in every interval of the form [c, T ], with c > 0. We will show that it also attains a minimum in (0, T ].


Table 1 :
1
Summary of Strategies
Strategy
Summary


Lewin et al. (1944)
 and
Atkinson (1957)
 suggest that failure to reach a previously set aspiration level is accompanied by negative


There are various types of optimal stopping problems (for examples, see
DeGroot, 2005)
. The dynamic decision-making problem we study shares concepts and methods with three distinct lineages of such problems. The first can be traced toWald's  sequential analysis (1945), where an agent has to decide when to stop evaluating alternative hypotheses. This line of thought has been pursued further in psychology and neuroscience, leading to choice models where evidence in favor of different alternatives unfolds dynamically in time as a stochastic process
(Ratcliff, Smith, Brown, & McKoon, 2016;
Tajima, Drugowitsch, & Pouget, 2016)
. A second lineage studied in economics and finance builds on stochastic processes to investigate when to make high-stakes decisions when the value of assets fluctuate
(Dixit & Pindyck, 1994;
Jacka, 1991)
. The main theoretical result of this line of research is that agents should wait longer before making consequential decisions. A third strand of research in management science and operations research relies on dynamic optimization techniques to explore when to adopt a new technology or give up investing in it when new information about its potential unfolds dynamically
(McCardle, 1985;
Ulu & Smith, 2009)
.


The Fast-Fourier-Transform algorithm is considered one of the most important algorithms of the 20th century
(Cipra, 2000)
 because it reduced the computational cost of the discrete Fourier transform from O n 2 to O(n log n).


In the absence of uncertainty (σ = 0), the problem degenerates and both strategies follow the same solution as the one-shot problem, where the optimal allocation can be derived from the outset and the notion of a giving-up threshold becomes meaningless. That's exactly the version of the problem studied by
Kukla (1972)
.


Note that it is not always feasible to derive the optimal strategy for dynamic decision-making problems. In multi-armed bandit problems, for example, the optimal policy (i.e., Gittins index) can only be derived under restrictive assumptions
(Whittle, 1980)
. Similarly, psychologists have long investigated human decision-making in challenging dynamic tasks where it is impossible to even derive the optimal policy
(Brehmer, 1992;
Gonzalez et al., 2017)
 7 Note that some form of intertemporal discounting
(Berns, Laibson, & Loewenstein, 2007)
 is often built into dynamic decisionmaking models. We avoided adding a discount factor in the main text to prevent the mathematics from appearing more daunting without adding much substance. We explore that version of the problem in Supplementary Material section 7.4.1.


Continuity of h(t m ) guarantees that a "first" such point exists.


This does not have to be the case, especially if the reward threshold ∆ is low. If it is not true, then we can show that t opt (λ m ) is always positive and varies continuously with λ m .


We ignore the case that the reward threshold was reached at an earlier step, before t k , because then the agent would have already switched to the safe-task earlier and there is nothing to compute.








We interpret τ m t and τ s t as the time devoted to the make-or-break task and safe-reward task, respectively, up to time t. The first condition says that the time devoted to both tasks together up to time t, is t. The second condition says that, inside an interval of time [t 1 ,t 2 ], the time devoted to either task should be between 0 and t 2 −t 1 . And the last condition makes sure that decisions on how much time to allocate to each task are based on the observed performance for the make-or-break task so far. Note that we do not allow τ m t and τ s t to depend on W s , because the past performance in the safe-reward task has no effect on the future rewards.
Next we want to distinguish time allocation policies that switch tasks at most once and only from the make-or-break task to the safe-reward task. We call these simple time allocation policies. More precisely, we have the following definition.
Definition 7.8. A time allocation policy (τ m , τ s ) is simple if there exists some stopping time ρ adapted to the filtration generated by W m , such that τ m t = min{t, ρ} for each t.
Intuitively, the above relation says that the time devoted to the make-or-break task increases linearly with time, until some point, where it stops increasing and takes the value ρ. This terminal value should depend only on the observed performance in the make-or-break task; this is the content of requiring ρ to be adapted to the filtration generated by W m .
By using a time allocation policy (τ m , τ s ), the reward from the safe-reward task is v • λ s • τ s T and the reward from the make-or-break task is B, if τ m T ≥ ∆, and 0 otherwise. These quantities are random, because τ m and τ s are themselves random. We denote probability and expectation with respect to a time allocation policy τ m , τ s by P τ m ,τ s and E τ m ,τ s , respectively. Hence, the total expected reward associated with the time
where 1 τ m T ≥∆ denotes the indicator function of the set {τ m T ≥ ∆}. Our central claim in this section is that instead of searching over all time allocation policies for an optimal one, it is enough to search among the simple ones. This is the content of the following proposition.
Proposition 7.9. For any time allocation policy (τ m , τ s ), there exists a simple time allocation policy (τ m , τ s ) with the same total expected reward.
Proof: Let (τ m , τ s ) be any time allocation policy and define
It is easy to verify that (τ m , τ s ) is also a time allocation policy. Moreover, by definition, τ m T is a stopping time adapted to the filtration generated by W m . Therefore, (τ m , τ s ) is simple. Finally, note that τ m T = τ m T and τ s T = τ s T , so (τ m , τ s ) and (τ m , τ s ) give the same total expected reward, by Equation 42. Proposition 7.9 is crucial because it reduces the dynamic time allocation problem to an optimal stopping problem, a class of stochastic optimization problems that has been extensively studied 
(Peskir & Shiryaev, 2006
). This allows us to employ the broadly used algorithmic solution described in the next section.


Algorithmic solution for the dynamic allocation problem
The goal of this section is twofold. First, we want to describe how to numerically calculate a time allocation policy for the dynamic time allocation task that is arbitrarily close to being optimal. Second, we want to highlight the fact that calculating such a policy involves a backwards induction mechanism, which in our opinion is more readily appreciated in the discretized version of the problem, rather than in the continuous one.
The dynamic time allocation problem, in the form described in Section 3.2.1, is a stochastic optimal control problem, whose solution is described by a partial differential equation known as the Hamilton-Jacobi-Bellman equation 
(Bertsekas, 1995)
. In solving it, one has to work backwards in time, at least implicitly, since the "initial" conditions refer to the final time T . A standard method to approximate an optimal solution is to discretize the problem 
(Kushner & Dupuis, 2013)
, which leads to the discrete version of the Hamilton-Jacobi-Bellman equation, referred to as the Bellman equation 
(Bellman, 2013)
. This has the advantage of making much clearer the need for backwards induction in order to calculate the optimal policy. Here we describe the discrete version of our problem and derive the Bellman equation associated with it.
Recall that the agent has total time T to allocate between two tasks, one of which provides a reward proportional to the time invested, and the other provides a large reward B only if the performance in this task exceeds some threshold ∆. In the dynamic allocation problem, the agent at each time knows their performance so far and can use this information to adapt their strategy. The agent's performance q m in the make-or-break task and q s in the safe-reward task are given by
respectively, where t m is the time allocated (so far) to the make-or-break task, λ m is the skill level parameter for this task, W m is a Wiener process, σ m measures the uncertainty of the performance, and similarly for the safe-reward task.
The agent then has to decide on how to distribute time between the two tasks, with the goal of maximizing the total expected reward. We consider the following discrete version of the problem: the agent may only switch tasks at times that are multiples of T /n, for some natural number n. In other words, the time T is divided into n intervals, and the agent commits to a single task during each interval. Moreover, the agent receives the reward for the make-or-break task only if their performance exceeds the reward threshold ∆ at one of these discrete time points. As the number of allowed switching points increases, the difference between the discrete and continuous version of the problem becomes negligible and the expected reward of the optimal policy for the discrete version converges to the optimal reward of the continuous version (see Chapter 10 in 
Kushner & Dupuis, 2013)
.
In specifying a time allocation policy for the make-or-break task, the agent has to make n choices, at times t 0 = 0, t 1 = T n , . . . , t n−1 = (n−1)T n , based on the performance for the make-or-break task at that time.
As in the continuous time version of the problem, there is no benefit from starting with the safe-reward task earlier before switching to the make-or-break task; the optimal strategy involves beginning with the
 










Picoeconomics: The strategic interaction of successive motivational states within the person




G
Ainslie








Cambridge University Press












Multi-attribute utility models as cognitive search engines




P
P
Analytis






A
Kothiyal






K
V
Katsikopoulos








Judgment and Decision Making




9


5
















Is human cognition adaptive?




J
R
Anderson








Behavioral and Brain Sciences




14


3
















The psychology of sunk cost




H
R
Arkes






C
Blumer








Organizational Behavior and Human Decision Processes




35


1
















Motivational determinants of risk-taking behavior




J
W
Atkinson








Psychological Review




64


6
















Self-efficacy: toward a unifying theory of behavioral change




A
Bandura








Psychological Review




84


2
















Negative self-efficacy and goal effects revisited




A
Bandura






E
A
Locke








Journal of Applied Psychology




88


1
















A model of casino gambling




N
Barberis








Management Science




58


1


















R
F
Bass




Stochastic processes


Cambridge, United Kingdom




Cambridge University Press




33












A theory of the allocation of time




G
S
Becker








The Economic Journal




75


299
















Dynamic programming




R
Bellman








Courier Corporation


Princeton, NJ












Exposition of a new theory on the measurement of risk




D
Bernoulli








Econometrica




22


1
















Intertemporal choice: toward an integrative framework




G
S
Berns






D
Laibson






G
Loewenstein








Trends in Cognitive Sciences




11


11
















Dynamic programming and optimal control




D
P
Bertsekas








Athena Scientific


Belmont, MA












The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks




R
Bogacz






E
Brown






J
Moehlis






P
Holmes






J
D
Cohen








Psychological Review




113


4
















The matthew effect in science funding




T
Bol






M
De Vaan






A
Van De Rijt




10.1073/pnas.1719557115








Proceedings of the National Academy of Sciences


the National Academy of Sciences


















G
J
Borjas






J
C
Van Ours




Labor economics


Boston, MA




McGraw-Hill




2












Convex optimization




S
Boyd






L
Vandenberghe








Cambridge University Press


Cambridge, United Kingdom












The priority heuristic: making choices without trade-offs




E
Brandstätter






G
Gigerenzer






R
Hertwig








Psychological Review




113


2
















Dynamic decision making: Human control of complex systems




B
Brehmer








Acta psychologica




81


3
















The complexity of multiple-precision arithmetic




R
P
Brent








The complexity of computational problem solving


R."""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]