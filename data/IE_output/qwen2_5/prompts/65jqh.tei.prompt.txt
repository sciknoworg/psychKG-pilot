You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



while innovative, still depend on pre-existing models and extensive data. To address these gaps and promote model discovery in the social sciences, we propose to adopt algorithms designed for datadriven discovery of nonlinear differential equations in physics and engineering. These data-driven approaches allow the freedom to explore a vast range of functional forms in relatively small datasets while constraining the models to be interpretable.
The notion that dynamic models can be discovered using bottom up approaches received increased attention in recent decades, especially in physics 
(29,
30)
. Early work in this space suffered from overfitting and required immense computing power. However, recent developments allow for implementations of bottom-up equation discovery in complex, noisy and multidimensional systems 
(31)
(32)
(33)
, making it well suited for model discovery in social sciences. Unlike other, more opaque machine learning approaches, these algorithms generate systems of equations that researchers can interpret. Users can also predetermine the space of possible terms that describe the system and control the level of complexity of the obtained model.
Here we utilize an equation discovery algorithm, SINDy (Sparse Identification of Nonlinear Dynamics; 24), to develop and improve human RL models. SINDy is based on the idea of sparse regression, seeking to identify a minimal set of ordinary differential equations that aim describe the underlying dynamics of a system that produced the observed data (here, the underlying cognitive process). It uses a combination of optimization and feature selection to find the sparse set of candidate functions through iterative multiple regression, and it can amalgamate a wide variety of linear and nonlinear terms (see Methods for details). SINDy has been applied in physics 
(35,
36)
, engineering 
(37,
38)
 and biology 
(19,
39)
. An introductory paper suggested its use in social sciences 
(40)
, but it has not
been used yet for model development with empirical data.
The goal of the current project is to discover novel models of RL. We use SINDy to enable testing of multiple RL models without the biases inherent to traditional top-down model development.
In phase 1, we designed a simple RL task that allows us to capture participants' estimation of a probability of reward across multiple trials. Using SINDy, we then revealed a new interpretable model-termed the Quadratic Q-Weighted model-that introduces new behavioral insights on how people learn the probability of reward. This model, in line with probability weighting theories, demonstrates that participants exhibit a systematic distortion in their estimation or probability, which is similar to the nonlinear probability weighting seen in previous decision-making research. What sets the model apart, however, is its ability to capture a dynamic transition between S-shaped and inverse S-shaped distortions, revealing a context-dependent flexibility influenced by participants' expectations.
In phase 2 we then take the Quadratic Q-Weighted model and compare its ability to predict reward data on completely different kinds of tasks involving evaluating reward in much more complicated situations such as a two-armed bandit task. We do not use SINDy directly in this phase; rather, we take the Quadratic Q-Weighted model discovered using our simple RL task and embed that model within existing models of more complex decision-making. We demonstrate that the application of the Quadratic Q-Weighted model achieves better results than previous state-of-the-art models across eight of nine public datasets, each published in leading academic journals. This work therefore makes a twofold contribution: First, it provides a proof of concept for utilizing an equation discovery algorithm in the social sciences, enabling the discovery of a novel RL model from behavioral data. Second, it suggests a new model of human RL that accounts for probability weighting distortions and demonstrates its generalization capabilities to more complex decision-making tasks, thereby unveiling novel insights into human cognition.


Results


Phase 1: Equation Discovery from Empirical Probability Estimates.
Our first goal was to determine whether algorithms discovered by SINDy can provide novel insights into probabilistic learning when trained on empirical data from human learners. To this end, we conducted two empirical studies using a novel learning task composed of 100 trials. Participants assumed the role of an inspector tasked with identifying the rate at which a factory produces working versus defective phones ( 
Figure 1
). On each of the 100 trials, participants inspected a new phone produced by the factory and learned whether it was working or defected. Participants then reported the probability that the next phone would be working (see Methods for detailed description). The true probability of a receiving a working phone changed trial-to-trial according to a Gaussian random walk (SD=0.1), bounded between 0.1-0.9; the initial value was drawn from a uniform distribution in the range 0.1-0.9. To incentivize accurate predictions, we offered participants a $0.03 bonus per response within 5% of the true probability. Attention checks were included during the task to ensure data quality; participants who did not pass our criteria for attention checks were excluded from analysis (see Methods). On each trial, a single phone was revealed to either be working or defective. Following each observation, participants were asked to rate on a scale from 0-100% what they thought was the likelihood of the next phone being a working phone.
We ran two versions of the task. In Study 1 (N = 455), we set the initial probability of a working phone to 0.5. This probability changed every trial according to a Gaussian random walk with SD=0.025; the random walk was unique for every participant. We used diffusion in the true reward rate in order to keep participants engaged with the task, as done in similar tasks 
(41)
(42)
(43)
.
In Study 2 (N = 177), the task was the same as in Study 1 except for two modifications. First, the initial value of the true probability of a working phone was randomly drawn from a uniform distribution U(0.1, 0.9) rather than being fixed at 0.5. Second, we increased the SD of the random walk from 0.025 to 0.1. The purpose of these modifications was to explore how SINDy performed across a broad range of task parameters. Neither Studies 1 or 2 were preregistered and all analyses should be considered exploratory.
We trained SINDy using data from all participants who met our inclusion criteria (see Methods for exclusions), separately for each study. Input data provided to SINDy were limited to participants'
reported expectations of observing a working phone , their observations of whether a phone was working or defective , and trial number t. We also provided SINDy with a matrix of candidate functions for feature selection, allowing for a variety of critic models to be identified. These included identity functions for previous expectations and reward, time-dependent decaying functions, and exponential functions for non-linearity (see Methods for specifics on candidate functions and fitting procedure). Consequentially, the Rescorla-Wagner model could be discovered by SINDy if it best explained the empirical data from either study. This was ensured through a series of simulation studies (see Methods: Simulations).
For Study 1, SINDy discovered the following novel model (R 2 =0.204):
+1 = 0.11 − 0.24 2
For Study 2, SINDy discovered a near identical model (R 2 =0.196):
+1 = 0.10 − 0.17 2
To demonstrate that these models were superior in fit to the Rescorla-Wagner model, we separately trained SINDy with a smaller matrix of candidate features limited to only the r-Q term. This limited SINDy to only discover the Rescorla-Wagner model. These limitations yielded worse fit in both studies (Study 1 R2=0.144; Study 2 R2=0.174).
Note that the coefficients of the discovered models' parameters are not symbolic and are fixed across participants. In both studies, SINDy discovered models of identical form, albeit with slightly different numerical values. We termed the model that SINDy produced the Quadratic Q-Weighted model since the model includes a quadratic term on previous expectation rather than a linear one (hence "Quadratic") and the model includes unequal scaling coefficients for present reward and previous expectation (Q-value; hence "Q-Weighted"). The Quadratic Q-Weighted model accounts for several interesting behavioral phenomena discussed in the results. Most importantly, the functional form of the Quadratic Q-Weighted model leads to an asymptotic bias in the estimation of the true probability. Namely, the model implies that over the long term, participants tend to underestimate Q values when reward probability is high and tend to overestimate Q values and reward probability is low. The transition between under-and over-estimation happens approximately when the true probability of reward is equal to a/b where:  
Figure 2
 demonstrate that the Quadratic Q-Weighted model implies the asymptotic bias in the estimation of the true probability. One example of the underestimation is that within this model an agent cannot predict Q values larger than the stable point √ / even when the reward probability is 1 -this is where expectations stabilize. Noisy agents, like humans, can occasionally predict Q values larger than √ / , but thereafter will be biased to lower their expectations back toward √ / even if met with further reward. To further explore the implications of the Quadratic Q-Weighted model on participants'
+1 = −
behavior, we employed linear mixed effects models to predict changes in expectations as a function of reward and distance from the stable point √ / . We conducted a total of four models; two for each study, one of which included only post-reward trials and the other post-non-reward trials. The independent variable in each of the models was whether the current Q value was lower or higher than the stable point √ / . The dependent variable was change in Q from previous trial. We dummy coded the model such that the data above the stable point would be the intercept of the model. This allowed us to not only compare significance between the conditions (above or below √ / ), but also compare  Error bars are 95% confidence intervals. 10% of observations are included as dots to visualize the response distribution. Decreases in Q can be observed when Q is greater than the stable point, even following reward.
Building on these findings, we next employed a complementary approach to balance the discovery of generalizable learning dynamics with the need to capture individual variability. Although the analysis conducted with the SINDy algorithm allowed us to identify the core functional form of a learning model by pooling data across participants, we recognize that pooling data in this way can obscure meaningful individual differences. To address this possibility, we next used the probabilistic programming language Stan 
45
 
Figure 4
. These results at the individual participant-level support our prior grouplevel analyses conducted with the support of SINDy: Participants' behavior in making probabilistic inference in our study is best explained by a novel RL algorithm, the Quadratic Q-Weighted model.  Although it may be necessary to tailor models to task-specific dynamics, perhaps like those of the go/no-go task, it is equally as valuable that models generalize across a wider range of decisionmaking contexts. Importantly, we did show in our empirical studies that the identified Quadratic Q- More broadly, this work highlights how data-driven discovery can complement theory-driven approaches, offering a path toward more interpretable and predictive models that deepen our understanding of human behavior.Methods Explaining SINDy. For our analysis, we used PySINDy (60, 61), a Python package that provides tools for applying the SINDy algorithm (34) for model discovery. SINDy is used to approximate the unknown governing equations of a dynamical system using a sparse regression framework. It assumes that the dynamics can be expressed as a sum of known functions, multiplied by unknown coefficients.
By leveraging sparsity-promoting techniques, SINDy aims to identify the most relevant terms in the equation, effectively providing a parsimonious representation of the system's behavior. Typically, SINDy estimates derivatives for system variables of the following form:
= ( , )
Where X is a matrix of observed variables to be modeled, U is a matrix of control variables that are not to be modeled but may be important for modeling variables in X, ( , ) is a matrix of candidate features selected by the researcher that transform the data, and B is a vector of coefficients that scale the candidate features. Through sparsification techniques such as L2 ridge regression, most of these coefficients are reduced to zero and only the most predictive features remain (see below for details on how we chose to promote sparsity). The objective of the SINDy algorithm is to solve for B
given the researcher selected candidate features and the approximated first-0rder derivatives of the observed data.
For most experiments in the social sciences, observed data are collected in discrete trials.
Therefore, we used SINDy to estimate discrete-time models of the form:
+1 = ( , )
Where are the observed variables to be modeled from X at timepoint k, and are the control variables (such as r and t in this case) at timepoint k. Rather than calculating a system of derivatives, using SINDy we calculated a matrix X' where the columns of X' are measures of x moved forward in time until the final datapoint at time K (i.e., 
[1,2,3, … ,̇]
). With this approach, SINDy estimates discrete-time equations for system variables in the form:
′ = ( , )
We solve for B by using a variation of stepwise sparse regression (SSR) 
(62)
 to minimize the objective function:
‖ ′ − ( , ) ‖ 2 2 + ‖ ‖ 2 2
Where each element of the coefficient vector B is regularized with the L2 ridge regression value in the penalty term. We chose = 0.2 for all analyses (simulations and empirical data).
Selecting the value of is crucial as it determines the trade-off between accuracy and parsimony; it is a hyperparameter that should be tuned by the modeler. On each iteration of the minimization procedure, SINDy first solves a standard least square regression to obtain a tentative, non-sparse solution b:
= argmin ∈ℝ ‖ ′ − ( , ) ‖ 2 2
All possible b are considered, each with one coefficient set to zero, and the solution b with the smallest residual error is selected. Least-squares regression is then again performed on the remaining degrees of freedom. This process continues until there is only one coefficient remaining. Next, we iterate in reverse order over the history of solutions starting from the simplest solution where all but one coefficient is set to 0. We continue to add non-0 coefficients back to the model until the next change in residual error is less than 0.05 times the previous iteration's residual error, at which point the process terminates, providing a sparse solution vector B. This provides a solution that is most parsimonious with the least loss. Like , this multiplier of 0.05 is also a hyperparameter and can be tuned to select a sparsity level for the solution that neither under-or over-fits the data.
Simulations. The first step of any SINDy analysis is collecting or simulating data to populate the timeseries observational data X and U. These functions were chosen since they provide necessary components to allow SINDy to discover existing theories of learning. Of course, SINDy can also find novel combinations of these candidate functions. Q and r are the building blocks for the model and serve as basic variables for delta-updating rules (2) and as changes may vary over time we also added t as a potential variable. A few decay rates (e.g., -t/10, -t/20) were chosen to allow for a variety of exponential shapes. We initialized the model with linear terms to allow for a classic Rescorla-Wagner model. We also introduced quadratic terms as we wanted to test whether changes in prediction of Q or in the evaluation of r may not be linear, in line with the notion that stimuli is represented by individuals with some form of power transformation 
(63)
. We also wanted to examine whether the two terms, Q and r, interact with each other or with time, and whether the absolute difference between Q and r might modulate learning.
Finally, we wanted to allow changes in both Q and r to decay at different rates. As can be seen from this list, we were conservative in our function choices in order to make sure that the resulting model was interpretable. Future work may use other terms in line with the researcher's goals and evaluation of the appropriate relationship between variables. informed consent was obtained from all research participants. In setting our sample size for Study 1, we decided to start with a large sample of 500 participants in order to determine the appropriate sample required for SINDy to make accurate predictions. We recruited participants through Prolific.
Participants were paid $4 for their participation in the study in addition to $.03 for every time that their estimate was within 5% of the true reward rate. Participants were given attention checks to ensure data quality. On the first and fiftieth trials, participants were asked to type a predetermined word. On the second trial, and every twenty trials thereafter, participants were asked to respond to the slider scale with a specific percentage. On these attention check trials, participants were instead prompted with "This is an attention check. Please move the slider to _%", where the percentage was a number between 0-100. Participants who failed either of the word typing checks or more than 2 of the slider checks were excluded from analysis. Because we assumed that some participants would not be able to complete the task, we recruited a larger number of participants than required N = 543. Our exclusion criteria were conditioned on attention checks (see Attention Checks and Exclusion Criteria below). We removed 3 participants for failed word typing checks and 85 for failed slider checks, for a After establishing the results from Study 1, we conducted an analysis to test the appropriate sample size required for SINDy to capture the appropriate model. We randomly sampled participants from Study 1 to find the smallest N necessary to reliably recover the Quadratic Q-weighted model. 100 iterations of this sampling procedure suggested that ~200 participants were enough for SINDy to reliably capture the model. In Study 2, we therefore aimed for N = 200. We again used Prolific for recruitment and paid participants the same sum as in Study 1. Our initial sample was 206. We used the same selection criteria for exclusions. We remove 29 participants for failed slider checks, for a final sample was N = 177 (Men: 87, Women: 85, other or refused to answer: 5; Age, M = 37.88, SD = 12.06).
Task. We used jsPsych 
(64)
 to conduct our study. Participants logged in and were told to imagine that they are inspecting a factory that produces phones. The factory produces phones one at a time and will then be inspected by the participant. Participants were told that the phone would wither be working on defective. Participants were told that they are asked to estimate the probability that the next phone will be a working phone (see 
Figure 2
). After a single practice trial, participants completed 100 trials of the task. Participants were first presented with an inspection slide, depicting a factory and a phone with a "?" printed on it. Participants were to click a button labeled "Inspect" below the images without any time constraints. After clicking the "Inspect" button the phone with the "?" printed on it was revealed to either be working with a green check mark, or defective with a red "X". Participants were required to observe this feedback screen for 3 seconds before they could advance the page. On the following slide, participants were asked to respond on a slider scale from 0% to 100% what they believed the probability was that the next phone would be a working phone. Participants had to move the slider from its initial value (50%) in order to make their prediction. Participants were given as much time as they needed to make this judgment, but were required to wait 3 seconds before they could respond. This completed a trial and was repeated for a total 100 trials. A fixation cross was presented for 1 second between trials. After completing the task, participants were sent to a Qualtrics survey to fill in their demographics.
Measures. When participants completed the learning task, they were asked to estimate the probability that the next phone will be defected. After completing the task, participants filled out a TIPI 10-item personality measure (65) and a short demographics survey in which they were asked for their name, age, gender, race, ethnicity, first language, political affiliation, citizenship, nation of birth, annual income, and email address. See SI for full analysis of demographics.


Phase 2: Evaluating Decision Models by assuming the Quadratic Q-Weighed Model in existing datasets
Paper selection. We identified papers and datasets for reanalysis using the Niv Lab OpenData repository (https://nivlab.github.io/opendata/). Tags "2-arm bandit", "restless bandit", and "twostep" were considered. Criteria for reanalysis included (1) having freely accessible trial-level data, 
(2)
 having freely accessible code for model fitting and analysis, (3) the use of a Rescorla-Wagner deltaupdating rule nested within the fitted model (4) and association with a published paper. Many datasets did not meet the criteria, narrowing our search to the following 9 datasets:
Kool et al., 2017 
Experiments 1 & 2 (66)
. Published in Psychological Science (https://osf.io/yg82m/).
The goal of this project was to examine whether people choose between model-free versus modelbased control based on a cost-benefit analysis. The task was based on the Daw two-step decision making task 
(67)
. Participants made a first choice between two spaceships (green or blue), each leading to two planets with different probabilities in each of the studies (red, or purple). In Study 1, the probability of getting to a certain planet with a certain ship was always 100%. In Study 2 the probability of getting to one of the two planets was always 70% for each spaceship. When they arrived to the planet participants met either one alien (Experiment 1) or chose one two aliens 
(Experiment 2)
 who gave them a reward. Aliens were either in a good or a bad part of a mine and the probability of quality of their reward changed over time (drift rate in reward). In both studies, the researchers manipulated the size of the reward (stakes: 1 point or 5 points).  
(57)
. Published in Nature Human Behavior. Each of these datasets and analysis code were acquired from a meta-analytic study, Palminteri, 2023(70) (https://github.com/spalminteri/conf-biasmeta-analysis). We followed the models of Palminteri, 2023 for reanalyzing each of these four datasets. Shared amongst the authors was an interest in the confirmation bias hypothesis, in which a person learns more from positive reinforcement that supports their preexisting biases than they do negative reinforcement disproving their beliefs. The tasks were all variants of a simple two-armed bandit. Participants chose between two alternatives presented as symbols and either received or did not receive reward. Each bandit had a predesignated probability of reward but in all tasks, participants observed the outcome of their chosen symbol, but received no information from the unchosen symbol. 
Figure 1 .
1
Structure of learning task used in Studies 1 and 2. Participants inspected phones produced from an assembly line.


Figure 2 .
2
An overview of behavior of the Quadratic Q-Weighted model we discovered using empirical data with SINDy. The x-axes reflect reported Q value and the y-axes are the median change in value. Grey dots show binned Q into 10 discrete categories, each with a bin size of 0.1. Categories were labeled with the upper bound of each bin. Error bars are 95% confidence intervals. (A) Study 1 empirical change in Q following no reward. (B) Study 1 empirical change in Q following reward. (C) Study 2 empirical change in Q following no reward. (D) Study 2 empirical change in Q following reward. Predicted changes in Q according to the best fit Quadratic Q-Weighted model (solid red) and the best fit Rescorla-Wagner model (dashed blue) are overlaid.


Figure 3 .
3
Empirical changes in expectation Q as a function of Q's position relative to the stable point and reward (√ / ).


Figure 4 .
4
Expected value estimates (Qt) over 100 trials for a single representative participant. The "Empirical" panel represents the participant's reported values (black lines), while the remaining panels depict predictions (red lines) from models: RW (Rescorla-Wagner), RW with exponential decay, RW with asymmetric learning rates, QQW (Quadratic Q-Weighted), and the Kalman Filter. Missing data correspond to trials where attention checks were administered. Although all models demonstrate a generally good fit to the observed data, the QQW model stands out with a superior fit.Having established theQuadratic Q-Weighted model's superior fit at both the group and individual levels, we next examined the distribution of individualized parameters to explore the extent of heterogeneity in participants' learning behavior. Beyond the superior group-level fit indicated by BIC, we also find at the individual-level that the Quadratic Q-Weighted model best fit 68.35% of participants in Study 1, and 64.41% in Study 2. Importantly, the a and b coefficients scaling the reward and Q terms ( +1 = − 2 ) were free to vary between participants in these fitted models. This revealed substantial heterogeneity amongst individuals around the group coefficient values discovered by SINDy: Study 1 mean a = 0.21 ± 0.18, Study 1 mean b = 0.45 ± 0.46; Study 2 mean a = 0.28 ± 0.19, Study 2 mean b = 0.61 ± 0.52. These findings suggest significant individual differences in how participants weigh recent rewards and adjust for previous estimates.


Weighted
model was robust to changes in initial reward probability and to varying levels of diffusion noise. However, the model's utility in tasks where reward probabilities remain static over time or where probabilities are more conservatively bounded has yet to be tested. Future studies could further evaluate the model under these conditions, such as the change-point detection studies conducted by Nassar and colleagues
(58,
59)
, where participants are asked to predict continuously varyingoutcomes. These tasks involve different reward structures that could help determine whether the nonlinearity identified in our model captures more general aspects of human learning, particularly under conditions of uncertainty and dynamically changing environments. Future research should extend the model to such decision-making problems and explore additional nonlinearities, such as exponential or logarithmic transformations. It is also important that future research consider edge cases. An important aspect of the Quadratic Q-Weighted model is its prediction of asymptotic behavior, particularly in situations where the model suggests an inherent bias in how individuals estimate extreme probabilities. Specifically, the model predicts that participants cannot estimate Q values beyond a certain stable point (√(a/b)), even in situations where the true reward probability is maximal (e.g., 1). This creates an opportunity for empirical validation, whereby edge cases can be specifically designed to test these predictions. For example, future experiments could involve a prolonged sequence of trials with a constant reward probability of 1 to determine if participants' estimates truly converge at the stable point predicted by the model or if they adaptively reach the true value. If participants are found to be limited by this predicted asymptotic bias, it would lend further support to the model's validity. Conversely, if participants adapt beyond the predicted stable point, it may indicate the need for further refinement of the model. Such empirical tests would help to identify the conditions under which the model captures or fails to capture human learning behavior and highlight the importance of understanding model limitations within different reinforcement learning environments. That said, we acknowledge that the model may fail to predict behavior under such rigid conditions. Although future studies have the potential to further validate the Quadratic Q-Weighted model, we suggest that the model's scope be interpreted with the constraints of the learning environment in mind. Overall, this study demonstrates the potential of bottom-up equation discovery methods, such as SINDy, to advance model development in the social sciences. The Quadratic Q-Weighted model provides key insights into human learning, uncovering systematic biases in probability estimation and generalizing across diverse datasets. By improving fit and influencing interpretations of prior studies, the model showcases the power of integrating non-linear dynamics into decision-making frameworks.


final sample of N = 455 (Men: 216, Women: 216, Other or refused to answer: 23; Age, M = 36.25, SD = 12.94).


Lefebvre et al., 2017 Experiments 1 & 2 used probabilities 25%/75%, Chambon et al., 2020 Experiment 4 used 30%/70%, and Palminteri et al. Experiment 1 used 50%/50%, 25%/75%, and a 17%/83%. These probabilities reversed halfway through the task, depending on assignment to experimental conditions. Decker et al., 2016(71). Published in Psychological Science; Potter et al., 2017(72). Published in Developmental Cognitive Neuroscience; and Nussenbaum et al., 2020(73) Published in Collabra: Psychology. Each of these three datasets and analysis code were acquired from a reanalysis conducted by Nussenbaum et al., 2020 (https://osf.io/we89v/). The authors all investigated the emergence of model-based control across development, using the classic version of the Two-Step task as a propensity measure of model-based control. The task was identical to the version used in Kool et al., 2017 Experiment 2, without manipulating the size of reward. Data and Code Availability All simulation and empirical data are available on the Open Science Framework here: https://osf.io/aeujf/?view_only=88b2b75499f54a3895502fc353f4d244. All analysis scripts and modeling code are available on Github here: https://github.com/GoldenbergLab/analysis-rl-sindykyle.


Figure 2 illustrates why the Quadratic Q-Weighted model implies such over/under estimation, showing the change in Q as a function of either reward or no-reward and as dependent on previous Q (see SI for a proof of this point of under-to-over estimation). For low values of Q, the change in Q in
the Quadratic Q-Weighted model is positively shifted both for reward and no reward compared to
classic Rescorla-Wagner when Q values are low. Conversely, for high values of Q, the change in Q in
the Quadratic Q-Weighted model is negatively shifted both for reward and no reward compared to
classic Rescorla-Wagner when Q values are high. Since the Rescorla-Wagner model asymptotically
always converges to the true probability for any learning rate (44), the shifts shown in


results from above the stable point to zero. Our model also included a random variable of participant id. Starting with the intercept of the model, which compared the above the stable point results to zero, results suggested that in both Study 1 and Study 2, when received a reward and when they were above
the stable point, participants significantly lowered their estimation of Q (Figure 3 Orange bar
compared to 0; Study 1: b=-0.167, p<0.001; Study 2: b=-0.085, p<0.001). These results would not
have been seen if participants were using a classical Rescorla-Wagner model in which participants
always increase their estimation of Q following a reward. Similar results were found in cases where
there was no reward, such that when above the stable point, participants also significantly lowered
their estimation of Q (Figure 3 Orange bar compared to 0; Study 1: b=-0.291, p<0.001; Study 2: b=-
0.328, p<0.001). These results should be expected, as both in our model and in a classic Rescorla
Wagner model, participants would lower their estimation of Q following a no-reward. Having
established this difference from zero, results also suggested that in all cases, there was a significant
difference in change in Q as a function of whether the previous Q was above or below the stable point (Study 1 Rewarded: b=0.320, p<0.001; Study 1 Unrewarded: b=0.341, p<0.001; Study 2 Rewarded: b=0.223, p<0.001; Study 2 Unrewarded: b=0.337, p<0.001). These results are congruent with Rescorla-Wagner.


Evaluating Decision Models by assuming the Quadratic Q-Weighed Model in existing datasetsLimitations and Future Directions We
To further probe this variability, we allowed the exponent on Q to vary freely as an exploratory follow-up. This adjustment improved the model fit for 61.1% of participants in Study 1 and 77.96% in Study 2, indicating that the fixed exponent of 2 used in the original Quadratic Q-Weighted model does not fully capture all individual differences. These individualized exponents were estimated to be on average 1.52 ± 0.76 in Study 1 and 1.43 ± 0.81 in Study 2, indicating a high degree of variability amongst individuals in how they update expectations.After finding the Quadratic Q-Weighted model with empirical data, we aimed to demonstrate its value by reanalyzing prior studies of choice behavior. Our goal was to determine whether the Quadratic Q-Weighted model provided a better account of learning in decision-making tasks than does the Rescorla-Wagner model. The vast majority of decision-making tasks do not probe estimates of probability overtly; they instead ask participants to act on implicit learned probabilities by selecting between two or more alternatives. Researchers are often most interested in the elements governing these selections, such as explore-exploit tendencies and stochasticity. However, because it is assumed that selection depends on one's estimates of reward probability, it is critical that researchers assume a learning model that best captures those estimates and their dynamics. To this end, we reanalyzed open datasets sourced from nine papers published in leading academic journals, each using the Rescorla-Wagner updating rule nested within larger decision models, with the goal of replacing that rule with our Quadratic Q-Weighted model. In each of the datasets, we used the authors' original analysis scripts for model fitting (See SI for details). To compare the authors' model and our variation with the Quadratic Q-Weighted model as a learning rule, we modified the original authors' scripts to fit a variation of their model using the general form of the Quadratic Q-Weighted model in place of Rescorla-Wagner. Notice that the model produced by SINDy in the empirical phase had a specific coefficient value for each term. In these analyses we allowed those coefficients to vary freely between subjects, as is common in the field.To compare the model that was used in each paper to our Quadratic Q-Weighted we calculated the Bayesian information criterions (BIC) of both models using the summed likelihood estimates of each participant's data. BIC was chosen due to its consistency in identifying parsimonious yet wellfitting models by penalizing more heavily for superfluous parameterization. Likewise, BIC was used by all authors of the selected datasets. In all but one dataset, the model using Quadratic Q-Weighted learning rule outperformed the original best model(Table 1). Beyond fit, models using the Quadratic Q-Weighted learning rule may provide downstream benefits to understanding processes of decision-making. For example, one dataset in particular, Kool et al., 2017 Experiment 2, SINDy's yielded decision-making parameter estimates that diverged from the authors' main findings. In their 2017 article, Kool and colleagues proposed that arbitration between model-based (MB) and model-free (MF) learning systems involves a cost-benefit analysis. The MB system, which plans towards goals, is more accurate but computationally demanding, whereas the MF system relies on habits and is computationally efficient but less flexible. Kool et al. suggested that people decide which system to use by weighing the benefits of the MB system's accuracy against its cognitive cost. This implies that MB control is employed when its benefits (in terms of reward) outweigh the costs (in terms of cognitive effort). The most common method for measuring individual difference in this cost-benefit analysis is with the Two-Step Task (Daw et al., 2011), a complex, multi-step decision-making task. Despite this, Kool et al. demonstrated in an earlier paper (Kool et al., 2016) that there exists no cost-benefitrelationship between model-based vs. model-free strategy and performance on the task, and remediate this with a novel version of the task. To explore this further, they tested the effect of monetary stakes on strategy use in the original task, positing that high stakes should fail to yield increased MB control They found that there was no difference in MB control between high and low stakes on the original version of the task (Experiment 2; t(99)=0.4132, p=0.6804). In interpreting these findings, the authors suggest that participants might have a prior belief that MB control is generally associated with higher rewards, driven by real-world experiences where MB control is usually beneficial. This belief, reinforced by training, led participants to maintain a mix of MB and MF strategies. This is because high stakes can be stressful and impose cognitive load. MB control is costlier, despite it being equally as effective in this case, and the cognitive effort required to use MB control could be too demanding in high stakes situations. It is possible that parameter estimates could be improved and better reflect our hypothesized effect if a novel learning model were fit to the data which better reflects peoples' true learning. To test this, we modified the fitted dual systems RL model to instead use the Quadratic Q-Weighted model discovered by SINDy in our empirical studies. We found that our version of the dualsystems model with the Quadratic Q-Weighted model as a learning rule better fit Kool et al.'s Experiment 2 data (BICQQW=450.23) than the original dual systems RL model (BICRW=474.32). Note: BIC is Bayesian Information Criterion. Lower BIC reflects better model fit. Original BIC is for the model used by the authors of the dataset. Quadratic Q-weighted BIC is for the variation of those models using the Quadratic Q-Weighted model as a nested learning rule in place of the Rescorla-Wagner learning rule. Across multiple levels of analysis, what stands out as the most important feature of the Quadratic Q-Weighted model is its prediction that participants tend to overestimate low probabilities and underestimate high probabilities. This systematic bias could be reminiscent of a persistent prior in Bayesian inference (47, 48)). In Bayesian models, a prior represents the learner's initial beliefs about This bias exists even when expectations of reward are high and reward is received, such that expectations are predicted to decrease. This raises an interesting question about the cognitive processes underlying such behavior: are individuals relying on a heuristic that resembles the application of a prior, or is there an explicit internal representation akin to Bayesian updating? Future work could explore this connection further by comparing the performance of the Quadratic Q-Weighted model with explicit Bayesian models that incorporate a persistent prior. Additionally, empirical studies could manipulate participants' initial beliefs or expectations to determine whether similar biases are observed, thereby testing whether the quadratic form is indeed capturing the influence of a persistent prior While the current work focused on model development within RL, we envision many exciting new directions for model development in various social domains. Many subdomains in social sciencesare still utilizing existing top-down models and these models can potentially be improved while maintaining interpretability. For example, models of social contagion, such the SIR epidemic models acknowledge several limitations in using SINDy for model discovery in social science. First, our implementation is at present limited to modeling directly observable data. Across our empirical studies, Q-value was an explicit variable. In many RL experiments, expected value is a latent variable that is estimated from directly observable decisions
(47,
55)
. Other models could include these latent variables, such as for uncertainty in expectations, which could potentially improve their quality in terms of predictability and generalizability. Despite these limitations, the approach we adopted here has the potential to change how models are developed in the social science.A second limitation is that the SINDy algorithm is bounded by the specific decisions made in its implementation, such as the list of candidate functions and hyperparameters that govern the discovered model's sparsity (and hence control over its complexity). These decisions, as well as the indication of whether the discovered model is suitable, are subjective. Therefore, it is possible that there may exist other alternative models which could be better fitting. For example, the present study omitted candidate functions of continuous time. Although our discovered Quadratic Q-Weighted model provides a novel perspective on probability weighting, it departs from the broader theoretical basis of RL models like Temporal Difference (TD) learning
(2)
, which can be applied in real-time and have demonstrated strong links to learning processes in the brain
(44,
56)
. The quadratic transform used here, although beneficial for capturing nonlinearity in probability estimates, does not provide the same foundation for understanding learning as a general, time-continuous process. However, it may be possible to incorporate the discovered nonlinearity within a TD learning framework. Specifically, future work could modify the TD update equations to include a transformed value function, such as f(V)=V+βV2, allowing us to retain the incremental, time-based learning properties of TD while introducing systematic biases that capture non-linearities in human learning behavior. This would create a hybrid model that not only improves predictive performance but also preserves the dynamic learning structure of TD, potentially bridging these two perspectives effectively.A third limitation of this study is that the model that was discovered by SINDy was based on a relatively narrow empirical task. Although the Quadratic Q-Weighted model showed excellent generalizability in predicting behavior across two new empirical studies and eight of nine reanalyzed datasets, its broader applicability requires further exploration. One notable exception to the Quadratic Q-Weighted model's superior performance was observed in a go/no-go dataset
(57)
, where the Rescorla-Wagner model provided a better fit to the underlying learning process. We note that this dataset's structure-featuring a large number of trials per participant (N=600) but a small number of participants (N=20)-may have reduced its ability to robustly differentiate between models. However, the unique demands of go/no-go tasks, which rely heavily on inhibitory control, may inherently favor Weighted model captures well. Moreover, the candidate features that we provided to SINDy for modeling learning in forced-choice tasks may not adequately capture learning dynamics unique to go/no-go tasks. These findings may suggest that tailoring candidate functions to task-specific dynamics is crucial for improving the generalizability and performance of discovered models.
Original BIC
Quadratic Q-Weighted BIC
Potter et al. 2017
25784.64
25373.13
Quadratic Q-Weighted BIC 62360.65 the probability distribution of an outcome. If individuals maintain a strong prior centered around a Nussenbaum et al. 2020 63378.57 moderate probability value (e.g., 0.5) and continue to apply this prior across multiple learning trials, it will naturally lead to a pattern where extreme probabilities are systematically pulled toward the center-low probabilities are overestimated, and high probabilities are underestimated. This behavior observe this in both our first empirical study where all reward probabilities start at 0.5, thereby encouraging the integration of this prior, and in our second empirical study where starting probabilities vary. (49), which already benefit from further development using tools such as SINDy (19), can also be tested in explaining social interaction and contagion. Other domains such as decision making (50), Phase 2: Original BIC is qualitatively similar to the effects captured by our model's quadratic weighting term. And we planning (51), norm formation (52), affect (53) and many others all have models that are constantly
Kool et al. 2017 Experiment 1
461.35
458.47
Kool et al. 2017 Experiment 2
474.32
450.23
Lefebvre et al. 2017 Experiment 1
3857.55
3806.44
Lefebvre et al. 2017 Experiment 2
2512.77
2496.62
Palminteri et al. 2017 Experiment 1
3206.48
3199.18
Chambon et al. 2020 Experiment 4
10987.38
10997.49
Decker et al. 2016
37463.96
36956.91
when it provides no advantage They tested this by fitting a dual-systems RL model to their data (Daw et al., 2011). This RL model includes three separate Rescorla-Wagner updating rules for changing participants' expectations of reward followed by choosing specific spaceships and aliens.We disagreed with this conclusion: If MB control is no better than MF on the original version of the task, participants should use MB control less than MF under high stakes.Furthermore, we observed that the fitted free parameters for the MB weight in high stakes was indeed significantly less than the MB weight in low stakes (t(99)=2.9303, p=0.0042), supporting our hypothesis. Table 1. Model fits from each reanalyzed datasets using original authors' models and variations replacing Rescorla-Wagner learning rules with the Quadratic Q-Weighted model.Discussion In this work we demonstrated that bottom-up equation discovery algorithms can be used for model development in social sciences. We collected empirical data in two variations of a learning task and used SINDy to develop an appropriate model for participants' behavior. This novel model -the Quadratic Q-Weighted model -provided new insights into human learning and accounts for several interesting behavioral phenomena. Most importantly, the model introduces a tendency over the long term to underestimate Q values when true reward rates are high and overestimate Q values and true reward rates are low. Finally, we nested the Quadratic Q-Weighted model within existing, more complex decision models used by the authors of nine published datasets. These models notably were of decisions made on complex decision-making tasks, entirely different from our probability estimation task. We found that the new models using our Quadratic Q-Weighted model instead of the Rescorla- Wagner updating rule provided a better fit in eight out of nine of those cases compared to the models originally used by the authors. We further demonstrate that the Quadratic Q-Weighted model does more than simply improve fit; it impacted the conclusions and interpretation of a previous study of complex decision-making. These results provide a promising path for the use of the Quadratic Q- Weighted model, as well as the use of equation discovery algorithms to development of interpretable but more predictive computational models.The persistence of such a prior, even after observing evidence that should shift beliefs significantly, reflects a form of 'conservatism' in updating. Instead of fully integrating new information, individuals may effectively average it with their prior beliefs, leading to a systematic under-adjustment. This can explain why the observed estimates do not align perfectly with the objective probabilities but are instead biased toward intermediate values. In our model, the quadratic term captures these non-linearities in value estimation without explicitly invoking a Bayesian prior. However, the resulting behavior-where estimates of extreme probabilities are biased toward the middle-suggests that the quadratic transformation may implicitly represent the effect of a strong, persistent prior belief.being developed using top-down approaches, and these domains could potentially benefit from using equation discovery algorithms for model improvement and development. Finally, many domains in the social sciences involve analysis of longitudinal data, which is often analyzed using structural equation modeling or other tools that mostly test for linear processes(54). SINDy and other equation discovery tools are well suited to fit existing longitudinal data in order to uncover driving equations. It's important to note that we do not wish to eliminate hypothesis testing or theory-based models, but rather to expand the modeler's toolbox in considering alternative models for comparison and later confirmatory analyses, thus encouraging the discovery of novel, interpretable, predictive and generalizable models.simpler models like Rescorla-Wagner. Unlike the other datasets analyzed, the go/no-go task may not highlight the nuanced reward-probability interactions or asymptotic behaviors that the Quadratic Q-


: Equation Recovery from Simulated Data
To investigate the applicability of SINDy in recovering the governing equations of established reinforcement learning models, we generated synthetic data through simulations. We selected well-known reinforcement learning models -the Rescorla-Wagner model and variants -as the basis for generating the data. See SIfor details. Our simulated agents estimated the probability of reward as X. The history of reward and trial number were represented as separate columns in U. From these observations, we used SINDy to calculate X' as the matrix of discrete-time variables xk shifted xk+1. To ensure that SINDy's solution B was interpretable, we provided SINDy with the following matrix of
candidate functions:
( , ) = [
2
2
*
*
*
1 + 100
− 30
− 20 ⋯ ]
[⋯
− 10
+ 100
*  − 30
*  − 20
*  − 10
+ 100
| − | | −
2 | ⋯ ]
[⋯  *  − 30
*  − 20
*  − 10
+ 100
*  − 30
*  − 20
*  − 10 ⋯ ]


Phase 1: Equation Discovery from Empirical Probability Estimates.
Participants. All experiments received IRB approval from Harvard Business School (IRB22-0546) and


Lefebvre et al., 2017 Experiments 1 & 2(68). Published in Nature Human Behavior; Palminteri et al., 2017 Experiment 1(69). Published in PLOS Computational Biology; and Chambon et al., 2020 Experiment 4














How computational modeling can force theory building in psychological science




O
Guest






A
E
Martin








Perspect Psychol Sci




16
















Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








MIT press












What is dopamine doing in model-based reinforcement learning? Current Opinion in Behavioral Sciences




T
Akam






M
E
Walton








38














The ubiquity of model-based reinforcement learning




B
B
Doll






D
A
Simon






N
D
Daw








Current Opinion in Neurobiology




22
















Believing in dopamine




S
J
Gershman






N
Uchida








Nat Rev Neurosci




20
















Reinforcement learning in artificial and biological systems




E
O
Neftci






B
B
Averbeck








Nat Mach Intell




1
















Reinforcement learning in the brain




Y
Niv








Journal of Mathematical Psychology




53




















M
Campbell






A
J
Hoane






F
Hsu






Deep
Blue








Artificial Intelligence




134
















Mastering the game of Go with deep neural networks and tree search




D
Silver








Nature




529
















Grandmaster level in StarCraft II using multi-agent reinforcement learning




O
Vinyals








Nature




575


















J
Von Neumann






O
Morgenstern




Theory of games and economic behavior




Princeton University Press














On the possible psychophysical laws




R
D
Luce








Psychological Review




66
















Prospect theory: An analysis of decision under risk




A
Tversky






D
Kahneman








Econometrica




47
















Reference-dependent risk sensitivity as rational inference




J
C
Denrell








Psychological Review




122
















A meta-analytic review of two modes of learning and the description-experience gap




D
U
Wulff






M
Mergenthaler-Canseco






R
Hertwig








Psychol Bull




144
















The reversed description-experience gap: Disentangling sources of presentation format effects in risky choice




A
Glöckner






B
E
Hilbig






F
Henninger






S
Fiedler








J Exp Psychol Gen




145
















A theory of Pavlovian conditioning : Variations in the effectiveness of reinforcement and nonreinforcement




R
A
Rescorla






A
Wagner








Classical Conditioning II: Current Research and Theory




Appleton-Century-Crofts














What do reinforcement learning models measure? Interpreting model parameters in cognition and neuroscience




M
K
Eckstein






L
Wilbrecht






A
G
Collins








Current Opinion in Behavioral Sciences




41
















Algorithmic discovery of dynamic models from infectious disease data




J
Horrocks






C
T
Bauch








Sci Rep




10


7061


















L
K
Bartlett






A
Pirrone






N
Javed






F
Gobet








Computational Scientific Discovery in Psychology. Perspect Psychol Sci




18
















Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences




A
Almaatouq








Behavioral and Brain Sciences




1


55
















10.1017/S0140525X22002874














Using deep learning to predict human decisions and using cognitive models to explain deep learning models




M
Fintz






M
Osadchy






U
Hertz








Sci Rep




12


4736














Integrating explanation and prediction in computational social science




J
M
Hofman








Nature




595
















Automatic discovery of cognitive strategies with tiny recurrent neural networks




L
Ji-An






M
K
Benna






M
G
Mattar




10.1101/2023.04.12.536629v2








27












Modelling human behavior in cognitive tasks with latent dynamical systems




P
I
Jaffe






R
A
Poldrack






R
J
Schafer






P
G
Bissett








Nat Hum Behav




7
















Extracting the dynamics of behavior in sensory decision-making experiments




N
A
Roy






J
H
Bak






A
Akrami






C
D
Brody






J
W
Pillow








Neuron




109
















Cognitive Model Discovery via Disentangled RNNs




K
J
Miller






M
Eckstein






M
M
Botvinick






Z
Kurth-Nelson




10.1101/2023.06.23.546250v1








16












Scaling up psychology via Scientific Regret Minimization




M
Agrawal






J
C
Peterson






T
L
Griffiths








Proceedings of the National Academy of Sciences




117
















Automated reverse engineering of nonlinear dynamical systems




J
Bongard






H
Lipson








Proceedings of the National Academy of Sciences




104
















Distilling Free-Form Natural Laws from Experimental Data




M
Schmidt






H
Lipson








Science




324
















Sparse identification of nonlinear dynamics for model predictive control in the low-data limit




E
Kaiser






J
N
Kutz






S
L
Brunton








Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences




474


20180335














Model selection for dynamical systems via sparse regression and information criteria




N
M
Mangan






J
N
Kutz






S
L
Brunton






J
L
Proctor








Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences




473


20170009














Data-driven discovery of partial differential equations




S
H
Rudy






S
L
Brunton






J
L
Proctor






J
N
Kutz








Science Advances




3


1602614














Discovering governing equations from data by sparse identification of nonlinear dynamical systems




S
L
Brunton






J
L
Proctor






J
N
Kutz








Proceedings of the National Academy of Sciences




113
















Data-driven discovery of reduced plasma physics models from fully kinetic simulations




E
P
Alves






F
Fiuza








Phys. Rev. Research




4


33192














Learning Discrepancy Models From Experimental Data




K
Kaheman






E
Kaiser






B
Strom






J
N
Kutz






S
L
Brunton










18






Accessed








Sparse structural system identification method for nonlinear dynamic systems with hysteresis/inelastic behavior




Z
Lai






S
Nagarajaiah








Mechanical Systems and Signal Processing




117
















Sparse identification for nonlinear optical communication systems: SINO method




M
Sorokina






S
Sygletos






S
Turitsyn








Opt. Express, OE




24
















Inferring biological networks by sparse identification of nonlinear dynamics




N
M
Mangan






S
L
Brunton






J
L
Proctor






J
N
Kutz








IEEE Transactions on Molecular, Biological and Multi-Scale Communications




2
















Equations of mind: Data science for inferring nonlinear dynamics of sociocognitive systems




R
Dale






H
S
Bhat








Cognitive Systems Research




52
















Reminders of past choices bias decisions for reward in humans




A
M
Bornstein






M
W
Khaw






D
Shohamy






N
D
Daw








Nat Commun




8


15958














Cortical substrates for exploratory decisions in humans




N
D
Daw






J
P
O'doherty






P
Dayan






B
Seymour






R
J
Dolan








Nature




441
















Uncertainty and exploration in a restless bandit problem




M
Speekenbrink






E
Konstantinidis








Topics in Cognitive Science




7
















Temporal difference models and reward-related learning in the human brain




J
P
O'doherty






P
Dayan






K
Friston






H
Critchley






R
J
Dolan








Neuron




38
















Stan: A probabilistic programming language




B
Carpenter








Journal of Statistical Software




76














A simple model for learning in volatile environments




P
Piray






N
D
Daw








PLoS Comput Biol




16


1007963














Bayesian theories of conditioning in a changing world




A
C
Courville






N
D
Daw






D
S
Touretzky








Trends in Cognitive Sciences




10
















Brain networks for confidence weighting and hierarchical inference during probabilistic learning




F
Meyniel






S
Dehaene








Proceedings of the National Academy of Sciences




114
















A contribution to the mathematical theory of epidemics




O
W
Kermack






A
G
Mckendrick








Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences




115














Using large-scale experiments and machine learning to discover theories of human decision-making




J
C
Peterson






D
D
Bourgin






M
Agrawal






D
Reichman






T
L
Griffiths








Science




372
















People construct simplified mental representations to plan




M
K
Ho








Nature




606
















The emergence of social norms and conventions




R
X D
Hawkins






N
D
Goodman






R
L
Goldstone








Trends in Cognitive Sciences




23
















Tears and fears: Modeling emotions and emotional behaviors in synthetic agents




J
Gratch






S
Marsella




10.1145/375735.376309








Proceedings of the fifth international conference on Autonomous agents


the fifth international conference on Autonomous agents


















Structural Equation Modeling: Reviewing the Basics and Moving Forward




J
Ullman








Journal of personality assessment




87
















A unifying probabilistic view of associative learning




S
J
Gershman








PLoS Computational Biology




11
















Temporal difference models describe higher-order learning in humans




B
Seymour








Nature




429
















Information about action outcomes differentially affects learning from selfdetermined versus imposed choices




V
Chambon








Nat Hum Behav




4
















An Approximately Bayesian Delta-Rule Model Explains the Dynamics of Belief Updating in a Changing Environment




M
R
Nassar






R
C
Wilson






B
Heasly






J
I
Gold








J. Neurosci




30
















Rational regulation of learning dynamics by pupil-linked arousal systems




M
R
Nassar








Nat Neurosci




15
















PySINDy: A Python package for the Sparse Identification of Nonlinear Dynamics from Data




B
M
Silva










27






Preprint] (2020). Available at








PySINDy: A comprehensive Python package for robust sparse system identification




A
A
Kaptanoglu








JOSS




7


3994














Sparse learning of stochastic dynamical equations




L
Boninsegna






F
Nüske






C
Clementi








The Journal of Chemical Physics




148


241723














On the psychophysical law




S
S
Stevens








Psychological Review




64
















jsPsych: Enabling an open-source collaborative ecosystem of behavioral experiments




J
R
De Leeuw






R
A
Gilbert






B
Luchterhandt








Journal of Open Source Software




8


5351














A very brief measure of the Big-Five personality domains




S
D
Gosling






P
J
Rentfrow






W
B
Swann








Journal of Research in Personality




37
















Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems




W
Kool






S
J
Gershman






F
A
Cushman








Psychol Sci




28
















Model-Based Influences on Humans' Choices and Striatal Prediction Errors




N
D
Daw






S
J
Gershman






B
Seymour






P
Dayan






R
J
Dolan








Neuron




69
















Behavioural and neural characterization of optimistic reinforcement learning




G
Lefebvre






M
Lebreton






F
Meyniel






S
Bourgeois-Gironde






S
Palminteri








Nat Hum Behav




1
















Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing




S
Palminteri






G
Lefebvre






E
J
Kilford






S.-J
Blakemore








PLoS Comput Biol




13


1005684














Choice-confirmation bias and gradual perseveration in human reinforcement learning




S
Palminteri








Behavioral Neuroscience




137
















From creatures of habit to goal-directed learners: Tracking the developmental emergence of model-based reinforcement learning




J
H
Decker






A
R
Otto






N
D
Daw






C
A
Hartley








Psychol Sci




27
















Cognitive components underpinning the development of model-based learning




T
C S
Potter






N
V
Bryce






C
A
Hartley








Developmental Cognitive Neuroscience




25
















Moving developmental research online: comparing in-lab and web-based studies of model-based reinforcement learning




K
Nussenbaum






M
Scheuplein






C
V
Phaneuf






M
D
Evans






C
A
Hartley








Collabra: Psychology




6


17213















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]