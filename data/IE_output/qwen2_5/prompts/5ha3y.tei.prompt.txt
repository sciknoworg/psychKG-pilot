You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



INTRODUCTION
Decision making in everyday life depends on predicting outcomes associated with potential choices.
These choices can be defined through a reinforcement learning algorithm, a modelling framework that uses the prediction error as a learning signal to update future outcome expectations 
[1]
. Prediction error represents the difference between actual and expected outcomes, and has a positive or negative valence.
When a decision outcome exceeds expectations, the value associated with the chosen option is increased, making it more likely to be chosen again. When a decision outcome is less than expected, the value associated with the chosen option is decreased, making it less likely to be chosen again. Prediction error also functions as surprise, based on the degree of absolute prediction error. When people are confronted with unexpected events, regardless of its valence, they experience surprise. A greater surprise increases the strength of the association between outcome and expectations 
[2,
3]
. However, how surprise affects the value of outcome and subsequent decision making remains unclear.
Recent human and monkey experimental studies have shown that surprisal events or outcomes have a negative impact on individuals, even when the error is positive 
[4,
5]
. Topolinski and Strack 
[5]
 employed facial electromyography to measure people's responses immediately following surprising information; they observed that participants flexed their corrugator muscles, corresponding to a furrowing of the eyebrows, which indicated negative valence. Knight et al. 
[4]
 showed that, in a predictable food taking paradigm, rhesus monkeys were slower to accept unexpected offers and exhibited aversive reactions such as repeatedly turning their heads and looking away before accepting the food item, especially in response to better-than-expected offers. These studies indicate that surprise decreases, at least initially, the value of an event for individuals, and decreases the value of the offer associated with the event.
The function of surprise as reducer of outcome value can be explained from the perspective of predictive coding, a computational theory of how the brain facilitates perception from sensation by minimizing the prediction error between the expected and received sensory input 
[6,
7]
. Some researchers have proposed that surprise affects the reward value of a decision outcome 
[8]
[9]
[10]
, stating that an increase of surprise decreases the value of the outcome, since inconsistency corresponds to an uncertainty about what to do that is generally associated with negative affect. For example, Van de Cruys 
[9]
 suggested that a positive affect is induced when prediction error is reduced, while a negative effect is induced when a 5 situation with lower prediction errors shifts to one with higher prediction errors. Further, understanding how surprise affects decision-making is important to understand the underlying mechanisms of psychiatric disorders, such as autism spectrum disorder (ASD), which is characterized by an avoidance of social situations in which prediction errors often happen 
[11]
[12]
[13]
.
Here, we propose the surprise-sensitive utility model, in which the degree of outcome surprise affects the outcome value. We investigated how the negative value of a surprise outcome affects subsequent decisions by using two reinforcement learning tasks, the risky probabilistic learning task 
[14]
, and the simple two-armed bandit task. We predicted that a surprise outcome that decreases the value of a decision outcome will decrease the preference for risky choices, in which prediction error often happens.
To investigate this prediction, we first simulated the risk sensitive learning task and compared our proposal model with previous computational models on the task. We validated the simulated results by performing model selection with real experimental data taken from Niv et al. 
[14]
. As we found a better fit of our proposed model to the dataset than previous models, we conducted model selection using an additional dataset, from Goris et al. 
[15]
, which utilized decision-making tasks unrelated with riskiness to investigate the generalizability of our proposed model to different task and population. We further simulated the same task and the simple two-armed bandit task to investigate how parameters within the proposed model modulate the choice preference.


Model description
The Q-learning model is the base model of our surprise-sensitive utility model. The Q-learning model incorporates a Rescorla-Wagner rule where only the Q-value of the chosen option is updated by a prediction error. It explains the observed behavior by computing an action value ( + ) for each trial , which represents the expected outcome of the action. The Q-value of the chosen action is updated iteratively by a prediction error, which is the difference between the expected outcome ( ) and the actual received outcome . Based on Niv et al. 
[14]
, a decaying term of . /( + ) (with being the number of trials in which the consequences of stimulus S had been experienced) was added to a 6 learning rate , and termed 0 . For all models, the probability of choosing option i during trial is given by the softmax function:
( ( ) = ) = ( • ( )) ∑ A ( • ( ))
where β is the inverse temperature parameter that determines the sensitivity of the choice probabilities to differences in the values, and K represents the number of possible actions. In this study, K = 2 unless otherwise stated. a(t) denote the option that was chosen at trial t.


RESULTS
We first simulated a risky probabilistic learning task from Niv et al. 
[14]
 with 1,000 agents and compared the probability of opting for a safe choice using four different models: our proposed model (the surprisesensitive utility model) and the three previous models described in Niv et al. 
[14]
 (the Q-learning model, the utility model, and the risk-sensitive Q-learning model) (see 
Supplemental table 1
 and Methods for detail of each model). The previous winning model was the risk-sensitive Q-learning model which had separate learning rates in which positive and negative prediction errors have asymmetric effects on learning. As shown in 
Fig. 1
, the surprise-sensitive utility model had a similar probability of choosing the sure option (risk aversion) as the risk-sensitive Q-learning model and the utility model. Also, probability of choosing a safe choice simulated with the proposed model was approximated to the result of real experimental data. Furthermore, within the surprise-sensitive utility model, we examined how the free parameters and are related to the risk sensitive choice. We found that risk aversion increased as modulation rate of surprise (d) increased in the surprise-sensitive utility model, 
(Fig. 2)
, and that this tendency increased as the learning rate (α) increased.
We next conducted model selection with actual experimental data of risky probabilistic learning task from Niv et al. 
[14]
 to investigate the fit of our model. We compared the model evidence of each model (log marginal likelihood), and found that the surprise-sensitive utility model had the largest value 
(Fig. 3A)
. We then carried out a Bayesian model comparison to determine the best model to explain the choice behavior, and found that the surprise-sensitive utility model had a decisively higher protected exceedance probability, indicating that it is more frequent in the population than the other models ( 
Fig.    3B
). Each estimated parameter of the model is shown in 
Supplemental Fig 1.
 We confirmed the estimated surprise decay parameter was positive even for a uniform prior boundary from -1 to 1 with one-sample T test (t(15)=9.34, p<0.0001). These findings indicate that modelling the surprisal decay of outcome value led to a better fit to the dataset than separate learning rates.
In light of finding that our proposed model had a better fit to the dataset of Niv et al. 
[14]
 than previous models, we then performed model selection using a dataset from Goris et al. 
[15]
. Here we utilized decision-making tasks unrelated with riskiness in order to investigate the generalizability of our proposed model to different tasks and populations. The dataset of Goris et al. 
[15]
 consists of data from participants who performed a gamble task, a form of four-armed bandit task. Only the predictability of the gain was manipulated between choices. The authors did not fit the computational model to the data; we fitted the models from Niv et al 
[14]
 plus our proposed model. Following calculation of model evidence and Bayesian model comparisons, we found the surprise-sensitive utility model had the largest value (Supplemental 
Fig 2A)
 and had a decisively higher protected exceedance probability (Supplemental 
Fig    2B)
. Each estimated parameter of the model is shown in Supplemental 
Fig 2C.
 Our proposed model fitted these datasets better than the other models, with the same free parameters, thus providing a more parsimonious and robust account for human reinforcement learning.
This suggests that there should be some difference in the prediction regarding choice which given choice history and reward history. Therefore, finally, to investigate what kind of history dependence in the proposed model cause difference in prediction from other models, we simulated the two-armed bandit task and compared basic Q-learning model and surprise-sensitive utility model on the relationship between stay probability and reward history. We found a smaller stay probability in proximate reward conditions (r(t)=1), especially in the surprised reward condition (r(t)=1, r(t-1)=0), in the surprise-sensitive utility model compared to the Q-learning model 
(Fig. 4)
. This suggested that a reward history that generated a larger surprise affected the stay probability. We further investigated how modulation parameter (d) affected the stay probability, and found that as the parameter increased, the stay probability of proximate reward conditions (r(t)=1) decreased 
(Fig. 5
).


DISCUSSION
In this study, we proposed the surprise-sensitive utility model in which prediction error works not only to update the prediction, but also to decrease the outcome value. As a result of simulation analysis, we found the surprise-sensitive utility model explains the risk averse choices in a similar manner to a previous model 
[14]
 
(Fig. 1)
. We further confirmed this result with model selection analysis with three actual experimental datasets 
(Fig. 3A, 3B & Supplemental Fig. 2
). These findings suggest that surprise can affect subsequent decision making by reducing outcome value.
In our proposed model, the probability of choosing the safe choice increased as the modulation of the rate of surprise (d) increased 
(Fig. 2)
. This tendency was increased as the learning rate (α)
increased. This make sense, since both parameters are kind of sensitivity parameter for prediction error.
The modulation rate of surprise is a fixed parameter per agent, and agents whose modulation rate is set high are much more affected by a surprisal outcome. Since the learning rate is also a fixed parameter which affects the ability to acquire and update a prediction in response to prediction error, the agents whose outcome value is susceptible to surprise and have the highest learning rates might adjust their behavior quickly to avoid risky choices.
The proposed model fits these datasets better than the previous models, with same or fewer free parameters, thus providing a more parsimonious and robust account for risk averse choices. First this model fitted to the dataset of Niv et al. 
[14]
 and Goris et al. 
[15]
 than previous winning model, risk sensitive Q-learning model which utilized separate learning rate for positive or negative prediction error, having asymmetric effects on changes in predictions. It has been shown that the learning rate asymmetry is robust across experiments and the negative learning rate is generally higher than the positive learning rate, which may reflect risk aversion by being higher negative learning rate drives choices away from risky options 
[20]
. Thus, a result which our proposed model fitted these datasets well than asymmetric learning rates model imply the importance of nonlinear modulation of evaluation of outcomes itself.
Since surprise decay parameter decreases the outcome value with the influence of prediction error per trial, characteristics of the surprise-sensitive utility model is a smaller stay probability in proximate reward conditions (r(t)=1), especially in the surprised reward condition (r(t)=1, r(t-1)=0).
Also, as the parameter increase, the stay probability of proximate reward conditions (r(t)=1) decrease.
This suggested that a reward history that generated a larger surprise affected the stay probability and made the surprise-sensitive utility model well fitted to risk aversion tasks.
This surprise sensitive utility model may explain the symptomatic behavior of psychiatric disorders, such as ASD. Our results showed that a decreased outcome value, a reward history that generates larger surprises, and a higher modulation rate all negatively affected an agent's stay probability.
These findings indicate that agents who are susceptible to surprise avoid choosing the same choice, even when the proximate outcome is rewarded. Individuals with ASD prefer repetitive behaviors with perfect contingency that do not generate large surprises over social interaction, where contingency is non-perfect surprise is more likely 
[11]
[12]
[13]
. Previous models could not explain such preferential behaviors, since, in these models, a positive prediction error leads to a preference for the chosen option. However, in the surprise-sensitive utility model, the value of surprisal outcome decreases, which could explain the tendency to avoid social interaction and prefer repetitive behaviors. Also, on average, individuals with ASD are known to have a lower social motivation than neuro-typical people 
[21]
[22]
[23]
[24]
. However, at the individual level, individuals with ASD exhibit highly variable levels of social motivation 
[25]
. The modulation parameter may be associated with these individual differences in social motivation within ASD. In the future, it may be useful to expand the use of this proposed model to investigate the differences in choice preference in individuals with ASD whose preference is influenced by the surprise.


Conclusion
Previously, the effect of surprise on the value of an outcome and subsequent decision making was unclear.
In this study, we proposed the surprise-sensitive utility model, a reinforcement learning model in which 


METHODS Analysis 1: Simulation analysis with a risk sensitive task
We first simulated a risk sensitive task with 1,000 agents and compared the probability of choosing a safe choice on four models: our proposed model (the surprise-sensitive utility model) and three previous models described in Niv et al. 
[14]
 ( For the free parameter of each model (α and β for the Q-learning model; α, β, and u for the utility model; α + , α -, and β for the risk sensitive model; α, β, and d for the surprise model), we estimated and utilized the median of the best-fitting parameter values from 16 participants' data in Niv et al. 
[14]
 (see Analysis 2 for details of model fitting).
Furthermore, within the surprise-sensitive utility model, we examined how the free parameters and are related to the risk sensitive choice. For the surprise decay rate d, we simulated 1,000 values as the number of agents for three learning rates (0.2, 0.5, and 0.8).


Risky probabilistic learning task
The risky probabilistic learning task is described in detail in Niv et al. 
[14]
. Briefly, five different-colored stimuli (portrayed as casino-style slot machines or bandits) were randomly allocated to five payoff schedules: sure 40¢, sure 20¢, two sure 0¢ stimuli, and one variable-payoff risky stimulus associated with equal probabilities of 0¢ or 40¢ payoffs. Two types of trials were intermixed randomly:
choice trials and forced trials. In choice trials, two stimuli were displayed, and the subject was instructed to quickly select one of them. In forced trials, only one stimulus was displayed and the subject was forced to select it and obtain its associated outcome. The task consisted of 234 trials (three sessions of 78 trials).
Each trial comprised: (1) 30 "risk" choice trials involving a choice between the 20¢ stimulus and the 0/40¢ stimulus (target choice to assess subjects' behavioral risk sensitivity); (2) 20 "test" choice trials involving each of the pairs 40¢ versus 0/40¢, 20¢ versus 40¢, 0¢ versus 0/40¢, and 0¢ versus 20¢; 
(3) 24
forced trials involving each of the stimuli (16 only for each of the 0¢ stimuli); and (4) 20 trials in which subjects chose between the two 0¢ stimuli.


Analysis 2-1: Model selection with actual experimental data
To investigate how the model fit to actual experimental data, we conducted model selection to define a model that fits best with the actual experimental data from Niv et al. 
[14]
 presented on the author's homepage (http://www.princeton.edu/~nivlab/data.html). This dataset consists of data from 16
participants who conducted the same risk sensitive task as the simulation analysis.
The free parameters of each model for model fitting were as follows: α and β for the Q-learning model; α, β, and u for the utility model; α + , α -, and β for the risk sensitive model; α, β, and d for the surprise model (Supplemental table1). The learning rate was constrained to the range 0≦α (α + , α -)≦1
with a beta (2,2) prior distribution, and inverse temperature was constrained to 0≦β≦10 with a gamma (2,3) prior distribution. The utility parameter was constrained to the range 1≦u≦30 with a uniform prior distribution; these priors are based on Niv et al. 
[14]
. Additionally, the decay rate of the surprise model was constrained to -1≦d≦1 with a uniform prior distribution. We fit the parameters of each model with a maximum a posteriori (MAP) estimation using gradient descent and calculated the Laplace approximation 
[17]
 to the log marginal likelihood (evidence) for each model. We used the R function "solnp" in the Rsolnp package 
[18]
 to estimate the fitting parameters. We confirmed whether the estimated surprise decay parameter was positive or negative using a one-sample T test.
In the model selection, model evidences (Log marginal likelihood) for each model and participant were subjected to random-effects Bayesian Model Selection (BMS, spm_BMS in SPM12,
Stephan, Penny, Daunizeau, Moran, and 
Friston, 2009)
. Each model was assumed to occur in the population with a latent frequency estimated using variational Bayesian inference. Once these frequencies were estimated, we calculated the protected exceedance probability, defined as the probability that a particular model is more frequent in the population than all the other models, averaged over the probability of the null hypothesis that all models are equally frequent.


Analysis 2-2: Model selection with actual experimental data
To ensure the fitting of the proposed model in different tasks and population, we also conducted the model selection with the actual experimental datasets from Goris et al. 
[15]
 (https://osf.io/pkq3u) whose datasets are presented on the open science framework. The dataset included score of autism spectrum condition for each participant. However, we did not consider the participants' autistic score. This is because the purpose of this analysis is to confirm the fitness of newly proposed model among other model, but not to investigate the specific characteristics of psychiatric disorder, which would be done in the future study. Here we briefly describe their task and utilized computational models. thus we fitted the models from Niv et al 
[14]
 and our above proposed model. The probability of choosing option i was calculated as the dataset of Niv et al. 
[14]
, but the number of possible actions (K =4). As the analysis for dataset of Niv et al. 
[14]
, we first calculated the log marginal likelihood (evidence) for each model with MAP estimation and then calculated the protected exceedance probability with BMS.


Analysis 3: Simulation analysis with two-armed bandit task
To investigate the behavioral difference between models on a simple task, we simulated the simple two-armed bandit task and investigated the relationship between stay probability and reward history. We simulated 1,000 agents on the surprise-sensitive utility model and the basic Q-learning model, and compared the stay probability, defined as the probability of choosing same choice in the next trial, t+1, as the current trial, t, with the simple two-armed bandit task. In the task, agents chose one of two choices in which the probability of reward acquisition was different (0.8 vs. 0.2). The free parameters for 13 each model were: α = 0.3, β = for Q-learning; α = 0.3, β = 2, and d = 0.5 for the surprise model. In the proposed model, surprise decreased the outcome value, and thus lowered the stay probability of the proximate reward condition (r(t)=1), especially the surprisal reward condition (r(t-1)=0, r(t)=1), more than the basic Q-learning model predicted.
Furthermore, we investigated the influence of the modulation parameter (d) on the stay probability within the surprise-sensitive utility model. We predicted that the stay probability of the proximate reward condition (r(t)=1) would decrease as the modulation parameter increases.


Disclosure of potential conflicts of interest
The authors have no conflicts of interest to declare 
The surprise-sensitive utility model is a model in which the actual received outcome r is affected by the surprise (absolute value of prediction error). In this model, ( ) is the subjective utilitymodulated by surprise. The degree of modulation is controlled by a parameter . ( ) = ( ) − | ( ) − ( )| ( + ) = ( ) + 0 ( ( ) − ( ))


surprise decreases the outcome value. Comparing our proposed model with previous models, we found the new model explains risk averse choices as well as previous models. Risk averse choices increased as the surprise-based modulation parameter of outcome value increased. The present findings suggest that surprise can affect subsequent decision making by reducing outcome value. This proposed model could be the basis for a model linking the experimental data and the notion that surprise decreases outcome values.


the Q-learning model, utility model, and risk-sensitive Q-learning model). The utility model is a Q-learning model that incorporates nonlinear subjective utilities for the different amounts of reward. In this model, U(t) is the subjective utility of the reward at time t: U0 = 0, U20 = 2, and U40 = u• 2. Value of u affect the utility curve. The risk-sensitive model is a Q-learning model in which positive and negative prediction errors have asymmetric effects on learning. Specifically, there are separate learning rates for positive and negative prediction errors: α + and αfor a positive or a negative prediction error, respectively.


The dataset of Goris et al.
[15]
 consists of data from 163 participants who conducted a gamble task, a kind of four-armed bandit task, having 200 trials. Each of the four choices had an average reward outcome of € 250. Only the predictability of the gain was manipulated. The fixed choice always returned a gain of € 250. The other three choices returned a gain drawn from a normal distribution with mean € 250 and standard deviation of € 10, € 30, and € 70. They did not fit the computational model to the data,


Figure captions:Fig. 1 Fig. 2 Fig. 3 Fig. 4 Fig. 5 20 Fig. 2 21 Fig . Fig. 4 23 Fig.
1234520221.423
Probability of choosing the sure choice. Data are presented as the mean ± standard error Plot and second order polynomial fit for probability of choosing sure choices depending on modulation rate and learning rate Model selection Effect of reward history on stay probability of Q-learning model and surprise-sensitive utility model: Data are presented as the mean ± standard error Stay probability and modulation parameter. Data are presented as the mean ± standard errorFig. Probability of choosing the sure choice. Data are presented as the mean ± standard error Plot and second order polynomial fit for probability of choosing sure choices depending on modulation rate and learning rate Model selection A) Model evidence for each model. Data are presented as the mean (bars) and data value (plots and lines) per subject. B) Bayesian model selection Effect of reward history on stay probability of Q-learning model and surprise-sensitive utility model. Data are presented as the mean ± standard error Stay probability and modulation parameter. Data are presented as the mean ± standard error








Acknowledgements
This work was supported by the Grant for Research Fellow of JSPS to M.S. and JSPS KAKENHI Grants (JP18KT0021 and JP18K03173) to K.K.






Writing; Motofumi Sumiya, Kentaro Katahira
 










Dopamine reward prediction error signalling: a twocomponent response




W
Schultz




10.1038/212111d0






Nat Rev Neurosci




17
















Precisionweighting of superior frontal cortex unsigned prediction error signals benefits learning, is mediated by dopamine, and is impaired in psychosis. bioRxiv




J
Haarsma






P
C
Fletcher






J
D
Griffin






H
J
Taverne






H
Ziauddeen






T
J
Spencer




10.1101/558478


















Reinforcement Learning in Multidimensional Environments Relies on Attention Mechanisms




Y
Niv






R
Daniel






A
Geana






S
J
Gershman






Y
C
Leong






A
Radulescu




10.1523/JNEUROSCI.2978-14.2015






J Neurosci




35
















Too Good to Be True: Rhesus Monkeys React Negatively to Better-than-Expected Offers




E
J
Knight






K
M
Klepac






J
D
Kralik




10.1371/journal.pone.0075768






PLoS One




8


75768














Corrugator activity confirms immediate negative affect in surprise




S
Topolinski






F
Strack




10.3389/fpsyg.2015.00134






Front Psychol




6
















A theory of cortical responses




K
Friston




10.1098/rstb.2005.1622






Philos Trans R Soc Lond B Biol Sci




360
















Whatever next? Predictive brains, situated agents, and the future of cognitive science




A
Clark
















10.1017/S0140525X12000477






Behav Brain Sci




36
















Uncertainty and stress: Why it causes diseases and how it is mastered by the brain




A
Peters






B
S
Mcewen






K
Friston




10.1016/j.pneurobio.2017.05.004






Prog Neurobiol




156
















Affective Value in the Predictive Mind




S
Van De Cruys




10.15502/9783958573253






Philos Predict Process


















Active Inference and Cognitive Consistency




K
J
Friston




10.1080/1047840X.2018.1480693






Psychol Inq




29
















Nearly, but clearly not, like me": Contingency preference in normal children versus children with autism




G
Gergely




10.1521/bumc.65.3.411.19853






Bull Menninger Clin




65










The obscure object of desire








Contingency detection in a complex world




J
B
Northrup




10.1177/0165025416668582






Int J Behav Dev




41
















Precise minds in uncertain worlds : Predictive coding in autism




S
Van De Cruys






K
Evers






Hallen R Van Der






L
Eylen






Van






B
Boets






J
Wagemans




10.1037/a0037665






Psychol Rev












In press








Neural prediction errors reveal a risk-sensitive reinforcement-learning process in the human brain




Y
Niv






J
A
Edlund






P
Dayan






O
Doherty






J
P




10.1523/JNEUROSCI.5498-10.2012






J Neurosci




32
















The relation between preference for predictability and autistic traits




J
Goris






M
Brass






C
Cambier






J
Delplanque






J
R
Wiersema






S
Braem




10.1002/aur.2244






Autism Res


















Altered learning under uncertainty in unmedicated mood and anxiety disorders




J
Aylward






V
Valton






W
Y
Ahn






R
L
Bond






P
Dayan






J
P
Roiser




10.1038/s41562-019-0628-0






Nat Hum Behav




3
















Bayes factors




R
E
Kass






A
E
Raftery




10.1080/01621459.1995.10476572






J Am Stat Assoc




90
















Rsolnp ': General Non-Linear Optimization




A
Ghalanos






S
Theussl






Package








Cran












Bayesian model selection for group studies




K
E
Stephan






W
D
Penny






J
Daunizeau






R
J
Moran






K
J
Friston




10.1016/j.neuroimage.2009.03.025






Neuroimage




46
















Do learning rates adapt to the distribution of rewards?




S
J
Gershman




10.3758/s13423-014-0790-3






Psychon Bull Rev




22
















Social reward processing in individuals with autism spectrum disorder: A systematic review of the social motivation hypothesis




S
Bottini




10.1016/j.rasd.2017.10.001






Res Autism Spectr Disord




45
















Evaluation of the social motivation hypothesis of autism a systematic review and meta-analysis




C
C
Clements






A
R
Zoltowski






L
D
Yankowitz






B
E
Yerys






R
T
Schultz






J
D
Herrington




10.1001/jamapsychiatry.2018.1100






JAMA Psychiatry




75
















The social motivation theory of autism




C
Chevallier






G
Kohls






V
Troiani






E
S
Brodkin






R
T
Schultz




10.1016/j.tics.2012.02.007






Trends Cogn Sci




16
















Attenuated activation of the anterior rostral medial prefrontal cortex on self-relevant social reward processing in individuals with autism spectrum disorder




M
Sumiya






Y
Okamoto






T
Koike






T
Tanigawa






H
Okazawa






H
Kosaka




10.1016/j.nicl.2020.102249






NeuroImage Clin




102249














Emotions surrounding friendships of adolescents with autism spectrum disorder in Japan : A qualitative interview study




M
Sumiya






K
Igarashi






M
Miyahara




10.1371/journal.pone.0191538








PLoS One




13


191538















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]