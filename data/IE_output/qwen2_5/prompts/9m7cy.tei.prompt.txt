You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



PREDICTING DECISION-MAKERS' ALGORITHM USE 4 Predicting Decision-makers' Algorithm Use
Decision makers typically use multiple pieces of information to make performance predictions and decisions. For example, organizational psychologists may use standardized tests and interviews to hire new managers, and admissions officers may use grades and personal statements to admit students. In these situations, decision makers typically integrate information based on their expert judgment 
(Neumann et al., 2023;
Vrieze & Grove, 2009)
, but they also sometimes receive algorithmic advice in the form of a performance prediction.
An exemplary algorithm's prediction may be an unweighted sum score of a test score and an interview rating.
Various meta-analyses showed that consistently using algorithmic advice (mechanical, statistical, or actuarial prediction) results in more valid performance predictions than combining information 'in the mind' (holistic, clinical, impressionistic, or expert prediction, 
Dawes et al., 1989;
Grove et al., 2000;
Hanson & Morton-Bourgon, 2009;
Kuncel et al., 2013;
Meehl, 1954)
. In practice, however, decision makers are often algorithm averse. They discount algorithmic advice, especially when seeing such advice result in an error 
(Dietvorst et al., 2015)
. Discounting algorithmic advice results in less accurate predictions than strictly following the algorithm 
(Dietvorst et al., 2018;
Hoffman et al., 2017;
. To improve decision making, researchers started investigating factors and interventions that could explain or encourage use of algorithmic advice (for a general overview see 
Burton et al., 2020;
 for an overview in personnel and educational selection, see 
Neumann et al., 2021)
. For example, 
Dietvorst and Bharti (2020)
 showed that decision makers were less likely to use algorithmic advice in situations that were inherently more uncertain. Furthermore, increasing decision-makers' understanding of an algorithm and allowing decision makers to adjust an algorithm's predictions seem to increase use of algorithmic advice 
(Dietvorst et al., 2018;
Yeomans et al., 2019)
. In contrast, presenting PREDICTING DECISION-MAKERS' ALGORITHM USE 5 decision makers with outcome feedback usually decreases use of algorithmic advice 
(Arkes et al., 1986;
Dietvorst et al., 2015;
A. T. Jackson et al., 2019)
. These studies provide a first insight into factors that can explain use of algorithmic advice and improve decision making.
Yet, a better understanding of what factors explain the use of algorithmic advice is clearly needed 
(Blacksmith et al., 2020;
Burton et al., 2020;
Highhouse & Brooks, 2023;
Neumann et al., 2021)
. The aim of this paper was to shed more light on such factors.
We hypothesized that individual differences on psychological traits predict decisionmakers' use of algorithmic advice in a personnel selection task. If this were the case, organizations could select decision makers based on these traits. We first review the literature on individual differences on psychological traits and judgment accuracy, and then develop our hypotheses for the relevant psychological traits.


Individual Differences and Judgment Accuracy
Much research has focused on identifying the profile of a 'good judge', to answer the question of what individual characteristics are associated with judgment accuracy. These studies mostly focused on judgment accuracy when information is combined purely holistically, without using advice 
(algorithmic or other, De Kock et al., 2020;
Tetlock & Gardner, 2015)
. A meta-analysis in the human resource management domain showed that judgment accuracy was moderately positively correlated with cognitive ability, while near zero, inconsistent relationships were found with the Big Five personality traits 
(De Kock et al., 2020)
. In line with these results, another large-scale study in which participants predicted broad geopolitical events showed that various cognitive ability measures, behaviors (e.g., searching for information), and political knowledge were positively related to prediction accuracy 
(Mellers et al., 2015)
. Cognitive ability seems to be positively related to judgment accuracy because it enables effective information processing 
(Funder, 1995;
Kolk et al., PREDICTING DECISION-MAKERS' ALGORITHM USE 6 2002)
. So, when making holistic predictions, a good judge seems to be mostly characterized by high cognitive ability and knowledge, while personality seems largely irrelevant.
A robust finding is that more valid predictions are made when predictors are combined mechanically, through an algorithm, rather than holistically in the decision-maker's mind 
(Dawes et al., 1989;
Kuncel et al., 2013;
Meehl, 1954)
. Algorithms do not have to be complex; even simple algorithms with a few variables and equal or random weights result in more valid performance predictions than the holistic judgment from experienced decision makers 
(Bobko et al., 2007;
Dawes, 1971
Dawes, , 1979
Yu & Kuncel, 2020
, 2022
. These results are in line with the meta-analytic finding that algorithms outperform holistic judgment irrespective of decision-makers' experience or training 
(Grove et al., 2000)
. Moreover, deviating from an algorithm's prediction, for example to account for perceived exceptions, decreases prediction accuracy 
(Guay & Parent, 2018;
Hoffman et al., 2017;
. This shows that, when an algorithm is available to combine information, a 'good judge' is someone who uses it.
The problem is that decision makers typically deviate from an algorithm's prediction (i.e., they discount algorithmic advice), even after observing an algorithm outperform their holistic judgment 
(Dietvorst et al., 2015)
. Such deviation should decrease judgment consistency and hence predictive validity, as higher judgment consistency is the main reason why algorithms outperform expert judgment 
(Karelaia & Hogarth, 2008;
Yu & Kuncel, 2020)
.
Given that decision makers typically deviate from an algorithm's prediction, an important question to answer is which characteristics are associated with the use of algorithmic advice. However, to our knowledge, no research has been conducted on the profile of a good judge in the presence of algorithmic advice, and factors related to using algorithmic advice may in part be different from factors related to accurate holistic judgment. Specifically, it is plausible that cognitive ability is more important than personality for consistent information combination PREDICTING DECISION-MAKERS' ALGORITHM USE 7 when no algorithmic advice is available, as information combination is a cognitively demanding task. Yet, when valid algorithmic advice is available, strictly following the advice is a successful strategy, which may be better predicted by traits such as conscientiousness than by cognitive ability.
Only a few studies have focused on how individual differences on psychological traits relate to use of algorithmic advice. In contrast to the 'good judge' literature where higher domain knowledge was related to higher accuracy 
(Mellers et al., 2015)
, 
Arkes et al. (1986)
 showed that decision makers with high domain knowledge were less likely to use algorithmic advice and hence made less accurate predictions than decision makers with moderate domain knowledge. In another study, decision-makers' attitude towards statistics was positively related to the use of algorithmic advice 
(Sieck & Arkes, 2005)
. Other studies looked at the relation between personality and use of algorithmic advice. Merrit and Ilgen 
(2008) expected
but did not find that more extraverted decision makers are more likely to trust and hence use algorithmic advice in a weapon detection task. Similarly, Sharan and Romano (2020) did not find evidence that the Big Five were related to trust and use of algorithmic advice in a card game. One explanation for this may be that the higher-order factors of the Big Five are too broad to predict narrow behavior (i.e., use of algorithmic advice), as research showed that narrow behavior can be better predicted by narrow traits, compared to higher-order factors (de Vries et al., 2011).


Relevant Psychological Traits


Dutifulness
Dutifulness is a facet of conscientiousness as specified in the NEO framework 
(Costa & McCrae, 1992)
. Dutiful decision makers are cooperative and dependable, and should be more likely to follow advice and instructions 
(Costa et al., 1991;
Roberts et al., 2005)
.
Therefore, in the context of this study, it can be expected that dutiful decision makers are PREDICTING DECISION-MAKERS' ALGORITHM USE 8 more likely to use algorithmic advice when they have been informed that doing so most likely results in the best hiring decisions. We considered dutifulness the most relevant facet of the conscientiousness facets for the consistent use of algorithmic advice, both conceptually and in terms of item content. We hypothesized that more dutiful decision makers would be less likely to deviate from algorithmic advice, and hence make more consistent and more valid predictions 
(Yu & Kuncel, 2020
). Yet, other facets as well as the general conscientiousness factor may also be partly relevant for the consistent use of algorithmic advice, as conscientiousness refers to an individual's propensity to be organized, ambitious, selfcontrolled, rule abiding, and motivated to perform well 
(Roberts et al., 2009)
. More conscientious individuals should be more likely to show a certain behavior when it is clear that such behavior allows them to achieve a desired goal 
(Tett & Burnett, 2003)
. We did not pre-register specific hypotheses for other facets or the general conscientiousness factor since we considered them less relevant for the consistent use of algorithmic advice, compared to dutifulness. Therefore, we explored relations between our outcomes and the other facets of conscientiousness and the general factor. Based on the argument presented above, we hypothesized that:
Hypothesis 1: Dutifulness is a) negatively related to algorithm deviation, and positively related to b) judgment consistency and c) predictive validity.


Decision-making Style
We also expected that a preference for making judgments based on one's feelings would be positively related to algorithm deviation. Cognitive-experiential self-theory 
(Epstein et al., 1996;
Pacini & Epstein, 1999)
 suggests that people process information according to two distinct, parallel systems. Decision makers who rely on the rational system tend to engage PREDICTING DECISION-MAKERS' ALGORITHM USE 9 in intentional, effortful, and analytic reasoning and justify statements via logic and evidence, quickly changing their minds when new evidence accompanied by strong arguments is available 
(Epstein et al., 1996)
. Conversely, decision makers who rely on the experiential system tend to process information automatically, effortlessly, and holistically. They tend to engage in affective decision making (doing what feels good), are more resistant to change, and encode reality in the form of images, metaphors, and narratives. Furthermore, they tend to believe what they experience and act based on experiences from past events 
(Epstein et al., 1996)
.
Indeed, in a sample of human resource professionals, 
Lodato (2008)
 found that experientiality was strongly positively correlated with a self-reported preference for intuitionbased hiring (e.g., "I believe it is important to rely on your 'gut' when hiring employees"), opposed to algorithm-based hiring. Yet, it remains unknown how experientiality relates to a behavioral measure of algorithm use. Based on these findings and theoretical arguments, we expected that more experiential decision makers should be more likely to deviate from an algorithm's prediction, and hence make less consistent and less accurate predictions. We hypothesized that:
Hypothesis 2: Experientiality is a) positively related to algorithm deviation, and negatively related to b) judgment consistency and c) predictive validity.


Predictor Validity Beliefs
Cognitive ability tests and structured interviews are among the most valid predictors of job performance 
(Lievens et al., 2021;
Sackett et al., 2022;
Schmidt & Hunter, 1998
). Yet, decision-makers' beliefs about predictor validities often diverge from these empirical validity PREDICTING DECISION-MAKERS' ALGORITHM USE 10 estimates. For example, many practitioners involved in hiring still wrongly believe that unstructured interviews are more valid than structured interviews, and they consider conscientiousness more important for predicting job performance than cognitive ability 
(Fisher et al., 2021)
. However, decision makers differ in their beliefs about predictor validities (D. J. R. 
Jackson et al., 2018;
Rynes et al., 2002;
Sanders et al., 2008)
. We expected that when decision-makers' predictor validity beliefs do not align with an algorithm's predictor weights, they should be more likely to deviate from an algorithm's predictions, and hence make less consistent and less valid predictions. This prediction was based on the 'good judge' literature, which showed that domain knowledge is positively related to prediction accuracy 
(Mellers et al., 2015)
. Furthermore, this prediction is in line with the advice-taking literature and theories on attitude change: Decision makers discount advice more when it is further away from their own judgment 
(Aronson et al., 1963;
Bochner & Insko, 1966;
Yaniv, 2004;
Yaniv & Milyavsky, 2007)
. Decision makers should be more likely to argue against an algorithm's prediction when it is very discrepant from their own prediction 
(Bochner & Insko, 1966;
Davis et al., 1997
). Yet, when an algorithm's prediction closely matches their own prediction, they should be more likely to use an algorithm's prediction because they recognize that it falls within a plausible range of predictions, that is, within their "latitude of acceptance" 
(Sherif & Hovland, 1980, p. 50
). Applied to this study, decision makers with inaccurate validity beliefs should produce predictions that are more discrepant from the predictions that are generated by a valid algorithm. Hence, these decision makers should be more likely to deviate from the algorithm's prediction. We hypothesized that: 


Cognitive Ability
We also explored the relation between cognitive ability and algorithm deviation, judgment consistency, and predictive validity. The reviewed literature above showed that cognitive ability is a moderate predictor of judgment accuracy when decision makers make holistic predictions, without receiving any advice 
(De Kock et al., 2020;
Mellers et al., 2015)
. This is likely explained by the high cognitive load that is required to combine information in the absence of valid advice 
(Kolk et al., 2002
). Yet, when valid advice is available and its use is encouraged, cognitive ability may be less predictive of judgment accuracy, as dutifully following such advice should result in valid predictions. On the other hand, algorithmic advice constitutes additional information that decision makers would have to integrate with other available information. This may happen in case they wanted to deviate from the algorithm's prediction to account for perceived exceptions to the algorithm, or to evaluate the algorithmic advice in light of the available information. This would require cognitive resources. We formulated the following research question:
Research question: What is the relation between cognitive ability and algorithm deviation, judgment consistency, and predictive validity?
The pre-registration, data, code, and study materials are available on the Open Science Framework (OSF), https://osf.io/4fv35/?view_only=9a7264ddfd29465ba654ad674add8fa4.
Data were analyzed using R, version 4.2.1 (R Core Team, 2022).


Method


Participants
Our budget allowed us to recruit 300 participants. Sensitivity power analyses showed that the smallest detectable effect size given 80% power, α = .05, N = 300, and ρ H0 = 0 was r PREDICTING DECISION-MAKERS' ALGORITHM USE 12 = .143 for one-tailed tests and r = .161 for exploratory, two-tailed tests. Given that we deemed smaller effects practically irrelevant, our study was sufficiently powered. We recruited employees via Amazon MTurk and excluded participants who (1) are not involved in hiring decisions at work, (2) do not make a single hiring decision per year, or (3) failed at least one of five attention checks (see the pre-registration on OSF). As mentioned in our updated preregistration, we also excluded 26 participants for who the difference between their reported age and number of years of hiring experience was less than 15, given that smaller differences seem implausible. We recruited new participants to replace those participants. Since Amazon MTurk requires to collect data from minimally ten additional participants, we slightly exceeded our planned sample size and eventually recruited 308 participants (56% female, 82% white) who ranged in age from 24 to 63 (M = 33.9, SD = 9.5). Most participants were U.S. citizens (89%). Other participants had Indian (5%), UK (2%), Brazilian (2%), or other non-EU nationalities (2%). On average, participants made 14 hiring decisions per year. The median study completion time was 34 minutes. The ethics committee of the Heymans institute for psychological research approved this study (code: PSY-2122-S-0195).


Applicant Data
Participants predicted the performance of applicants who had applied for the job of a ticket agent at an airline company. The applicant data (N = 236) was originally presented in 
Kausel et al. (2016)
 and contained cognitive ability test and conscientiousness questionnaire scores, and unstructured interview ratings (henceforth called predictors). All applicants except for those with the lowest possible interview rating were hired, and their supervisors assessed their job performance. Since it was not feasible to have participants make 236 predictions, we selected 40 applicants such that the correlations among variables in the reduced dataset did not differ by more than .015 from the correlations in the original dataset PREDICTING DECISION-MAKERS' ALGORITHM USE 13 (see the R script "Applicant selection.R" on OSF). The descriptive statistics and correlations are shown in Supplement S1.


Algorithmic Advice
Participants received advice from an algorithm that was constructed based on metaanalytic correlations rather than primary data to avoid overfitting, and because large enough primary datasets are rarely available in practice. 
Cortina et al. (2000)
 reported a metacorrelation matrix with correlations between cognitive ability, conscientiousness, an unstructured interview, and job performance (p. 339, 
Table 3
). To construct the algorithm, we derived standardized regression weights from this meta-correlation matrix using the setCor function from the psych package in R. We multiplied the standardized regression weights by the standardized predictor scores and summed up the products. Then, we rescaled these algorithm's predictions to a five-point scale up to one decimal (1 = very poor job performance, 5 = very good job performance) using min-max scaling (see p. 114, formula 3.8
in 
Han et al., 2011)
.


Procedure
After participants reported their primary work activities and hiring experience, they completed conscientiousness, decision-making style, and predictor validity belief measures.
Then, participants learned how to interpret the predictors. Cognitive ability test scores were presented as the percentage of correct answers. The conscientiousness score was the mean of a multi-item measure and was presented on a five-point scale, up to one decimal. Lastly, the interview rating was presented as an integer and ranged from two to five. Participants also received the mean, minimum, and maximum score for each predictor. They were also informed how the algorithm weights each predictor, and that the algorithm was designed by an assessment professional based on numerous empirical research findings. Moreover, participants were explicitly advised to use the algorithm, as it would likely result in more PREDICTING DECISION-MAKERS' ALGORITHM USE 14 valid predictions than their own judgment. They were told that although the algorithm is not perfect, deviating from its predictions usually decreases predictive accuracy. After a practice prediction trial, participants learned that they could earn 0.80$ for making good performance predictions. Participants whose predictions correlated r = .28 or higher with the applicants' observed job performance received the reward. This cutoff was based on a previous study in which participants made performance predictions based on the same applicant data and the same algorithmic advice as in this study 
(Neumann, Niessen, Linde, et al., 2022)
. Next, participants predicted the performance of 40 applicants on a five-point slider scale up to one decimal. For each applicant, they saw the predictor scores and the algorithm's prediction.
Afterwards, participants completed the cognitive ability measures. These measures were presented after the prediction task because we expected that participants may not complete them seriously, given that their completion requires much effort. To encourage effort, participants could earn up to 1$ (0.05$ for each correct answer) for correctly answering the cognitive ability items. To judge whether participants spend enough effort, we inspected the completion time of the cognitive ability measures and excluded participants who spent less than 7 minutes in total on the measures. This resulted in a sample size of 197, which allowed us to detect a correlation of r = .20 with 80% power, given α = .05, ρ H0 = 0, and a two-sided test.


Measures
All measures used a five-point scale (1 = strongly disagree, 5 = strongly agree), unless indicated otherwise. Scale items and their scoring key are presented in the pre-registration on OSF.


Conscientiousness
We measured each of the six conscientiousness facets within the NEO framework with 10 items from the International Personality Item Pool PREDICTING DECISION-MAKERS' ALGORITHM USE 15 (https://ipip.ori.org/newNEOFacetsKey.htm). The facets showed good reliability (selfefficacy: α = .82, orderliness: α = .77, achievement striving: α = .77, self-discipline: α = .84, cautiousness: α = .85, and dutifulness: α = .87, respectively). For exploratory purposes, we also created a general conscientiousness score by calculating the mean score across all 60 items (α = .96).


Experientiality
We measured the preference for making judgments based on feelings with 
Pacini and Epstein's (1999)
 20-item experientiality scale, which showed good reliability (α = .83).


Predictor Validity Beliefs
Participants indicated how important they believed the predictors were by assigning percentage weights (e.g., 70% cognitive ability, 20% conscientiousness, 10% interview). We subtracted from the participant's percentage weights the algorithm's relative percentage weights (53%, 28%, and 19%, respectively) 1 . Then, we calculated the absolute value for each of the three subtractions and summed up these values. This score reflects a participant's inaccuracy of predictor validity beliefs. Higher scores mean less accurate predictor validity beliefs.


Cognitive Ability
We measured cognitive ability using the International Cognitive Ability Resource 
(Condon & Revelle, 2014)
 11-item matrix reasoning (α = .64) and 9-item letter and number series test (α = .77). Example items are shown in the pre-registration on OSF. We created a composite score (α = .75) by calculating the mean of the standardized sum scores of the two tests.


Algorithm Deviation
Algorithm deviation was operationalized as the mean absolute deviation between a participant's 40 predictions (P) and the algorithm's predictions (A), as shown in formula 1.
Higher scores mean less algorithm use. To provide a reliability estimate, we used a split-half approach and correlated the mean absolute deviation for the 20 even predictions with the mean absolute deviation for the 20 odd predictions. This correlation was very high (r = .96).


ℎ =
∑ |#$%&$| !" # '(
(1)


Judgment Consistency
For each participant, we regressed the three predictors (cognitive ability, conscientiousness, and interview) on their 40 predictions. The resulting Multiple R reflected to what extent the participant consistently combined the predictors in a linear fashion 
(Hammond & Summers, 1972
). Fisher-z transformed correlations served as the unit of analysis. As for the algorithm deviation measure, judgment consistency for the even predictions correlated very strongly with the judgment consistency for the odd predictions (r = .88).


Predictive Validity
For each participant, we correlated their 40 predictions with the applicants' observed job performance. Fisher-z transformed correlations served as the unit of analysis. We operationalized predictive validity as the correlation between predictions and observed performance because the rank order is most relevant in selection contexts. Predictive validity for the even predictions was moderately correlated with the predictions for the odd predictions (r = .43). This is not surprising and can be largely explained by sampling error. Furthermore, this estimate reflects a correlation with an external variable (the applicants' job performance)
which was outside of the participants' control or knowledge. Moreover, when using an PREDICTING DECISION-MAKERS' ALGORITHM USE 17 optimal regression model (job performance ratings regressed on the cognitive ability and conscientiousness scores, and the interview rating) it was easier to make valid predictions in the even (multiple R = .44) than in the odd set (multiple R = .33). This is not a problem across all 40 predictions, where the optimal model validity was the same for each participant.


Results
Descriptive statistics and correlations are shown in 
Table 1
. Since our dependent variables were somewhat skewed, we also calculated Spearman's rank correlations. Pearson's and Spearman's correlations did not differ by more than .10 and led to very similar conclusions (see 
Table 1
). Therefore, we reported and interpreted Pearson's correlations in the text. To create an equal playing field, we also corrected the correlations for predictor unreliability, by dividing correlations by the square root of the predictor reliability 
(Hunter et al., 2006)
. The corrected correlations resulted in similar conclusions (see Supplement S2).
The algorithm's predictions were more valid (r = .35) than the participants' predictions, on average ( ̅ = .26). So, advising participants to use the algorithm's predictions was justified, but not adhered to consistently. Exploratory analyses showed that the correlations between the dependent measures and the other conscientiousness facets, and the general conscientiousness score were very similar (see 
Table 1
).


Personality


Experientiality
Contrary to our expectation, we did not find evidence that experientiality correlated with algorithm deviation (r = -.02, 95% CI 
[-.13, .09
 


Discussion
Decision makers often discount algorithmic advice, which reduces predictive validity, mainly due to lower judgment consistency 
(Yu & Kuncel, 2020)
. Therefore, we investigated whether psychological traits can predict use of algorithmic advice, judgment consistency, and predictive validity. Decision makers with more accurate predictor validity beliefs discounted algorithmic advice less and made more consistent and more valid predictions. This implies that it may be meaningful to provide decision makers with training on the validity of predictors 
(Balzer et al., 1989)
. We also found that more dutiful decision makers discounted algorithmic advice less and made more consistent and more valid predictions. This finding is not in line with earlier research in which no relation between a broader conscientiousness factor and consistent use of algorithmic advice was found 
(Sharan & Romano, 2020)
. This discrepancy cannot be explained by measuring traits at the facet vs. the factor level, since we also found similar relations between other conscientiousness facets, the general factor, and PREDICTING DECISION-MAKERS' ALGORITHM USE 19 our outcomes. A possible explanation is that we strongly encouraged participants to use the algorithmic advice and told them that discounting algorithmic advice usually decreases predictive accuracy compared to strict algorithm use. These instructions highlighted that strict algorithm use poses a strategy that would allow participants to achieve the goal of the task: make accurate performance predictions. Conscientious individuals should be more likely to show a certain behavior when it is clear that such behavior allows them to achieve a desired goal 
(Tett & Burnett, 2003)
. This reasoning is in line with trait activation theory 
(Tett & Guterman, 2000)
, which suggests that a personality trait is expressed in response to, or "activated by", a trait-relevant situational cue, which in this study were instructions to use the algorithmic advice. Our decision to highlight that algorithm use would likely result in good performance predictions may thus explain why conscientiousness predicted algorithm use in our study, but not in earlier research 
(Sharan & Romano, 2020)
.
Contrary to our expectations, we did not find evidence that an intuitive decisionmaking style was associated with the outcomes. Although 
Lodato (2008)
 found that an intuitive decision-making style correlated strongly positively with a self-reported preference for holistic prediction, we could not replicate this finding with a behavioral measure. One limitation may be that, similar to Lodato (2008), we used a broad intuitive decision-making style measure that was not contextualized. Therefore, future research may investigate whether other, narrow constructs that are conceptually closer to consistent use of algorithmic advice are more predictive, such as decision-makers' tendency to detect exceptions, or perceived importance of judgment consistency.
Exploratory analyses showed that cognitive ability correlated moderately negatively with algorithm deviation and moderately positively with judgment consistency. Furthermore, cognitive ability correlated weakly positively, but not significantly with predictive validity. A limitation was that our incentive for correctly answered cognitive ability items may have been PREDICTING DECISION-MAKERS' ALGORITHM USE 20 too low to encourage sufficient effort, although our subsample of participants who completed the items in a reasonable amount of time allowed for enough statistical power to detect practically relevant effects. Earlier research found that cognitive ability (but not personality) was moderately positively related to judgment accuracy when no algorithmic advice was available 
(De Kock et al., 2020)
. This result is only partially in line with our results, as conscientiousness correlated more strongly with predictive validity than did cognitive ability.
It is plausible that cognitive ability and personality variables such as conscientiousness affect judgment accuracy differently depending on the availability of algorithmic advice. In situations where no algorithmic advice is available, information still needs to be effectively processed and combined, which requires cognitive ability 
(Funder, 1995;
Kolk et al., 2002)
.
Conscientiousness may be less relevant in such situations as no valid algorithm is available that, when followed consistently, would achieve the goal of making accurate performance predictions. However, when a valid algorithm is available and its use is encouraged, cognitive ability may be less important than conscientiousness. This is because no information has to be combined anymore, unless a decision maker wants to deviate from the algorithm's prediction.
The relevant behavior that is required when algorithmic advice is available is consistent use of such advice. This may be better predicted by conscientiousness than by cognitive ability.
Therefore, future research could manipulate the availability of algorithmic advice, to investigate whether cognitive ability and conscientiousness affect predictive validity differently depending on the absence or presence of algorithmic advice.
Furthermore, we encourage replications of our study, to overcome the typical single study limitations. We collected data via Amazon MTurk, which had the advantage that participants did not come from a single or only a few organizations 
(Landers & Behrend, 2015)
. Moreover, MTurk samples are similar in data quality compared to other traditional samples such as student or community samples 
(Buhrmester et al., 2011;
Cheung et al., 2017;
 PREDICTING DECISION-MAKERS' ALGORITHM USE 21 
Keith et al., 2022)
. Additionally, to ensure data quality, we followed recommendations and carefully screened participants based on attention checks and impossible answers 
(Aguinis et al., 2021)
. Still, future research could replicate this study with samples from other sources than MTurk.
Asking experienced decision makers to make performance predictions based on real applicant data allowed us to extend 
Lodato's (2008)
 findings by investigating whether psychological traits are also related to a behavioral measure of algorithm use, and to investigate the theoretical effect of consistent use of algorithmic advice on judgment consistency and validity. Despite this strength of the study, it is plausible that decision makers had discounted the algorithmic advice more, due to more perceived exceptions, if they had also interviewed the applicants themselves or had received 'richer' information than three numerical predictor scores 
(Niessen et al., 2022)
. Therefore, future research would ideally have decision makers also collect and quantify information.
Future research could also build more directly on the advice-taking literature, which has robustly shown that decision makers are 'egocentric' and do not just discount algorithmic advice, but also advice from other human decision makers, in favor of their own judgment 
(Bonaccio & Dalal, 2006;
Yaniv & Kleinberger, 2000)
. Individual characteristics such as an unclear self-concept, which is the tendency to have conflicting beliefs about oneself, are positively associated with advice taking 
(Duan et al., 2021)
. Similarly, the extent to which people have secrets and thus want to conceal negatively perceived personal information is positively associated with human advice taking, as taking advice from others deflects social attention away from oneself 
(Duan et al., 2022)
. Characteristics that are negatively associated with advice taking include feelings of power 
(Tost et al., 2012)
, narcissism 
(Kausel et al., 2015)
, and agency (i.e., the belief in one's competence, 
Schultze et al., 2018)
. Future studies could investigate whether these characteristics also relate to the use of algorithmic advice.


PREDICTING DECISION-MAKERS' ALGORITHM USE 22
The advice-taking literature suggests that decision makers are more likely to use advice when it closely matches their own prediction 
(Yaniv, 2004)
. Therefore, we expected that participants with more accurate predictor validity beliefs would generate holistic predictions that match the algorithm's predictions more closely, and hence use the algorithmic advice more. We indeed found that participants with more accurate predictor validity beliefs were more likely to use the algorithmic advice. This suggests that increasing knowledge on predictor validities through training is a useful strategy to encourage the acceptance of valid algorithmic advice. However, in contrast to our finding, a couple of studies have also shown that more knowledgeable decision makers are less likely to use both human and algorithmic advice in tasks where mostly factual questions have to be answered 
(Arkes et al., 1986;
Yaniv, 2004)
. For example, in the first study by 
Yaniv (2004)
, decision makers estimated the date of historical events in a first phase of the experiment and then again in a second phase, with their initial estimate and an advisor's estimate available. Decision makers were grouped into high and low knowledge groups based on their performance in the first phase. The results showed that highly knowledgeable decision makers placed less weight on the advisor's estimate than less knowledgeable decision makers. The explanation for this was that highly knowledgeable decision makers could retrieve more pieces of evidence from their memory to support their initial estimate. In this study, retrieving supporting evidence was likely much harder when participants had to estimate predictor validities and subsequently predict applicants' job performance, as this situation contains irreducible uncertainty (i.e., performance is very difficult to predict, 
Highhouse, 2008
), compared to a situation where objective and clearly defined historical events are estimated. Future research could investigate whether the nature of the task (answering factual questions vs. predicting human performance) moderates the relationship between knowledge and advice-taking.


PREDICTING DECISION-MAKERS' ALGORITHM USE
As expected, our results showed that algorithm deviation, judgment consistency, and predictive validity were strongly correlated. We measured these variables separately because deviation from an algorithm's prediction should, in most cases, decrease judgment consistency and hence predictive validity 
(Yu & Kuncel, 2020)
. Nevertheless it could theoretically increase predictive validity if decision makers were able to account for valid exceptions (also known as "broken legs", 
Meehl, 1954, p. 24)
, interactions, or nonlinearities.
However, in the context of personnel selection, inconsistent use of algorithmic advice typically decreases predictive validity compared to the consistent use of algorithmic advice 
(Hoffman et al., 2017;
. To streamline the presentation of the results, future studies may want to focus on predictive validity, as this was the most important outcome. Alternatively, meaningful studies can be conducted even when criterion data (e.g., job performance ratings) are absent, by investigating algorithm deviation alone. For example, future studies could capture decision-makers' judgment policies 
(Karren & Barringer, 2002)
, which may shed more light on the information (e.g., unstructured interview ratings) that decision makers attend to when deviating from algorithmic advice.


Implications and Conclusion
Our results have a few implications. First, if valid algorithmic advice is available, a 'good judge' is someone who follows an algorithm consistently, and this is predicted by conscientiousness and predictor validity beliefs. Second, our results showed that narrowly defined behavior was not necessarily more related to narrow than broad personality traits.
Third, our results provide yet another reason for organizations to hire decision makers that are conscientious and have accurate knowledge of predictor validities, given that we found some evidence that these traits were associated with higher predictive validity, and are positively related to job performance in general 
(Sackett et al., 2022)
. Lastly, organizations may provide training on decision making in general 
(Neumann, Hengeveld, et al., 2022)
, and on predictor PREDICTING DECISION-MAKERS' ALGORITHM USE 24 validities specifically, as decision-makers' predictor validity beliefs do not always align with research-based validity estimates (D. J. R. 
Jackson et al., 2018)
. 
Hypothesis 3 :
3
Inaccurate predictor validity beliefs are a) positively related to algorithm deviation, and negatively related to b) judgment consistency and c) predictive validity. PREDICTING DECISION-MAKERS' ALGORITHM USE 11


Supporting hypotheses 1a, 1b, and 1c, dutifulness correlated moderately with algorithm deviation (r = -.29, 95% CI [-.39, -.18]), judgment consistency (r = .22, 95% CI [.11, .32]), and predictive validity (r = .31, 95% CI [.21, .41]), in the expected directions.


.09, .14]), or predictive validity (r = .09, 95% CI[-.02, .20]). Therefore, hypotheses 2a, 2b, and 2c were not supported.
PREDICTING DECISION-MAKERS' ALGORITHM USE
18
[-Predictor Validity Beliefs
Supporting hypotheses 3a, 3b, and 3c, inaccurate predictor validity beliefs correlated
moderately positively with algorithm deviation (r = .31, 95% CI [.20, .41]), and moderately
negatively with judgment consistency (r = -.27, 95% CI [-.37, -.16]), and predictive validity (r
= -.18, 95% CI [-.29, -.07]).
Cognitive Ability (Exploratory Analysis)
Cognitive ability was negatively related to algorithm deviation (r = -.22, 95% CI [-.35,
-.08]), positively correlated with judgment consistency (r = .24, 95% CI [.10, .36]), and
weakly positively, but not significantly related to predictive validity (r = .13, 95% CI [-.01,
.26]).
]), judgment consistency (r = .03, 95% CI


Table 1
1
Pearson's and Spearman's Correlations among Study Variables Note. Correlations including cognitive ability (subtests) are based on N = 197. All other correlations are based on N = 308. Pearson's and Spearman's correlations are presented below and above the main diagonal, respectively. a LN = letter and number series. b MR = matrix reasoning. * p < .05.
Variable
M
SD
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
1. Conscientiousness
3.60
0.62
-
.92* .88* .84* .89* .86* .93* .37* -.11*
.10
-.05
.24* -.24* .32* .26*
2. Self-efficacy
3.77
0.65
.92*
-
.77* .82* .79* .74* .84* .39* -.14*
.07
-.06
.21* -.23* .31* .27*
3. Orderliness
3.47
0.61
.90* .78*
-
.69* .75* .76* .81* .33*
-.03
.12
-.03
.24* -.18* .25* .21*
4. Achievement striving
3.72
0.59
.88* .83* .74*
-
.74* .62* .77* .40* -.11*
.06
-.02
.14
-.22* .30* .24*
5. Self-discipline
3.61
0.70
.93* .82* .80* .79*
-
.76* .81* .33*
-.06
.02
-.09
.14* -.18* .25* .22*
6. Cautiousness
3.33
0.76
.92* .79* .80* .70* .83*
-
.81* .24*
-.06
.13
-.02
.28* -.23* .29* .20*
7. Dutifulness
3.71
0.75
.95* .87* .82* .79* .84* .87*
-
.33*
-.09
.12
-.02
.25* -.22* .29* .22*
8. Experientiality
3.18
0.46
.32* .32* .30* .34* .31* .23* .28*
-
-.12*
.08
.01
.11
-.04
.11*
.08
9. Inaccurate validity beliefs
32.48 23.41 -.10* -.16* -.04
-.09
-.06
-.09 -.11* -.10
-
.04
.08
.00
.26* -.27* -.16*
10. Cognitive ability
0
0.80
.13
.10
.15*
.07
.05
.16*
.14
.02
.05
-
.81* .75* -.20* .24*
.07
11. Cognitive ability (LN) a
6
2.41
-.03
-.05
-.01
-.01
-.06
-.02
-.02
-.05
.11
.80*
-
.25*
-.12
.15*
-.02
12. Cognitive ability (MR) b
4.62
2.42
.23* .21* .25*
.13
.13
.28* .24*
.08
-.03
.80* .28*
-
-.22* .26* .17*
13. Algorithm deviation
0.49
0.40 -.30* -.31* -.24* -.25* -.24* -.29* -.29* -.02
.31* -.22* -.10 -.25*
-
-.93* -.65*
14. Judgment consistency
0.76
0.29
.22* .22* .18* .21* .16* .22* .22*
.03
-.27* .24* .18* .20* -.84*
-
.67*
15. Predictive validity
0.26
0.15
.32* .34* .27* .30* .28* .27* .31*
.09
-.18*
.13
.01
.19
-.71* .60*
-


We derived the algorithm's percentage weights by dividing each standardized regression weight (cf. the section Algorithmic Advice) by the sum of the standardized regression weights.














MTurk research: Review and recommendations




H
Aguinis






I
Villamor






R
S
Ramani








Journal of Management




47


4


















10.1177/0149206320969787














Factors influencing the use of a decision rule in a probabilistic task




H
R
Arkes






R
M
Dawes






C
Christensen




10.1016/0749-5978








Organizational Behavior and Human Decision Processes






37














Communicator credibility and communication discrepancy as determinants of opinion change




E
Aronson






J
A
Turner






J
M
Carlsmith




10.1037/h0045513








The Journal of Abnormal and Social Psychology




67


1
















Effects of cognitive feedback on performance




W
K
Balzer






M
E
Doherty






R
Connor
Jr




10.1037/0033-2909.106.3.410








Psychological Bulletin




106


3
















A unifying framework to study workplace decision-making aptitude and performance




N
Blacksmith






M
E
Mccusker






T
L
Hayes




10.25035/pad.2020.02.008








Personnel Assessment and Decisions




6


2
















The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and metaanalysis




P
Bobko






P
L
Roth






M
A
Buster








Organizational Research Methods




10


4


















10.1177/1094428106294734














Communicator discrepancy, source credibility, and opinion change




S
Bochner






C
A
Insko








Journal of Personality and Social Psychology




4


6


















10.1037/h0021192














Advice taking and decision-making: An integrative literature review, and implications for the organizational sciences




S
Bonaccio






R
S
Dalal








Organizational PREDICTING DECISION-MAKERS' ALGORITHM USE 26
















Behavior and Human Decision Processes




101
















10.1016/j.obhdp.2006.07.001














Amazon's Mechanical Turk: A new source of inexpensive, yet high-quality, data?




M
Buhrmester






T
Kwang






S
D
Gosling




10.1177/1745691610393980








Perspectives on Psychological Science




6


1
















A systematic review of algorithm aversion in augmented decision making




J
W
Burton






M
K
Stein






T
B
Jensen




10.1002/bdm.2155








Journal of Behavioral Decision Making




33


2
















Amazon Mechanical Turk in organizational psychology: An evaluation and practical recommendations




J
H
Cheung






D
K
Burns






R
R
Sinclair






M
Sliter




10.1007/s10869-016-9458-5








Journal of Business and Psychology




32


4
















The international cognitive ability resource: Development and initial validation of a public-domain measure




D
M
Condon






W
Revelle




10.1016/J.INTELL.2014.01.004








Intelligence




43


1
















The incremental validity of interview scores over and above cognitive ability and conscientiousness scores




J
M
Cortina






N
B
Goldstein






S
C
Payne






H
K
Davison






S
W
Gilliland








Personnel Psychology




53


2


















10.1111/j.1744-6570.2000.tb00204.x














Four ways five factors are basic




P
T
Costa






R
R
Mccrae




10.1016/0191-8869(92)90236-I








Personality and Individual Differences




13


6
















Facet scales for agreeableness and conscientiousness: A revision of the NEO personality inventory




P
T
Costa






R
R
Mccrae






D
A
Dye




10.1016/0191-8869(91)90177-DPREDICTINGDECISION-MAKERS'ALGORITHMUSE27








Personality and Individual Differences




12


9
















The committee charge, framing interpersonal agreement, and consensus models of group quantitative judgment




J
H
Davis






P
Zarnoth






L
Hulbert






X
Chen






C
Parks






K
Nam








Organizational Behavior and Human Decision Processes




72


2


















10.1006/obhd.1997.2733














A case study of graduate admissions: Application of three principles of human decision making




R
M
Dawes








American Psychologist




26


2


















10.1037/h0030868














The robust beauty of improper linear models in decision making




R
M
Dawes




















10.1037/0003-066X.34.7.571








American Psychologist




34


7














Clinical versus actuarial judgment




R
M
Dawes






D
Faust






P
E
Meehl




10.1126/science.2648573








Science




4899
















The profile of the 'good judge' in HRM: A systematic review and agenda for future research




F
S
De Kock






F
Lievens






M
P
Born




10.1016/j.hrmr.2018.09.003








Human Resource Management Review




30


2


100667














Broad versus narrow traits: Conscientiousness and honesty-humility as predictors of academic criteria




A
De Vries






R
E
De Vries






M
P
Born




10.1002/PER.795








European Journal of Personality




25


5
















People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error




B
J
Dietvorst






S
Bharti




10.1177/0956797620948841








Psychological Science




10
















Algorithm aversion: People erroneously avoid algorithms after seeing them err




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1037/xge0000033








Journal of Experimental Psychology: General




144


1
















Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1287/mnsc.2016.2643








Management Science




64


3
















The influence of secrecy on advice taking: A self-protection perspective




J
Duan






A
Song






Y
Sun






L
Van Swol








Current Psychology


















10.1007/s12144-022-02982-7














Influence of self-concept clarity on advice seeking and utilisation




J
Duan






Y
Xu






L
M
Van Swol








Asian Journal of Social Psychology




24


4


















10.1111/ajsp.12435














Individual differences in intuitiveexperiential and analytical-rational thinking styles




S
Epstein






R
Pacini






V
Denes-Raj






H
Heier




10.1037/0022-3514.71.2.390








Journal of Personality and Social Psychology




71


2
















Selection myths: A conceptual replication of HR professionals' beliefs about effective human resource practices in the US and Canada




P
A
Fisher






S
D
Risavy






C
Robie






C
J
König






N
D
Christiansen






R
P
Tett






D
V
Simonet




10.1027/1866-5888/a000263








Journal of Personnel Psychology




20


2
















On the accuracy of personality judgment: A realistic approach




D
C
Funder




10.1037/0033-295X.102.4.652








Psychological Review




102


4
















Clinical versus mechanical prediction: A meta-analysis




W
M
Grove






D
H
Zald






B
S
Lebow






B
E
Snitz






C
Nelson








Psychological Assessment




12


1


















10.1037/1040-3590.12.1.19














Broken legs, clinical overrides, and recidivism risk: An analysis of decisions to adjust risk levels with the LS/CMI




J
P
Guay






G
Parent




10.1177/0093854817719482








Criminal Justice and Behavior




45


1
















Cognitive control




K
R
Hammond






D
A
Summers




10.1037/h0031851








Psychological Review




79


1


















J
Han






J
Pei






M
Kamber




Data mining: Concepts and techniques




Morgan Kaufmann




3












The accuracy of recidivism risk assessments for sexual offenders: A meta-analysis of 118 prediction studies




R
K
Hanson






K
E
Morton-Bourgon




10.1037/a0014421








Psychological Assessment




21


1
















Stubborn reliance on intuition and subjectivity in employee selection




S
Highhouse




10.1111/j.1754-9434.2008.00058.x








Industrial and Organizational Psychology: Perspectives on Science and Practice




1


3
















Improving workplace judgments by reducing noise: Lessons learned from a century of selection research




S
Highhouse






M
E
Brooks








Annual Review of Organizational Psychology and Organizational Behavior




10


1


















10.1146/annurev-orgpsych-120920-050708














Discretion in hiring




M
Hoffman






L
B
Kahn






D
Li




10.1093/qje/qjx042








The Quarterly Journal of Economics




133


2
















Implications of direct and indirect range restriction for meta-analysis methods and findings




J
E
Hunter






F
L
Schmidt






H
Le




10.1037/0021-9010.91.3.594








Journal of Applied Psychology




91


3




















A
T
Jackson






M
E
Young






S
S
Howes






P
A
Knight






S
L
Reichin


















Examining factors influencing use of a decision aid in personnel selection


10.25035/pad.2019.01.001








Personnel Assessment and Decisions




5


1














Journal of PREDICTING DECISION-MAKERS' ALGORITHM USE 30 Occupational and Organizational Psychology




D
J R
Jackson






C
Dewberry






J
Gallagher






L
Close








91








A comparative study of practitioner perceptions of selection methods in the United Kingdom










10.1111/joop.12187














Determinants of linear judgment: A meta-analysis of lens model studies




N
Karelaia






R
M
Hogarth








Psychological Bulletin




134


3


















10.1037/0033-2909.134.3.404














A review and analysis of the policy-capturing methodology in organizational research: Guidelines for research and practice




R
J
Karren






M
W
Barringer








Organizational Research Methods




5


4


















10.1177/109442802237115














Too arrogant for their own good? Why and when narcissists dismiss advice




E
E
Kausel






S
S
Culbertson






P
I
Leiva






J
E
Slaughter






A
T
Jackson








Organizational Behavior and Human Decision Processes




131


















10.1016/j.obhdp.2015.07.006














Overconfidence in personnel selection: When and why unstructured interview information can hurt hiring decisions




E
E
Kausel






S
S
Culbertson






H
P
Madrid








Organizational Behavior and Human Decision Processes




137


















10.1016/j.obhdp.2016.07.005














Scale mean and variance differences in MTurk and non-MTurk samples: A meta-analysis




M
G
Keith






B
A
Stevenor






S
T
Mcabee




10.1027/1866-5888/a000309








Journal of Personnel Psychology
















Assessment center procedures: Cognitive load during the observation phase




N
J
Kolk






M
P
Born






H
Van Der Flier






J
M
Olman




10.1111/1468-2389.00217








International Journal of Selection and Assessment




10


4
















Mechanical versus clinical data combination in selection and admissions decisions: A meta-analysis




N
R
Kuncel






D
M
Klieger






B
S
Connelly






D
S
Ones




10.1037/a0034156








Journal of Applied Psychology




98


6
















An inconvenient truth: Arbitrary distinctions between organizational, Mechanical Turk, and other convenience samples




R
N
Landers






T
S
Behrend




10.1017/iop.2015.13








Industrial and Organizational Psychology




8


2
















Personnel selection: A longstanding story of impact at the individual, firm, and societal level




F
Lievens






P
R
Sackett






C
Zhang








European Journal of Work and Organizational Psychology




30


3


















10.1080/1359432X.2020.1849386














Going with your gut: An investigation of why managers prefer intuitive employee selection




M
A
Lodato












Doctoral dissertation








Clinical versus statistical prediction: A theoretical analysis and a review of the evidence




P
E
Meehl




10.1037/11281-000








University of Minnesota Press












Identifying and cultivating superforecasters as a method of improving probabilistic predictions




B
Mellers






E
Stone






T
Murray






A
Minster






N
Rohrbaugh






M
Bishop






E
Chen






J
Baker






Y
Hou






M
Horowitz






L
Ungar






P
Tetlock




10.1177/1745691615577794








Perspectives on Psychological Science




10


3
















Not all trust is created equal: Dispositional and historybased trust in human-automation interactions




S
M
Merritt






D
R
Ilgen








Human Factors




50


2


















10.1518/001872008X288574














Education increases decision-rule use: An investigation of education and incentives to PREDICTING DECISION-MAKERS' ALGORITHM USE 32 improve decision making




M
Neumann






M
Hengeveld






A
S M
Niessen






J
N
Tendeiro






R
R
Meijer




10.1037/xap0000372








Journal of Experimental Psychology: Applied




28


1
















Holistic and mechanical combination in psychological assessment: Why algorithms are underutilized and what is needed to increase their use




M
Neumann






A
S M
Niessen






P
M
Hurks






R
R
Meijer




10.1111/ijsa.12416








International Journal of Selection and Assessment
















When and why decision makers use algorithms in personnel selection: Stakeholder perceptions, use intentions, and predictive validity




M
Neumann






A
S M
Niessen






M
Linde






J
Tendeiro






R
Meijer




10.31234/osf.io/743hn


















Implementing evidence-based assessment and selection in organizations: A review and an agenda for future research




M
Neumann






A
S M
Niessen






R
R
Meijer








Organizational Psychology Review




11


3


















10.1177/2041386620983419














The autonomyvalidity dilemma in mechanical prediction procedures: The quest for a compromise




M
Neumann






A
S M
Niessen






J
N
Tendeiro






R
R
Meijer




10.1002/bdm.2270








Journal of Behavioral Decision Making




35


4














Using narratives and numbers in performance prediction: Attitudes, confidence, and validity




A
S M
Niessen






E
E
Kausel






M
Neumann




10.1111/IJSA.12364








International Journal of Selection and Assessment




30


2
















The relation of rational and experiential information processing styles to personality, basic beliefs, and the ratio-bias phenomenon




R
Pacini






S
Epstein




10.1037/0022-3514.76.6.972








Journal of Personality and Social Psychology




76


6
















R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing




R Core Team




















The structure of conscientiousness: An empirical investigation based on seven major personality questionnaires




B
W
Roberts






O
S
Chernyshenko






S
Stark






L
R
Goldberg




10.1111/j.1744-6570.2005.00301.x








Personnel Psychology




58


1




















B
W
Roberts






J
J
Jackson






J
V
Fayard






G
Edmonds






J
Meints




















Conscientiousness




Handbook of individual differences in social behavior


M. R. Leary & R. H. Hoyle




The Guilford Press














HR professionals' beliefs about effective human resource practices: Correspondence between research and practice




S
L
Rynes






A
E
Colbert






K
G
Brown




10.1002/hrm.10029








Human Resource Management




41


2
















Revisiting meta-analytic estimates of validity in personnel selection: Addressing systematic overcorrection for restriction of range




P
R
Sackett






C
Zhang






C
M
Berry






F
Lievens








Journal of Applied Psychology




107


11


















10.1037/APL0000994














The gap between research and practice: A replication study on the HR professionals' beliefs about effective human resource practices




K
Sanders






M
Van Riemsdijk






B
Groen








International Journal of Human Resource Management




19


10


















10.1080/09585190802324304














The validity and utility of selection methods in personnel psychology: Practical and theoretical implications of 85 years of research findings




F
L
Schmidt






J
E
Hunter




10.1037/0033-2909.124.2.262








Psychological Bulletin




124


2
















Some people heed advice less than others: Agency (but not communion) predicts advice taking




T
Schultze






T
M
Gerlach






J
C
Rittich




10.1002/BDM.2065








Journal of Behavioral Decision Making




31


3
















The effects of personality and locus of control on trust in humans versus artificial intelligence




N
N
Sharan






D
M
Romano




10.1016/j.heliyon.2020.e04572








Heliyon




6


8
















M
Sherif






C
I
Hovland




Social judgment: Assimilation and contrast effects in communication and attitude change




Greenwood Press




4












The recalcitrance of overconfidence and its contribution to decision aid neglect




W
R
Sieck






H
R
Arkes




10.1002/bdm.486








Journal of Behavioral Decision Making




18


1
















Superforecasting: The art and science of prediction




P
E
Tetlock






D
Gardner








Crown Publishers/Random House












A personality trait-based interactionist model of job performance




R
P
Tett






D
D
Burnett








Journal of Applied Psychology




88


3


















10.1037/0021-9010.88.3.500














Situation trait relevance, trait expression, and crosssituational consistency: Testing a principle of trait activation




R
P
Tett






H
A
Guterman




10.1006/jrpe.2000.2292








Journal of Research in Personality




34


4
















Power, competitiveness, and advice taking: Why the powerful don't listen. Organizational Behavior and Human Decision Processes




L
P
Tost






F
Gino






R
P
Larrick




10.1016/j.obhdp.2011.10.001








117














Survey on the use of clinical and mechanical prediction methods in clinical psychology




S
I
Vrieze






W
M
Grove




10.1037/a0014693








Professional Psychology: Research and Practice




40


5
















Receiving other people's advice: Influence and benefit




I
Yaniv








Organizational Behavior and Human Decision Processes




93


1


















10.1016/j.obhdp.2003.08.002














Advice taking in decision making: Egocentric discounting and reputation formation




I
Yaniv






E
Kleinberger




10.1006/OBHD.2000.2909








Organizational Behavior and Human Decision Processes




83


2
















Using advice from multiple sources to revise and improve judgments




I
Yaniv






M
Milyavsky




10.1016/j.obhdp.2006.05.006


104- 120








Organizational Behavior and Human Decision Processes




103


1














Making sense of recommendations




M
Yeomans






A
Shah






S
Mullainathan






J
Kleinberg








Journal of Behavioral Decision Making




32


4


















10.1002/bdm.2118














Pushing the limits for judgmental consistency: Comparing random weighting schemes with expert judgments




M
C
Yu






N
R
Kuncel










Personnel Assessment and Decisions




6


2
















Testing the value of expert insight: Comparing local versus general expert judgment models




M
C
Yu






N
R
Kuncel




10.1111/IJSA.12356PREDICTINGDECISION-MAKERS'ALGORITHMUSE








International Journal of Selection and Assessment




30


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]