You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Surveys have increasingly been plagued by individuals' decreased willingness to participate in surveys (i.e., low response propensity) 
(Groves 2006;
Williams and Brick 2018)
. Fewer completed surveys increase the risk of biased point estimates, correlations, and generalizability of findings 
(Abraham, Maitland, and Bianchi 2006;
Couper and de Leeuw 2015)
. Weaker generalizability leads to misperceptions about reality and/or public opinion, and consequently, to poorly informed policy decision-making 
(Groves 2006)
.
In previous studies, immigrants and individuals living in socially disadvantaged areas (areas characterized by widespread organized crime, distrust in the authorities, and low willingness to collaborate with law enforcement) have been identified as especially unlikely to participate in surveys 
(Shaghaghi, Bhopal, and Sheikk 2011;
van Wees et al. 2019
).
Furthermore, a meta-analysis of survey data in Sweden found that immigrants and young adults were among the least likely to complete mixed-mode questionnaires and that-despite controlling for several individual demographic attributes-people living in socially impoverished areas were also less likely to do so 
(Lundmark and Backström, 2023)
. It seems likely that sharing several of such demographic attributes may exacerbate the likelihood of not participating in surveys even further.
The nonresponse of such groups may be even more worrisome for survey research given the recent rapid growth of the population having such characteristics in Western countries. For example, the population of Sweden went from 11.8% of the inhabitants being foreign-born in 2000 to 20% foreign-born in 2021 (Statistics Sweden 2022). Increasing the response propensity among such groups may, therefore, be particularly effective in halting the increasing nonresponse bias found in Western societies.
However, to date, suggestions of the potential causes of the low response propensities among immigrants and groups in socially disadvantaged neighborhoods have remained relatively idle. Only a few studies have focused on conceiving potential mechanisms behind the lower response propensity among these groups, suggesting a need for a resurgence of new ideas for mechanisms of response propensities 
(Bates 2017;
Vahabi et al. 2015)
. Specifically, calls have been made for hypotheses-generating research where respondents "introspect" over their unwillingness to complete surveys 
(Singer 2011)
. Allowing respondents to discuss their reasons for not participating in surveys may provide fertile ground for the generation of hypotheses of the mechanisms behind response propensities among the general public, and among harder-to-reach groups specifically.
To remedy the stagnation of hypotheses generation of the mechanisms behind response propensities among immigrants and individuals living in socially disadvantaged areas, this manuscript presents the results of nine focus group interviews. These interviews resulted in the generation of ten hypotheses, two of which were novel in the published literature. Based on the findings of the focus group interviews, a second study testing one of the novel hypotheses was administered to a probability sample of socially disadvantaged and not socially disadvantaged areas in Sweden.


Proposed Explanations for Survey Nonresponse among Immigrants and Socially


Disadvantaged Areas
The declining response propensities of surveys in the decades prior and after the new millennia led to a growing body of literature that focused on factors associated with survey nonresponse. However, such research has tended to be descriptive rather than explanatory, with a strong focus on how socio-demographic factors relate to survey nonresponse. In such descriptive inquiries, educational attainment 
(Keeter et al. 2006;
van Wees et al. 2019
) and age 
(Bates 2017;
Shaghaghi et al. 2011
) stand out as one of the strongest predictors of nonresponse. However, three other demographic groups have also been identified as particularly unlikely to participate in surveys: ethnic minorities, immigrants, and inhabitants of economically/socially disadvantaged areas 
(Bates 2017;
Bates et al. 2019;
Couper and de Leeuw 2015;
Lundmark and Backström, 2023;
Shaghaghi et al. 2011;
van Wees et al. 2019)
.
That immigrant background is negatively related to response propensity may be of particular importance for European and North American countries, which all saw high rates of immigration between the 1990s and 2020s. In Sweden, in addition to doubling the proportion of the foreign-born population between 2000 and 2021, many Swedes were in 2021 identified as living in socially disadvantaged areas, making up a total of 5% of the entire population.
This creates a setup for a double curse of fostering low response propensity as areas identified as disadvantaged hold a population overrepresented by individuals with an immigrant background (Statistics Sweden 2018).
Some mechanisms of nonresponse in such groups have been suggested. For example, distrust of public institutions has been considered an explanatory factor of low response propensities among immigrants and individuals living in disadvantaged neighborhoods 
(Couper and de Leeuw 2015;
Vahabi et al. 2015;
Nápoles-Springer et al. 2004;
Yancey, Ortega, and Kumanyika 2006)
. A lack of social integration has also been proposed as a leading influence for nonresponse, perhaps leading to a fear of mistreatment, and exploitation of the data provided 
(Abraham et al. 2006;
Bates 2017)
. For instance, research has identified that such individuals were concerned with data being collected to benefit the personal careers of researchers rather than being for the good of the community or that data would be collected to portray the surveyed community unfavorably 
(Yancey et al. 2006)
.
Unfortunately, most of the studies on distrust as a mechanism of nonresponse have been conducted among the U.S. Black population. Therefore, the lack of trust found in previous studies may partly be explained by the U.S.'s history of slavery and racial discrimination 
(Nápelos-Springer et al. 2004)
. It remains unclear to what extent distrust as a mechanism for nonresponse can be generalized to other countries and minority groups.
Furthermore, economically disadvantaged people and people who fear confrontation with legal authorities have been found less likely to participate in surveys 
(Abraham et al. 2006;
Bates and Mulry 2011)
. However, to the best of our knowledge, it has not yet been studied whether immigrants and inhabitants of disadvantaged areas tend to be overrepresented in the groups who fear confrontation with legal authorities. Research from the U.S. has found that being an ethnic minority, such as having a Black African/American or Hispanic background, lower education, and unemployment rates tended to coincide with fear of confrontation and with living in more disadvantaged areas 
(Bates and Mulry 2011)
.
Consequently, it may be likely that disadvantaged areas and immigrants in non-U.S. contexts show similar patterns of participation in surveys. Lastly, a wide range of studies have identified language barriers as a crucial explanation for nonresponse among immigrants (e.g.,


Bates 2017; Brick and Williams 2013).
A theory of survey response suggests that a respondent needs to believe that the benefits of completing a questionnaire will outweigh the costs of completing it 
(Dillman et al., 2014, p. 25)
. To increase the perceived benefit, theories such as the leverage-saliency theory of survey response (e.g., 
Groves et al. 2000;
Groves and Heeringa 2006)
 suggest that any survey request hold several attributes (e.g., the topic of the questions, design of the questionnaire, the sender of the request, what data will be used for). Furthermore, the relevance of these attributes is thought to vary for groups of respondents, and highlighting some attributes may increase response propensity for some while decreasing it for others. To that end, tailoring the survey request for different groups of respondents 
(Dillman et al., 2014;
Dillman, 2020;
Lynn, 2014;
Tourangeau et al., 2017)
 as well as tailoring refusal conversion attempts toward the hard-to-reach groups has been suggested (e.g., 
Stoop 2004;
Stoop et al., 2010, chapter 7)
. A tailored request can entail offering different groups of respondents varying amounts of monetary incentives 
(Singer and Ye 2013)
, asking questions that hard-to-reach groups will find interesting (see Singer 2011), targeting the survey invite to the group 
(Christensen et al., 2019)
, targeting the reminder to the group 
(Stoop et al., 2010, chapter 7)
, or offering a respondent's preferred mode for completing the interview (see de Leeuw 2018 for an overview). However, to tailor the survey attributes to increase response propensities, survey researchers need to acquire a good understanding of which survey attributes are important for the different groups of respondents and how these attributes should be used for nudging them toward survey participation.
Exploring nonresponse among immigrants and individuals living in socially impoverished areas should increase the ability of future survey researchers to successfully tailor survey requests optimizing response propensities of hard-to-reach groups. Furthermore, assessing the generalizability of the mechanisms behind nonresponse uncovered among hardto-reach subgroups in North America to European contexts (e.g., 
Abraham et al. 2006;
Bates 2017;
Couper and de Leeuw 2015;
Vahabi et al. 2015;
Nápoles-Springer et al. 2004;
Yancey, Ortega, and Kumanyika 2006)
, and undertaking a theory-generating approach on the causes of nonresponse of hard-to-reach subgroups in Europe, may provide insights on how to combat nonresponse bias in surveys. The present manuscript attempts to shed light on nonresponse through respondents' introspection of their nonresponse behavior.


Exploratory Phase: Focus Group Interviews


Method
To generate hypotheses of nonresponse among immigrants and inhabitants of socially disadvantaged areas, nine focus groups were conducted in February 2020. Focus groups have been found especially apt for understanding behavior and perceptions of specific groups 
(Ekström 2010;
Morgan and Krueger 1993)
, as well as for generating new ideas and developing hypotheses of such behavior or perceptions 
(Breen 2006)
. Compared to individual interviews and other group interviews, focus group interviews encourage interaction between participants rather than between the participants and the moderator. Encouraging interaction is thought to allow the participants less aware of their perceptions and behavior to, by contrasting and comparing their thoughts to those expressed by other participants, also become able to elicit their own perceptions 
(Morgan and Krueger 1993)
. The group interaction in focus groups is also suggested as fitting when interviewing individuals belonging to groups with limited power, influence, and language skills 
(Barron et al. 2010;
Morgan and Krueger 1993)
. In addition, compared to individual in-depth interviews focusing on a specific individual's experiences, focus groups may be appropriate when a breadth of generated hypotheses is sought rather than an understanding of a specific individual behavior 
(Crabtree et al., 1993)
. Given that the purpose was to generate hypotheses from the experiences, behavior, and perceptions of survey participation among disadvantaged and minority groups, focus groups were deemed more appropriate than in-depth individual interviews.
The recruitment and the interviews were conducted in Biskopsgården, a residential area in Gothenburg, Sweden, identified by the Swedish Police as extremely socially disadvantaged 
(Polisen 2021)
. To be identified as a socially disadvantaged area by the Swedish police, the area had to have a high crime rate with widespread organized crime and where inhabitants showed great distrust in the police. At the time of the interviews, 60% of Biskopsgården's residents were immigrants (Göteborgs Stad 2022). Biskopsgården was chosen as the venue for recruitment to increase the chances of finding individuals residing in socially impoverished areas and who were immigrants (i.e., socio-demographic factors linked to nonresponse). Immigrants were defined as individuals not born in Sweden and who have since taken up residence in Sweden. Recruiting individuals who had one or both of these demographic attributes increased the chances of finding individuals who were likely especially hard-to-reach in surveys but still recruitable for focus group interviews. In addition, before being recruited, participants were asked if they typically agreed to answer surveys to which most answered no. All participants knew what a survey was, which should facilitate a fruitful discussion, but may limit the hypothesis generation to individuals who have at least some knowledge of surveys. To increase the likelihood of theoretical saturation, we allowed individuals who possessed just one or both of the demographic attributes to be recruited.
The interviews centered around reasons for nonresponse and self-experienced low willingness to participate in surveys. Each interview consisted of three to six individuals.
Participants in three of the nine focus groups were recruited via the Swedish national language education course Swedish for Immigrants (SFI) in Biskopsgården. SFI is a voluntary language course offered by the Swedish state and for which all recent immigrants from non-Nordic countries are invited to participate (provided they have a Swedish social security number). In the language course, participants are taught basic Swedish at a level of making it through everyday life in Sweden. Participants enrolled at SFI come from a diverse set of immigrant backgrounds and many have only an elementary education from other countries. The other six focus group participants were recruited at the cultural center/library or in the main square of Biskopsgården. Across all nine focus groups, only 24% had studied at a college or university.
All interviews were conducted in a room at the library/cultural center and were conducted in Swedish, using a semi-structured interview guide designed to tap the perceptions and experiences of surveys. All participants were sufficiently proficient in Swedish to approximate fluent conversation. The full transcripts, the exit questionnaire, and the interview guide can be found at osf.io/8nfcd/.


Sample
The nine focus groups comprised 33 participants with a majority of young, loweducated, and immigrant participants (for exact demographic composition, see Supplementary Online Materials, SOM, 
Table S1
). Within each group, there was homogeneity in terms of age, language skills, and being an immigrant or not. Homogeneity within groups is preferable for establishing hypotheses-generating interaction and dynamics (see 
Grønkjaer et al. 2011)
. Our recruitment was not aimed specifically toward young adults.
However. given that young adults have been identified as likely nonrespondents (e.g., 
Lundmark & Backström, 2023)
, interviewing young adults should have increased the likelihood of having recruited low response propensity participants, and doing so allowed us to identify several hypotheses relating to young immigrants and young individuals living in socially disadvantaged neighborhoods.


Procedure
First, participants were welcomed, the intended focus group process was explained, were informed that their participation was voluntary and anonymous, and that the interviews would be audio recorded but that the recording would be deleted after transcription.
Participants were also informed they could exit the interview at any time and would be compensated regardless of completing the interview or not. The interviews lasted 20 to 40 minutes and were transcribed in as much detail as possible to provide a text that was true to the original. Minor changes (grammatical corrections and minor clarifications) were made in cases where it was considered necessary to facilitate reading.


Analysis
The interviews were analyzed following 
Braun and Clarke's (2006)
 step-by-step guide to thematic analysis, a method for identifying, analyzing, and reporting themes within data. At the final step of the thematic analysis, vivid quotes/dialogues were selected to represent each theme and were included in the results section. Each step of the thematic analysis is detailed in SOM, section S1.


Results


Overview of the Themes
The analysis resulted in a thematic map with three main themes with a total of eight subthemes (see 
Table 1
). Quotes supporting each of the themes can be found in SOM, section S2.


Main Theme 1: Perceptions of Societal Factors
A main theme that participants elaborated on was how societal factors may explain why they choose not to participate in surveys. Within this theme, two subthemes were identified. Subtheme 1.1. Opportunity to Exert Influence -Responsiveness by Society. The most prominent subtheme, appearing in each interview, was the need for an opportunity to exert influence and responsiveness by society. Arguments within this theme were often long and developed, and participants frequently returned to this subtheme during the interview. In particular, the participants projected a strong willingness to influence issues in their own residential area. However, they also expressed that they believed their possibilities to influence things were limited and that those asking them questions were likely not going to listen to their individual beliefs or suggestions anyway. Participants described this lack of responsiveness and the limited opportunities to influence as key reasons for not participating in surveys. whether it was possible to trust people and organizations, and that this distrust caused them to not participate in surveys or when government officials approached them. Participants who said they trusted people in general or that they trusted most authorities expressed that trust may be important for them and others when deciding to complete a survey. Similarly, if they felt aversion or disliked the sender of the survey, they said they would not answer. Several of the youngest participants mentioned that they would be unlikely to respond to a survey by the police because of their deep aversion toward the police in their neighborhood.


Main Theme 2: Characteristics of the survey
In the second main theme, different characteristics of the survey were identified as influencing response propensity across four subthemes. Subtheme 2.1. Financial Incentives. Participants often discussed the importance of direct rewards-such as money or gift cards-for participating in surveys. However, not all focus group participants felt that incentives were a prerequisite for participation. For instance, one participant expressed that the opportunity to influence policy was a stronger mechanism for participation than financial incentives. Subtheme 2.2. Method and Recruitment Strategy. Moreover, participants tended to discuss the pros and cons of different survey modes. Whereas some participants argued they would be more likely to participate if receiving a paper-and-pencil questionnaire, othersoften younger participants-said they preferred online questionnaires. Participants also exuded positivity toward face-to-face interviews, and none had anything negative to say about that mode. Furthermore, participants elaborated on how innovative recruitment strategies would increase their response propensity, such as asking them to participate while queuing at the migration agency. Under this subtheme, many participants mentioned that it could be off-putting if the survey looked formal and that simple language would be beneficial, especially for immigrants. Whereas many participants agreed that less formal language would encourage participation, others expressed wanting more color, figures, slogans, and pictures. Moreover, participants discussed different ways to frame the survey. For example, emphasizing that the participant was important and valuable and that their opinions would affect the outcome of the study would increase survey participation.


Main theme 3: Individual-Level Factors
Finally, participants elaborated on individual-level factors they thought might influence the likelihood of participating in surveys across two subthemes.


Survey Self-Efficacy, Interest, and Engagement. Several participants believed
that their ability to formulate their thoughts into opinions was a hurdle for participation (which we in the suggested hypotheses below define as low survey self-efficacy). They expressed that they did not want to cause problems for the interviewer by not knowing how to appropriately fill out questionnaires or develop their thoughts into opinions. Furthermore, interest and engagement in the topics being asked about were also mentioned as factors for participation.
Subtheme 3.2. Language Skills. Participants also expressed that Swedish language skills were important to get them to participate in surveys and that translating questionnaires would have increased their response propensity.


Hypotheses Generated
To guide future research, the mechanisms identified in the exploratory phase were translated into suggested hypotheses presented in 
Table 2
 and discussed below.


Table 2.
Hypotheses generated through the focus group analysis.


Hypothesis Response propensities among immigrants and inhabitants of socially impoverished areas may be increased by…
Based on the subtheme discovered in the focus group interviews H1: informing respondents how their participation could enable them to exert an influence over political policies.
1.1. Opportunity to exert influenceand the responsiveness from society; & 2.4. Framing and style of the survey. H2: increasing generalized trust/trust in the authorities.
1.2. Trust and attitude toward the sender. H3: offering a monetary incentive.
2.1. Financial incentives. H4: tailoring the survey to be administered online for younger respondents, and paper-and-pencil for older age groups .


Method and recruitment strategy.
H5: face-to-face interviews instead of other survey modes.


Method and recruitment strategy.
H6: shortening the questionnaire.
2.3. Design of the questionnaire. H7: using less formal language in the invitation and the questions.


Framing and style of the survey.
H8: increasing the interest in the survey.
3.1. Interest, engagement, and perceived self-efficacy; & 2.4. Framing and style of the survey.
H9: increasing the respondents' self-efficacy. 3.1. Interest, engagement, and perceived self-efficacy. H10: translating the survey into respondents' mother tongue.


Language skills and understanding of questionnaires.
Note. The hypotheses were generated through focus group interviews with immigrants and young adults living in socially disadvantaged areas.
H1 suggests that perceiving oneself as not able to influence and understand policy may decrease response propensities. Such internalized perception of the ability to influence is called internal political efficacy 
(Balch 1974)
. The participants expressed desires to influence issues but expressed limits to their potential of doing so. They argued this limit was a reason for their non-participation in surveys. Research has identified that a decreasing willingness to bear social responsibility (e.g., voter turnout) was related to declining response rates 
(Brick and Tourangeau 2017)
, and internal political efficacy has, in turn, been found to be related to voting behavior (see 
Balch 1974)
. However, to the authors' knowledge, the hypothesis that leveraging internal political efficacy may increase response propensities among hard-to-reach groups has not been proposed previously.
H2 posits that increasing generalized trust and trust in the authorities may increase response propensities. Several focus groups suggested that distrust toward other people and the authorities was a reason for their non-participation. The decline in societal trust has been identified as a likely cause for the declining response rates in the U.S. and Europe 
(Couper and de Leeuw 2015;
Shaghaghi et al. 2011)
. Finding ways to increase generalized and institutional trust, at least temporarily, may persuade low-trusting respondents to participate in surveys.
H3 suggests that incentives may increase response propensities, especially among young adults, immigrants, and people living in socially disadvantaged areas. Several studies have found that incentives increase response propensities 
(Singer and Ye 2013)
. H3 suggests that incentives may be of increased importance for raising response propensities among these hard-to-reach subgroups and survey requests may be fruitfully tailored to offer incentives identified as relevant for the subgroups with low response propensities.
H4 posits that response propensity may be increased by administering surveys online among younger respondents while administering mail surveys to older respondents. This hypothesis fits the theory of a tailored survey design 
(Dillman et al., 2014)
 suggesting that the survey mode be tailored to the mode the respondent prefers (see also 
Sakshaug et al. 2019)
 or offer multiple modes concurrently 
(Olson et al 2012;
 for an overview, see de Leeuw 2018).
H5 suggests that response propensities may be increased by offering face-to-face interviews over other modes, especially among immigrants. Previous research has found that allowing reluctant respondents to be interviewed face-to-face interview can increase a respondent sample's resemblance to the population 
(Klausch et al 2015)
 and that immigrants seem especially helped by in-person interviews 
(Kappelhof 2015)
. Face-to-face interviews may allow immigrants the chance to ask clarifying questions, which was mentioned as important by several of the immigrant focus group participants, and surveys may be optimized by tailoring the mode to be face-to-face for this group.
H6 suggests that shorter surveys may increase response propensities among young adults and immigrants. Short surveys have been found to increase response propensities in general 
(Japec et al. 1997;
Singer 2011
) but may be especially important when interviewing immigrants given that several focus group participants disclosed unfamiliarity with evaluating their attitudes toward their new home country.
H7 suggests that using less formal language in surveys may increase response propensities, especially with the subgroups in the focus groups. Ensuring that survey invitations and questions remain jargon-free and use easy-to-understand language is perhaps the first thing survey methodologists are taught (see 
Dillman et al. 2014
), and appears likely extra important for surveys administered to young adults and immigrants.
H8 suggests that interest in the topic may increase response propensities in general and among low-response propensity groups in particular. This is in line with theories that the topic of the survey can be tailored to increase response propensities (see Singer 2011) and identifying topics of interest for hard-to-reach groups may allow for optimized tailored designs.
H9 suggests that increased survey self-efficacy may increase response propensities.
Survey self-efficacy has, to the authors' knowledge, not been defined before and we, therefore, define it as an individual's perceived capacity to successfully fill out a questionnaire or answer questions in a survey interview setting. In general, self-efficacy within a certain domain is inferred through self-esteem, previous performances and anxiety toward the task, support from others and society, and the successful completion of the task by peers 
(Luzzo et al. 1999)
. Research has found self-efficacy in specific domains to be highly predictive of whether an individual decides to attempt the completion of a task within that domain 
(Witte 1992)
. Hence, increasing respondents' survey self-efficacy may increase response propensity, especially among immigrants and those living in socially disadvantaged areas where such efficacy may be lower. To the authors' knowledge, the role of survey selfefficacy for response propensity has not been suggested before. H10 suggests that the translation of surveys is likely to generate higher response propensities. This hypothesis has frequently been suggested in the survey methodology literature on surveying immigrants (Bates 2017; Couper and de Leeuw 2015) and tailoring the information in survey requests and the interview may be a fruitful way to optimize response propensities for immigrants especially, even among immigrants who are proficient in the main language of the country in which the survey is administered (as the participants in the focus group interviews were).


Confirmation Phase: Survey Experiment
In the confirmation phase, a survey experiment was administered to assess the first hypothesis generated; That response propensity may be increased when informing respondents that their participation might enable them to exert an influence over policies (i.e., internal political efficacy). This hypothesis was assessed since it captured the subtheme that was prominent across all nine focus group interviews. To assess the hypothesis, a random sample of respondents was randomly assigned to get an invitation to participate in a survey that either included a paragraph on how their participation may influence a policy (treatment group) or not (control group). The procedure, hypotheses, methods, and analytical strategies were preregistered and can be found at osf.io/uxywr.
The survey was administered in September 2020 in collaboration with the Police Department in Trollhättan, Sweden. Given that some participants in the focus groups expressed negative opinions specifically about the police, assessing the hypothesis may seem problematic. However, collaborating with the police offered a unique opportunity to stress the fact that answers may actually influence a real-life policy without the use of deception:
Every year, the local police ask the inhabitants what tasks they want the police to prioritize in the upcoming year. This is called a "citizen promise," and all local police departments in Sweden must generate one such promise each year in collaboration with the municipalities and the inhabitants. The police must base this promise on the results of a survey administered to the public as well as on the input from the local police and municipal civil servants. Hence, explaining to the sampled persons how their participation would inform policy was indeed true in our case. Swedish Tax Authority estimated that about 198,000 individuals (1.9% of the total population) were registered at the wrong address in 2018 of which 68,000 were estimated to no longer live in Sweden. Using the registry for sampling should, therefore, be unlikely to skew the results of the study due to undercoverage of immigrants or individuals living in socially impoverished areas.


Sample


Procedure
Sampled persons were invited by mail to complete an online questionnaire. The letter of invitation was placed in an envelope (the outside of the envelope had the Swedish Police and the survey organization's logo). In the invitation, all sampled persons read that they had been invited to complete a survey about their feelings of safety and the upcoming citizen promise. The sampled persons were instructed how to access the online questionnaire (either through a URL or by scanning a unique quick response (QR) code printed at the bottom of the invitation) and that the survey would take about 10 minutes to complete.
In the middle of the frontpage of the invitation, half of the sampled persons were randomly assigned to read a boxed paragraph telling them that, previously, very few people from their area had participated in the citizen promise development and that by completing the questionnaire they would have a better opportunity to influence the policy. The other half of the sampled persons did not see that paragraph. Section S3 in the SOM details the versions of the invitations. Twenty-four days after being invited, a reminder was mailed, and the identical manipulation was administered to the same individuals once more. No other interventions or efforts to raise response rates were administered.


Evaluation Criteria and Measures
The hypothesis was assessed by estimating the intervention's impact on response rate 1 (RR1) (AAPOR, 2016), nonresponse bias (R-indicators) 
(Schouten, Cobben, and Bethlehem 2009)
, and measurement error (item nonresponse, length of responses to openended questions, and non-differentiation) (e.g., 
Oscarsson and Arkhede 2019)
. For the estimations and coding of the measures, see SOM, section S4.


Results


Response Propensity
In contrast to the expected, sampled persons were not more likely to answer the survey when reading that they had the chance to influence a policy (M = 30.77%, SE = 0.02, N = 397) than when not reading this (M = 33.50%, SE = 0.02, N = 403) (b = 0.03, SE = 0.03, p = .41, d = 0.06). Furthermore, the effect that reading about the chance to influence a policy was not moderated by whether the sampled persons lived in a socially disadvantaged area (b = -0.09, SE = 0.07, p = .15, d = 0.10) (see 
Figure 1)
. The effect was also not moderated by whether the sampled person was an immigrant or not (b = -0.05, SE = 0.07, p = .46, d = 0.05) (see SOM, 
Table S3
). Immigrants were here defined as individuals not born in Sweden who had a Swedish social security number and were currently registered as living in Sweden. A nonpreregistered alternative logistic regression did not change any of these findings (see 
Table S4
). Note. Data from the survey experiment of the confirmation phase.


Nonresponse Bias
In contrast to the expected, the nonresponse bias was greater, with non-overlapping confidence intervals (CIs), when sampled persons read that they had the chance to influence a policy (R-indicator = .72, SE = .05, N = 397, 95% CI[.68, .76]) than when they did not read about that chance (R-indicator = .85, SE = .05, N = 403, 95% CI 
[.80, .90]
). This increase in nonresponse bias was due to immigrants being slightly less likely to complete the survey if they read about the opportunity to exert influence (bnot born in Sweden = -0.81, SE = 0.25, p < .01) than when they did not (bnot born in Sweden = -0.38, SE = 0.23, p = .10) (see 
Table S5
). In addition, participants aged 40-49 years old were slightly more likely than those 16-24 years old to complete the questionnaire when being told about exerting an influence (b40-49 = 1.69, SE = 0.43, p < .001) than when not (b40-49 = 0.62, SE = 0.41, p = .13).


Measurement Error
Sampled persons reading that they had the chance to influence a policy were not more likely to answer more questions (M = 94.94%, SE = 0.01, N = 144) than when not reading that (M = 94.53%, SE = 0.02, N = 131) (b = 0.00, SE = 0.02, p = .83). This effect was not moderated by living in a socially disadvantaged area (b = 0.02, SE = 0.04, p = .63), nor by immigrant status (b = 0.06, SE = 0.04, p =.11) (see 
Table S6
). However, reading about influencing policy made respondents type statistically significantly fewer characters (M = 281.68, SE = 22.48, N = 138) than not reading about influencing policy (M = 370.33, SE = 40.10, N = 123) (b = -88.65, SE = 44.72, p = 0.05). But an exploratory analysis revealed that this finding was driven entirely by 30 outliers (typing three standard deviations more characters than the sample average) and excluding those revealed that no difference was found between the treatment and control group (b = -15.71, SE = 20.12, p = .436). The effect on the number of characters typed was not moderated by living in a socially disadvantaged area (b = -54.85, SE = 41.43, p = 0.19), nor by immigrant status (b = -46.90, SE = 43.30, p = .28) (see 
Table S7
).


Conclusion
In this manuscript, an exploratory and a confirmatory phase were used to generate and assess hypotheses for nonresponse and response propensities to surveys in general and among immigrants and socially disadvantaged areas in particular. The exploratory phase produced several different hypotheses for mechanisms driving non-response among immigrants, young adults, and people living in socially disadvantaged areas. Many of the generated hypotheses have already been proposed in survey research, but our findings allude to their added importance when studying hard-to-reach groups like immigrants and people living in socially disadvantaged areas. However, two of the hypotheses generated appear completely novel to survey methodology: internal political efficacy (the perceived ability to influence policy) and survey self-efficacy (the perceived ability to answer surveys) may play a particularly important role for these groups. Perhaps hard-to-reach groups are opting not to participate because they believe they will do poorly in describing their attitudes and behaviors. If so, such respondents may inaccurately believe that non-participation is the most altruistic act and poses the best way to help the researcher or survey sender. Furthermore, even if they believe in their ability to answer the survey, a lack of internal political efficacy may persuade them to not participate since their attitudes and beliefs will not be listened to anyway.
In the confirmation phase, we did not find support for that attempting to increase internal political efficacy increased response propensities. We posit four explanations for this lack of support: (1) The sample was not large enough to detect small response propensity changes and a posthoc power analysis indicated that 2,175 individuals would have been needed for our observed effect size to be statistically significant with 80% power. 
2
 whether trust in the institutions or organizations influences respondents' trust that participation will influence real-life policy. Or (4) the hypothesis generated in the focus group simply did not generalize to a population outside the interviews.
Lastly, we want to encourage other social scientists to assess the hypotheses generated here. Evaluating the proposed mechanisms behind response propensities will be crucial for developing the tools needed to stem the rising tide of nonresponse bias in surveys and among hard-to-reach populations. 


S1. Description of Thematic Analysis
In the first step of 
Braun and Clarke's (2006)
 step-by-step guide to thematic analysis, the moderator and the project leader read the transcripts several times and shared reflections and initial ideas on the interviews. In the second step, keywords and sentences describing the essence of the data were coded by the moderator. Importantly, in the present analysis, the entire dataset was coded to avoid the risk of missing out on aspects that might be relevant. In the third step, themes were identified by analyzing the codes and checking whether codes belonged to overarching themes. This step resulted in a list of several candidate themes. In the fourth step, the candidate themes were reviewed. A few themes were identified as too broad or too complex and had to be separated into more distinct themes, whereas a few other themes were merged as they were not mutually exclusive. In the fifth step, definitions for each theme were generated by reading and rereading all the data belonging to each theme.
Thereafter, an intercoder reliability test was conducted by an external coder with experience in conducting focus groups but without experience in survey methodology (see, e.g., 
Wibeck, 2010)
. Ten percent of the material was randomly selected and then categorized under the themes based on the definitions of each theme. The test had an agreement rate of 72.7%. The majority of the errors were related to subthemes 2.3. and 2.4. The description of these subthemes was clarified to resolve the discrepancies between the two coders. At the final step of the thematic analysis, vivid quotes/dialogues were selected to represent each theme and presented in the results section of the hypothesis generation phase in the main text.


S2. Results and Quotes supporting the results of the Focus Group Interviews


Main Theme 1: Perceptions of Societal Factors
One of the main themes that participants elaborated on centered on societal factors explaining why people choose not to participate in surveys. Within this theme, two subthemes were identified: (1) Opportunity to exert an influenceresponsiveness by society;
and (2) Trust and attitude toward the sender. The most prominent subtheme, central to the majority of the focus groups, was the need for an opportunity to exert influence and responsiveness by society. Arguments within this theme were often long and developed, and participants frequently returned to this subtheme during the interview. In particular, the focus groups projected a strong willingness to influence issues in their own residential area:
Focus group 1.
-So if we sit down and talk about a youth club … maybe we're participating in an interview to express that we want a youth club; then we'll get something out of participating if it's realized. And it's like that, you want to fight for something … -Yeah.
-You want something in the end. Something to work for, and that we get something out of in the end.
Yet many participants expressed that they thought their possibilities to influence things were usually limited and that there was a lack of responsiveness from the decisionmakers. Participants described this lack of responsiveness and the limited opportunities to influence as a key reason for not participating in surveys: Many participants said they would be more likely to respond if they perceived the sender's organization to be credible and trustworthy: Focus group 5.
-If the questionnaire is from the police, I don't think anyone would like to participate.
-Moderator: No?
-No, and not if it is from the social administration. But like this, from the university … I don't have any problems with that.
-It's like, "Help me, help you." You help us, and we help you with your study so … for example, your boss or your teacher or whoever it is … I mean it gets better for you and better for us. However, not all focus groups felt that incentives were a prerequisite for participation.
For instance, one participant expressed that the opportunity to influence policy was a stronger mechanism for participation than financial incentives: argued they would be more likely to participate if receiving a paper-and-pencil questionnaire, others said paper questionnaires were the least appealing for them and preferred online questionnaires. Some participants mentioned the positive aspects of face-to-face interviews, and hardly anyone had anything negative to say about such interviews. For brevity of presentation, the opinions of different survey modes are summarized in 
Table 3 (for detailed
 excerpts, see SOM, S2). Furthermore, participants elaborated on different innovative recruitment strategies that might increase the response propensity among immigrants as well as among young people:
Focus group 4.
-Moderator: Do you think something can be done to get more people to answer?
-Yes, maybe you can go to the Swedish Migration Agency. In the queue. They can answer the questions.
-Or go to SFI schools.


Focus group 5.
Starting with mail questionnaires, some perceived these to be an inefficient way of recruiting participants and felt that they were too demanding:
-Do you mean that you send these out by mail?
-Moderator: Yes.
-Who would … I mean … who would sit down voluntarily … -But not only that … do you have to return it as well?
-Moderator: 
Yes. -No, no, wait, wait, wait.
 -Yes, it's a lot of work. "What 17/18-year-old is going to sit down and check the mail?" Online questionnaire "Young people are more likely to answer online surveys than mail surveys." "Not reliable: I don't want them to see me online." Face-to-face interview "You might feel more comfortable if you see the person." "Easy to ask follow-up questions and get help if you don't understand." N/A Phone interview N/A "You don't see the person." Note: Quotes from the nine focus groups. N/A = not applicable.


Focus group 9.
On the other hand, some participants said that they preferred mail questionnaires over online questionnaires, as they thought that mail questionnaires were more secure and provided an easier overview of the questionnaire:
-Moderator: And here we have two letters that are sent out to get people to join online surveys.
-I've received this.
-Moderator: Have you been asked to join an online survey?
-Yes.
-Moderator: Did you join?
-No, I refused.
-Moderator: What made you decide not to join?
-I didn't want them to see me online.
-Moderator: So, you thought that it wasn't that safe because it was online?
-Exactly.
-Moderator: Would you, for example, trust a mail questionnaire more?
-Yes.
-Because in today's society, you can do 1,000 different things online. That's why it might not be reliable. 


Focus group 2.
Another benefit of personal interviews that was raised during the interviews was that it was easier to ask follow-up questions if you did not understand the question:
-I don't remember which company it was, or what it was about. But anyway, I looked through it. I didn't understand that much, but I tried a bit. But then I finally got tired so I threw it out. interesting. Under this subtheme, many participants mentioned that it could be off-putting if the survey looked formal and that simple language would be beneficial, especially for immigrants:
Focus group 1. Moreover, participants discussed different ways to frame or "sell" the survey. For example, emphasizing that the participant was important and valuable and that their opinions would affect the outcome of the study would increase survey participation: Focus group 7.
-And, like I said, I think it should be clearly described how this will affect me.   -Then it's more interesting to answer.  Furthermore, the focus groups expressed that Swedish language skills and understanding the survey were important to get them to participate in surveys:


Main theme 3: Individual-Level Factors
Focus group 6.
-I received questionnaires from the housing company. Sometimes, at the end of the year, they ask questions like "Is everything alright?" But I don't answer.
-Moderator: You don't answer?
-Sometimes they send questionnaires, for instance, the doctor. They ask if I'm sick or if there are any other problems. But it's in difficult Swedish. I don't understand much.
-Difficult words.
-There might be three pages. We aren't able to understand three pages. It takes a long time to translate.
Many participants thought that translating questionnaires might increase survey participation:
Focus group 1.
-Yes, but do the questionnaire in several languages, and stuff. As we said before. It helps a lot, I think.
-And then you would involve different ethnicities in the questionnaires.
Because Sweden consists of lots of different ethnicities, and stuff, and it would be good to get everyone's view. People come from different places, so they also have experiences about things like, different politics. And they can also compare, so they will give you even more opinions. Or make you get more out of it.


S3. Confirmation Phase. Invitation Letters


S3.1. Translation of the Treatment Vignette Appearing within the Light-Blue Box of the invitation Letter.
In the preparation of previous citizen promises, 
the
 
Figure S1
. Letter without the information that tells participants that answering the questionnaire may increase their chance to influence a policy. 
Figure S2
. Letter with the information that tells the participants that answering the questionnaire may increase their chance to influence a policy.


S4. Confirmation Phase: Survey Experiment


Measures
Response. Participants who completed 80% or more of the applicable questions were coded as 1 (response) and 0 (nonresponse) otherwise. Response propensity was calculated using response rate 1 (RR1) according to the American Association for Public Opinion Research (AAPOR)'s definition (AAPOR 2016).
Nonresponse Bias. Nonresponse bias was assessed by estimating so-called "representativity indicators (R indicators)" 
(Schouten et al. 2009)
. The R indicators are an expression of the standard deviation (SD) of probabilities of responses of units in the population. The R indicators were estimated by fitting a logit regression predicting response with registry data on the participants' age group 
(16-24, 25-29, 30-39, 40-49, 50-59, 60-75,
 and 76 years or older), sex, and birth country (born in Sweden/not born in Sweden), and estimating Eq. 1 (Eq. 5 in 
Schouten et al. 2009, and unadjusted
  Non-differentiation (Straight-lining). Non-differentiation was estimated by identifying participants who gave the same response to all items in a grid-type question. A participant who gave the same response to all items was coded as 1; participants not giving the same response throughout were coded as 0. Participants who did not answer at least one of the items were coded as missing.
Covariates. Socially Disadvantaged Area. A variable called socially disadvantaged area was coded 1 for participants who were registered as living in zip codes 46160-46167, and 0 for participants who were registered as living in zip codes 46132, 46140-46144, and


46192.
Immigrant. A variable called immigrant was coded 1 for participants who were registered as not born in Sweden and 0 for participants who were registered as born in Sweden. 
Phase: Tables.   Table S3
. Ordinary least squares (OLS) regression results predicting response with treatment and covariates. 


S5. Confirmation
Subtheme 2. 3 .
3
Design of the Questionnaire. Participants also discussed technical aspects of the survey mattering for their not responding, such as the amount of information in the invitation, the length of the survey, and the design of the survey questions (e.g., openended questions vs. fixed response options). One prominent view was that a lengthy survey would negatively affect willingness to participate. Lastly, some participants said they preferred being offered response options over being asked open-ended questions since preset options would require less effort to answer. Subtheme 2.4. Framing and Style of the Survey. The participants discussed how the style of the text in a survey and the visual aspects affected their first impression of the survey.


The
Swedish Tax Authority drew a simple random sample of 400 individuals from seven zip codes selected to cover the socially disadvantaged areas in the city Trollhättan (zip codes 46160-46167), and a simple random sample of 400 individuals from seven not socially disadvantaged areas in Trollhättan (zip codes 46132, 46140-46144, and 46192). Any person aged 16 or older and living in one of the zip codes could be randomly selected. None of the sampled persons were excluded from the study. The Swedish Tax Authority is an inclusive sampling frame where anyone with a Swedish social security number and registered as living in Sweden is included. The register has been shown reliable with only minor estimated underand overcoverage (Statistics Sweden, 2020). Overcoverage (individuals who have moved from Sweden but have not notified any of the Swedish government agencies) has been identified as the primary source of error (Statistics Sweden, 2020). Statistics Sweden and the


Figure 1 .
1
Percentage of completed surveys, by treatment and type of area.


The manipulation confounded information about exerting influence over policy and informing that few in the neighborhood had previously participated in citizen promises. Informing about both simultaneously was done to increase the strength of the manipulation by explicating that the individual's participation would be especially likely to affect policy. Future research could investigate the impact of deconfounding the manipulation and whether the manipulations canceled each other out. (3) Distrust in the police may have influenced the believability of our manipulation. Although the collaboration with the police allowed testing the impact of telling respondents about their real chances to influence policy, distrust in the policy may have influenced the believability of our manipulation. Future research should test


Subtheme 1. 1 .
1
Opportunity to Exert an Influence -Responsiveness by Society.


-
Moderator: But do you trust the police and the social administration less, or why …? -Yeah, exactly. Or people have problems with them. People feel hatred. I mean real hatred toward the police. So, should they sit down and answer their questionnaires? It doesn't work like that. It won't work. -The police usually push people down in these areas, you understand? Areas like this, not just this area, many areas. Maybe they see someone with a bag and they think "Drug dealer," and they approach you and ask questions. It's happened to me several times. From nowhere they appear and say, "Your eyes are red. It looks like you're on drugs," or something like that. Then they take a test and it's negative. But still, they want a reason for … Main Theme 2: Characteristics of the surveyIn the second main theme, the characteristics of the survey were identified as influencing response propensity across four subthemes: (1) Financial incentives; (2) Method and recruitment strategy; (3) Design of the questionnaire; and (4) Framing and style of the questionnaire. Subtheme 2.1. Financial Incentives. Participants discussed the importance of direct rewards-such as money or gift cards-for participating in surveys: Focus group 8. -… as I said, incentives. That you get some kind of reward. Because I mean, usually … if we're talking about adolescents, there might be questions that they aren't that interested or involved in: Compared to adults who work and have children and a car and …, you know? So then you might have to give them something small. A scratcher lottery ticket or whatever.


Focus group 5 .-
5
It's not only about that. If you answer and see a change, you will do it without getting any reward, do you understand? If you see that something is really going to happen. Subtheme 2.2. Method and Recruitment Strategy. Moreover, participants tended to discuss the pros and cons of different ways of answering surveys. Whereas some participants


Focus group 6 .-
6
Sending it by mail is good too. Because then you have an opportunity to look at it and see what it's about. Focus group 8. Another central feature of this subtheme was that most participants appeared to prefer face-to-face interviews over other modes. The personal contact with the interviewer in faceto-face interviews seemed to be key: -I also think it's about what attitude you have as an interviewer. How you approach people. It might be much more convenient when you introduce yourself and tell where you come from. Who you are and what kind of questionnaire it is. Compared to answering a phone call. Then you don't see the person. You might be more comfortable if you see the person.


-
Moderator: Okay, did you get tired because it was too long, or because the questions were … -I didn't really get the questions so … -Moderator: Okay, so the questions weren't clear … -Yes, but when I'm at the center, I can answer this kind of question. -Okay, so it's more difficult if it's sent home? -Yes, it gets … I don't understand … Imagine you don't understand the question. It takes a long time to look it up. So, if you're at the center and they ask the same questions, then you can at least get help.Subtheme 2.3. Design of the Questionnaire. The participants discussed technical aspects of the survey that they said influenced their likelihood of responding. These included the amount of information in the questionnaire, the length of the survey, and the design of the survey questions (e.g., open-ended questions vs. fixed response options). One prominent view was that a lengthy survey would negatively affect willingness to participate. Focus group 2. -Look at how many pages this is [referring to a questionnaire from the SOM Institute]. -It's like a national university aptitude test. -Look at this! I can be honest: You would look at this. You'd open it. Then you'd see all this, and you'd just throw it out. -Moderator: You don't think that people would even look at the questions? -Well, you read the first sentence and then: "Okay, there are just questions like that." -Ten percent of people may read the whole thing. -If they are retired. -Yeah, exactly. Another aspect of this theme was the design of the questions. Some participants said they preferred being offered response options over being asked open-ended questions: Focus group 1. -I like this! It's checking-off-questions. -Yes! -It's really nice because it's fun to make a check, haha! Because if I see a lot of text I think: "I don't have the energy for this." -Or, like: "What does this mean?" And then you're supposed to write this much [participant demonstrates how much with her hands]. If I have to sit down and write, you probably won't get detailed answers. I would probably just write two words and then just: "Never mind, I can't cope." But to answer this, you have to read and think a little. Subtheme 2.4. Framing and Style of the Survey. The participants discussed how the style of the text in a survey and the visual aspects affected their first impression of the survey: For instance, it could make the survey look important, serious, boring, difficult, or


Finally
, participants elaborated on individual-level factors they thought might influence the likelihood of participating in surveys across three subthemes: (1) Interest, engagement, and perceived self-efficacy; (2) Language skills and understanding of questionnaires; and (3) Attitudes toward the sender.


3. 1 .
1
Interest, Engagement, and Perceived Self-Efficacy. Several participants believed that interest and engagement were key factors for participation. Interest and engagement often depended on whether the issue was perceived as important: Focus group 2. -If it's about yourself, then, of course, you're willing to join, you know what I mean? -Mm.


R indicators in the R script created by de Heij,Schouten, and Shlomo, 2015). In similarity to other studies on response propensity interventions (e.g.,
Oscarsson and Arkhede, 2019)
, the impact of the manipulation on measurement error wasassessed by estimating item nonresponse, length of responses to open-ended questions, and non-differentiation (straight-lining). Percentage of Questions Answered. The percentage of questions answered was estimated by calculating the proportion of questions a participant did not answer out of the total number of questions in the questionnaire (the questionnaire included no filter questions or skip logic, so everyone was eligible to answer all questions). All participants who started the questionnaire and answered at least one question were included in the measure. Participants who did not answer a single question were coded as missing. Characters Typed in Open-Ended Questions. The number of characters written in the open-ended questions was estimated for each participant as one of the data quality measurements. All participants who saw an open-ended question were included in the analyses; participants who saw an open-ended question but did not type any characters were coded as 0.


Table 1 .Main theme 1: Perceptions of societal factors SubthemeMain theme 2: Characteristics of the survey SubthemeMain theme 3: Individual-level factors Subtheme1.2. Trust and attitude toward the sender. Participants also discussed
1
Main themes and subthemes regarding factors that might influence the likelihood of participating in surveys. 2.1: Financial incentives Subtheme 2.2: Method and recruitment strategy Subtheme 2.3: Design of the questionnaire Subtheme 2.4: Framing and style of the survey 3.1: Survey self-efficacy, interest, and engagement Subtheme 3.2: Language skills Note: Themes are presented in no particular order.
1.1: Opportunity to exert influence -and responsiveness from society
Subtheme 1.2: Trust and attitude toward the sender


Table S1 .
S1
Demographics of the focus groups.
Age
Residential area
16-21 years
17 (52%)
Askim
1 (3%)
22-27 years
4 (12%)
Biskopsgården
21 (64%)
28-33 years
6 (18%)
Kortedala
2 (6%)
34-39 years
2 (6%)
Central Gothenburg
2 (6%)
40-60 years
1 (3%)
No answer
7 (21%)
60-70 years
2 (6%)
No answer
1 (3%)
Gender
Swedish citizenship
Male
18 (55%)
Yes
22 (67%)
Female
15 (45%)
No
9 (27%)
No answer
2 (6%)
Education
Born in …
Primary, <9 years
8 (24%)
Sweden, & both parents born in
Primary, 9-10 years
6 (18%)
Sweden
7 (21%)
Secondary, ≤2 years
2 (6%)
Secondary, 3 years
8 (24%)
Sweden, & both parents born in
Post-secondary, <3 years
2 (6%)
another European country
3 (9%)
Post-secondary, ≥3 years
6 (18%)
No answer
1 (3%)
Sweden, & mother born in
Sweden & father in another
European country
1 (3%)
Sweden, & mother not born in
Europe & father born in another
European country
1 (3%)
Another European country, &
both parents born in another
European country
5 (15%)
Country outside of Europe, &
both parents not born in Europe
12 (36%)
No answer
4 (12%)
Occupation
Marital status
Full-time employment
2 (6%)
Single and never been married
10 (30%)
Part-time employment
4 (12%)
In a relationship and never been
Own business
6 (18%)
married
4 (12%)
Unemployed
1 (3%)
Married
10 (30%)
Retired
1 (3%)
Divorced and single
4 (12%)
Student
14 (42%)
Widow/ widower and not in a new
Other
4 (12%)
relationship
2 (6%)
No answer
1 (3%)
No answer
3 (9%)
Note: Percentages are rounded to whole numbers.


Focus group 7.2. Trust and Attitudes toward the Sender.
So, there is social exclusion, right? As I said, when people notice that nothing happens … do you understand? It's completely pointless to answer a question if nothing happens about the issue. That's the reason. The groups also discussed whether it was possible to trust people and organizations and whether lack of trust was an obstacle to their participation in surveys. Participants who said they trusted people in general or that they trusted most authorities expressed that trust may be important for their willingness to participate in surveys as well as society in general: . I think … people are scared maybe. People are scared. And society's moral level has sunk. It's not like before. I've been living in Sweden for almost 30 years. When I came here, it was much better. People trusted each other more and became friends more easily. Now it's not the same.
to fool them
--Moderator: Okay, so you think that one of the reasons why people don't
answer is that they feel socially excluded?
-Yeah, nothing happens, you understand? It's mostly in disadvantaged areas
where people don't answer, right?
-Moderator: Yes.
-Yes, that's why … in disadvantaged areas, people want a change, maybe
people don't want it to be … -people want it to be better here, you know?
So, if you don't see a change where you live, then it's completely pointless
to answer questions.
-Yes, I agree completely.
-That's right.
Subtheme 1.Focus group 8.
-Older people are like children. Someone will call them and say, "We're
calling from the bank. Can you give me your account details? There's a
problem we have to solve." They're like children: They answer. It's easy


Table S2 .
S2
Opinions about different data collection methods.
Method
Positive
Negative
Mail
"Easy to get a picture of what
"It's a lot of work."
questionnaire
the survey is about."
"Elderly people are likely to
prefer mail surveys over
online surveys."


There are often difficult words in questionnaires. So, you may not understand all the words. I agree. The envelope looks better. It looks more serious. -It's credible. The envelope is the most secure. -I agree that it is more credible, but I think that one [the postcard] looks nicer. I mean, compared to a boring envelope. It's a little more color and a little … that can be a factor as well. But this looks for sure more … safe.
complicated words. Words that they might not understand. It will also
make them feel stupid.
Whereas many participants agreed that less formal language would encourage
participation, different views were expressed regarding the visual aspects and the degree of
formality in the survey. Some participants wanted more color, figures, slogans, and pictures,
whereas others thought the invitation and survey should look formal to make a serious
impression:
Focus group 1.
-Well, I think this one looks scary [participant is referring to an envelope].
It feels like: "I've received a letter from school and now my parents will
be super upset."
-Hahaha!
-And this one feels more like: "Oh, how fun! What is this?" [participant is
referring to an invitation that looks rather like a postcard]
-Yes exactly, it looks super personal.
-And that's what adolescents like. We get hooked on the sentence: "What
do you think?" [participant is referring to the headline of the invitation]
-This has got very formal language [participant is referring to the SOM -We like ourselves. Haha! We're selfish!
questionnaire]. Those who move to Sweden … I mean, I have friends from -Hahaha!
other countries. They have good Swedish skills. But they know a more Focus group 8.
everyday language, and then it might feel uncomfortable: To participate -Maybe it's more paper [participant is talking about the letter and
in an interview where they know that there will be a lot of formal and envelope, compared to the postcard]. So, it's not good for the environment.
But the envelope is more secure.
--


If I answer, like, if I give you my time … because often you don't get a reward. You said we'll get a gift card, but normally you get nothing for participating. So, you must give me a good reason for spending my time on this. For free. I really have to know how this will affect me or society or the world. How will my voice, my opinion, and my habitsthat you want to know about. How will they help to answer questions about this? You know what I mean? This must be stated clearly but briefly.


Table S3 .Table S5 .Table S6 .Table S7 .
S3S5S6S7
= 800. Entries are unstandardized regression coefficients. Standard errors in parentheses. p< .1; *p<.05; ***p<.001. Ordinary least squares (OLS) regression results predicting response with treatment and covariates. = 800. Entries are unstandardized regression coefficients. Standard errors in parentheses. p< .1; *p<.05; ***p<.001. Logistic regression results for the R-indicator estimation. = 800. Entries are logistic regression coefficients. Standard errors in parentheses. p< .1; *p<.05; **p<.01; ***p<.001. Ordinary least squares (OLS) regression results predicting the proportions of questions answered with treatment and covariates. The proportion of questions answered Note. N = 800. Entries are unstandardized regression coefficients. Standard errors in parentheses. *p<.05; ***p<.001. Ordinary least squares (OLS) regression results predicting the number of characters written with treatment and covariates. The number of characters written Note. N = 800. DV = dependent variable. Entries are unstandardized regression coefficients. Standard errors in parentheses. *p<.05; ***p<.001.
Completed the questionnaire Opportunity to exert an influence Opportunity to exert an influence, and socially disadvantaged area Opportunity to exert an influence, and immigrant status 0.03 (0.03) 0.07 (0.05) 0.04 (0.04) -0.10* (0.05) -0.08 + (0.05) -0.09 (0.07) -0.05 (0.07) 0.31*** (0.02) 0.36*** (0.03) 0.34*** (0.03) 0.00 0.03 0.01 Opportunity to exert an influence Opportunity to exert an influence, and socially disadvantaged area Opportunity to exert an influence, and immigrant status 0.13 (0.15) 0.27 (0.21) 0.18 (0.19) -0.49* (0.22) -0.39+ (0.23) -0.43 (0.31) -0.23 (0.32) -0.81*** (0.11) -0.56*** (0.15) -0.66*** (0.14) 0.00 0.02 0.01 Note: N Completed the questionnaire Predictors Manipulation Message about exerting influence Covariates Socially disadvantaged area Not born in Sweden Interaction Message about exerting influence * Socially disadvantaged area Message about exerting influence * Not born in Sweden Constant R 2 Predictors Manipulation Message about exerting influence Covariates Socially disadvantaged area Not born in Sweden Interaction Message about exerting influence * Socially disadvantaged area Message about exerting influence * Not born in Sweden Constant Pseudo-R 2 No message about exerting influence Message about exerting influence Individual characteristics Immigrant Not born in Sweden -0.38 (0.23) -0.81** (0.25) Sex Female -0.18 (0.22) -0.09 (0.22) Age 25 to 29 -0.23 (0.55) 0.50 (0.48) 30 to 39 0.38 (0.41) 0.26 (0.43) 40 to 49 0.62 (0.41) 1.69*** (0.43) 50 to 59 0.56 (0.41) 0.75+ (0.42) 60 to 75 0.77* (0.38) 1.19** (0.39) 76 or older 0.64 (0.46) 0.00 (0.50) Constant -1.04** (0.32) -1.08** (0.33) Pseudo-R 2 .02 .07 Note. N Predictors Opportunity to exert an influence Opportunity to exert an influence, and socially disadvantaged area Opportunity to exert an influence, and immigrant status Manipulation Message about exerting influence 0.00 (0.02) -0.01 (0.02) -0.02 (0.02) Covariates Socially disadvantaged area -0.06* (0.03) Not born in Sweden -0.06* (0.03) Interaction Message about exerting influence * Socially disadvantaged area 0.02 (0.04) Message about exerting influence * Not born in Sweden 0.06 (0.04) Constant 0.95*** (0.01) 0.97*** (0.02) 0.97*** (0.02) R 2 0.00 0.02 0.02 Predictors Opportunity to exert an influence Opportunity to exert an influence (excluding DV outlier) Opportunity to exert an influence, and socially disadvantaged area (excluding DV outliers) Opportunity to exert an influence, and immigrant status (excluding DV outliers) Manipulation Message about exerting influence -88.65* (44.72) -15.71 (20.12) 3.95 (25.92) -7.03 (24.15) Covariates Socially disadvantaged area 11.52 (29.76) Not born in Sweden -29.69 (30.35) Interaction Message about exerting influence * Socially disadvantaged area -54.85 (41.43) Message about Note: N Completed the questionnaire exerting influence -46.90
* Not born in
(43.30)
Sweden
Constant
370.33*** (32.52)
229.23*** (14.80)
224.12*** (19.82)
240.15*** (18.41)
R 2
0.01
0.00
0.01
0.03
+++








Supplementary Online Materials (SOM)
Ten Hypotheses Generated for Increasing Survey Response Propensity Among


Immigrants and Inhabitants of Socially Disadvantaged Areas


Index
Page 2. 
Table S1
. 
 










Standard Definitions: Final Dispositions of Case Codes and Outcome Rates for Surveys




Aapor








The American Association for Public Opinion Research






9th edition








Nonresponse in the American time use survey: Who is missing from the data and how much does it matter? The Public Opinion Quarterly




K
G
Abraham






A
Maitland






S
M
Bianchi








70














Multiple indicators in survey research: The concept sense of political efficacy




G
I
Balch








Political Methodology


















Seen but not heard -Ethnic minorities' views of primary health care interpreting provision: a focus group study




D
S
Barron






C
Holterman






P
Shipster






S
Batson






M
Alam








Primary Health Care Research & Development




11
















The Morris Hansen lecture. Hard-to-survey populations and the U.S. census: Making use of social marketing campaigns




N
Bates








Journal of Official Statistics




33
















Using a geographic segmentation to understand, predict, and plan for census and survey mail nonresponse




N
Bates






M
H
Mulry








Journal of Official Statistics




27
















Using thematic analysis in psychology




V
Braun






V
Clarke








Qualitative Research in Psychology




3
















A practical guide to focus-group research




R
L
Breen




10.1080/03098260600927575






Journal of Geography and Higher Education




30
















Responsive survey designs for reducing nonresponse bias




J
M
Brick






R
Tourangeau








Journal of Official Statistics




33
















Explaining rising nonresponse rates in cross-sectional surveys




J
M
Brick






D
Williams








The ANNALS of the American Academy of Political and Social Science




645
















Can targeted cover letters improve participation in health surveys? Results from a randomized controlled trial




A
I
Christensen






P
Lynn






J
S
Tolstrup




10.1186/s12874-019-0799-4








BMC Medical Research Methodology




19


1
















Nonresponse in cross-cultural and cross-national surveys




M
P
Couper






E
De Leeuw




J. Harkness, F. van de Vijver, & P. P. Mohler






Wiley












Selecting individual or group interviews




B
Crabtree






M
Yanoshik






W
Miller






P
Connor








Successful Focus Groups: Advancing the State of the Art




















10.4135/9781483349008






SAGE Publications, Inc












Towards Survey Response Rate Theories That No Longer Pass Each Other Like Strangers in the Night




D
A
Dillman








Understanding Survey Methodology: Sociological Theory and Applications


P. S. Brenner












1st ed.










10.1007/978-3-030-47256-6_2














Internet, phone, mail, and mixedmode surveys: the tailored design method




D
A
Dillman






J
D
Smyth






L
M
Cristian








John Wiley & Sons












Gruppintervjuer (Fokusgrupper)




M
Ekström








Metoder i kommunikationsvetenskap


Ekström, M., & Larsson, L.


Lund




Studentlitteratur














Nonresponse rates and nonresponse bias in household surveys




R
M
Groves








Public Opinion Quarterly




70
















Nonresponse in household interview surveys




R
M
Groves






M
P
Couper








Wiley












Leverage-Saliency Theory of Survey Participation -Description and an Illustration




R
M
Groves






E
Singer






A
Corning








Public Opinion Quarterly




64
















Responsive design for household surveys: Tools for actively controlling survey errors and costs




R
M
Groves






S
G
Heeringa








Journal of the Royal Statistical Society Series A (Statistics in Society




169
















Analysing group interaction in focus group research: Impact on content and the role of the moderator




M
Grønkjaer






T
Curtis






C
Crespigny






C
Delmar








Qualitative Studies




2
















Antal personer födda i Sverige eller utlandet i Norra och Södra Biskopsgården 2022




Göteborgs
Stad










Statistikdatabas Göteborgs Stad. Accessed on


















L
Japec






A
Ahtiainen






J
Hörngren






H
Lindén






L
Lyberg






P
Nilsson






Minska bortfallet. Statistiska centralbyrån. Accessed on
















The impact of face-to-face vs sequential mixed-mode designs on the possibility of nonresponse bias in surveys among non-western minorities in the Netherlands




J
Kappelhof








Journal of Official Statistics




31


1
















Gauging the impact of growing nonresponse on estimates from a national RDD telephone survey




S
Keeter






C
Kennedy






M
Dimock






J
Best






P
Craighill








Public Opinion Quarterly




70


5
















Selection error in single and mixed mode surveys of the Dutch general population




T
Klausch






J
Hox






B
Schouten








Journal of the Royal Statistical Society: Series A




178


4
















Mixed-mode: Past, present, and future




E
D
De Leeuw








Survey Research Methods




12


2
















Predicting Survey Nonresponse with Registry Data in Sweden between 1992 to 2022: Cohort Replacement or a Deteriorating Survey Climate




S
Lundmark






K
Backström




10.31234/osf.io/8xsyb






















D
Luzzo






P
Anthony






H
A
Katrice






A
Maureen






A
Bibby






E
A
Martinelli
Jr


















Effects of self-efficacy-enhancing interventions on the math/science self-efficacy and career interests, goals, and actions of career undecided college students






Journal of Counseling Psychology




46


233












Targeted Response Inducement Strategies on Longitudinal Surveys




P
Lynn








Improving Survey Methods: Lessons from Recent Research






















Routledge














The effectiveness of an advance notice letter on the recruitment of African Americans and whites for a mailed patient satisfaction survey




A
M
Nápoles-Springer






M
N
Fongwa






A
L
Stewart






G
Gildengorin






E
J
Pérez-Stable








Journal of Aging and Health




16
















When to use focus groups and why




D
Morgan






R
Krueger




10.4135/9781483349008








Successful Focus Groups: Advancing the State of the Art




SAGE Publications, Inc
















Does giving people their preferred survey mode actually increase survey participation rates? An experimental examination




K
Olson






J
D
Smyth






H
M
Wood








Public opinion quarterly




76


4
















Effects of conditional incentives on response rate, nonresponse bias and measurement error in a high response-rate context




H
Oscarsson






S
Arkhede




10.1093/ijpor/edz015






International Journal of Public Opinion Research




32
















Utsatta områden -polisens arbete




Polisen




















Paper, E-mail, or Both? Effects of Contact Mode on Participation in a Web Survey of Establishments




J
W
Sakshaug






B
Vicari






M
P
Couper




10.1177/0894439318805160








Social Science Computer Review




37


6
















Indicators for the representativeness of survey response




B
Schouten






F
Cobben






J
Bethlehem








Survey Methodology




35
















Approaches to recruiting "hard-to-reach" populations into research: A review of the literature




A
Shaghaghi






R
S
Bhopal






A
Sheikk








Health Promotion Perspectives




1
















Toward a benefit-cost theory of survey participation: Evidence, further tests, and implications




E
Singer








Journal of Official Statistics




27
















The use and effects of incentives in surveys




E
Singer






C
Ye








The ANNALS of the American Academy of Political and Social Science




645
















Tydlig skillnad bland unga i särskilt utsatta områden








Statistics Sweden


















Utrikes födda i Sverige. Accessed on








Statistics Sweden
















1E/InrUtrFoddaRegAlKon/table/tableViewLayout1












Surveying nonrespondents




I
A
Stoop








Field Methods




16


1
















Improving survey response: Lessons learned from the European Social Survey




I
A
Stoop






J
Billiet






A
Koch






R
Fitzgerald








John Wiley & Sons












Adaptive and responsive survey designs: a review and assessment




R
Tourangeau






J
Michael Brick






S
Lohr






J
Li




10.1111/rssa.12186








Journal of the Royal Statistical Society. Series A: Statistics in Society




180


1
















Challenges in recruiting hard-to-reach populations focusing on Latin American recent immigrants




M
Vahabi






S
Isaacs






M
Koc






C
Damba








International Journal of Human Rights in Healthcare




8














Who drops out and when? Predictors of nonresponse and loss to follow-up in a longitudinal cohort study among STI clinic visitors




D
A
Van Wees






C
Daas






M
E
Kretzchmar






J
C M
Heijne




10.1371/journal.pone.0218658






PLOS ONE




14
















Trends in US face-to-face household survey nonresponse and level of effort




D
Williams






M
Brick




10.1093/jssam/smx019






Journal of Survey Statistics and Methodology




6
















Putting the fear Back into fear appeals: The extended parallel process model




K
Witte








Communications Monographs




59
















Effective recruitment and retention of minority research respondents




A
Yancey






A
N
Ortega






S
K
Kumanyika








Annual Review of Public Health




27

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]