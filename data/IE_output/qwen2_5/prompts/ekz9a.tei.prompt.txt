You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Theory of Machine 2.0: Artificial Versus Artificial Intelligence
Much like believing that humans constitute creation's crowning glory, there is a persistent belief that we are distinct from machines (see also 
Castelo et al., 2019;
Haslam et al., 2008)
. Obviously, however, humans are also distinct from each other. Nevertheless, putting oneself in someone else's shoes colloquially means taking another person's perspective in an attempt to understand their thoughts and feelings in a certain situation. In philosophy and cognitive psychology, the classical Theory of Mind is concerned with people's inferences about the intentions and beliefs of other human beings 
(Cuzzolin et al., 2020;
Kim et al., 2021
; see also 
Premack & Woodruff, 1978)
. For instance, observing someone outside in the winter wearing shorts and shivering suggests to most people that the person is cold. Semantically and conceptually reminiscent of this notion, 
Logg's (2022)
 "Theory of Machine" provides a framework for describing and analyzing "people's lay theories about algorithmic judgment" (p. 352). It formally conceptualizes the idea of ascribing thought processes or mental states to algorithms (i.e., rules for mathematical computation) used in decision-support systems and artificial intelligence (AI) agents.
Theory of Machine distinguishes between people's perceptions and expectations of "how algorithmic and human judgment (at their finest) differ in their input (the information used), process (how the same information is assessed), and output (predictions, advice, and feedback that are produced)" 
(Logg, 2022, p. 360)
. Essentially, however, ascribing thought processes and mental states to algorithms does neither require nor guarantee that people attribute human-like qualities to algorithms or anthropomorphize machines. Instead, the aforementioned persistent belief that humans are distinct from other species, including machines, implies that people may conceive of humans and algorithms as functionally distinct ontological entities (cf. 
Banks, 2020, for robots;
Nass et al., 1994, for computers;
and Naquin & Kurtzberg, 2004
, for technology more broadly). Consequently, a minimal understanding of the algorithm's operational mechanics-or at the very least striving for it-should be sufficient to guide people's perspective-taking in their interactions with AI. This understanding mainly results from people's own observations of a system's output, less often its input, and only rarely its processes 
(Diakopoulos, 2015)
, and therefore need not be adequate or consistent with an algorithm's actual programming. 
Logg (2022)
 argues that there are fundamental differences between artificial and human intelligence. Indeed, many cognitions that are nearly automatic, or at least performed without much effort by humans, are particularly difficult and problematic for algorithms 
(Fazi, 2019
(Fazi, , 2021
. For instance, algorithms are relatively poor at extrapolating beyond the training data, identifying causal relationships, providing explanations, relying on common sense, or being creative. Moreover, algorithms cannot evaluate what they do not know, and thus lack the ability to judge how feasible or plausible something is 
(Ignizio & Soltys, 1996)
. Crucially, algorithms are incapable of moral reasoning and ethical framing 
(Kraemer et al., 2011)
 and do not engage in "hot cognition" (i.e., emotional thinking; 
Cuzzolin et al., 2020)
. In general, algorithms are rather slow to adapt to changing environmental conditions and often need to be retrained, for instance, to keep their knowledge data base up-to-date or to account for deviations from normality (cf.


Differences Between Artificial and Human Intelligence


Dudhwala & Björklund Larsen, 2019).
While algorithms have not yet and probably never will reach human intelligence in these areas, they do exhibit other, functionally different types of intelligence that are unattainable by humans, at least at comparable levels of performance 
(Fazi, 2019
(Fazi, , 2021
Rahwan et al., 2019)
. In fact, algorithms achieve "superhuman performance" 
(Cuzzolin et al., 2020)
 in memorizing and keeping track of massive-compared to humans, almost infinite-amounts of data 
(Yampolskiy, 2020)
. For this reason, they are much better at recognizing (correlational) patterns among variables and instances. Moreover, algorithms can process data and perform complex calculations much faster, more efficiently, and with greater consistency and accuracy than humans 
(Korteling et al., 2021)
. In terms of efficiency, algorithms do not require regular breaks, do not bother to solve tedious and repetitive tasks, and do not demand financial compensation for their work. Although, admittedly, the development and operation of scalable and sophisticated algorithms is often quite costly (e.g., in terms of energy consumption; 
Luccioni et al., 2024)
. Humans generally better than AI 
(Fazi, 2019
(Fazi, , 2021
 Some algorithms better than others, e.g.: -Extrapolation beyond training data: Linear beats polynomial regression 
(Pargent et al., 2023
) -Natural language and reasoning:
Data-driven learning has replaced rule-based systems 
(Chernyavskiy et al., 2021;
Johri et al., 2021)
 Judgments of Knowledge, Feasibility, and Plausibility More difficult for AI than humans 
(Ignizio & Soltys, 1996)
 Active development of metacognitive AI 
(Kawato & Cortese, 2021
; see also 
Scholten et al., 2024)
 -Improvements through search engine integration 
(Press et al., 2023
) and multi-model collaboration 
(Borges & Meyer, 2008;
Feng et al., 2024)
 Morality, Ethics, and Emotions AI, but not humans, generally incapable of morality and ethics 
(Kraemer et al., 2011)
 and emotional thinking 
(Cuzzolin et al., 2020)
 Active development of moral/ethical AI 
(Jiang et al., 2022)
 and emotional AI 
(Castelo et al., 2019)
 Reactivity to Environmental Shocks
Humans can rely on tacit knowledge, experiences and intuition, whereas AI usually requires manual adaptation (cf. 
Dudhwala & Björklund Larsen, 2019)
 Increased flexibility for AI incorporating and acting on externally provided input without retraining -Conversational user interfaces allow humans-in-the-loop to provide additional information (e.g., muti-shot prompting; Brown et al., 2020) -Self-asking plus internet access enables generative AI to retrieve upto-date information 
(Press et al., 2023)
 Memory Constraints AI generally better than humans 
(Cuzzolin et al., 2020;
Yampolskiy, 2020)
 Various hardware (e.g., disk or cloud storage) and software (e.g., context window sizes in generative AI; 
Liu et al., 2024)
 constraints


Computational Efficiency and Cost
AI typically faster as well as more efficient, consistent, and accurate than humans 
(Korteling et al., 2021)
, but development and deployment of AI often quite costly (e.g., 
Luccioni et al., 2024)
 Indirect costs of more powerful models, such as computation time, amount of labeled training data required, etc. 
(Lim et al., 2000)
 Historically Human-Centric Behavioral Research Despite these fundamental differences between human and algorithmic cognition, which are summarized in the second column of 
Table 1
, the behavioral scientific literature is predominantly concerned with differences in human judgment and decision-making (JDM) processes when interacting with algorithms versus other humans (see 
Burton et al., 2020;
Jussupow et al., 2020;
Mahmud et al., 2022
, for reviews; but see also 
Hou & Jung, 2021;
Niszczota & Abbas, 2023;
Poursabzi-Sangdeh et al., 2021;
Rebholz et al.,
 in press, for recent exceptions). Presumably, this is due to the fact that most research on augmented JDM is conducted by scholars with a background and primary interest in human behavior. For instance, "algorithm aversion" was originally defined as discounting the advice of an erroneous algorithm more strongly than the same advice from erroneous humans 
(Dietvorst et al., 2015
(Dietvorst et al., , 2018
. It was later defined as a general preference for humans (including oneself) over algorithms 
(Mahmud et al., 2022;
Prahl & van Swol, 2017)
. Indeed, since the publication of 
Meehl's (1954)
 seminal work on clinical versus statistical prediction, it has been common sense in psychology that people prefer the judgments of human experts, despite the accuracy advantages of "actuarial" (i.e., algorithmic) methods (see also 
Dawes, 1979;
Dawes et al., 1989)
.
More recently, it has also been shown that people integrate algorithmic advice more than quantitatively equivalent human judgments in a variety of tasks 
(Logg et al., 2019)
. Essentially, this "algorithm appreciation" was originally defined to also include cases in which users' discount their own initial judgments more strongly than the algorithmic advice they receive. This is the opposite of the egocentrism that has been observed in traditional research on advice taking for more than two decades. 1 On average, judges often prefer their own judgment to that of their human advisors 
(Harvey & Fischer, 1997;
Yaniv & Kleinberger, 2000)
. First, however, this egocentric discounting has been shown to merely constitute an analytic artifact. Focusing on aggregated data masks a variety of distinct advice weighting strategies, some of which are not egocentric (e.g., equal weighting of both sources or complete adoption of the advice; 
Soll & Larrick, 2009)
. Second, and more importantly, egocentrism does not apply equally to algorithmic advisors in augmented JDM scenarios. An intriguing example are feedback loops, which challenge the idea of users being privy to their own thoughts as an explanation for egocentric 
1
 Advice taking research is primarily concerned with human interactions in the judge-advisor system, where one person serves as an advisor and provides additional information, data, or recommendations, and the other person, the judge, is responsible for making the final judgment or decision 
(Sniezek & Buckley, 1995
; see also 
Bonaccio & Dalal, 2006;
Kämmer et al., 2023;
Rader et al., 2017, for reviews)
. However, this traditional version of the paradigm has also recently been used to study algorithmic advice taking (e.g., 
Logg et al., 2019;
Niszczota & Abbas, 2023;
Önkal et al., 2009;
Prahl & van Swol, 2017)
.
biases 
(Yaniv, 2004a
(Yaniv, , 2004b
. The more data about its users' past behavior an algorithm has access to, which often depends on the privacy policies implemented, the more sophisticated the user representations in terms of preferences and decision strategies that can be inferred 
(Domingos, 2015;
Jordan & Mitchell, 2015
; but see 
Chaney et al., 2018
, for negative consequences). Given this, the definition of egocentrism should imply a strong preference for advice from personalized over non-personalized recommender and decision-support systems.
Most augmented JDM studies have implemented comparisons of human judgments with the output of virtually any kind of algorithm (e.g., the true value ± X; Hütter & Fiedler, 2019, aggregated judgments of multiple other participants; 
Logg et al., 2019, or linear cue-weighting models;
Dietvorst et al., 2015)
. This is particularly eminent in the measurement of algorithm aversion, which is either based on surveying people's preferences for humans versus algorithms (e.g., 
Araujo et al., 2020;
Lennartz et al., 2021;
Thurman et al., 2019)
, or operationalized as one of the following: the tendency to choose human over algorithmic judgments (e.g., 
Castelo et al., 2019;
Dietvorst et al., 2015;
Logg et al., 2019)
, stronger judgmental shifts in the direction of human rather than algorithmic advice (e.g., 
Logg et al., 2019;
Önkal et al., 2009;
Prahl & van Swol, 2017)
, or more emotional and sentimental responses in interactions with erroneous algorithms as compared to erroneous humans (e.g., 
Renier et al., 2021
; see also Lee, 2018, for task-dependent emotional reactions). By selectively comparing human judgments to specific algorithms that may only be appropriate for the specific use case under investigation, previous studies have not explicitly distinguished between the different types of intelligence that imply divergent JDM processes between humans and algorithms 
(Fazi, 2019;
Rahwan et al., 2019)
.
Therefore, most of the standing conclusions regarding differences in the utilization of human versus algorithmic output inevitably reflect differences in research designs as well.


Algorithmic Cognitive Idiosyncrasies
Of course, it is important to study how humans deal with imperfect human output compared to-ideally, but not necessarily empirically (e.g., 
Lambrecht & Tucker, 2019;
Renier et al., 2021)
-less imperfect algorithmic output, as is done in large parts of the augmented JDM literature 
(Burton et al., 2020;
Jussupow et al., 2020;
Mahmud et al., 2022)
. For some tasks, however, modern algorithms achieve performance that is unlikely ever to be achieved by humans 
(Cuzzolin et al., 2020)
. Indeed, one of the core objectives of the computer science and machine learning disciplines, at least historically, has been to design efficient and accurate algorithms for automatically solving well-defined tasks, such as recognizing patterns in data 
(Domingos, 2015;
Jordan & Mitchell, 2015;
Rahwan et al., 2019)
. Consequently, different algorithms will inevitably differ in their respective artificial 'intelligences' to solve specific tasks. Following this line of reasoning, it is equally important to also study how the output of different types of algorithms implementing different operational mechanics is processed or integrated by humans.


Differences Between Various Artificial Intelligences
Some algorithms are better than others at extrapolating beyond the training data, identifying causal relationships, providing explanations, relying on common sense, or being creative. For instance, linear regression is limited to straight-line extrapolation, whereas polynomial regression can model more complex relationships. Essentially, however, linear regression is often preferred in prediction tasks for its reduced risk of overfitting the training data (e.g., 
Pargent et al., 2023
; see also 
Karelaia & Hogarth, 2008
, for a meta-analysis of human judgment strategies). Generative AI models are a notable example of addressing even more of these factors. For instance, adversarial neural networks can generate creative artwork by deliberately deviating from established styles 
(Elgammal et al., 2017
; but see 
Berryman, 2024)
, or large language models (LLMs), such as OpenAI's ChatGPT or Google's Bard, can provide output along with self-generated explanations for it. Essentially, users rely more on explained than unexplained LLM-generated output, despite the questionable validity of the accompanying explanations 
(Rebholz et al., in press)
. In principle, their ability to generate novel text by means of probabilistic token forecasts 
(Vaswani et al., 2017)
 makes LLMs capable of moral reasoning or ethical framing, which used to be an extensive area of research in AI prior to the emergence and popularity of transformer LLMs (e.g., 
Castelo et al., 2019;
Jiang et al., 2022)
.
Although the likelihood of an LLM responding to a user query with "I don't know" is generally low 
(Feng et al., 2024;
Scholten et al., 2024)
, generative AI models can also act on, and in some sense even evaluate, their own knowledge and capabilities. For instance, ChatGPT often responds with "I am sorry, but as a large language model, I cannot ..." or indicates the limits of its training database with statements like "The knowledge cut-off date for my training data is ...". Thus, LLMs are also capable of making judgments about how feasible or plausible something is. Essentially, newer generations are equipped with internet access, such as OpenAI's GPT-4 and Google's Gemini, which alleviates the cut-off date problem and further enhances the ability of such contemporary LLMs to assess their lack of knowledge by retrieving current information 
(Press et al., 2023)
. Alternatively, conversational user interfaces allow relevant context to be provided directly to the AI through prompting 
(Brown et al., 2020)
. In general, AI systems that allow for humans-in-the-loop implicitly provide means for dynamic adaptation to changing environmental conditions, even without the need for active retraining.
Whether algorithms achieve superhuman performance in memorizing and keeping track of massive amounts of data depends on many technical aspects and programming factors. For instance, because the computational cost of the attention mechanism of transformer LLMs grows quadratically in context length, different generative AI implementations vary in their context window size 
(Ratner et al., 2022)
. This parameter essentially determines the ability of various chatbots to handle extensive prompts, large datasets, and long conversation histories. Although the attention mechanism is technically insensitive to token position, the performance of contemporary LLMs has been found to degrade when identifying relevant information in the middle of prompts 
(Liu et al., 2024)
. The LLM getting "lost in the middle" (p. 157) resembles the serial positioning effects observed in human free recall 
(Murdock, 1962)
. Accordingly, this phenomenon may result in reduced coherence and relevance of the generated output, highlighting the importance of context management in generative AI performance.
Processing data and performing complex computations much faster, more efficiently, and with greater consistency and accuracy than humans may not always be the ultimate training goal.
For instance, the "sycophancy" problem describes the trade-off between generating satisfactory responses 
(Sharma et al., 2023)
 and avoiding fabricating facts, so-called "hallucinations" 
(Azamfirei et al., 2023)
. Rather than restricting the text generation to the most likely sequence of tokens, ChatGPT's "temperature" parameter allows developers to control the balance between creativity and consistency (OpenAI, n.d.). Changing this value affects the likelihood of generating the same response to the same prompt, and thus its performance at solving repetitive tasks. Furthermore, algorithms do at least indirectly demand financial compensation for their work and sometimes even pause (e.g., shutdowns due to immense resource consumption, as seen with ChatGPT's "rate limits"). The training process of sophisticated machine learning models is usually quite complex and time-consuming 
(Lim et al., 2000)
, which indirectly imposes speed limits and resource constraints. In addition, there are non-negligible costs associated with expertlabeled training data for classical supervised learning algorithms (e.g., 
Asheghi et al., 2016)
, obtaining human feedback for reinforcement learning 
(Christiano et al., 2017)
, and "pay per token" pricing models for LLM-generated output (e.g., OpenAI, n.d.; but see also 
Erion et al., 2022
, for a "cost-aware AI" developed for the medical sector).


A Refined and Extended Theory of Machine 2.0
In their daily lives, people encounter various decision-support systems in different situations, which compute and communicate their output in highly idiosyncratic ways and exhibit different levels of transparency, accountability, and fairness 
(Ouchchy et al., 2020;
Rahwan et al., 2019;
Zarsky, 2016)
. As the use of AI in everyday life becomes more and more common, people inevitably draw their own conclusions from their personal experiences with the capabilities and limitations of particular algorithms they work and live with 
(Dudhwala &
 Björklund Larsen, 2019)-a process called "reverse engineering" 
(Diakopoulos, 2015)
. A fascinating example of this is the case of Microsoft's chatbot Tay, which launched on Twitter in 2016 and was designed to learn from its interactions with users. Within hours, users manipulated Tay by bombarding it with offensive and inappropriate messages, causing it to mimic and propagate harmful content 
(Neff & Nagy, 2016)
. This incident not only exposed the vulnerabilities in AI-based systems, but also demonstrated how users could successfully reverseengineer and exploit an algorithm's learning process for malicious purposes.
Another example of this kind involves ride-sharing platforms, where both passengers and drivers have reverse-engineered the dynamic surge pricing algorithm. While Uber passengers attempt to lower fare prices by strategically manipulating their pickup location, Uber drivers create an artificial scarcity of available rides by colluding to log out and trigger the algorithm to increase fare prices 
(Rosenblat & Stark, 2016)
. Once the prices surge, drivers log back in to accept the rides at higher rates. This type of reverse-engineering not only demonstrates a deep understanding of the algorithm's operational mechanics, but also raises important questions about the fairness and transparency of such systems. Users can apply different strategies to manipulate prices and availability on competing platforms like Uber and Lyft, each of which uses its own dynamic surge pricing algorithm. In other words, people must navigate and adapt to the idiosyncratic behaviors of different AI systems designed to solve similar tasks.
In addition to personal experiences, the popularity of AI-related topics has made people increasingly aware of transparency issues and other distinguishing characteristics of certain AI systems through public media 
(Diakopoulos, 2015;
Ouchchy et al., 2020)
. Indeed, the results of 
Ouchchy et al. (2020)
 show that the most frequently discussed topics are related to issues concerning the negative consequences of relying on AI, such as lack of control (i.e., related to transparency and interpretability) or equity and exaggerated prejudices (i.e., essentially fairness concerns). A notable example is the widespread media coverage of racial or gender bias in facial recognition technology, leading to wrongful arrests and misidentifications. The case of Robert Williams was covered extensively by major news outlets such as The New York Times 
(Hill, 2020)
. According to The Guardian, Williams was the first African American person to be wrongfully arrested due to a false match by facial recognition software 
(Bhuiyan, 2023)
. Stories like these not only draw attention to the technical flaws and ethical implications of certain AI systems, but also spark discussions about how specific design choices and training data affect the fairness and reliability of such technology. Essentially, there was a shift in tone from enthusiastic to critical in non-balanced media coverage of AI implementations between 2014 and 2018 
(Ouchchy et al., 2020)
.
In summary, people encounter diverse instances of algorithmic JDM in their daily lives, which contributes to their understanding of the underlying processes. Crucially, future research in augmented JDM should thus prioritize exploring people's perceptions of how and why the behavior and cognitive abilities of various AI systems differ from one another. In other words, an algorithm-centric perspective on augmented JDM should be adopted, focusing on conducting more systematic investigations of algorithm-algorithm comparisons. Accordingly, the next section proposes several new research questions aimed at providing insights into a refined 2.0 version of individuals' Theory of Machine, extended by explicitly considering a more nuanced human understanding of algorithmic cognitive idiosyncrasies.


Suggestions for Algorithm-Centric Research Agendas


Accountability and Social Norms


How does the perception of accountability for algorithmic output affect people's inferences about, for instance, positive versus negative intentions of various algorithms?
Some scholars argue that developers are (morally) responsible for the behavior of their systems 
(Kraemer et al., 2011;
Rahwan et al., 2019)
, and that algorithms inherently convey the intentions of their creators 
(Diakopoulos, 2015)
. Indeed, evidence from studies implementing the traditional human-algorithm comparison suggests that users tend to blame the humans in the loop (e.g., the developer or company in charge) for algorithmic errors 
(Leo & Huh, 2020;
Renier et al., 2021)
. A recent survey study on the fairness of automated JDM provides qualitative evidence that the same is likely true for the valence of intentions of AI systems 
(Helberger et al., 2020)
.
The most likely psychological mechanism may be that algorithmic JDM elicits fewer counterfactual thoughts about alternative, potentially better states of the world 
(Naquin & Kurtzberg, 2004)
. With the rise of generative AI, however, this seems to be challenged by the fact that developers are not able to fully control the output of these systems by design.
Accordingly, these systems can be exploited and manipulated, for instance, by experimenting with similar prompts and selecting the most preferred output.


What is the influence of social norms on interactions with algorithms that operate at different levels of prosociality?
In repeated interactions with other humans, people reward reciprocal behavior and punish nonreciprocal behavior 
(Rabin, 1993)
. We know that people also act on social norms such as reciprocity, empathy, and altruism in human-computer interactions (see 
Nielsen et al., 2022
, for a review). Indeed, in strategic settings, human players are likely to punish a non-reciprocal algorithm (e.g., rejecting as responders the proposals they offered as proposers in ultimatum games with role reversal; Costa-Gomes et al., 2019) more than a reciprocal one (cf. 
Fogg & Nass, 1997
, for a single repetition). This behavior may occur despite users' awareness that algorithms do not care about being rewarded with, for instance, monetary compensation 
(Nass et al., 1994)
 or social benefits such as relational closeness (cf. 
Biella et al., 2023)
. In turn, users can take advantage of altruistic and empathic AI systems by, for instance, pretending to have certain feelings or emotions in order to elicit the desired behavior from an empathic algorithm that is programmed with the primary goal of improving its users' emotional states 
(Zaki, 2017)
.
Because conceptions of fairness are generally shaped by cultural norms and values 
(Shen et al., 2011)
, the best approach to studying how users respond to different types of prosocial algorithmic behavior is from an intercultural perspective.


Explainable Versus Self-Explanatory AI


How does individuals' socioeconomic background shape their Theory of Machine, and thus affect their utilization of algorithmic output provided along with explanations of varying degrees of mathematical and logical complexity?
In the field of explainable AI (XAI), there are huge efforts to develop (complex) mechanisms to generate (often still quite complex) explanations for certain behaviors of (complex) algorithms 
(Pazzani et al., 2001;
Poursabzi-Sangdeh et al., 2021;
Rahwan et al., 2019;
 see also Barredo-Arrieta et al., 2020, for a comprehensive taxonomy). Mathematical skills facilitate users' understanding of such explanations, but depend on the level of education (e.g., 
Zacharopoulos et al., 2021)
, which in turn is usually a matter of economic affordability 
(Ware, 2019)
. In addition to numeracy 
(Logg et al., 2019)
, education 
(Thurman et al., 2019)
, and personal experience 
(Diakopoulos, 2015;
Dudhwala & Björklund Larsen, 2019)
, users' perceptions of AI are also shaped by the debate about the varying degrees of explainability of AI in the public media 
(Ouchchy et al., 2020)
. Interestingly, empirical evidence from a large-scale comparison of different XAI methods suggests that people do not value comparatively simple confidence scores over more complex means of explanation 
(Silva et al., 2023)
.


Does the persuasiveness of algorithms suffer from the fact that they have no real insight into their operational mechanics, and thus no basis for truthful contemplation of their own behavior?
Logg (2022) argues that algorithms tend to be less persuasive than humans because they lack the ability to provide (personalized) explanations for their behavior (see also 
Sovrano & Vitali, 2022;
Yampolskiy, 2020)
. However, contemporary versions of LLMs at least pretend to be able to describe and even have control over their behavior. Thus, this distinction between algorithmic and human levels of persuasion by explanation is challenged by the emergence of conversational user interfaces that facilitate natural conversations with AI systems. Indeed, there is empirical evidence for a positive effect of natural language explanations provided by selfexplanatory generative AI on users' willingness to integrate explained over unexplained algorithmic output 
(Rebholz et al., in press
). However, even internet access does not provide conversational agents with real insight into their input, processes, and output, or the tools to deliberately influence these factors 
(Yampolskiy, 2020;
cf. Ignizio & Soltys, 1996, on unjustified automatic classifications)
. This lack of control implies a high risk for generative AI to hallucinate factually inconsistent and erroneous information with high confidence 
(Azamfirei et al., 2023)
.


Other Humans-in-the-Loop and Theory of Mind


Does the social environment, specifically the peer-to-peer influence exerted by an individual's social network, influence users' personal experiences and interactions with various AI systems?
In general, algorithms interact with masses of users simultaneously and, especially with increasing levels of personalization, also in very idiosyncratic ways 
(Domingos, 2015;
Jordan & Mitchell, 2015)
. Therefore, talking to other people about their specific personal experiences in interacting with various algorithms should have a corresponding impact on individuals' own Theory of Machine. For example, hearing from a peer that he or she received advertisements for certain products on a particular social media platform after previously searching for related things online should increase one's suspicion of being constantly monitored across platforms. In fact, empirical evidence suggests that prevention focus is positively related to increased suspicion and consequently makes people more vigilant against persuasive advertising 
(Kirmani & Zhu, 2007)
. Peer reports that elicit surveillance impressions should thus increase the likelihood of detecting and resisting algorithmic attempts to induce purchase intentions in similar situations.


Are Theory of Mind and Theory of Machine conceptually related, and if so, does this relationship vary across individuals?
A positive correlation between Theory of Mind and Theory of Machine would indicate similarity between the two concepts (cf. 
Banks, 2020)
. More importantly, varying correlations across individuals would suggest inter-individual differences in conceptual similarity, with a relatively high correlation indicating more human-centric inferences for algorithmic JDM. 2
Consequently, attempts to test for causality could make use of interventions that target individuals' Theory of Mind. For example, 
Cavallini et al. (2015)
 training programs aimed at enhancing Theory of Mind, such as social cognitive training and visual and conceptual perspective-taking exercises, were demonstrated to improve individuals' ability to infer the mental states of others. These interventions should not only enhance human-human interactions, but could also improve human-AI interactions by enabling individuals-particularly those for whom mentalizing about humans and AIs is highly correlated-to be more confident in predicting and understanding algorithmic behavior. A related follow-up question is whether the integration of output from algorithms can be improved by specifically designing algorithms to behave in a more human-like manner, making their JDM more easily predictable by Theory of Mind (e.g., 
Cominelli et al., 2018;
Cuzzolin et al., 2020;
Williams et al., 2022)
.


Limitations of Theory of Machine 2.0
A holistic algorithm-centric research approach requires the incorporation of algorithms' theories of the human mind 
(Hoffman et al., 2018)
. In the words of 
Cuzzolin et al. (2020)
, the concept of social cognition "needs to be extended to how humans deal with intelligent machines (e.g. an airline's conversational bot) and vice-versa" (p. 1057). Within the limits of introspection (e.g., 
Pronin, 2009)
, rule-based expert systems by implication 'think' like their human templates 
(Barredo-Arrieta et al., 2020)
. On the one hand, the scope and amount of manual programming required often limits the applicability of such systems. In contrast, data-driven learning techniques infer the rules of the game from observed instances in large amounts of data. On the other hand, however, processing differences imply that the internal representations of inferred rules are rather concrete, whereas human inferences from the same observed data are generally more abstract. Thus, socially intelligent AI should ideally internalize meaningful information from the social environment "to reason on it at a higher level, build its own opinions independently, and then automatically bias the decision-making according to its unique experience" 
(Cominelli et al., 2018, p. 1)
. In summary, algorithms necessarily have their own, very specific, potentially complex, and often rather opaque theories of the human mind.
People's mentalizing about and perspective-taking regarding other humans' behavior depends on social distance and group membership 
(Kim et al., 2021)
. Thus, systematic investigations of a more fine-grained Theory of Machine 2.0 must also acknowledge people's potential awareness that personalized algorithmic output constitutes a reflection of their own and similar others' past behavior 
(Araujo et al., 2020)
. Again, this potential awareness does not require a very deep understanding of any technical aspects of the algorithm's operational mechanics, but may simply be based on personal experience from past interactions with the same or similar algorithms. In less technical terms, the impression that some AI systems know a user better than other systems may be justified under certain circumstances. Corresponding algorithmic behavior is human-like-even (past) self-like-by implication, which renders Theory of Mind a reasonable conceptual framework that can help users understand and predict such AI systems' behavior.
As argued at the beginning of this article, different types, latitudes, and limits of intelligence imply a divergence between human and algorithmic JDM processes, rendering them functionally distinct ontological entities in terms of capabilities and behaviors 
(Fazi, 2019;
Rahwan et al., 2019)
. Inevitably, this also suggests that the often highly complex procedures involved in algorithmic JDM are intractable to the human mind (cf. 
Yeomans et al., 2019
; see also 
Gillespie, 2006)
. In fact, most algorithms are black boxes, even to their developers. Therefore, it is actually a quite reasonable heuristic to simply compare algorithmic behavior to well-known-or at least much more extensively studied from past personal experience-human behavior in an attempt to understand and predict the output of AI systems 
(Banks, 2020;
Nass et al., 1994)
.


Toward a Unified Theory of Human and Artificial Minds
Previous research on augmented JDM, including but not limited to the original version of the Theory of Machine from Logg (2022), has implemented a restrictive focus on humanalgorithm comparisons, thereby neglecting differences between various algorithms. Instead, people may conceive of humans and algorithms as functionally distinct ontological entities, resulting from their awareness of fundamental differences between artificial and human intelligence. People's (aspirational) lay understanding of the operational mechanics of algorithms is gained, for example, through personal and communicated experiences with various AI and decision-support systems, or through public media coverage. The delineated refinement, or rather extension, of the Theory of Machine framework should thus be understood as a proposal to recognize these circumstances by conducting more algorithm-centric JDM research in the future.
To this end, several research questions are proposed that act on this proposal, taking into account differences in algorithmic accountability and explainability, or the role of social norms and environments in human-computer interactions. Of course, this is not meant to be an exhaustive list of examples of important design factors that determine people's Theory of Machine 2.0, that is, their inferences about the behavior of various algorithms with which they interact. However, a systematic examination of these factors is a first step toward potential improvements in augmented JDM.
As people's interactions with other humans become increasingly digitized, the physical and temporal space between human interaction partners is expanding 
(Mervyn & Allen, 2012;
 see also 
Domingos, 2015)
. At the same time, the ability of contemporary generative AI systems to engage in natural conversations, such as OpenAI's ChatGPT or Google's Gemini, makes them increasingly difficult to debunk. In other words, the distinction between human and algorithmic interaction partners is deemed to become increasingly difficult in the future. Therefore, inferences for algorithmic JDM based on users' personal past experiences with other humans can be considered adaptive. In other words, rapid technological innovation implies that the distinction between the Theory of Machine and Theory of Mind frameworks may eventually become redundant. Consequently, the formulation of a unified theory of human and artificial minds may be indispensable in the future.
Table 1 :
1
Differences Between Artificial and Human as well as Artificial and Artificial Intelligence for
Different Tasks, Phenomena, and Challenges
Task/Challenge
Human vs. AI
AI vs. AI
Extrapolation,
Causality,
Explanations,
Common Sense,
and Creativity


More mechanistic inferences about human behavior (i.e., a negative correlation) are probably rather unlikely because of people's awareness of the serious limitations of the human mind compared to algorithms (e.g., in terms of memory or computational power), as discussed above.


& HumanValues, 41(1), 118-132. https://doi.org/10.1177/0162243915605575














In AI we trust? Perceptions about automated decision-making by artificial intelligence




T
Araujo






N
Helberger






S
Kruikemeier






C
H
Vreese




10.1007/s00146-019-00931-w








AI & Society


35














Crowdsourcing for web genre annotation. Language Resources and Evaluation




N
R
Asheghi






S
Sharoff






K
Markert




10.1007/s10579-015-9331-6








50














Large language models and the perils of their hallucinations




R
Azamfirei






S
R
Kudchadkar






J
Fackler




10.1186/s13054-023-04393-x








Critical Care




27


1
















Theory of mind in social robots: Replication of five established human tests




J
Banks




10.1007/s12369-019-00588-x








International Journal of Social Robotics




12


2




















A
Barredo-Arrieta






N
Díaz-Rodríguez






J
Del Ser






A
Bennetot






S
Tabik






A
Barbado






S
Garcia






S
Gil-Lopez






D
Molina






R
Benjamins






R
Chatila






F
Herrera


















Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion




58
















10.1016/j.inffus.2019.12.012














Creativity and style in GAN and AI art: Some art-historical reflections




J
Berryman




10.1007/s13347-024-00746-8








Philosophy & Technology




37


2


61














First man wrongfully arrested because of facial recognition testifies as California weighs new bills. The Guardian




J
Bhuiyan




















The interaction game: A reciprocity-based minimal paradigm for the induction of social distance




M
Biella






T
R
Rebholz






M
Holthausen






M
Hütter




10.1111/jasp.12969








Journal of Applied Social Psychology




53


8
















Advice taking and decision-making: An integrative literature review, and implications for the organizational sciences




S
Bonaccio






R
S
Dalal








Organizational Behavior and Human Decision Processes




101


2


















10.1016/j.obhdp.2006.07.001














Unsupervised distributional anomaly detection for a selfdiagnostic speech activity detector




N
Borges






G
G L
Meyer




10.1109/CISS.2008.4558655








42nd Annual Conference on Information Sciences and Systems






















T
Brown






B
Mann






N
Ryder






M
Subbiah






J
D
Kaplan






P
Dhariwal






A
Neelakantan






P
Shyam






G
Sastry






A
Askell






S
Agarwal






A
Herbert-Voss






G
Krueger






T
Henighan






R
Child






A
Ramesh






D
Ziegler






J
Wu






C
Winter






D
Amodei


















Language models are few-shot learners


H. Larochelle, M. Ranzato, R. Hadsell, M. F












Advances in Neural Information Processing Systems


Balcan, & H. Lin




Curran Associates, Inc


33














A systematic review of algorithm aversion in augmented decision making




J
W
Burton






M
Stein






T
B
Jensen




10.1002/bdm.2155








Journal of Behavioral Decision Making




33


2
















Task-dependent algorithm aversion




N
Castelo






M
W
Bos






D
R
Lehmann




10.1177/0022243719851788








Journal of Marketing Research




56


5
















Training for generalization in theory of mind: A study with older adults




E
Cavallini






F
Bianco






S
Bottiroli






A
Rosi






T
Vecchi






S
Lecce




10.3389/fpsyg.2015.01123








Frontiers in Psychology




6














How algorithmic confounding in recommendation systems increases homogeneity and decreases utility




A
J B
Chaney






B
M
Stewart






B
E
Engelhardt




10.1145/3240323.3240370








Proceedings of the 12th ACM conference on recommender systems


S. Pera, M. Ekstrand, X. Amatriain, & J. O'Donovan


the 12th ACM conference on recommender systems




ACM
















Transformers: "The end of history" for natural language processing




A
Chernyavskiy






D
Ilvovsky






P
Nakov








Machine Learning and Knowledge Discovery in Databases. Research Track


N. Oliver, F. Pérez-Cruz, S. Kramer, J. Read, & J. A. Lozano




Springer International Publishing




12977
















10.1007/978-3-030-86523-8_41














Deep reinforcement learning from human preferences




P
F
Christiano






J
Leike






T
B
Brown






M
Martic






S
Legg






D
Amodei








Proceedings of the 31st International Conference on Neural Information Processing Systems


the 31st International Conference on Neural Information Processing Systems


















SEAI: Social emotional artificial intelligence based on Damasio's theory of mind




L
Cominelli






D
Mazzei






D
E
Rossi




10.3389/frobt.2018.00006








Frontiers in Robotics and AI




5














Role-reversal consistency: An experimental study of the golden rule




M
A
Costa-Gomes






Y
Ju






J
Li




10.1111/ecin.12708








Economic Inquiry




57


1
















Knowing me, knowing you: Theory of mind in AI




F
Cuzzolin






A
Morelli






B
Cîrstea






B
J
Sahakian








Psychological Medicine




50


7


















10.1017/S0033291720000835














The robust beauty of improper linear models in decision making




R
M
Dawes




















10.1037/0003-066X.34.7.571








American Psychologist




34


7














Clinical versus actuarial judgment




R
M
Dawes






D
Faust






P
E
Meehl




10.1126/science.2648573








Science




4899
















Algorithmic accountability




N
Diakopoulos




10.1080/21670811.2014.976411








Digital Journalism




3


3
















Algorithm aversion: People erroneously avoid algorithms after seeing them err




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1037/xge0000033








Journal of Experimental Psychology: General




144


1
















Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them




B
J
Dietvorst






J
P
Simmons






C
Massey




10.1287/mnsc.2016.2643








Management Science




64


3
















The master algorithm: How the quest for the ultimate learning machine will remake our world




P
Domingos








Basic Books












Recalibration in counting and accounting practices: Dealing with algorithmic output in public and private




F
Dudhwala






L
Björklund Larsen




10.1177/2053951719858751








Big Data & Society




6


2
















CAN: Creative Adversarial Networks, generating "art" by learning about styles and deviating from style norms




A
Elgammal






B
Liu






M
Elhoseiny






M
Mazzone




10.48550/arXiv.1706.07068


















CoAI: Cost-aware artificial intelligence for health care




G
Erion






J
D
Janizek






C
Hudelson






R
B
Utarnachitt






A
M
Mccoy






M
R
Sayre






N
J
White






S.-I
Lee




10.1038/s41551-022-00872-8








Nature Biomedical Engineering




6


12
















Can a machine think (anything new)? Automation beyond simulation




M
B
Fazi




10.1007/s00146-018-0821-0








AI & Society


34
















M
B
Fazi




10.1177/0263276420966386




Beyond human: Deep learning, explainability and representation. Theory, Culture, & Society






38














Don't hallucinate, abstain: Identifying LLM knowledge gaps via multi-LLM collaboration




S
Feng






W
Shi






Y
Wang






W
Ding






V
Balachandran






Y
Tsvetkov




10.48550/arXiv.2402.00367


















How users reciprocate to computers: An experiment that demonstrates behavior change




B
J
Fogg






C
Nass




10.1145/1120212.1120419








CHI '97 Extended Abstracts on Human Factors in Computing Systems


A. Edwards & S. Pemberton




ACM Press
















Designed to 'effectively frustrate': Copyright, technology and the agency of users




T
Gillespie




10.1177/1461444806065662








New Media & Society




8


4
















Taking advice: Accepting help, improving judgment, and sharing responsibility




N
Harvey






I
Fischer




10.1006/obhd.1997.2697








Organizational Behavior and Human Decision Processes




70


2
















Subhuman, inhuman, and superhuman: Contrasting humans with nonhumans in three cultures




N
Haslam






Y
Kashima






S
Loughnan






J
Shi






C
Suitner




10.1521/soco.2008.26.2.248








Social Cognition




26


2
















Who is the fairest of them all? Public attitudes and expectations regarding automated decision-making




N
Helberger






T
Araujo






C
H
Vreese




10.1016/j.clsr.2020.105456








Computer Law & Security Review




39














Wrongfully accused by an algorithm. The New York Times




K
Hill






















R
R
Hoffman






S
T
Mueller






G
Klein






J
Litman




10.48550/arXiv.1812.04608




Metrics for explainable AI: Challenges and prospects
















Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making




Y
T
Hou






.-Y
Jung






M
F




10.1145/3479864








Proceedings of the ACM on Human-Computer Interaction




5
















Advice taking under uncertainty: The impact of genuine advice versus arbitrary anchors on judgment




M
Hütter






K
Fiedler




10.1016/j.jesp.2019.103829








Journal of Experimental Social Psychology




85


103829














An ontogenic neural network for bankruptcy classification




J
P
Ignizio






J
R
Soltys








IMA Journal of Management Mathematics




7


4


















10.1093/imaman/7.4.313














Can machines learn morality? The Delphi experiment




L
Jiang






J
D
Hwang






C
Bhagavatula






R
Le Bras






J
Liang






J
Dodge






K
Sakaguchi






M
Forbes






J
Borchardt






S
Gabriel






Y
Tsvetkov






O
Etzioni






M
Sap






R
Rini






Y
Choi




10.48550/arXiv.2110.07574


















Natural language processing: History, evolution, application, and future work




P
Johri






S
K
Khatri






A
T
Al-Taani






M
Sabharwal






S
Suvanov






A
Kumar




















O
Abraham






Castillo




10.1007/978-981-15-9712-1_31




Proceedings of 3rd International Conference on Computing Informatics and Networks


& D. Virmani


3rd International Conference on Computing Informatics and Networks
Singapore




Springer


167














Machine learning: Trends, perspectives, and prospects




M
I
Jordan






T
M
Mitchell




10.1126/science.aaa8415








Science




349


6245
















Why are we averse towards algorithms? A comprehensive literature review on algorithm aversion




E
Jussupow






I
Benbasat






A
Heinzl








Proceedings of the 28th European Conference on Information Systems (ECIS)


F. Rowe


the 28th European Conference on Information Systems (ECIS)












AISeL








A systematic review of empirical studies on advice-based decisions in behavioral and organizational research




J
E
Kämmer






S
Choshen-Hillel






J
Müller-Trede






S
L
Black






J
Weibler




10.1037/dec0000199








Decision




10


2
















Determinants of linear judgment: A meta-analysis of lens model studies




N
Karelaia






R
M
Hogarth




10.1037/0033-2909.134.3.404








Psychological Bulletin




134


3
















From internal models toward metacognitive AI




M
Kawato






A
Cortese




10.1007/s00422-021-00904-7








Biological Cybernetics




115


5
















Theory of mind following the violation of strong and weak prior beliefs




M
J
Kim






P
Mende-Siedlecki






S
Anzellotti






L
Young




10.1093/cercor/bhaa263








Cerebral Cortex




31


2
















Vigilant against manipulation: The effect of regulatory focus on the use of persuasion knowledge




A
Kirmani






R
Zhu




10.1509/jmkr.44.4.688








Journal of Marketing Research




44


4
















Human-versus artificial intelligence




J
E
Korteling






)
Hans






G
C
Van De Boer-Visschedijk






R
A M
Blankendaal






R
C
Boonekamp






A
R
Eikelboom




10.3389/frai.2021.622364








Frontiers in Artificial Intelligence




4


622364














Is there an ethics of algorithms?




F
Kraemer






K
Van Overveld






M
Peterson




10.1007/s10676-010-9233-7








Ethics and Information Technology




13


3
















Algorithmic bias? An empirical study of apparent genderbased discrimination in the display of STEM career ads




A
Lambrecht






C
Tucker




10.1287/mnsc.2018.3093








Management Science




65


7
















Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management




M
K
Lee




10.1177/2053951718756684








Big Data & Society




5


1














Use and control of artificial intelligence in patients across the medical workflow: Single-center questionnaire study of patient perspectives




S
Lennartz






T
Dratsch






D
Zopfs






T
Persigehl






D
Maintz






N
Große Hokamp






Pinto Dos






D
Santos




10.2196/24221








Journal of Medical Internet Research




23


2














Who gets the blame for service failures? Attribution of responsibility toward robot versus human service providers and service firms




X
Leo






Y
E
Huh




10.1016/j.chb.2020.106520








Computers in Human Behavior




113














A comparison of prediction accuracy, complexity, and training time of thirty-three old and new classification algorithms




T.-S
Lim






W.-Y
Loh






Y.-S
Shih




10.1023/A:1007608224229








Machine Learning






40














Lost in the middle: How language models use long contexts




N
F
Liu






K
Lin






J
Hewitt






A
Paranjape






M
Bevilacqua






F
Petroni






P
Liang








Transactions of the Association for Computational Linguistics




12


















10.1162/tacl_a_00638














The psychology of big data: Developing a "Theory of Machine" to examine perceptions of algorithms




J
M
Logg




10.1037/0000290-011








The psychology of technology: Social science research in the age of big data


S. C. Matz




American Psychological Association
















Algorithm appreciation: People prefer algorithmic to human judgment




J
M
Logg






J
A
Minson






D
A
Moore




10.1016/j.obhdp.2018.12.005








Organizational Behavior and Human Decision Processes




151
















Power hungry processing: Watts driving the cost of AI deployment?




S
Luccioni






Y
Jernite






E
Strubell




10.1145/3630106.3658542








Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency


the 2024 ACM Conference on Fairness, Accountability, and Transparency


















What influences algorithmic decision-making? A systematic literature review on algorithm aversion




H
Mahmud






A
K M N
Islam






S
I
Ahmed






K
Smolander




10.1016/j.techfore.2021.121390








Technological Forecasting and Social Change




175














Clinical versus statistical prediction: A theoretical analysis and a review of the evidence




P
E
Meehl




10.1037/11281-000








University of Minnesota Press












Sociospatial context and information behavior: Social exclusion and the influence of mobile information technology




K
Mervyn






D
K
Allen








Journal of the American Society for Information Science and Technology




63


6


















10.1002/asi.22626














The serial position effect of free recall




B
B
Murdock




10.1037/h0045106








Journal of Experimental Psychology




64


5
















Human reactions to technological failure: How accidents rooted in technology vs. Human error influence judgments of organizational accountability




C
E
Naquin






T
R
Kurtzberg




10.1016/j.obhdp.2003.12.001








Organizational Behavior and Human Decision Processes




93


2
















Computers are social actors




C
Nass






J
Steuer






E
R
Tauber




10.1145/191666.191703








Proceedings of the SIGCHI conference on human factors in computing systems


the SIGCHI conference on human factors in computing systems




ACM Press














Talking to bots: Symbiotic agency and the case of Tay




G
Neff






P
Nagy








International Journal of Communication




10
















Prosocial behavior toward machines




Y
A
Nielsen






S
Pfattheicher






M
Keijsers








Current Opinion in Psychology




43


















10.1016/j.copsyc.2021.08.004














GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice




P
Niszczota






S
Abbas




















10.1016/j.frl.2023.104333








Finance Research Letters




58














The relative influence of advice from human experts and statistical methods on forecast adjustments




D
Önkal






P
Goodwin






M
Thomson






S
Gönül






A
Pollock




10.1002/bdm.637








Journal of Behavioral Decision Making




22


4


















Openai






Docs. Retrieved
















AI in the headlines: The portrayal of the ethical issues of artificial intelligence in the media




L
Ouchchy






A
Coin






V
Dubljević








AI & Society


35
















10.1007/s00146-020-00965-5














Best practices in Supervised Machine Learning: A tutorial for psychologists




F
Pargent






R
Schoedel






C
Stachl




10.1177/25152459231162559








Advances in Methods and Practices in Psychological Science




6


3
















Acceptance of rules generated by machine learning among medical experts




M
J
Pazzani






S
Mani






W
R
Shankle




10.1055/s-0038-1634196








Methods of Information in Medicine




40


05
















Manipulating and measuring model interpretability




F
Poursabzi-Sangdeh






D
G
Goldstein






J
M
Hofman






J
W
Wortman Vaughan






H
Wallach




Y. Kitamura, A


















K
Quigley






T
Isbister






P
Igarashi




10.1145/3411764.3445315




Proceedings of the 2021 CHI conference on human factors in computing systems


Bjørn, & S. Drucker


the 2021 CHI conference on human factors in computing systems




ACM














Understanding algorithm aversion: When is advice from automation discounted




A
Prahl






L
Van Swol








Journal of Forecasting




36


6


















10.1002/for.2464














Does the chimpanzee have a theory of mind?




D
Premack






G
Woodruff




10.1017/S0140525X00076512








Behavioral and Brain Sciences




1


4
















Measuring and narrowing the compositionality gap in language models




O
Press






M
Zhang






S
Min






L
Schmidt






N
Smith






M
Lewis




H. Bouamor, J. Pino, & K


















Bali




10.18653/v1/2023.findings-emnlp.378




Findings of the Association for Computational Linguistics: EMNLP 2023




Association for Computational Linguistics














The introspection illusion




E
Pronin








Advances in Experimental Social Psychology


M. P. Zanna




41






Elsevier Academic Press














10.1016/S0065-2601(08


















Incorporating fairness into game theory and economics




M
Rabin








The American Economic Review




83


5
















Advice as a form of social influence: Informational motives and the consequences for accuracy. Social and Personality Psychology Compass




C
A
Rader






R
P
Larrick






J
B
Soll




10.1111/spc3.12329








11














I
Rahwan






M
Cebrian






N
Obradovich






J
Bongard






J.-F
Bonnefon






C
Breazeal






J
W
Crandall






N
A
Christakis






I
D
Couzin






M
O
Jackson






N
R
Jennings






E
Kamar






I
M
Kloumann






H
Larochelle






D
Lazer






R
Mcelreath






A
Mislove






D
C
Parkes






A
S
Pentland






M
Wellman




10.1038/s41586-019-1138-y








Machine behaviour






568














Parallel context windows for large language models




N
Ratner






Y
Levine






Y
Belinkov






O
Ram






I
Magar






O
Abend






E
Karpas






A
Shashua






K
Leyton-Brown






Y
Shoham




10.48550/arXiv.2212.10947




















T
R
Rebholz






A
Koop






M
Hütter




10.31234/osf.io/jq9se




Conversational user interfaces: Explanations and interactivity positively influence advice taking from generative artificial intelligence. Technology, Mind, and Behavior






in press








To err is human, not algorithmic -Robust reactions to erring algorithms




L
A
Renier






M
Schmid Mast






A
Bekbergenova




10.1016/j.chb.2021.106879








Computers in Human Behavior




124


106879














Uber's drivers: Information asymmetries and control in dynamic work




A
Rosenblat






L
Stark








International Journal of Communication




10
















A case for metacognitive myopia in large language models




F
Scholten






T
R
Rebholz






M
Hütter












Manuscript in preparation












M
Sharma






M
Tong






T
Korbak






D
Duvenaud






A
Askell






S
R
Bowman






N
Cheng






E
Durmus






Z
Hatfield-Dodds






S
R
Johnston






S
Kravec






T
Maxwell






S
Mccandlish






K
Ndousse






O
Rausch






N
Schiefer






D
Yan






M
Zhang






E
Perez


















Towards understanding sycophancy in language models


10.48550/arXiv.2310.13548














Cross-cultural differences in the refusal to accept a small gift: The differential influence of reciprocity norms on Asians and North Americans




H
Shen






F
Wan






R
S
Wyer








Journal of Personality and Social Psychology




100


2


















10.1037/a0021201














Explainable artificial intelligence: Evaluating the objective and subjective impacts of xAI on humanagent interaction




A
Silva






M
Schrum






E
Hedlund-Botti






N
Gopalan






M
Gombolay




10.1080/10447318.2022.2101698








International Journal of Human-Computer Interaction




39


7
















Cueing and cognitive conflict in judge-advisor decision making




J
A
Sniezek






T
Buckley








Organizational Behavior and Human Decision Processes




62


2


















10.1006/obhd.1995.1040














Strategies for revising judgment: How (and how well) people use others' opinions




J
B
Soll






R
P
Larrick




10.1037/a0015145








Journal of Experimental Psychology: Learning, Memory, and Cognition




35


3
















Explanatory artificial intelligence (YAI): Human-centered explanations of explainable AI and complex data




F
Sovrano






F
Vitali




10.1007/s10618-022-00872-x








Data Mining and Knowledge Discovery


















N
Thurman






J
Moeller






N
Helberger






D
Trilling




My friends, editors, algorithms, and I. Digital Journalism






7
















10.1080/21670811.2018.1493936














Attention is all you need




A
Vaswani






N
Shazeer






N
Parmar






J
Uszkoreit






L
Jones






A
N
Gomez






Ł
Kaiser






I
Polosukhin








Advances in Neural Information Processing Systems


I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett




Curran Associates, Inc




30












Property value as a proxy of socioeconomic status in education




J
K
Ware




10.1177/0013124517714850








Education and Urban Society




51


1
















Supporting artificial social intelligence with theory of mind




J
Williams






S
M
Fiore






F
Jentsch




10.3389/frai.2022.750763








Frontiers in Artificial Intelligence




5














Unexplainability and incomprehensibility of AI




R
V
Yampolskiy








Journal of Artificial Intelligence and Consciousness




7


2


















10.1142/S2705078520500150














Receiving other people's advice: Influence and benefit




I
Yaniv








Organizational Behavior and Human Decision Processes




93


1


















10.1016/j.obhdp.2003.08.002














The benefit of additional opinions




I
Yaniv




10.1111/j.0963-7214.2004.00278.x








Current Directions in Psychological Science




13


2
















Advice taking in decision making: Egocentric discounting and reputation formation




I
Yaniv






E
Kleinberger




10.1006/obhd.2000.2909








Organizational Behavior and Human Decision Processes




83


2
















Making sense of recommendations




M
Yeomans






A
Shah






S
Mullainathan






J
Kleinberg








Journal of Behavioral Decision Making




32


4


















10.1002/bdm.2118














The impact of a lack of mathematical education on brain development and future attainment




G
Zacharopoulos






F
Sella






R
Cohen Kadosh




10.1073/pnas.2013155118








Proceedings of the National Academy of Sciences




118


24














Moving beyond stereotypes of empathy




J
Zaki




10.1016/j.tics.2016.12.004








Trends in Cognitive Sciences




21


2
















The trouble with algorithmic decisions: An analytic road map to examine efficiency and fairness in automated and opaque decision making




T
Zarsky








Technology










Science









"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]