You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
The ability to flexibly plan and adapt to changing environments is a fundamental aspect of intelligent behavior, and understanding its underlying mechanisms remains a central goal in neuroscience and psychology 
[1]
[2]
[3]
[4]
[5]
[6]
 . It has been proposed that this ability relies on the brain's computational representations, known as cognitive maps, which organize task-related information in a flexible and efficient manner 3;7-13 . However, it is quite challenging to efficiently create representations that are shaped by our current goals and environment, but remain useful in the future. Despite numerous attempts, particularly those built on reinforcement learning (RL), existing computational models of planning face significant challenges. These models often struggle to achieve a balance between flexibility, efficiency in decision-making, and efficiency in updating their representations. Here, we introduce a new model to address these issues.
Reinforcement learning (RL) offers crucial insight into the brain's capacity for efficient and flexible behavior, stemming from its ability to reuse previous computations, a concept that aligns with cognitive maps 
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
 . However, a significant challenge lies in organizing these computations to adapt to new tasks, goals, or environmental changes, as the efficiency of reuse often conflicts with the need for flexibility 
[24]
[25]
[26]
 . Consider this scenario: You leave your apartment for morning coffee, navigating the city while learning and updating your mental map of the city. The next day, your destination changes to the library. Ideally, your cognitive map should support this new goal without being constrained by yesterday's coffee shop route. While humans and animals may sometimes exhibit habitual behavior (like reflexively heading towards the coffee shop when stepping out of your house), they often demonstrate goal-directed or "model-based" behaviors 
[27]
[28]
[29]
[30]
[31]
[32]
 . These behaviors efficiently utilize mental representations of tasks and environments to achieve current objectives. The key challenge lies in understanding how these flexible yet efficient representations are constructed and updated 
8;33
 .
The successor representation (SR) 
34
 , a prominent proposal from RL, attempts to address this challenge by caching expectations about future state visits 
[35]
[36]
[37]
[38]
 . Importantly, the SR can be efficiently learned using the same "temporal-difference" (TD) 
39
 algorithms that are popular in RL and have been influential in understanding the role of phasic dopamine responses as prediction errors representing the difference between observations and expectations 
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]
 . In the case of SR, the TD prediction errors represent the difference between the observed and expected future state visits. However, while the SR can be efficiently learned and updated, it struggles with flexibility. This inflexibility stems from the SR's dependence on the specific policy used during learning-as the stored predictions reflect the agent's past behavior rather than the underlying structure of the environment. Its reliance on old decision policies limits its usefulness when goals or environmental transitions change, resulting in behavior that is far more rigid than that observed in humans and animals 
21;48
 .
Building on theoretical advances from control theory 
49;50
 , a new RL model, linear RL constructs a computational map similar to the SR, but under a default policy 
51
 . This policy is independent of previous or current goals, and the resulting map, called the default representation, depicts the probability of visiting each future state when following this default policy. When the default policy is unbiased (e.g., uniform), the linear RL approach provides a highly accurate approximation of the optimal solution to RL problems, particularly those with deterministic transitions and specific goals 
51
 . The solution offers two key advantages: First, it is a closed-form, linear solution that does not depend on future values or actions, allowing for efficient implementation with a single layer of a neural network. Second, it enables efficient computation of changes in the map when transitions are altered or new goals are introduced. Although these changes in representations are are massively complex (i.e., as large as a massive matrix with the size being the number of states in the environment), they can be efficiently constructed based on low-dimensional representations (i.e., another matrix whose size depends on changes in the environment). However, unlike the SR, there is no efficient TD algorithm for learning the default representation. In this work, we address this issue.
Here, we introduce SR-IS (Successor Representation with Importance Sampling), a new model that leverages the principles of importance sampling from probability theory. In our context, this technique allows us to compute an unbiased, general successor map while the world is experienced based on some specific goaldirected decision policy. Consider the coffee shop example again. As you leave your house to get coffee, you experience the successor states (future locations) based on your current decision policy, which is obviously shaped by your goal to get to the coffee shop. However, the computations that you ideally want to cache should be expected occupancy under a default policy (e.g., an unbiased uniform policy) that is independent of your current goal. Importance sampling enables us to do that.
We demonstrate that importance sampling can be effectively combined with the TD method to produce unbiased successor maps that are useful beyond the current goal and task. These maps represent likely future states under a default policy, despite being constructed from experiences derived based on a goal-directed decision policy. The solution is straightforward and closely resembles the classical TD algorithm for learning the SR. When the default and decision policies are matched, it reduces to the standard SR; otherwise, it smoothly debiases the SR. We show that such maps can be efficiently used within the linear RL framework, maintaining its advantages while also addressing its shortcoming. Importantly, the model explains aspects of human replanning behavior that earlier models struggled to account for 
38
 .


Results


Model
Successor representation. In Markov decision tasks (such as navigating mazes or playing video games), an agent moves through a sequence of states. At each state, the agent receives a reward or punishment and must choose from available actions, which determine the next state. RL captures this process by maximizing the expected sum of future rewards, called the "value" function.
Finding the optimal value function requires solving a series of interdependent optimizations, as the standard RL objective has no closed-form solution. Since this is often intractable, one simplified approach instead assumes a fixed decision policy π and calculate the value function under that policy, V π (s). This represents the expected sum of (temporally discounted) future rewards from state s when following policy π. The SR emerges naturally from this framework, as V π (s) can be calculated using the expected future occupancy of each state s ′ along trajectories that start in state s and follow policy π.
M π (s, s ′ ) = E t=0 γ t I(s t = s ′ )|s 0 = s ,
(1)
where s t is the state visited at time t, γ ∈ [0, 1] is the discount factor that downweights distal rewards, and I(s t = s ′ ) is an indicator function that is 1 if s t = s ′ and 0 otherwise. The superscript (π) in M π indicates that the expectation is taken with respect to the decision policy distribution, π. Thus, two physically adjacent states that predict divergent future states under the decision policy will have dissimilar representations, and two states that predict similar future states will have similar representations. The SR is typically represented as a matrix M π , where M π (s, s ′ ) is the element at position (s, s ′ ).
An online TD learning approach has long been used to learn the SR, M π , from experiences. This approach caches rows of M π and incrementally updates them after transitioning to a new state 
34
 . Specifically, following a transition, the corresponding row of the state in which the transition was made from is updated as follows,
M π (s, :) ← (1 − α)M π (s, :) + α 1 T s + γM π (s ′ , :) ,
(2)
where M π (s, :) is the sth row of the SR matrix, 1 T s is a binary row vector that is all zeros except for a 1 in the sth position, and α is the learning rate parameter that controls the step size at each iteration.
Linear RL. A new model "linear RL," dramatically simplifies reinforcement learning problems with terminal states by providing a closed-form solution to the value function 
51
 , building on Todorov's planning as inference framework 
49;50
 . This approach approximates the original problem by introducing a new gain function for each state, defined as that state's reward minus a control cost term. This control cost reflects the dissimilarity between the decision policy and a default policy, π d . Importantly, when the default policy is unbiased (e.g., uniform), the control cost term promotes stochastic behavior -traditionally useful for solving the exploreexploit dilemma -without biasing the resulting behavioral policy. As shown by Piray et al. this solution depends on a predictive map called the default representation (DR) which is similar to the SR, but crucially it is constructed with respect to the default policy and not the decision policy. This addresses a key deficiency of the SR, which depends on a fixed (and typically outdated) policy.
However, learning the DR online presents significant challenges. In any online RL problem, the agent must act according to the current decision policy, not a uniform (i.e., random) default policy. However, using these actions directly with the TD rule would result in learning the map under the decision policy rather than the desired default policy. The key question becomes: how can we act according to the current decision policy while learning representations under the default policy that will remain useful even after the current policy becomes irrelevant? Importance sampling. Importance sampling is a statistical method that enables efficient estimation of properties from a target probability distribution while sampling from an alternative distribution 
52;53
 . It addresses a common challenge in statistics-calculating the expected value of a function under probability distribution p when we cannot solve the expectation analytically or sample directly from p. Simply drawing samples from a different distribution q would create a biased estimator, with the bias increasing the more q diverges from p. Importance sampling offers an elegant solution by drawing samples from the alternative distribution q (typically one that is easy to sample from) and correcting for the bias through "importance weights," which are calculated as the ratio p/q. These weights adjust for sampling discrepancy by increasing the influence of samples that are likely under p but unlikely under q, while decreasing the influence of samples that are unlikely under p but likely under q. This approach constructs an unbiased estimator of the expectation.
New model: SR with importance sampling (SR-IS) Let's now return to a fundamental challenge in representation learning: how can we act according to our current policy while building representations that will remain useful when that policy becomes irrelevant? Importance sampling provides the solution. While we act according to the decision policy, we can learn the predictive representation under the default policy by using importance weights. This creates an unbiased estimator of the default representation (i.e., the SR  
Figure 1
: The SR-IS model. (a) Showing the impacts that an agent's decision has on the importance sampling term, w(s, s ′ ). After training, the agent will have a decision policy that preferentially chooses to move to the right and go towards the higher reward, the default policy, on the other hand, is constant and uniform. Because moving towards the right is more probable under the agent's decision policy, the resulting importance sampling term is lower than if the agent were to choose the less probable action of moving leftwards. (b) High-level overview of the framework. We assume a classic reinforcement learning paradigm where our model, the agent, interacts with an environment. The interaction can be formalized with the agent acting on the environment with each resulting in a new state and reward. The agent's action is used to inform the importance sampling term which is then used, in conjunction with the classic TD update, to update the agent's internal state-state representation.
under the default policy). We call this approach SR with Importance Sampling (SR-IS), which modifies the traditional temporal difference learning algorithm:
M(s, :) ← (1 − α)M(s, :) + α 1 T s + γM(s ′ , :) w(s, s ′ ),
(3)
where w(s, s
′ ) = π d (s ′ |s) π(s ′ |s)
represents the importance weight. The superscript π is omitted as these representations are learned under the default policy, which we assume to be uniform throughout this work (though the framework generalizes to arbitrary default policies).
The importance weights w(s, s ′ ) correct for the discrepancy between the sampling distribution (decision policy π) and the target distribution (default policy π d ). Specifically, transitions that occur more frequently under π d than π receive higher weights, while transitions that are more probable under π than π d receive lower weights. This reweighting mechanism ensures unbiased estimation of the default policy's successor representation, despite experiencing state transitions generated by a potentially very different decision policy.
To illustrate the key mechanisms of the model, consider a simple environment where an agent faces a choice between two reward states equidistant from its current location 
(Figure 1a)
. Although both states are equally accessible under a uniform default policy, they offer different rewards, with the right state providing a larger reward. The agent's decision policy appropriately favors movement toward the higher reward whenever it reaches this junction. This behavioral preference creates a systematic bias in the SR's representation: the connection between the agent's current state and the higher-reward state to the right is updated more frequently than the connection to the left state. As a result, the SR progressively overweights the representation of the more frequently visited state, despite both states being equally accessible in terms of their spatial distance from the agent's current state. This aspect of the model proves crucial for explaining animals' behavioral adaptation following reward revaluation in classical tasks where cognitive maps are thought to play a central role 
3
 , which we will examine later.
Intuitively, the importance sampling term w(s, s ′ ) up-weights transitions that are more likely under the default policy π d and down-weights those that are more likely under the decision policy π 
(Figure 1a
). By incorporating w(s, s ′ ) into the TD update rule (equation 3), the model is able to de-bias its learned policy from the decision policy ( 
Figure 1b)
. This allows the model to learn a representation that is less biased towards the specific actions taken by the agent during training and instead captures the underlying dynamics of the environment. As a result, the model learns a more robust and adaptable cognitive map that can support flexible decision-making and generalization to new tasks.


Model Performance
SR-IS converges to the DR. Our first experimental result validates our approach by demonstrating that the representation learned by SR-IS converges to the "complete" (but unrealistic) representation obtained through matrix inversion 
51
 . To test convergence we used a classic RL benchmark 
54
 , the four-room environment 
(Figure 2a, b)
.
In this environment, we compared three key quantities: the representation learned by SR-IS through online temporal difference learning with importance sampling, the representation learned by the standard SR without importance sampling, and the complete representation computed directly through matrix inversion. We measured the mean absolute difference between these representations as the two agents (SR, SR-IS) navigated the environment and updated their respective representations.
The results revealed that SR-IS converges to the complete representation as the number of learning steps increases, with the difference between the two representations approaching zero. This confirms that importance sampling enables the model to learn the correct underlying structure through experience alone, without requiring expensive matrix operations. In contrast, the standard SR without importance sampling converges to a distinctly different representation that reflects the specific policy used during training rather than the true environmental structure. The disparity between the two models will only become more pronounced as we move to larger and more complicated environments as the SR will accumulate increasingly more bias from its training policy. The error between SR and the complete representation will grow exponentially with the size of the state space, while SR-IS maintains convergence to the true representation regardless of environmental complexity.
SR-IS demonstrates cross-room planning efficiency. We next evaluated how the unbiased nature of SR-IS affects planning flexibility across different spatial domains. After training both models to navigate to an initial goal state (r 1 ) in the four-room environment, we evaluated their ability to replan routes to eight new goal states (r 2 , r 3 , ..., r 9 ). Two new goals were placed in the same room as r 1 , with the remaining six distributed evenly across the other three rooms. SR-IS demonstrated consistent performance across all goals, efficiently computing paths regardless of the goal's location relative to r 1 . In contrast, the SR model showed a dramatic spatial bias. While it maintained effectiveness for goals within the same room as r 1 , its performance degraded substantially for goals in other rooms. This disparity reflects the SR's policy dependence during initial training, it builds a representation biased toward frequently visited states under the r 1 -directed policy, limiting its ability to plan efficiently in less-visited regions. SR-IS overcomes this limitation through importance sampling, which enables learning of a globally valid representation that supports flexible planning across all spatial domains.
These results complement our convergence findings by demonstrating that SR-IS's theoretical advantages translate into practical benefits for spatial navigation and planning. The model's ability to maintain performance across spatially distinct regions highlights the importance of learning unbiased representations for flexible behavior.
The representation learned by SR-IS is able to replan with a single update. As shown by Piray & Daw 
51
 , one of the most important features of the default representation map used by the linear RL framework is that it can be efficiently and exactly updated when state transitions change. This capability builds on matrix algebra (specifically the Woodbury matrix inversion lemma 
55;56
 ), which allows us to update the representation of M in place by accounting for local changes in the transition graph:
M = M old + ∆
(4)
where ∆ is a low-rank, easy-to-compute change matrix whose rank equals the number of states with modified transitions (see Methods for the exact definition). This is important for adapting both to structural changes in the environment, such as new barriers (as we see later in the Tolman detour task), and when previous non-goal states become goals. Since SR-IS learns an unbiased representation, it can leverage the same efficient update method, inheriting all the computational advantages of the linear RL framework.
We conducted simulations in a maze environment to demonstrate this point. We first used the SR-IS model to learn the successor map from a starting state in one corner to a goal state in the opposite corner of the maze, (r 1 ). We then tested whether this learned representation, combined with Equation 4, could find the shortest path to various non-goal states, i.e., every other state in the maze. This analysis revealed that the SR-IS model can be efficiently reused to find the shortest path in all cases 
(Figure 2d
, e). Unlike the four-room simulations 
(Figure 2a
, c) where all target states were pre-defined goals, this scenario required the model to adapt to (r 2 ) transitioning from a non-goal to a goal state using Equation (4).
SR-IS replans on par with linear RL. We next compared SR-IS against a number of alternative models in the same 10x10 maze, including the Complete model (which uses matrix inversion to calculate its map) and the standard SR model. Each model first constructed a predictive map with an initial goal state r 1 ( 
Figure  2d
). We then switched the goal to r 2 and evaluated how well each model could reuse its previous map to plan a new path. SR-IS not only found the shortest path during replanning but matched the performance of the Complete model ( 
Figure 2f
 )-a notable achievement given that the Complete model relies on computationally intensive matrix inversions that would be infeasible for biological systems.
Additionally, we see that the SR model, when tasked with replanning between different goal states, is only slightly more efficient than the random walk through the environment. For comparison, we also tested an enhanced version of SR, labeled SR*, which incorporates the Woodbury update. While one might expect this enhancement to help, it was mathematically clear from the outset that it would not: the SR fundamentally depends on the decision policy, which changes in response to modifications in both transition and goal structure -a complexity that cannot be adequately addressed with a simple low-rank update. Our analysis revealed that SR* was only marginally better than the SR and Random models. These results, as well as those shown in the four-room environment 
(Figure 2c
), align with our theoretical understanding of the SR's policy dependence. As we discussed, the initial representation, learned with respect to the goal state r 1 , accumulates higher weights along frequently traversed paths, creating a biased map that overrepresents the states visited under the original policy 
(Figure 1a)
. When the goal switches to r 2 , these inflated state representations-accumulated through repeated visits to states along the path to r 1 -continue to bias the value function and consequently the agent's policy, even though these states may be irrelevant or suboptimal for reaching r 2 .


SR-IS replans similar to humans
We next examine a set of replanning tasks that reveal intriguing but puzzling patterns of both flexibility and inflexibility in human behavior. These tasks, originally introduced by Russek et al. 
37
 and later tested experimentally by Momennejad et al. 
38
 , provide an ideal testbed for evaluating our model against existing alternatives. The experimental paradigm consists of three cleverly designed tasks that probe specific aspects of revaluation behavior. A key finding was that humans display an asymmetric pattern of flexibility across these replanning tasks -a pattern that could not be explained by either the SR model or even unrealistic model-based accounts (such as the Complete model shown in 
Figure 2
).
The three tasks -termed reward, policy, and transition revaluation -are illustrated in 
Figure 3a
. In these experiments, human subjects were initially trained to navigate a three-stage sequential task leading to one of three terminal states. The training phase was followed by a revaluation phase, during which participants either experienced a significant reward change in a previously disfavored terminal state or learned about a change in the transition structure. Importantly, they did not have to go through the entire task again to experience this change. In the final testing phase, the participants were placed back at the beginning of the maze. The results 38 showed that participants were generally able to modify their behavioral policy despite never having directly reached the new terminal state through actions in the maze. Interestingly, while human participants were, on average, able to replan in all three conditions, they displayed a lower sensitivity to the policy and transition revaluation tasks than to the reward revaluation task 
(Figure 3b
). This was measured as a change in preference during the testing phase when the subjects, after being made aware of the change in the environment, were placed back in the original starting state. It was observed that after learning about the change in reward structure in both the reward and policy revaluation settings, and the change in transition structure in the transition revaluation setting, more participants choose the transition from state 1 to state 2 in the reward revaluation setting than in the policy and transition revaluation setting. This behavioral asymmetry is particularly intriguing because all three conditions presented participants with new information that should theoretically lead to the same optimal policy.
For the policy and transition revaluation tasks, the new optimal action requires a change in preference that is directly antithetical to the current policy. Even though the reward revaluation setting also requires humans to make a change in preference, the path from state 1 to state 2 was still rewarding and allowed the participant to still achieve the maximal reward, whereas in the policy and transition revaluation setting going to state 2 was never as rewarding as state 3. The SR model, which is learned with respect to the decision policy π used during the training phase, cannot adapt in the test phase for these settings.
The SR-IS model successfully replicates human behavior across all three revaluation tasks 
(Figure 3c
). Through importance sampling, the model achieves the same rapid adaptations that humans show when faced with changes in reward or transition structure. This suggests that importance sampling serves as a computationally efficient bridge, combining the speed of cached predictions with the flexibility needed for rapid behavioral adaptation across diverse environmental changes. The model's success in explaining human behavior stems from integrating importance sampling into the SR Reconstruction of human data from the task 
38
 , more participants changed their preference (policy) during reward revaluation than policy revaluation. We compare the human data with model performance showing the model's probability of switching states for the importance sampling (c), SR (d), and complete (e) models. The importance sampling model was able to most accurately capture this relationship between switching preferences in the reward and policy revaluation settings.
framework. This integration allows it to handle all three types of revaluation while naturally reproducing the performance patterns seen in humans 
(Figure 3c)
. Importantly, the model also captures human limitations. While SR-IS is theoretically unbiased, it exhibits high variance in practice due to sampling variability. This variance is most pronounced when states that are rare under the current decision policy are common under the default policy. Since these states are sampled infrequently, they may require extensive TD learning to be properly evaluated. This limitation manifests particularly in the policy and transition revaluation tasks, where some simulated agents, like their human counterparts, fail to make optimal choices.
Neither the standard SR model nor the unrealistic Complete model can adequately explain this pattern of behavior 
(Figure 3d,e)
. The standard SR model, being inherently policy-dependent, is expected to entirely fail at policy revaluation as it cannot update its cached predictions without direct experience under the new policy 
37
 . Conversely, the Complete model, behaves like an optimal model-based agent, predicts equal performance across all three conditions -failing to capture the subtle suboptimality observed in human behavior.


SR-IS solves the Tolman latent learning & detour tasks
We next validate the SR-IS model using two of Tolman's classic animal paradigms -latent learning and detour tasks -which were instrumental in developing the cognitive map concept. Considering these tasks through the lens of RL, we can divide them into two different categories. The latent learning task can be viewed as changing the reward structure of the environment, whereas the detour task changes the physical, transition, structure of the environment through the addition of a barrier.
For the latent learning simulation, we use a version of Tolman's latent learning task 
(Figure 4a,b)
. Rats were initially allowed to forage freely in a maze with two rewarding end boxes where they would receive an equal amount of reward in either end box. In the next phase, one of the boxes resulted in a shock without warning, devaluing that terminal state. This is a simple manipulation, yet one that normal model-free RL algorithms, like TD learning, cannot overcome because in order to update their previously learned estimates of long-term value or policy, they need to follow paths that lead from the chosen option to the less valuable outcome. Because this manipulation only changes the reward structure of the environment and not the transition structure, it is not a representative test to display policy dependence. Thus, the SR model should still be able to replan, after receiving the shock, to the rewarding terminal state whether or not it has importance sampling. Both models preferentially choose state s 2 which leads to reward r 2 rather than state s 1 , as s 1 leads to r 1 which is where the agents received the negative stimulus 
(Figure 4c,d
).
The detour task was similar to those used in previous work 
37;51
 
(Figure 4e)
. In the task, the environment initially has three possible paths, with the agent preferentially choosing to go straight (state s 1 ) ( 
Figure 5g
) as this is the shortest path to the reward. In the next phase, the path going straight is blocked 
(Figure 5f
 ) and the agent must be able to update its representation to account for the barrier, ideally choosing the next shortest path through state s 2 
(Figure 3g,h)
. The detour task highlights a key limitation of the SR model: due to its policy dependence, it cannot solve this task without extensive replay 
37
 .
We make use of the low-rank update of Equation 4 in order to update the agent's state-state representation in a one-step fashion 
(Figure 3g
). The updated representation results in a new optimized policy that will both avoid the obstacle while also preferentially selecting the next shortest path (going left into state s 2 ). As expected, the SR is unable to solve the detour task 
(Figure 3h)
, the new policy based on the updated representation shows no change in preference as the agent mistakenly still prefers to go straight into state s 1 .


SR-IS results in a better match to rat and human navigation trajectories
The previous analyses demonstrated the advantages of the SR-IS model in experimental replanning paradigms with humans and rats. Here, we examine the model using the rich dataset from de Cothi et al., which stands out for both its comprehensive set of complex mazes and its more naturalistic experimental setup. de Cothi et al. 
19
 conducted a comprehensive cross-species comparison using their "Tartarus maze" -an innovative experimental paradigm requiring rapid adaptation to changing environmental obstacles while maintaining goal-directed navigation.
In their experiment, the goal state was fixed and then they had both humans and rats navigate through 25 different maze configurations, each having 10 different starting states. Rats were tested with a physical instantiation, humans in immersive head-mounted virtual reality, and RL agents via simulation. They tested three different RL agents the SR, model-free (MF), and model-based (MB). Their findings revealed that both species showed remarkable similarity in their navigation patterns, with performance most closely matching SR agents. However, they noted that the SR model struggled with certain maze configurations, particularly those requiring significant policy changes -a limitation we hypothesized could be addressed through importance sampling.
During our analysis, we identified a previously undocumented phenomenon we termed "first-order policy dependence." This effect emerged through our observations of agent behavior between different starting locations. We observed that when the optimal policy for reaching the goal state from a previous starting location contradicted the optimal policy for the current starting location, the SR agent's performance significantly degraded.
For example, if the SR agent starts in a location where the optimal path to the goal state is to go to the upwards and then to the left it will struggle when it is then placed in a starting location requiring it to navigate upwards and then to the right. This challenge intensifies when multiple starting positions require similar navigational policies, as the agent develops an increasingly biased representation of the environment. Notably, both human and rodent subjects demonstrated superior flexibility, suggesting their ability to develop more generalized spatial representations that enable rapid adaptation to new starting positions.
Our analysis focuses on Mazes 15 and 22 
(Figure 5a,d)
, which exemplify first-order policy dependence through their barrier structures and starting location layouts. For these mazes, the SR-IS model shows higher correlation with both rat and human mean path lengths (across all starting points) compared to the standard SR model. Another form of policy dependence that was mentioned in the original work stemmed from the transition from one maze to another, we call this "second-order" policy dependence. This would occur if the preceding maze configuration had an optimal policy contradictory to the current configuration. For example, in the previous maze the agents had to navigate upwards from all the start locations but now they have to go downwards. Importance sampling was able to address this issue, showing a higher success rate across the different mazes, specifically the ones that suffered from this second-order policy dependence. Interestingly, humans also seemed to struggle more on these mazes as well, but not to the same degree as the SR agent ( 
Figure S1
).


Discussion
The present work introduces SR-IS, a novel computational model that addresses a fundamental challenge in cognitive map construction: how to create task representations that are both flexibly reusable across contexts and computationally efficient through cached planning computations. This advances beyond previous RL approaches, which have been limited either by their reliance on specific policies (like SR) or by their dependence on computationally intensive and unrealistic processes (like model-based 5;29;37 accounts including linear RL 51 ).
At the core of SR-IS is a reweighting mechanism that systematically debiases learned representations. This term effectively rebalances the learning process by increasing the influence of transitions that are more likely under the default policy while reducing the impact of those dictated by the current decision policy. By incorporating this weighting into the TD learning framework, the model systematically removes biases stemming from specific decision policies and instead constructs a representation that better reflects the environment's inherent structure. This approach yields a more general and robust predictive map that maintains its utility across different tasks and goals, enabling flexible decision-making even in novel situations. Such an unbiased representation stands in contrast to traditional cached successor maps that are typically constrained by the specific experiences accumulated during training.
Our findings demonstrate that the SR-IS model successfully captures key aspects of human behavior in decision revaluation tasks while providing a computational account of observed behavioral asymmetries. In a series of tasks designed to test different forms of revaluation -including reward, policy, and transition revalu- ation -the SR-IS model exhibited patterns strikingly similar to those observed in human participants. Unlike humans and our model, the standard SR model fails to solve both policy and transition revaluation tasks due to its complete dependence on previously learned policies that become irrelevant after revaluation. Importantly, while both humans and the model showed the ability to adapt across all conditions, they displayed reduced sensitivity to policy and transition revaluation compared to reward revaluation. The SR-IS model suggests that this behavioral pattern emerges naturally from the mechanics of importance sampling. Although the method is theoretically unbiased, it exhibits increased variance in situations where some states are almost never visited under the current decision policy. This variance is particularly pronounced in policy and transition revaluation tasks, leading some simulated agents to make suboptimal choices -precisely matching the pattern observed across human participants. These results stand in marked contrast to both the standard SR model, which completely fails at policy and transition revaluation due to its inherent policy dependence, and the model-based system (including the linear RL model with matrix inversion), which incorrectly predicts uniform performance across all conditions. Thus, beyond providing a computationally efficient solution for flexible planning, the SR-IS model offers a mechanistic explanation for human behavioral patterns that previous approaches failed to capture.
Previous studies 
37;38
 have introduced a wide array of SR variations, including a model-based version (SR-MB), a replay 57 version (SR-Dyna), as well as a hybrid model-based version (Hybrid SR-MB) as different attempts to solve the SR's problems. While these alternative solutions warrant consideration, they ultimately either share the same limitations as the base SR-TD model or become as computationally intensive as exhaustive search methods. In contrast, our SR-IS model successfully addresses the key challenges that previous approaches attempted to solve, while maintaining the efficiency and cost-effectiveness of SR-TD.
The SR-IS model seamlessly integrates with the linear RL framework while addressing its primary limitation: the absence of an efficient learning algorithm. Through importance sampling, the model develops default representations even during goal-directed policy execution, similar to how individuals construct a general cognitive map of their city while traveling specific routes. When paired with an unbiased default policy, this approach generates accurate approximations for optimal solutions to RL problems without sacrificing com-putational efficiency. Furthermore, the model is able to efficiently update its representations in response to environmental changes or new objectives. Critically, this update mechanism demonstrates stronger alignment with observed human and animal behavior in replanning scenarios.
The hippocampus is thought to be a central region for cognitive map construction, and recent work suggests that hippocampal place cells encode SR-like computations, potentially including those proposed in our SR-IS model 
36
 . In simple environments, SR generates representations that share notable similarities with hippocampal place fields, producing radially symmetric activation patterns that adapt to environmental complexities. These adaptations parallel several documented properties of hippocampal place cells, including distortions around barriers 
[58]
[59]
[60]
 and directional skewing with repeated traversals 61 . However, a theoretical inconsistency challenges the relationship between SR and hippocampal functioning. While key place cell effects, such as directional skewing, have been simulated using the policy-dependent property of SR 
36
 , grid cell properties require a policy-independent successor map, as was previously noted 
51
 . The SR-IS model potentially resolves this inconsistency by naturally producing a degree of directional skewing through sampling variability, particularly for states rarely visited under the decision policy, while ultimately constructing its map under the default policy -thereby accommodating both policy-dependent place cell effects and policy-independent grid cell properties. Furthermore, unlike the strong directional dependency produced by the standard SR model under decision policies, the directional skewing in SR-IS remains subtle, better matching empirical observations.
Recent work building upon the linear RL framework, proposed that cognitive maps could be learned through compositional positioning of object and barrier representations within an open-space baseline map 
62
 . While SR-IS offers an alternative approach by creating maps purely through learning, these models can coexist complementarily -one explaining compositional map building and the other addressing learning-based map construction. Importantly, many situations, including the human revaluation tasks simulated in the current study, cannot be handled through compositional positioning of familiar barriers and objects. Thus, while the compositional model offers valuable insights, it cannot explain the specific behavioral patterns in revaluation tasks that are successfully captured by SR-IS.
Beyond cognitive science, our approach could guide the development of novel artificial intelligence systems that can plan efficiently and adapt to new situations. The successor representation has already influenced several advances in artificial intelligence, including the development of more generalized environmental features known as "successor features" 
[63]
[64]
[65]
 ) and methods for sub-goal discovery 
66;67
 . These applications leverage the SR's ability to create environmental representations that facilitate transfer learning, allowing systems to apply knowledge from one task to enhance performance on others 
68;69
 . We propose that incorporating importance sampling could provide a straightforward yet powerful enhancement to these approaches. By addressing bias in the SR, importance sampling should yield more generalized features and more effective sub-goal identification, while maintaining computational efficiency.


Methods
This section details the mathematical and computational framework underlying SR-IS, beginning with fundamental reinforcement learning concepts and building to our novel contributions. We first outline the basic reinforcement learning problem and temporal difference learning, then introduce the default representation that motivated our work. Finally, we describe how SR-IS enables efficient replanning and revaluation through importance sampling.


Reinforcement learning
Reinforcement learning addresses the problem of planning in a Markov decision process (MDP) framework, which consists of the following components 
70
 : a set of states (s), a set of actions (a), a transition distribution P (s ′ |s, a) specifying the probability of transitioning from state s to state s ′ after taking action a, a reward function R(s) giving the expected immediate reward in state s, and a discount factor γ ∈ [0, 1] that discounts future rewards. The main objective in reinforcement learning is to estimate the value of each state, which is defined as the expected total future reward from that state onwards. Here, the expectation is taken to account for any randomness that may result from probabilistic state transitions or reward distributions. The value of state s under policy π is defined as the expected discounted cumulative reward if the agent chooses actions according to policy π.
V π (s) = E ∞ t=0 γ t R(s t )|π, s 0 = s ,
(5)
where s t is the state visited at time t. The optimal value function is given by
V * (s) = R(s) + max a s ′ γP (s ′ |s, a)V * (s ′ ) .
(6)
Value functions are central to RL, whether in the form of optimal value functions V * (s) or policy-dependent values V π (s). The challenge of reinforcement learning can fundamentally be understood as learning to accurately predict these value functions. particularly the optimal value function. Two distinct approaches have been developed to tackle this challenge. Model-based algorithms represent the first approach. These methods explicitly learn the environment's dynamics by learning the transition and the reward function. Once these functions are learned, value iteration algorithm can be employed to compute V * recursively unfolding Equation 6 into nested computations. The second approach is model-free RL, which directly updates a cached value of the estimate itself using algorithms such as TD-learning 
39
 . After observing a transition s − → s ′ with a reward, R(s ′ ), we can calculate a reward prediction error, δ to update the value of our state, V (s).
δ = R(s) + γV (s ′ ) − V (s), V (s) ← V (s) + αδ,
(7)
where α is the learning rate parameter which controls the step size of the agent as it updates its estimate of the value function.


Successor representation
The SR, previously defined in Equations 1-2, has an important matrix formulation as shown by Russek et al 
37
 . Let T π be the one-step transition matrix under policy π, where element T π ij represents the probability of transitioning from state i to state j. This transition matrix can be written as:
T π (s, s ′ ) = a π(a|s)P (s ′ |s, a).
(8)
Using this transition matrix, the SR can be expressed as:
M π = (I − γT π ) −1 ,
(9)
where I is the identity matrix and γ is the discount factor. This matrix formulation of the SR will be crucial for developing our model.


Default representation
The linear RL model 51 provides a closed-form approximation to the optimal value function defined in Equation 6 for finite-horizon Markov decision problems, such as maze navigation towards a specific goal. At its core, this model optimizes behavior by maximizing a gain function that balances rewards against control costs. The gain function g for a state s is defined as:
g(s) = r(s) − λKL(π ∥ π d ),
(10)
where r(s) represents the reward at state s, and λ > 0 is a control cost parameter. The second term measures the divergence between two policies: the agent's decision policy π and a default policy π d under which the predictive map is built. This divergence is quantified using the Kullback-Leibler (KL) divergence, which equals zero only when the policies match exactly (π = π d ) and is positive otherwise. Throughout our simulations, we use a uniform default policy where all transitions to neighboring states are equally likely. A uniform random default policy introduces stochasticity into decision-making without biasing the decision policy toward any specific actions. It can be shown that the optimal value function V * (s) for this problem has an analytical solution, which can be viewed as an approximation to the value function of the original RL problem without control costs (Equation 6). To derive this solution, we first define the one-step state transition matrix T. Each element T ij represents the probability of transitioning from state i to state j under the default policy.
Next, we partition the state space into terminal states (containing goals) and nonterminal states. Let t = T g exp(r g /λ) be a vector determined by transitions from nonterminal to terminal states T g and their associated rewards r g . For a vector v * containing optimal values across all nonterminal states, we have:
exp(v * /λ) = D N N t,
(11)
where D N N is the submatrix of the default representation (DR) matrix D corresponding to nonterminal states. The DR matrix is defined as:
D = (diag(exp(−r/λ)) − T) −1 ,
(12)
where r is the reward vector across all states (assuming non-zero rewards at terminal states). If we assume that the reward at all nonterminal states is equal to cost c, then we can write D = γM, where we have defined γ = exp(−c/λ):
M = (I − γT) −1 ,
(13)
Throughout the paper, we generally refer to both D and M as the DR matrix because they are simply related to each other through the constant γ, except where this might cause confusion. The DR matrix, defined in Equation 12, shares a strong similarity with the SR matrix (Equation 9) -in fact, the DR matrix can be understood as the SR under the default policy. Moreover, γ can be interpreted as equivalent to the discount factor in Equation 9.


Replanning
A key advantage of the linear RL framework and its DR matrix is that it enables efficient low-rank updates to the predictive map when the environment changes. This contrasts with both classic model-based approaches and the SR, which typically require recalculating the entire map even for minor environmental changes.
Formally, when an environmental change modifies matrix L 0 = diag(exp(−r/λ)) − T to a new matrix L (due to alterations in either T, r, or both), we can apply the Woodbury matrix inversion lemma 
55;56
 . Given D 0 = L −1 0 , this lemma allows us to compute the new matrix D = L −1 through a computationally efficient low-rank update of D 0 . Now, let us introduce two sparse matrices R and C of size J × S and S × J respectively, where S is the total number of states and J is the number of states for which L 0 has changed. Let j = (j 1 , j 2 , . . . , j J ) be a vector of length J containing the indices of all those states, where j i denotes the i-th element of the vector. For each changed state, matrix R contains rows representing the difference between the corresponding rows in L and L 0 , such that R = L(j, :) − L 0 (j, :). Matrix C acts as a selection matrix: for each changed state j i , it contains a column with a value of 1 in row j i and zeros elsewhere. Using these matrices C and R, we can efficiently express the change in L 0 as:
L = L 0 + CR.
(14)
Using this formulation, we can apply the Woodbury matrix inversion lemma 
55;56
 to express D (the inverse of L) as a low-rank update of D 0 :
D = D 0 − D 0 CARD 0 ,
(15)
where A = (I − RD 0 C) −1 . While this formulation still requires matrix inversion for replanning, it operates on a much smaller matrix: A has dimensions J × J, where J is the number of changed states and is typically much smaller than the total number of states S.


Simulation Parameters
All of the maze simulations used a custom maze environment created using OpenAI's gym 
71
 . For all of the simulations we assumed a uniform default policy. Additionally, we use the cost to calculate the discount rate term, γ = exp(−c/λ), where c is the cost at non-terminal states.
For the convergence simulations in 
Figure 2a
,b a 7 × 7 four-room maze was considered. The temperature and lambda parameters were both set to 1, the reward across all of the states was -0.2, the learning rate was set to 0.1, the models were trained for 15k steps, and we averaged the mean error across 40 runs. Here we define a "step" as when the agent takes an action, receives a reward, and consequently updates its representation. The terminal state was set to the bottom-right of the maze. For the four-room revaluation problem in 
Figure  2a
,c we consider a the same 7 × 7 maze with the training goal state set to the top-right corner of the maze, the terminal state has a reward of 10 and the non-terminal states all have a reward of −0.1. For training the SR-IS model, we used a learning rate of 0.2 and both the temperature and lambda parameters were set to 1. We trained SR-IS for 30k steps and averaged across 20 runs. The parameters were all the same for the SR model except the non-terminal reward was 0. For the goal revaluation problem in 
Figure 2d
,e a 10 × 10 maze was considered. The initial goal state was set to be in the bottom right corner of the maze at state. The reward at non-terminal states was set to be −1 and the reward at the terminal state(s) was set to 10. We used a learning rate of 0.25, a temperature parameter of 1.5, a lambda of 1, and trained for 80k iterations. After initially calculating the DR for the first terminal state, we moved the terminal state to every other open state and re-used the initial DR to solve for an updated DR using Equation 15. For the model comparison planning problem in 
Figure 2d
,f we considered the same 10 × 10 maze as in the goal revaluation problem. For the Complete and SR-IS models the reward across all states was -0.8. For SR-IS, the learning rate was 0.05, and temperature and lambda were both 1. For the SR agent the reward at non-terminal states was 1, the reward at the terminal state was 10, the learning rate was 0.05, and the temperature was 1. We trained both the SR-IS and SR models for 80k steps and averaged the number of replanning steps across 20 simulations. It is important to note that unlike the analysis done by Piray & Daw 51 , which computed a goal-independent representation, we require a goal state to test SR-IS as with no goal state the learning policy would be random and we would not need importance sampling.
For the replication of the experiment by Momennejad et al. 
38
 in 
Figure 3a
-e we followed the payout structure from the paper, with the exception of setting the reward a non-terminal states to be -1. We set the learning rate to be 0.15, temperature to be 0.6, lambda to 10. We trained the agents in a similar fashion to how the participants experienced the task where they started 14 times from state 1, 7 times from states 2 and 3, and 2 times from each terminal state. At test time we changed the temperature to 1.0 and averaged our results across 800 runs. To make the SR and DR directly comparable in this simulation, we added auxiliary terminal states, with each one connected to a corresponding goal state. For the latent learning task in 
Figure 4a
-d, a 9 × 9 maze was considered. Initially, the agent learns a representation of the environment when the rewards for the terminal states are set to +5. After training, the reward in terminal state r 1 is changed to -5 and the representation is updated using equation (??). We used a learning rate of 0.2 and the temperature and beta values were both set to 1. We trained the agent for 2,500 steps. For the detour task in 
Figure 4e
-h a 9 × 9 maze was considered. The reward across the non-terminal states was set to -0.1 and the reward at the terminal state was set to 10, the learning rate was set to 0.55, the temperature was set to 2, lambda to 1, and we trained for 80k steps. For this experiment we had to introduce two extra "tricks" to make it work as we expected in a fewer number of iterations. The first addition was using learning rate decay 
72
 . For this we used exponential decay with an initial learning rate of 0.55, a decay rate of 0.99, and the decay steps to 150. The second trick we used was to add the minimum absolute z-value back to our z-values as the DR for a fewer number of iterations resulted in a sub-optimal application of the Woodbury update and we wanted to avoid negative values due to this.
For the simulation in 
Figure 5a
-f we used the source code provided by De Cothi et al. 
19
 . To simulate the SR-IS agent we modified their code for the SR agent by adding an importance sampling term outlined in equations (3) to the TD update for the SR agent. For the analysis in the main text, we computed the correlation between the reported human and rat path lengths with respect to the path lengths of the SR and SR-IS models. For the analysis in the supplementary material, we used the provided code for the original analysis, making a minor modification to it by adding our SR-IS agent.
FIGURE 1
1
FIGURE 1.4


Figure 2 :
2
SR-IS learns a more general representation than the SR and is capable of optimal replanning. (a) Four-room environment with agent start, blue circle, and initial reward state, r 1 , shown by the green square. (b) Mean absolute error convergence between complete and SR-IS models with/without importance sampling during navigation learning to r 1 . (c) Replanning comparison between SR and SR-IS models: after learning representation with respect to r 1 , agents replan to 8 new terminal states, r 2−9 , with 2 in the same room as r 1 and the other 6 evenly distributed in other rooms. SR-IS maintains efficiency across rooms while SR only replans effectively within the same room. (d) 10x10 maze with initial agent position shown by the blue circle and goal state, r 1 , shown by the green square. (e) Path lengths from start to all other states match optimal paths from exhaustive search, using single representation computed with respect to r 1 . (f ) Replanning performance comparison after initial training to r 1 : SR performs near random walk level, SR* with Woodbury update remains biased to previous reward, while SR-IS matches complete model performance in replanning to r 2 .


FIGURE 3 PhaseFigure 3 :
33
SR-IS demonstrates similar replanning biases to humans. (a) The underlying structure of the reward, policy, and transition revaluation experiment's. (b)


Figure 4 :
4
Performance of SR-IS on Tolman's tasks. The mazes (a), (b) represent the task during the training and testing phase respectively. As before the blue circle represents the agent's current location, the green square represents the reward location, and the muted orange, green, and purple represent the states available for transition by the agent. (c) Shows the probability of transitioning to each state with importance sampling, and (d) shows the probability without importance sampling. The model with importance sampling is able to correctly update its DR and avoid the barrier.


Figure 5 :
5
SR-IS produces a closer match to rat and human path lengths. The mazes shown in panels (a, d) represent two distinctive examples selected from a total set of 25 mazes, each demonstrating the challenge of policy dependence that arises when navigating from different starting states. For both maze 15 (b, c) and maze 22 (e, f ), the SR-IS model demonstrates superior performance by achieving higher correlation coefficients with respect to path length when compared to the standard SR model, indicating better consistency in path planning across different starting positions.














Introduction and removal of reward, and maze performance in rats. University of California publications in psychology




Charles
H
Edward Chace Tolman






Honzik


















Studies in spatial learning. ii. place learning versus response learning




Edward C Tolman






F
Benbow






D
Ritchie






Kalish








Journal of experimental psychology




36


3


221














Cognitive maps in rats and men




C
Edward






Tolman








Psychological Review
















Motivational control of goal-directed action




Anthony
Dickinson






Bernard
Balleine








Animal learning & behavior




22


1
















Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control




Yael
Nathaniel D Daw






Peter
Niv






Dayan








Nature neuroscience




8


12
















Reasoning, learning, and creativity: frontal lobe function and human decision-making




Anne
Collins






Etienne
Koechlin








PLoS biology




10


3


1001293














The Hippocampus as a Cognitive Map




O'
John






Lynn
Keefe






Nadel








Oxford University Press














E
J
Timothy






Timothy
H
Behrens






Muller






C
R
James






Shirley
Whittington






Mark






B
Alon






Kimberly
L
Baram






Zeb
Stachenfeld






Kurth-Nelson








What is a cognitive map? organizing knowledge for flexible behavior






100














How to build a cognitive map




C
R
James






David
Whittington






Mccaffary






J
W
Jacob






Timothy
Ej
Bakermans






Behrens








Nature neuroscience




25


10
















Human orbitofrontal cortex represents a cognitive map of state space




W
Nicolas






Ming
Bo
Schuck






Cai






C
Robert






Yael
Wilson






Niv








Neuron




91


6
















Two anatomically and computationally distinct learning signals predict changes to stimulus-outcome associations in hippocampus




Erie D Boorman






Jill
X
Vani G Rajendran






Tim
E
O'reilly






Behrens








Neuron




89


6
















Path integration and the neural basis of the'cognitive map'




Francesco
P
Bruce L Mcnaughton






Ole
Battaglia






Jensen






I
Edvard






May-Britt
Moser






Moser








Nature Reviews Neuroscience




7


8
















Space in the brain: how the hippocampal formation supports spatial cognition




Tom
Hartley






Colin
Lever






Neil
Burgess






John O'
Keefe








Philosophical Transactions of the Royal Society B: Biological Sciences




369


20120510














A neural substrate of prediction and reward




Wolfram
Schultz






Peter
Dayan






P Read
Montague








Science




275


5306
















Reinforcement learning: the good, the bad and the ugly




Peter
Dayan






Yael
Niv








Current opinion in neurobiology




18


2
















Reinforcement learning in the brain




Yael
Niv








Journal of Mathematical Psychology




53


3
















The algorithmic anatomy of model-based evaluation




D
Nathaniel






Peter
Daw






Dayan








Philosophical Transactions of the Royal Society B: Biological Sciences




369














Interplay of approximate planning strategies




J
M
Quentin






Níall
Huys






Paul
Lally






Neir
Faulkner






Erich
Eshel






Seifritz






J
Samuel






Peter
Gershman






Jonathan
P
Dayan






Roiser








Proceedings of the National Academy of Sciences




112


10
















Predictive maps in rats and humans for spatial navigation




Nils
William De Cothi






Eva-Maria
Nyberg






Carole
Griesbauer






Fiona
Ghanamé






Julie
M
Zisch






Lydia
Lefort






Coco
Fletcher






Sophie
Newton






Daniel
Renaudineau






Bendor








Current Biology




32


17
















Orbitofrontal cortex as a cognitive map of task space




Robert C Wilson






K
Yuji






Geoffrey
Takahashi






Yael
Schoenbaum






Niv








Neuron




81


2
















The successor representation: its computational logic and neural substrates




J
Samuel






Gershman








Journal of Neuroscience




38


33
















Prefrontal cortex as a meta-reinforcement learning system




X
Jane






Zeb
Wang






Dharshan
Kurth-Nelson






Dhruva
Kumaran






Hubert
Tirumala






Joel
Z
Soyer






Demis
Leibo






Matthew
Hassabis






Botvinick








Nature neuroscience




21


6
















Reinforcement learning, fast and slow




Matthew
Botvinick






Sam
Ritter






Jane
X
Wang






Zeb
Kurth-Nelson






Charles
Blundell






Demis
Hassabis








Trends in cognitive sciences




23


5
















An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment




Matthew R Nassar






C
Robert






Benjamin
Wilson






Joshua
I
Heasly






Gold








Journal of Neuroscience




30


37
















Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective




M
Matthew






Yael
Botvinick






Andew G
Niv






Barto








cognition




113


3
















Cognitive control over learning: creating, clustering, and generalizing task-set structure




G
E
Anne






Michael J
Collins






Frank








Psychological review




120


1


190














States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning




Jan
Gläscher






Nathaniel
Daw






Peter
Dayan






John P O'
Doherty








Neuron




66


4
















Speed/accuracy trade-off between the habitual and the goal-directed processes




Mehdi
Keramati






Amir
Dezfouli






Payam
Piray








PLoS computational biology




7


5


1002055














Model-based influences on humans' choices and striatal prediction errors




D
Nathaniel






Daw






J
Samuel






Ben
Gershman






Peter
Seymour






Raymond J
Dayan






Dolan








Neuron




69


6
















Goals and habits in the brain




J
Ray






Peter
Dolan






Dayan








Neuron




80


2
















Neural computations underlying arbitration between model-based and model-free learning




Shinsuke
Sang Wan Lee






John P O'
Shimojo






Doherty








Neuron




81


3
















Model-based learning protects against forming habits




Ross
Claire M Gillan






Elizabeth
A
Otto






Nathaniel
D
Phelps






Daw








Cognitive, Affective, & Behavioral Neuroscience




15
















Learning task-state representations




Yael
Niv








Nature neuroscience




22


10
















Improving generalization for temporal difference learning: The successor representation




Peter
Dayan








Neural computation




5


4
















The successor representation and temporal context




Samuel J Gershman






D
Christopher






Moore






T
Michael






Todd






A
Kenneth






Norman






B
Per






Sederberg








Neural Computation




24


6
















The hippocampus as a predictive map




L
Kimberly






Stachenfeld






M
Matthew






Samuel
J
Botvinick






Gershman








Nature neuroscience




20


11
















Predictive representations can link model-based reinforcement learning to model-free mechanisms




Ida
Evan M Russek






Momennejad






M
Matthew






Botvinick






J
Samuel






Nathaniel
D
Gershman






Daw








PLoS computational biology




13


9


1005768














The successor representation in human reinforcement learning




Ida
Momennejad






M
Evan






Jin
H
Russek






Cheong






M
Matthew






Nathaniel
Douglass
Botvinick






Samuel
J
Daw






Gershman








Nature human behaviour




1


9
















Learning to predict by the methods of temporal differences




S
Richard






Sutton








Machine learning




3
















A model of how the basal ganglia generate and use neural signals that predict reinforcement. Models of Information Processing in the Basal Ganglia




James
C
Houk






Joel
L
Davis






David
G
Beiser


















A framework for mesencephalic dopamine systems based on predictive hebbian learning




Peter
P Read Montague






Terrence
J
Dayan






Sejnowski








Journal of neuroscience




16


5
















Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making




Lorenz
Deserno






J
M
Quentin






Rebecca
Huys






Ralph
Boehme






Hans-Jochen
Buchert






Anthony
A
Heinze






Grace






J
Raymond






Andreas
Dolan






Florian
Heinz






Schlagenhauf








Proceedings of the National Academy of Sciences




112


5
















Variability in dopamine genes dissociates model-based and model-free reinforcement learning




B
Bradley






Kevin
G
Doll






Nathaniel
D
Bath






Michael J
Daw






Frank








Journal of Neuroscience




36


4
















Midbrain dopamine neurons compute inferred and cached value prediction errors in a common framework




F
Brian






Joshua
L
Sadacca






Geoffrey
Jones






Schoenbaum








5


13665












Dopamine selectively remediates 'model-based'reward learning: a computational approach




Madeleine
E
Sharp






Karin
Foerde






Nathaniel
D
Daw






Daphna
Shohamy








Brain




139


2
















Dopamine enhances model-based over model-free choice behavior




Klaus
Wunderlich






Peter
Smittenaar






Raymond J
Dolan








Neuron




75


3
















Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by pruning decision trees




J
M
Quentin






Neir
Huys






Eshel






O'
Elizabeth






Luke
Nions






Peter
Sheridan






Jonathan
P
Dayan






Roiser








PLoS computational biology




8


3


1002410














How green is the grass on the other side? frontopolar cortex and the evidence in favor of alternative courses of action




Timothy
Ej
Erie D Boorman






Behrens






W
Mark






Matthew Fs
Woolrich






Rushworth








Neuron




62


5
















Linearly-solvable markov decision problems




Emanuel
Todorov








Advances in neural information processing systems






19












Efficient computation of optimal actions




Emanuel
Todorov








Proceedings of the national academy of sciences


the national academy of sciences






106














Linear reinforcement learning in planning, grid fields, and cognitive control




Payam
Piray






Nathaniel
D
Daw








Nature communications




12


1


4942














Bayesian inference in econometric models using monte carlo integration




John
Geweke








Econometrica: Journal of the Econometric Society


















Importance sampling: a review




T
Surya






Robert
E
Tokdar






Kass








Wiley Interdisciplinary Reviews: Computational Statistics




2


1
















Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning




Doina
Richard S Sutton






Satinder
Precup






Singh








Artificial intelligence




112


1-2
















Inverting modified matrices




A
Max






Woodbury








Memorandum Rept. 42, Statistical Research Group










page 4. Princeton Univ.








Updating the inverse of a matrix




W
William






Hager








SIAM review




31


2
















Dyna, an integrated architecture for learning, planning, and reacting




S
Richard






Sutton








ACM Sigart Bulletin




2


4
















The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells




U
Robert






John
L
Muller






Kubie








Journal of Neuroscience




7


7
















Spatial firing properties of hippocampal ca1 populations in an environment containing two visually identical regions




E
William






Skaggs






Bruce L Mcnaughton








Journal of Neuroscience




18


20
















Local remapping of place cell firing in the tolman detour task




Alice
Alvernhe






Etienne
Save






Bruno
Poucet








European Journal of Neuroscience




33


9
















Experience-dependent asymmetric shape of hippocampal receptive fields




R
Mayank






Mehta






C
Michael






Matthew A
Quirk






Wilson








Neuron




25


3
















Reconciling flexibility and Efficiency: Medial Entorhinal Cortex Represents a Compositional Cognitive Map




Payam
Piray






Nathaniel
D
Daw




http://biorxiv.org/lookup/doi/10.1101/2024.05.16.594459


















Deep reinforcement learning with successor features for navigation across similar environments




Jingwei
Zhang






Jost
Tobias
Springenberg






Joschka
Boedecker






Wolfram
Burgard








2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)




IEEE
















Successor features for transfer in reinforcement learning




André
Barreto






Will
Dabney






Rémi
Munos






Jonathan
J
Hunt






Tom
Schaul






David
Hado P Van Hasselt






Silver








Advances in neural information processing systems






30














D
Tejas






Ardavan
Kulkarni






Simanta
Saeedi






Samuel
J
Gautam






Gershman




arXiv:1606.02396


Deep successor reinforcement learning










arXiv preprint








Eigenoption discovery through the deep successor representation




C
Marlos






Clemens
Machado






Xiaoxiao
Rosenbaum






Miao
Guo






Gerald
Liu






Murray
Tesauro






Campbell








International Conference on Learning Representations (ICLR)
















Count-based exploration with the successor representation




C
Marlos






Machado






G
Marc






Michael
Bellemare






Bowling








AAAI Conference on Artificial Intelligence (AAAI)




2020












Transfer learning for reinforcement learning domains: A survey




E
Matthew






Peter
Taylor






Stone








Journal of Machine Learning Research




10


7














Transfer in reinforcement learning: a framework and a survey




Alessandro
Lazaric








Reinforcement Learning: State-of-the-Art




Springer
















Reinforcement learning: An introduction




S
Richard






Andrew
G
Sutton






Barto








MIT press
















Greg
Brockman






Vicki
Cheung






Ludwig
Pettersson






Jonas
Schneider






John
Schulman






Jie
Tang






Wojciech
Zaremba




arXiv:1606.01540










Openai gym. arXiv preprint








A stochastic approximation method. The annals of mathematical statistics




Herbert
Robbins






Sutton
Monro





















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]