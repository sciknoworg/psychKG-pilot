You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Creative or Not? Hierarchical Diffusion Modeling of the Creative Evaluation Process
Creative ideas are essential for tackling today's problems, from personal obstacles, such as combining work and childcare during a pandemic lockdown, to societal challenges such as climate change 
(Hennessey & Amabile, 2010)
. It is no wonder that educators are increasingly including creativity in curricula and that managers consider creativity a key skill 
(Casner-Lotto & Barrington, 2006;
IBM, 2010)
. Divergent thinking tasks are often used to assess creative ability, the ability to produce original and useful ideas 
(Barron, 1955;
Runco & Jaeger, 2012;
Stein, 1953)
. Perhaps the most common divergent thinking measure is the alternative uses task (AUT; 
Guilford, 1967;
Runco & Acar, 2012)
. On the AUT, people are typically asked to come up with as many unusual uses as possible for a given object (e.g., "bath toy" for the object "brick") within a certain time interval. When solving the AUT, two main processes occur: ideation and evaluation 
(Basadur, 1995;
Guilford, 1967;
Runco & Acar, 2012)
. Ideation is the "brainstorm" phase where one comes up with ideas, and evaluation is the decision making phase where one judges which ideas are creative enough to pursue (or in the case of the AUT to list as a response). While most research has focused on ideation and how to improve it (e.g., 
Benedek, Fink, & Neubauer, 2006;
Forthmann et al., 2019)
, how people evaluate and select ideas is less understood 
(Grohman, Wodniecka, & Kłusak, 2006;
Ritter, van Baaren, & Dijksterhuis, 2012;
Silvia, 2008
). Yet, in the real world, it only takes one well-selected idea to solve a problem (e.g., the printing press reduced the cost and labor of printing books, thereby increasing literacy and making knowledge more accessible) or disregarding a good idea to lose the battle in innovative business (e.g., Blockbuster vs. Netflix; 
Randolph, 2019)
. In this paper, we take a cognitive modeling approach to fill this gap and study how people decide which ideas are creative or not on the AUT.
While the standard definition of creativity states that originality and utility are both needed for creativity, individuals may differ in how much they value these two dimensions.
Previous research has mainly focused on population-level effects and found that people tend CREATIVE OR NOT? 5 to value originality more than utility when judging the creativity of an idea 
(Caroff & Besançon, 2008;
Diedrich, Benedek, Jauk, & Neubauer, 2015;
Runco & Charles, 1993)
.
However, creativity is a widely used word and regularly discussed by laypeople 
(Davies, 2008;
Mueller, Melwani, Loewenstein, & Deal, 2018)
. Consequently, different people may have substantially different conceptions of how important originality and utility are for creativity (e.g., 
Loewenstein & Mueller, 2016)
. For example, some may value utility more than others when judging creativity, and some may even find ideas or products that are not useful more creative (see 
Haaf & Rouder, 2017
 for a discussion of individual differences).
In order to examine the evaluation phase of the AUT, we focus on the decision-making process of whether an alternative use is creative or not. Previous work on creative idea evaluation has primarily focused on how mental or emotional states, or instructions lead to better judgements of how creative ideas are, whether of people's own ideas or those of others (de Buisonjé, 
Ritter, de Bruin, ter Horst, & Meeldijk, 2017;
Grohman et al., 2006;
Herman & Reiter-Palmon, 2011;
Mastria, Agnoli, & Corazza, 2019;
Puente-Diaz, Cavazos-Arroyo, & Puerta-Sierra, 2021;
Rietzschel, Nijstad, & Stroebe, 2010;
Ritter et al., 2012;
Runco & Smith, 1992;
Silvia, 2008)
. In contrast, we use a process modeling approach to better understand the cognitive underpinnings of creativity evaluation and apply the commonly used two-choice response time (RT) paradigm from the decision making literature (e.g., 
Krypotos, Beckers, Kindt, & Wagenmakers, 2015;
Ratcliff, 1978;
Wagenmakers, Ratcliff, Gomez, & McKoon, 2008)
, here in the form of the Creative-or-Not task (CON-task). For example, given the object "book" one must decide, as fast as possible, whether using it as a "roof tile" is creative (or not). The CON-task stimuli vary on the two dimensions of originality and utility, allowing us to unobtrusively assess individuals' implicit values of originality and utility when judging creativity in the CON-task. Our task is somewhat similar to the creativity evaluation test 
(Benedek et al., 2016)
, a test to detect individual differences in creative evaluation ability amongst prospective secondary school teachers, as we also present others' ideas on the AUT to participants. However, where their focus was on assessing evaluation ability, we aim CREATIVE OR NOT? 6 to explore the decision-making process and individual differences in implicit conceptions of creativity. As such, responses are not correct or incorrect, but just the respondent's opinion.
To gain insight into the cognitive basis of the evaluation process, we model CON-task data using the drift diffusion model (DDM; 
Ratcliff, 1978;
Wabersich & Vandekerckhove, 2014)
. The DDM is a cognitive model of the processes during two-choice decisions . The model essentially assumes that people make decisions through noisy evidence accumulation (see 
Figure 1
). Over time, the individual supposedly accrues more and more information about a stimulus to answer a posed question and then ultimately crosses a decision boundary, for example to decide that "Yes, roof tile is a creative use for a book". For every new sample of information, the individual determines whether it is in line with the "Yes" or the "No" decision and thereby sequentially adds new to the old evidence 
(Wagenmakers, 2009)
. This information accumulation process ends when a certain threshold of evidence is reached.
The DDM has commonly been applied to studies of language, perception, or memory retrieval (e.g., 
Ratcliff, 1978
Ratcliff, , 2002
Ratcliff, Gomez, & McKoon, 2004)
 where participants make simple and fast decisions that are either correct or incorrect (e.g., Is CFREE a word or not? 
Meyer & Schvaneveldt, 1971)
. But it has also been used to model longer, value-based choices where there is no objectively correct response 
(Hutcherson, Bushong, & Rangel, 2015;
Krajbich, Armel, & Rangel, 2010;
Milosavljevic, Malmaud, Huth, Koch, & Rangel, 2010)
.
For example, 
Milosavljevic et al. (2010)
 showed that the DDM can computationally describe decisions in a binary food choice task. For two-choice decisions where a simple random walk is too simplistic, the DDM might not be complex enough. However, numerous complex decision-making models can be reduced to the DDM 
(Bogacz, Brown, Moehlis, Holmes, & Cohen, 2006;
van der Maas, Molenaar, Maris, Kievit, & Borsboom, 2011)
. Since the cognitive processes underlying creativity evaluation are largely unknown, we believe that the DDM serves as a good starting model for the evaluation phase of the AUT -also due to its useful psychometric properties: the DDM can be linked to the two parameter logistic (2PL) CREATIVE OR NOT? 7 model, a classic item response theory model to measure individual differences 
(Tuerlinckx & Boeck, 2005;
van der Maas et al., 2011)
. Furthermore, it separates potentially confounding processes related to stimulus encoding or motor control.
Taking into account both responses and RTs, the DDM allows us to explore the role of speed in creativity evaluation as assessed by the CON-task. The originality and utility of the CON-task stimuli may not only influence decision outcomes, but also decision speed. RTs tend to increase with difficulty . Accordingly, the more clearly creative a CON-task stimulus may be perceived, the faster it would be judged. Since highly original and useful ideas tend to be considered creative, they might be more easily evaluated regarding creativity than medium original ones and they might be judged faster. Using this logic, highly unoriginal stimuli might also be evaluated faster than medium original ones.
Altogether, this would suggest an inverted u-or even v-shaped relationship between originality and RT. The same might be the case for utility. In this paper, we explore this idea by measuring both decision outcomes and decision speed in the CON-task. As a plausibility check for our cognitive modeling approach, we also examine how well CREATIVE OR NOT? 8 individuals' implicit values and their explicit opinions about the relevance of the two dimensions for creativity are aligned. Furthermore, we investigate whether those who value originality (or utility) also tend to produce more original (or useful) AUT responses, respectively. Regarding the latter, we build on 
Caroff and Besançon (2008)
's study who found a positive relationship for originality and called for research examining the same for "appropriateness" (i.e., a term often used as synonym for the utility dimension).
To this end, we conducted two studies. Study 1 is exploratory where we initially fit the DDM model. In Study 2 we validate our pre-registered model from Study 1 and test specific hypotheses based on Study 1's results.


General Method Bayesian Hierarchical Diffusion Modeling of the CON-task
The DDM conceptualizes the response process in the CON-task as an interaction of several unobservable cognitive processes 
(Ratcliff, 1978;
Vandekerckhove, Tuerlinckx, & Lee, 2011)
. Each of these is represented by a parameter (see 
Figure 1
). We use the simplest complete version of the DDM comprising four parameters 
(Ratcliff, 1978;
Wabersich & Vandekerckhove, 2014)
. First, the model assumes that the decision whether a use is creative or not is initially determined by β, which reflects the a priori bias towards either choice, regardless of stimulus characteristics.
Applied to the CON-task, this is an initial preference for "Yes, creative" or "No, not creative". Second, according to the DDM, individuals gradually extract and accumulate noisy information from the stimulus regarding its creativity, which in turn determines the drift rate δ, the tendency to respond "creative" or "not creative". Positive values suggest a drift towards the upper boundary and negative ones a drift towards the lower boundary.
Drift rates around zero suggest that a stimulus is perceived as ambiguous. The higher the absolute drift rate, the easier and faster the creativity evaluation, and the stronger the CREATIVE OR NOT? 9 evidence for the decision. The evidence accumulation ends when either of the two decision boundaries is reached. Third, the boundary separation parameter α reflects the distance between the two response boundaries and can be interpreted as response caution, where more hesitant creativity judges have a greater boundary separation. Finally, the parameter τ refers to the non-decision time. This parameter captures the processes taking place before and after the actual decision process such as stimulus encoding and motor control processes.
In this paper, the most central DDM parameter is the drift rate. The assumption is that stimulus originality and utility both positively affect the drift rate in that the more original and useful a CON-task stimulus is, the greater the tendency to respond "creative".
Moreover, the drift rate is the only model parameter that is influenced by stimulus characteristics because the remaining parameters are already set before the creativity evaluation starts (e.g., 
Vandekerckhove et al., 2011)
.
Bayesian hierarchical modeling. We estimated the model in a Bayesian hierarchical framework 
(Lee, 2011;
Rouder & Lu, 2005;
Vandekerckhove et al., 2011)
, allowing us to examine the data both at the population-level and at the individual-level.
Hierarchical modeling provides rather conservative estimates of individual differences because it shrinks the individual effects towards the population mean (e.g., 
Efron & Morris, 1977;
Haaf & Rouder, 2018)
.
We chose Bayesian estimation for three reasons. First, even without a hierarchical extension, applying the DDM to data is computationally expensive (e.g., 
Tuerlinckx, 2004)
.
Extending it hierarchically makes the model quickly intractable when using the frequentist approach of maximum likelihood estimation 
(Vandekerckhove et al., 2011)
. Second, Bayesian inference has several advantages such as an intuitive treatment of uncertainty regarding the model parameters 
(Wagenmakers, 2009)
. Third, Bayesian hierarchical modeling is the preferred method for small trial numbers as simulation studies suggest that this method can recover individual variation relatively successfully even with small numbers of observations per participant 
(Ratcliff & Childers, 2015)
.


CREATIVE OR NOT?
10


Model Specification
A detailed and complete model specification of the DDM used in Study 1 and 2 can be found in Appendix A. Here, we describe how we decomposed the drift rate parameter and the hierarchical structure of the model. To explore the influence of originality and utility when judging creativity in the CON-task, we regressed the drift rate on the originality and utility ratings of the stimuli. In both studies, we included random intercepts and random slopes to explore individual differences. Furthermore, because the response times and proportions of "creative" responses vary considerably across the 64 CON-task stimuli, we also included random intercepts pertaining to the stimuli.
In both studies, we decomposed the drift rate as follows. Let δ (ij) denote the drift rate for the ith participant, i = 1, ..., I, in the jth trial or stimulus, j = 1, ..., 64, of the CON-task, then
δ (ij) = θ δ(i) + φ δ(j) + θ OR(i) z OR(j) + θ U T (i) z U T (j) . The parameters θ δ(i) , θ OR(i) , θ U T (i)
, and φ δ(j) reflect the drift rate decomposition.
Specifically, θ δ(i) denotes the drift rate intercept, representing individual i's drift rate for stimuli with average originality and utility ratings. φ δ(j) is stimulus j's deviation from the drift rate intercept. Furthermore, z OR(j) and z U T (j) are z-scores of the originality and utility of stimulus j. Lastly, θ OR(i) denotes the originality effect, and θ U T (i) the utility effect of individual i on the drift rate.
For most of the remaining DDM parameters, we also incorporated random effects to examine individual differences. In particular, we allowed the boundary separation and the bias parameter to vary across individuals. However, because in Study 1 we encountered identifiability issues when estimating random effects for β, we fixed the bias at the population level. Another exception is that we constrained the non-decision parameter, τ , to CREATIVE OR NOT? 11 be constant across participants in both studies because interpreting random effects for this parameter has shown to be problematic 
(Singmann, 2018b)
.
To examine the interplay of the DDM parameters across participants, we also allowed the random effects pertaining to individuals to be correlated. As such, we assume that the individual effects are drawn from the same multivariate normal distribution with population
means [µ δ , µ θ OR , µ θ U T , µ α , µ β ]
T and a variance-covariance matrix Σ, i.e.,
                θ δ(i) θ OR(i) θ U T (i) α (i) β (i)                 ∼ Multivariate-Normal                                 µ δ µ θ OR µ θ U T µ α µ β                 , Σ                 .
Σ allows for correlations across the random effects pertaining to the individuals. The random effects of the stimuli are orthogonal to the individual random effects. They are also assumed to be randomly sampled from a population distribution (of stimuli),
φ δ(j) ∼ Normal(0, σ δ φ ),
where 0 is the mean and σ δ φ is the standard deviation.
Since we estimated the model in the Bayesian framework, we needed to specify a prior distribution for each parameter. For Study 1, we used weakly informative priors that restricted the parameter space to a plausible range (see Appendix A). For Study 2, we used the insights gained from Study 1 and specified informative priors to test hypotheses in the Bayesian setting. We discuss these prior choices in the Methods, with details in Appendix A.  
Guilford, 1967)
 used to assess their divergent thinking performance. The name of an object was presented on the screen, and participants had two minutes to type as many creative uses for the object as possible (e.g., the use "bath toy" for the object "brick"). During the session, participants were asked to generate uses for two objects, either "brick" and "fork", "fork" and "paperclip". or "paperclip" and "towel". The pairs were counter-balanced over participants. Generated solutions were listed on the screen and new ones were continuously added. Two independent raters who were unaware of the research questions/hypotheses of this study separately scored participants' answers with respect to originality and utility on a five-point scale (1 = not original/useful, 5 = very original/useful). Invalid responses were coded as 0. To assess interrater reliability, we computed the intraclass correlation coefficient (ICC) 2 . The ICC for the originality scores
1 The Alternative Uses Task was administered in at least one other study during the six testing sessions but no other study made use of the CON-task.
2 Since we considered both responses and raters as random effects and considered consistency in ratings more important than absolute agreement, we used a twoway model and computed single score ICCs of the type consistency using the irr package throughout this paper 
(Gamer, Lemon, & Singh, 2019
 .71] for "brick", "fork", "paperclip" and "towel"
respectively. As performance indicators, we used the mean originality and mean utility score across raters, objects, and responses.
Creative-or-not Task. Participants completed 64 trials of the CON-task. The instructions were in Dutch and read the following: "In a moment, you'll see other people's answers to the 'Creative Uses task'. We would like to know if you think the answers are creative or not creative. Decide as quickly as you can. We will do this task four times, each time with a different object (such as book). You will be shown 16 ideas for each object." On each trial, they were asked "Do you think this use for [object] is creative?", followed by a specific use. Importantly, participants were not instructed regarding the criteria they should apply when deciding whether they find a use creative or not. RTs as well as responses ("creative" or "not creative") were recorded. Trials automatically counted as missing when
participants did not answer within nine seconds. The stimuli used had been selected from a collection of AUT responses. Their originality and utility had been independently scored on a scale from 1 to 5 by two creativity researchers. The ICC was 0.88 95%CI [0.80, 0.92] for originality and 0.65 [.49, 0.77] for utility. As stimulus ratings, we took the average originality and utility rating, respectively, across raters. The mean originality rating of the stimuli was M = 2.98 (SD = 1.20), and the mean utility rating was M = 3.37 (SD = 1.07). Stimulus originality and utility were negatively correlated, r = -0.61, 95% credible interval (CrI)
[-0.91, -0.45], BF 10 = 1.43 × 10 63 . The correlation is representative of the trade-off between compute the ICCs for the AUT objects separately, we used all collected AUT responses during the testing sessions and not only the responses of participants who also completed the CON-task. the two dimensions "originality" and "utility" that is often found in AUT responses (e.g., 
Rietzschel et al., 2010;
Runco & Charles, 1993)
.
Importance Ratings of Originality and Utility. After the CON-task, participants indicated, separately, to what extent they thought utility, innovativeness, originality and appropriateness played a role when evaluating creativity (1 = not important at all to 5 = very important).


Results Study 1
The data cleaning is described in Appendix B. The cleaned dataset comprised 293 participants and 17806 trials. The mean RT across participants and trials after excluding data was 1.86 seconds (M edian = 1.61, SD = 0.92). The overall RT distribution is shown in Appendix C. The overall percentage of "creative" responses was 53.83%. The mean RT for "creative" responses across participants and trials was 1.87 s (SD = 0.93), and 1.86 s (SD = 0.92) for "not creative" responses. 
Figure 2A
 shows that RTs for some stimuli were longer than for others. A Bayesian correlation analysis with median-split stimulus data suggested weak evidence for a correlation between RT and originality in the high-utility stimulus group, r = 0.33, 95%CrI [0.04, 0.57], BF 10 = 4.70, and anecdotal evidence for no relationship between originality and RT in the low-utility stimulus group 4 , r = 0.17, 95%CrI 
[-0.19, 0
.50],
factors to quantify the evidence for a correlation (H 1 : ρ = 0) as opposed to no correlation (H 0 : ρ = 0; ) and report it together with the posterior mean of the correlation coefficient and the corresponding credible interval.
BF 01 = 1.47 (see 
Figure 2A
). Furthermore, 
Figure 2B
 and a Bayesian paired t-test analysis suggested that participants, on average, responded equally fast with "not creative", M = 1.88 s, SD = 0.39 s, as opposed to "creative" M = 1.89 s, SD = 0.43 s, BF 01 = 11.32.
Model Fit. We fitted the DDM using the R package brms  which works with Stan to draw samples from the posterior distribution of Bayesian models 
(Carpenter et al., 2017)
. We ran 4 chains with 5000 iterations each. 1500 iterations per chain were used as warmup to adapt the sampler. Consequently, our analyses were based on a total of 14000 iterations. 5
We decided to not allow the bias parameter β to vary across individuals because when we did, the random effects of the bias and drift rate intercept were highly correlated, suggesting identifiability issues. We therefore estimated the DDM with only a population-level bias parameter. The remaining DDM parameters were not affected by this change.
We performed several model diagnostics procedures and inspected the model fit. There were no signs of non-convergence, with 0 divergent transitions andR values 
(Gelman & 5
 For all analyses, we used R (Version 3.6.1; R Core Team, 2019) and the R-packages BayesFactor (Version Rubin, 1992) below 1.01 
(Vehtari, Gelman, Simpson, Carpenter, & Bürkner, 2020)
.
Additionally, we assessed the model fit using posterior predictive checks (see the online supplementary material). Overall, apart from some misfit in the outer quantiles of the RT distribution, the DDM could reproduce the data quite accurately and appeared to provide an acceptable account of the data.
Modeling Results. A summary of the estimated fixed and random effects parameters can be found in 
Table 1
 and 2, respectively, and a summary table with the random effects correlation parameters in Appendix B.
Regarding the fixed effects, our main focus of interest was on µ θ OR and µ θ U T , the overall effects of stimulus originality and utility on the drift rate δ. For µ θ OR , the posterior mean was 0.41, and the 95% CrI was [0.31, 0.50]. For µ θ U T , the posterior mean was 0.10, and the 95% CrI was [0.00, 0.19]. Both posterior means were positive. However, while the 95% credible interval (CrI) of µ θ OR did not include zero, the CrI of µ θ U T was very close to zero.
In general, all estimated posterior means of the remaining fixed effects parameters seem plausible as the CrIs were rather narrow and the parameters lie within a reasonable range (see 
Table 1
). On average, there was no a-priori bias towards the response options "creative" or "not creative" (see µ β in 
Table 1
). This suggests that participants were on average equally likely to choose either of the two response options before stimulus onset.
However, the boundary separation, or response caution, was higher than found in most applications of the diffusion model (e.g., 
Matzke & Wagenmakers, 2009
, see µ α in 
Table 1
).
One explanation for this rather high value is that the RTs in the CON-task were considerably slower than RTs in tasks typically modeled by the diffusion model.
We found substantial individual differences in all variability parameters. Notably, the results showed substantial variability across participants in the originality and utility effects on the drift rate. The posterior means of σ OR and σ U T were 0.23 and 0.19, respectively. The posterior means and credible intervals of all variability parameters are listed in suggesting that, at the very least, for some individuals, the effect of stimulus originality on the drift rate was weaker and for some it was stronger.
Participants further differed in their utility slopes. As shown in 
Figure 3B
, for some, the utility effect on the drift was around zero, for some it was positive, and for a few the effect was even negative. Specifically, the majority of the individual CrIs (n = 207; 70.65%)
included zero, 80 (27.30%) of them excluded and were above zero, and 6 (2.05%) individual
CrIs excluded and were below zero.
Individual differences also manifested themselves in a negative correlation between the originality and utility slopes. Here the posterior mean of ρ σ θ OR σ θ U T was -0.44, 95%CrI 
[-0.60, -0.27]
, and the correlation between the individual originality and utility slopes, based on the posterior means, was r = -0.67, 95%CrI [-0.73, -0.61], BF 10 = 6.33 × 10 37 . It is expected that r is greater than ρ because ρ σ θ OR σ θ U T is a population parameter taking uncertainty into account and r reflects the data in our sample. The greater the individual effect of stimulus originality on the drift rate, the smaller the effect of stimulus utility and vice versa. This correlation could also be explained by the substantial negative correlation between the originality and utility ratings for the CON-task stimuli (r = -0.61). We ruled this out by re-estimating the DDM when excluding the stimuli that contributed the strongest to the negative correlation. Specifically, we excluded the data from 20 items leaving us with  random effects parameters can be found in 
Table A1
 in Appendix D. 
Figure 4B
 further depicts two clusters that are the results from an exploratory k-means cluster analysis: it seems that one cluster comprises individuals with a positive effect of utility and a rather small effect of stimulus originality, and the other one individuals with a stimulus utility effect around zero and a positive effect of originality.


CON-task and self-report ratings of originality and utility. As a plaubility
check for our rationale behind the drift rate regression, we examined whether the self-reported importance ratings of originality, innovativeness, utility, and appropriateness corresponded to the originality and utility effects on the drift rate. We summed up the ratings of appropriateness and utility and innovativeness and originality, respectively. There was a positive correlation between participants' importance ratings of originality and the posterior means of their originality slopes, r = 0.32, 95%CrI [0.21, 0.42], BF 10 = 1.04 × 10 6
and between their ratings of utility and utility slopes, r = 0.36, 95%CrI [0.26, 0.45] ,
BF 10 = 1.66 × 10 8 . The more participants indicated that originality was important when determining whether something is creative or not, the greater their influence of stimulus originality on their drift rate. The more they indicated that utility was important, the greater their effect of stimulus utility on their drift rate. There were also negative correlations between the originality importance ratings and the utility slopes, r = -0.33, CREATIVE OR NOT?
19 CON-task and divergent thinking. Given the substantial variability in the stimulus originality and utility effects on the drift rate, we explored whether this variability was related to variability in divergent thinking performance as assessed by the AUT. To this end, we computed correlations among the individual posterior means and participants' AUT performance scores. The data cleaning for the AUT task is described in Appendix B.
We found a positive correlation between AUT originality scores and the originality slope posterior means, r = 0.18, 95%CrI [0.06, 0.30] , BF 10 = 8.41, suggesting that the more original the AUT responses, the greater the influence of originality on the drift rate in the CON task. We also found a negative correlation between the posterior means of the utility slopes and participants originality scores, r = -0.16, 95%CrI [-0.28, -0.04], BF 10 = 3.11. The more original responses participants produced in the AUT, the smaller their effects of stimulus utility on the drift rate. However, there was no correlation between the AUT utility scores and the stimulus utility effects on the drift rate, r = 0.02, 95%CrI 
[-0.11, 0
.15] , BF 01 = 6.15, and also no correlation between AUT utility scores and stimulus originality effects, r = -0.03, 95%CrI [-0.16,0.10], BF 01 = 5.82.
Since the application of the DDM to creativity is novel and since Study 1 was conducted in a rather exploratory manner, we aimed to assess the robustness of our findings in a second, preregistered Study (https://osf.io/7gt45/). In Study 2, we therefore specified hypotheses based on Study 1's results as well as previous research and re-fitted the DDM on an independent dataset.


Study 2
Based on what we had learned from Study 1 and based on previous research, we expected positive effects of stimulus originality (H1) and stimulus utility (H2) on the drift rate. We also expected the effect of stimulus originality to be larger than the effect of stimulus utility (H3). Furthermore, given the observed substantial negative correlation between the individual stimulus originality and utility effects in Study 1, we expected a CREATIVE OR NOT? 20 negative correlation among those effects in Study 2 (H4). Specifically, we hypothesized that the greater an individual's effect of stimulus originality on the drift rate, the smaller the effect of stimulus utility would be and vice versa. Since in Study 1, we observed substantial individual differences in the extent to which stimulus originality and utility influenced the drift rate, we also expected non-zero variability across individuals in those effects (H5a and H6a).
Previous research suggests that originality plays a superior role in creativity judgments compared to utility (e.g., 
Caroff & Besançon, 2008;
Diedrich et al., 2015;
Runco & Charles, 1993
). Based on these findings, we tested the hypothesis that everyone would have a positive effect of stimulus originality on the drift rate (H5b). Note that in Study 1, a few participants seemed to have a negative effect of originality. However, because this finding concerned only few participants and because we were not aware of any theory or research supporting it, we did not consider it robust enough to inform Study 2. Instead we decided to quantify the evidence for the ordinal constraint that everyone has a positive effect in Study 2 
(Haaf & Rouder, 2017
. Regarding the effects of stimulus utility on the drift rate, we expected that some individuals would have a positive effect, some a negative effect, and some no effect (H6b). Given Study 1's results, we also expected that individuals' stimulus originality and utility effects would be positively associated with their self-reported importance ratings of originality and utility for creativity. More specifically, we expected individual originality effects on the drift rate would increase as the self-reported importance ratings of originality increase (H7a), and individual utility effects on the drift rate would increase as the importance ratings of utility increase (H7b).
Finally, we specified hypotheses regarding the association between CON-task judgements and AUT performance. We expected that originality scores on the AUT would be positively correlated with stimulus originality effects and negatively correlated with stimulus utility effects on drift rates (H8a and H8b). Similarly, we expected that AUT utility scores would be positively correlated with stimulus utility effects on CON-task drift rates CREATIVE OR NOT? 21 and negatively correlated with stimulus originality effects (H9a and H9b). Note that Study 1 did not support H9a and H9b. However, in both cases, we did not consider the evidence for the null to be convincing. Based on common sense, we still expected that the more useful one's AUT responses are, the more one values utility when judging creativity and the more one disregards originality -also since studies suggest that the more creative someone is, the better they are at judging creativity (e.g., 
Benedek et al., 2016;
Silvia, 2008)
. Study 2 served as a robustness check for this belief.


Data Collection Procedure and Materials
As in Study 1, data collection was centrally organized by the faculty of Psychology at the University of Amsterdam and took again place over different sessions. All tasks related to creativity were administered during the same session in the order listed below. In total, 172 first-year psychology students completed the CON-task, the age range was 17-47 years, (M = 20.50, SD = 2.98, and participants again received course credit for their participation.
Alternative-Uses-Task Participants completed the AUT 
(Guilford, 1967)
 for the objects "brick" and "paperclip" and were again given two minutes for each object. Two independent raters who were unaware of the research questions/hypotheses of this study again separately scored participants' answers with respect to originality and utility on a five-point scale and coded invalid responses as zero. Interrater reliability as assessed by
ICCs 6 can be considered moderate to good: for the object "brick", the ICC was 0. As in Study 1, we used a twoway model and computed single score ICCs of the type consistency using the irr package 
(Gamer, Lemon, & Singh, 2019)
. Note that to compute the ICCs for the objects separately, we again used all collected AUT responses and not only the responses of the participants who also completed the CON task.


CREATIVE OR NOT?
22 raters, objects, and responses.
Creative-Or-Not task Participants again completed the same 64 stimuli as the participants in Study 1.
Self-reported importance-ratings On four separate items, participants again indicated after the CON-task how important they thought originality, innovativeness, appropriateness, and utility were when deciding whether something is creative or not.


Results Study 2
The hypotheses and analysis plan for Study 2 were preregistered before seeing the data.
We employed the same exclusion criteria as in Study 1 (see Appendix B). The descriptives were very similar to Study 1. The mean RT across participants and trials was 1.62 s and the median 1.44 s (SD = 0.79). The overall RT distribution is shown in Appendix C. The overall proportion of "creative" responses was 0.57. The mean RT for "creative" responses was 1.62 s (SD = 0.78). For "not creative" responses, this was 1.63 s (SD = 0.79). Stimuli with higher scores on originality and utility were again answered more slowly (see 
Figures 2C)
 Furthermore, as in Study 1, 
Figure 2D
 and a Bayesian paired t-test analysis suggested that participants, on average, responded equally fast when they answered "not creative", (M = 1.65 s, SD = 0.38 s), as opposed to "creative" M = 1.64 s, SD = 0.40 s, BF 01 = 10.96.


Low Utility High Utility
Model Fit. In contrast to Study 1, we used informative priors for the stimulus originality and utility effects on the drift rate, based on Study 1's estimation results.
Specifically, we specified truncated normal distributions as priors for µ θ OR , and
µ θ U T , µ θ OR , µ θ U T ∼ Normal + (0, 0.2), .
All remaining priors were the same as in Study 1 except for β (see Appendix A). We again fitted the model using the R package brms , ran 4 chains with 4000 iterations of which 500 iterations per chain were used as warmup, leaving us with 14000 iterations to base the analysis on.
We again inspected the model diagnostics and model fit. There were no signs of non-convergence, with 0 divergent transitions, andR values below 1.01 
(Vehtari et al., 2020)
.
Moreover, we again assessed the model fit using posterior predictive checks. The model fit was similar as in Study 1 and overall acceptable (see the online supplementary material). As a robustness check, we also re-estimated the model including participants that we excluded based on exclusion criterion 3 (fewer than 47 remaining trials; see the online supplementary materials).
7
The exact results again depended on whether the median of the stimuli's utility ratings was included in the low-or the high-utility group. When the median was assigned to the low-utility group, the evidence for the correlation between originality and RT in the high-utility group was even smaller: r = 0.27, 95%CrI [-0.07, 0.56], BF 10 = 1.52. However, the Bayes factor in favor of no correlation between originality and RT in the low-utility group was slightly bigger, r = 0.10, 95%CrI [-0.21, 0.39], BF 01 = 2.21
Modeling Results. Summary statistics of the estimated model parameters are shown in 
Table 1 and 2. A table with the correlations among the random effects can be
 found in Appendix B. Overall, the estimated DDM parameters were very similar to the ones in Study 1.  
(Dickey, 1971)
 of approximating Bayes factors. In this method, the Bayes factor is computed by a ratio of the prior and posterior density at the value zero. Assessing H1, we computed a Bayes factor comparing how well the hypothesis of a positive effect of stimulus originality on the drift rate predicted the data in comparison to the null hypothesis. All posterior samples were greater than zero. Therefore, the evidence in favor of H1 can be regarded as greater than 14000. In contrast, there was mere anecdotal evidence for an effect of utility on the drift rate, the Bayes factor was 1.88. This means that the data was 1.88 times more likely to have occurred under H2 than under the null hypothesis. Not suprisingly, the Bayes factor for H3 that the overall effect of stimulus originality is greater than the effect of stimulus utility CREATIVE OR NOT? 27 (rather than the other way around), was again greater than 14000.
We examined the correlation between the stimulus originality and utility effects on the drift rate (H4). The random effects correlation as estimated by the model, ρ σ θ OR σ θ U T , had a posterior mean of -0.34, 95%CrI [−0.59, −0.09], and the correlation based on the individual posterior means was r = -0.64 95%CrI 
[-0.73, -0
.54], BF 10 = 2.79 × 10 16 . We again explored the robustness of the correlation between the originality and utility effects on the drift rate and by estimating the DDM based on data from a subset of weakly correlated stimuli (see Study 1) 8 . Unlike in Study 1, the correlation between the effects became considerably smaller.
The posterior mean of the correlation was reduced to the size of the stimulus correlation in the reduced item pool (r = -.17), suggesting that the negative correlation between originality and utility stimulus effects on the drift rate was not as robust as in Study 1.
The individual differences were again reflected in the variability parameters of the originality and utility effects on the drift rate. The posterior mean of the variability parameters for σ OR was 0.20 and for σ U T this was also 0.20. The Savage-Dickey density ratio cannot be used to compute a Bayes factor quantifying the evidence for individual variability.
However, the 95% credible intervals of both variability parameters did not include zero,  In line with 
Rouder (2017, 2018)
, we tested the hypothesis that all individuals have a positive effect of stimulus originality on the drift rate using the encompassing prior approach (H5b; 
Klugkist, Laudy, & Hoijtink, 2005;
Klugkist & Hoijtink, 2007)
. Here, we compared the predictive accuracy of the hypothesis that everyone's originality slope is positive to the hypothesis that originality effects can be positive, zero or CREATIVE OR NOT? 29 negative. The constraint that everyone has a positive effect was not fulfilled in any of the iterations. Therefore, the Bayes factor for the latter, unconstrained hypothesis can be considered at least 2,261.16, (assuming that the next iteration might be the first in favor of the ordinal constraint) suggesting that not everyone has a positive originality effect on the drift rate. Regarding the individual utility effects, we tested the hypothesis that some individuals have a positive effect of stimulus utility on the drift rate, some have a negative effect, and some have no effect (H6b). Here, the data provided overwhelming support for the unconstrained hypothesis H6b over the hypothesis that everyone has a positive utility effect as the Bayes factor was again at least 2,261.16 (assuming that the next iteration might be the first in favor of the ordinal constraint).
θ UT(i) B −0.7 −0.5 −0.3 −0.1 0.1 0.3 0.5 0.7 0.9 1.1 θ OR(i) C −0.7 −0.5 −0.3 −0.1 0.1 0.3 0.5 0.7 0.9 1.1 θ UT(i) D S1 S2


CON-task and self-report ratings of originality and utility.
To test the hypothesis that stimulus originality and utility effects on the drift rate were positively correlated with self-reported importance ratings of originality and utility, respectively (H7a plots depict the negative correlation between the originality and utility effects.


CON-task and divergent thinking.
To examine associations between the stimulus originality and utility effects on the drift rate and creative performance (H8 and H9), we followed the same data cleaning procedures as described in Study 1. Since we had directed hypotheses, we used one-sided correlation tests. There was a positive correlation between the stimulus originality effects and the AUT originality scores, r = 0.21, 95%CrI
CREATIVE OR NOT?


31
[0.05, 0.36], BF = 6.32. However, there was no evidence for a correlation between AUT originality scores and the stimulus utility effects from the CON-task, r = -0.13, 95%CrI
[-0.28, -0.01], BF 01 = 1.19 (H8b). Moreover, we considered the evidence to be inconclusive as the data were only 1.19 times more likely to have occured under the null hypothesis rather than hypothesis H8b. Furthermore, there was a positive correlation between the AUT utility scores and the stimulus utility effects, r = 0.20, 95%CrI [0.05, 0.36], BF 10 = 5.90 (H9a), and a negative correlation between the AUT utility scores and the stimulus originality effects on the drift rate, r = -0.26, 95%CrI [-0.41, -0.10], BF 10 = 34.54 (H9b).
CREATIVE OR NOT? 32


Discussion
In this paper, we aimed to study the cognitive basis of how people decide whether an idea is creative or not during divergent thinking. Divergent thinking is a two-phase process where people come up with ideas (ideation phase) and decide whether or not to use them (evaluation phase; 
Basadur, 1995;
Guilford, 1967;
Runco & Basadur, 1993)
. We focused on the evaluation phase and used Bayesian cognitive modeling to better understand it. Using a novel application of the drift diffusion model (DDM) we aimed to understand how people weigh the two components of creativity, originality and utility, when deciding whether an idea is creative or not. We used the Creative-or-Not (CON) task, a timed two-choice decision-making task, where people are presented with ideas of how to use an everyday object (e.g., use a book as a roof tile) and then decide whether these are creative or not. The CON-task stimuli varied on how original and how useful they were (e.g., using a book to read is useful, but not original, whereas using a book as roof tile can be both). This way we could estimate individual differences in how originality and utility implicitly contributed to people's creativity judgements by examining their effect on the DDM drift rate parameter.
The drift rate represents people's tendency to respond "creative" or "not creative". The higher its absolute value, the greater the accumulated evidence for creative-or-not decisions, and the faster the response. Our main findings were: (1) stimulus originality strongly influenced participants' tendency towards creative responses (i.e., the drift rate), whereas stimulus utility was only somewhat related to it;
(2) there were large individual differences in how much participants took the originality and utility of presented ideas into account (i.e., varying individual effects of stimulus originality and utility on the drift rate); (3) participants' implicit tendencies or values of originality and utility on the CON-task were aligned with their self-reported importance ratings of originality and utility for creativity; (4) alternative uses task (AUT) scores for originality and utility coincided with how much the originality and utility of the presented CON-task idea weighed in on their decisions. We now CREATIVE OR NOT? 33 discuss each of these results in more detail.
Our finding that people value originality when evaluating creativity, evidenced by a substantial overall effect of originality on DDM drift rates, are in line with previous studies showing that people associate originality more so than utility with creativity (e.g., 
Caroff & Besançon, 2008;
Diedrich et al., 2015;
Runco & Charles, 1993)
. In both studies, the utility of ideas was only somewhat related to participants' tendency towards creative responses. In earlier work we also see that utility, or one of its creative synonyms (e.g., appropriateness, value) is less valued when it comes to judging creativity (e.g., 
Diedrich et al., 2015)
.
However, despite the considerably smaller effect, a Bayesian model comparison supports our proposed model and suggests that utility matters when evaluating the creativity of ideas 10 .
Future work with the CON-task could include additional information about presented ideas, such as "surprise" 
(Boden, 2004)
 and "impact" 
(Sternberg & Lubart, 1996)
, to examine how these components implicitly influence people's creativity judgments.
Interestingly, just as researchers sometimes disagree on the definition of creativity 
(Simonton, 2018)
, the homogenous group of psychology students in our study also varied greatly in their conceptions of creativity. We found both quantitative and qualitative individual differences in how they valued originality and utility 
(Haaf & Rouder, 2017
.
Originality was usually important, but a few people seemed to regard original stimuli "not creative". Regarding utility, there were large individual differences too. Some participants seemed to value utility, a few rated any useful idea as "not creative", and some appeared to disregard it entirely. Moreover, the implicit values were negatively correlated (although in Study 2 we partly attributed this association to the stimulus correlation): the more participants valued originality, the less they valued utility when judging creativity. Future 
10
 We conducted a Bayesian model comparison using Bridge sampling (not pre-registered; 
Meng & Wong, 1996)
 where we compare our original model to a model that has neither an overall nor a random utility effect on the drift rate. We found extreme evidence for our original model over the model without any utility effects. We report this analysis in the supplementary materials. We thank an anonymous reviewer for suggesting this analysis.


CREATIVE OR NOT?
34 research should look to generalize these results to other populations or types of creativity judgments. For example, adolescents appear to place more value on originality and less on utility 
(Stevenson, 2022)
, so there may be a developmental trend. Also, judging the creativity of AUT ideas is interesting as the task is used so often in psychological assessment and educational assessment (e.g., 
Benedek et al., 2016)
. But, would the utility of an idea be considered more valuable in real-world situations, for example when judging the creativity of ideas to combat climate change?
After the CON-task, we asked participants to rate how important they thought originality and utility were for creativity. These self-reported relevance ratings more or less corresponded to their implicit values of originality and utility on the CON-task. For example, those who strongly and explicitly indicated that originality/innovativeness is important for creativity also tended to implicitly value originality in the CON-task. In addition, it appears that the CON-task provides an indirect way to tap into what people consider creative.
Similar to 
Caroff and Besançon (2008)
, our results also showed that the more participants took originality and utility into account on the CON-task, the more original and useful their responses to the AUT were. We cannot talk about causality, but it seems plausible that people's implicit values of originality and utility influence what ideas they produce and how they evaluate and select ideas during divergent thinking, like on the AUT.
This coincides with findings that the more creative someone is, the better their creativity judgements are, whether its of their own or other's ideas (e.g., 
Benedek et al., 2016;
Silvia, 2008
).


Methodological Implications
To our knowledge, this is the first time a mathematical model such as the DDM has been applied to the evaluation phase of divergent thinking. The DDM is generally applied to timed two-choice decision tasks where there is clearly a correct answer, such as lexical decision tasks (e.g., 
Wagenmakers et al., 2008)
. Therefore, applying the DDM to the CREATIVE OR NOT? 35 CON-task, where there is no correct answer and we are basically assessing what factors influence people's tendency to decide if an idea is creative, is novel. Applying the DDM generally worked well on both studies, suggesting that the DDM might provide a reasonable account of the evaluation process in the AUT. A substantive interpretation of the evaluation process could be that when people judge creativity in the CON-task, they stochastically extract, accumulate, and integrate internal noisy signals about a stimulus' creativity, for example regarding the stimuli's originality and utility (for an interpretation regarding value-based decisions, see 
Milosavljevic et al., 2010)
.
Although overall we deemed the model fit to be acceptable, the model predicted slightly more right-skewed RT distributions at the individual level than there were in the data. This kind of misfit suggests that participants responded slower than the model predicted (but also note the predicted longer RTs in the upper quantiles of the RT distribution; see online supplementary materials). Since we explicitly instructed participants to respond as fast as possible, these longer RTs appear necessary for participants to evaluate creativity in the CON-task. The greater need for time is also reflected in a larger than usual boundary separation parameter which can be equated with the discrimination parameter in a 2PL model. It is directly influenced by the time participants have or take to respond 
(van der Maas et al., 2011;
Wickelgren, 1977)
. Longer RTs therefore imply greater discriminatory power of the CON-task stimuli 
(van der Maas et al., 2011)
, which is desirable given our aim to study individual differences in creativity conceptions. However, the relatively long RTs might also suggest that creativity evaluation comprises relatively long, possibly sequential decision-making stages where one simple random walk as assumed in the DDM might be too simplistic. On the other hand, the DDM might still be a good enough approximation of the evaluation process as several more complex decision-making models for binary choices boil down to the DDM 
(Bogacz et al., 2006;
van der Maas et al., 2011)
.
Regarding the role of RTs in our two studies, decision speed did not vary much across stimulus originality and utility ratings. Accordingly, there was no inverted u-or v-shaped CREATIVE OR NOT? 36 relationship between RTs and stimulus characteristics as one might have expected. At best, there was a tendency for stimuli with higher originality and utility ratings to be answered more slowly. A possible explanation is that the effects of originality and utility on the drift rate (i.e., people's tendency towards deciding "creative") were negatively correlated in both studies. This means that the more participants valued originality, the less they valued utility and vice versa. There appears to be a trade-off where people balance one dimension against the other when judging creativity, which could explain why stimuli with both higher originality and utility were processed more slowly.
The lacking influence of RTs begs the question of whether the DDM is not overly complex for the CON-task data from our two studies. For example, a hierarchical probit analysis 11 of our data resulted in similar conclusions. It would be interesting to examine whether RT effects are more apparent if participants are given less time (e.g., 3s versus the 9s in this study); especially since in the real-world people sometimes need to make split-second choices about which idea to pursue. Perhaps this would also lead to a better DDM model fit regarding the RT distribution. Also, instructions that emphasize speed more could affect the evaluation process, just as those of quantity or quality affect the ideation phase of divergent thinking (Said-Metwaly, Fernández-Castilla, Kyndt, & Van den Noortgate, 2020).


Conclusion
This paper demonstrates a new approach to studying the evaluation process during divergent thinking. Our novel application of the drift diffusion model provides a mathematical method to study how people decide what's creative. The results imply that it is important to take both originality and utility into account when examining how people judge creativity. Also, given how conceptions of creativity vary, our findings suggest that when assessing creativity using divergent thinking tasks like the AUT, managers or researchers should clarify what the rating criteria are to provide a fair playing field for all.


Appendices Appendix A
This Appendix contains the DDM specification. Model differences between the studies are mentioned where necessary.
Let Y (ij) denote a response vector of the decision and response time (X (ij) , T (ij) ) for the ith participant, i = 1, ..., I in the jth trial (or stimulus), j = 1, ..., J of the CON task.
The bivariate data Y (ij) is assumed to be distributed according to a Wiener distribution,
Y (ij) ∼ Wiener(α (ij) , β (ij) , τ (ij) , δ (ij) ),
with the four model parameters boundary separation α, bias β, non-decision-time τ , and drift rate δ. The Wiener distribution is a joint density function of deciding whether a use is creative or not, X (ij) , at time T (ij) 
(Vandekerckhove et al., 2011)
.
The double index notation suggests that, in principle, the four parameters may differ across participants, as well as across trials. To reduce model complexity, we constrain the model in several ways. First, we treat all parameters as constant across trials. Second, at the participant level, we allow participants to differ in three out of four parameters, α, β, and δ.
The non-decision time parameter τ is constrained to be constant across trials as well as across participants (i.e., τ (ij) = τ ), because interpreting random effects for the non-decision time parameter has shown to be problematic 
(Singmann, 2018b)
. We treat differences across individuals as random effects, assuming that participants are a sample from a population distribution.
Our main focus of interest is on the drift rate parameter δ. It is the only model parameter assumed to be influenced by stimulus characteristics because the remaining CREATIVE OR NOT? 38 parameters are already set before the decision-making of whether something is creative or not takes place (e.g., 
Vandekerckhove et al., 2011)
. To examine the influence of originality and utility when evaluating creativity, we regress δ on the originality and utility ratings of the stimuli. We include random intercepts as well as random slopes to take interindividual variation into account. Furthermore, because the response times and proportions of "creative" responses have shown to vary considerably across the 64 task stimuli, we also include random intercepts pertaining to the stimuli. Mathematically, we express the above described as follows,
δ (ij) = θ δ(i) + φ δ(j) + θ OR(i) z OR(j) + θ U T (i) z U T (j) . The parameters θ δ(i) , θ OR(i) , θ U T (i)
, and φ δ(j) reflect the drift rate decomposition.
Specifically, θ δ(i) denotes the drift rate intercept, θ OR(i) the originality effect, and θ U T (i) the utility effect of individual i. Furthermore, φ δ(j) is stimulus j's individual deviation from the drift rate intercept. Lastly, z OR(j) and z U T (j) refer to z-scores of the originality and utility ratings of stimulus j.
In Study 1, the boundary separation parameter has random and fixed effects and the bias parameter is fixed. In Study 2, both the boundary separation and the bias parameter are allowed to vary across individuals. All random effects pertaining to the individuals are correlated in both studies. As such, we assume that the random effects are drawn from the same multivariate normal distribution with variance-covariance matrix Σ, i.e.,
            θ δ(i) θ OR(i) θ U T (i) α (i)             ∼ Multivariate-Normal                         µ δ µ θ OR µ θ U T µ α             , Σ             , CREATIVE OR NOT? 39
in Study 1, and
                θ δ(i) θ OR(i) θ U T (i) α (i) β (i)                 ∼ Multivariate-Normal                                 µ δ µ θ OR µ θ U T µ α µ β                 , Σ                 , in Study 2.
Σ is further defined below. The random stimulus effects are orthogonal to the random effects concering the individuals. They are also assumed to be randomly sampled from a population distribution (of stimuli),
φ δ(j) ∼ Normal(0, σ δ φ ),
where 0 is the mean and σ δ φ is the standard deviation.
We need to specify priors for all fixed and random effects parameters as well as for the correlations among the random effects parameters.
Prior specification Study 1. We use a standard normal prior for the originality and utility effects on the drift rate, θ OR , θ U T ∼ Normal(0, 1), .
For the remaining fixed effects we use the following weakly informative priors:
θ δ ∼ Normal(0, 1) µ β ∼ Beta(1.3, 1.3) µ α ∼ Normal + (0, 2)
τ ∼ Uniform(0, 0.3).
These prior distributions restrict the parameters to a plausible range. The range of the a-priori bias parameter is from 0 to 1 and the boundary separation is restricted to be positive. The non-decision time generally needs to be smaller than the RTs. We therefore use 0.3 seconds, the minimally required response time (see exclusion criterion II.), for the prior on τ . Note that due to model convergence issues, we increase the upper bound by one millisecond to 0.301 which is the minimum RT in the data.
For all variability parameters we use the following prior, σ δν , σ δ φ , σ OR , σ U T , σ α ∼ Normal + (0, 0.3).
Lastly, we place a prior on the random effects correlations concerning the individuals.
The variance-covariance matrix Σ needs to be decomposed such that we can specify a prior for the correlations only. We refer to the matrix containing the random effects correlations as P. Specifically, Σ can be rewritten as ΦPΦ, whereby Φ is a 4x4 matrix with only the variability parameters on the diagonal, Φ = diag(σ δν , σ OR , σ U T , σ α ), and P is a 4x4 correlation matrix. For example, the correlation between the random originality and utility effects is expressed as ρ σ OR σ U T . We place a Lewandowski-Kurowicka-Joe (LKJ) prior with the shape 3 on P 
(Lewandowski, Kurowicka, & Joe, 2009)
, P ∼ LKJ(3).


CREATIVE OR NOT? 41
This prior restricts the correlations to the range -1 to 1, makes it a proper correlation matrix, and places most prior mass around 0.
Prior specification Study 2. We use informative, truncated prior distributions for the parameters where we expect a positive effect (i.e., the originality and utility effects on the drift rate),
µ θ OR , µ θ U T ∼ Normal + (0, 0.2),
where 0 is the mean and 0.2 the standard deviation. This truncated prior distribution is informed by previous research that, overall, people take into account both originality and utility when they evaluate creative ideas. It is also informed by data. In Study 1's dataset, the presence of effects of both stimulus originality and utility on the drift rate are detectable using this prior.
For the remaining fixed effects we use the following weakly informative priors:
µ θ δ ∼ Normal(0, 1) µ β ∼ Beta(1, 1) µ α ∼ Normal + (0, 2)
τ ∼ Uniform(0, 0.3).
These prior distributions again restrict the parameters to a plausible range. Note that to successfully estimate the model, the upper bound of the uniform prior on τ is again set to 0.301 (instead of 0.300), the minimum response time in Study 2.
For all variability parameters we use the following prior, CREATIVE OR NOT? 42 σ δν , σ δ φ , σ OR , σ U T , σ β , σ α ∼ Normal + (0, 0.3).
This prior is again informed by previous analyses on Study 1's dataset. Lastly, we need to place a prior on the random effects correlations concerning the individuals. Here, Σ can again be rewritten as ΦPΦ, whereby Φ is a 5x5 matrix with only the variability parameters on the diagonal, Φ = diag(σ δν , σ OR , σ U T , σ α , σ β ), and P is a 5x5 correlation matrix. We again place a Lewandowski-Kurowicka-Joe (LKJ) prior with the shape parameter 3 on P 
(Lewandowski et al., 2009)
, P ∼ LKJ(3).
Study 2.


CON-task.
The full dataset comprised 10972 trials and 172 participants. First, we excluded 8 participants for giving the same response in at least 57/64 (≈ 90%) of the trials.
We then removed the first two trials for each participant (= 328 trials) and all trials with RTs greater than 6 seconds and less than 0.3 seconds (= 529 trials). Finally, data from 12 participants were excluded because they had fewer than 47 (≈ 3/4) remaining trials. The sample used to estimate the DDM comprised 152 participants and 9291 trials. Although we pre-registered to include only data from Dutch native speakers, we retrospectively decided not to exclude data from non-Dutch native speakers as long as they were able to read and respond fluently in Dutch to be as inclusive as possible. Thus, we included all participants who chose to do the Dutch (rather than English) version of the experiment. Participants were required to read an instruction in Dutch in order to do the experiment in Dutch. If they chose the Dutch version, we considered them sufficiently fluent to read and respond in Dutch for our study's purposes where only a few simple phrases had to be read or written in Dutch.


AUT.
In Study 2, 14 participants who were included in the CON-task analyses did not complete the AUT. Again a few participants (n = 3) only submitted responses for one of the two objects. We again cleaned the AUT data by removing all within-participant duplicates as well as data from all participants with less than 90 percent responses that were scored as valid by both raters. Additionally, we again removed all responses that both raters had scored as invalid (n = 12). As performance indicators, we again used the mean originality and mean utility score across raters, objects and responses.
Appendix D 
Table A1
 Posterior mean, standard deviation, and 95% credible interval for the correlations among random effects parameters 
Figure 1
1
. A graphical illustration of the DDM; α = boundary separation, indicating the evidence required to respond 'Creative' or 'Not creative'; β = initial bias to choose one response over the other; δ = average drift rate, indicating the rate of information accumulation; τ = non-decision time, indicating the time used for processes apart from the actual decisionmaking.


Figure
4A, shows the multivariate, joint posterior distribution of the originality and utility effects and Figure 4B the individual posterior means and corresponding standard deviations to visualize this correlation. An overview table of the correlations among all


95%CrI[-0.43, -0.23] , BF 10 = 4.39 × 10 6 , and between the utility ratings and the originality slopes, r = -0.36, 95%CrI[-0.46, -0.26], BF 10 = 1.90 × 10 8 .


Figure 2
2
. A and C show the mean stimulus response times as a function of stimulus originality and utility in Study 1 and 2, respectively. Each dot represents a stimulus. Low and high utility stimuli were categorized by median split. B and D show the distribution of mean response times for participants' 'creative' and 'not creative' decisions.


Figure 3 .
3
The plots show the posterior means and the 95 % credible interval (CrI) for each participant in increasing order. The dashed horizontal line denotes the population-level posterior means µ θ OR , µ θ U T . CrIs colored in red included zero. Plot A and C show the individual estimates of the originality effect in Study 1 and Study 2, respectively. Only a few lower bounds of the CrIs were below zero. Plot B and D show the individual estimates of the utility effect in Study 1 and Study 2, respectively. Some CrIs are above zero, some around zero, and a few below zero.


Figure 4
4
and b), we conducted one-sided Bayesian correlation tests again using sum scores. There was a positive correlation between the originality sum scores and the individual originality effects, r = 0.17, 95%CrI [0.03, 0.31], BF 10 = 2.56 as well as a positive correlation between the utility sum scores and the individual utility effects, r = 0.24, 95%CrI [0.08, 0.38], BF 10 = 24.83, supporting both H7a and H7b. We again conducted an exploratory cluster analysis on the individual posterior means (see Figure 4), which yielded practically the same result as in Study 1 9 . . A and C show the multivariate, joint posterior distribution of the originality effects (θ OR(i) ) and utility effects (θ U T (i) ) on the drift rate, in Study 1 and Study 2, respectively. Darker areas indicate greater density. B and D show the individual posterior means of the originality and utility effects including the standard deviations. Each dot represents a participant. The dots are colored according to the results of k-means cluster analysis. Both


OR σ θ U T -0.44 0.08 -0.6 -0.27 -0.34 0.13 -0.59 -0.09 ρ σ δν σ θ OR -0.16 0.07 -0.3 -0.02 -0.03 0.11 -0.24 0.18 ρ σ δν σα -0.1 0.07 -0.24 0.03 0.03 0.09 -0.16 0.21 ρ σ θ U T σα -0.06 0.08 -0.21 0.1 -0.21 0.12 -0.43 0.02 ρ σ δν σ θ U T Note. SD = standard deviation; LB = lower bound; UB = upper bound.


). Note that to were 0.65 95%CI [0.63, 0.67], 0.68 95%CI [0.67, 0.70], 0.73 95%CI [0.71, 0.76] and 0.67 95%CI [0.63, 0.70] for "brick", "fork", "paperclip" and "towel" respectively. For the utility scores, the corresponding ICCs were 0.5695%CI [0.54, 0.58], 0.61 95%CI [0.59, 0.63], 0.83   95%CI [0.82, 0.85] and 0.68 95%CI [0.65, 0


Table 2 .
2
None of the CrIs included zero suggesting considerable variability across stimuli and acrossFigures 3A and B visualize this variability in the originality and utility effects on the drift rate by depicting the posterior means of the individual originality slopes θ OR(i) and utility slopes θ U T (i) and their corresponding CrIs in increasing order. The figures show substantial individual differences. Regarding the individual originality effects, there were even 11 participants with a negative posterior mean (3.75%). However, the 95%CrI of these estimates included zero. In total, the CrI of 54 individuals included zero (18.43%),
CREATIVE OR NOT?


Despite this reduced CREATIVE OR NOT? 18 correlation across stimuli, the negative correlation across originality and utility effects on individual drift rates remained substantial, r = -.49, 95%CrI [-0.67, -0.29] (see supplementary materials). This result suggests that the correlation across effects is not (solely) a function of stimulus characteristics.
CON-task data based on 44 stimuli. Excluding those items reduced the stimulus originality-utility ratings correlation from r = -0.61 to r = -.15.


.01, 0.53], BF 10 = 2.09, and also weak evidence for no association between originality and RT in the low-utility group 7 , r = 0.05, 95%CrI[-0.31, 0.40], BF 01 = 2.18.
CREATIVE OR NOT?
24
0.27, 95%CrI [-0
. A Bayesian correlation analysis with median-split data suggested weak evidence for a correlation between RT and originality in the high-utility group, r =


Table 1
1
Note. µ θ OR , µ θ U T , and µ δ are standardized estimates as the originality and utility ratings of the stimuli are z-scores. SD
Posterior mean, standard deviation of the posterior
distribution, 95% credible interval andR statistic for the
fixed effects (population-level) parameters
Study 1
Study 2
Mean SD
LB
UB Mean SD
LB UB
µ δ
0.06
0.04 -0.01 0.14
0.15 0.05 0.05 0.26
µ β
0.49
0.00 0.49 0.50
0.49 0.01 0.48 0.50
µ θ OR 0.41
0.05 0.31 0.50
0.40 0.05 0.30 0.51
µ θ U T 0.10
0.05 0.00 0.19
0.10 0.05 0.01 0.21
µ α
2.96
0.03 2.91 3.01
2.72 0.04 2.64 2.80
τ
0.28
0.00 0.27 0.28
0.29 0.00 0.29 0.29
= standard deviation; LB = lower bound; UB = upper
bound.
Hypotheses testing. To test our hypotheses, we used the Savage-Dickey method


Here and for all subsequently reported correlations, we conducted Bayesian correlation analyses using the Bayes factor package including the default prior scale
(Morey & Rouder, 2018)
. Specifically, we used Bayes


The exact results depended on whether the median of the stimuli's utility ratings was included in the lowor the high-utility group. When the median was assigned to the low-utility group, the evidence for the correlation between originality and RT in the high-utility group was even smaller: r = 0.31, 95%CrI[-0.03,   0.59], BF 10 = 2.40. The evidence for no correlation between originality and RT in the low-utility group was also even smaller to the point where there was practically neither evidence for the presence nor for the absence of a correlation, r = 0.21, 95%CrI [-0.10, 0.49], BF 01 = 1.06


This analysis was not pre-registered.


This analysis was not pre-registered.








Appendix B
This Appendix contains the data cleaning procedure that we applied in Study 1 and Study 2. Study 1.


CON-task.
Before analyzing the data, we employed the following exclusion criteria.
The full dataset comprised 18984 trials from 299 participants. First, we excluded data from all participants who gave the same response (either "creative" or "not creative") in at least 57/64 (≈ 90%) of the trials. This step removed data from 2 individuals. We then removed the first two trials to account for the fact that participants needed time to get acquainted with the task (= 594 trials). We also excluded all trials with response times greater than 6 seconds and less than 0.3 seconds to exclude unreasonably fast and slow responses. In this step, 337 trials were excluded. Finally, we excluded data from individuals with fewer than 47 (≈ 3/4) remaining trials. This last step removed data from 4 participants.


AUT.
Out of the participants who were included in the CON-task analysis, only 1 did not complete the AUT which left us with 292 participants for the AUT analysis.
However, 6 participants did not submit responses to one of the two AUT objects. To clean the AUT data, we first removed within-participant duplicates (e.g., from the responses "toy 1", "toy 2", "toy 3" for the object "brick", we only kept the first response). We then removed data from all participants with less than 90 percent valid responses (n = 66). We treated a response as "invalid" if at least one rater had scored it as such (i.e., a rating of "0").
Examples are responses where participants responded with associations rather than uses (e.g., "rectangular" as response for the object "brick"). Finally, we removed all responses that both raters had scored as invalid before computing the performance indicators. As performance indicators, we used the mean originality and mean utility score across raters, objects, and responses.


Appendix C
This Appendix shows the response time distributions of the cleaned data in Study 1 and Study 2.


A. Study 1
Response time (in s) 
 










DescTools: Tools for descriptive statistics




A
S
Al




















GridExtra: Miscellaneous functions for "grid" graphics




B
Auguie




















papaja: Create APA manuscripts with R Markdown




F
Aust






M
Barth




















Magrittr: A forward-pipe operator for r




S
M
Bache






H
Wickham




















Examining the reliability of interval level data using root mean square differences and concordance correlation coefficients




K
A
Barchard




10.1037/a0023351








Psychological Methods




17


2
















The disposition toward originality




F
Barron




10.1037/h0048073








The Journal of Abnormal and Social Psychology




51


3
















Optimal ideation-evaluation ratios




M
Basadur




10.1207/s15326934crj0801_5








Creativity Research Journal




8


1
















Matrix: Sparse and dense matrix classes and methods




D
Bates






M
Maechler




















Enhancement of ideational fluency by means of computer-based training




M
Benedek






A
Fink






A
C
Neubauer








Creativity Research Journal




18


3


















10.1207/s15326934crj1803_7


















M
Benedek






N
Nordtvedt






E
Jauk






C
Koschmieder






J
Pretsch






48


Krammer, G., & CREATIVE OR NOT












Assessment of creativity evaluation skills: A psychometric investigation in prospective teachers




A
C
Neubauer








Thinking Skills and Creativity




21


















10.1016/j.tsc.2016.05.007


















M
Berkelaar






















M
A
Boden




The creative mind: Myths and mechanisms


New York, NY




Routledge








2nd edition








The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks




R
Bogacz






E
Brown






J
Moehlis






P
Holmes






J
D
Cohen








Psychological Review




113


4


















10.1037/0033-295X.113.4.700














brms: An R package for Bayesian multilevel models using Stan




P.-C
Bürkner




10.18637/jss.v080.i01








Journal of Statistical Software




80


1
















Advanced Bayesian Multilevel Modeling with the R Package brms




P.-C
Bürkner




10.32614/RJ-2018-017








The R Journal




10


1


395














Advanced Bayesian multilevel modeling with the R package brms




P.-C
Bürkner




10.32614/RJ-2018-017








The R Journal




10


1
















Variability of creativity judgments




X
Caroff






M
Besançon




10.1016/j.lindif.2008.04.001








Learning and Individual Differences




18


4




















B
Carpenter






A
Gelman






M
D
Hoffman






D
Lee






B
Goodrich






M
Betancourt














Stan: A Probabilistic Programming Language




A
Riddell




10.18637/jss.v076.i01








Journal of Statistical Software




1


76














Are they really ready to work? Employers' CREATIVE OR NOT?




J
Casner-Lotto






L
Barrington








49














U
S
Workforce




Partnership for 21st Century Skills


Washington, DC












The corpus of contemporary american english (coca)




M
Davies














Available online at








Facilitating creative idea selection: The combined effects of self-affirmation, promotion focus and positive affect




D
R
De Buisonjé






S
M
Ritter






S
De Bruin






J
M
Ter Horst






.-L
Meeldijk






A








Creativity Research Journal




29


2


















10.1080/10400419.2017.1303308














The weighted likelihood ratio, linear hypotheses on normal location parameters




J
M
Dickey








The Annals of Mathematical Statistics




















J
Diedrich






M
Benedek






E
Jauk






A
C
Neubauer




Are creative ideas novel and useful? Psychology of Aesthetics, Creativity, and the Arts






9
















10.1037/a0038688














Creative or not? Hierarchical diffusion modeling of the creative evaluation process




M
C
Donzallaz






J
M
Haaf






C
Stevenson




















10.17605/OSF.IO/73C2D














Extending extitR with extitC++: A Brief Introduction to extitRcpp




D
Eddelbuettel






J
J
Balamuta








The American Statistician




72


1


















10.1080/00031305.2017.1375990














Rcpp: Seamless R and C++ integration




D
Eddelbuettel






R
François




10.18637/jss.v040.i08








Journal of Statistical Software




40


8
















Stein's Paradox in Statistics




B
Efron






C
Morris




10.1038/scientificamerican0577-119








Scientific American




236


5




















Creative Or Not






50












Creative ideation, broad retrieval ability, and processing speed: A confirmatory study of nested cognitive abilities




B
Forthmann






D
Jendryczko






J
Scharfen






R
Kleinkorres






M
Benedek






H
Holling








Intelligence




75


















10.1016/j.intell.2019.04.006














Visualization in bayesian workflow




J
Gabry






D
Simpson






A
Vehtari






M
Betancourt






A
Gelman








J. R. Stat. Soc. A




182


2


















10.1111/rssa.12378














Irr: Various coefficients of interrater reliability and agreement




M
Gamer




. F. P. S.








J
Lemon




. F. P. S.
















& <puspendra.pusp22@gmail.com>, I








Inference from Iterative Simulation Using Multiple Sequences




A
Gelman






D
B
Rubin




10.1214/ss/1177011136








Statistical Science




7


4
















Computation of multivariate normal and t probabilities




A
Genz






F
Bretz








Springer-Verlag


Heidelberg












Divergent Thinking and Evaluation Skills: Do They Always Go Together?




M
Grohman






Z
Wodniecka






M
Kłusak




10.1002/j.2162-6057.2006.tb01269.x








The Journal of Creative Behavior




40


2
















The nature of human intelligence




J
P
Guilford








McGraw-Hill


New York, NY












Developing constraint in bayesian mixed models




J
M
Haaf






J
N
Rouder




10.1037/met0000156








Psychological Methods




22
















Some do and some don't? Accounting for variability of individual difference structures




J
M
Haaf






J
N
Rouder








Psychonomic Bulletin & Review


















10.3758/s13423-018-1522-x


















B
A
Hennessey






T
M
Amabile




10.1146/annurev.psych.093008.100416








Creativity. Annual Review of Psychology




61


1
















The effect of regulatory focus on idea generation and idea evaluation




A
Herman






R
Reiter-Palmon




10.1037/a0018587








Psychology of Aesthetics, Creativity, and the Arts




5


1
















A Neurocomputational Model of Altruistic Choice and Its Implications




C
A
Hutcherson






B
Bushong






A
Rangel








Neuron




87


2


















10.1016/j.neuron.2015.06.031
















Ibm




Capitalizing on complexity. IBM Global CEO Study
















Multi-state models for panel data: The msm package for R




C
H
Jackson










Journal of Statistical Software




38


8
















The Bayes factor for inequality and about equality constrained models




I
Klugkist






H
Hoijtink








Computational Statistics & Data Analysis




51


12


















10.1016/j.csda.2007.01.024














Inequality Constrained Analysis of Variance: A Bayesian Approach




I
Klugkist






O
Laudy






H
Hoijtink








Psychological Methods




10


4


















10.1037/1082-989X.10.4.477














Visual fixations and the computation and comparison of value in simple choice




I
Krajbich






C
Armel






A
Rangel




10.1038/nn.2635








Nature Neuroscience




13


10
















A Bayesian hierarchical diffusion model decomposition of performance in Approach-Avoidance CREATIVE OR NOT?




A.-M
Krypotos






T
Beckers






M
Kindt






E.-J
Wagenmakers








52


















Tasks. Cognition and Emotion




29


8
















10.1080/02699931.2014.985635














How cognitive modeling can benefit from hierarchical Bayesian models




M
D
Lee








Journal of Mathematical Psychology




55


1


















10.1016/j.jmp.2010.08.013














Generating random correlation matrices based on vines and extended onion method




D
Lewandowski






D
Kurowicka






H
Joe




10.1016/j.jmva.2009.04.008








Journal of Multivariate Analysis




100


9














Implicit theories of creative ideas: How culture guides creativity assessments




J
Loewenstein






J
Mueller








Academy of Management Discoveries


2
















10.5465/amd.2014.0147














How does emotion influence the creativity evaluation of exogenous alternative ideas?




S
Mastria






S
Agnoli






G
E
Corazza




10.1371/journal.pone.0219298








PLOS ONE




14


7














Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis




D
Matzke






E.-J
Wagenmakers




10.3758/PBR.16.5.798








Psychonomic Bulletin & Review




16


5
















Simulation ratios of normalizing constants via a simple identity: A theoretical exploration




X.-L
Meng






W
H
Wong








Statistica Sinica




6
















Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations




D
E
Meyer






R
W
Schvaneveldt




10.1037/h0031564








Journal of Experimental Psychology




90


2
















The Drift CREATIVE OR NOT?




M
Milosavljevic






J
Malmaud






A
Huth






C
Koch






A
Rangel








53












Diffusion Model Can Account for the Accuracy and Reaction Time of Value-Based Choices Under High and Low Time Pressure


10.2139/ssrn.1901533








SSRN Electronic Journal












BayesFactor: Computation of bayes factors for common designs




R
D
Morey






J
N
Rouder




















Reframing the decision-makers' dilemma: Towards a social context model of creative idea recognition




J
Mueller






S
Melwani






J
Loewenstein






J
J
Deal








Academy of Management Journal




61


1


















10.5465/amj.2013.0887














Here: A simpler way to find your files




K
Müller




















Tibble: Simple data frames




K
Müller






H
Wickham




















CODA: Convergence diagnosis and output analysis for mcmc




M
Plummer






N
Best






K
Cowles






K
Vines










R News




6


1
















Idea Generation, Selection, and Evaluation: A Metacognitive Approach




R
Puente-Diaz






J
Cavazos-Arroyo






L
Puerta-Sierra




10.1002/jocb.505


jocb.505








The Journal of Creative Behavior
















That will never work: The birth of Netflix and the amazing life of an idea




M
Randolph








Hachette Book Group


New York, NY












A theory of memory retrieval




R
Ratcliff




10.1037/0033-295x.85.2.59








Psychological Review




85


2




















Creative Or Not






54












A diffusion model account of response time and accuracy in a brightness discrimination task: Fitting real data and failing to fit fake but plausible data




R
Ratcliff




















10.3758/BF03196283








Psychonomic Bulletin & Review




9


2














Individual differences and fitting methods for the two-choice diffusion model of decision making. Decision




R
Ratcliff






R
Childers








2
















10.1037/dec0000030














A Diffusion Model Account of the Lexical Decision Task




R
Ratcliff






P
Gomez






G
Mckoon








Psychological Review




111


1


















10.1037/0033-295X.111.1.159














The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks




R
Ratcliff






G
Mckoon








Neural Computation




20


4


















10.1162/neco.2008.12-06-420














R: A language and environment for statistical computing




R Core Team










Vienna, Austria






: R Foundation for Statistical Computing








Psych: Procedures for psychological, psychometric, and personality research




W
Revelle










Evanston, Illinois






Northwestern University












The selection of creative ideas after individual idea generation: Choosing between creativity and impact




E
F
Rietzschel






B
A
Nijstad






W
Stroebe




10.1348/000712609X414204








British Journal of Psychology




101


1
















Creativity: The role of unconscious processes in idea generation and idea selection




S
M
Ritter






R
B
Van Baaren






A
Dijksterhuis




10.1016/j.tsc.2011.12.002








Thinking Skills and Creativity




7


1




















Creative Or Not






55












An introduction to Bayesian hierarchical models with an application in the theory of signal detection




J
N
Rouder






J
Lu




10.3758/BF03196750








Psychonomic Bulletin & Review




12


4
















Divergent thinking as an indicator of creative potential




M
A
Runco






S
Acar








Creativity Research Journal




24


1


















10.1080/10400419.2012.652929














Assessing ideational and evaluative skills and creative styles and attitudes




M
A
Runco






M
Basadur








Creativity and Innovation Management




2


3
















10.1109/iemc.1990.201291














Judgments of originality and appropriateness as predictors of creativity




M
A
Runco






R
E
Charles








Personality and Individual Differences




15


5


















10.1016/0191-8869(93)90337-3


















The Standard Definition of Creativity




M
A
Runco






G
J
Jaeger




10.1080/10400419.2012.650092








Creativity Research Journal




24


1
















Interpersonal and intrapersonal evaluations of creative ideas




M
A
Runco






W
R
Smith








Personality and Individual Differences




13


3


















10.1016/0191-8869(92)90105-x


















S
Said-Metwaly






B
Fernández-Castilla






E
Kyndt






W
Van Den Noortgate


















Testing conditions and creative performance: Meta-analyses of the impact of time limits and instructions


10.1037/aca0000244








Psychology of Aesthetics, Creativity, and the Arts




14


1














Discernment and creativity: How well can people identify their most creative ideas?




P
J
Silvia








Psychology of Aesthetics, Creativity, and the Arts




2


3


















10.1037/1931-3896.2.3.139


















Creative Or Not






56












Defining Creativity: Don't We Also Need to Define What Is Not Creative?




D
K
Simonton








The Journal of Creative Behavior




52


1


















10.1002/jocb.137
















H
Singmann






Diffusion/Wiener Model Analysis with brms -Part II: Model Diagnostics and Model Fit


















H
Singmann






Diffusion/Wiener Model Analysis with brms -Part III: Hypothesis Tests of Parameter Estimates
















RStan: The R interface to Stan








Stan Development Team
















StanHeaders: Headers for the R interface to Stan








Stan Development Team
















Creativity and Culture




M
I
Stein




10.1080/00223980.1953.9712897








The Journal of Psychology




36


2




















R
J
Sternberg






T
I
Lubart








51








Investing in creativity








Creative or not: The originality-utility tradeoff in divergent thinking




C
E
Stevenson








Society for the Neuroscience of Creativity (SfNC) annual meeting May












conference presentation








The efficient computation of the cumulative distribution and probability density functions in the diffusion model




F
Tuerlinckx




10.3758/BF03206552








Behavior Research Methods, Instruments, & Computers




36


4
















Two interpretations of the discrimination parameter




F
Tuerlinckx






P
D
Boeck




OR NOT? 57


















10.1007/s11336-000-0810-3








Psychometrika




70


4














Hierarchical diffusion models for two-choice response times




J
Vandekerckhove






F
Tuerlinckx






M
D
Lee








Psychological Methods




16


1


















10.1037/a0021765


















H
L J
Van Der Maas






D
Molenaar






G
Maris






R
A
Kievit






D
Borsboom


















Cognitive psychology meets psychometric theory: On the relation between process models for decision making and latent variable models for individual differences


10.1037/a0022749








Psychological Review




118


2


















A
Vehtari






A
Gelman






D
Simpson






B
Carpenter






P.-C
Bürkner


















Rank-normalization, folding, and localization: An improved $\widehat{}R{}$ for assessing convergence of MCMC












Retrieved








The RWiener package: An R package providing distribution functions for the Wiener diffusion model




D
Wabersich






J
Vandekerckhove




10.32614/RJ-2014-005








The R Journal




6


1


49














Methodological and empirical developments for the Ratcliff diffusion model of response times and accuracy




E.-J
Wagenmakers




10.1080/09541440802205067








European Journal of Cognitive Psychology




21


5
















A diffusion model account of criterion shifts in the lexical decision task




E.-J
Wagenmakers






R
Ratcliff






P
Gomez






G
Mckoon




10.1016/j.jml.2007.04.006








Journal of Memory and Language




58


1
















R package "corrplot": Visualization of a correlation matrix




T
Wei






V
Simko




















Speed-accuracy tradeoff and information processing dynamics




W
A
Wickelgren




10.1016/0001-6918(77)90012-9








Acta Psychologica




41
















Ggplot2: Elegant graphics for data analysis




H
Wickham










Springer-Verlag


New York












Stringr: Simple, consistent wrappers for common string operations




H
Wickham




















Dplyr: A grammar of data manipulation




H
Wickham






R
François






L
Henry






K
Müller




















Tidyr: Tidy messy data




H
Wickham






L
Henry




















Cowplot: Streamlined plot theme and plot annotations for 'ggplot2




C
O
Wilke





















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]