You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



How do people predict a random walk? Lessons for models of human cognition Introduction Many aspects of our world are not static but changeable, requiring us to regularly update our expectations as time passes: will last week's rain continue this week? Will tomorrow's traffic be better or worse? Will food prices increase or remain stable? We refer to such cases here as iterative predictions, reflecting scenarios in which forecasts are made repeatedly over time as the target value being predicted evolves. A particularly widely studied case of this is financial markets: prices of assets and commodities can fluctuate substantially, and predictive inaccuracy has immediate and potentially severe financial consequences. Understanding how people approach such tasks is therefore of vital importance, particularly whether generated forecasts are optimal given available information, or whether there are systematic biases in our predictions. The current paper thus seeks to offer insight into the cognitive systems supporting iterative predictions, focusing on a comparison of several previously proposed computational models in their fit to behaviour in such tasks.
The first challenge for any such examination, however, is deciding the target series being predicted, as this will determine both the actual behaviour under investigation and its idealised standard. One possibility is to investigate real world series, though in such cases the underlying generative process is often highly complex, or indeed might be entirely unknown, making definition of optimal responses difficult. Conversely, artificial series used in laboratory experiments offer greater certainty and control, though this comes at the cost of potential realism. In addition, use of artificial series raises further questions on how to select a specific target, offering the freedom to choose data of any form which could naturally advantage particular models suited to that structure; to provide an extreme example, if target data approximately follows a straight line, simple linear models will provide better matches than those using sine waves, whereas oscillating data would reverse this pattern.
The ideal test case is thus a series which provides a compromise between experimental control and external validity, as well as a neutral test-bed for various different approaches to forecasting. While the perfect target may remain elusive, one particularly appealing option is a random walk process: a random walk is a sequence of values in which each new point represents a random movement from its immediate predecessor with no further influence from previous data. Here, we focus specifically on a log-normal random walk where steps are randomly drawn from a Gaussian distribution in log space (illustrated in 
Figure 1A
) as this naturally allows movements to scale according to the magnitude of the most recent value.
There are several reasons to focus on the random walk. First, this provides perhaps the most basic form of a 'randomly moving target', with unpredictable movements that cannot be adequately captured by fixed predictions, as would be the case in white noise processes with a consistent mean value. Second, as values are dependent only on the directly previous point, there is a definitive delineation between relevant and irrelevant past information. Third, the random walk has clearly defined statistical properties which can be compared with the patterns of human forecasts, and has established measures to do so. Fourth, the process offers an easily derived rational solution for predicting the next value in the sequence which minimises error via the expected value of its step distribution, which could be learned through experience.
Fifth, random walks present idealised market processes in finance theory, where any available information is immediately incorporated into price 
(Samuelson, 1965;
Mandelbrot, 1966;
Fama, 1970)
. More broadly, the random walk arguably provides one of the simplest forms within the limitless space of possible iterative prediction problems, so offering a basic foundation for testing cognitive models of forecasting.
A particular advantage of this random walk target is that the rational solution for predictions of subsequent values is clearly defined, providing a simple guideline for optimal behaviour: forecasters should predict the most likely change given by the step distribution (in this case, the mean of the Gaussian distribution). This is especially true if the expected movement is zero as forecasts need only directly repeat the preceding value, though non-zero means also provide reasonably simple trends to follow; for this reason, we shall initially focus on the zero-mean case, though we return to such alternate definitions later. This places optimal behaviour well within human capabilities, in contrast to other potential targets which might require more extensive computation. Indeed, there have been a number of theories which suggest forecasters will adhere to the rational solution, particularly in economics 
(Lucas, 1972;
Muth, 1961)
; in 
Figure 1
: A) Illustration of the log-normal random walk target: each movement in price represents a draw from a Gaussian distribution in log space centred on the immediately preceding price, shown by the red curve. The expected movement is thus 0, meaning the rational prediction on each trial is to repeat the previous price. B) Trial structure for the price prediction task as performed by participants, adapted from 
Zhu et al. (2021)
: after viewing the most recent price (shown in green), participants entered their prediction of the subsequent price (shown in red), before receiving feedback on the true next price and the potential reward given the accuracy of their prediction. such cases, forecasters are assumed to know that the true data generating process follows a random walk and act accordingly in their predictions, though similar behaviour could also be achieved by sufficiently advanced learning systems which are able to determine the random walk structure based on observation, such as complex connectionist networks or Bayesian models with appropriate prior beliefs. Moreover, merely repeating the current value provides a simple "fast and frugal" heuristic 
(Gigerenzer & Todd, 1999
) that, while optimal for the random walk, is also likely to provide a good default strategy when the data deviates from a random walk in some unknown way. These types of approaches imply that human-generated predictions should mirror the properties of the original target series, in this case inheriting the features of the random walk.
In reality, however, actual behaviour does not appear to follow this standard: in a recent set of experiments, we observed that participant predictions of random walk series deviated from the properties of their target 
(Zhu et al., 2021)
. Such deviation is perhaps to be expected given that human judgements have been widely observed to be noisy, generally resulting in suboptimal performance 
(Kahneman, Sibony, & Sunstein, 2021)
. What is notable, however, is that this noise appears to produce consistent patterns, including extreme changes between subsequent predictions and long-range dependencies in forecast values. Thus, people seem to be adding statistical structure into their forecasts which is not present in the target series being predicted. These patterns link to a wider literature on the features of repeated human estimation beyond random walk targets which has found evidence of specific statistical signatures such as long-range dependencies in both estimates and response times, and power-law distributions of movements between estimates 
(Gilden, Thornton, & Mallon, 1995;
Gilden, 2001;
). The 'noise' observed in these predictions thus appears to be an inherent aspect of human judgment, and so could speak to the nature of the underlying cognitive mechanisms.
How then might we explain these deviations from random walk structures? What is it about forecasters that leads their predictions to stray from the properties of their target?
Fortunately, there exist a myriad of theories and models in the existing psychological literature offering descriptive accounts of behaviour, using a variety of different approaches and techniques, which can be applied to model such predictions. The particular pattern of deviation from a pure random walk produced by forecasts from these models then offers a method to distinguish between these accounts, suggesting which (if any) mirrors the patterns produced by human forecasters. Of course, this is not the first study to compare models using fits to human forecasting data, though this does have a distinctive focus: previous studies of forecasting have often concentrated on whether models match the accuracy of human forecasters, or display similar reactions to trends 
(Wagenaar & Sagaria, 1975;
Eggleton, 1982;
Lawrence & Makridakis, 1989;
Sanders, 1992;
Reimers & Harvey, 2011;
Kusev, Van Schaik, Tsaneva-Atanasova, Juliusson, & Chater, 2018)
. In contrast, we focus here on general statistical features displayed by participants in our random walk prediction task, testing whether models produce similar patterns of deviation in the higher-level properties of their forecasts. This follows the examination of such features to assess deviations from random walk structures in studies of financial markets when comparing actual price fluctuations against idealised standards 
(Shiller, 1981;
LeRoy & Porter, 1981;
Cont, Potters, & Bouchaud, 1997;
Campbell, Lo, & McKinlay, 1999;
Mantegna & Stanley, 1999;
Cont, 2001)
, here being applied to the behaviour of individuals rather than macroeconomic systems. The use of such criteria thus offers a new metric of model performance, allowing for novel insights into the psychology of prediction.
The present paper thus seeks to compare several computational models of human forecasting drawn from various branches of psychology, assessing their ability to generate the specific statistical properties seen in individuals. The structure of the paper is as follows.
We begin with a brief review of the prior literature on this subject, and then introduce the empirical properties targeted in our comparison. We then define the set of candidate models to be considered, and contrast their ability to generate the specific properties seen in individuals.
Results from this comparison show support for a model in which predictions are generated by a local mental sampling process, a type of model with wide applicability beyond forecasting, and for which there is other independent support. We next assess the generality of these findings by examining the ability of these models to produce other prominent effects in the forecasting literature. We then extend our comparison to a new experimental data set to test the reliability of these results, finding similar evidence with alternate target series with positive or negative trends. We close with a discussion of the implications of these findings for depictions of human predictions.


Models of Iterative Predictions
As a fundamental aspect of human behaviour, the mechanisms underlying predictions have been a key focus across a wide range of psychological approaches, with many models specifically targeting how expectations are updated with further experience. While an exhaustive review of all existing approaches to human predictions is certainly beyond the scope of this paper, we here provide a brief summary of some of the more prominent methods that have been developed, focusing on models which can be applied to iterative numerical predictions of a random walk target.
One of the most notable approaches to iterative predictions is provided by associative learning methods, where expectations are captured psychologically via mental associations between relevant stimuli: these connections are updated with experience according to errors in anticipated events, strengthening where unexpected outcomes occur and weakening when expected outcomes fail to appear 
(Bush & Mosteller, 1951;
Rescorla & Wagner, 1972;
Pearce & Hall, 1980;
Pearce & Bouton, 2001
). In the case of basic numerical prediction tasks such as that considered here, such techniques can be directly applied to continuous estimates, using anticipation errors to update predicted values themselves, for example in estimates of probabilities 
(Behrens, Woolrich, Walton, & Rushworth, 2007;
Forsgren, Juslin, & Van Den Berg, 2023)
.
Similar error-based learning techniques also appear in depictions of market predictions in economics, in this case being described as adaptive expectations models 
(Hey, 1994;
Hommes, 2011;
Afrouzi, Kwon, Landier, Ma, & Thesmar, 2020)
. Alternatively, multiple associative links can be combined into complex networks of interconnected nodes to provide much more detailed representations, allowing for greater abstraction between stimuli and responses 
(Rumelhart, McClelland, & the PDP Research Group, 1986)
. Such advanced networks have shown success in matching human performance in complex tasks 
(Lake, Zaremba, Fergus, & Gureckis, 2015;
Testolin & Zorzi, 2016)
, though for the present random walk case, such complexity may be unwarranted.
In contrast to the abstraction of connectionist methods, an alternate approach to iterative predictions is given by exemplar models, in which experiences are stored in memory and aggregated to provide expectations 
(Nosofsky, 1992;
Jäkel, Schölkopf, & Wichmann, 2007)
.
The weighting of memories in the aggregate can be adjusted to place greater focus on certain events over others, thus capturing observed behavioural biases. For example, many models define similarity functions which weight events according to correspondences in other trial features 
(Shepard, 1987;
Elwin, Juslin, Olsson, & Enkvist, 2007;
Henriksson, Elwin, & Juslin, 2010)
, while others place higher weights on more recent examples to reflect recency biases in human memory 
(Anderson, Bothell, Lebiere, & Matessa, 1998)
. This does however come at potentially substantial memory cost where experience is extensive, possibly suggesting a need for limits on exemplar representations to save cognitive resources.
Other models offer a more rule-based approach to predictions, depicting behaviour using higher-level strategies to direct expectations 
(Hommes & Wagener, 2009)
. Key examples of this are extrapolative models which seek to capture the apparent tendency for trend-chasing in human forecasts; this can be achieved in a number of ways, from basic statistical models such as limited window autoregressive processes which base expectations on recent events 
(Hommes, 2011)
, to exponential smoothing models which apply the previously described error-based learning to trends 
(Gardner Jr, 1985;
Gardner Jr & McKenzie, 1985)
, to complex integrations across the complete history of the series 
(Barberis, Greenwood, Jin, & Shleifer, 2015;
Afrouzi et al., 2020)
. Such strategies are often contrasted with mean-reverting or fundamentalist approaches where trends are instead expected to dissipate, in keeping with the rational solution described above 
(Frankel & Froot, 1990;
Brock & Hommes, 1998;
Barberis, Shleifer, & Vishny, 1998;
Boswijk, Hommes, & Manzan, 2007)
. This has led to the development of strategy switching models where agents can select between rules across trials based on their performance, so adapting their behaviour as more information is received 
(Barberis et al., 1998;
Anufriev & Hommes, 2012;
Hommes, 2011)
. Another suggested prediction strategy is anchoring and adjustment, in which predictions are initially based on a previously experienced value which is then slightly adjusted to provide a forecast 
(Tversky & Kahneman, 1974;
Lawrence & O'Connor, 1992;
Bolger & Harvey, 1993;
Kusev et al., 2018)
. The strength of these rule-based models is their simplicity, while the cost is their potential suboptimality: basic rules may be easy to use, but may not offer the best solution. For the present case, however, such suboptimality may itself be beneficial in capturing the deviations observed in actual behaviour.
Other work has drawn on statistical processes to describe behaviour, with a particularly notable example being the autoregressive integrated moving average (ARIMA) family of models; ARIMA combines autoregressive mechanisms to capture the previously noted extrapolative nature of predictions with moving averages as an adaptive baseline 
(Box, Jenkins, Reinsel, & Ljung, 2015)
. These models have often been used as a standard against which human forecast performance can be compared 
(Lawrence, Edmundson, & O'Connor, 1985;
Lawrence & O'Connor, 1992;
Kusev et al., 2018)
, though this has been primarily focused on measures of accuracy rather than the specific statistical features targeted here. Such a model is particularly well-suited to the present random walk case as a simple autoregressive process with no return to the average matches with the rational solution which repeats the most recent value; indeed, the current random walk target itself essentially represents an ARIMA(1,0,0) process with the addition of random noise on each step. This being said, this correspondence may not be beneficial in the current context, where the aim is to capture human behaviour that strays from the random walk.
An alternate framework for iterative predictions suggests that forecasters may not have sufficient information regarding the task to achieve rational solutions immediately, but may attempt to learn such solutions as they gain experience. A key example of this is given by Bayesian approaches which seek to determine optimal responses given uncertainty: these models operate by defining probability distributions over potential outcomes which can be iteratively updated with new information according to Bayes' rule to produce normative expectations.
Bayesian models have shown strong correspondence with human behaviour in a range of tasks, including predicting category structure 
(Anderson, 1991;
Lake, Salakhutdinov, & Tenenbaum, 2015)
, intuitive physics 
(Battaglia, Hamrick, & Tenenbaum, 2013;
Sanborn, Mansinghka, & Griffiths, 2013)
, magnitudes 
(Griffiths & Tenenbaum, 2011;
Petzschner, Glasauer, & Stephan, 2015)
, language 
(Chater & Manning, 2006)
, and perceptual features 
(Sanborn & Beierholm, 2016;
Spicer, Sanborn, & Beierholm, 2020)
. As noted above, by offering rational learning systems, Bayesian models offer one potential method by which forecasters may obtain rational solutions through aggregated experience, eventually settling on optimal predictions. This is, however, restricted by the pre-existing beliefs of the model: Bayesian solutions are only optimal where prior assumptions hold true, meaning substantial deviations could arise if these beliefs are mis-specified. A further caveat to Bayesian approaches is their complexity: these models require decision makers to work with what may be highly complex probability distributions, requiring substantial computation 
(Sanborn, Griffiths, & Navarro, 2010;
Sanborn & Chater, 2016
).
Fortunately, another class of models provides a solution to this complexity issue: in place of full distributions, decision makers can instead use a limited number of samples from the target as an approximation, drastically reducing computational costs 
(Sanborn & Chater, 2017;
Chater et al., 2020;
Sanborn et al., 2021)
. While large sample counts will produce more accurate approximations, the limits of human cognitive resources mean that responses will often be based on only a few samples, so rarely meeting with the true optimum and thus leading to potential biases 
(Dasgupta, Schulz, & Gershman, 2017;
Lieder, Griffiths, Huys, & Goodman, 2018;
Zhu, Sundh, Spicer, Chater, & Sanborn, 2023)
. In addition, the inherent stochasticity of sampling algorithms offers explanations for the noisiness of human behaviour: samples can vary even in response to the same query, potentially resulting in different behaviours. Furthermore, the method by which samples are generated will also lead to patterns in responses that may not be present in the target sequence being predicted, such as autocorrelations between sequential responses 
(Zhu et al., 2022
(Zhu et al., , 2023
. Indeed, in previous work, we have suggested that use of particular sampling algorithms can explain the specific patterns shown by human subjects in repeated estimation tasks 
(Zhu et al., 2021)
, though this model has not yet been directly compared with competitors such as those listed above.
Note that these modelling approaches to iterative predictions are not necessarily mutually exclusive: in fact, some proposed models use a mixture of these elements to better match actual behaviour, such as connectionist networks with external memory stores 
(Graves et al., 2016)
 or error-based learning within Bayesian frameworks 
(Piray & Daw, 2021)
. Such hybrids naturally combine the strengths of their component elements, though this does come at the cost of increased complexity. While these hybrids are interesting candidates, we restrict our focus here to base models rather than potential combinations both for simplicity as well as to characterise the individual behaviour of each element, leaving comparisons of these advanced models to future work.
While this is only a cross-section of the many models that can be adapted to iterative predictions, this should illustrate the breadth of available approaches that could be applied, and that a full and comprehensive comparison is beyond the scope of any one paper. We thus seek here to offer a high-level comparison, evaluating a set of representative key models selected from across these branches.


Behavioural Targets
We next outline the properties of human behaviour which will provide the criteria for the subsequent model comparison. These properties are assessed using data from a previous experiment in which we contrasted individual predictions with the movements of financial markets, finding notable correspondence in several features 
(Zhu et al., 2021)
. The present comparison then takes these measures as empirical standards of human behaviour, providing targets for model assessment. We first summarise the details of this experiment, before discussing the key measures taken from the data.


Task Details
The task involved trial-by-trial predictions of the price of a fictional financial asset: on each trial t, participants were asked to predict the immediately subsequent price (p t+1 ) of the asset given only their experience with its price history up to that point (p 1:t ). 
Figure   1B
 provides an illustration of the task structure. The true price series was predefined using a random walk process taking Gaussian steps in log space (shown in 
Figure 1A)
:
ln(p t ) = ln(p t−1 ) + ε t (1) ε t ∼ N(0, σ 2 )
(2)
though these values were exponentiated to prices, multiplied by 100 and rounded to the nearest integer for presentation in the task 1 . Participants were incentivised to minimise error in their predictions by offering them a monetary reward at the end of the task based on their prediction error in a randomly selected trial. This used an exponential reward function based on absolute logarithmic difference between predicted and actual price:
reward = £2 + 18 * exp(−9(|ln(p i ) − ln(p e i )|))
(3)
where p e i is the prediction of p i and i is the randomly selected trial. The use of absolute error in the reward function means that participants maximise expected reward by focusing on the median of the step distribution, which is 0; this then means that the rational prediction strategy on any trial is to simply repeat the preceding price, leading the optimal prediction series to mirror the target random walk at a one-step lag.
After each prediction, the task advanced to the next trial, and the true price for that period was revealed, as well as the error in the preceding prediction and the potential reward resulting from that error should it be selected. Participants continued predicting prices until reaching the maximum experimental duration of 1 hour, with participants completing 449 trials (±16.2 95% CI) on average. Once the task was complete, participants were asked whether they had been interrupted during the task to act as an exclusion criterion. Finally, one of the participant's trials was randomly selected and their error in that trial was used to determine their reward value using Equation 3, paid in British pounds. Data were only accepted for analysis if the participant completed a minimum of 350 trials, and reported no interruptions.
Full details on this experiment are given by 
Zhu et al. (2021)
.


Exclusions
As participant responses in this task represent a series of potentially interconnected predictions, we sought to avoid editing this data as much as possible to preserve its integrity.
This being said, there are of course concerns that any errors in response could pollute the data and so confound measures taken from it. This is most notable when examining extreme movements between predictions: in such cases, does this truly reflect an extreme change in beliefs, or simply a mistake in the submitted response? We therefore applied an exclusion criterion within the present analysis (in addition to those used in 
Zhu et al., 2021)
 as a precaution for such cases: responses which differed from both the preceding and subsequent predictions by a factor of 10 were treated as errors and replaced with a non-number filler to preserve series spacing, removing an average of 0.37% of responses across participants. This criterion was not used in our previous paper 
(Zhu et al., 2021)
, but was deemed necessary for the more intensive quantitative analyses performed here. We revisit results without this exclusion in Appendix C.


Measures
We next describe the key properties of the human data which will be used as criteria for model comparison. These properties reflect particular statistical features which have been previously used to assess deviations from random walk patterns in financial studies 
(Shiller, 1981;
LeRoy & Porter, 1981;
Cont et al., 1997;
Campbell et al., 1999;
Mantegna & Stanley, 1999;
Cont, 2001)
, following the suggestion that random walks provide idealised market structures 
(Samuelson, 1965;
Mandelbrot, 1966;
Fama, 1970)
. We used these measures in our prior work to examine the correspondence between cognitive and market fluctuations 
(Zhu et al., 2021)
, though here our focus is on the values shown by human forecasters themselves rather than their similarity with market measures.
Target properties can broadly be divided into three key aspects: autocorrelations, excess volatility and heavy tails. Multiple measures are used for some features to both provide greater confidence in the assessment of these properties and control for any issues which may be raised with any individual measure. The following provides a brief introduction to each of these measures, as well as their distributions in the experimental sample, as summarised in 
Figure 2
. More detailed definitions of these measures are given in Appendix A.


Autocorrelations
Financial markets display little evidence of autocorrelations in their changes (i.e., an absence of correlation in returns) beyond short time-lags, leading to the appearance of a firstorder random walk 
(Cont et al., 1997;
Cont, 2001)
. We use two measures of this property given by existing studies: the autocorrelation function in changes between subsequent log predictions and the power spectral density of the log prediction series. The autocorrelation function reflects the relationship between trial-to-trial changes in log predictions when compared across longer time lags, and is expected to fall to approximately 0 after the first lag; we therefore assess this property by taking the proportion of autocorrelation coefficients across the first 100 lags which are found to fall outside of 95% confidence intervals around zero, which for ease of presentation will here be termed the significant rate. The mean significant rate across participants was 0.069 (±0.011), suggesting such correlations were indeed rare in this sample 2 .
The power spectral density is used to examine long-range autocorrelations in the logarithmic prediction series according to the relation between frequency and density following a Fourier analysis, with random walks typically showing linear slopes of approximately -2 in log-log space 
(Mantegna & Stanley, 1999)
; indeed, the present random walk target itself shows a slope of -1.90, with the difference from the theoretical standard value likely being attributable to the limited length of the experimental target. Participant results fell slightly above this value, with a mean slope of -1.71 (±0.04), potentially suggesting a degree of long-range autocorrelation in the data, as have been observed in other repeated estimation tasks 
(Gilden et al., 1995;
Gilden, 2001;
Zhu et al., 2022)
.


Excess Volatility
Markets have been found to demonstrate higher levels of volatility than is suggested by underlying fundamentals, leading to greater movements in price than predicted by rational approaches 
(Shiller, 1981;
LeRoy & Porter, 1981)
. We measure this property here according to the variance in the distribution of changes between subsequent log predictions (i.e., Var[ln(p e t )− ln(p e t−1 )]); following this property, this variance should exceed that of the true distribution of price changes in the underlying random walk (from Equation 2), though we here directly use this variance to provide a continuous measure rather than a binary comparison. Mean variance in this distribution was 0.188 (±0.032) across participants, with almost all individuals exceeding the true variance of the target series (0.063).


Heavy Tails
Distributions of price changes show heavier tails than Gaussian standards, demonstrating substantial increases and decreases between subsequent points 
(Cont et al., 1997;
Cont, 2001
).
We again use multiple measures to assess this features based on suggestions from past research:
first, we estimate the tail index of the distribution of changes between log predictions by fitting a Generalised Pareto distribution to the top 5% of these changes and taking the resulting shape parameter 
(Embrechts, Klüppelberg, & Mikosch, 1997;
Kotz & Nadarajah, 2000)
, where an index of 0 implies Gaussian tails while positive values imply heavier tails. Participant values tend towards this on average, with a mean index of 0.34 (±0.20), though there are some participants displaying thin tails according to this measure. Second, we take the kurtosis (i.e., the fourth standardised moment) of the distribution of changes between log predictions, where kurtosis values above 3 imply heavier tails than Gaussian. Kurtosis was similarly high in participant predictions, with a mean of 14.6 (±1.96), though in this case measures consistently indicated heavy tails for all participants.


Additional Properties
While the above measures are the key focus of the present study, there is one other property which deserves special mention: volatility clustering, which refers to the finding that market volatility is not even across time but appears in concentrated bursts. This was a key property in our previous comparison of the fluctuations in market prices and human predictions 
(Zhu et al., 2021)
, but is not assessed here as measures of volatility clustering are substantially noisier in individuals than groups, meaning this may not offer a stable behavioural criterion. We do however revisit volatility clustering in Appendix D, further detailing the prevalence of this feature in the human data, and outlining that inclusion of this property does not meaningfully alter model comparison results.


Model Definitions
We next define the models that will be evaluated in this task. Rather than defining independent candidate models, we here use a factorial modelling comparison procedure in which models reflect combinations of separate factors; this is used as model behaviour is here dependent not only on the method by which predictions are generated but also the representation used 3 . Models are thus composed of four key factors: the prediction mechanism, the target format, the target domain, and response noise, where the prediction mechanism captures the method and the format and domain capture the representation, with response noise reflecting potential errors in expression of expectations. We therefore first define these representational factors as this will determine some aspects of the prediction mechanisms.


Target Format
Target format refers to the specific aspect which is being predicted, here reflecting two potential methods by which price predictions may be generated: the decision maker may directly estimate the subsequent price, or may simply estimate the expected change from the most recent price. This is an important distinction as predictions of price change are naturally anchored to the most recent price point, considering only the movement from that value and so allowing predictions to remain closer to the target series. In contrast, predictions of price do not hold such an attachment and so may show greater deviations.


Target Domain
Target domain refers to the scale in which the target format is considered, here divided between standard and log-scale; while participants in the actual task were presented with standard prices, many economic theories would suggest a focus on logarithmic prices to account for differences in scale between movements, particularly where the target series itself is a random walk in log space. We therefore consider both domains within this comparison to determine which better reflects behaviour.


Prediction Mechanisms
The prediction mechanism refers to the method by which a prediction may be generated within the considered format and domain. We examine nine potential prediction mechanisms taken from existing approaches to cognitive predictions suggested by previous research, each described in detail below. For ease of interpretation, throughout the following, X t refers to the target at time t in the considered format and domain, being either price (p t ), log price (ln(p t )), price change (p t − p t−1 ) or log price change (ln(p t ) − ln(p t−1 )), in all cases being available to the mechanism at time t. X e t+1 meanwhile reflects the predicted value of the target X t+1 made by that mechanism for the immediately subsequent trial (i.e., the prediction made on trial t for trial t + 1). Given the task considered here is restricted to one-trial ahead predictions only, we do not consider mechanisms using a longer prediction horizon; while such methods have been used in previous examinations of the revision of predictions with further information 
(Afrouzi et al., 2020)
, these do not offer meaningful predictions within the present framework. Such revisions are not however a key focus of the present study.


Full Information Rational Expectations (FIRE)
The Full Information Rational Expectations mechanism, or FIRE, represents the optimal prediction method given knowledge of the true data generating process, and is included as an idealised standard for behaviour. As previously noted, this is dependent on the step distribution ε t , here being a normal distribution in log space centred on 0; as such, the rational expectation on each trial is to repeat the preceding price point, regardless of target format and domain:
p e t+1 = p t
(4)
This operates on the assumption of an absolute loss function in prediction errors, leading to a focus on the median of the change distribution, being 0 in both standard and log domains. This is reinforced by the reward scheme in the experiment, which was based on absolute log error between predictions and targets, meaning expected reward is maximised where predictions match the present price. FIRE thus directly reflects the target series, and so includes no free parameters.


Direct Sampling (DS)
In contrast to the FIRE predictor which reduces the step distribution to a single rational expectation for all trials, we next consider a range of predictors which sample from this distribution in various ways, leading to greater variation in forecasts. These mechanisms are in fact free to use any distribution for this process, reflecting subjective expectations regarding future values, though we here limit these to the true distribution to both simplify and constrain these predictors. Direct sampling represents the simplest form of this process, with values being drawn directly from the step distribution according to their probability:
ln(p e t+1 ) = ln(p t ) + η t (5) η t ∼ N(0, σ 2 )
(6)
where σ 2 is the true variance of the random walk step distribution given by Equation 2. Direct sampling is therefore here functionally equivalent to a noisy form of FIRE in which the true step distribution of the target series is known but not maximised, leading to variations in beliefs across trials, though this equivalence would be diminished if using alternate subjective target distributions. This predictor then also has no free parameters, with all elements being dependent on the true data generating process.


Markov Chain Monte Carlo (MCMC)
Markov Chain Monte Carlo, or MCMC, is an alternate sampling method commonly used to approximate a target distribution using only sequential samples: a Markov chain explores the distribution by iteratively proposing new states within that distribution, comparing the current and proposed positions, and either accepting the transition or remaining in place. With a sufficient number of steps, the proportion of trials spent in each state reflects its probability.
As with Direct sampling, MCMC is here assumed to explore the true distribution of potential price changes given by the random walk data generating process, though in this case, samples
are not independent of one another but autocorrelated, meaning samples closer in time will be more similar.
In detail, the mechanism begins by sampling a proposed state from nearby space in the given representation, simulated using a Gaussian distribution around the most recent prediction with a preset variance term, with the first prediction (X e 2 ) being a free parameter:
Y e t ∼ N(X e t , σ 2 s )
(7)
where σ s represents the step-size of the sampler. The probability of this sample is then compared to the probability of the preceding sample, here again defined by the true distribution given the domain and format of the proposed value:
p(Y e t ) ∼                        logN(ln(p t ), σ 2 ) for standard price N(ln(p t ), σ 2 ) for log price logN(0, σ 2 ) for standard price change N(0, σ 2 )
for log price change 
8
We here focus on the Metropolis Hastings algorithm to determine state transitions 
(Hastings, 1970)
: if the probability of the proposed state exceeds that of the current state, it is immediately accepted. Conversely, if the probability of the proposed state is less than that of the current state, the transition probability is defined by the relative ratio of these probabilities:
p(X e t+1 = Y e t ) = min 1, p(Y e t ) p(X e t )
(9)
where the probabilities of both the proposed value Y e t and the previous sample X e t are defined according to Equation 8. The chain is then more likely to move to a lower probability state if the proposed state is more similar to the current state, and vice versa. If the proposed state is accepted, it becomes the prediction for that trial, while if the state is rejected, the previous prediction is repeated.


Metropolis Coupled Markov Chain Monte Carlo (MC 3 )
Metropolis Coupled Markov Chain Monte Carlo, or MC 3 , is another sampling algorithm which extends standard MCMC procedures to include multiple chains exploring the space simultaneously, but at different "temperatures": higher temperatures distort the target distribution, making it more uniform and therefore increasing the probability of transitions to lower probability regions 
(Geyer, 1991;
Altekar, Dwarkadas, Huelsenbeck, & Ronquist, 2004)
. This is achieved by raising the target probability distribution (that is, the true step distribution for the relevant representation defined in Equation 8) to an individual exponent for each chain: p(Y e i,t ) τ i , where Y e i,t is the proposed state now specific to chain i and τ i is the temperature parameter of that chain, defined as:
τ i = 1 1 + φ(i − 1) (10)
where φ controls the rate of difference in temperature between chains, and the first chain remains undistorted with temperature 1.
Each chain then proceeds through the same procedure described above for the MCMC predictor to determine state transitions, again using the Metropolis Hastings algorithm. Once these transitions are decided, however, chains are also compared with one another to allow potential switches in temperature: following state selections, chains are randomly paired (without replacement) and the joint probability of their respective positions is compared between their current temperatures and their switched temperatures. The probability of switching is then determined by the ratio of these products using the same transition rule used to determine state transitions:
p(switch(τ a , τ b )) = min 1, p(Y e a,t ) τ b p(Y e b,t ) τ a p(Y e a,t ) τ a p(Y e b,t ) τ b (11)
where these state probabilities are again defined by Equation 8. Once these switches are decided, the final prediction for each trial is then taken from the undistorted chain. The parameters of MC 3 match those of MCMC with the addition of the number of parallel chains c and the temperature parameter φ; with a single chain, MC 3 reduces to MCMC.


Bayesian Updating (BU)
Whereas the previous mechanisms know the true distribution used by the data generating process (but differed in their access to it), the Bayesian prediction mechanism instead seeks to infer this distribution based on both observed data and prior beliefs, updating this distribution with each new observation. For simplicity, we here assume the Bayesian predictor infers a stationary distribution within its considered format and domain; in the case of log change representations, this is accurate to the true random walk process, whereas price formats will show greater deviations as no stable price distribution exists. The definition of this mechanism follows that given by 
Anderson (1991)
 for inference of continuous variables in which the statistical properties observed in the target to that point are combined with prior parameters to form a posterior distribution from which a prediction can be drawn. The prior distribution is defined by four parameters: the prior mean µ 0 , the prior variance σ 0 2 , and the respective confidence in these values λ 0 and β 0 . Prior variance is estimated using an inverse chi-squared distribution:
σ 2 0 ∼ β 0 σ 0 2 χ β 0 2
(12)
while prior mean is estimated using a Gaussian distribution:
µ 0 |σ ∼ N µ 0 , σ √ λ 0
(13)
Note that the second parameter of this distribution is the standard deviation rather than the variance. These two distributions can then be combined to produce a t-distribution across potential values for the next point of the target series (again, the second parameter of this tdistribution is the standard deviation rather than the variance):
p(X e t+1 |X 1:t ) = t β t (µ t , σ t 1 + 1/λ t )
(14)
where the parameters of this distribution are based on combinations of the prior mean µ 0 and variance σ 0 2 with the observed meanx and variance s 2 taken from the series at time t, using the confidence values β 0 and λ 0 :
β t = β 0 + n t (15) λ t = λ 0 + n t (16) µ t = λ 0 µ 0 + n tx λ 0 + n t (17) σ 2 t = β 0 σ 2 0 + (n t − 1)s 2 + λ 0 n t λ 0 +n t (µ 0 −x) 2 β 0 + n t (18)
where n t is the number of observations at time t. Predictions can then be taken from the posterior distribution using a number of methods, though for simplicity we here focus solely on the mean of this distribution µ t .


Delta Rule Learning (DR)
The Delta Rule Learning mechanism represents a simple form of the associative learning methods described above; like the Bayesian predictor, this also estimates the next value in the target series with iterative adjustments trial-to-trial, but in this case these adjustments are based only on errors in the immediately previous prediction, controlled by the learning rate α:
X e t+1 = X e t − α(X e t − X t )
(19)
This mechanism therefore requires definition of the first prediction (i.e. X e 2 ) to begin this process, meaning this value is included as an additional free parameter. This implementation of Delta
Rule Learning also matches with Adaptive Expectations approaches used in economic models in which predictions are updated on each trial using a weighted average of the most recent prediction and target 
(Afrouzi et al., 2020)
.


Exponential Smoothing (ES)
The Exponential Smoothing predictor is a generalisation of the Delta Rule predictor allowing for similar error-based learning in trends as well as values; while the target random walk does not include such trends, we include this predictor to allow for the possibility that forecasters may mistakenly assume them nonetheless. There are multiple potential definitions of Exponential Smoothing, though for the present purposes we use the definition given by 
Gardner Jr (1985)
 for damped trends without seasonality (pg. 5, Model 7-1). As with the Delta rule predictor, this adjusts predictions according to previous error, though in this case this is separated into a point term S and a trend term T :
S e t+1 = X e t − α(X e t − X t ) (20) T e t+1 = δT e t − αζ(X e t − X t )
(21)
X e t+1 = S e t+1 + δT e t+1
where α is again the learning rate for the point term, while δ and ζ control learning of the trend term; where these trend parameters are set to 0, Exponential Smoothing matches the Delta Rule predictor defined above. As with the Delta Rule predictor, this also requires definition of a first prediction X e 2 to begin error based learning, leading to four total parameters for the Exponential Smoothing mechanism.


First-Order Autoregressive Process (AR1)
The AR1 mechanism represents a statistical approach to forecasting, using a subform of the ARIMA family of models (specifically ARIMA(1,0,0)) which predicts prices using a first-order autoregressive process where expectations are based only on the most recent observation of the target with no moving average. This takes the form of an anchor value γ plus the current deviation from that constant modified by coefficient ρ to suit all potential representations:
X e t+1 = γ + ρ(X t − γ)
(23)
A ρ value of 0 then leads to constant predictions of γ, while a ρ value of 1 repeats the previous observation; in price formats, this is equivalent to the FIRE model above, matching the original time series, while in change formats, this will produce more trend-chasing behaviour.


Memory-Weighted (MW)
The Memory-Weighted mechanism represents an exemplar-based approach to predictions, making estimates of future values based on a weighted average of previous observations to that point:
X e t+1 = t ∑ i=1 w i X i (24)
Note that in change formats, this mechanism is unable to generate a prediction in the first trial as no previous change observations are available at this point; unlike the previous models, this is not included as an additional parameter as this would necessarily differ between formats. To reflect decay in memory over time, this uses an exponential weighting system placing greater focus on more recent events:
w i = κ t−i ∑ t j=1 κ t− j (25)
where κ is a variable base parameter controlling the speed of decay of the weighting function for older observations. We here constrain the range of this parameter between 0 and 1 to focus on instances of recency bias, with 1 reflecting uniform weighting and 0 reflecting focus on only the most recent observation.


Response Noise
With the exception of the sampling mechanisms (Direct Sampling, MCMC and MC 3 ), all the above mechanisms offer deterministic predictions, producing constant forecasts for a given input series and set of parameter values. Human behaviour rarely offers such consistency,
however, suggesting some form of stochasticity is necessary; the sampling models could therefore be seen as holding an advantage in this comparison, as such variation is inherent to their operation.
To counter this, we include a noise factor in our combined models (including those using the sampling predictors) to capture potential imprecision in the expression of model predictions as actual responses, reflecting factors such as rounding or typing errors. While there are many possible definitions for such noise, in order to limit the current contrasts, we here focus on the simple addition of independent and identically distributed (IID) log-normal noise to each forecast value, allowing errors to scale with the magnitude of predictions:
ln(R t ) = ln(p e t+1 ) + δ t (26) δ t ∼ N(0, σ 2 n )
(27)
where R t is the response ultimately given on trial t, being the next value predicted by the candidate mechanism plus a randomly sampled noise term in log space, with σ n being an additional free parameter. In order to determine the contribution of such noise to model fit, however, we evaluate model performance both before and after the addition of noise, treating this as a binary factor in the comparison.


Model Comparison
To assess the correspondence of the above models with behaviour, we use a form of Approximate Bayesian Computation, or ABC 
(Turner & Van Zandt, 2012
Beaumont, 2019)
, examining the similarity between model-simulated data and that produced by human participants. ABC is used here for two key reasons: first, traditional model fitting via likelihood estimation is challenging for long prediction series such as those targeted here as each data point is related to its predecessors, meaning a series of predictions must be examined as a high dimensional object rather than many independent samples. ABC circumvents this issue by comparing model simulations with human observations on selected summary statistics, using the difference in these measures between models and participants to quantify model fit.
Second, ABC allows for a particular focus on the key features of human predictions targeted in this study by selecting these measures as the summary statistics, meaning models are evaluated on their ability to produce these specific properties rather than the raw match between human and model price predictions. While these features are not necessarily the sufficient statistics of the fitted data, it is highly difficult to define the true sufficient statistics for the complex human-generated series targeted here, if such statistics even exist. We therefore use the previously described empirical measures (see 
Figure 2
) as a proxy for these values, evaluating model performance according to the expression of these aspects.
We use two forms of ABC to compare empirical data against the candidate models.
First, we use an approximate likelihood method to estimate the fit of each simulation by placing a kernel around its generated summary statistics and using this to calculate approximate likelihoods for the participant measures 
(Turner & Van Zandt, 2012
. While these are not traditional likelihoods, they can be used in a similar fashion to compare the ability of the considered models to capture the target behaviours, allowing for designation of best-fitting models at both the individual and aggregate levels, as well as approximate distributions across parameters and measures. Second, we use a random forest (RF) analysis in which machine learning techniques are trained on the distribution of statistics from each model in order to predict the most likely assignment for each participant given their individual measures 
(Pudlo et al., 2016)
. RFs are advantageous as the use of machine learning allows for determination of the most useful measures for model discrimination from the data itself rather than requiring this to be defined a priori.
This also allows RFs to better scale when the number of summary statistics is high, whereas
the likelihood kernel method suffers as its dimensions increase (the so-called 'curse of dimensionality').
At the same time, however, RFs do not provide the same level of detail as the likelihood method in their fits, offering only the most common assignments for each participant (or a single posterior probability of that model versus all others), as opposed to distributions across all models or parameters. We therefore utilise both methods here to offer parallel perspectives on model performance, using the strengths of each approach to support the limitations of the other.
Further details on the implementation of ABC are given in Appendix B, while code used in this analysis is available on our OSF page at: https://osf.io/h57a8/?view only= d62f758c0e0b45018fa9045d2002744d.
Model comparisons involved all possible combinations of the two formats, two domains, nine prediction mechanisms and two response noise types, though as the FIRE and Direct Sampling predictors are insensitive to representation, these models were collapsed across format and domain, leading to 60 complete models. For ease of presentation, however, we here aggregate results across response noise types as these can be treated as differing values on a consistent dimension: models without response noise essentially have a noise variance parameter of 0, meaning trials of both noise types can be treated as samples from a common 'spike-and-slab' prior according to their relative trial counts (here being equal). The following results therefore also marginalise across this prior to simplify reporting, providing 30 total candidates. We do however separate the noise types when discussing the performance of the best scoring models below to assess the impact of this factor, though note that this does alter some best fitting models at the participant level for the approximate likelihood method due to changes in the marginal likelihoods between models.


Model Recovery
Before proceeding to results of the comparison, we should first examine the ability of these methods to identify and separate the candidate models. This can be assessed using model recovery exercises to determine the discriminability of the models within both the 
Figure 3
: Model assignment rates for the simulated data using the approximate likelihood analysis. approximate likelihood and random forest methodologies, though the precise procedure of these exercises does slightly differ between the methods.


Approximate Likelihood
For the approximate likelihood method, 100 simulated data sets were generated for each model using uniform sampling across their respective parameter spaces. Each artificial data set was then fit back to other model simulations using a similarity kernel around their generated statistics (outlined in Appendix B), providing marginal likelihood values for each data set from each model. The proportions of simulations best fitting each candidate model were then recorded, collapsing across response noise by treating these assignments as equivalent to reduce the size of the model space to 30 candidates.
Assignment rates for each generating model are summarised in 
Figure 3
; model recovery rates were generally moderate, with a mean of 0.387 (±0.079 95% CI), though some such as the Bayesian and MC 3 predictors had higher accuracy. This also finds certain models were better able to capture the data of other generative processes: Direct Sampling and FIRE in particular show moderate fits to several other model's data, as does AR1 in log change. While this could raise some concerns with the accuracy of this methodology, it is notable that these rates are often still considerably higher than might be expected given the breadth of the present model space: even with the current reduction to 30 candidate models, most rates are above chance level assignment, with 23 models being significantly above this criterion at the 5% level (based on a binomial test with n = 100 and p = 1/30).


Random Forest
For the random forest analysis, a measure of discriminability is provided by the accuracy of this method is assigning the simulations used as training data to their true generating process (so-called 'out-of-bag' error). 
Figure 4
 shows the accuracy rate for the candidate models in these assignments, finding similarly moderate recovery rates overall (mean = 0.536 ±0.088 95% CI), again partially due to the breadth of the model space, though these were notably higher than those of the likelihood method. In addition, all but one model were best matched by their true generating process (with the exception being Exponential Smoothing in standard change), and far above chance level. As with the approximate likelihood method, there was some confusion between certain models, in this case particularly between the MCMC and MC 3 predictors, and between the Delta Rule and Exponential Smoothing predictors, though such confusion is understandable given the similarity of these mechanisms (MCMC matches MC 3 with one chain, and Delta Rule matches Exponential Smoothing with no trend terms).


Fitting Results


Approximate Likelihood
Each model was evaluated using an ABC process to estimate its marginal likelihood for each participant, with the product of these individual likelihoods providing an aggregate 
Figure 4
: Model assignment rates for the simulated data using the random forest analysis. score across subjects. 
Table 1
 gives results for the five best scoring models by aggregate marginal likelihood, with results from all models being listed in Appendix C. For concision, we discuss only the three best scoring models in detail; features of these models are illustrated in 
Figure   5
, while 
Figure 6
 shows average likelihood distributions across parameter values. Complete simulation results are also available on our OSF page at the link given above.
The MC 3 predictor in log price held the best fit by aggregated likelihood, and best accounted for the largest proportion of participants at the individual level with a rate of 46.3%.
The predictions of this model also show a strong correspondence to participant behaviour: measures showed close adherence to the target behaviours for autocorrelations and heavy tails, though the spectral density line does appear to stray from the fit line for higher frequencies.
As shown in 
Figure 6
, marginal distributions over model parameters show reasonably stable performance across sampled values, though particularly small step sizes show a deficit in fit.
It is also notable that the distribution across chain count suggests little impact of the number of chains on fit, though the advantage of MC 3 over MCMC does suggest that the additional chains of this predictor were helpful in matching the target properties in this comparison.
Separating the samples with and without response noise, simulations without additional noise both performed better by aggregate marginal log likelihood (noisy: -64.46; non-noisy: -54.76) and best captured a higher proportion of participants at the individual level (31.7% vs. 4.88%), offering strong evidence that additive noise was not the reason for this model's success.
The second-best scoring model was the MCMC predictor in log price, which best accounted for 26.8% of participants. Predictions of this model are also fairly consistent with the target behaviours, while marginal parameter distributions again show stable likelihood across starting points, though here moderate step sizes show a clearer advantage. This may reflect a stronger dependence in fit on the step size parameter for the MCMC mechanism compared to MC 3 above: movements in MCMC are solely driven by the proposal distribution whose width is captured by the step size, whereas movements in MC 3 are also driven by swapping of chains, leading to less focus on particular step sizes. Again separating noisy and non-noisy samples, in this case model simulations including response noise showed higher aggregate fit values (noisy: -55.66; non-noisy: -82.57), but the non-noisy version of this model best fit a higher proportion of participants than its noisy equivalent (24.4% vs. 2.44%). The noisy version's advantage in aggregate likelihood thus appears to result from improved fit to those not best fit by this model initially, whereas those who were best fit are better captured by the non-noisy version. This could indicate that additional noise leads to less variation in fit between  subjects, thus producing a better aggregate measure of fit, but leading individual fits to be less distinct.
Finally, the third-best model by aggregate likelihood was the Delta Rule predictor in standard change, though this held a substantially lower likelihood than the preceding models, and only accounted for 2.44% of participants. Similar to the other top scoring models, this showed reasonable correspondence to participant behaviour in our key criteria, though unlike the previous models, changes between predictions here appear to show greater variability, with higher absolute log changes. There is also higher variability in the autocorrelation function in changes, though this predominantly remains within the bounds around zero, while the empirical quantiles show less deviation at the centre of the range. As with the previous models, this model showed stable performance across starting point, though moderate learning rates show a clear advantage, with an additional cut-off for high learning rates as these tended to produce negative price predictions in the standard change representation. Simulations including response noise held a higher marginal log likelihood value (noisy: -95.59; non-noisy: -102.50), though the non-noisy version again best fit a higher proportion of participants at the individual level (2.44% vs. 0%).
Autocorrelated sampling models thus place first and second in this comparison, accounting for a combined 73.2% of participants, with a further 9.76% being captured by the MC 3 predictor in standard price, though this placed ninth by aggregate likelihood. Delta Rule predictors also score well by aggregate likelihood, but hold much lower fit rates at the individual level, with a total of 4.88% across representations. The remaining participants meanwhile are divided between the Direct Sampling (2.44%), AR1 (4.88%) and Memory Weighted (4.88%) predictors, though these models did not perform as well according to aggregated likelihood. Sampling models thus appear to dominate this comparison, suggesting strong evidence that these mechanisms offer the best accounts for the behaviour observed in our experimental data. What is more, the above model recovery exercise shows that these results are unlikely to be attributable to the sampling models merely better capturing other generative processes, as both MCMC and MC 3 in log price show relatively little confusion with other models.
It is also valuable to examine model fit not just in relative terms as in the above comparisons, 
Figure 6
: Binned mean likelihoods across both simulations and participants for each parameter of the three best scoring models by aggregate likelihood.
but also in absolute terms: a model may perform best within a given set of candidates, but does this offer an accurate depiction more generally? To assess this, we also looked at the posterior predictive distributions across the five key measures from the top scoring models, contrasting these with the equivalent distribution from participants, as illustrated in 
Figure 7
.
Such distributions demonstrate reasonable correspondence between participants and models, though participant values do show greater variation; this is particularly notable for the tail index measure, which shows a stronger tendency towards zero than demonstrated by participants.
In addition, kurtosis values are lower for the models than the participants, with the exception of the MCMC predictor in log price. This might suggest that these models may benefit from alterations to further encourage heavy tails in order to match the higher levels seen in real forecasters. In the case of the sampling models, this could be achieved by simply sampling from a heavy tailed distribution rather than the present Gaussian, though this would of course mean that these mechanisms would no longer be drawing from the true change distribution of the random walk target. This being said, there is no guarantee within these models that forecasters have an accurate mental representation of the true distribution, meaning such distorted mental distributions are possible. We do not however consider such extensions further here in order to maintain focus on behaviour arising from the operations of the sampling method rather than aspects of the target distribution.


Random Forest
A random forest was trained on the same set of simulations used in the approximate likelihood method, using the collected distributions of the five key measures from each model to produce predicted assignments for each participant. As before, we collapsed the FIRE and Direct Sampling models across format and domain given that these predictors are insensitive to representation, providing a total of 60 models, though sample counts for all models remained equal to ensure a uniform prior. We also again collapse the results here over response noise to simplify reporting, though the RF was trained on this distinction.
Much like the likelihood method, results from the RF analysis showed strong support for the sampling models: 53.7% of participants were classified as MCMC in log price (39.0% with response noise, 14.6% without), 22.0% as MC 3 in log price (all without response noise), 12.2% as MCMC in standard change (4.88% with response noise, 7.32% without), 9.76% as MC 3 in standard price (all without response noise), and 2.44% as MCMC in log change (all without response noise), meaning all participants were assigned to sampling methods. The RF analysis thus conforms with the likelihood comparison above, suggesting the particular patterns shown by human learners best correspond with use of cognitive sampling methods, further bolstering support for these models.


Alternate Forecasting Properties
Thus far, our analyses have focused on a particular set of characteristics used to investigate deviations from idealised random walk structures in financial research 
(Shiller, 1981;
LeRoy & Porter, 1981;
Mantegna & Stanley, 1999;
Cont, 2001)
. While the use of these features does offer a novel lens into forecasting behaviour, we next examine some other measures used in previous studies, namely the psychophysical kernel of predictions, the deviation of forecasts from rational solutions, and potential optimistic biases in expectations. In addition, we also use this as an opportunity to further investigate an aspect raised in our preceding analyses which may deserve particular attention: the first lag of the autocorrelation function. The following section introduces these properties in more detail and examines their patterns in both the empirical and simulated data. For this purpose, we present posterior predictive measures for these features when considering the correspondence between human and simulated predictions on both these new properties and the original measures defined above. We then apply these measures as additional model fitting criteria to examine the impact on model performance. 
Figure 8
: Illustrations of the novel features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations). Column A shows the psychophysical kernel of predictions over the previous 5 price points; column B shows the mean absolute log deviation from the rational prediction; column C shows mean predicted log change as a measure of optimism; and column D shows the autocorrelation in the changes between predictions, focusing on lag 1. Shaded regions indicate 95% confidence intervals, and black dashed lines indicate equality.


Psychophysical Kernel
The psychophysical kernel is a statistical tool used to examine the dependence of responses on previous observations 
(Neri & Heeger, 2002;
Nienborg & Cumming, 2009;
Okazawa, Sha, Purcell, & Kiani, 2018;
Waskom, Okazawa, & Kiani, 2019
). In the present task, this can be used to capture the relation of new predictions to previous price points; while new values in the random walk target by definition depend only on their immediate predecessors, this may not apply to human forecasts of this series given the deviations from rationality described previously. The kernel is captured here via a regression of predictions over the recent history of prices, examining the influence of past values on subsequent forecasts. We therefore performed linear regressions on the prediction series of each participant and model simulation using the five preceding price values as predictors for each forecast. The kernel window was limited to the five previous points (that is, p t−4:t ) to simplify the use of these factors in model comparison, detailed further below.
Results from this analysis are illustrated in column A of 
Figure 8
, showing that participants appear to closely follow the dependence structure of the true series: predictions strongly depend on the most recent price value, but earlier values have no substantive influence. This may again reflect the lack of autocorrelations in responses, mirroring the spectral density results examined in the previous analysis: by this metric, participant predictions do follow the dependency structure of the series, whereas it is the distribution of movements which differs. The topscoring models meanwhile capture this pattern reasonably well, though MCMC in log price shows a notably high coefficient for the earlier points, while Delta Rule in standard change shows a negative coefficient for the preceding price value, potentially indicating some oscillation in predictions.


Deviation from Rational Predictions
A factor often examined in previous forecasting studies is the deviation of predictions from rational solutions 
(Wagenaar & Sagaria, 1975;
Reimers & Harvey, 2011;
Kusev et al., 2018
). As previously noted, because the rational prediction strategy for the present random walk target is to simply repeat the latest value, the rational forecast series should mirror the properties of the target; as such, the deviation in measures between the target and participant responses noted above implies behaviour strays from optimality in this task. Indeed, as shown in Appendix D, the rate of precise repetitions in participant forecasts is very low, with a mean of 6.64%, reinforcing that behaviour here deviated from this rational solution. This is however a somewhat blunt measure of rationality, giving no consideration as to the degree of optimality of individual predictions. As such, we here consider this aspect more directly by examining the precise numerical deviation of each prediction from its particular rational value (that is, the preceding price).
To provide a quantitative measure of deviation from rationality, we used the mean absolute difference between each log prediction and its preceding log price for each participant and simulation, where perfectly rational responses should produce a value of zero. These deviations are illustrated in column B of 
Figure 8
: participant scores showed moderate deviation on this measure, with a mean of 0.121 (±0.016), suggesting participants did consistently deviate from optimal forecasts in their individual responses. Furthermore, a supplementary analysis of this aspect (summarised in Appendix D) finds this measure was also stable across trials, implying participants also showed no learning of rational solutions with experience.
Deviation measures are, however, substantially higher for the models, particularly those using sampling mechanisms, as demonstrated when comparing the empirical values against their posterior predictive equivalents: MC 3 in log price shows a mean deviation of 0.250 (±0.007),
while MCMC in log price shows a mean deviation of 0.327 (±0.012). The Delta Rule predictor in standard change meanwhile shows a closer match to participant levels, though this is again slightly higher than the empirical values (mean = 0.161 ±0.012).


Optimism
Optimism refers to a bias observed in past studies of prediction in which forecasters expect new values to be higher than their true expected value, particularly for targets such as prices or sales where higher values are more desirable 
(Lawrence & Makridakis, 1989;
Harvey & Bolger, 1996;
O'Connor, Remus, & Griggs, 1997;
Reimers & Harvey, 2011)
. As the present random walk target has no consistent trend, this would thus be expressed here as a bias towards positive movements in price; we therefore measured optimism according to the mean expected movement from the most recent price point, again using log space to account for differences in scale. This is in fact a very similar measure to that used for deviation from rational expectations above, though here the sign of the predicted change matters: positive values imply optimism, while negative values imply pessimism.
Empirical measures of optimism are illustrated in column C of 
Figure 8
, showing participant expectations were not in fact optimistic in this task in general: mean predicted log movement across participants was -0.005 (±0.009), suggesting expectations were generally unbiased, though this varied somewhat between individuals. The current data thus does not appear to replicate previously observed optimism effects, placing our data in line with recent evidence suggesting optimism may not be as reliable as previously assumed 
(Shah, Harris, Bird, Catmur, & Hahn, 2016;
Burton, Harris, Shah, & Hahn, 2022;
Powell, 2022)
, though this does come from belief updating tasks rather than forecasting, so caution should be taken when generalising between these tasks. Model predictions meanwhile also tend to fall around zero, though the range of simulated values is notably narrower, while the Delta Rule predictor in standard change shows a slight pessimistic bias (-0.022 ±0.004) compared to the MC 3 (0.004 ±0.003) and MCMC (0.014 ±0.004) models.


Autocorrelation Function at Lag 1
While not a key measure in previous forecasting work, the autocorrelation at lag 1 captures an effect observed in our existing measures which may deserve further attention:
as previously noted, human predictions display little evidence of autocorrelations in their changes, as demonstrated in the low rate of autocorrelation coefficients differing significantly from zero in our preceding comparison. There is however a notable exception to this in the first lag, which tends to show substantial and significant negative correlations: participants displayed a mean coefficient of -0.325 (±0.024) for this value. This might reflect a particular reaction to feedback, with forecasters having to adjust their predictions in light of new information, and so could offer an additional signature of human behaviour not captured in the overall significant rate used above; while 
Figure 5
 does show the top scoring models in the previous comparison also tend to display such a negative autocorrelation, this was not assessed quantitatively, being only one lag within the significant rate. We therefore take this value as its own distinct measure in this expanded comparison. Model measures show close correspondence with this effect, though each of the top three scoring models does tend slightly closer to zero (MC 3 : -0.298 ±0.013; MCMC: -0.288 ±0.030; Delta Rule: -0.286 ±0.015).


Expanding the Model Comparison
Having introduced these new behavioural measures, we next apply these as additional criteria for model fitting. Such an expansion is unfortunately somewhat difficult within the approximate likelihood methodology as this relies on increasing the dimensions of the likelihood kernel, and risks overweighting certain features reflected by multiple measures (such as the multiple coefficients of the psychophysical kernel). In contrast, the random forest methodology is better able to scale with higher counts of summary statistics, as this analysis infers the most useful criteria directly from the simulated data. We therefore restrict this expanded analysis to the random forest procedure, calculating the psychophysical kernel coefficients, mean deviation from rational predictions, level of optimism and lag 1 ACF value for the existing simulations and including these as additional potential classification criteria for a new random forest. The equivalent empirical measures were then passed to this new forest to predict model assignments for each participant.
Results from this analysis are summarised in 
Table 2
. Assignments are slightly more divided here than the previous RF analysis, though this still shows strong support for sampling models overall: a total of 70.7% of participants were assigned to either MCMC or MC 3 predictors, though change formats do perform better in this case. In addition, the AR1 predictor in standard change notably performs better here, capturing the third-most participants, whereas this model was assigned to no participants previously, potentially suggesting this model is particularly well suited to these new criteria. It is also notable that models including response noise perform substantially better in this analysis, being assigned to 80.5% of the sample (compared to 43.9% previously).  
Table 2
: Model assignment rates from the random forest analysis when using the psychophysical kernel, deviation from rationality, optimism and lag 1 ACF as additional features.


Model Components


Generalising to New Series: Experiment 1
The above sections indicate that autocorrelated sampling mechanisms provide good fits to human behavioural patterns when predicting a random walk series, but have so far been restricted to a single instance of this function. This may lead to concerns with the generality of these findings, as results may differ when considering alternative target series. We thus next seek to address this concern by examining predictions of a new target series with different statistical properties, though our focus remains on random walk functions for the reasons stated previously. This also offers an opportunity to examine additional features that were not possible in the previous series.
We therefore ran a new experiment to provide human prediction data for comparison with model simulations. This experiment was highly similar to that in our previous paper 
(Zhu et al., 2021)
, though two key changes were made to the design to allow for examination of novel aspects. First, while the target series is again a Gaussian random walk, the parameters of this series were altered, leading to distinct statistical properties. In particular, the mean of the change distribution is no longer zero, allowing for overarching trends in price:
ln(p t ) = ln(p t−1 ) + ε t (28) ε t ∼ N(µ, σ 2 )
(29)
where positive values of µ make upward movements more likely, and vice versa. This change was made to allow for examination of reactions to trends in the target series, adding this as a new manipulation. Second, rather than predicting a single series, each participant predicted two distinct series in separate blocks, allowing for assessment of reactions to differing trends, as well as model performance across different targets. 


Method


Design
As with the experiment of 
Zhu et al. (2021)
, this experiment involved repeated predictions of the price of a fictional financial asset (as previously illustrated in 
Figure 1B)
. A key difference from the previous experiment however was that each participant completed two blocks of this task, each focused on a distinct series differing in their underlying trend: one block used a positive trend (µ = 0.01), while the other block used a negative trend (µ = -0.01) 4 . To make these trends apparent whilst also maintaining random movements, the standard deviation of the step distribution σ was reduced to 0.05 for both trend conditions. Each block lasted 25 minutes to fit a total experimental duration of 1 hour; this naturally limits the number of trials in each condition, but allows for examinations of model fits across trend types for each participant.
Procedure Experiment 1 followed a highly similar procedure to that of 
Zhu et al. (2021)
 as described above, with a few alterations. First, participants completed two 25 minute blocks (one positive trend, one negative trend, randomly ordered) rather than a single 1 hour task, with a 1 minute interval between the blocks. Second, a time limit was placed on each trial to encourage participants to respond quickly in order provide reasonable response counts for analysis, though the limit was set to 60 seconds to avoid missing responses. Third, the underlying log price series were each exponentiated and multiplied by 500 (rather than 100) for presentation to participants to allow more space for downward movements in the negative trend case. Fourth, the reward function was altered to provide a higher base payment and lower maximum:
reward = £8 + 2 * exp(−9(|ln(p i ) − ln(p e i )|))
(30)
where i is again a randomly selected trial, here sampled across all trials from both blocks. This again means the rational predicted movement on any trial is equal to the median of the step distribution, though here this is equal to the trend term µ in that block.


Transparency and Openness
Data from this experiment and code for the subsequent analysis is available on our OSF page (https://osf.io/h57a8/?view only=d62f758c0e0b45018fa9045d2002744d).
Definition of sample size is given above, all data exclusions are described below, and we report all manipulations and measures used in this study. This experiment was not preregistered.


Results
Similar exclusion criteria to those of 
Zhu et al. (2021)
 were applied to the data, with slight alterations to suit the design changes: data were only accepted if participants completed a minimum of 175 trials in each block (half the previous limit), and reported no interruptions during the task. 43 participants met these criteria for further analysis. Mean response counts were 246 (±11.0) for the positive trend condition and 272 (±13.9) for the negative trend condition.
As with the previous data set, predictions which differed from both their preceding and subsequent responses by a factor of 10 were treated as errors and replaced with a non-number filler; this removed 0.17% of responses across participants.


Empirical Measures
We shall first examine the empirical distributions of the 5 key measures which were the initial focus of this paper in the new data set as these offer our primary assessment of deviations from random walk structures. 
Figure 9
 shows measure distributions from predictions of both the positive and negative series, demonstrating reasonably similar properties between conditions. As with the previous data set, the rate of significant autocorrelations was again low, though in this case more participants fell below the true levels of the series (positive mean: 0.033 ±0.009, negative mean: 0.039 ±0.008). Spectral density slopes were again generally flatter than those of the target series, but were closer in the negative trend case, while both showed wider ranges than the previous task (positive mean: -1.76 ±0.09, negative mean: -1.81 ±0.07). Variance is naturally lower than in the previous data set given the reduction in the true variance for both conditions, though this was still predominantly above the actual level of the targets (positive mean: 0.058 ± 0.019, negative mean: 0.035 ±0.012). Tail indices were again mostly above 0, implying heavy tails in both conditions, though as with the previous data set some participants did also fall below this level (positive mean: 0.80 ±0.30, negative mean:
1.67 ±1.78) 5 . Finally, kurtosis again tended above Gaussian levels, though here this is more 
Figure 9
: Histograms for the five key measures from the two trend conditions of Experiment 1. Dashed lines indicate values from the target series. skewed with some participants having particularly high values (positive mean: 44.8 ±12.3, negative mean: 38.4 ±12.2). Comparisons of measures between conditions using paired t-tests found only one significant difference in variance, being higher for the positive series (t(84) = 2.09, p = .040).


Model Recovery
Given the difference in the structure of this experiment, we repeated the model recovery exercise described above to re-examine the ability of these methods to separate the candidate models, in this case considering fit across the two trend conditions simultaneously. 
Figure 10
: Model assignment rates for the simulated data from the trend experiment using the approximate likelihood analysis.


Approximate Likelihood
As with the previous data set, 100 simulations were generated for each model, here predicting both trend conditions using a common parameter set. These were then fit back to the candidate models to produce assignment rates between each true and fitted model, as summarised in 
Figure 10
. Rates were broadly similar to those of the previous data set: the mean recovery rate was 0.442 (±0.074), falling slightly above the mean rate found above, though there was again a fair amount of variation between models. All models held recovery rates significantly above chance levels (1/30), while 28 were best matched by themselves, with the exceptions both being Exponential Smoothing models due to their confusion with the Delta Rule mechanism. The Direct Sampling and FIRE mechanisms again show a tendency to capture the data of other models, though this is not as prevalent for AR1 in log change here.
Discriminative ability thus appears very similar between this experiment and the previous task.


Random Forest
Recovery rates for the random forest analysis were again determined according to the assignment of simulations used as training data by the decision trees to their true generating process, as summarised in 
Figure 11
. As with the previous data set, recovery rates for the random forests were higher than the approximate likelihood method, with a mean of 0.516 (±0.069), though the margin of this advantage is slightly narrower here. All but two models were best matched by their true generating process, with the exceptions again being the Exponential Smoothing mechanism in change formats due to its continued confusion with the Delta Rule predictor.


Model Comparison
Data from the new experiment were passed through the same model comparison procedure used for the previous data set, calculating both approximate marginal likelihoods for each model and predicted assignments for each participant from a random forest trained on the simulated data. As the task used a within-participants design, we fit both trend conditions together using common parameters under the assumption that these were stable aspects of each individual, meaning likelihoods reflect performance across series. Model definitions were the same as those given above, though as the FIRE, Direct Sampling, MCMC and MC 3 mechanisms make use of the true change distribution, this naturally differed both from the previous data set and between trend conditions. 
Table 3
 lists results for the five best scoring models by aggregate marginal likelihood across participants for the trended series experiment, while 
Figure 12
 provides a comparison of the properties of the top scoring models against participant behaviour. As in the previous data set, autocorrelated sampling algorithms in price formats performed well by this metric, 
Figure 11
: Model assignment rates for the simulated data from the trend experiment using the random forest analysis. placing first, second and third, though these do differ in domain here. In contrast to the previous results, however, rational models also performed well, with FIRE placing fourth, followed closely by Bayesian models seeking to infer the appropriate change distribution. Delta Rule mechanisms meanwhile perform substantially worse here, with the previous third place model falling to fourteenth. There is also greater division in the individual fits, with no model best capturing more than 18.6% of participants. We shall again focus discussion here on the three best scoring models for concision, with full results given in Appendix C. Parameter distributions for these models are shown in 
Figure 13
, while 
Figure 14
 compares posterior predictive measure values against their empirical equivalents, here separated by trend condition. MCMC in standard price held the highest aggregate likelihood and fit rate for this data set, in contrast to its lower placement in the previous comparison. Model predictions showed similar statistical features to the empirical data in the ACF and PSD slope, as well as heavy tails in the change distribution, though kurtosis was still generally lower than empirical levels, and demonstrated some differences between trend conditions. Parameter distributions show better fits with more central starting points, but were fairly stable across step size. This is likely due to the model here having to simultaneously account for both upward and downward trends: while intermediate starting points offer a compromise between the trend conditions, because step size does not scale with price in the standard domain, no parameter value holds an advantage as any benefit in one condition may be counteracted by a deficit in the other due to differences in price magnitude. Separating by response noise, simulations without additional noise performed better by both aggregate likelihood 
(−253.90 vs. −305.60
) and fit rate (18.6% vs. 0%).


Approximate Likelihood
The second best scoring model was MC 3 in log price, which ranked first in the previous comparison, though its individual fit rate was substantially lower here. Much like the above MCMC model, simulations again showed similar statistical features to the empirical data in the considered measures, though here deviations from Gaussian standards in changes between predictions appear to persist further towards the centre of the distribution. Parameter distributions were also consistent with those described above, showing a preference for central starting points but reasonably uniform weighting on step size. Temperature and chain count parameters were also fairly stable, though there does appear to be a very slight deficit for the lowest potential chain count. Simulations without response noise performed better by marginal likelihood 
(−251.25 vs. −307.34
) and fit rate (2.33% vs. 0%), though rates were low for both versions.    


Model Components


Random Forest
A random forest was again trained on the collected statistics from all model simulations, in this case collecting separate measures for each trend condition. Results from this analysis are summarised in 
Table 4
; much like the previous analyses, this found strong support for sampling models, with a total of 88.4% of participants being classified to either MCMC or MC 3 mechanisms, though assignments are again more divided than those found in the untrended experiment. It is also notable that these results do not show similarly strong support for rational models to the above likelihood results, with no participants being assigned to either FIRE or Bayesian change models. This may due to the RF focusing more on certain features which specifically discriminate these models, whereas the approximate likelihood method considers  


Additional Features
As with the previous data set, we next expanded the model comparison to consider additional prediction features used in previous studies of forecasting. This used the same four aspects noted above, calculating the psychophysical kernel, deviation from rational predictions, optimistic bias and lag 1 ACF value for both participant responses and model simulations, in this case separated by trend condition. These were determined using the same methods described above, with the only minor difference being that the rational prediction series now considers the trend of that condition. We shall first give some brief descriptions of these features in both the empirical and simulated data, before investigating how inclusion of these properties as additional criteria impacts model fits.


Psychophysical Kernel
Psychophysical kernel results are shown in column A of 
Figure 15
. Much like the previous data set, participant predictions showed similar dependence structures to the random walk target for both trend cases, with a strong dependency on the most recent observation but far weaker focus on earlier points, though there does appear to be more variation here between individuals. Results from the models meanwhile are more varied: while MC 3 and MCMC in log price are fairly similar to the empirical results, MCMC in standard price shows substantial deviations, placing greater focus on earlier points.


Deviation from Rational Predictions
We again measured deviation from rational predictions by calculating the mean absolute log difference between each prediction and its optimal response, in this case being based on the change distribution of that condition. Distributions of this measure are shown in column B of 
Figure 15
; participants again showed consistent deviation from rational responses, with a mean deviation of 0.039 (±0.006) in the positive trend condition and 0.046 (±0.005) in the negative trend condition. Deviation was also found to be significantly higher in the negative condition (t(42) = 2.37, p = .023), most likely because the negative series included more low price values where any deviation represents a larger proportional difference. Model simulations similarly showed persistent deviations from rational values, though this was substantially higher for MCMC in standard price (positive mean = 0.074 ±0.001, negative mean = 0.092 ±0.001) compared to MC 3 in log price (positive = 0.055 ±0.001, negative = 0.055 ±0.001)
or MCMC in log price (positive = 0.066 ±0.001, negative = 0.070 ±0.001), while posterior predictive measures show narrower ranges than the empirical distributions for all three models.
In addition, predicted deviations were significantly higher in the negative condition for MCMC in standard price (t(42) = 58.8, p < .001) and log price (t(42) = 5.31, p < .001), mirroring the 
Figure 15
: Illustrations of the novel features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations). Column A shows the psychophysical kernel of predictions over the previous 5 price points; column B shows the mean absolute log deviation from the rational prediction; column C shows mean predicted log change as a measure of optimism and trend damping; and column D shows the autocorrelation in the changes between predictions, focusing on lag 1. Colours indicate trend conditions, shaded regions indicate 95% confidence intervals, and black dashed lines indicate equality. empirical data, whereas MC 3 generated no significant difference (t(42) = 1.39, p = 0.172).


Optimism & Trend Damping
Optimism was again measured according to mean signed deviation from rational predictions given the true change distribution of each of the two trend conditions. This also allows for assessment of another feature considered in past studies of forecasting, 'trend damping', in which forecasters seemingly mute actual trends in observed series when predicting future data points: compared to their expected value, new values are underestimated for series with positive trends but overestimated for series with negative trends. This behaviour has been observed across a range of target functions, including linear 
(Lawrence & Makridakis, 1989;
Kusev et al., 2018)
, power law 
(Harvey & Reimers, 2013)
 and exponential series 
(Wagenaar & Sagaria, 1975)
, finding errors of approximately 5-10% of the target depending on series type, presentation method or noise level. Trend damping did not apply to the previous data set as that target series had no trend to damp, but can be investigated in this case by considering how predictions relate to the true trend: positive mean deviation values thus suggest damping for negative trends, and negative mean deviation values suggest damping of positive trends.
Mean signed deviation values are illustrated in column C of 
Figure 15
; much like the previous data set, participant responses did not show evidence of optimism, in this case in fact suggesting trend damping, with predicted movements being significantly below the trend term in the positive condition (M = −0.011 ± 0.007,t(42) = 3.11, p = .003) but significantly above the trend term in the negative condition (M = 0.012 ± 0.006,t(42) = 4.15, p < .001). Such a pattern is also shown in the predictions of the sampling models (MCMC in standard price: positive = −0.031 ± 0.001, negative = 0.037 ± 0.002; MC 3 in log price: positive = −0.017 ± 0.001, negative = 0.017 ± 0.001; MCMC in log price: positive = −0.027 ± 0.002, negative = 0.024 ± 0.002), though in all three cases the level of difference between conditions is notably higher than the empirical results, indicating stronger trend damping.


Autocorrelation Function at Lag 1
We again isolated the first lag of the autocorrelation function as its own measure as participant responses again demonstrated substantial negative coefficients at this lag in both trend conditions: mean values were −0.354 (±0.053) in the positive case and −0.326 (±0.049) in the negative case. Similar behaviour was also demonstrated by the top scoring models, though at a slightly lower magnitude: all three models showed initially negative coefficients at the first lag (MCMC in standard price: positive = −0.066 ± 0.006, negative = −0.078 ± 0.008; MC 3 in log price: positive = −0.185 ± 0.011, negative = −0.214 ± 0.016; MCMC in log price: positive = −0.117 ± 0.011, negative = −0.113 ± 0.008), though this effect appears to persist through subsequent lags for the MCMC mechanism.


Expanding the Model Comparison
As with the previous data set, we restricted analysis of these additional properties to the random forest method to avoid overloading the likelihood kernel with a high count of dimensions. A new random forest was trained on the expanded set of statistics from all simulations across models, again taking separate measures for each trend condition. Predicted assignments from this analysis are given in 
Table 5
. Rates are again more divided when including these additional features, though sampling mechanisms continue to perform well: a collected 79.1% of participants were assigned to these models, with the majority being MC 3 . Much like the previous data set, the expanded measures also increase support for samplers using change formats, suggesting these representations may better suit these additional properties.
In addition, rational models again perform substantially worse here than by the likelihood metrics above, capturing no participants, further suggesting these models are unable to capture the distinctive elements of human behaviour.


Discussion
The collected model comparison results given above provide evidence that the MC 3 prediction mechanism is best able to account for the specific features of human forecasts examined here, performing well across fitting methods, considered features and data sets. This was closely followed by MCMC models, which are themselves subforms of MC 3 with reduced exploratory ability, offering further evidence in support of autocorrelated sampling algorithms.
In contrast, rational models performed relatively poorly here: while the FIRE and Bayesian learning predictors had some success in fitting predictions of trended series, these methods  
Table 5
: Assignment rates from the random forest analysis for Experiment 1 when using the psychophysical kernel, deviation from rationality, optimism and lag 1 ACF as additional features. rank lower when aggregating evidence across experiments, and do not appear to capture the distinctive features of human behaviour. This reinforces the observation that behaviour in this task specifically deviates from rational principles, naturally implying that rational methods, whether based on perfect knowledge of the generating process or learning through observation, will offer limited descriptions.
Such results suggest strong support for sampling models of cognition: the particular deviations from rational behaviour observed in our empirical data are best explained by the use of stochastic mechanisms which introduce additional variability into judgments. Indeed, sampling explanations are dominant in these results: collecting across the two data sets, the three sampling mechanisms considered in this comparison best account for a combined 68% of participants by approximate likelihood and 94% by random forest classification (75% with the expanded set of measures), providing substantial evidence for their use. Moreover, these methods are here shown to provide an equivalent or even better match to behaviour than a number of other established learning mechanisms, including Bayesian, error-based learning, and memory-weighting systems. This support for sampling corresponds with findings in other areas of psychology: recent work has found sampling methods provide explanations for various biases in judgments and estimates beyond iterative predictions 
(Dasgupta et al., 2017;
Lieder et al., 2018;
Zhu et al., 2020;
Sanborn et al., 2021;
Spicer et al., 2022;
Zhu et al., 2023)
. The present results then contribute to a growing body of evidence for a general sampling approach to cognition used across many tasks and scenarios in which the limitations of human cognitive resources lead to deviations from what would traditionally be considered optimal behaviours.
Sampling models of cognition also raise an important distinction regarding the role of noise in human predictions: in contrast to sensory noise where external signals are not perfectly captured by the mind or response noise where agents make errors in translating their internal beliefs into actions, sampling methods include computational noise within the judgment process itself. This is an important distinction as this noise is thus inherent to cognition, and so influences the signatures of this process; for example, sequential dependencies between mental samples may lead to apparent autocorrelations in responses 
(Zhu et al., 2022)
. Additionally, this noise may not be detrimental to performance, but may assist in exploring alternative hypotheses 
(Sanborn et al., 2022)
. Recent work has suggested that such computational noise is in fact the key determinant of variability in behaviour, as opposed to response or sensory noise 
(Drugowitsch, Wyart, Devauchelle, & Koechlin, 2016;
Findling & Wyart, 2021)
.
The current results also offer some support for this suggestion, not just in the success of sampling models over deterministic methods, but also through comparisons between models with and without additional response noise: 84.5% of participants across the two tasks were better fit by models without this component by approximate likelihood, though this rate was considerably lower when using the machine learning method (36.9% by the base measures, 53.6% by the expanded measures). Moreover, even where these sampling models include response noise, computational noise remains the dominant source of stochasticity in their forecasts: the current simulations predict 80-90% of variability when forecasts are repeated arises from computational sources versus response errors in the best scoring models (detailed further in Appendix E), though this form of stochasticity was not directly assessed in the empirical data. Further contrasts of these noise types are therefore a key avenue for future work, clarifying the contribution of these aspects to behaviour in prediction tasks. This will likely require more targeted experimental designs to isolate these components, for example having forecasters predict the same sequence multiple times and examining the evolution of their variability across repetitions, or new model comparisons specifically contrasting differing noise definitions. This is not to say that sampling models provide perfect explanations for behaviour, of course: even in these comparisons, there are some features of human predictions which the sampling mechanisms do not exactly reproduce, such as the specific dependency patterns of the psychophysical kernel or the scale of deviations from rational responses. This being said, sampling models do also hold a particular flexibility which was not utilised in these comparisons that may allow for closer matches to behaviour: as the distribution being sampled is psychological, this can be adjusted to capture the particular beliefs of a given individual, for example increasing focus on recent values or making certain movements more likely. The current comparisons restricted the sampling mechanisms to using the true change distribution of each target series to reduce this flexibility and so simplify fitting, but alterations to capture differing subjective assumptions between individuals could improve model performance, offering a valuable line of investigation for future work. Alternatively, rather than assuming the internal distribution a priori, samplers could be combined with other learning mechanisms to acquire these distributions through experience, such as the present Bayesian predictor, which may provide greater variety in the statistical properties of the resulting forecasts.
On a similar note, the success of sampling methods in these results also does not invalidate other potential cognitive approaches to prediction: the current comparisons selected representative algorithms to reflect key modelling techniques used in the existing forecasting literature, but these are themselves each extensive frameworks, offering many potential methods to capture behaviour beyond the specific mechanisms contrasted here. The observed support for particular sampling models in these data should thus not be used to refute the broader concepts underlying the other candidates (e.g., Bayesian updating, associative learning, memory weighting etc.), simply showing that these sampling algorithms offer a closer match to the features of participant responses than the considered competitors. Such results do however provide an indication of the components needed to capture the distinctive signatures of human behaviour; in this regard, the current findings point to the value of stochasticity in cognition exemplified by sampling mechanisms, suggesting other frameworks may benefit from incorporating such variability in their own systems. Indeed, as noted when initially introducing these approaches above, existing hybrid models demonstrate the possibility of integrating components from differing branches to produce better (if more complex) depictions. It is therefore advisable to consider the present findings within these differing frameworks rather than purely as evidence for a single framework to provide a more complete understanding of the systems underlying human prediction.
A natural question given these results then is how such findings might generalise to other sequential prediction or production tasks: are these similarly well-explained by the use of sampling methods? Fortunately, the current methodology can be readily applied to these cases, examining noise patterns in such judgments to gain insight into underlying cognitive processes and so distinguish between potential models. Indeed, there has been much work suggesting that consistent noise patterns are exhibited across a wide range of human judgments, leading to the suggestion that this might be a signature of human cognition: for example, research has found evidence of long-range sequential dependencies in estimates of fixed physical quantities such as lengths and time intervals 
(Gilden et al., 1995;
Gilden, 2001;
Zhu et al., 2022)
, as well as in the inter-response intervals in generation of category members 
(Zhu et al., 2022)
. Similarly, real-world series including speech and music have also shown evidence of such dependencies using the same spectral density measures used here 
(Voss & Clarke, 1975
, 1978
Kello, Anderson, Holden, & Van Orden, 2008;
Hennig et al., 2011)
. Given the current finding that human noise patterns are best explained by the use of mental sampling methods, this might then imply that similar mechanisms also underlie these cases, with agents using sampling algorithms to explore their own mental representations. While the present random walk target is a valuable test bed for models of prediction, these alternate settings will offer a crucial test of the generality of the current findings.
These results also raise important questions on the rationality of behaviour: given both the observed deviations from rational properties and the relatively poor overall performance of rational methods in this comparison, the current data could be used to argue that human predictions should not be considered optimal. At the same time, however, the success of sampling methods here depicts behaviour from a resource-rational standpoint 
(Lieder & Griffiths, 2020;
, with participants using sampling techniques to approximate full Bayesian inference with limited cognitive resources. We therefore avoid labelling behaviour in this case as 'irrational', instead offering a different lens through which behaviour should be understood: rationality should be considered within the constraints of our own systems rather than purely objective terms.
Finally, we should note some limitations of the particular task used here. First, within the current task, predictions are restricted to only the immediately subsequent trial, which contrasts with many real-life decisions in which forecasts can reflect events some distance in the future, or even an evolving series. As such, behaviour may differ when the forecast horizon is extended, meaning further comparisons are required to assess whether the current findings extend to such scenarios. This being said, such an examination will require substantial revisions to the present models to suit alternate tasks, though some existing research does offer definitions for such cases 
(Afrouzi et al., 2020)
. Related to this, the time scale of the current task is limited to that of an hour-long experiment as oppose to the considerable scales of real predictions which could be over days or months; this is of course a necessary abstraction for laboratory studies, but future work may wish to contrast the present results with longer scale forecasts, potentially using real-world data. A final note is that the current focus on random walk targets is itself a limitation, and may not reflect the more complex series seen in everyday life, though as noted in the Introduction to this paper, the random walk target is advantageous given that its properties can be more easily defined. Nonetheless, extensions to other series will be crucial to test the generality of these findings.


Conclusion
Iterative predictions of random walk targets provide a valuable lens into how our forecasts are formed, and how they may deviate from optimal answers. In an extensive comparison of potential models, we find sampling approaches offer a particularly compelling candidate for explaining forecasts in such a task, in keeping with a growing literature describing human behaviour using sampling mechanisms. Our results thus fall in line with a general sampling approach to behaviour used across a range of tasks, though of course further work will be required to determine the extent to which such findings generalise to other scenarios. 
(i.e., Var[ln(p e
 t ) − ln(p e t−1 )]). Tail indices were calculated by fitting a Generalised Pareto distribution to the top 5% of changes between log predictions and taking the resulting shape parameter, following the procedure of 
Embrechts et al. (1997)
 and 
Kotz and Nadarajah (2000)
.
Kurtosis was taken from the distribution of changes between subsequent log predictions using the kurtosis function of MATLAB R2019b. To be clear, we do not use excess kurtosis, meaning the expected kurtosis value for a Gaussian distribution is 3.


Comparing Measures Between Series
Both experiments described in this paper used multiple instances of each random walk target to control for any potential issues with any particular instance, but collapsed across these series when reporting empirical measures under the assumption that behaviour can be treated as equivalent between these cases when reduced to the noted summary statistics. We here further assess this assumption by contrasting empirical measures from each series to test for any differences. 
Table 6
 shows mean measures separated between the two potential series (labelled 'A'
and 'B') from each data set, with a further separation by trend condition for Experiment 1.
We performed independent t-tests to compare the measures between the subsets of participants viewing each series, with Bonferroni corrections on the significance criterion given the number of comparisons (α = .05/15 = .003). No significant differences were observed between series in any of the measures at this level, indicating behaviour can be considered as reasonably equivalent across random walk cases. It is notable however that spectral density slopes did
show near-significant difference for both trend conditions of Experiment 1; this is somewhat surprising given that the only differences between these cases here were in their random movements, with no association between trend conditions. This could then be attributable to individual differences in participants rather than reactions to the series, with participants assigned these series happening to generally show flatter PSD slopes. Further contrasts with a wider range of series are therefore advisable to confirm how robust these results are to differing targets.  
Table 6
: Comparisons of the key measures between alternate series in each data set, including mean values for each series and results from independent t-tests.


Appendix B -Modelling Procedure
As noted in the main text, model evaluation involved an Approximate Bayesian Computation (ABC) process for each model to determine the correspondence between its predictions and human responses using two parallel methodologies: approximate model likelihood and random forest assignment. To provide data for both analyses, each model was simulated at 10,000 independent parameter sets sampled randomly from the model's prior parameter distribution.
We used IID sampling rather than the MCMC-like processes of previous ABC methods (e.g. 
Turner & Van Zandt, 2018;
Beaumont, 2019)
 to allow for evaluation of likelihoods for all participants simultaneously. To avoid strong assumptions on likely parameter values, all parameters for all models were assumed to have uniform prior distributions across predefined ranges (detailed below). Models predicted the same random walk series given to participants in the experiments, though to simplify fitting, only one series was used in each comparison (with separate series for the positive and negative trend conditions in Experiment 1). Each simulation ran for 512 prediction steps in the first comparison and 256 in each trend condition of the second comparison to roughly correspond with the length of the human data in each task, as well as to give a reasonable count for analysis. Any parameter sets which produced negative predictions of price were discarded and replaced.


Approximate Likelihood
The collected simulations were used to approximate the marginal likelihood of each model when focusing on the key qualitative features observed in our experiments; while these are not true likelihood scores in the traditional sense, these fulfil the same role, and so are referred to as likelihood here for ease of interpretation. In particular, they can be considered approximations to the likelihood of the statistics of the true generating model, assuming that each participant follows the same model and set of parameters.
Each simulated prediction series was assessed on the 5 measures detailed in the main text, calculating the raw difference in these measures between simulated and actual data. For the different trend conditions of Experiment 1, these measures were calculated separately in each condition and treated as distinct dimensions. These differences were then passed through a similarity kernel function to return a probability of the observed data given the model prediction, representing an approximate likelihood for that simulation. For simplicity, we used a multivariate Gaussian kernel to fit all measures simultaneously, with mean 0 for each dimension and covariance equal to that of the participant scores across measures in that experiment to account for any relationships between features. Ideally, this kernel should capture the noise in the measures for that parameter set if the simulation were repeated, though in practice this is difficult to specify in advance (without extensive additional simulations). In addition, many of the candidate models are deterministic, meaning no such noise function applies; the Gaussian kernel could thus be seen as a concession to these models, as these would otherwise hold only infinite or zero likelihoods depending on whether or not they achieved a perfect match.
Likelihoods were calculated for each participant separately to produce individual distributions across the sampled parameter values rather than assuming common parameters between subjects.
After all simulations for a model were complete, marginal likelihoods were approximated for each participant by taking the mean likelihood across the randomly sampled parameter values. Aggregate scores across subjects were then calculated by taking the product of all participants' marginal likelihoods. To provide the model measures shown in the figures of the main text, simulations were reweighted using a form of importance sampling based on the discrepancy ratio between the prior and posterior distributions, which reduces to the likelihood normalised by its own sum. These weights were then applied to each simulation during aggregation to provide weightedaverage measures for each participant, as summarised in 
Figures 5, 7
, 12 and 14.


Random Forests
Random forests were used to predict most likely model assignments for each participant by training a set of 500 classification and regression trees (CARTs) to distinguish the candidate models based on their particular distribution of statistics, following the procedure of 
Pudlo et al. (2016)
. The same simulations used in the approximate likelihood analysis were compiled into a 'reference table' of summary statistics for all models and passed to the forest for training, with each tree using a randomly sampled subset of the simulations and measures. Simulation counts were equal between models to ensure uniform prior probability. Once the forest was trained, empirical measures were passed to the trees to provide predicted classifications for each participant, with the majority vote across trees determining assignments.


Parameter Ranges
Parameter ranges were defined according to the role of that parameter in the given representation and any relevant information available in the target price series, with some parameters from different models sharing bounds due to the similarity of their role in their respective models. The starting point of the MCMC and MC 3 predictors X e 2 , the prior mean of the Bayesian predictor µ 0 , the first prediction of the Delta Rule predictor X e 2 and the constant of the AR1 predictor γ shared common bounds, being defined by the first value in the target series (139 for the first comparison, 500 for both trend conditions of the second comparison)
due to the impact of format on these parameters: for price formats, these parameters were restricted to fall within 50% of the first value in the price series, while for change formats, changes were restricted to ±50% of the first price value. Ranges were matched between standard and log domains. The step size of the MCMC and MC 3 predictors σ s was restricted between 0 and 50% of the upper limit of the first prediction X e 2 . The temperature parameter φ of the MC 3 predictor was fixed between 1 and 10, while the number of chains c was fixed between 2 and 10 (as 1 chain reduces to MCMC), though note that unlike other parameters c is also restricted to discrete values. The prior variance of the Bayesian predictor σ 2 0 was restricted to be between 0 and 20% of the upper bound of the prior mean µ 0 , while the confidence values β 0 and λ 0 were restricted between 0 and 30. Finally, the Delta Rule learning rate α, the Exponential Smoothing learning rates α, δ and ζ, the AR1 persistence rate ρ and the Memory Weighted base parameter κ were all restricted to fall between 0 and 1.


Appendix C -Model Scores
Models were primarily scored according to the aggregate marginal likelihood across participants, though best-fitting models were also determined for each individual according to their maximum marginal likelihood across models as an alternate metric. 
Table 7
 shows the aggregate scores for each of the considered model combinations as well as the percentage of participants best fit by that model at the individual level for both the untrended experiment of 
Zhu et al. (2021)
 and the trended experiment of this paper.


Collapsing Factors
As noted in the main text, the models examined in this comparison are composed of distinct factors, separating each prediction mechanism from its representation. This might naturally lead to questions of how certain factors perform when collapsing across others, such as how each predictor fares when disregarding format and domain. We avoided such comparisons in the main text as we believe it is best to view the models as packages of components due to interactions between elements; for example, the behaviour predicted by the MCMC predictor can be very different between price and change formats. In case this is of interest, however, we here report simplified comparisons reducing the above results for each factor.  
Table 7
: Approximate marginal log likelihood (MLL) results for all model combinations for both data sets. Note that these measures aggregate across noisy and nonnoisy versions of each model, while the FIRE and Direct Sampling predictors are insensitive to format and domain.
when marginalising across all other factors (assuming all levels of each factor are equally likely) from both experiments. First examining the predictors, this again finds support for sampling models: MC 3 holds both the highest score and largest proportion in the untrended task, followed by MCMC and Direct Sampling. Samplers also score well in the untrended task, though the margins are narrower here, while FIRE has the highest fit rate, again demonstrating the better performance of the rational models in this experiment Format shows a fairly even split in the untrended task, but change holds a substantial advantage in fit rate in the trended task. Domains show stronger support for log representations over standard in both likelihood and fit rates for both experiments, suggesting an advantage for representations able to scale with price magnitude. Finally, response noise shows distinct effects between tasks, with models with additional response noise performing better than those without in the untrended task, but vice versa in the trended task. As referenced in the main text, however, the best fitting sampling models did tend to show better fits without such noise, again suggesting caution should be taken when considering the fit of each factor separately rather than the full model packages.


Relaxing Exclusions
As noted in the main text, exclusions were applied to our empirical data to remove responses with extreme deviations from both directly preceding and subsequent estimates to filter potential errors. Questions may then be asked as to how the above results might differ if these exclusions are relaxed, treating each participant's sequence as an accurate reflection of their beliefs regarding future values. We therefore examined this by recalculating participant measures without response exclusions, and re-evaluating likelihood scores from the model simulations with these as the fitting criteria using the above method.
Results from the top five scoring models of each data set from this analysis are given in 
Table 9
, showing this not only generally reduced the fit of the models in both tasks, but also led to some differences in ranks and fit rates. For the untrended task, MC 3 in log price remains first by aggregate likelihood, though its proportion best fit falls from 46.3% to 9.76%, while the previously second place MCMC in log price falls to sixth, with a similar reduction in fit 
Zhu et al. (2021)
 Experiment  Differences are however seen in the fourth and fifth place rankings, with the FIRE and Bayesian models being replaced by other sampling mechanisms, suggesting reduced support for rational models.
Relaxing these exclusions thus appears to alter results somewhat, particularly for the untrended experiment which shows greater division in fit rate across models. Even so, these results continue to offer support for sampling models, still placing highly in both comparisons, and seemingly dominating in the trended task. It is also interesting that this change substantially  
Table 9
: Model scores for the top five scoring models of each data set by aggregate likelihood when extreme responses are not excluded from participant data.
increases evidence for the AR1 model in the untrended comparison, here ranking second by both aggregate likelihood and fit rate; this might suggest that this predictor has a particular ability to capture extreme fluctuations. Once again, however, caution should be taken when interpreting these results, as part of the behaviour captured by these models may simply reflect basic errors in response rather than stable signatures of the underlying cognitive mechanism.
Our focus therefore remains on the comparison including exclusions featured in the main text as this should be more robust to such influences, though this is a valuable avenue for future assessment of these models.


Appendix D -Additional Empirical Measures Volatility Clustering
In addition to the features considered in the main text, another key aspect examined in our previous comparison of individual predictions and market movements is volatility clustering: periods of volatility tend to be clustered together in time with intervening periods of relative calm. This can be measured according to a slow decay in the autocorrelation function in absolute log change between predictions over increasing lags 
(i.e., Corr[|ln(p e
 t ) − ln(p e t−1 )|, |ln(p e t+k ) − ln(p e t+k−1 )|], where k is the lag), with markets being described by power-law decays with exponents between -0.2 and -0.4 
(Cont, 2001)
. In our previous paper, we found evidence of similar volatility clustering in the aggregated predictions of small groups of participants, though results from individuals showed notably faster rates of decay 
(Zhu et al., 2021)
. Examining each participant's volatility autocorrelation function separately, however, demonstrates substantial variation in observed autocorrelation values, perhaps suggesting power-law fits provide a poor reflection of such patterns in this case. We therefore did not consider volatility clustering as one of our criteria for model fitting in the above comparison as this measure may not be sufficiently reliable to distinguish between candidate models. We do however here provide a supplementary investigation of the impact of volatility clustering on the present results, determining whether inclusion of this property substantially alters findings.
To examine the impact of volatility clustering on model fits, we performed a supplementary model comparison including this feature alongside the five key measures: power-laws were fit to the autocorrelation function in absolute log change for each simulated prediction series where the resulting exponent acted as the key statistic. This measure was then used along with the five principle measures to recalculate likelihoods for each simulation for each participant using the same method described above, producing new marginal likelihood values. Results of this analysis are summarised in 
Table 10
, focusing on the top scoring models for concision.
While the inclusion of an additional criterion naturally lowers likelihood for all models, these results are highly consistent with those of the main text: MC 3 and MCMC in price formats continue to score well with only slight differences in the proportion of participants best fit.
The inclusion of volatility clustering as an additional model criterion thus does not diminish the strong support in these results for sampling models, suggesting these methods are also able to account for such behaviour, though again caution is advised given the poten"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]