You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Main text
The reinforcement learning framework Reinforcement learning (RL) captures the ability to guide choices to maximize the occurrence of pleasant events (rewards) and minimize the occurrence of unpleasant events (punishments) 
(Sutton & Barto, 2018)
. In other words, RL is the process of learning through experience, employing trial and error to optimize outcomes 
(Dayan & Abbott, 2005)
. RL has immense adaptive value, which explains its presence across diverse group of species (phyla), including nematodes, arthropods, mollusks, and chordates 
(Brembs, 2003)
.
Reinforcement learning in animals and humans exists in various forms. One form is classical or Pavlovian conditioning, which is of less concern here because its effects are exerted without involving a decision-making component and are accordingly referred to as "reflexes". A wellknown example is Pavlov's dog, which learned to salivate in response to a neutral stimulus previously associated with food 
(Lavond & Steinmetz, 2003)
. The other form of reinforcement learning, known as instrumental or operant learning, is the primary focus here, as it deals with how past outcomes shape future decisions (J. E. R. 
Staddon & Cerutti, 2003)
.
The reinforcement-learning process is often summarized by the idiom "carrot and stick." What does this phrase mean? It refers to rewards and punishments, which are specific stimuli capable of eliciting particular responses 
(Frank et al., 2004;
Palminteri & Pessiglione, 2017;
Shohamy et al., 2004;
Young, 1952)
. In the context of instrumental learning, rewards and punishments are defined precisely by their effects on the frequency of future actions. Rewards are stimuli that increase the frequency of the actions that precede them and reduce the frequency of actions that prevent their delivery. Conversely, punishments are stimuli whose delivery decreases the frequency of the preceding action and whose omission increases it ( 
Table 1)
. While these are necessary operational definitions, we all have an intuitive understanding of what constitutes a reward (e.g., good food, sex, success, winning money) and a punishment (e.g., pain, losing money). 
Table 1
. The table illustrate different configurations of instrumental learning in relation to different types of outcomes (rewards and punishments) and their effects on the subsequent action (reinforcement: increase in frequency; weakening: decrease in frequency). The frequency of an action increases if it is rewarded or if it allows escaping a punishment. Symmetrically the frequency of an action decreases if it leads to a punishment or the omission of a reward.
This learning mechanism (improving choices based on the past experience of rewards and punishments) can capture virtually all levels of decision-making. For instance, RL guides motor learning, such as mastering the use of a spoon. In this example, the reward is represented by successfully eating without spilling food, while the punishment is represented be dropping food and creating a mess. Similarly, RL may underpin higher cognitive tasks cognitive tasks, like optimizing the route between home and work; educational strategies, such as tailoring study methods for university exams based on past performance; and professional activities, like a physician refining treatment based on therapeutic outcomes 
(He et al., 2022;
Jayaraman et al., 2024;
Palminteri et al., 2011)
. More recently, the RL framework has been applied to explain the dynamics of financial markets, as well as social media engagement 
(Lindström et al., 2021;
Lussange et al., 2024;
Turner et al., 2024)
. In essence, RL describes an evolutionary old and preserved cognitive building block that can underpin the generation of more complex behaviors.


Cognitive bias: an ecumenic definition for a debated concept
After this general preamble on some basic notions of reinforcement learning, it is essential to introduce an operational definition of cognitive bias that aligns with our specific goals and could also serve as a valuable resource for future explorations of this topic. Establishing such a definition is particularly important because "cognitive bias" is one of the most contested concepts in the broader rationality debate. This debate contrasts the view that human decision-making should be evaluated against normative standards of logic and probability with the perspective that rationality lies in finding efficient solutions to real-life constraints 
(Brighton & Gigerenzer, 2015)
. This is particularly important given that the notion of cognitive bias plays a central role in the debate on how and when psychological insights should be used to improve, shape, or inform policymaking across various domains, such as economic outcomes, well-being, and climate change. 1: information processing-based illustration of noise and bias within cognitive processes, whose goal is to transform an objective value (input) into a subjective one (output). (A) Noiseless and unbiased process. (B) Noisy and unbiased process. In the simulation we used a Gaussian noise, with a mean of zero and a variance proportion to the value of the objective variable (Weber's law) (C) Noiseless and biased process. In the simulation, the cognitive process overestimates the value of the objective variable in the lower range and underestimates it in mid/high range. (D) Noisy and biased process. We argue that much of this debate could be simplified-or even circumvented-if a clear and operational definition were available. Traditionally, within the heuristics and biases framework, cognitive biases are often treated as flaws in human cognition-features that undermine performance, coherence, or lead to undesirable outcomes across various contexts 
(Camerer, 1998;
DellaVigna, 2009)
. In contrast, the adaptive rationality perspective views these heuristics and biases as elements of an (evolutionarily or environmentally) crafted adaptive toolbox. From this perspective, cognitive biases, while not necessarily normative in a logical or statistical sense, are seen as highly adapted to real-world, ecological scenarios 
(Gigerenzer & Gaissmaier, 2011;
Haselton et al., 2009
Haselton et al., , 2015
Hertwig et al., 2022)
. The divergence between these frameworks primarily lies in the valence ascribed to the instrumental or functional consequences of cognitive biases-whether they are considered negative or positive. For this reason, we believe that fruitful debate and characterization of cognitive biases require adopting a definition grounded in computational and information-processing terms, which emains independent of any a priori judgment of biases as inherently "good" or "bad."
Rooted in the metaphor of the brain as a computer, our operational definition of a cognitive bias begins with the assumption that a cognitive process can be conceptualized as a series of computational operations transforming input into output. The input is characterized as having an "objective" value, such as the physical properties of sensory stimuli (e.g., light intensity or sound pressure levels impacting the eyes or ears), which is then transformed into a subjective (internal) estimation (or representation). This transformation is carried out by the cognitive process, which can vary in its features. In an idealized scenario, the process might be noiseless and unbiased ( 
Figure 1A)
. In such a case, the relationship between the objective variable and its subjective representation would be linear and error-free, theoretically allowing perfect inference of the objective value from its subjective counterpart. However, this scenario is highly unrealistic for biological cognitive systems, facing in open-ending, stochastic scenarios.
Cognitive processes are performed by the brain, a biological organ whose physical implementation inherently introduces errors and compromises precision due to the trade-off between accuracy and efficiency. Moreover, many objective variables are intrinsically stochastic, meaning that any given sample may not perfectly represent its true distribution at a given moment. As a result, subjective representations are realistically affected by both internal (computational errors generated by the process itself) and external (random errors in variable sampling) sources of noise ( 
Figure 1B)
 
(Faisal et al., 2008;
Findling et al., 2019;
Sanborn et al., 2024)
. Although noise reduces the capacity to perfectly infer objective variables from subjective estimations, it does not introduce systematic deviations between the two.
Systematic deviations, however, are a hallmark of biased cognitive processes ( 
Figure 1C)
. The key distinction between noise and bias lies in their nature: noisy processes create random, nondirectional distortions, whereas biased processes produce systematic deviations between objective variables and their internal representations. Importantly, bias and noise are orthogonal features of cognitive processes; they are independent and can coexist in practice ( 
Figure 1D
).  It is now crucial to understand why, from this straightforward yet robust information-processing perspective, cognitive biases are not inherently negative features of a cognitive process. This requires distinguishing between epistemic accuracy and instrumental accuracy. Epistemic accuracy refers to the correspondence between the objective value of a variable and its internal representation-in essence, the degree to which the knowledge (in ancient Greek epistemeἐπιστήμη -means knowledge) of the cognitive system aligns with reality. In this framework, maximum epistemic accuracy is achieved by noiseless and unbiased cognitive processes.
However, for biological cognitive systems (until recently the only type of cognitive systems), epistemic accuracy is not always the primary goal. Instead, the primary goal is often to enable the best possible course of action-what we refer to as instrumental accuracy. Ultimately, survival depends not on having perfect knowledge of the prey's distribution but on successfully catching the rabbit to satisfy immediate needs. Although epistemic and instrumental accuracy often align, they can diverge. In some cases, distorting or ignoring certain pieces of objective information may lead to better decisions. Consequently, when focusing on instrumental accuracy, the value of cognitive biases should not be assessed based on their contribution to epistemic accuracy but rather on their role in facilitating better decisions, regardless of the gap between subjective representations and reality. In the remainder of this paper, we will explore examples of the dissociation between epistemic and instrumental accuracy within the context of reinforcement learning.


Reinforcement learning formal framework
We now turn to a more formal description of the reinforcement learning process. A reinforcement learning agent operates within an environment that provides a context, offering options among which the agent can choose, and rewards that follow the agent's choices. In this framework, options, actions, and rewards are the observable elements.
The internal representations of a reinforcement learning agent refer to the latent variables and stored information that underlie its learning and action-selection (or decision-making) processes. While there is no complete consensus, most accounts agree that these representations include context-specific action values, which guide the selection of the most advantageous or appropriate action for a given trial and context. These context-specific hidden values are typically updated after the agent receives new outcome (reward or punishment), reflecting a dynamic process of learning and adaptation 
(Figure 2A)
.
At the formal level, the process of selecting an action in reinforcement learning is often implemented using a Softmax function 
(Luce, 2005)
. This function determines the probability of choosing a particular option based on the difference in value between the available options.
(Eq.1)
( ) = 1 1 + ( ( , )− ( , )) *
(where and are available options in context and is a parameter governing the stochasticity of the process). While other selection rules exist, the Softmax function remains the most widely used in behavioral reinforcement learning due to its simplicity and effectiveness.
Once a new outcome is obtained, whether it is a reward or punishment, the update process begins with the calculation of a reward prediction error 
(Ljung, 2002;
Niv & Schoenbaum, 2008)
. This prediction error is the difference between the reward received and the value previously assigned to the chosen option.
(Eq.2) = − ( , )
(where is the reward obtained in a given trial). The prediction error is then used to update the estimated value of the option through a delta rule, a straightforward and intuitive error correction model 
(Lee et al., 2020;
Rescorla & Holland, 1977)
.
(Eq.3) ( , ) ← ( , ) + *
This prediction error and delta rule framework is not only foundational in machine learning (dating back to the perceptron) but is also deeply established in human and animal behavior, with welldocumented neural correlates 
(Fouragnan et al., 2018;
Schultz & Dickinson, 2000)
.
With this basic model in place, we can now identify potential sources of bias within this framework, as defined earlier. These biases fall into two categories. The first arises at the interface between option values and action selection-essentially, in the selection process. The second occurs at the interface between outcomes and option values, during the update process ( 
Figure 2B
).
Because they pertain to action selection, we term the first category of biases praxic (from the Greek "praxis", πρᾶξις, meaning action). Praxic biases are characterized as mismatches between the calculated option values and the actions that are actually chosen. These mismatches may arise from interference between different memory systems. A common example of a praxic bias in human reinforcement learning is choice repetition or perseveration, where actions are repeated regardless of the rewards received, causing actions to deviate from the calculated option values.
The second category of biases is epistemic, because they relate to the agent's internal representation of option values (in a sense, the 'knowledge' of the agent). Epistemic biases reflect mismatches between the outcomes experienced and the values assigned to the options. In the remainder of this paper, we will explore two specific epistemic biases in detail, examining them at computational, behavioral, and functional levels. : two epistemic biases of human reinforcement learning with their key equations mapped into the reinforcement learning agent. The first step is represented by subjective outcome encoding (relative valuation), the second one the error prediction error calculation, the third one by value update and, finally, the last one by action selection.


Epistemic biases in reinforcement learning
In this section, we will describe in detail two epistemic biases recently reported in human reinforcement learning, both by our lab and others around the world 
(Figure 3)
. The first bias concerns the encoding of outcomes, occurring before the calculation of the reward prediction error. The central idea is that objective rewards (e.g., the amount of money a participant wins on a given trial) are transformed into subjective rewards 
(Palminteri & Lebreton, 2021)
. This transformation is influenced by other outcomes encountered in the same context, either simultaneously (spatial context) or in the recent past (temporal context).
Conceptually, this effect is analogous to classic optical illusions, such as the Ebbinghaus circles or the Müller-Lyer arrows, where subjective judgments about size are influenced by the context surrounding of the object under evaluation 
(Bratzke, 2024)
. In reinforcement learning, outcome context-dependence often manifests as a normalization process. Two popular functional formsexpressing how the input are related to the output values-have been proposed for this normalization. The first is reference point-dependent normalization, a feature shared with prospect theory and consistent with the phenomenon of incentive relativity, where motivation for a reward depends on the expectation of the average reward in a given situation 
(Flaherty, 1996;
Kahneman & Tversky, 1979)
. Here, the subjective value of an outcome is rescaled as a function of the average value of the context in which it is experienced, a context value known as the reference point (a process often referred to as reward "centering") 
(Hayes & Wedell, 2023a;
Naik et al., 2023;
Palminteri et al., 2015)
. Another popular functional form of outcome context-dependence is range normalization, in which the subjective value of an outcome is rescaled based on the minimum and maximum outcomes possible within a given context, defining the contextual range 
(Bavard et al., 2018
(Bavard et al., , 2021
Bavard & Palminteri, 2023;
Hayes & Wedell, 2023b)
, as illustrated in the equation below:
(Eq.4) ( ) = − ( ) ( ) − ( )
Where ( ) and ( ) are the contextual he contextual bounds, i.e., the current estimations of the maximum and minimum rewards expected in a given situation. Both functional forms have been descriptively successful in explaining human behavior in reinforcement learning tasks. However, the range normalization rule is more general, as it can account for some effects traditionally explained by reference point centering, whereas the reverse is not true 
(Bavard et al., 2021)
. A typical experimental demonstration of outcome context-dependence employs a twophase design, comprising a learning phase and a transfer phase. During the learning phase, participants learn to associate options (e.g., A, B, C, D) with specific outcomes. These options often have different expected values, which refer to the average reward one can expect from a given option. During the learning phase, are presented within stable contexts (e.g., option A is always presented with option B, while option C is consistently paired with option D). Following this, the transfer phase, also called the post-learning or generalization phase, requires participants to make choices between options presented in novel pairings (e.g., B with C). The architecture of the learning phase is designed such that the preferences expressed during the transfer phase will differ depending on whether option values are encoded in an objective manner or in a context-dependent manner (e.g., using range normalization) 
(Figure 4)
. Importantly, the options in the transfer phase are often structured in a way that results in an economic cost for participants who encode outcomes and option values in a context-dependent manner. Specifically, such encoding leads to suboptimal choices that minimize expected value, meaning that the choices made result in lower rewards compared to the best possible options. This pattern is frequently, if not systematically, observed and provides compelling evidence for the prevalence and impact of context-dependent biases in human reinforcement learning Evidence supporting context-dependence in reinforcement learning tasks, particularly those employing a learning/transfer design as described above, appears to be both robust and highly replicable. These findings have been consistently reported across various countries and species, suggesting a very preserved and generalizable phenomenon 
(Anlló et al., 2024;
Pompilio & Kacelnik, 2010;
Solvi et al., 2022)
. Similar patterns of context-dependent behavior have been observed across a broad range of outcome values, encompassing both gains and losses, different outcomes magnitudes, and outcomes distributions such as Gaussian or Bernoullian 
(Bavard et al., 2021;
Bavard & Palminteri, 2023)
.
Moreover, recent evidence suggests that this context-dependent reward encoding is genuinely epistemic, as it affects participants' declarative memory and knowledge 
(Soukupova et al., 2024)
. Post-learning preferences consistently display clear signs of context-dependence, regardless of whether values are inferred through choices or directly assessed via episodic memory or subjective ratings. This further underscores the stability and pervasiveness of context-dependent biases in human reinforcement learning, as well as its truly epistemic nature.
Another well-documented epistemic bias in human reinforcement learning concerns the update step that occurs after the calculation of the prediction error 
(Figure 3)
. Specifically, studies have shown that option values are updated more following positive prediction errors than negative ones 
(Palminteri & Lebreton, 2022)
. Computationally, this bias is modeled by assuming different learning rates for positive and negative prediction errors 
(Figure 5)
.
(Eq.6) ( , ) ← ( , ) + { + * , > 0 _ * , < 0
Where the asymmetry + > _ indicates that individuals learn faster from outcomes that are better than expected, while + < _ suggests faster learning from worse-than-expected outcomes. Conceptually, it resembles the "good news/bad news" effect observed in belief updating across various domains, which may contribute to the broader optimism bias often displayed in decisionmaking 
(Sharot & Garrett, 2016)
. However, it is important to note that this update bias favoring positive prediction errors is not unconditional as it depends on several key factors. Notably, the bias is observed primarily when outcomes follow freely chosen actions, meaning rewards or punishments are directly tied to an individual's performance 
(Chambon et al., 2020)
. Conversely, the bias is reversed when learning from the outcomes of forgone options (i.e., counterfactual updates). This reversal is intuitive: a negative prediction error for a forgone outcome signals "good news" for the participant (e.g., avoiding a negative outcome), while a positive prediction error for a forgone option signals "bad news" (e.g., missing out on a reward) .
The positivity bias in option value updating is a robust phenomenon, replicated across species and across a variety of task designs 
(Farashahi et al., 2019;
Ohta et al., 2021)
. Strikingly, it has been observed even in scenarios involving primary punishments (e.g., electric shocks) and in volatile environments 
(Gagne et al., 2020)
. The latter condition is particularly informative, as volatile tasks-featuring frequent changes in option values-require heightened sensitivity to negative prediction errors, which signal shifts in contingencies and should prompt a change in choice policy. In such situations, the behavioral hallmark of the positivity bias manifests as a tendency to persist with previously rewarded options, even in the face of accumulating negative feedback.  
Figure 1
, how we move from unbiased and noiseless update to biased and noisy one. More specifically the graph shows the relationship between the prediction error and the Q-value modification in the positive (green) and negative (red) domains. Note that the noise takes the form of a Gaussian noise with a mean of zero and variance proportional to the size of the prediction error, while the update bias takes the form of a positivity bias (the slope is higher positive, compare to negative prediction errors).
Research has shown that asymmetries in learning rates for positive and negative prediction errors predict subjective and explicit confidence ratings during reinforcement learning tasks 
(Salem-Garcia et al., 2023)
. Specifically, the positivity bias has been linked to over-confidence, defined as the tendency to overestimate one's own probability of success. This connection suggests that the asymmetry in learning rates for positive and negative prediction errors may not only influence the updating of option values but also shape subjective judgments about performance and outcomes. By favoring positive feedback over negative, individuals may develop an inflated sense of confidence in their decisions, reinforcing the cognitive and behavioral patterns associated with optimism bias 
(Gervais & Odean, 2001)
. The fact that the signature of the bias (in this case over-confidence) is reflected in the explicit reports of the subject, further underscores the "epistemic" nature of the positivity bias.
Finally, additional evidence for the stability and pervasiveness of these biases in human cognition comes from recent pioneering studies examining the reinforcement learning capacities of Large Language Models (LLMs), such as ChatGPT, Claude, Llama, and PaLM. In these studies, bandit tasks, similar to those used to uncover these biases in humans, were adapted into textual formats and presented to LLMs. The responses generated by the models were then collected and analyzed using computational methods similar to those applied to human data 
(Yax et al., 2024)
. The results revealed that, when fitted with computational models, LLM-generated responses exhibited both relative value learning biases and positivity bias 
(Hayes et al., 2024;
Schubert et al., 2024)
. Given that these models are trained on vast corpora of human-generated text, followed by additional training with human feedback, these findings suggest that relative value encoding and positivity bias are deeply embedded in the way human language is produced and used.


The possible adaptive values of epistemic biases
Epistemic biases in human reinforcement learning have been widely studied in contexts where they are shown to be detrimental to performance. For example, relative value learning has been shown to induce expected-value-minimizing choices in novel decision-making scenarios, where options are extrapolated from their original learning contexts 
(Bavard et al., 2021)
. Similarly, the positivity bias has been found to promote reluctance to switch options in reversal learning tasks, where option values change, causing previously correct options to become incorrect . Demonstrating these biases in situations where they result in economic costs is significant because it reinforces the notion that these biases are relatively rigid features of human cognition. This methodological focus often perpetuates the view that cognitive biases are flaws. The very existence of these biases, however, suggests they have been shaped by evolution and may serve some adaptive functions. This perspective is frequently overlooked when research focuses solely on contexts where these biases appear detrimental, neglecting the potential advantages they may offer to decision-makers in other environments. One potential adaptive advantage lies in the generation of self-serving beliefs and representations, which we term "weak" optimality. The "weak" aspect refers to the idea that these biases may not lead to optimal outcomes in terms of maximizing rewards or efficiency within a given reinforcement learning context. Optimality, rather, lies in their ability to enhance subjective utility, by producing internal beliefs and representations that are self-serving 
(Shepperd et al., 2008)
. For instance, evaluating outcomes relative to a contextual range can help adjust expectations and, consequently, happiness to a set of reasonable outcomes 
(Diener et al., 2009;
Parducci, 1984)
. This mechanism could, for example, prevent dissatisfaction with one's salary when compared to unrealistic benchmarks, such as the exorbitant remunerations of top CEOs.
Similarly, the positivity bias, by increasing subjective confidence in the probability of being correct, can foster overconfidence and optimism. These traits have been associated with numerous positive life outcomes and are evolutionarily stable under conditions where mating success is linked to assertiveness 
(Bailey et al., 2007;
Gannon & Zhang, 2020;
Heifetz & Spiegel, 2000)
. Thus, both relative value learning and positivity biases can be understood as cognitive adaptations that, while sometimes suboptimal in specific reinforcement learning tasks, provide broader psychological and evolutionary benefits to the individual.
Another possibility, which we term "strong" optimality, is that cognitive biases have been selected because they are advantageous even within the context of reinforcement learning problems. They could be optimal by solving fundamental information-processing challenges and by being welladapted to problems commonly encountered in ecological scenarios. In fact, while it is possible to identify situations and tasks where these biases appear detrimental (e.g., the post-learning phase), it is equally possible to reverse-engineer scenarios where the same biases prove optimal. The key question then becomes how plausible and general these scenarios are. This is typically addressed through mathematical analysis or simulations, both of which can provide insight into the adaptive value of these biases.
In the case of range normalization, both mathematical analysis and simulation studies suggest that its adaptive value lies in maintaining internal representations within a consistent scale. This consistency enables the adjustment of response stochasticity-commonly referred to as balancing exploration and exploitation-to the range of possible outcomes 
(L'Hôtellier et al., 2024;
Rustichini et al., 2023)
. As a result, algorithms endowed with range normalization achieve what is known in machine learning as "scale invariance," the ability to perform equally well regardless of the magnitude of rewards at stake ( 
Figure 6A
) 
(L'Hôtellier et al., 2024)
. Crucially, range normalization achieves this result in a bottom-up manner, without incurring the cost of encoding task-specific information 
(Mnih et al., 2015)
. In other words, it allows for effective adjustment without any additional computational effort or resource expenditure. This property mirrors how the human brain appears to automatically adjust value encoding across vastly different contexts and quickly move from making decisions that involve quite high stakes (such as picking a hotel room -in the hundreds of euros) versus very small ones (such as choosing what to have for breakfast -in the range of a few cents). Moreover, we note that magnitude insensitivity is a robust feature of human reinforcement learning, as evidenced by numerous studies where participants' performance is the same across, sometimes very different, magnitude domains 
(Anlló et al., 2024;
Bavard et al., 2018
Bavard et al., , 2021
.
The optimality of biased update, such as positivity bias, has been assessed in several studies 
(Cazé & van der Meer, 2013;
Lefebvre et al., 2022)
. A consistent finding is that in bandit tasks similar to those described earlier, algorithms incorporating a positivity bias (or confirmation bias) tend to outperform unbiased algorithms across many types of environments, with the exception of those with generally high reward rates, which are unlikely to have been encountered in natural settings (Finding food, especially high-calorie food, was a relatively rare event for our ancestors). More specifically, a recent study using evolutionary simulations demonstrated the evolvability of positivity bias in various two-armed bandit scenarios, including, somewhat counterintuitively, environments with moderate levels of volatility (i.e., changes in reward probabilities) ( 
Figure 6B
) 
(Hoxha et al., 2024)
. Another study employing a meta-reinforcement learning framework-training a neural network to perform bandit tasks optimally-revealed that positivity bias naturally emerged as a cognitive feature of the network, suggesting its adaptive value in reinforcement learning contexts 
(Schubert et al., 2024)
.
In summary, both relative value encoding (in the form of range normalization) and biased updating (in the form of positivity bias) have been shown to outperform unbiased models across a wide array of scenarios. This suggests that these biases may have been selected and maintained due to their statistical optimality. However, the two mechanisms address different challenges in reinforcement learning. Range normalization functions as a "tuning" mechanism, adjusting both the "perceptions" and "responses" of the model to virtually any range of potential outcomes. In contrast, positivity bias, particularly in the context of probabilistic (Bernoullian) outcomes, serves as a form of "noise cancellation". A moderate insensitivity to negative outcomes can be advantageous-for instance, when a correct option is rewarded 75% of the time, it is optimal to choose it 100% of the time while disregarding the occasional negative outcomes. As expected the unbiased model presents a scale-dependent performance (e.g., it is tuned well only for a narrow range of outcome magnitudes). On the other side the range model performs equally well across the whole range of magnitudes (adapted from 
(L'Hôtellier et al., 2024)
). (B) Evolutionary trajectory of the positive (y-axis) and the negative (x-axis) learning rate in population of 1000 agents across 200 generations, whose fitness is defined as the performance in a bandit task. Regardless of very different initial points the population converge to a positive bias (points above the diagonal) (adapted from 
(Hoxha et al., 2024)
).
These findings illustrate an important principle: epistemic accuracy and instrumental accuracy can be dissociated 
(Kelly, 2003)
. It is possible to hold distorted or biased knowledge (as resulting from range normalization and positivity bias) while performing better than if one's knowledge were unbiased. Since evolutionary success is ultimately determined by the quality of decisions and actions that we make, rather than the accuracy of their underlying beliefs, this dissociation between epistemic and instrumental accuracy provides a compelling explanation for the prevalence of cognitive biases.


Conclusion: applying reinforcement learning
Reinforcement learning is a powerful cognitive process with recognized potential as a strategy to correct and enhance decision-making. As a fundamental component of behavioral analysis -the applied branch of Skinner's behaviorism, aimed at promoting behavioral change through learning. It has been widely applied in therapeutic and educational contexts (although its principles are often applied implicitly or incompletely, leaving considerable room for improvements) 
(Pierce & Cheney, 2017
. An additional testament to the power of RL-inspired techniques is their routine use in marketing to increase user engagement and by gambling institutions, sometimes leading to maladaptive behaviors akin to behavioral addiction 
(Foxall, 2001;
Linehan et al., 2015)
. Despite this, RL's proven potential as a tool for improving decision-making and acquiring skills, it remains surprisingly overlooked in the ongoing debate dominated by approaches such as "nudges" and, to a lesser extent, "boosts" 
(da Rocha & Hunziker, 2020;
Hallsworth, 2023;
Hertwig & Grüne-Yanoff, 2017;
Tagliabue, 2023)
. We believe this is at least partly due to the long-standing misconceptions associated with behaviorism in psychology, ranging from the incorrect belief that it is epistemologically flawed to the perception that it is not humane 
(Leahey, 1992;
J. Staddon, 1993
J. Staddon, , 2014
.
However, it is crucial to recognize the comparative advantages and disadvantages of reinforcement learning-based interventions compared to traditional nudges. Unlike nudges, which focus on modifying the choice architecture 
(Table 2)
, RL-based strategies modify the learning architecture, which will generally result in slower and more resource-intensive interventions. However, the trade-off is that RL-based intervention will result in longer-lasting effects, making them more valuable for promoting sustained behavioral change. Furthermore, while nudges are inherently limited in their ability to generalize by design, RL-based interventions have the potential for broader applicability, as generalization is a core feature of reinforcement learning.


Nudging


Applied behavioral analysis
Underlying framework Heuristic and biases Reinforcement learning


Focus
Antecedent of decisions (choice architecture)
Consequences of decisions (learning architecture)


Role of learning No Yes


Duration of the effect Short Long


Generalizability of the effect No Yes
Cost of the intervention Low Low to High 
Table 2
. Main approaches to improve decision-making based on behavior sciences. Underlying framework refers to the scientific tradition upon which the framework is based. Learning is central to "applied behavioral analysis" (name given by Skinner to the reinforcement learning-based interventions) and not leveraged by "nudges", whose effect disappears as soon as the choice architecture is reverted back to "normal". For this reason (lack of learning) the effect of "nudges" is short lived and, since it's not associated to the acquisition of a new skills, does not generalize.
Beyond shaping decisions in desirable directions, RL could also serve as a valuable tool for teaching abstract information in a direct, engaging, and effective manner. A great example is that of RL widespread use in language-learning apps where learners earn points, badges, and streaks for completing lessons (Laura De La 
Cruz et al., 2023)
. These features provide positive reinforcement, which helps sustain motivation and encourages continued engagement. This same principle could be applied to address critical societal challenges rooted in widespread misconceptions such as those surrounding migration or climate change 1 . Traditional approaches to correcting these beliefs often rely on presenting abstract information. However, RL-based training offers a potentially more effective alternative by shifting from top-down teaching to a feedback-driven, bottom-up learning experience.
Of course, as with any behavioral science-based intervention, ethical considerations are in order.
In the context of RL-based strategies, it is essential to prioritize the use of rewards over punishments. This is not only more ethically sound but also leads to more robust and sustainable results. Punishments, while effective at suppressing specific actions, fail to guide individuals toward desirable alternatives and may even produce unintended or undesirable behaviors 
(Eysenck, 2013;
Solomon, 1964)
. By emphasizing rewards, RL-based interventions can achieve their goals in a manner that is both effective and ethically responsible. When developing and implementing RL-based interventions, it is crucial to consider the findings reviewed here, particularly those concerning epistemic biases. Assuming that humans process feedback in an unbiased manner can lead to flawed conclusions and ineffective interventions. Moreover, the reviewed results raise new ethical concerns, especially the demonstrated dissociation between epistemic and instrumental accuracy. While biased knowledge can, in many situations, lead to better decisions, would it be acceptable to deliberately induce biased or "wrong" knowledge to improve decision-making outcomes? This question demands careful consideration to balance the benefits of improved instrumental accuracy with the ethical implications of distorting epistemic accuracy.
Another critical consideration is that, in many real-world scenarios-particularly those involving communication-epistemic accuracy may be as important as instrumental accuracy. For example, in situations where individuals need to not only act based on the value of an option but also communicate their reasoning or justify their choices, biased knowledge could have negative consequences. Addressing this balance will be essential for the responsible application of RLbased strategies.
Finally, a major challenge for scaling RL-based interventions lies in understanding how findings on epistemic biases from controlled laboratory settings translate to complex ecological environments. In the case of contextual effects, laboratory studies often treat "context" as a clearly defined variable. In real-world environments however, defining what constitutes a context becomes far more complex as spatial and temporal contextual factors often overlap and extend across multiple scales.
As for positivity bias, it has been predominantly studied in individual scenarios where a single person interacts with their environment. However, real-world decision-making often involves social interactions of multiple nature (e.g., cooperation, competition). These social components play a big role in shaping behaviors and outcomes, yet their interaction positivity bias remains underexplored. Investigating how positivity bias manifests in social contexts will be critical for understanding its broader implications and leveraging it effectively in practical applications 
(Bergerot et al., 2024;
Lefebvre et al., 2024;
Najar et al., 2020;
Schultner et al., 2024;
Witt et al., 2024)
.
Figure
Figure 1: information processing-based illustration of noise and bias within cognitive processes, whose goal is to transform an objective value (input) into a subjective one (output). (A) Noiseless and unbiased process. (B) Noisy and unbiased process. In the simulation we used a Gaussian noise, with a mean of zero and a variance proportion to the value of the objective variable (Weber's law) (C) Noiseless and biased process. In the simulation, the cognitive process overestimates the value of the objective variable in the lower range and underestimates it in mid/high range. (D) Noisy and biased process.


Figure 2 :
2
a box-and-arrow representation of a reinforcement learning agent. (A) Options, actions and outcomes are observables (i.e., in the environment), option values are latent variable (i.e., internal to the agent). The agent interfaces with the environment in at least three main aspects: options recognition, action emission and outcome integration. (B) Praxic biases: the actions do not match the internal representations of values (e.g., because of the interference of another decision-making system). (C) Epistemic biases: the internal representations of value do not match the experienced outcomes.


Figure 3
3
Figure 3: two epistemic biases of human reinforcement learning with their key equations mapped into the reinforcement learning agent. The first step is represented by subjective outcome encoding (relative valuation), the second one the error prediction error calculation, the third one by value update and, finally, the last one by action selection.


Figure 4 :
4
Relation between subjective values and objective after range normalization in different contexts. The figure shows how relative value encoding can distort subjective value of options as a function of different learning architectures (i.e., contexts). In all cases the hypothetical agent is represented with four options with increasing expected values (A,B,C and D). The three columns represent three different scenarios where the option values have been learned from different learning architectures (i.e., different contexts). (A) The values of the four options were learned within the same context. In this case objective (absolute) and subjective (relative) value representation coincide. (B) and (C) The values of the four options were learned in different contexts (AB vs CD or AC vs. BD). In this cases objective (absolute) and subjective (relative) value representation do not coincide.


Figure 5 :
5
From unbiased and noiseless update to biased and noise update. The figure illustrates, in a way similar to that used in


Figure 6 :
6
(A)  performance of an unbiased model (blue) and a model featuring range normalization (red) in bandits task featuring a large range of outcome magnitudes (x-axis).


https://impactco2.fr/outils/quiz














Comparing experience-and description-based economic preferences across 11 countries




H
Anlló






S
Bavard






F
Benmarrakchi






D
Bonagura






F
Cerrotti






M
Cicue






M
Gueguen






E
J
Guzmán






D
Kadieva






M
Kobayashi






G
Lukumon






M
Sartorio






J
Yang






O
Zinchenko






B
Bahrami






J
Silva Concha






U
Hertz






A
B
Konova






J
Li






S
Palminteri




10.1038/s41562-024-01894-9








Nature Human Behaviour




8


8
















Hope and optimism as related to life satisfaction




T
C
Bailey






W
Eng






M
B
Frisch






†
Snyder






C
R




10.1080/17439760701409546








The Journal of Positive Psychology




2


3
















Reference-point centering and rangeadaptation enhance human reinforcement learning at the cost of irrational preferences




S
Bavard






M
Lebreton






M
Khamassi






G
Coricelli






S
Palminteri




10.1038/s41467-018-06781-2








Nature Communications




9


1


4503














The functional form of value normalization in human reinforcement learning. eLife, 12, e83891




S
Bavard






S
Palminteri




10.7554/eLife.83891


















Does the notion of intrinsic reward explain context-dependent valuation in reinforcement learning? OSF




S
Bavard






S
Palminteri




10.31234/osf.io/23ng4


















Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning




S
Bavard






A
Rustichini






S
Palminteri




10.1126/sciadv.abe0340








Science Advances




7


14














Moderate confirmation bias enhances decision-making in groups of reinforcement-learning agents




C
Bergerot






W
Barfuss






P
Romanczuk




10.1371/journal.pcbi.1012404








PLOS Computational Biology




20


9














Three examples of bidirectional space-time interference




D
Bratzke






Ebbinghaus






Ponzo
Müller-Lyer




10.3758/s13423-024-02491-7








Psychonomic Bulletin & Review




31


5
















Operant conditioning in invertebrates




B
Brembs




10.1016/j.conb.2003.10.002








Current Opinion in Neurobiology




13


6
















The bias bias




H
Brighton






G
Gigerenzer




10.1016/j.jbusres.2015.01.061








Journal of Business Research




68


8
















Prospect Theory In The Wild: Evidence From The Field




C
Camerer








Advances in Behavioral Economics
















Adaptive properties of differential learning rates for positive and negative outcomes




R
D
Cazé






M
A A
Van Der Meer




10.1007/s00422-013-0571-5








Biological Cybernetics




107


6
















Information about action outcomes differentially affects learning from self-determined versus imposed choices




V
Chambon






H
Théro






M
Vidal






H
Vandendriessche






P
Haggard






S
Palminteri




10.1038/s41562-020-0919-5








Nature Human Behaviour




4


10
















A Behavior-Analytic View on Nudges: Individual, Technique, and Ethics




C
A A
Da Rocha






M
H L
Hunziker




10.1007/s42822-020-00037-9








Behavior and Social Issues


29














Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems




P
Dayan






L
F
Abbott








MIT Press












Psychology and Economics: Evidence from the Field




S
Dellavigna




10.1257/jel.47.2.315








Journal of Economic Literature




47


2
















Happiness is the Frequency, Not the Intensity, of Positive Versus Negative Affect




E
Diener






E
Sandvik






W
Pavot








Assessing Well-Being: The Collected Works of Ed Diener


E. Diener






















Springer Netherlands




10.1007/978-90-481-2354-4_10














Experiments in Behaviour Therapy: Readings in Modern Methods of Treatment of Mental Disorders Derived from Learning Theory




H
J
Eysenck








Elsevier












Noise in the nervous system




A
A
Faisal






L
P J
Selen






D
M
Wolpert




10.1038/nrn2258








Nature Reviews Neuroscience




9


4
















Flexible combination of reward information across primates




S
Farashahi






C
H
Donahue






B
Y
Hayden






D
Lee






A
Soltani




10.1038/s41562-019-0714-3








Nature Human Behaviour




3


11
















Computational noise in reward-guided learning drives behavioral variability in volatile environments




C
Findling






V
Skvortsova






R
Dromnelle






S
Palminteri






V
Wyart




10.1038/s41593-019-0518-9








Nature Neuroscience




22


12
















Incentive relativity




C
F
Flaherty








Cambridge [England






with Internet Archive








Separate neural representations of prediction error valence and surprise: Evidence from an fMRI meta-analysis




E
Fouragnan






C
Retzler






M
G
Philiastides




10.1002/hbm.24047








Human Brain Mapping




39


7
















Foundations of Consumer Behaviour Analysis




G
R
Foxall




10.1177/147059310100100202








Marketing Theory




1


2
















By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism




M
J
Frank






L
C
Seeberger






R
C
Reilly




10.1126/science.1102941








Science




306


5703
















Impaired adaptation of learning to contingency volatility in internalizing psychopathology. eLife, 9




C
Gagne






O
Zika






P
Dayan






S
J
Bishop




10.7554/eLife.61387


















An Evolutionary Justification for Overconfidence (SSRN Scholarly Paper No. 3026885). Social Science Research Network




K
Gannon






H
Zhang




10.2139/ssrn.3026885


















Learning to Be Overconfident




S
Gervais






T
Odean








The Review of Financial Studies




14


1
















Heuristic Decision Making




G
Gigerenzer






W
Gaissmaier




10.1146/annurev-psych-120709-145346








Annual Review of Psychology




62
















A manifesto for applying behavioural science




M
Hallsworth




10.1038/s41562-023-01555-3








Nature Human Behaviour




7


3




















M
G
Haselton






G
A
Bryant






A
Wilke






D
A
Frederick






A
Galperin






W
E
Frankenhuis






T
Moore


















Adaptive Rationality: An Evolutionary Perspective on Cognitive Bias


10.1521/soco.2009.27.5.733








Social Cognition




27


5














The Evolution of Cognitive Bias




M
G
Haselton






D
Nettle






P
W
Andrews




10.1002/9780470939376.ch25








The Handbook of Evolutionary Psychology




John Wiley & Sons
















Effects of blocked versus interleaved training on relative value learning




W
M
Hayes






D
H
Wedell




10.3758/s13423-023-02290-6








Psychonomic Bulletin & Review




30


5
















Testing models of context-dependent outcome encoding in reinforcement learning




W
M
Hayes






D
H
Wedell




10.1016/j.cognition.2022.105280








Cognition




230


105280














Large Language Models are Biased Reinforcement Learners




W
M
Hayes






N
Yax






S
Palminteri




10.48550/arXiv.2405.11422


arXiv:2405.11422


















A comparison of reinforcement learning models of human spatial navigation




Q
He






J
L
Liu






L
Eschapasse






E
H
Beveridge






T
I
Brown




10.1038/s41598-022-18245-1








Scientific Reports




12


1


13923














On the Evolutionary Emergence of Optimism (SSRN Scholarly Paper No. 247355). Social Science Research Network




A
Heifetz






Y
Spiegel




10.2139/ssrn.247355


















Nudging and Boosting: Steering or Empowering Good Decisions




R
Hertwig






T
Grüne-Yanoff




10.1177/1745691617702496








Perspectives on Psychological Science




12


6
















Studies in Ecological Rationality




R
Hertwig






C
Leuker






T
Pachur






L
Spiliopoulos






T
J
Pleskac




10.1111/tops.12567








Topics in Cognitive Science




14


3
















Evolving choice hysteresis in reinforcement learning: Comparing the adaptive value of positivity bias and gradual perseveration




I
Hoxha






L
Sperber






S
Palminteri




10.48550/arXiv.2410.19434


arXiv:2410.19434


















A Primer on Reinforcement Learning in Medicine for Clinicians




P
Jayaraman






J
Desman






M
Sabounchi






G
N
Nadkarni






A
Sakhuja




10.1038/s41746-024-01316-0








Npj Digital Medicine




7


1
















Prospect Theory: An Analysis of Decision under Risk




D
Kahneman






A
Tversky




10.2307/1914185








Econometrica




47


2
















Epistemic Rationality as Instrumental Rationality: A Critique




T
Kelly




10.1111/j.1933-1592.2003.tb00281.x








Philosophy and Phenomenological Research




66


3
















Use of gamification in English learning in Higher Education: A systematic review




Laura De La
Cruz






K
M
Noa






S
Turpo Gebera






O
W
Valencia






C
C
Milagritos Bazán






S
Velasquez






G
S
Postigo








JOTSE




13


2
















The Classical Conditioning Paradigm




D
G
Lavond






J
E
Steinmetz




10.1007/978-1-4615-0263-0_1








Handbook of Classical Conditioning


D. G. Lavond & J. E. Steinmetz




Springer US
















The mythical revolutions of American psychology




T
H
Leahey




10.1037/0003-066X.47.2.308








American Psychologist




47


2
















The human as delta-rule learner. Decision




S
Lee






J
I
Gold






J
W
Kable




10.1037/dec0000112








7














The roots of polarization in the individual reward system




G
Lefebvre






O
Deroy






B
Bahrami




10.1098/rspb.2023.2011








Proceedings of the Royal Society B: Biological Sciences




291


20232011














A Normative Account of Confirmation Bias During Reinforcement Learning




G
Lefebvre






C
Summerfield






R
Bogacz




10.1162/neco_a_01455








Neural Computation




34


2
















Achieving Scale-Invariant Reinforcement Learning Performance with Reward Range Normalization




M
L'hôtellier






J
Perez






S
Palminteri




10.31234/osf.io/bjyr9


















A computational reward learning account of social media engagement




B
Lindström






M
Bellander






D
T
Schultner






A
Chang






P
N
Tobler






D
M
Amodio




10.1038/s41467-020-19607-x








Nature Communications




12


1


1311
















C
Linehan






B
Kirman






B
Roche




10.7551/mitpress/9788.003.0006




Gamification as Behavioral Psychology
















Prediction error estimation methods. Circuits, Systems and Signal Processing




L
Ljung




10.1007/BF01211648








21














Individual choice behavior: A theoretical analysis




R
D
Luce










Dover Publications












Mesoscale effects of trader learning behaviors in financial markets: A multi-agent reinforcement learning study




J
Lussange






S
Vrizzi






S
Palminteri






B
Gutkin




10.1371/journal.pone.0301141








PLOS ONE




19


4














Human-level control through deep reinforcement learning




V
Mnih






K
Kavukcuoglu






D
Silver






A
A
Rusu






J
Veness






M
G
Bellemare






A
Graves






M
Riedmiller






A
K
Fidjeland






G
Ostrovski






S
Petersen






C
Beattie






A
Sadik






I
Antonoglou






H
King






D
Kumaran






D
Wierstra






S
Legg






D
Hassabis




10.1038/nature14236








Nature




518


7540
















Reward Centering




A
Naik






Y
Wan






R
S
Sutton




















The actions of others act as a pseudo-reward to drive imitation in the context of social reinforcement learning




A
Najar






E
Bonnet






B
Bahrami






S
Palminteri




10.1371/journal.pbio.3001028








PLOS Biology




18


12














Dialogues on prediction errors




Y
Niv






G
Schoenbaum




10.1016/j.tics.2008.03.006








Trends in Cognitive Sciences




12


7
















The asymmetric learning rates of murine exploratory behavior in sparse reward environments




H
Ohta






K
Satori






Y
Takarada






M
Arake






T
Ishizuka






Y
Morimoto






T
Takahashi




10.1016/j.neunet.2021.05.030








Neural Networks




143
















Contextual modulation of value signals in reward and punishment learning




S
Palminteri






M
Khamassi






M
Joffily






G
Coricelli




10.1038/ncomms9096








Nature Communications




6


1


8096














Context-dependent outcome encoding in human reinforcement learning. Current Opinion in Behavioral Sciences




S
Palminteri






M
Lebreton




10.1016/j.cobeha.2021.06.006








41














The computational roots of positivity and confirmation biases in reinforcement learning




S
Palminteri






M
Lebreton




10.1016/j.tics.2022.04.005








Trends in Cognitive Sciences




26


7




















S
Palminteri






M
Lebreton






Y
Worbe






A
Hartmann






S
Lehéricy






M
Vidailhet






D
Grabli






M
Pessiglione


















Dopamine-dependent reinforcement of motor skill learning: Evidence from Gilles de la Tourette syndrome


10.1093/brain/awr147








Brain




134


8














Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing




S
Palminteri






G
Lefebvre






E
J
Kilford






S.-J
Blakemore




10.1371/journal.pcbi.1005684








PLOS Computational Biology




13


8














Chapter 23 -Opponent Brain Systems for Reward and Punishment Learning: Causal Evidence From Drug and Lesion Studies in Humans




S
Palminteri






M
Pessiglione




10.1016/B978-0-12-805308-9.00023-3








Decision Neuroscience


J.-C. Dreher & L. Tremblay




Academic Press
















Value Judgments: Toward a Relational Theory of Happiness




A
Parducci




10.1007/978-1-4613-8251-5_1








Attitudinal Judgment


J. R. Eiser




Springer
















Behavior Analysis and Learning: A Biobehavioral Approach




W
D
Pierce






C
D
Cheney




10.4324/9781315200682












Sixth Edition. 6th ed.








Context-dependent utility overrides absolute memory as a determinant of choice




L
Pompilio






A
Kacelnik




10.1073/pnas.0907250107








Proceedings of the National Academy of Sciences


the National Academy of Sciences






107














Associations in Pavlovian conditioned inhibition




R
A
Rescorla






P
C
Holland




10.1016/0023-9690(77)90044-3








Learning and Motivation




8


4
















Adaptive Coding is Optimal in Reinforcement Learning (SSRN Scholarly Paper No. 4320894). Social Science Research Network




A
Rustichini






M
Soukupova






S
Palminteri




10.2139/ssrn.4320894


















Linking confidence biases to reinforcement-learning processes




N
Salem-Garcia






S
Palminteri






M
Lebreton




10.1037/rev0000424








Psychological Review




130


4


















A
N
Sanborn






J.-Q
Zhu






J
Spicer






P
León-Villagrá






L
Castillo






J
K
Falbén






Y.-X
Li






A
Tee






N
Chater






Noise in cognition: Bug or feature? Perspectives on Psychological Science


















J
A
Schubert






A
K
Jagadish






M
Binz






E
Schulz




10.48550/arXiv.2402.03969


arXiv:2402.03969




-context learning agents are asymmetric belief updaters
















Transmission of social bias through observational learning




D
T
Schultner






B
R
Lindström






M
Cikara






D
M
Amodio




10.1126/sciadv.adk2030








Science Advances




10


26














Neuronal Coding of Prediction Errors




W
Schultz






A
Dickinson




10.1146/annurev.neuro.23.1.473








Annual Review of Neuroscience




23
















Forming Beliefs: Why Valence Matters




T
Sharot






N
Garrett




10.1016/j.tics.2015.11.002








Trends in Cognitive Sciences




20


1
















Exploring Causes of the Self-serving




J
Shepperd






W
Malone






K
Sweeny




10.1111/j.1751-9004.2008.00078.x








Bias. Social and Personality Psychology Compass




2


2
















Cortico-striatal contributions to feedback-based learning: Converging data from neuroimaging and neuropsychology




D
Shohamy






C
E
Myers






S
Grossman






J
Sage






M
A
Gluck






R
A
Poldrack




10.1093/brain/awh100








Brain




127


4




















R
L
Solomon




10.1037/h0042493








Punishment. American Psychologist




19


4
















Bumblebees retrieve only the ordinal ranking of foraging options when comparing memories obtained in distinct settings. eLife, 11, e78525




C
Solvi






Y
Zhou






Y
Feng






Y
Lu






M
Roper






L
Sun






R
J
Reid






L
Chittka






A
B
Barron






F
Peng




10.7554/eLife.78525


















Context induces distortions in value representations: A test across multiple elicitation methods and learning modalities




M
Soukupova






B
Garcia






S
Palminteri




10.31234/osf.io/yrjfaStaddon








Behaviorism. Duckbacks










J.










J
Staddon




10.4324/9781315798172




The New Behaviorism: Second Edition




Psychology Press








2nd ed.








Operant Conditioning




J
E R
Staddon






D
T
Cerutti




10.1146/annurev.psych.54.101601.145124








Annual Review of Psychology




54


















R
S
Sutton






A
G
Barto




Reinforcement Learning




An Introduction. MIT Press








second edition










M
Tagliabue




10.1007/s40614-021-00324-9




Tutorial. A Behavioral Analysis of Rationality, Nudging, and Boosting: Implications for Policymaking. Perspectives on Behavior Science






46














Old strategies, new environments: Reinforcement Learning on social media




G
Turner






A
M
Ferguson






T
Katiyar






S
Palminteri






A
Orben




10.1016/j.biopsych.2024.12.012








Biological Psychiatry
















Humans flexibly integrate social information despite interindividual differences in reward




A
Witt






W
Toyokawa






K
N
Lala






W
Gaissmaier






C
M
Wu




10.1073/pnas.2404928121








Proceedings of the National Academy of Sciences




121


39


2404928121














Studying and improving reasoning in humans and machines




N
Yax






H
Anlló






S
Palminteri




10.1038/s44271-024-00091-8








Communications Psychology




2


1
















The role of hedonic processes in the organization of behavior




P
T
Young




10.1037/h0057176








Psychological Review




59


4

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]