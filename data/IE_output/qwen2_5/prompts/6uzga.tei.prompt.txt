You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Log pointwise predictive density, ∑
(
1 ∑ ( | ) =1 ) =1
, likelihood of each observed data point conditional on the model parameters . In practice, this quantity is estimated using samples (for = 1,2,3, … , ) drawn from the posterior distribution.
The drift-diffusion model (DDM) is one of the most widely used computational models (for an overview, see 
Ratcliff et al., 2016)
 to quantify the evidence accumulation processes during decision-making in neuroscience 
(Cavanagh et al., 2011;
Herz et al., 2017;
Shadlen & Shohamy, 2016)
, psychology 
(Hu et al., 2020;
D. J. Johnson et al., 2017;
Kutlikova et al., 2023)
, behavioral economics 
(Desai & Krajbich, 2022;
Sheng et al., 2020)
, and psychiatry 
(Ging-Jehli et al., 2021;
Pedersen et al., 2022)
. According to the DDM, experimentally observed pairs of response times and choices arise from a process of stochastic evidence accumulation to a decision boundary (e.g., 
Voss et al., 2013
; 
Figure 1
 and the related DDM glossary 
Table 1
). This theoretical framework has been shown not only to correlate robustly with established neural substrates 
(Chandrasekaran et al., 2017;
Forstmann et al., 2016)
, but also to serve as a powerful measurement tool for examining individual differences across cognitive tasks, experimental manipulations, and participant populations 
(Boag et al., 2024;
Donkin & Brown, 2018;
Evans & Wagenmakers, 2020
; but see 
Liu et al., 2023)
. Despite its theoretical contributions, the DDM is difficult to apply to experimental data in practice, because the derivation of inference-relevant quantities (e.g., the likelihood , a measure of outof-sample predictive performance for new data ̃ generated by the true data-generating process.
(̃) is the predictive density for ̃ based on the posterior distribution, f is the true underlying model, and denotes the expectation that averages over the true data-generating distribution 
(Gelman et al., 2014)
. ELPPD is commonly used to compare the predictive performance of different models, as it provides an estimate of how well a model is expected to perform on new data.
Highest density interval (HDI), an estimate of a parameter's credible range in the context of Bayesian statistics. It encompasses an interval of the posterior distribution where each point within this interval has a higher density than points outside of it. For instance, a 95% HDI means that there is a 95% chance that the true parameter value falls within this range, making it a reliable indicator of parameter uncertainty. HDIs are commonly used for hypothesis testing regarding effect sizes, as well as comparisons across different conditions or groups.
A region of practical equivalence (ROPE), a predefined range of parameter values that are considered practically equivalent to zero, which could be based on existing literature or theoretical reasoning 
(Kruschke, 2018
(Kruschke, , 2021
. To determine whether a parameter estimate is significantly different from zero, a ROPE might be set as a range around zero. If the 95% HDI of the parameter lies entirely outside this ROPE, the parameter is considered credibly different from zero. If the HDI is entirely within the ROPE, the parameter is effectively zero for practical purposes. Partial overlap suggests that the parameter's result should be interpreted with caution. Note that caution should be taken when using the HDI + ROPE method for statistical inference on transformed parameters, because of an inconsistency in transformation properties between ROPE and HDI 
(Etz et al., 2024)
.
Bayes Factor (BF) and Savage-Dickey Density Ratio (SDDR). BF quantifies the strength of evidence for one statistical model over another. A value greater than 1 suggests more support for the alternative model relative to the original model, offering a continuous measure of evidence 
(Kass & Raftery, 1995)
. The SDDR simplifies Bayes Factor computation for nested models by comparing a parameter's posterior density at a specific point (typically zero) to its prior density at the same point. This method is efficient and effective for evaluating whether a parameter is significantly different from zero 
(Wagenmakers et al., 2010)
. function) requires a mathematical understanding of the complex stochastic process of evidence accumulation. 
Figure 1
. Illustration of the evidence accumulation process assumed by DDM. DDM has four basic parameters: drift rate ( ), decision boundary ( ), initial bias ( ), and non-decision time ( ). The drift rate ( ) is the average speed of evidence accumulation toward a decision; the decision boundary ( ) is the distance between two decision thresholds, and the evidence needed to make a decision increase as increases; the initial bias ( ) reflects the starting point of evidence accumulation. non-decision time ( ) is the time not used for evidence accumulation, e.g., stimulus encoding or motor execution. A more detailed description of the DDM and its parameters is given in 
Table 1.
 Several software packages have been developed to facilitate the application of DDM (see Section 5.1), proving particularly beneficial for researchers with limited computational expertise.
Among them, HDDM, a Python library for hierarchical drift diffusion modeling, is by far the most cited toolbox in the community 
(Wiecki et al., 2013
, with 996 citations in Google Scholar, accessed 26 August 2024). Despite the success and popularity of HDDM, it suffers from several practical issues. First, the installation process of HDDM is cumbersome, exacerbated by its reliance on PyMC 2.3.8 for Markov Chain Monte Carlo (MCMC) sampling, a package that is no longer supported and may clash with latest computer modules. Second, and for the same reason, out of the box HDDM is not compatible with apple chips, which creates a significant barrier for Mac users. Third, although HDDM natively centers around Bayesian methods, it does not conveniently support all aspects of the evolved standards in Bayesian modeling workflows 
(Ahn et al., 2017;
Gelman et al., 2020;
Kruschke, 2021)
. Significant progress has recently been made in supporting the principled Bayesian modeling workflow in easy-to-use toolkits, such as the Python package ArviZ 
(Kumar et al., 2019)
. Bridging these new capabilities with HDDM facilitates a one-stop Bayesian modeling pipeline for experimentalists and computational modelers interested in applying the DDM to their experimental data.


Table 1 DDM Glossary


Term Description
Accumulator A component of the DDM that accumulates evidence for different decision options until a threshold is reached, triggering a decision.


Random walk
A stochastic process that describes a path consisting of a sequence of random steps. It refers to the modeling of decision-making as a process of accumulating evidence over time.


Diffusion
The diffusion refers to the variability in the evidence accumulation process that represents random fluctuations in the decision variable.


Optional stopping
The concept of stopping the decision-making process at a point chosen by the decision-maker, often when a certain level of confidence or evidence threshold is reached.


Drift rate ( )
The average rate of evidence accumulation towards one of the decision boundaries. The more difficult the task, the less stimulus discrimination and the smaller the drift rate.


Decision boundary ( )
The threshold that, when reached by the accumulated evidence, triggers a decision. It represents the speed-accuracy trade-off or caution, and the higher its value, the higher the accuracy at the expense of slower response time.
Non-decision time ( ) The time taken by processes other than decision-making (e.g., sensory encoding and motor execution). It simply shifts response time distribution.


Initial bias ( )
The initial value of the decision variable, which indicates any initial bias in evidence accumulation, is also called 'starting point' in the literature. The closer it is to a boundary (1 and 0 correspond to the upper and lower boundaries, respectively), the faster and more frequent the response.
Drift Rate variability ( ) The variability in the drift rate parameter across trials. It increases the proportion of slow errors.


Initial bias variability ( )
The across-trial variability in the starting point parameter in the DDM. It increases the proportion of fast errors.


Non-decision time variability ( )
The across trial variability in the non-decision time parameter in the DDM. It simultaneously increases the probability of both faster and slower responses, resulting in a thicker tail of the RT distribution.
Note: The terms used here are defined within the framework of the sequential sampling model 
(Forstmann et al., 2016;
Ratcliff et al., 2016)
, and some of them, such as diffusion and optional stopping, differ from those used in the mathematical literature. RT = reaction/response time.
To address the above issues, we leveraged the Docker container technology to create dockerHDDM, a stable and complete virtualized Python computing environment that enables out-of-the-box implementations of Bayesian hierarchical drift-diffusion models. dockerHDDM has three major advantages 
(Table 2)
. First, it benefits from the easy-to-deploy nature of the Docker environment to avoid compatibility issues. Second, it is compatible with both Intel or Apple chips.
Third, it augments HDDM with ArviZ, a Python module that enables a wide range of advanced Bayesian modeling analyses. We expect dockerHDDM to provide an easy-to-use environment to help researchers across various backgrounds efficiently use DDM in their research.  
Plotting (e.g., HDI,)
 No Yes 
Diagnosis (e.g., ESS)
 No Yes
Model Comparison 
(LOO, WAIC)
 No Yes


Installation
Hard Easy


Parallel processing Hard Easy
Compatibility with Apple chips Hard Easy


How to Follow This Tutorial
The primary goal of this paper is to present a practical guide to dockerHDDM for beginners with little modeling experience. The tutorial starts with step-by-step instructions on how to configure the dockerHDDM environment and how to use it in practical data analysis ( 
Figure 2
). To assist reproducibility and easy application, a corresponding step-by-step video walk-through is available on YouTube at https://www.youtube.com/watch?v=ZU1fbXEuP8s or on OSF at https://osf.io/ 3upng/files/osfstorage/66d5c2a3f2abc7a7a359a26c/.
In the setup section (top panel in 
Figure 2
, corresponding to Section 2.1 in this paper), we provide instructions on how to install Docker. After that, we demonstrate how to obtain the dockerHDDM image and how to use this image to access the Jupyter notebook interface (middle   After installing Docker Desktop (or Docker Engine for Linux users), one can verify the installation by running the following command in a terminal 
(Figure 3
). If the container starts and runs successfully, it will display a confirmation message and then exit ( 
Figure 3
).


Box 2. Basic Introduction to Docker
Docker is an open source platform that automates the deployment, scaling and management of applications. It achieves this through containerization, a process that packages an application and its dependencies into a single, portable, and consistent unit known as a container image. Containers ensure that applications run reliably regardless of the environment 
(Peikert & Brandmaier, 2021;
Wiebels & Moreau, 2021)
. Docker utilizes a client-server architecture where the Docker client communicates with the Docker daemon, responsible for building, running, and distributing containers. The core components of Docker are the Docker Engine, Docker Hub, and Docker Compose. The Docker Engine is the runtime that enables containerization, while Docker Hub is a cloud-based registry for sharing and managing container images. Docker Compose, on the other hand, is a tool for defining and running multi-container Docker applications. $ docker run hello-world 
Figure 3
. Command to check Docker installation in Terminal. After running the command `docker run hello-world` (highlighted at first line), the printout tells us that Docker has been successfully installed on the system. The schematic interfaces of the Terminal on different platforms: MacOS (left), Windows (middle), and Ubuntu (right).


Pull dockerHDDM Image
After ensuring that Docker has been successfully installed and the Docker engine is running 
(Figure 3
), you can pull the dockerHDDM image by simply running the command in the terminal (see the meaning of each argument in 
Figure 4A
): $ docker pull hcp4715/hddm or $ docker pull hcp4715/hddm:latest This command will pull the latest default version of dockerHDDM, which corresponds to the image with the tag `1.0.1`. One can also select different tags for different versions of HDDM (see https://hub.docker.com/r/hcp4715/hddm/tags). Note that the tutorial in this paper works with the `latest` or `1.0.1` tags, it is compatible with 0.8.0, with minor grammar changes. . Docker commands to download and run dockerHDDM. (A) Download/pull dockerHDDM from the Docker hub. The command by default downloads the latest version of `hcp4715/dockerHDDM` if the image tag is not specified. The CPU architecture (Apple or Intel chips, corresponding to ARM64 and AMD64 architectures, respectively) is automatically recognized when the image is downloaded. (B) Command to start a container. Note, "\" separates different lines of a command in Linux and MacOS Terminal but not in Windows.


Run dockerHDDM Container
After pulling the Docker image to a local machine, you can start a computing environment by running the dockerHDDM image with the command in the terminal ( 
Figure 4B
): $ docker run -v $(pwd):/home/jovyan/work -p 8888:8888 -it --rm hcp4715/hddm jupyter notebook This command creates a Docker container, which is a specialized environment encapsulated within the Docker platform. The `-v` option is used to mount a local folder into the container's filesystem, enabling file exchange from the host machine. The example code `$(pwd):/home/jovyan/work` specifies two paths separated by a colon. The path on the left, denoted by `$(pwd)`, represents the current working directory on the host machine; and the path on the right, `/home/jovyan/work`1, is the location inside the container where the folder will be mounted ( 
Figure 4B
). This means that you can read and write the files from your local machine in the "work" directory in the browser. `$(pwd)` can be replaced with a valid folder path on your local machine. For example, for a folder named "ddm_project" on the drive D, it can be mounted with the following arguments in the respective operating systems:
in Linux, `-v /mnt/d/ddm_project:/home/jovyan/work`; in Windows `-v D:\ddm_project:/home /jovyan/work`; and in MacOS `-v /Volumes/D/ddm_project:/home/jovyan/work`.
The other arguments in the command are explained in 
Figure 4B
.
After running the `docker run …` command, a URL appears at the end of the terminal output (middle panel in 
Figure 2
). You can copy and paste this URL "http://127.0.0.1:8888/?token=..." into any web browser (such as Firefox or Chrome) to launch a Jupyter interface based on the dockerHDDM container. If the URL does not load properly, check whether port 8888 is being used by other docker containers or programs. If so, close those containers or programs. Alternatively, you may change the port, e.g. use port 7777, i.e. set `-p 7777:8888`, in this case, you should replace the "8888" in the URL to "7777", e.g., "http://127.0.0.1:7777/?token=...". You can then open or initialize a Jupyter notebook 2 to code, run and view the output directly. It is worth noting that the `--rm` flag included in the command means that the dockerHDDM container, along with any data or newly installed Python modules, will be deleted when the container stops. However, any files or data mounted to the container from the `$(pwd)` path will remain unaffected. This ensures the reproducibility of the computing environment. If you wish to modify the computing environment, for example by installing additional Python modules, we recommend that you first read the Docker API before removing `--rm` directly.
In the Jupyter interface, you will find two files and two folders (middle panel in 
Figure 2
).
The notebook dockerHDDM_workflow.ipynb offers a detailed reproduction of the analyses presented in this article, which we will discuss further in Section 4. In contrast, the notebook dockerHDDM_Quick_View.ipynb provides a brief overview of the dockerHDDM image's new features and an introduction to basic modeling processes. One folder is "work", which mounts the local path into the docker environment. The other folder, "OfficialTutorials" contains notebooks that reproduce the official tutorials available at https://hddm.readthedocs.io/en/latest/tutorials.html. LAN_Tutorial.ipynb introduces advanced use of LAN functions that address the problematic likelihood of more complicated models based on neural network methods 
(Fengler et al., 2021)
.


Novel Features of dockerHDDM
The dockerHDDM_Quick_View.ipynb illustrates two novel features in dockerHDDM (compared to HDDM installed directly without Docker): parallel computing for MCMC chains and creating InferenceData data for ArivZ analyses (as shown in the <Code Block 1>). For all models defined by methods such as `hddm.HDDM()` or `hddm.HDDMRegressor()`, we can employ the `.sample()`, method to run the MCMC algorithm for model fitting. The original HDDM provided two main parameters to set the MCMC algorithm, the first parameter was the number of samples (`500`) and the second was the number of burn-ins (`burn=100`) 3 .
In dockerHDDM, we included five extra arguments in `.sample()` method to provide parallel computing for MCMC chains and create InferenceData.
To preserve compatibility and consistent output with origin HDDM, the arguments are configured with the following defaults: `return_infdata=False`, `sample_prior= False`, `loglike=False`, and `ppc=False`, `save_name=None`, and `chains=1`.
The `chains` argument determines the number of MCMC chains. Using more than two chains triggers multi-threaded parallel computation, which can significantly reduce the time when multiple chains are needed to compute model diagnosis index ̂ (see Section 4.4). The number of parallel MCMC chains is limited by the number of available CPU cores/threads available. For example, the maximum number of chains for a computer with 4 cores (8 threads) is 8. Setting the "chains" argument more than 8 may degrade performance. Nonetheless, whenever possible, a number of 4 chains is commonly used.
The `return_infdata` argument converts HDDM results into the InferenceData structure 4 , accessible via `model.infdata`, by default set to `False` to maintain compatibility with original HDDM output. Additionally, we have included `loglike` for computing and saving log-likelihood values (see Section 4.5); `ppc` for posterior predictive checks (see Section 4.6); and `sample_prior=True` for calculating Savage-Dickey Density Ratio 
(Wagenmakers et al., 2010)
 to approximate the Bayes Factor (see Section 4.7). When setting `ppc` as `True`, it defaults to generating 500 predictions for each observed data, but users can adjust this by adding argument `n_ppc`. Similarly, when setting `sample_prior` as `True`, it defaults to sampling 2000 draws for each prior parameter, but users can adjust this by adding argument `n_prior`.
Finally, the `save_name` argument specify the path and filename for saving the model and InferenceData, which is convenient for reusing results. One can load the model using `model = hddm.load('example.hddm')` and the InferenceData with `infdata = az.from_netcdf('example.nc')`.


Example of Workflow
In this section (bottom panel of 
Figure 2
), we demonstrate how to use dockerHDDM (i.e., HDDM and ArviZ) to perform key steps of Bayesian modeling 
(Gelman et al., 2020;
Martin et al., 2024)
:  "subj_idx is" the subject index; "rt" is the response time (in seconds), and "response" in this case represents the accuracy, where 1 is correct and 0 is incorrect. These three columns of data are mandatory when using HDDM and must be kept consistent with the column names, as well as the units 
(rt, seconds)
. "conf" is an optional variable, corresponding to the conflict level, where "HC" denotes high conflict and "LC" denotes low conflict. "conf" is not a mandatory variable or column, meaning that different factor names and levels can be used depending on the experimental design. In addition, multiple variables may be maintained in the data, which may be categorical or continuous.
model


Example data
For convenience, we use the data from 
Cavanagh et al. (2011)
, which is built within HDDM, as an example to demonstrate how to implement the modeling workflow. This dataset contains response time and choice data from 14 Parkinson's patients (see 
Table 3
). In the experiment, participants were asked to choose between two options associated with either high or low reward values (i.e., reward probabilities in typical reinforcement learning tasks). The relative value differences between the two options define two levels conflict: high conflict for low-low and high-high trials ("HC" in variable "conf"), and low conflict for low-high trials ("LC" in variable "conf").
Note that, HDDM requires the inclusion of three columns of variables, "subj_idx", "rt" and "response", to construct the hierarchical model. This means that when analyzing your own data, these three columns of variables must appear in the dataset with identical column names. In addition, the unit of "rt" must be seconds, and "response" is coded as 1 for the upper boundary of the corresponding choice and 0 for the lower boundary (see https://hddm.readthedocs.io/en/ latest/howto.html for more details).


Box 3. Parameters in hierarchical drift-diffusion models
HDDM employs hierarchical Bayesian modeling by default, where each participant's free parameters are sampled from population-level distributions 
(Wiecki et al., 2013)
. Taking full DDM (Model 0) as an example, non-decision time is assumed to be drawn from a normal distribution: ~( , ), where and are the mean and standard deviation of the population-level normal distribution of non-decision time t. Similarly, / / and / / are the means and standard deviations for the other three parameters, respectively. In addition, three free parameters / / indicate the trial-by-trial variability of non-decision time ( ), drift rate ( ), and initial bias ( ), which are estimated only at the population level.
Consequently, there are a total of 11 population-level parameters. At the subject level, each subject has her own estimate of the parameter of a, v, t, z, leading to a total of 4 * subject -level parameters. Thus, in the full DDM, the number of parameters is 11 plus 4 * .
The hierarchical structure of the full DDM in HDDM. The parameters inside and outside the rectangle are subject and population level parameters, respectively. / are the indices of participants ( = 1, 2, . . . , ) and trials ( = 1, 2, … . ), where , is the data (choice/response time) of the i-th trial in the p-th subject. HDDM provides two types of priors: weakly informative priors and non-informative priors. By defa ult, dockerHDDM uses weakly informative priors as summarized in the Table below 
(Wiecki et al., 2013)
. T he default informative priors are suitable for most perceptual tasks. However, for tasks with longer response times, it is recommended to use non-informative priors. In this case, one has to set the parameter `informa tive=False` when defining the model, e.g., `m = hddm.HDDM(data, informative=False)`.
DDM parameters' informative prior
~(2,3) ~ℋ (2) ~( , 2 ) ~(1.5,0.75) ~ℋ (2) ~( , 2 ) ~( (0.5,0.5)) ~ℋ (0.05) ~( , 2 ) ~(0.4,0.2) ~ℋ (1) ~( , 2 ) ~ℋ (2) ~ℋ (0.3) ~ℬ(1,3)
Note, table extracted and refined from 
(Wiecki et al., 2013)
. represents a Normal distribution parameterized by the mean ( ) and standard deviation ( ). ℋ represents a Half-Normal distribution, which is a positive-only distribution parameterized by the standard deviation. represents a Gamma distribution, parameterized by the mean ( ) and the rate ( ). ℬ represents a Beta distribution, parameterized by alpha and beta. The term represents the inverse logit function also known as the logistic function.
HDDM also allows parameters to vary with variables by integrating hierarchical linear regression models (also called linear mixed models or multi-level models). Specifically, the `hddm.HDDMRegressor()` function allows any or all of the four parameters of DDM 
(a, v, t, z)
 to be modelled as a function of experimental conditions or other variables (e.g., EEG signal). In HDDM, the regression models are defined using the Python package patsy (see https://patsy.readthedocs.io/en/latest/quickstart.html), which uses the same syntax for defining regression functions as in other commonly used statistical packages. For example, in Model 2 in the main text, we used the expression ` ~ 1 + C conf Tre men 'LC' `, where the term to the left of "~" is the dependent variable and the term to the right of "~" is the regression equation. The term '1' refers to the intercept, which corresponds to the variable _ in the output. The term 'C(conf, Treatment('LC'))' indicates the slope coefficient, which corresponds to the variable _ ( , (' ')) 
[ . ]
 . As in other hierarchical regression models, both the intercept and the slope can be estimated at the population level and the subject level (referred to as "fixed effects" and "random effects" or "varying effects" respectively, D. J. 
Johnson et al., 2017;
Pedersen & Frank, 2020;
Wiecki et al., 2013)
, depending on how the model is specified. In `hddm.HDDMRegressor()`, the default is hierarchical model with random intercept but no random slope. We need to set `group_only_regressors=False` to include the random slope (as we did int Model 2).
Although both the `depends_on` argument and the `HDDMRegressor` function allow parameters to vary with discrete variables (e.g., conflict levels), there is an important difference between them. The `depends_on` argument defines the parameter split by condition. Specifically, the means of the parameters under each condition are derived from a share prior, whereas the variability of the parameters is consistent across conditions. The `HDDMRegressor` function defines the relation between parameters and condition by a linear model specification, which mean the intercept and slope in the linear regression both has their own priors. In a word, `depends_on` is unable to utilize within-subject effects because each subject's condition is derived from the population prior, whereas `HDDMRegressor` allows each subject to have their own intercept, which allows for the estimation of within-subject variation across conditions. Thus, the choice of model definition is relevant to the assumptions made about the relationship between parameters and the experimental conditions. See 
Wiecki et al. (2013)
 for more details.


Model Specification
As a demonstration of model specification, we will test an example question: is there an effect of conflict levels on drift rate 
(Wiecki et al., 2013
). To answer the question, we constructed three computational models (see 
Table 4
). 
Table 4
. Models used in this tutorial.
Note: `hddm.HDDM()` is the default function for constructing a hierarchical drift diffusion model. The `include` argument allows the addition of free parameters, which are fixed by default. The `depends_on` argument specifies a parameter (e.g., v) that depends on a categorical independent variable (e.g., 'conf'). The `hddm.HDDMRegressor()` is a HDDM function that includes effects of conditions in a linear regression fashion. The `keep_regressor_trace` argument allows a trace of the regressor to be kept, which is needed for posterior predictive checks. By default, the hierarchical regression allows only the intercept to vary across participants, while the slope is fixed at the population level. The `group_only_regressors = FALSE` argument additionally estimates the slopes at the individual level in the regression model. 11 + = 67 free parameters.
Model 1 allows the drift rate to vary as a function of the conflict levels (i.e.,
`depend _on {' ': 'conf'}` in HDDM). Specifically, Model 1 sets two drift rate variables each for low and high conflict levels at the both population-and individual-level, respectively.
Thus, Model 1 has 12 population-level parameters: the mean and standard deviation for , , and ; two means ("v_(LC)" and "v_(HC)") and one standard deviation for ; and three inter-trial variability parameters ( / / ). Similarly, at the individual level, there are 5 ( / / / / ) x 14 (subjects) = 70 individual-level parameters. Thus, Model 1 has a total of 82 free parameters.
Note that Model 1 assumes complete independence between high and low levels of conflict within subjects. This assumption may be inappropriate, as it is likely that a person who responded relatively quickly in the "LC" condition will also respond relatively quickly in the "HC" condition, and vice versa. See Box 3 for more detailed differences between Model 1 and Model 2.
Model 2 was constructed to include correlations between drift rates across conflicting levels. In Model 2, we use a hierarchical regression model with `hddm.HDDMRegressor()` by using the formula ` ~ 1 + C conf Tre men 'LC' ` (see Box 3). This formulation automatically assigns two free parameters, the intercept and slope, to each subject. Thus, there are 5 * 14 = 70 individual-level parameters in Model 2. Accordingly, Model 2 has four parameters for v: "v_Intercept" and "v_Intercept_std" are the mean and standard deviation of the intercept;
"v_C(conf)[T.HC]" and "v_C(conf)[T.HC]_std" are the mean and standard deviation of the slope.
Therefore, Model 2 has 13 population-level parameters: the mean and standard deviation for , , and ; the mean and standard deviation of the slope and the intercept of the regression for ; and three inter-trial variability parameters ( / / ). Taken together, Model 2 has a total of 13 + 70 = 83 free parameters.


Model Fitting
The defined HDDM model allows the MCMC algorithm to be run using the `.sample()` To accurately estimate parameters and ensure convergence in hierarchical modeling, we set up four MCMC chains of 10,000 samples with 5,000 burn-ins (i.e., a total of 20,000 samples for each parameter). Please refer to Section 3 for the more detailed settings and arguments description.
With the new functionality introduced by dockerHDDM, we can calculate the loglikelihood of the model and generate posterior predictions after model fitting. Furthermore, the output of the model fitting can be converted into InferenceData, `m2_infdata`, for subsequent analyses as described in Section 3. We emphasize that model fitting is demanding in terms of computational resources and memory. For example, in our tests with the Apple M1 chip, Intel i7-10700 CPU and AMD Ryzen 9-5900HX, model fitting took around 2-3 hours for 10,000 samples. Consequently, fitting three models took about 6-9 hours, with memory usage ranging between 8-12 GB. Additionally, if pointwise likelihood calculations (i.e., with the argument `loglike=True`) and posterior predictive data generation (i.e., with the argument `ppc=True`) are enabled, an extra 1-3 hours are needed for each model. More importantly, the memory consumption could escalate to 20-30 GB because pointwise likelihood and posterior predictive data generation will result in a large number of new data. See discussion for recommendations to improve efficiency.


Model Diagnosis
In Bayesian inference, it is crucial to ensure the convergence of MCMC chains. With ArivZ, dockerHDDM supports both visual inspection and quantitative convergence checks (see Chapter 10 in 
Martin et al., 2024)
. 
Figure 5
. Model diagnosis. (A) Visualization of the traces of all chains using `az.plot_trace()`, with the argument `var_names` set to focus on the parameter "v_Intercept" as an example. `compact=False` and `legend=True` ensured that the individual traces of each chain would be visible. The MCMC chains are valid and reliable when they fluctuate around a value and different chains are indistinguishable from each other, a scenario often referred to as a "caterpillar" shape. (B) Output of `az.summary()`, which includes the mean and standard deviation of the Monte Carlo standard error (MCSE), the effective sample sizes (bulk-ESS and tail-ESS), and ̂. Note that the summary data frame has been sorted by ̂ so that we can easily compare the minimum and maximum values of ̂. `az.plot_trace()` can be used to visualize the posterior distributions of parameters (i.e., trace plots of the MCMC, 
Figure 5A
).
The Gelman-Rubin statistics (̂), and effective sample size (ESS) provide quantitative measures (see Box 1).
`az.rhat()`computes ̂, which should be close to 1 for good convergence; values below 1.01 are typically recommended 
(Gelman & Rubin, 1992)
.
`az.ess()` calculates ESS, a measure of the precision of posterior estimates. If the ESSbulk is over 400 (see Box 1), the distribution's center is well-resolved, and we should ensure high ESS across all regions of the parameter space 
(Martin et al., 2024;
Vehtari et al., 2021)
.
The latter two methods are covered by ArviZ's `az.summary()` 
(Figure 5B
).


Model Comparison
Upon verifying chain convergence, we proceed with model comparison to identify the best-fitting model. The evaluation metric provided in the original HDDM is deviance information criterion 
(DIC, Spiegelhalter et al., 2002)
. We include two more methods in dockerHDDM: widely applicable information criterion (WAIC, 
Watanabe, 2010)
 and Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV, 
Vehtari et al., 2017)
. These methods comprehensively integrate posterior samples for model comparison and evaluation (see Box 4).


Box 4. Linking DIC, WAIC, and PSIS-LOO-CV to AIC
The Deviance Information Criterion (DIC), Widely Applicable Information Criterion (WAIC), and Pareto-Smoothed Importance Sampling Leave-One-Out Cross-Validation (PSIS-LOO-CV) are criteria founded on the concept of out-of-sample predictive accuracy, i.e., the accuracy of using the fitted model to predict new data generated by the assumed data-generating process. Predictive accuracy is often encapsulated by the log predictive density (Box 1). However, the log predictive density approximated using the observed data and the posterior estimates of parameters is biased, and an adjustment is required to correct the bias. Thus, the key difference between DIC, WAIC and PSIS-LOO-CV lies in the difference between the two terms of log predicted density and corrected bias (see the table below).
DIC uses the Bayesian posterior means for estimating log predictive density and includes an adjustment based on the effective number of parameters ( ). It is particularly suited for hierarchical models, offering an improved estimate of predictive density 
(Spiegelhalter et al., 2002)
.
WAIC further refines DIC, evaluating the log predictive density across the entire posterior and correcting bias via the variability of log predictive density (̂). This adjustment is crucial for measuring model robustness and guarding against overfitting 
(Watanabe, 2010)
. Both DIC and WAIC rely on estimating the effective number of parameters, but DIC assumes a Gaussian distribution for the likelihood, which simplifies the calculation 
(Lunn et al., 2012)
. In contrast, WAIC does not rely on this strict assumption and uses the full posterior distribution, offering greater flexibility and accuracy but at a higher computational complexity 
(Gelman et al., 2014)
.
For the demonstration, we compared three models across all three evaluation metrics (lower value is better) . As shown in 
Table 5
, Model 2 exhibits the lowest values on all three metrics, indicating it is the best model. The results of model comparison revealed that Models 1 and 2 are much better than the baseline Model 0, suggesting that experimental conflict conditions have a substantial effect on drift rates. Moreover, Models 2 is slightly better than Model 1, suggesting that regression model may suit the data better. Nevertheless, the similarities between Model 1 and Model 2 suggests that both models fit the data adequately in this case. PSIS-LOO-CV estimates the predictive density by simulating the leave-one-out cross-validation, which by definition is the out-of-sample predictive accuracy, so bias correction is no longer needed for PSIS-LOO-CV. Please see 
Gelman, Hwang, & Vehtari (2014)
 and 
Vehtari, Gelman, & Gabry (2017)
 for more details on these three indices.


Predictive accuracy Adjustment
Formula  
(Spiegelhalter et al., 2002)
. ̂ is the WAIC's approach to adjusting the effective number of parameters 
(Watanabe, 2010)
.
AIC ( | ̂) k −2 ( ( | ̂) − ) DIC ( | ̂) −2 ( ( | ̂) − ) WAIC ̂ ̂ −2 (̂ − ̂) PSIS-LOO-CV ̂− N.A. −
posterior trace (see Box 1). This variable is not directly provided in the HDDM object and must be customized to be calculated via the likelihood function and posterior trace.
In dockerHDDM, the pointwise log-likelihood can be computed at sampling and fitting stage, via `m.sample(... , retutn_infdata = True, loglike = True)` (see <Code Block 2>), or after the model has been sampled and fitted, by `m.to_infdata(loglike = True)`. Both ways return InferenceData, allowing users to immediately compute WAIC and PSIS-LOO-CV. After that, the evaluation metrics for each model's InferenceData are available using ArviZ's `compare` method (see <Code Block 3>), which returns the results of WAIC for the argument `ic="w c"` or PSIS-LOO-CV for `ic=" oo"`.
<Code Block 3>
```Python compare_dict = { 'm0': m0_infdata, 'm1': m1_infdata, 'm2': m2_infdata } az.compare(compare_dict, ic = 'loo') ``` inally, it's important to note that the model comparison metrics only allow us a relative ranking of alternatives. To assess the absolute goodness-of-fit of the model, we recommend performing the posterior predictive check (PPC), as discussed in the next section, alongside the diagnostic information provided by LOO and WAIC (see Chapter 5 in 
Martin et al., 2024;
Vehtari et al., 2017)
.


Posterior Predictive Check
In addition to model comparison, which assesses relative performance, the posterior predictive check (PPC) evaluates how well predictive data generated from posterior samples of parameters align with the actual data. PPC is crucial because model comparison only evaluates the "least worst" model, but this model may not necessarily account for the data very well (see Chapter 5 in 
Martin et al., 2024)
.
ArviZ offers convenient visualization tools for inspecting PPC 
(Kumar et al., 2019)
. The function `az.plot_ppc()` is helpful to visualize PPC at the individual or condition level ( 
Figure 6
). In the demonstration, the synthetic data from Model 2 match more closely the actual data compared to the baseline Model 0, and this difference becomes apparent when examining PPC at the individual-( 
Figure 6A
) and condition-level ( 
Figure 6B
). Other approaches for PPCs can be used to quantify accordance between data and model across quantiles of the RT distribution, for example using Bayesian predictive versions of quantile probability plots 
(Frank et al., 2015;
Ging-Jehli et al., 2021)
, and example code in HDDM is available upon request. 
Figure 6
. Posterior predictive check plot `az.plot_ppc()` for Model 0 "m0" and Model 2 "m2". Solid black lines are the density plot of the observed RT data; blue lines are the posterior predictive samples; each line represents the predicted RT distribution based on one posterior predictive sample; yellow dashed lines represent the mean of all predicted RT distributions across all posterior predictive samples. (A) shows the results of the comparison between the two models (m0 vs. m2) at the individual level (subjects 3 and 11 as an example); (B) shows the results of the comparison at the condition level (i.e., "LC" represents lower conflict and "HC" represents higher conflict). All plots in the left column are for m0 and all plots in the right column are for m2. Note that the argument `coords` specifies the PPC level (individual or group level) that should be preprocessed before plotting. `num_pp_samples` is used to set the number of predictive data required for plotting. 


Statistical Inference
A final step in Bayesian modeling is to draw statistical inferences from the posterior parameter distributions in the best-fitting model. In our example, we will test the hypothesis whether drift rates significantly differ between high and low conflict conditions based on Model2 ("m2" in the Notebook). This hypothesis will be tested using the posterior samples of the regression coefficient in "m2", which has a variable name "v_C(conf, Treatment('LC'))[T.HC]". `var_names` argument can be used to select both group-level and individual-level parameters for analysis. `hdi_prob` argument specifies the probability of the HDI, typically set at 0.95 to correspond to a 95% confidence interval. `rope` defines the limitations of ROPE, which is a range considered to be equivalent to the null hypothesis or a reference value for the parameter.
The results show no overlap between the 95% HDI and the ROPE, indicating that the parameter is credibly different from zero. (B) Violin plot of parameter posteriors at two conflict levels. The black line is the 95% HDI and the white dot is the mean. The drift rate is lower in high conflict (HC) than in low conflict (LC) conditions.
Note that there are several acceptable methods for Bayesian hypothesis testing, such as Bayes factors 
(Boehm et al., 2023;
Wagenmakers et al., 2010)
, Maximum a posteriori (MAP) based p-value 
(Mills, 2018)
, Directional Probabilities (PD, 
Makowski et al., 2019)
, and the Full Bayesian Significance Test (FBST, 
Kelter, 2022)
. In cognitive science and psychology, while Bayesian factors are often advocated as a Bayesian alternative to frequentist p-values 
(Kelter, 2021;
van de Schoot et al., 2017;
Wagenmakers et al., 2010)
 should be used in which settings of scientific hypothesis testing 
(Kelter, 2023;
Makowski et al., 2019)
. Therefore, it is useful to consider various Bayesian hypothesis testing methods depending on the study objectives and design 
(Kelter, 2023;
Kruschke, 2021;
Makowski et al., 2019)
.
Here, we demonstrate Bayesian inference using an approach that combines the approach HDI of the regression coefficient to this ROPE, we find that the HDI falls completely outside the ROPE ( 
Figure 7A
), suggesting that the drift rate is higher in the low conflict condition than the high conflict condition ( 
Figure 7B
).
Therefore, considering the results from various aspects (model comparison, PPC, and posterior inference), we conclude that the model which takes into account the influence of conflict level on drift rate performs the best. Moreover, high conflict affects the cognitive process of decision-making by impeding the speed of evidence accumulation.


Discussion
This tutorial focuses on an easy-to-use computational environment for HDDM, including installation of the tool, its features, and case applications. While some conceptual discussions have been addressed in other papers 
(Boag et al., 2024;
Shinn et al., 2020;
Voss et al., 2013)
, we have nevertheless discussed some relevant issues below.


Why use dockerHDDM among tools?
Inference for the DDM can be implemented via multiple software/packages, such as fast-DM 
(Voss & Voss, 2007)
, flexDDM (LaFollette et al., 2024), rtdists 
(Singmann et al., 2022)
, EZ-DDM 
(Wagenmakers et al., 2007)
, pyDDM 
(Shinn et al., 2020)
. For more details on tool and algorithm 
6
 The ROPE should be tailored to the specific paradigm and research question 
(Dienes, 2021)
 and reflect the range of possible values for each parameter (e.g., 
Tran et al., 2021)
. For example, a recent systematic parameter review of DDM found that the absolute value of a drift rate ranged from 0.01 to 18.51, with a median of 2.25 
(Tran et al., 2021)
; another simulation and metaanalysis of conflict tasks showed that a drift rate between 0.05 and 0.35 captured the conflict effect 
(Hedge et al., 2018)
. Thus, we choose ROPE [-0.2 0.2] for illustrative purposes, implying that effects on drift rates smaller than 0.2 are not of interest.
Running title: dockerHDDM for Bayesian HDDM Modeling comparisons, see 
Shinn et al. (2020)
. While all the above tools are estimated in a frequency framework and fit data at the individual participant level, HDDM takes the Bayesian approach and estimates model parameters at both the individual and group level (i.e., the hierarchical models or multilevel model approach, see 
Wiecki et al., 2013)
. Tools that also allow the Bayesian hierarchical modeling approach of DDM include brms based on RStan 
(Henrich et al., 2023)
, the Wiener module in JAGS 
(Wabersich & Vandekerckhove, 2014)
, EMC2 
(Stevenson et al., 2024)
 and
hBayesDM 
(Ahn et al., 2017)
. See 
Table 6
 for comparison between these tools and HDDM.  
(Matzke & Wagenmakers, 2009)
, and applicable to typical cognitive experiments.
Another advantage of HDDM is its support for diverse accumulation models, including models with collapsing boundaries and those integrated with reinforcement learning, called RLDDM 
(Fengler et al., 2022;
Pedersen et al., 2017;
Pedersen & Frank, 2020)
. Additionally, the latest version of HDDM provides many likelihood-free models, broadening its applications. For instance, its integration with neural networks, such as the LANs 
(Likelihood Approximation Networks, Fengler et al., Fengler et al., 2021)
, has greatly enhanced the efficiency of model design and development.
A notable limitation of dockerHDDM is its lack of integration with the most advanced parameter estimation techniques. For instance, its successors, HSSM and EMC2, have begun incorporating advanced MCMC methods. Moreover, innovative neural network approaches, such as LANs 
(Fengler et al., 2021)
, MNLE 
(Boelts et al., 2022)
, and Bayesflow 
(Radev et al., 2022)
, have the potential to significantly enhance these estimation procedures. However, the mastery of these cutting-edge techniques requires a higher level of expertise to prevent misuse.
Consequently, we propose that the mission of dockerHDDM should be to streamline operations and lower the barrier to entry, facilitating analogical learning, and ultimately preparing users for the transition to the more sophisticated methods.


Whether to include parameters' inter-trial variability?
As a demonstration, we utilized the seven-parameter full Drift Diffusion Model. If a user wishes to fit only the four-parameter model, the unnecessary parameters can be removed from the include argument, such as `include= 
['a', 'v', 't', 'z']
`. In contrast, the full model, which integrates trial-by-trial variability, is known for its robustness in fitting various datasets and accommodating extreme response times, including fast and slow errors 
(Schubert et al., 2017)
.
However, 
Lerche & Voss (2016)
 argue that exclude trial-by-trial parameters can enhance the fit and recovery of fundamental parameters.
Consequently, the choice to include trial-by-trial variability requires a delicate balance between the prediction and complexity of the model and the specific requirements of the data.
Given the extensive data requirements for inferring across-trial variability, our stance is to cautiously include across-trial variability in the model for a more robust fit and more precise inference of the basic parameters (see similar discussion in 
Boag et al., 2024)
. For instance, since the variability of the non-decision time tends to be easily recovered (e.g., the result of the parameter recovery in Appendix 
Figure S2
), it may be prudent to include only this parameter, but not the other variability parameters by default. Nevertheless, when the dataset is substantial and the research objective prioritizes the analysis of specific response time pattern, such as fast or slow errors, the selective integration (the parameter variability of drift and start point, also see 
Table 1)
 of these parameters may be warranted. We recommend reading the work by 
Boehm et al. (2018)
, which offers expert advice and recommendations on estimating across-trial variability parameters.


Data quantity and quality for fitting DDM
Both the number of subjects and the number of trials should be considered. Due to the hierarchical nature of the model, hierarchical models typically require fewer trials than non-hierarchical models 
(Alexandrowicz & Gula, 2020;
Wiecki et al., 2013)
. In general, 12 subjects are sufficient to obtain stable results 
(Wiecki et al., 2013
), but we recommend collecting data from more than 20 subjects for a more robust fit. However, the number of sufficient trials vary depending on the parameters of interest. For the basic four-parameter model, the number of trials has a small effect on parameter estimates 
(Alexandrowicz & Gula, 2020)
. 20 trials appear to be the minimum standard, and more than 50 trials tend to produce robust results 
(Wiecki et al., 2013)
. Estimates of and tend to be superior to those of and . To obtain more accurate estimates of , a number of trials greater than 100 is recommended 
(Alexandrowicz & Gula, 2020)
. For parameters such as , , and , a large number of trials are required for estimation, preferably more than 120 trials 
(Wiecki et al., 2013)
. Recent discourse emphasizes that the determination of the number of subjects and trials should be aligned with considerations of experimental design, desired target effects, and parameter recovery simulations 
(Boag et al., 2024)
. For further empirical guidelines, see 
Boehm et al. (2018)
 and 
Lerche & Voss (2017)
.
It is important to note that parameter estimation can be affected by extreme values, such as very fast response times. HDDM addresses this issue by assuming a mixture model where a proportion of the response times are from a uniform distribution 
(Ratcliff & Tuerlinckx, 2002;
Wiecki et al., 2013)
. The proportion of response time is controlled by the parameter `p_outlier`, which is set to 0.05 by default. This approach helps mitigate the effect of extreme values and ensures a more robust parameter estimation.
Finally, it is essential to conduct posterior predictive checks to validate the model (see Section 4.6). These checks help to ensure that the model is capable of accurately reproducing the observed data, thus providing confidence in the evaluation of the model and parameters.


Computational Resources and Tips
To achieve accurate estimates, more subjects, more trials, and often more samples are required, leading to increased demands on computational resources. This is not unique to dockerHDDM;
other tools using MCMC algorithms, such as DMC and brms mentioned earlier, are also affected by these factors. In the examples provided in this article, fitting each model with 14 subjects and 3988 trials takes 2-3 hours and requires 8-12 GB of memory. Running out of memory can cause the Jupyter kernel to suspend and restart, interrupting the process. Predictably, computational resources become a limiting factor with increasing data. To facilitate better model analysis, we offer the following tips and recommendations:
1. Initial Testing: When initially building the model, use subset data from a small number of subjects and reduce the MCMC sample size to verify that the model definition and code are correct.
Once validated, increase the data and sample sizes.
2. Adjust memory settings. If users experience a Jupyter kernel suspension or restart due to memory constraints, they can attempt to configure or increase virtual memory. For Windows users, it is necessary to check and remove the memory usage limitations imposed by WSL (Windows Subsystem for Linux).
3. Separate Execution: Model fitting, calculation of point-wise log-likelihood, and generation of posterior predictive checks (PPC) data can be executed separately. This approach helps prevent interrupting long-running processes due to errors and ensures that each step can be independently validated and debugged before proceeding to the next.
4. Notebook Segmentation: Fit models into separate notebooks to reduce the resource load of loading multiple models.
5. Model Saving: Save the fitted models and then load only the inferenceData files instead of the entire models to reduce resource usage.
6. Cloud Deployment: Docker is easily deployed in cloud computing environments (or use the docker image in Singularity). Use your institution's computing services or rent cloud computing services to handle larger datasets.


Summary
In this article, we introduce dockerHDDM, a user-friendly, out-of-the-box, and one-stop Docker image for implementing HDDM analysis within a modern Bayesian hierarchical workflow. Our dockerHDDM has three major advantageous: (1) it leverages Docker to solve compatibility issues and simplify the installation process;
(2) it ensures broad support across different machines equipped with either Intel or Apple chips; and (3) it integrates state-of-the-art Bayesian modeling practices with ArviZ, facilitating a more principled Bayesian workflow. We also provide a stepby-step video tutorial on how to use dockerHDDM.
While we have provided a step-by-step guide to using dockerHDDM, it is unfortunately not possible to provide a comprehensive introduction to computational modeling. Given the extensive knowledge required for principled computational modeling, we recommend readers refer to the materials in Box 5 for a deeper understanding of the DDM family, computational modeling,


Box 5. Recommendation for Further Reading
A full understanding of how Bayesian hierarchical drift-diffusion modeling works requires not only basic knowledge of DDM, but also knowledge of Python programming, Bayesian statistics, and hierarchical regression models. This background knowledge is generally not part of the coursework in psychology or neuroscience education, although the situation is changing in recent years. We recommend the following resources to quickly catch up and avoid misuse or abuse of HDDM.
Background knowledge/skills Resource Bayesian statistics 
Etz & Vandekerckhove, 2018;
Kruschke, 2014
Kruschke, , 2018
Lambert, 2018;
Martin et al., 2024;
McElreath, 2020;
van de Schoot et al., 2021.
 (Bayesian) Hierarchical (regression) models https://twiecki.io/blog/2014/03/17/bayesian-glms-3/; https://github.com/lei-zhang/BayesCog_Wien; Capretto et al., 2020.
Computational modeling 
Blohm et al., 2020;
Busemeyer, 2015;
Busemeyer & Diederich, 2009;
Etz & Vandekerckhove, 2018;
Farrell & Lewandowsky, 2018;
Lee & Wagenmakers, 2014;
Wilson & Collins, 2019;
Zhang et al., 2020
. Drift Diffusion Models Boag et al., 2024
Ratcliff et al., 2016;
Ratcliff & McKoon, 2008;
Voss et al., 2013.
 Sequential sampling models beyond DDMs 
Fengler et al., 2022;
Forstmann et al., 2016;
Ratcliff et al., 2016.
 hierarchical models, and Bayesian modeling. We expect that dockerHDDM and this detailed tutorial will reduce the technical burden and help readers get started with computational modeling.
Ultimately, we hope that this tool and the computational modeling concepts presented in the tutorial will promote the computational reproducibility of drift-diffusion modeling for users of all levels of computational expertise. 
Wiecki et al. (2013)
 demonstrated the superiority of Bayesian methods and hierarchical models for parameter recovery in HDDM. We illustrate the parameter recovery analysis of Model 2 in 
Figure S2
. The results show that our model fitting approach can yield good parameter recovery.


Parameter recovery result
For the code that repeats this result, see https://github.com/hcp4715/dockerHDDM/blob/master/ dockerHDDMTutorial/Parameter_recovery.ipynb. 
Figure S2
. Model 2 parameter recovery results. Blue is the true parameter; orange is the recovered parameter; white dots are the means; and the bar is the 95% HDI range. Subplot A shows the parameter recovery results at the group level, including 8 parameters, of which the first 5 are basic parameters and the last 3 are trial-by-trial variants; Subplot B shows the parameter recovery results at individual level, including 5 basic parameters for 13 subjects out of 65.
panel in Figure 2, corresponding to Sections 2.2 and 2.3). Finally, within a working Jupyter notebook we show how to analyze an example dataset with dockerHDDM in a principled Bayesian workflow (bottom panel in Figure 2, corresponding to Section 4).


Figure
Figure 2. dockerHDDM usage flowchart. Note that the code in the figures is for demonstration purposes only. Specific instructions and copyable code can be found in the corresponding sections. The top panel describes how to install Docker, corresponding to Section 2.1; the middle panel describes how to pull and run dockerHDDM, corresponding to Sections 2.2 and 2.3; and the bottom panel shows the workflow in dockerHDDM, corresponding to Section 4. A video tutorial is available at: https://www.youtube.com/watch?v=ZU1fbXEuP8s or at https://osf.io/3upng/ files/osfstorage/66d5c2a3f2abc7a7a359a26c/.


Figure 4
4
Figure 4. Docker commands to download and run dockerHDDM. (A) Download/pull dockerHDDM from the Docker hub. The command by default downloads the latest version of `hcp4715/dockerHDDM` if the image tag is not specified. The CPU architecture (Apple or Intel chips, corresponding to ARM64 and AMD64 architectures, respectively) is automatically recognized when the image is downloaded. (B) Command to start a container. Note, "\" separates different lines of a command in Linux and MacOS Terminal but not in Windows.


Beginners can follow HDDM_Basic_Tutorial.ipynb to get a basic understanding of HDDM, as discussed in Wiecki et al. (2013); HDDM_Regression_Stimcoding.ipynb covers more advanced models with regression, where parameters can vary based on experimental conditions and other covariates; Posterior_Predictive_Checks.ipynb introduces posterior predictive checks, showing how to generate predicted data from fitted parameter posteriors and how to analyze these predicted data;


specification and fitting, model diagnosis, model comparison, posterior predictive check, and statistical inference. The code reproduced in this section can be found in dockerHDDM_Workflow.ipynb in dockerHDDM environment.


Model 0 served as the baseline without considering the effect of conflict level on the model parameters. The model contains the seven parameters, referred to as the full DDM, including the decision boundary ( ), drift rate ( ), non-decision time ( ), and decision bias ( ), as well as , , and that indicates the trial-by-trial variations of , , and(Boehm et al., 2018;
Ratcliff & Tuerlinckx, 2002)
.By default, HDDM considers the hierarchical modeling approach that includes parameters at both the individual-and the group-level (see Box 3). Model 0 has 11 population-level parameters, including the mean and the standard deviation for the four basic parameters ( / / / ) and three parameters ( / / ) for the inter-trial variations. At the individual level, each subject also has a full set of four basic parameters, yielding a total of 56 = 14 * 4 parameters. Thus, Model 0 has Models Describe HDDM functions for defining a model (`df` is the data from Cavanagh et al.


method for model fitting and parameter estimation. The definition and fitting of Model 2 are used here as an example (see <Code Block 2>):<Code Block 2> ```Python # define a model by hddm.HDDMRegressor m2 = hddm.HDDMRegressor( df, 'v ~ C(conf, Treatment('LC'))', group_only_regressors = False, keep_regressor_trace = True, include=['a', 'v', 't', 'z', 'sv', 'st', 'sz']) # fitting model and return InferenceData m2_infdata = m2.sample( 10000, chains = 4, save_name = 'm2', return_infdata = True, sample_prior = True, loglike = True, ppc = True) ```


Figure 7 .
7
(A) Statistical inference of parameters. The high-density interval (HDI, black line and texts) is compared with the region of practical equivalence (ROPE, red line and text).


Table 2 .
2
Comparisons between dockerHDDM and the original HDDM package
HDDM
dockerHDDM


2. Install and Use dockerHDDM 2.1. Install Docker
2. dockerHDDM usage flowchart. Note that the code in the figures is for demonstration purposes only. Specific instructions and copyable code can be found in the corresponding sections.
Docker serves us to create an all-in-one, fast, cross-platform computing environment. The Docker
website provides easy-to-follow installation instructions (https://docs.docker.com/get-docker/)
and supports Windows, MacOS, and Linux (see Box 2). Windows users should ensure their system
version is 21H2 (build 19044) or higher and have either WSL or Hyper-V configured prior to
installation (see https://docs.docker.com/desktop/install/windows-install/).
The top panel describes how to install Docker, corresponding to Section 2.1; the middle panel
describes how to pull and run dockerHDDM, corresponding to Sections 2.2 and 2.3; and the
bottom panel shows the workflow in dockerHDDM, corresponding to Section 4. A video tutorial
is available at: https://www.youtube.com/watch?v=ZU1fbXEuP8s or at https://osf.io/3upng/
files/osfstorage/66d5c2a3f2abc7a7a359a26c/.


Table 3
3
Example dataset from Cavanagh et al. (2011).
subj_idx rt response
conf
0
1.21
1.0
HC
0
1.63
1.0
LC
0
1.03
1.0
HC
0
2.77
1.0
LC
0
1.14
0.0
HC
Note: The data structure required for HDDM is long-format data, where each row represents one trial.


Table 5 .
5
Model comparison with different criteria. Rank is from the best model to the worst. Models 0 to 2 are referred to as m0 to m2.Note that WAIC and PSIS-LOO-CV require the pointwise log-likelihood of each data point given a posterior sample of parameters, which must be computed using the likelihood function and
Rank*
DIC
PSIS-LOO-CV
WAIC
1
m2 (10654.89)
m2 (10646.25)
m2 (10646.20)
2
m1 (10655.24)
m1 (10647.21)
m1 (10647.15)
3
m0 (10835.24)
m0 (10824.93)
m0 (10824.89)
*


Table 6
6
HDDM stands out for its ease of use, enabling users to construct and fit basic models with just a few lines of code. It facilitates the definition of complex mixed-effects models without the need for prior specifications, making it more accessible for beginners. While brms and EMC2 also defines mixed effects models well, it necessitates users to manually define prior distributions for random effects and covariance structures. Additionally, RStan and JAGS require expertise in linear model reparameterization. The absence of these expertise may result in model fitting failures or biased estimates. On the other hand, the simplicity of HDDM comes at the cost of flexibility, as it restricts users to the default priors (see Box 3) and does not allow for customization. However, the weakly informative prior implemented in HDDM was based on previous meta-analyses of published results
Tools comparison for modeling hierarchical DDM
(docker)HDDM
brms/RStan/hBayesDM
JAGS
EMC2
Language
Python
R
R
R
MCMC
Metropolis-Hastings
NUTS
Gibbs
Particle
Algorithm
Sampling
Metropolis
Support Models
DDM, full DDM,
DDM,
DDM
DDM,
RLDDM,
full DDM
LBA,
collapsing boundary
RDM,
variants,
etc.
etc.
Custom Prior
No
Yes
Yes
Yes
Linear Mixed
Yes
Yes
Yes
Yes
extension
Likelihood-free
Yes
No
No
No
Note: DDM = drift-diffusion model; MCMC = Markov chain Monte Carlo; RLDDM = reinforcement
learning drift diffusion model; LBA = linear ballistic accumulator; RDM = racing diffusion model.


Running title: dockerHDDM for Bayesian HDDM Modeling


Note that `/home/jovyan/{any_folder_name}` is a path mounted in the Jupyter Docker image, and `{any_folder_name}` will be visible in the browser. The default username is `jovyan`, and it cannot be changed.2 For beginners unfamiliar with Jupyter Notebook, do not panic! It is just an interface where you can write code and immediately check results. You may visit the official website at https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb to try out a web-based platform online. The Jupyter website also provides extensive documentation for users who want to learn more about Jupyter Notebook and Python programming (see https://docs.jupyter.org/en/latest/).


To run the example notebooks faster, we only use 500 samples here. For a more in-depth understanding of the MCMC settings, we recommend reading
(van de Schoot et al., 2017;
Wiecki et al., 2013)
. The burn-in samples serve to calibrate the fitting, so the final samples need to exclude burn-in samples, yielding a total of 500 -100 = 400 samples per chain. Generally, a larger number of samples improves the estimation accuracy of a model.4  InferenceData is a more modern data construct that contains prior, posterior, a posterior predictive samples and observed data, facilitating the visualization and analysis of multiple joint datasets
(Hoyer & Hamman, 2017;
Kumar et al., 2019)
.Running title: dockerHDDM for Bayesian HDDM Modeling


DIC can be extracted directly from the model rather than InferenceData, e.g. `m0.dic`.








Acknowledgments
This work was supported by the National Natural Science Foundation of China (32100901) and
Natural Science Foundation of Shanghai (21ZR1434700) to R-Y. Z.






Code Availability
All resources are available on the Open Science Framework (OSF) at https://osf.io/3upng/?view_only=2425347775e749c3bab67af68607b918, which is linked to the GitHub repository at https://github.com/hcp4715/dockerHDDM/ and other resources.
The software, data, and scripts (Jupyter notebooks) used to generate the models and results described in this article can be accessed via the dockerHDDM image at https://hub.docker.com/r/hcp4715/hddm. Alternatively, readers can find our online notebooks and related materials here: https://git hub.com/hcp4715/dockerHDDM/ and https://github.com/hcp4715/dockerHDDM/tree/master/Off icialTutorials.
Additionally, the code used to create our dockerHDDM images is available at https://github.com/hcp4715/dockerHDDM/blob/master/Dockerfile.
For any questions regarding this tutorial or related dockerHDDM images, discussions can be held at: https://github.com/hcp4715/dockerHDDM/discussions.


Conflict of Interest
The authors declare no competing financial interests.


Author Contributions
H. C-P., H.G., L.Z., and R-Y.Z. conceived and designed the study. W.P. & H. C-P. implemented and have been maintaining the dockerHDDM Docker image. H. C-P., H.G., L.Z., and R-Y.Z. made the first draft of the manuscript. W.P., H. C-P., and R-Y.Z. re-organized the draft since version 7 of the preprint. All authors edited the manuscript.


Appendix


Bayesian hypothesis testing with savage-dickey method
Another method to test the experimental effect is to compute the Savage-Dickey Density Ratio to approximate the Bayes Factor (see Box 1). ArviZ provides the `plot_bf` function to visualize the differences between prior and posterior distributions and compute the Bayes Factor. Note that the Savage-Dickey ratio is related to the prior, which is weak in HDDM, resulting in a very large Bayes Factor values. We therefore urge caution in using this method, and that inference should be drawn by combining as many as possible (e.g. HDI or HDI+ROPE as mentioned in section 4.7).
In 
Figure S1
, the left panel displays the Bayes Factor favoring the alternative hypothesis ( 10 = 1.5 * 10 236 , 01 =0), indicating extremely strong evidence supporting the alternative hypothesis over the null hypothesis. This implies that the conflict condition significantly affects the drift rate. The right panel shows the Bayes Factor favoring the null hypothesis ( 10 = 0.14, 01 = 7.15), indicating moderate evidence supporting the null hypothesis over the alternative hypothesis. This suggests that there is no response bias, as evidenced by z being close to 0.5. 
Figure S1
. Bayes Factor test. This figure illustrates the prior (blue line) and posterior (orange line) density distributions for the drift rate parameter under the conflict condition. The dashed vertical line represents the reference/null value (zero), and the black dot indicates the Bayes Factor at this point. The notable difference between the probabilistic density of prior and posterior distributions at the reference value, which is used to calculate the Savage-Dickey Density Ratio and approximate the Bayes Factor, provides evidence to accept or reject the experimental effect.
 










Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package




W.-Y
Ahn






N
Haines






L
Zhang




10.1162/cpsy_a_00002








Computational Psychiatry




1


0
















Comparing eight parameter estimation methods for the ratcliff diffusion model using free software




R
W
Alexandrowicz






B
Gula




10.3389/fpsyg.2020.484737








Frontiers in Psychology




11


484737














Bayesian inference with stan: A tutorial on adding custom distributions




J
Annis






B
J
Miller






T
J
Palmeri








Behavior Research Methods




49


3


















10.3758/s13428-016-0746-9














A how-to-model guide for neuroscience




G
Blohm






K
P
Kording






P
R
Schrater




10.1523/ENEURO.0352-19.2019


ENEURO.352-19.2019








7












An expert guide to planning experimental tasks for evidence accumulation modelling




R
J
Boag






R
Innes






N
Stevenson






G
Bahg






J
R
Busemeyer






G
E
Cox






C
Donkin






M
Frank






G
Hawkins






A
Heathcote






C
Hedge






V
Lerche






S
Lilburn






G
D
Logan






D
Matzke






S
Miletic






A
.
Osth






T
Palmeri






P
B
Sederberg






B
Orstmann




10.31234/osf.io/snqgp






















U
Boehm






J
Annis






M
J
Frank






G
E
Hawkins






A
Heathcote






D
Kellen






A
M
Krypotos






V
Lerche






G
D
Logan






T
J
Palmeri






D
Van Ravenzwaaij






M
Servant






H
Singmann














Estimating across-trial variability parameters of the Diffusion Decision Model: Expert advice and recommendations




J
J
Starns






A
Voss






T
V
Wiecki






D
Matzke






E
J
Wagenmakers








Journal of Mathematical Psychology




87


















10.1016/j.jmp.2018.09.004














Inclusion Bayes factors for mixed hierarchical diffusion decision models




U
Boehm






N
J
Evans






Q
F
Gronau






D
Matzke






E.-J
Wagenmakers






A
J
Heathcote




10.1037/met0000582


















Flexible and efficient simulationbased inference for models of decision-making. eLife, 11, e77220




J
Boelts






J.-M
Lueckmann






R
Gao






J
H
Macke




10.7554/eLife.77220


















The Oxford handbook of computational and mathematical psychology




J
R
Busemeyer








Oxford University Press














J
R
Busemeyer






A
Diederich




Cognitive modeling




SAGE Publications, Inc








1st edition








Bambi: A simple interface for fitting Bayesian linear models in python




T
Capretto






C
Piho






R
Kumar






J
Westfall






T
Yarkoni






O
A
Martin




















10.48550/ARXIV.2012.10754














Subthalamic nucleus stimulation reverses mediofrontal influence over decision threshold




J
F
Cavanagh






T
V
Wiecki






M
X
Cohen






C
M
Figueroa






J
Samanta






S
J
Sherman






M
J
Frank








Nature Neuroscience




14


11


















10.1038/nn.2925














Laminar differences in decision-related neural activity in dorsal premotor cortex




C
Chandrasekaran






D
Peixoto






W
T
Newsome






K
V
Shenoy




10.1038/s41467-017-00715-0








Nature Communications




8


1


614














Decomposing preferences into predispositions and evaluations




N
Desai






I
Krajbich








Journal of Experimental Psychology-General




151


8


















10.1037/xge0001162














Obtaining evidence for No effect




Z
Dienes




10.1525/collabra.28202








Collabra: Psychology




7


1














Response times and decision-making




C
Donkin






S
D
Brown




10.1002/9781119170174.epcn509








Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience


J. T. Wixted




Wiley










1st ed.








The HDI + ROPE decision rule is logically incoherent but we can fix it




A
Etz






A
F
Chá Vez De La Peña






L
Baroja






K
Medriano






J
Vandekerckhove




10.1037/met0000660








Psychological Methods
















Introduction to Bayesian inference for psychology




A
Etz






J
Vandekerckhove




















10.3758/s13423-017-1262-3








Psychonomic Bulletin & Review




25


1














Evidence accumulation models: Current limitations and future directions. The Quantitative Methods for




N
J
Evans






E.-J
Wagenmakers








Psychology




16


2


















10.20982/tqmp.16.2.p073














Computational modeling of cognition and behavior




S
Farrell






S
Lewandowsky




10.1017/CBO9781316272503








Cambridge University Press












Running title: dockerHDDM for Bayesian HDDM Modeling












Beyond drift diffusion models: Fitting a broad class of decision and reinforcement learning models with HDDM




A
Fengler






K
Bera






M
L
Pedersen






M
J
Frank




10.1162/jocn_a_01902








Journal of Cognitive Neuroscience


















Likelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience. eLife, 10, e65074




A
Fengler






L
N
Govindarajan






T
Chen






M
J
Frank




10.7554/eLife.65074


















Sequential sampling models in cognitive neuroscience: Advantages, applications, and extensions




B
U
Forstmann






R
Ratcliff






E.-J
Wagenmakers




10.1146/annurev-psych-122414-033645








Annual Review of Psychology




67


1
















fMRI and EEG predictors of dynamic decision parameters during human reinforcement learning




M
J
Frank






C
Gagne






E
Nyhus






S
Masters






T
V
Wiecki






J
F
Cavanagh






D
Badre








Journal of Neuroscience




35


2


















10.1523/JNEUROSCI.2036-14.2015














Understanding predictive information criteria for Bayesian models




A
Gelman






J
Hwang






A
Vehtari








Statistics and Computing




24


6


















10.1007/s11222-013-9416-2














Inference from iterative simulation using multiple sequences




A
Gelman






D
B
Rubin








Statistical Science




7


4




















A
Gelman






A
Vehtari






D
Simpson






C
C
Margossian






B
Carpenter






Y
Yao






L
Kennedy






J
Gabry






P.-C
Bürkner






M
Modrá K




arXiv:2011.01808












Bayesian Workflow.








Improving neurocognitive testing using computational psychiatry-A systematic review for ADHD




N
R
Ging-Jehli






R
Ratcliff






L
E
Arnold




10.1037/bul0000319








Psychological Bulletin




147


2
















Low and variable correlation between reaction time costs and accuracy costs explained by accumulation models: Meta-analysis and simulations




C
Hedge






G
Powell






A
Bompas






S
Vivian-Griffiths






P
Sumner




10.1037/bul0000164








Psychological Bulletin




144


11
















The seven-parameter diffusion model: An implementation in stan for Bayesian analyses




F
Henrich






R
Hartmann






V
Pratz






A
Voss






K
C
Klauer




10.3758/s13428-023-02179-1


















Running title: dockerHDDM for Bayesian HDDM Modeling












Distinct mechanisms mediate speed-accuracy adjustments in cortico-subthalamic networks. eLife, 6




D
M
Herz






H
Tan






J.-S
Brittain






P
Fischer






B
Cheeran






A
L
Green






J
Fitzgerald






T
Z
Aziz






K
Ashkan






S
Little






T
Foltynie






P
Limousin






L
Zrinzo






R
Bogacz






P
Brown




10.7554/eLife.21481


















xarray: N-D labeled arrays and datasets in python




S
Hoyer






J
Hamman




10.5334/jors.148








Journal of Open Research Software




5


1














Good me bad me: Prioritization of the goodself during perceptual decision-making




C.-P
Hu






Y
Lan






C
N
Macrae






J
Sui




10.1525/collabra.301








Collabra: Psychology




6


1


20














Bayes rules! An introduction to applied Bayesian modeling




A
A
Johnson






M
Q
Ott






M
Dogucu




















Advancing research on cognitive processes in social and personality psychology: A hierarchical drift diffusion model primer




D
J
Johnson






C
J
Hopwood






J
Cesario






T
J
Pleskac








Social Psychological and Personality Science




8


4


















10.1177/1948550617703174














Bayes factors




R
E
Kass






A
E
Raftery




10.1080/01621459.1995.10476572








Journal of the American Statistical Association




90


430
















Bayesian model selection in the M-open setting-Approximate posterior inference and subsampling for efficient large-scale leave-one-out cross-validation via the difference estimator




R
Kelter




10.1016/j.jmp.2020.102474








Journal of Mathematical Psychology




100














fbst: An R package for the full Bayesian significance test for testing a sharp null hypothesis against its alternative via the e value




R
Kelter




10.3758/s13428-021-01613-6








Behavior Research Methods




54


3
















How to choose between different Bayesian posterior indices for hypothesis testing in practice




R
Kelter








Multivariate Behavioral Research




58


1


















10.1080/00273171.2021.1967716














Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan




J
K
Kruschke








Academic Press












Rejecting or accepting parameter values in bayesian estimation




J
K
Kruschke








Advances in Methods and Practices in Psychological Science




1


2


















10.1177/2515245918771304














Bayesian analysis reporting guidelines




J
K
Kruschke




10.1038/s41562-021-01177-7








Nature Human Behaviour




5


10








Article 10








ArviZ a unified library for exploratory analysis of Bayesian models in python




R
Kumar






C
Carroll






A
Hartikainen






O
A
Martí N




10.21105/joss.01143








Journal of Open Source Software




4


33


1143














Testosterone eliminates strategic prosocial behavior through impacting choice consistency in healthy males




H
H
Kutlikova






L
Zhang






C
Eisenegger






J
Van Honk






C
Lamm




10.1038/s41386-023-01570-y








Article 10






48












FlexDDM: A flexible decisiondiffusion python package for the behavioral sciences




K
Lafollette






J
Fan






A
Puccio






H
A
Demaree










Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






46












A student's guide to Bayesian statistics




B
Lambert








SAGE
















Bayesian cognitive modeling: A practical course




M
D
Lee






E.-J
Wagenmakers




10.1017/CBO9781139087759








Cambridge university press






1st ed.








Model complexity in diffusion modeling: Benefits of making the model more parsimonious




V
Lerche






A
Voss




10.3389/fpsyg.2016.01324








Frontiers in Psychology
















Retest reliability of the parameters of the ratcliff diffusion model




V
Lerche






A
Voss




10.1007/s00426-016-0770-5








Psychological Research




81


3
















A multiverse assessment of the reliability of the self matching task as a measurement of the self-prioritization effect




Z
Liu






M
Hu






Y.-R
Zheng






J
Sui






H
Chuan-Peng




10.31234/osf.io/g6uap




















D
Lunn






C
Jackson






N
Best






A
Thomas






D
Spiegelhalter




10.1201/b13613




The BUGS Book: A Practical Introduction to Bayesian Analysis










0 ed.








Indices of effect existence and significance in the Bayesian framework




D
Makowski






M
S
Ben-Shachar






S
H A
Chen






D
Lüdecke




10.3389/fpsyg.2019.02767








Frontiers in Psychology




10














Bayesian analysis with python: A practical guide to probabilistic modeling




O
Martin






C
Fonnesbeck






T
Wiecki












Third edition. Packt








Psychological interpretation of the ex-gaussian and shifted wald parameters: A diffusion model analysis




D
Matzke






E.-J
Wagenmakers




10.3758/PBR.16.5.798








Psychonomic Bulletin & Review




16


5


















R
Mcelreath




Statistical rethinking: A Bayesian course with examples in R and Stan




CRC Press








2nd ed.










10.1201/9780429029608/statistical-rethinking-richard-mcelreath














Objective Bayesian precise hypothesis testing




J
A
Mills












University of Cincinnati
















M
L
Pedersen






D
Alnaes






D
Van Der Meer






S
Fernandez-Cabello






P
Berthet






A
Dahl






R
Kjelkenes






E
Schwarz






W
K
Thompson






D
M
Barch






O
A
Andreassen














Computational modeling of the N-Back task in the ABCD study: Associations of drift diffusion model parameters to polygenic scores of mental disorders and cardiometabolic diseases




L
T
Westlye




10.1016/j.bpsc.2022.03.012








Biological Psychiatry: Cognitive Neuroscience and Neuroimaging
















Simultaneous hierarchical Bayesian parameter estimation for reinforcement learning and drift diffusion models: A tutorial and links to neural data




M
L
Pedersen






M
J
Frank








Computational Brain & Behavior




3


4


















10.1007/s42113-020-00084-w














The drift diffusion model as the choice rule in reinforcement learning




M
L
Pedersen






M
J
Frank






G
Biele








Psychonomic Bulletin & Review




24


4


















10.3758/s13423-016-1199-y














A Reproducible Data Analysis Workflow. Quantitative and Computational Methods in Behavioral Sciences




A
Peikert






A
M
Brandmaier




10.5964/qcmb.3763








1












BayesFlow: Learning complex stochastic models with invertible neural networks




S
T
Radev






U
K
Mertens






A
Voss






L
Ardizzone






U
Kothe








IEEE Transactions on Neural Running title: dockerHDDM for Bayesian HDDM Modeling Networks and Learning Systems






33
















10.1109/TNNLS.2020.3042395














The diffusion decision model: Theory and data for twochoice decision tasks




R
Ratcliff






G
Mckoon








Neural Computation




20


4


















10.1162/neco.2008.12-06-420














Diffusion Decision Model: Current Issues and History




R
Ratcliff






P
L
Smith






S
D
Brown






G
Mckoon








Trends in Cognitive Sciences




20


4


















10.1016/j.tics.2016.01.007














Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability




R
Ratcliff






F
Tuerlinckx




10.3758/bf03196302








Psychonomic Bulletin & Review




9


3
















The Metropolis-Hastings Algorithm




C
P
Robert






G
Casella




10.1007/978-1-4757-4145-2_7








Monte Carlo Statistical Methods


C. P. Robert & G. Casella




Springer
















Evaluating the model fit of diffusion models with the root mean square error of approximation




A.-L
Schubert






D
Hagemann






A
Voss






K
Bergmann




10.1016/j.jmp.2016.08.004








Journal of Mathematical Psychology




77
















Decision making and sequential sampling from memory




M
N
Shadlen






D
Shohamy




10.1016/j.neuron.2016.04.036








Neuron




90


5




















F
Sheng






A
Ramakrishnan






D
Seok






W
J
Zhao






S
Thelaus






P
Cen






M
L
Platt


















Decomposing loss aversion from gaze allocation and pupil dilation






Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America




117














10.1073/pnas.1919670117














A flexible framework for simulating and fitting generalized drift-diffusion models. eLife




M
Shinn






N
H
Lam






J
D
Murray




10.7554/elife.56938








9


















H
Singmann






S
Brown






M
Gretton






A
Heathcote






A
Voss






J
Voss






A
Terry












rtdists: Response time distributions (Version 0.11-5) [Computer software










10.32614/CRAN.package.rtdists














Bayesian Measures of Model Complexity and Fit




D
J
Spiegelhalter






N
G
Best






B
P
Carlin






A
Van Der Linde




10.1111/1467-9868.00353








Journal of the Royal Statistical Society Series B: Statistical Methodology




64


4
















Running title: dockerHDDM for Bayesian HDDM Modeling












EMC2: An R package for cognitive models of choice




N
Stevenson






M
C
Donzallaz






R
Innes






B
Forstmann






D
Matzke






A
Heathcote




10.31234/osf.io/2e4dq


















Systematic Parameter Reviews in Cognitive Modeling: Towards a Robust and Cumulative Characterization of Psychological Processes in the Diffusion Decision Model




N.-H
Tran






L
Van Maanen






A
Heathcote






D
Matzke




10.3389/fpsyg.2020.608287








Frontiers in Psychology




11














Bayesian statistics and modelling




R
Van De Schoot






S
Depaoli






R
King






B
Kramer






K
Mä Rtens






M
G
Tadesse






M
Vannucci






A
Gelman






D
Veen






J
Willemsen




10.1038/s43586-021-00017-2








Nature Reviews Methods Primers




1


1




















R
Van De Schoot






S
D
Winter






O
Ryan






M
Zondervan-Zwijnenburg






S
Depaoli


















A systematic review of bayesian articles in psychology: The last 25 years


10.1037/met0000100








Psychological Methods




22


2














Practical Bayesian model evaluation using leaveone-out cross-validation and WAIC




A
Vehtari






A
Gelman






J
Gabry








Statistics and Computing




27


5


















10.1007/s11222-016-9696-4














Ranknormalization, folding, and localization: An improved R 2 for assessing convergence of MCMC (with discussion)




A
Vehtari






A
Gelman






D
Simpson






B
Carpenter






P.-C
Bürkner




10.1214/20-BA1221








Bayesian Analysis




2


16














Diffusion models in experimental psychology




A
Voss






M
Nagler






V
Lerche




10.1027/1618-3169/a000218








Experimental Psychology




60


6
















Fast-dm: A free program for efficient diffusion model analysis




A
Voss






J
Voss




10.3758/BF03192967








Behavior Research Methods




39


4
















Extending JAGS: A tutorial on adding custom distributions to JAGS (with a diffusion model example)




D
Wabersich






J
Vandekerckhove




10.3758/s13428-013-0369-3








Behavior Research Methods




46


1
















Bayesian hypothesis testing for psychologists: A tutorial on the savage-dickey method




E.-J
Wagenmakers






T
Lodewyckx






H
Kuriyal






R
Grasman




10.1016/j.cogpsych.2009.12.001








Cognitive Psychology




60


3
















Running title: dockerHDDM for Bayesian HDDM Modeling












An EZ-diffusion model for response time and accuracy




E.-J
Wagenmakers






H
L J
Van Der Maas






R
P P P
Grasman




10.3758/BF03194023








Psychonomic Bulletin & Review




14


1
















Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory




S
Watanabe








Journal of Machine Learning Research




12


11














Leveraging containers for reproducible psychological research




K
Wiebels






D
Moreau




10.1177/25152459211017853








Advances in Methods and Practices in Psychological Science






4












HDDM: Hierarchical bayesian estimation of the drift-diffusion model in python




T
V
Wiecki






I
Sofer






M
J
Frank




10.3389/fninf.2013.00014








Frontiers in Neuroinformatics
















Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547




R
C
Wilson






A
G
Collins




10.7554/eLife.49547


















Using reinforcement learning models in social neuroscience: Frameworks, pitfalls and suggestions of best practices




L
Zhang






L
Lengersdorff






N
Mikus






J
Glascher






C
Lamm








Social Cognitive and Affective Neuroscience




15


6


















10.1093/scan/nsaa089














Running title: dockerHDDM for Bayesian HDDM Modeling













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]