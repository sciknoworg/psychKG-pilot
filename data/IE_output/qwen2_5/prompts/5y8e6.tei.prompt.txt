You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Information is typically assumed to be valuable for decision making, and in most cases it is. Information helps resolve uncertainty concerning the likelihood and value of outcomes and of the likely behavior and strategies of others. 
Stigler's (1961)
 seminal analysis of the economics of information spawned a considerable literature exploring the potential value from possessing information (e.g., 
Lewis and Sappington, 1997;
Osband, 1989;
Porter, 1995)
.
One fundamental assumption underlying almost all economic discussions of information is that more information is (weakly) better for decision making. 
1
 Information is rarely thought of as bad, in part because it is widely assumed that decision makers can ignore information that is not valuable. In most cases, using information will lead to better decisions, and in those in which it doesn't, the information will be ignored.
This "free disposal" or "reversibility" assumption may be of questionable validity. 
Camerer, Loewenstein, and Weber (1989)
 conducted experiments demonstrating that participants were not able to ignore previously received information when subsequently making a decision and ended up making worse decisions as a result, even though the information was accurate. In their experiments, one group of participants made guesses about the earnings of a series of companies based only on information in a report. A second group of participants then traded "assets" (one for each company) with underlying value equal to the average of the first group of participants' predictions for that company. When these traders were given the actual earnings 1 There are some cases in which information has been shown to hurt decision-making. For instance, 
Fischhoff (1975)
 demonstrates that knowing the outcomes of events makes people believe that those outcomes were more likely, ex ante, than they actually are. 
Goldstein and Gigerenzer (1999;
2005)
, in research on what they call the "less-is-more effect" show that, when the absence of information serves as a useful cue, people with poorer information can in some circumstances make more accurate judgments by employing a "recognition heuristic." Information might also hurt decision makers when they experience "information overload" with the arrival of too many pieces of information (see, for instance, 
Earl, 1990)
. Finally, in laboratory investment decisions 
Gneezy and Potters (1997)
 show that more frequent feedback (on the performance of risky investments) leads to more riskaverse behavior and lower expected returns. Work on "herd behavior" also demonstrates that greater information can produce worse outcomes ex post, even when the information use is rational, ex ante 
(Banerjee, 1992)
.
for the companies (in addition to the reports also received by the original guessers), their trades revealed a bias away from the guesses of the group they were trying to predict, and in the direction of the actual earnings. This phenomenon, dubbed "the curse of knowledge," indicates that individuals cannot always recover mental states in which they did not possess unhelpful information, even when such recovery would be beneficial. Participants trying to predict the guesses of other participants who did not know the actual earnings should have ignored the actual earnings when making their predictions, but did not do so. 
2
 Camerer et al. did not measure whether participants preferred to receive the actual estimates, which is important since in real economic environments the decision to acquire information is endogenously made by economic actors. Perhaps participants were aware of the negative effect of information, and hence would have been unwilling to pay for it (or even might have paid to avoid receiving it). Alternatively, they may have not recognized that it was affecting their judgments adversely and might have preferred receiving it. Therefore, while the experiment demonstrated the curse of knowledge, it did not address the question of whether participants given the choice of acquiring information would have fallen subject to the curse.
Some existing experimental evidence suggests that people choose to acquire harmful information. In a review paper, 
Camerer (1992)
 reports preliminary experiments in which he auctioned the information used in the 
Camerer et al. (1989)
 study, thus allowing subjects to state how much they valued, if at all, such harmful information. Though he conducted only two sessions with a total of 18 subjects, a majority of subjects initially stated a positive price. 3 More 2 Camerer et al. also found that market forces reduced the bias: when the "assets" were traded in a market, participants on average were less susceptible to the curse of knowledge than when they simply tried to predict what other participants had guessed. 3 However, with repetition, the tendency to value the harmful information dissipated: After the fourth period, very few subjects gave positive prices (and some even gave negative prices, indicating they wanted to be paid for acquiring the information). recently, 
Charness and Gneezy (2003)
 show that a majority of subjects choose to receive more frequent information about the performance of a risky asset (and to have the ability to make more frequent changes to their portfolio), even though previous research indicates that such information and discretion lead to lower returns on average 
(Gneezy and Potters, 1997)
.
However, Charness and Gneezy do not explore whether these particular participants performed better or worse as a result of the more frequent information. 
4
 While the above studies suggest that people may value harmful information, neither provides a conclusive demonstration. In this paper, we report experiments that explore the implications of the potentially harmful effects of more information and people's willingness to acquire such information. Like Camerer et al., we document situations in which information produces a "curse of knowledge" in predicting others' performance. We then explore whether people place value on such information, and whether they are subsequently harmed as a result.
Finally, we also explore whether such a bias persists with repetition.
We find that harmful information is valued positively by many of our subjects. In a first experiment, we find that a majority of subjects choose to "hire" an informed agent instead of an uninformed one, even though the latter actually make more money due to the curse of knowledge. In a second experiment, we find that almost a third of subjects are willing to pay for information that causes them to make worse predictions and earn less money. Finally, we find some evidence of learning, particularly when the actual performance to be predicted is revealed.
Our results are consistent with the notion that people's naïve theories about their use of information parallel economic theories in assuming that more information is good (or at least not bad). While this rule of thumb will most often lead to better decision making, our studies show that this is not always the case. We conclude the paper by exploring possible implications for economically consequential situations.


Experiment 1: Hiring "cursed" agents


Experimental Design
One large session was conducted with 166 students from Carnegie Mellon University and the University of Pittsburgh. Participants showed up to a large auditorium and were told that they would be paid based on their decisions in the experiment. 5 Upon arriving, they were seated and received written instructions, which differed by role. Roles were assigned by randomly distributing instructions.
Each participant was in one of four roles: Solver, Informed Predictor, Uninformed Predictor, or Chooser. Each Solver was given one puzzle to solve. The puzzle was a simple analytical problem in which participants needed to generate an insight to figure out the solution.
We used two different puzzles, labeled "boxes" and "chains". 6 Roughly half the participants in each role received each of the puzzles (see 
Table 1
).
Solvers (n = 14) were paid based on how quickly they solved the puzzle. Specifically, they were told that if they solved the puzzle immediately they would receive $6. The amount they received went down by one cent for each second they spent solving the puzzle. If they did not solve the puzzle after 10 minutes (600 seconds) then the payment was equal to zero.
Both types of Predictors (n = 99) were told that they would be paired with one randomly selected Solver. About half of the predictors were told the solution to the puzzle (Informed Predictors, n = 50) and half were not (Uninformed Predictors, n = 49). Predictors' task was to predict how long the Solver would take to solve the puzzle. Predictors were shown the puzzle.
They were rewarded for predicting longer times (i.e., waiting longer), but were penalized for exceeding the actual time it took the Solver to solve the puzzle. Specifically, Predictors received one cent for every second they predicted the Solver would take, but their payment was zero if their prediction was longer than the actual time it took the Solver. Thus, Predictors maximized their payoffs when they predicted exactly how long it took to solve the puzzle, but not longer.
We designed the incentives faced by Predictors to roughly mimic those faced by someone trying to decide how long to wait to introduce a new invention to the market, where there is a threat that a competitor may also come up with the invention, and if so may introduce it to the market first. In such cases, it often pays to be the first mover, but delaying introduction of the invention to the market allows one to refine its design. If people in this situation suffer from the curse of knowledge, then one would expect those who are aware of a key insight to exaggerate their competitors' progress and hence to introduce their own product too early. Thus, our first prediction was that Informed Predictors would make less money than Uninformed Predictors.
Another fifty-three participants were in the role of Chooser. Each Chooser's task was to decide whether to tie their payment to that of an Informed or Uninformed Predictor. The problem faced by Choosers is similar to that of a principal in the invention problem above who must hire an agent to predict how long it will take competitors to discover the invention.
Choosers were first asked to predict the average payoffs for the two types of Predictors. Then they were told that their payment would be equal to that of one randomly chosen Predictor, but they could pick whether that Predictor would come from the set of the Informed or the Uninformed. Our second, and main, hypothesis is that Choosers will misjudge the benefit of information and will guess that Informed Predictors will make more money and will select an Informed Predictor as their "agent." 
Table 1
 presents the number of participants for each role and for each puzzle.


Results
The cumulative frequencies of completion times by Solvers and guesses by the two kinds of Predictors are reported in 
Figure 1
. 
7
 Six of fourteen Solvers (43 percent) solved the puzzle, while the remaining eight Solvers did not solve the puzzle in 10 minutes. The average time spent for all Solvers (including the ones who did not finish) was 422 seconds (SD = 235s), and did not significantly differ between the boxes (mean = 441s, SD = 254s) and chains (mean = 398s, SD = 227s) puzzles (t12 = 0.33).
Predictors, on average, predicted that Solvers would require 248 seconds (SD = 152s) to solve the puzzle. Both Informed and Uninformed Predictors underestimated Solver solution times but Informed Predictors did worse, predicting that Solvers would solve the puzzle more quickly (mean = 216s, SD = 149s) than did Uninformed Predictors (mean = 281s, SD = 150s).
These means differ significantly (p = 0.03, t97 = 2.15), and the distributions also differ significantly in a Kolmogorov-Smirnov test (p = 0.03, D50,49 = 0.28). As a result, Informed Predictors earned less money on average (mean = $1.46, SD = $0.84) than Uninformed Predictors (mean = $1.77, SD = $0.82), and this difference is significant (p = 0.07, t97 = 1.84). 8
The above results demonstrate the curse of knowledge. Informed Predictors did worse in predicting the performance of Solvers than did Uninformed Predictors, and made less money as a result. Unbiased Choosers should therefore anticipate that Informed Predictors are likely to earn less money, and should select Uninformed Predictors as their "agents." This is not the case. Choosers tended to believe that Informed Predictors would earn more money than Uninformed Predictors. Choosers' average estimates of earnings for Informed Predictors were $3.43 (SD = 1.93) and for Uninformed Predictors they were $2.77 (SD = 1.51).
The average within-subject difference between these estimates ($0.65) is significantly different from zero (t52 = 2.26, p = 0.03). 9
Choosers' expectations that Informed Predictors would earn more money are reflected in how they chose to have their earnings determined. The majority of Choosers (33 of 53, or 62 percent) chose to tie their payoffs to Informed Predictors. 10 This proportion differs marginally significantly from 50 percent (p = 0.1 in a two-tailed Binomial test using the normal approximation with adjustment for continuity (z = 1.65, see 
Siegel and Castellan, 1988)
).
Moreover, Choosers who chose Informed Predictors earned less, on average, than Choosers who chose Uninformed Predictors ($1.51 vs. $2.63, t51 = 2.51, p = 0.02). 11
The results of Experiment 1 provide clear support for our hypotheses. Participants were subject to the curse of knowledge: Informed Predictors did significantly worse than Uninformed ones in predicting the amount of time it would take solvers to complete the puzzle. More 9 Of the 53 Choosers in the experiment, 28 (53 percent) gave an earnings prediction that was higher for the Informed Predictor than the Uninformed Predictor, 17 (32 percent) guessed that Uniformed Predictors would have greater earnings, and 8 (15 percent) guessed equal earnings for both types of predictors. 10 Of the 53 Choosers, only two chose agents inconsistently with their estimates (one Chooser gave a higher estimate of earnings for Informed Predictors but chose an Uninformed Predictor, and another agent did the opposite). Thus, while there was no monetary incentive for providing accurate guesses (which might have created problems due to hedging), we can be reasonably confident that the stated expectations reflected beliefs for most Choosers. 11 Of course, this comparison relies on the ex post realization of pairings between Predictors, Solvers, and Choosers. A more appropriate test involves calculating the ex ante likelihood of greater earnings for those choosing Uninformed Predictors. To conduct such a test, we constructed all the possible realizations of pairings, for each puzzle and for each choice by Choosers, and then randomly sampled according to the actual choices by Choosers 
(33 vs. 22)
. In 1000 such samples, the 22 Choosers who selected Uninformed Predictors earned more, on average, than the 33 who chose Informed Predictors 744 times (vs. 256 cases in which those choosing Informed Predictors earned more). Thus, the p-value obtained across multiple possible realizations of pairings would be about 0.26. importantly, Choosers' guesses exhibited the opposite pattern -they tended to believe that Informed predictors would do better and to select Informed agents, leading to lower payoffs. 12


Experiment 2: Paying for cursed knowledge
In Experiment 2 we provide a stronger test of the phenomenon and test its robustness using a different prediction task. Specifically, we explore directly whether people are willing to pay to acquire harmful information.


Experimental Design
Participants in two sections of an introductory business class at Carnegie Mellon (n = 66) viewed three video clips. Each 20-second clip alternated between two nearly identical images, with each one appearing for about one second. In between each appearance of the images, there was a very brief flash in which the screen was completely white. The two images differed in one important aspect. For instance, one set of images is pictured in 
Figure 2
. Before reading on, try to distinguish the difference between the two images.
12 One potential source of concern (noted by a reviewer) is that our payment mechanism is not incentive-compatible for reporting the mean (or median) of Predictors' subjective distributions of Solver completion times. Instead, if each additional second waited produces a payment of r (in our experiment, r = $0.01), rational (risk-neutral) Predictors should guess the time (t∈[0,600]) such that their expected benefit of waiting an additional second ((1-F(t))r) equals the expected loss ((F'(t))rt), were F(t) refers to the cumulative distribution of t. This yields the optimal guess t * = (1-F(t)) / F'(t).
We chose this mechanism -instead of, for instance, one incentive-compatible with reporting the mean of F(t) -for several reasons. First, this payment mechanism is easy for subjects to comprehend and to implement, and it corresponds to the situation we described involving waiting to market an invention. Second, we are not as much interested in whether Predictors "get it right," i.e., guess the correct distribution under the assumption of riskneutrality (which they don't -the optimal guess for risk-neutral Predictors, for both puzzles, would have been 600 seconds, which was guessed by only 4 Uninformed and 2 Informed predictors), as in whether guesses are more or less accurate when Predictors are informed (and, even then, our primary interest is in the perceptions of Choosers). Therefore, as long as the payment mechanism does not interact with the information received by Predictors this does not pose a significant problem. (As 
Figure 1
 reveals, the distributions of responses are quite similar, except that the one for Informed Guesses is shifted to the left, corresponding to lower (and less accurate) guesses, as we predicted.) Thus, the most serious threat to the validity of our conclusions would be if somehow becoming informed induced a belief distribution that, while being more accurate, also implied that lower estimates of t * produced higher expected payoffs. While there are such belief distributions, they neither seem natural nor correspond to the actual completion time or estimate distributions in 
Figure 1
. Perhaps the best assurance that such beliefs do not explain our subjects' reports is that the Informed Predictors earned less than the Uninformed.
While most people have a hard time noticing the differences between the paired images, they are quite obvious once they are highlighted. For instance, notice that the two images in 
Figure 2
 are identical except that the one on the right has the shadow cast by the helicopter below the jeep, while the one on the left does not. 
13
 As with previous experiments on the curse of knowledge, we predicted that participants informed of the difference would find it difficult not to notice it and would overestimate the frequency with which other participants would notice it.
For each video clip (each pair of images), participants were first told that their goal was to identify the difference between the two images. Specifically, they were instructed that, "There is one difference between the pictures you will see in each clip. Look to see if you can spot the difference." Participants were also asked to predict what percentage of their classmates who did not know the difference would be able to spot it. Participants were paid for the accuracy of their predictions. If a participant's guess was within 2 percentage points of the actual percentage, then he or she would receive $10. If the guess was 3, 4, or 5 percentage points away, the payment was $5. Guesses off by more than 5 percent earned nothing. Participants repeated this task three times (once for each video clip) and their earnings were summed across all three video clips.
Participants were not given any feedback until after the experiment.
Across the three clips, participants experienced each of three information conditions. In the Uninformed condition, participants were not informed of the difference between the two pictures. They simply watched the video clip and then made a prediction. In the Informed condition, participants' written instructions informed them, in bold type, of the difference. 14 In the Choice condition, participants were given the option of finding out what differed between the 13 These video clips have been previously used to demonstrate "change blindness" -the difficulty most people have noticing changes or inconsistencies in visual perception, even when these are as substantial as in 
Figure 2
 
(Rensink, O'Regan, and Clark, 1997;
Simons and Levin, 1997
). 14 For instance, for the clip shown in 
Figure 2
, participants in the Informed condition were told, "CLUE: The helicopter's shadow disappears." Subjects in the Informed condition were not asked to spot the difference.
images. Each participant received an envelope that revealed the difference inside; however, participants were told that by opening the envelope they would sacrifice a $0.50 bonus.
Each participant experienced all three information conditions. 
Table 2
 presents the three sequences in which participants experienced the conditions and the corresponding sample sizes.
To minimize potential curiosity effects, participants were told that they would be shown all three clips again and informed about the differences at the conclusion of the experiment.


Results
When participants were uninformed about the change, 20 percent correctly identified the change, and this did not differ by video clip (F2,63 < 1, ns). Our experiments, therefore, replicated the finding that the changes are difficult to detect.
As the results in 
Table 3
 indicate, uninformed participants on average guessed that 30 percent (SD = 26 percent) of their uninformed peers would spot the change; they earned an average of $1.21 (SD = $2.49). When participants were informed about the difference, they guessed that 58 percent (SD = 33 percent) of their uninformed peers would spot the difference, and earned an average of $0.45 (SD = $1.69). The average within-subject difference between guesses in the Informed and Uninformed conditions is significantly different from zero for both guesses (t65 = 6.28, p < 0.001) and payoffs (t65 = 2.19, p < 0.05). These results are consistent with the curse of knowledge: Participants who are told the difference between the two pictures are worse at predicting the performance of participants who do not know the difference. 
15
 The important question for our main hypothesis deals with what participants will do if given the choice of being informed or uninformed. That is, will they choose to pay to acquire harmful information? This is exactly the decision faced by participants in the Choice condition.
When given the choice of whether to learn the difference between the two pictures before seeing the clip and making their guess, 19 of 66 participants (29 percent) chose to open the envelope and become informed. These participants all sacrificed $0.50 for doing so.
The behavior and earnings of participants in the Choice condition also reflects the curse of knowledge. The 47 participants who chose not to open their envelopes guessed, on average, that 35 percent (SD = 29 percent) of their uninformed peers would see the difference, while the 19 participants who chose to pay $0.50 to become more informed guessed, on average, that 55 percent (SD = 26 percent) of their uninformed peers would see the difference. This difference is significant (t64 = 2.71, p < 0.01). As a result, those who chose to remain uninformed earned an average of $1.49 (SD = $3.10), whereas none of those who opened their envelopes earned anything. This difference is also significant (t64 = 2.08, p < 0.05).
Overall, the results support our main hypotheses. Participants are clearly better off if they are not informed (as in Experiment 1), but a significant proportion choose to pay a $0.50 fee to acquire information that harms their performance.
However, it is worth noting that there are sequence effects, particularly in the Choice condition. Perhaps most importantly, in Sequence 1, when Choice is last, subjects made slightly better predictions when they opened the envelope (mean guess: 41.8 percent) than when they did not (49.8 percent). 
16
 This suggests the possibility that learning may occur with experience, in mean guess by uninformed participants who did not figure out the difference was 21.9 percent, which is very close to the actual percentage (20 percent). 
16
 The other important effect is that participants in Sequence 3, for whom the Choice condition is first, were less likely to open the envelope (4.8 percent) than Choice participants in other sequences (40.0 percent). There are at that subjects may learn to adjust for biased information, and may eliminate or mitigate the bias. Experiment 3 tests this directly by exploring the roles of experience and feedback.


Experiment 3: Learning not to pay for the curse


Experimental Design
In this experiment, we combined elements from the first two experiments and added repetition to explore the possibility of learning. We used the task and video stimuli from Experiment 2 and, as in Experiment 1, had subjects hire "agents" to make predictions.
In the experiment, subjects in two sections of a large introductory business class at Carnegie Mellon (n = 59) were told about the task posed to subjects in Experiment 2: guessing what percentage of people would spot the change between two pictures. To help them understand the task, they were shown the first of the three clips from Experiment 2. They were not told what changed or how many people saw the change. Instead, they were told that there had been two types of guessers in Experiment 2: "Group I" and "Group U." Subjects were told that Group U members did not know what changed before they watched the clip and made their guess, and that Group I members were told what changed before doing so.
Subjects then performed six rounds of a task in which they selected an "agent" from Experiment 2 and received the same payoff as that subject. They made the choice by selecting from two stacks of sheets (Group I or Group U) containing guesses (and monetary earnings) of subjects in Experiment 2. There was a $0.10 fee for choosing from Group I. After selecting, least three possible explanations. First, participants might not open the envelope because they want to see if they can spot the difference on their own, a tendency which acts against our hypothesized effect. Second, participants may be overconfident in their ability to detect the change, which also works against our hypothesis. Finally, participants without prior experience with this task might not believe that knowing the difference between the images will increase their earnings beyond the $0.50 cost, but, after one experience, change this belief. This suggests that the effect could worsen with experience. subjects were shown the guess made by the randomly selected subject and their payoff from that choice. At the end of the experiment, two rounds were randomly selected to determine payoffs.
There was a slight difference between rounds 1 and 2 and rounds 3 through 6. After making choices in rounds 3 through 6, participants received one additional piece of information: the true percentage seeing the change. This information was provided to see if stronger feedback (about the difficulty of the task) could correct the bias if the initial feedback did not. 
Table 4
 presents, across the six rounds, the percentage of subjects who selected an agent from Group I. As the results indicate, subjects chose to draw their earnings from the informed group about 29 percent of the time. There is some variation between rounds and a downward trend after round 3, suggesting that some subjects learn to avoid the bias once they receive feedback on the actual percentage noticing the change (recall this was about 20 percent).


Results
Because uninformed guesses were more accurate, those who chose Group I on average earned less than those who chose Group U. The mean earnings for choices from Group U were $1.17, whereas mean earnings from Group I were $0.40, less the $.10 cost of choosing I, or $0.30. Therefore, as in Experiment 2, roughly a third of the time people opted to pay for information that proved harmful. 17 
Table 5
 tests the hypothesis that the bias decreases with experience, using logistic regressions with a subject's choice from Group I (1) or Group U (0) as the dependent variable.
The regressors include round, the difference in earnings (for that subject) between previous Group I and Group U choices, and the number of times that subject previously chose Group I.
The regressions allow for clustering (by subject) of standard errors. 
18
 The first regression (including all rounds) reveals no significant learning effect. However, the remaining regressions (using only rounds 3 through 6) find that the frequency of Group I choices decreases significantly across rounds. The final two regressions reveal that some of this learning results from sampling from both types of agents and receiving different payoffs from such choices (recall that choosing from Group U yielded, on average, payoffs almost four times as large). 
19
 Interestingly, the frequency of Group I choices is highly significant, indicating that those subjects who have chosen Group I frequently in the past, continue to do so.
The results of this experiment reveal that learning can diminish the frequency of biased (Group I) choices, especially if individuals receive feedback on the difficulty of the task and are able to sample the efficacy of both informed and unformed strategies. However, it is worth noting that, despite the richer feedback, experience, and learning present in the last four rounds, almost a fifth of the subjects continue to choose from Group I in the final round.


Discussion
Taken together, our experiments demonstrate that participants exhibit the curse of knowledge in predicting the performance of others. In the first two experiments, participants who were given the solution to a problem tended to make biased predictions about how easy it would be for others to obtain the solution, leading to lower performance and earnings.
We also demonstrate that a substantial percentage of subjects in all three experiments seem to think that choosing information, or paying a small sum for it, will help them when it actually does not. 
20
 We observe this most strongly in Experiment 1, in which a majority of participants opt to "hire" an informed agent and end up making less money as a result. The belief that information is beneficial is also reflected in estimates of the earnings of the two kinds of agents. In Experiments 2 and 3, we find that roughly 30 percent of subjects choose to acquire such information, even when they have to pay for it. The effect is diminished a little by learning but not eliminated in six periods. 
21
 Of course, we demonstrated this bias using decision tasks with specific characteristics. In our experiments, the problem to be solved required obtaining an insight or noticing something hard to see. Prior research has shown that outcome feedback (in this case the solution to the problems) biases people's predictions of others for insight problems, but not for all other types of problems 
(Hoch and Loewenstein, 1989)
. For example, being told the answers to trivia problems often helps one to predict whether others will be able to answer those problems correctly because, if the answer is surprising, one will recognize that few people will get it right. The types of problems we used, however, have exactly the opposite property: When subjects see the solution to an insight problem, it seems obvious, obscuring the difficulty of coming up with the solution in the first place. Therefore, one should be cautious of generalizing our results to too wide a domain of problems and tasks. Our results also do not address the question of whether more information will generally be better when decision makers are not trying to predict the performance of others. Our main result should be viewed more as a demonstration of the combined facts that accurate information can, in situations with some key characteristics, be harmful and that a significant percentage of people are not aware of when this is true. 
22
 There are many consequential economic and organizational situations with the key characteristics of our experiments. For instance, Experiment 1 serves as a metaphor for a situation in which a firm is trying to figure out how quickly or easily a competitor will develop a product or innovation requiring a key insight. Our results suggest that, in such situations, knowing more about the insight may lead to worse predictions, but that such predictions may be mistakenly delegated to those who know more.
A similar problem surrounds the question of who should design a product or write its instruction manuals. Product design is routinely imperfect, as suggested by the fact that half of high technology gadgets that are returned as malfunctioning are, in fact, in perfect working order 
(Ouden, 2006)
. The problem is that the consumer could not figure out how to use the product.
Our results suggest that the people who know the most about a product or topic may overestimate the ease with which others will be able to understand its use. 
23
 It has been shown, for example, that experts on the use of a telephone headset were worse than people with intermediate levels of experience when it came to predicting how long it would take novices to learn the basics of using the headset 
(Hinds, 1999)
. Therefore, the most informed or most knowledgeable 
22
 Two suggestions by anonymous reviewers for future work are worth noting here. First, our experiments involve asking people to predict how long it will take someone to solve a puzzle or to predict how many people will spot a difference, both positive results. It is possible that a negative framing, predicting how many people will fail to spot the difference or solve the puzzle (or will give up trying to do so), might produce different results. More precisely, drawing attention to the likelihood of failure might make people aware of the potential harmful effects of information or might simply lead them to choose the negative (no information) option. Second, informal debriefing of subjects might be a valuable tool for determining the precise extent to which they are aware of the harmful effects of information. 
23
 In a clever demonstration of this problem, 
Newton (1990
, as cited in Ross & Ward, 1996
 had people try to get others to identify a highly recognizable tune by tapping out its rhythm. Those doing the tapping thought their clues would be transparently obvious but those doing the guessing rarely found it easy to correctly identify the tune. individuals may be worse at writing such documentation than someone who is less informed, but there may be a common bias to assume that those with more information will be better.
Both of the above examples nicely correspond to our experimental results. However, it is not hard to think of other examples, which highlights the potential importance and broad applicability of our result. 
24
 


Final comments
Stigler's seminal paper on the economics of information initiated an extraordinarily productive line of research on the "new economics of information," which has encompassed phenomena such as signaling, adverse selection, asymmetric information in bargaining and "herd behavior." We hope that the work presented here will become part of a "new new" economics of information that draws on psychological research to revise some of the strong, unrealistic assumptions that economists typically make about the ways in which people use information.


Some of this new research calls into question conventional assumptions about information
processing, such as the idea that information can be freely disposed of or that people update probabilities in a fashion consistent with Bayes' rule. For example, people exhibit "hindsight bias" 
(Fischhoff, 1975)
 that is, in a way, a within-person version of the curse of knowledge: people overestimate their own ability to have predicted events which they know have taken place. People also have a difficult time reverting back to their original beliefs after evidence on the basis of which they updated those beliefs is discredited (e.g., 
Ross,
Lepper and Hubbard,
24
 For instance, there is often a belief that "experts" (people with a lot of information or knowledge about a domain or concept) should perform functions such as teaching and product support. However, these experts may be the least likely to be aware of potentially difficulties in understanding complex concepts or using complicated features of a product. People responsible for marketing or selling a product may also choose to acquire as much information and expertise regarding the product and its detailed features as possible, not recognizing that most consumers might not care about much beyond a few superficial features. Another related example is when the eager parents of a child trying out a new hobby or sport learn "too much" about the activity, and end up making the activity less fun to the child, who is interested only superficially. 1975). And in some situations, they seem to underweight base-rates in forming expectations of future events (e.g., 
Bar-Hillel, 1990
).
Another line of research challenges the conventional assumption that people process information in an impartial fashion. For example, research on the self-serving bias shows that people unconsciously and without deliberate intent interpret information in a fashion that is favorable to themselves 
(Babcock and Loewenstein, 1997)
. Research on the "confirmatory bias"
shows that people behave in a "super-Bayesian" fashion, dismissing evidence that contradicts their preexisting beliefs and overweighing evidence that confirms them (e.g., 
Lord, Lepper and Ross, 1979;
Rabin & Schrag, 1999
).
Yet a third line of work focuses on the non-controversial idea that information -or, relatedly, beliefs -can constitute a source of utility apart from any influence in securing desired material outcomes. Several existing economic models incorporate utility from such information or beliefs -e.g., from anticipation of future outcomes 
(Loewenstein, 1987;
Caplin and Leahy, 2001
), beliefs about one's own self-worth (e.g., 
Koszegi, 2001;
Loewenstein, 1999)
, information about others' actions (e.g., 
Benabou and Tirole, 2004;
Rabin, 1993)
, information about probability distributions (e.g., 
Gilboa and Schmeidler, 1989
; see also 
Camerer and Weber, 1992)
, and from feelings of identification with groups 
(Akerlof & Kranton, 2000)
.
Clearly, there is more to be learned about the economics of information.   


Role
Figure 2 .
2
Sample of images used in Experiment 2


Table 2 . Number of participants by sequence of conditions
2
Boxes
Chains
Total
Solvers
8
6
14
Choosers
27
26
53
Uninformed Predictors
23
26
49
Informed Predictors
27
23
50
Total
85
81
166
Table 1. Number of participants by puzzle and condition (Experiment 1)
Clip 1 -
Clip 2 -
Clip 3 -
Number of
"Statue"
"City"
"Chopper"
participants
Sequence 1
Uninformed
Informed
Choice
25
Sequence 2
Informed
Choice
Uninformed
20
Sequence 3
Choice
Uninformed
Informed
21
Information condition
Mean
Standard
N
prediction
deviation
Uninformed
30.1 %
25.6
66
Informed
58.2 %
32.7
66
Choice
40.6 %
29.5
66
Choice (unopened) (71%)
34.6 %
29.0
47
Choice (opened) (29%)
55.4 %
25.8
19
Table 3. Predictions pooled by information condition across sequences


Table


This is important, since investment decisions by the two (endogenously determined) groups might differ.


The experiment was the first part of several tasks that the participants completed (which included another experiment and filling out questionnaires). Since this was the first task in which they participated, and since they were not told about the other tasks, it is unlikely that any of the other tasks affected performance in this one.6  The two puzzles and their solutions are available with the on-line supplementary materials accompanying this journal volume.


The complete dataset from all three experiments is available with the on-line supplementary materials accompanying this journal volume. 8 Of course, these are ex post earnings, based on the actual pairings between Predictors and Solvers. Using the ex ante expected earnings for each Predictor, based on his or her prediction and the distribution of Solver completion times for that puzzle, the average earnings are $1.49 for Informed and $1.79 for Uninformed (p = 0.07, t97 = 1.84).


Among uninformed participants, some figured out the difference on their own (13 of 66). Since they did so before making guesses, we might expect them to correctly infer how difficult it is to notice the difference. This was not the case. For uninformed participants who figured out the difference, the mean guess was 63.4 percent, which is slightly higher than the mean guess in the Informed condition. Therefore, these participants were no less likely to be biased than those who were told the difference. (This behavior is also consistent with "consensus" effects, in which people judge others to behave like themselves, cf.
Ross, et al., 1977;
Engelmann & Strobel, 2004)
. Interestingly, the


Choices from Group I produced a positive payoff only 6 percent of the time (6/101), while choices from Group U produced a positive payoff 20 percent of the time (51/253).


The number of observations varies because difference in previous earnings is defined only for subjects who previously selected from both groups. There are no observations for this variable prior to round 3.19  The fact that neither coefficient is highly significant (p = 0.13 in the first regression, p = 0.08 in the second) is not surprising, given that most guesses produced no earnings.


One reviewer points out a potential concern with our interpretation of Experiments 2 and 3, namely that the positive price on information signals to subjects that the information is valuable (the preliminary experiment reported by
Camerer (1992)
 allowed subjects to state both positive and negative prices). This concern does not apply to Experiment 1, but then the proportion of Choosers selecting Informed Predictors is only marginally significantly higher than one-half. However, Choosers' estimates of the earnings for the two kinds of Predictorswhich correspond closely to Choosers' subsequent choices -are significantly higher for Informed Predictors, thus reflecting a bias in favor of information, even when no price is provided.21  Evidence of learning over these six rounds might suggest that the bias disappears rapidly in real-world tasks, which are often repeated more than six times. However, it is important to note that the task the task we employed was simpler and the feedback we provided was clearer than for most important real-world tasks. Thus, learning to write or teach in a way so as to most effectively transmit what one knows to those who do not know it is a task that writers and teachers struggle with for years, and on which their progress is often difficult and slow.














The economics of information




G
Stigler








Journal of Political Economy




69


3
















Economics and identity




G
A
Akerlof






R
E
Kranton








Quarterly Journal of Economics




115
















Explaining bargaining impasse: The role of self-serving biases




L
Babcock






G
Loewenstein








Journal of Economic Perspectives




11
















A simple model of herd behavior




A
V
Banerjee








Quarterly Journal of Economics




107
















Back to base rates




M
Bar-Hillel








Hogarth, Robin M.; Ed; Insights in decision making: A tribute to Hillel J. Einhorn; p


Chicago, IL




The University of Chicago Press
















The rationality of prices and volume in experimental markets




C
F
Camerer








Organizational Behavior and Human Decision Processes




51
















The curse of knowledge in economic settings: An experimental analysis




C
Camerer






G
Loewenstein






M
Weber








The Journal of Political Economy




97
















Psychological Expected Utility Theory and Anticipatory Feelings




A
Caplin






J
Leahy








Quarterly Journal of Economics




116
















Portfolio choice and risk attitudes: An experimental study




G
Charness






U
Gneezy












Working paper.








Economics and psychology: A survey




P
E
Earl








The Economic Journal




100


402
















The false consensus effect: Deconstruction and reconstruction of an anomaly




D
Engelmann






M
Strobel












Working paper.








Hindsight = / foresight: The effect of outcome knowledge on judgment under uncertainty




B
Fischhoff








Journal of Experimental Psychology: Human Perception and Performance




104
















An experiment on risk-taking and evaluation periods




U
Gneezy






J
Potters








Quarterly Journal of Economics




112
















The recognition heuristic: how ignorance makes us smart




D
G
Goldstein






G
Gigerenzer




G. Gigerenzer, P. M. Todd, & the ABC Research Group






Oxford University Press




New York






Simple heuristics that make us smart








The recognition heuristic and the less-is-more effect




D
G
Goldstein






G
Gigerenzer








Handbook of Experimental Results


C. R. Plott & V. L. Smith


Amsterdam




Elsevier Press




1












The curse of expertise: The effects of expertise and debiasing methods on prediction of novice performance




P
J
Hinds








Journal of Experimental Psychology: Applied




5


2
















Outcome feedback: Hindsight and information




S
Hoch






G
Loewenstein








Journal of Experimental Psychology: Learning, Memory and Cognition




15
















Who has anticipatory feelings




B
Koszegi












University of California, Berkeley






Working Paper








Information management in incentive problems




T
R
Lewis






D
E M
Sappington








The Journal of Political Economy




105


4
















Anticipation and the valuation of delayed consumption




G
Loewenstein








Economic Journal




97
















Because it is there: The challenge of mountaineering...for utility theory




G
Loewenstein








Kyklos




52
















Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence




C
Lord






M
R
Lepper






L
Ross








Journal of Personality and Social Psychology




37
















Overconfidence in the communication of intent: Heard and unheard melodies




L
Newton








Stanford, CA






Stanford University






Unpublished doctoral dissertation








Optimal forecasting incentives




K
Osband








The Journal of Political Economy




97


5
















Development of a design analysis model for consumer complaints: Revealing a new class of quality failures




P
H
Ouden








Eindhoven, the Netherlands






Technische Universiteit Eindhoven






Unpublished doctoral dissertation








The role of information in U.S. offshore oil and gas lease auctions




R
H
Porter








Econometrica




63


1
















Incorporating fairness into game theory and economics




M
Rabin








American Economic Review




83


5
















First Impressions Matter: A Model of Confirmatory Bias




M
Rabin






J
Schrag








Quarterly Journal of Economics




114


1
















To See or Not to See: The Need for Attention to Perceive Changes in Scenes




R
A
Rensink






J
K
O'regan






J
J
Clark








Psychological Science




8
















Perseverance in self perception and social perception: Biased attributional processes in the debriefing paradigm




L
Ross






M
R
Lepper






M
Hubbard








Journal of Personality and Social Psychology




32
















The 'false consensus effect': An egocentric bias in social perception and attribution processes




L
Ross






D
Greene






P
House








Journal of Experimental Social Psychology




13
















Naive realism in everyday life: Implications for social conflict and misunderstanding




L
Ross






A
Ward








Social cognition: The Ontario Symposium


E. Reed, E. Turiel, & T. Brown


Hillsdale, NJ




Erlbaum
















Nonparametric Statistics for the Behavioral Sciences




S
Siegel






N
J
Castellan






Jr








McGraw-Hill


Boston, MA












Change Blindness




D
J
Simons






D
T
Levin








Trends in Cognitive Science




1

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]