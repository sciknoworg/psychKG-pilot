You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Bayesian Estimation of Test Engagement Behavior Models with Response Times


Introduction
Test scores may be considered as a compound of individual proficiencies, test items characteristics, and several uncontrivable error factors. Item response theory models (IRT; e.g., 
Embretson & Reise, 2000)
 are popular frameworks used to analyze academic tests. Several classical unidimensional IRT models only assume a continuous latent proficiency and item characteristic parameters such as difficulty, discrimination, and guessing parameters.
Unidimensional IRT models have been employed in large scale assessments because of their simplicity.
However, such simple IRT models idealize individual item responses, and so, their model assumptions may not uphold in all testing situation. For example, test takers do not seriously take a test if the results do not affect their lives. In other words, test takers are not engaged in the test in low-stakes settings; for example, when a test is conducted to investigate current student learning states for deciding educational policies. In such cases, the test results are used by a government but do not directly affect individual test takers, and test takers have no incentive to seriously answer the items. Therefore, item responses are affected not only by academic proficiency, but also motivation for the test (e.g., 
Finn, 2015)
.
Without considering such disengaged responses and applying classical IRT models, the model parameters estimates may be biased because test taking motivation is a nuisance factor and it is neglected in the classical IRT models. If impactful policy decisions rely on such distorted results, it would be problematic because the results do not purely reflect actual individuals' proficiency. Therefore, considering test engagement behaviors is important to avoid misleading results.
One issue of classical IRT models is that the item responses are dichotomous and have limited information; therefore, external variables are required to model test engagement behaviors. One well-studied information regarding this is response time, and De 
Boeck and Jeon (2019)
 provided a review on the use of response times. Response times can be easily gathered in computer-based tests without huge data collecting cost 
(Ferrando & Lorenzo-Seva, 2007)
 and Programme for International Student Assessment (PISA) also recorded response time data (Organisation for Economic Co-operation and Development; 
OECD, n.d.)
. Further, response times can be used in adaptive testing in which test items are selected according to participant's responses. Considering the response time and information per time unit, measurement time can be minimized. However, this research does not study this method, but instead, focused on response times for modeling tests engagement behavior.
Response times and item responses were simultaneously modeled in the hierarchical Bayesian framework by van der 
Linden (2007)
. In addition, 
Ulitzsch et al. (2020)
 extended a joint modeling of item response and response times and modeled skip behavior. 
Pohl et al.
 (2019) employed a response time to model unreached responses. 
Ulitzsch et al. (2020)
 modeled missing responses due to low engagement with response times, and 
Ulitzsch, Penk, et al. (2021)
 provided a test-taking effort model using response times. Furthermore, 
Nagy and Ulitzsch (2022)
 formalized four types of test engagement models. While other variables, such as click streams 
(Ulitzsch, He, et al., 2021)
, may be used to provide another insight on test engagement behavior, we focused on response time in this study. 
Nagy and Ulitzsch's (2022)
 models were based on previous theoretical assumption of test engagement behaviors, making every part of the model interpretable. The four models were classified according to dependency of engagement, latent variables, and assumption of latent continuous variables. Therefore, these models were called 1) dependent latent class IRT model with single-level relationships of response times (DLC-SL-IRT), 2) dependent latent class IRT model with two-level relationships of response times (DLC-TL-IRT), 3) independent latent class IRT model with a random effect of the latent class variable on response times (ILC-RE-IRT), and 4) independent latent class IRT model with a random intercept of response times (ILC-RI-IRT). They also provided estimation scripts with Mplus 
(Muthén & Muthén, 1998
-2017
. One common important feature of their models was the introduction of a latent engagement indicator variable; this allowed modeling of item responses and response times under engaged and disengaged cases. As a statistical model, their models were a mixture of latent variable model, and were flexible in understanding engaged behavior. However, these models required maximum likelihood estimation.
Bayesian estimation has several benefits for 
Nagy and Ulitzsch's (2022)
 models. First, disengagement may be rare and estimating parameters under disengagement with maximum likelihood could become difficult. Bayesian estimation could easily incorporate domain knowledge as prior distributions (e.g., 
Lee & Wagenmakers, 2013)
. In addition, test engagement behaviors were modeled as hierarchical models with several random effects; therefore, the maximum likelihood estimation was unstable because of a multiple integration problem.
Bayesian estimation is feasible even when multiple latent factors are contained. The original parameter estimation with Mplus in 
Nagy and Ulitzsch's (2022)
 models required special data structure and did not directly represent data generating structure. Bayesian estimation, with Markov chain Monte Carlo (MCMC) which was implemented in this study, directly expressed data generating mechanisms and was easily programmed with "just another Gibbs sampler" (JAGS; 
Plummer, 2003)
 language. This also allowed model extensions to include variables that could be another information source, but it might be difficult under current maximum likelihood estimation. Finally, Bayesian formulation provided several model check methods, such as posterior predictive model check or widely applicable information criterion (WAIC; 
Watanabe, 2018
) that have practically and theoretically sound.
This study extends test engagement models developed by 
Nagy and Ulitzsch (2022)
 to Bayesian formulation and implements Bayesian estimation with JAGS language. The next section provides Bayesian formulation of the four models. The four types of Bayesian models were applied to real data that was analyzed by 
Nagy and Ulitzsch (2022)
 and the four models' parameter estimates were compared in the fourth section. The conclusion and further discussion are provided in the fifth section.


Model Formulation


Item Response Function
We borrowed basic notations from 
Nagy and Ulitzsch's (2022)
 models. The essential idea of the modeling framework of 
Nagy and Ulitzsch (2022)
 was to introduce a latent engagement indicator variable to represent the connection among the test engagement, response time, and item responses. Therefore, 
Nagy and Ulitzsch's (2022)
 
P( = 1|θ , , , = 1) = exp ( (θ − )) 1 + exp ( (θ − )) ,
(1)
where θ is an individual's latent proficiency, is an item discrimination parameter taking positive value, and is a difficulty parameter. In this study, probability mass and density functions are represented by the same notation, (⋅), and are distinguished based on their arguments. We also use (⋅) to define a distribution of its argument. The item response function of Equation (1) is conditioned on latent engagement indicator. A disengaged response was modeled as random guessing:
P( = 1| , = 0) = ,
(2)
where 0 ≤ ≤ 0.5. This means that the item response is not dependent on latent proficiency at all in disengaged cases and the value of should be sufficiently small. For simplicity, it is possible to think a common guessing: = , ∀ . Combining Equations (1) and (2), the item response function is defined as follows:
P( = 1|θ , , , , ) = { exp ( (θ − )) 1 + exp ( (θ − )) } 1− .
(3)
The complete data likelihood of an item response is expressed as follows:
P( |θ , , , , ) = P( = 1|θ , , , , ) {1 − P( = 1|θ , , , , )} 1− .
(4)
Assuming random sampling of individuals and conditional independence (also known as local independence), the joint likelihood of item response matrix whose th row and th column is is as follows:
P( | , , , , ) = ∏ ∏ P( |θ , , , , ),
(5)
where , , , and are sets of latent variables and item parameters:
{θ 1 , … , θ }, { 1 , … , },
{ 1 , … , }, and { 11 , … , }. In addition to the likelihood function, prior distributions should be specified for Bayesian estimation. In this study, the prior of item difficulty parameter, ( ), is a normal distribution with mean μ and variance σ 2 , denoted as (μ , σ 2 ). Here, subscripts on hyper-parameters represent corresponding model parameters. Similarly, the prior of item discrimination parameter, ( ), is a truncated normal distribution whose mean and variance are μ and σ 2 : (μ , σ 2 ) ( > 0), where (⋅) is an indicator function for restricting the support of as positive. The prior for guessing parameter is a truncated beta distribution: 
( ) = (α, β) ( < 0.3), where (α, β) is


DLC-SL-IRT model
The DLC-SL-IRT model assumes that latent engagement is determined by response time. More formally, latent engagement probability is modeled as a logistic regression form:
P( = 1| , γ, τ ) = exp (γ( − τ )) 1 + exp (γ( − τ )) .
(6)
The parameter γ is a common slope parameter over items and similar to a discrimination parameter in IRT models; therefore, the γ indicates engagement sensitively associated with the log item response time. τ is an item-specific threshold parameter, which is analog to the difficulty parameter in IRT models, determining difficulty of engagement.
The latent proficiency parameter θ in the DLC-SL-IRT model only affects item response and does not relate to engagement. Therefore, prior latent proficiency in the DLC-SL-IRT model is the standard normal distribution for model identification: (θ ) = (0,1). In addition, individual engagement tendency is not assumed in the DLC-SL-IRT model; therefore, all the information of an individual to determine engagement is contained in a single item response time. Similar to item response function, priors for γ and τ are assumed to be a truncated normal distribution and a normal distribution, respectively:
(γ) = (μ γ , σ γ 2 ) (γ > 0) and (τ ) = (μ τ , σ τ 2 ).
We can marginalize latent class indicator in the DLC-SL-IRT model because latent engagement indicator is defined for a single item response. This marginalization helps to improve the convergence of MCMC iterations. Then, the marginalized correct item response probability can be written as follows: 
P( = 1| , θ , ,
= { exp ( (θ − )) 1 + exp ( (θ − )) } { exp (γ( − τ )) 1 + exp (γ( − τ )) } + {1 − exp (γ( − τ )) 1 + exp (γ( − τ )) }.
(7)
In the MCMC procedure, latent engagement indicators can be gained as generated quantities if they are required.
Finally, the full conditional posterior is proportional to a product of the complete likelihood and priors:
( , , , , γ, | , ) ∝ P( | , , , , , γ, ) ( ) ( ) ( ) ( ) (γ) ( ) = {∏ ∏ P( | , θ , , , , γ, τ )} {∏ (θ )} {∏ ( ) ( ) (τ )} ( ) ( ),
(8)
where is a set of threshold parameter {τ 1 , … , τ }, and is a log item response time matrix whose element of the th row and th column is .


DLC-TL-IRT model
To relax the DLC-SL-IRT model assumptions, the DLC-TL-IRT model introduces the individual engagement difficulty parameter ζ in Equation 
6
and the engagement probability is defined as follows:
P( = 1| , γ, τ , ζ ) = exp(γ[ − (τ + ζ )]) 1 + exp(γ[ − (τ + ζ )]) .
(9)
In this model, the individual engagement difficulty parameter ζ represents an engagement difference among individuals and is treated as a random effect. Small ζ indicates that the th individual tends to engage in test items easily. In addition, the ζ is connected to latent proficiency θ via a multivariate normal distribution:
P (θ , ζ |μ θζ = [ μ θ μ ζ ] , Σ θζ = [ 1 ρ θζ σ ζ ρ θζ σ ζ σ ζ 2 ]) = (μ θζ , Σ θζ ),
(10)
where mean vector of latent variable μ θζ is set to the zero-vector for identifiability. Covariance matrix of person parameters is denoted as Σ θζ . Elements of the covariance matrix are the variance parameter σ ζ 2 , representing deviation of engagement tendency, and the correlation parameter ρ θζ between θ and ζ which may be negative in this case because a highproficiency person tends to easily engage in test items. In addition, the variance of θ needs to be fixed at one for the model identification.
In a traditional setting, an inverse Wishart distribution is assumed for a prior covariance matrix, but the variance of θ needs to be fixed in this model. Therefore, the inverse Wishart distribution is not appropriate. Instead, the covariance matrix is decomposed in two lowertriangle matrices, called Cholesky decomposition 
(Zhan et al., 2019)
:
Σ θζ = Δ θζ Δ θζ ⊤ ,
(11)
Δ θζ = [ 1 0 ϕ ψ ],
(12)
where, priors are set for ϕ and ψ, and so, (ϕ) = (0,1) (ϕ < 0) and (ψ) =
(1, 1) are assumed priors, and where (α, β) is a gamma distribution with shape α and rate β.
The same marginalization in the DLC-SL-IRT model is applied for the item response probability in the DLC-TL-IRT model and we get a correct response probability without latent engagement indicator which is represented as follows:
P( = 1| , θ , ζ , , , , γ, τ ) = ∑ P( = 1|θ , , , , = )P( = | , ζ i , γ, τ ) 1 =0
(13)
Finally, the joint posterior probability of the parameters and latent variables is expressed as follows:
( , , , , , γ, | , ) ∝ P( | , , , , , , γ, ) ( , ) ( ) ( ) ( ) (γ) ( ) = {∏ ∏ P( | , θ , ζ , , , , γ, τ )} {∏ (θ , ζ )} × {∏ ( ) ( ) (τ )} ( ) ( ) (ϕ)P(ψ),
(14)
where is a set of individual engagement difficulty parameters; {ζ 1 , … , ζ }.


ILC-RE-IRT model
The DLC-IRT models assume that latent engagement indicators are dependent on response times. In the ILC-IRT models, latent engagement indicators are not directly dependent on response times but connected through individual latent variables. One latent variable, η , that is represented as an individual engagement tendency and an item engagement difficulty parameter, κ , define latent engagement probability with one parameter logistic IRT model:
P( = 1|η , ) = exp(η − ) 1 + exp(η − ) .
(15)
Similar to IRT difficulty parameters, the prior for κ is a normal distribution denoted as (κ )= (μ κ , σ κ 2 ). Prior for η is defined for later.
The latent engagement status determines not only different item responses but also different response times. In this case, two different one-factor analysis models are assumed for the response times. In other words, we assume a mixture distribution of two normal distributions for response times:
{ P ( |ν , λ , σ ε 2 , = 0) = (ν + λ ξ , σ ε 2 ) , P ( |ν , λ , σ ϵ 2 , = 1) = ( + λ ξ , σ ϵ 2 ) ,
(16)
where and λ are an intercept and a factor loading parameter for engaged status, and ̃ and λ are an intercept and a factor loading parameter for disengaged status. The unique factor variance parameters for engaged and disengaged statuses are σ ϵ 2 and σ ε 2 . A factor score ξ , which can be thought of as a basic individual response speed, is common in both engaged and disengaged statuses.
In a regression formulation, Equation (16) is rewritten as follows:
= ( + λ ξ + ϵ ) + (1 − )(ν + λ ξ + ε ),
(17)
where two sets of residual terms {ϵ 1 , … , ϵ } and {ε 1 , … , ε } are independently and identically distributed random variables followed different normal distributions: (0, σ ϵ 2 ) and
(0, σ ε 2 ). Here, residual variances can be different among items. Priors are
( )= (μ , σ 2 ), (̃)= (μ̃, σ2 ), (λ ) = (μ λ , σ λ 2 ) (λ > 0), (λ ) = (μ λ , σ λ 2 ) (λ > 0), (σ ϵ 2 ) = (α , β )
, and (σ ε 2 ) = (α , β ). The gamma distributions here can be replaced by an inverse gamma distribution with shape α and rate β, which may be another standard choice. In this study, the hyper parameters of the gamma distributions are set as the same value, α = β = α = β = 1/2. This gamma distribution is equivariant to 2 distribution with one degree of freedom.
An important point of the ILC-IRT models is that three types of individual parameters, θ , η , and ξ , are related each other. The major distribution that represents a connection among three continuous random variables is a multivariate normal distribution is as follows:
P (θ , η , ξ |μ θηξ = [ μ θ μ η μ ξ ] , Σ θηξ = [ 1 ρ θη ρ θξ ρ θη 1 ρ ηξ ρ θξ ρ ηξ 1 ]) = (μ θηξ , Σ θηξ ),
(18)
where μ θηξ is a mean vector and Σ θηξ is a 3 × 3 positive definite covariance matrix. Again, μ θ , μ η , and μ ξ are zero and diagonal elements of Σ θηξ that are variances of three latent factors are set to one to identify the model. Therefore, Σ θηξ is a correlation matrix rather than a covariance matrix here. Off-diagonal elements of Σ θηξ are correlation parameters among three latent variables that are denoted as ρ θη , ρ θξ , and ρ ηξ , whose subscripts represent a combination of variables to be considered. Priors of correlation parameters are directly specified and are uniform distributions: (ρ θη ) = (0,1), (ρ θξ ) = (−1,1), and (ρ ηξ ) = (0,1). We assumed positive correlations between θ and η, and η and ξ; however, no-strong assumption was assumed between θ and ξ. Note that label switching problem need to be prevented for the general ILC-IRT models to put ordered constraints on intercepts or factor loadings.
Under conditional independence assumptions, the conditional distribution of model parameters of the general ILC-IRT models is represented as assembling the likelihood functions, individual parameters, and priors as follows: 
( , , , , , , ,
 , ,̃, ,̃, ϵ 2 , ε 2 , ρ θη , ρ θξ , ρ ηξ | , )
∝ P( | , , , , )P( | , ,̃, ,̃, ϵ 2 , ε 2 , ) ( | , ) ( , , ) × ( ) ( ) ( ) ( ) (̃) ( ) (̃) ( ϵ 2 ) ( ε 2 ) ( ) (ρ θη ) (ρ θξ ) (ρ ηξ ),
(19)
= {∏ ∏ P( |θ , , , , )P ( |ξ , ν , ν , λ , λ σ ϵ 2 , σ ε 2 , ) ( |η , )}
× {∏ (θ , η , ζ )} {∏ ( ) ( ) (τ ) (κ ) ( ) (̃) (λ ) (λ ) (σ ϵ 2 ) (σ ε 2 )} × ( ) (ρ θη ) (ρ θξ ) (ρ ηξ ),
where , , , ,̃, ,̃, ϵ 2 , and ε 2 are parameter sets corresponding to individuals' and item parameters. is a conditional variable on both and , and so marginalization of engagement indicator will generate dependency between and . Therefore, remains in the likelihood functions of the general ILC-IRT model. The general ILC-IRT model has two likelihood functions: one from item responses and the other from response times, which is different from the DLC-IRT models.
The general ILC-IRT model is over-parameterized and loses meaning of the parameters.
One simple constraint is to add a disengagement situation in which set factor loadings are 0 and residual variance are the same across items:
λ = 0, ∀ , σ ε 2 = σ ε 2 , ∀ .
(20)
This constraint means that the disengaged response times are not affected by individual response speed because a disengage response is a quick response and does not different across items. This constraint prove response times equation as follows:
= ν + (δ + λ ξ ) + ϵ + (1 − )ε ,
(21)
where δ = ν − ν . The second term in Equation 
21
 
{ ( | = 0, ν , σ ε 2 ) = (ν , σ ε 2 ), ( | = 1, ν , δ , λ , ξ σ ϵ 2 ) = (ν + δ + λ ξ , σ ϵ 2 ) .
(22)
The ILC-RE-IRT model assumes that an engaged response time takes longer than a disengaged response.
Finally, posterior distribution of the ILC-RE-IRT model is slightly simplified version of Equation 
19
: 
( , , , , , , , , ,̃, , ϵ 2
 , σ ε 2 , ρ θη , ρ θξ , ρ ηξ | , )
∝ P( | , , , , )P( | , ,̃, , ϵ 2 , σ ε 2 , ) ( | , ) ( , , ) × ( ) ( ) ( ) ( ) (̃) ( ) ( ϵ 2 ) ( ) (σ ε 2 ) (ρ θη ) (ρ θξ ) (ρ ηξ )
(23)
= {∏ ∏ P( |θ , , , , )P ( |δ , ν , λ σ ϵ 2 , σ ε 2 , ) ( |η , )}
× {∏ (θ , η , ζ )} {∏ ( ) ( ) (τ ) (κ ) (δ ) (̃) (λ ) (σ ϵ 2 )} × ( ) (σ ε 2 ) (ρ θη ) (ρ θξ ) (ρ ηξ ),
where = {δ 1 , … , δ }.


ILC-RI-IRT model
Different constraints posed on the general ILC-IRT model provide a different ICL-IRM model. For example, equality constraints on factor loadings between engagement and disengagement conditions (i.e., λ = λ , ∀ ) are possible. This provides following regression formulation of a log response time:
= ν + λ ξ + δ + ϵ + (1 − )ε ,
(24)
In this formula, the random effect ξ is outside of the regression coefficient and thought of as a random intercept. In the ILC-RI-IRT model, the individual response speed has an effect even in the disengagement situation. The effect of engagement δ is a fixed effect and does not vary 


Application to Real Data


Data Analysis Setup
Example data analyzed in 
Nagy and Ulitzsch (2022)
 were gained from the Programme for the International Assessment of Adult Competencies (PIAAC), which is an international large-scale assessment for adults. More detailed explanations were shown in 
Nagy and Ulitzsch (2022)
. The sample size was 637, and 20 item responses and log response times were included the data set.
The items used open response format and the correct item response probability in disengagement was expected to be close to zero. Log response times were standardized in this study.
The MCMC estimation code was written in JAGS language. Normal priors replaced the standard normal distribution. Correct response probability in disengagement prior parameters was α = 1 and β = 4 and the upper limit was set to 0.3 to represent low correct response probability. Additional constraints to the ̃ parameters were negative and the δ parameters were constrained as positive. The ̃ parameters were average log response time in the disengagement situation and the log response times were standardized in this study so we assumed the responses were faster than general average that was zero. This assumption provided previous negative constraints on the ̃ parameters. Similarly, the δ parameters were the effects of engagement on the log response times and the engagement ought to take several times. This consideration generated that he δ parameters were positive. The number of chains were three, total MCMC iterations were 40,000, burn-in period was the first 10,000 samples, and thinning number was five. Convergence criterion was Gelman-Rubin index (̂; 
Gelman & Rubin, 1992)
 lower than 1.10. WAIC and posterior predictive p-value (PPP) were employed for model comparisons. Employed data, JAGS model, and estimation codes are available from Open Science Framework page: https://osf.io/v4zk3/.


Results
The ̂s of the model parameters were less than 1.10, and so MCMC iterations were judged to be converged. The DLC-TL-IRT model (WAIC = 11430.940, SE = 118.911, PPP =.652) indicated a lower WAIC value than the DLC-SL-IRT model (WAIC = 11504.543, SE = 117.988, PPP = .697). In addition, the PPP of the DLC-TL-IRT model was closer to .5 than in the DLC-SL-IRT model. These results suggest that the DLC-TL-IRT model was better than the DLC-SL-IRT model. WAIC of the ILC-RI-IRT (WAIC = 36037.783, SE = 246.879, PPP for item response =.579, PPP for log response time = .524) was smaller than that of the ILC-RE-IRT model (WAIC = 36375.162, SE = 241.876, PPP for item response = .586, PPP for log response time =.519), and the PPP values of the ILC-RI-IRT and ILC-RE-IRT models were almost the same. These results were consistent with Nagy and Ulitzsch (2022)'s findings. 
Table 1
 shows the posterior means of response times thresholds in the DLC-IRT models, engagement difficulties in the ILC-IRT models, and averages of the posterior means of engagement probability over individuals. The absolute values of response time thresholds τ estimates between 
Table 1
 in this study and 
Table 4
 in 
Nagy and Ulitzsch (2022)
 were similar.
However, 
Nagy and Ulitzsch (2022, Table 4
) reported several extremes in κ estimates (e.g., absolute values greater than 5) but the results shown in 
Table 1
 are quite moderate because priors prevented extreme estimates. Engaged response results were also similar between the current and Nagy and Ulitzsch (2022)'s study. However, the values of current estimates were smaller. This means our Bayesian estimation showed that the individuals were less engaged than the maximum likelihood estimates in 
Nagy and Ulitzsch (2022)
.
Insert 
Table 1
 The correct response probability for disengaged status in the DLC-SL-IRT and DLC-   


Conclusion and Discussion
This study provided Bayesian formulation of four test engagement models and their likelihood functions with explicitly described priors. Furthermore, Bayesians estimation method with JAGS language was applied to PIAAC data. The real data example showed that the parameter estimates did not have extreme values and showed stable estimates. Parameter estimates similarity and differences among the models were shown.
Maximum likelihood estimation is difficult if the parameters are close to the boundaries. In such cases, maximum likelihood estimation procedure may provide unreasonable solutions. In the context of test engagement behavior, correct response probabilities in disengagement situation and engagement probabilism can be close to zero or one, and it is possible for the maximum likelihood estimation to not work well. In addition, test engagement models considered in this study combined multilevel and mixture models, which are known as difficult models to estimate. Sample size of lower-stakes tests may not be large, which would make parameter estimation harder.
The Bayesian MCMC method can handle these problems. Even if the parameters are close to their boundaries, MCMC provides appropriate approximated posteriors. In addition, prior distributions work as regularization terms and prevent irregular solution. These benefits are especially important in cases with small sample sizes. Additionally, JAGS codes for estimation are simple and naturally represent data generating functions, making model extension and parameter restriction easy. For adaptive testing, posterior distribution rather than point estimates was proposed 
(Chang & Ying, 1996)
. When researchers can specify engagement or disengagement, they can consider the stopping rule (when to stop measurement) more precisely because observations with disengagement have little information regarding proficiency.
One disadvantage of the MCMC procedure is that it takes a longer time for parameter estimation and requires powerful computers that was pointed out by 
Nagy and Ulitzsch (2022)
.
The estimation times of the real data example were several hours in the authors' computational environment. If the number of individuals and test items increased, Bayesian MCMC procedure will not be a good choice. Another approximation technique, such as variational Bayesian inference 
(Nakajima et al., 2019)
, will be required for larger datasets. 
Table 1
 Posterior means of engagement-related parameters and engagement probabilities in the four engagement models  


Figure 1
Scatter plots of latent individual parameters (θ, η, and ξ) in the four engagement models  


Figure 3
Response time model parameters (ν parameter: upper left, δ parameter: upper right, λ parameter: lower left, σ ϵ 2 parameter: lower right) estimates with the independent latent class IRT model with a random effect of the latent class variable on response times (ILC-RE-IRT), and independent latent class IRT model with a random intercept of response times (ILC-RI-IRT) models
beta distribution with parameter α and β with the upper limit 0.3. This upper limit shows that the correct response under the disengaged condition should be small. Note that prior means and variances can set other values if there are sufficient empirical knowledge. The differences among the models are assumption of the distributions of laten proficiency (θ ), the latent engagement indicator ( ), and modeling of the logarithm of item response time of an individual for an item , which is denoted by .


, , γ, τ ) = ∑ P( = 1|θ , , , , = )P( = | , γ, τ ) 1 =0 = P( = 1|θ , , , = 1)P( = 1| , γ, τ ) + P( = 1| , = 0)P( = 0| , γ, τ ),


represents the effect of engagement on a log response time and contains the random effect ξ. The third and fourth terms are residual corresponding engagement and disengagement situations. In other representation, conditional distributions of log response time given the engagement status are as follows:


among individuals. Conditional distributions given the latent engagement indicator are normal distributions whose means and variances are different: are the same as in ILC-RE-IRT model. Conditional posterior distribution is a representation of Equation (23) but with Equation (25) for corresponding terms.


TL-IRT models was the same [ = .002 ( = .002)] but that of the ILC-RE-IRT and ILC-RI-IRT models were = .025 ( = .008) and = .010 ( = .005), respectively, making the values larger than the DLC-IRT models. The latent class discrimination parameters γ of the DLC-SL-IRT and DLC-TL-IRT models were γ = 5.376 ( = 0.432) and γ = 4.663 ( = 0.430), respectively, providing congruent results. The correlation between θ and ζ of the DLC-TL-IRT model was ρ θζ = −.971 ( = .036), implying that the high proficiency individuals could easily engage in the test items. The variance of individual engagement difficulty was σ ζ 2 = 0.234 ( = 0.073); there were individual differences of engagement difficulty. Correlation among three latent variables in the ILC-RE-IRT model were ρ θη = .483 ( = .050), ρ θξ = .341 ( = .044), and ρ ηξ = .131 ( = .045), showing that the latent proficiency and engagement tendency, and the engagement tendency and response speed were correlated. The correlations in the ILC-RI-IRT model were ρ θη = .676 ( = .050), ρ θξ = .008 ( = .058), and ρ ηξ = .414 ( = .040). Both ILC-RE-IRT and ILC-RI-IRT models showed correlations that were greater than .45 between the latent proficiency and engagement tendency. However, the other two correlation results were not consistent with each other. The correlation between latent proficiency and response speed ρ θξ in the ILC-RI-IRT model was approximately zero, but it was positive for one in the ILC-RE-IRT model. The test engagement tendency and response speed ρ ηξ in the ILC-RI-IRT model was larger than in the ILC-RI-IRT model.


Figure 1
1
indicates scatter plots for latent proficiency θ between the DLC-SL-IRT and DLC-TL-IRT models (upper left panel), and three proficiencies (θ, η, and ξ) between the ILC-RE-IRT and ILC-RI-IRT models (upper right, lower left, and lower right panels, respectively).The latent proficiency between the DLC-SL-IRT and DLC-TL-IRT models was consistent and provided similar results. Comparison of the θ between the ILC-RE-IRT and ILC-RI-IRT models indicated that lower proficiency (less than -2) in the ILC-RE-IRT model was slightly highly estimated in the ILC-RI-IRT model. The range of latent engagement tendency η in the ILC-RI-IRT model was wider than that in the ILC-RE-IRT model. This implies that the ILC-RI-IRT model could capture detailed individual engagement tendency. Some individuals who took -1 to 0 values in the ILC-RE-IRT model were much smaller values (less than -3) in the ILC-RI-IRT model. The lower values (less than -2) in the response speed factor ξ in the ILC-RE-IRT model took higher values in the ILC-RI-IRT model. InsertFigure 1


Figure 2
2
presents the IRT discrimination and difficulty parameters (left and right panels, respectively) estimates with four engagement models. Four models did not show significant differences in difficulty parameters. Discrimination parameters indicated slightly different estimates across several items (e.g., item 1, 4, 7, 13, 16, and 18) but systematic tendency was not shown.Figure 3 depicts response time model parameters (ν parameter: upper left; δ parameter: upper right; λ parameter: lower left; σ ϵ 2 parameter: lower right) estimates with two engagement models. The ν parameters showed a different tendency in the ILC-RI-IRT model, which showed larger values than the ILC-RE-IRT model except for items 3 and 12. The δ parameters showed the opposite tendency: the ILC-RI-IRT model showed lower estimates than the ILC-RE-IRT model except for items 3 and 12. The λ parameters of the ILC-RE-IRT model were greater thanthe ILC-RI-IRT model's. The σ ϵ 2 parameter estimates were almost the same between the two ILC-IRT models. Finally, the residual variance in the discontinuous parameters of the ILC-RE-IRT model was σ ε 2 = 54.663 ( = 0.430), which was much larger than that of the ILC-RI-


Note.
The four models are the dependent latent class IRT model with single-level relationships of response times (DLC-SL-IRT), dependent latent class IRT model with two-level relationships of response times (DLC-TL-IRT), independent latent class IRT model with a random effect of the latent class variable on response times model (ILC-RE-IRT), and independent latent class IRT model with a random intercept of response times (ILC-RI-IRT) model.


Figure 2 IRT
2
discrimination parameters (left panel) and difficulty parameters (right panel) estimates with the four engagement models Note. The four models are the dependent latent class IRT model with single-level relationships of response times (DLC-SL-IRT), dependent latent class IRT model with two-level relationships of response times (DLC-TL-IRT), independent latent class IRT model with a random effect of the latent class variable on response times model (ILC-RE-IRT), and independent latent class IRT model with a random intercept of response times (ILC-RI-IRT) model.














A global information approach to computerized adaptive testing




H
H
Chang






Z
Ying








Applied Psychological Measurement




20


3


















10.1177/014662169602000303














An overview of models for response times and processes in cognitive tests




De
Boeck






P
Jeon






M








Frontiers in Psychology




10
















10.3389/fpsyg.2019.00102














Item response theory for psychologist




S
E
Embretson






S
P
Reise


















An item response theory model for incorporating response time data in binary personality items




P
J
Ferrando






U
Lorenzo-Seva




10.1177/0146621606295197








Applied Psychological Measurement




31
















Measuring motivation in low-stakes assessments




B
Finn




10.1002/ets2.12067








ETS Research Report Series




2
















Inference from iterative simulation using multiple sequences




A
Gelman






D
B
Rubin




10.1214/ss/1177011136








Statistical Science




7
















Bayesian cognitive modeling: A practical course




M
D
Lee






E.-J
Wagenmakers




10.1017/CBO9781139087759








Cambridge University Press












Mplus: Statistical analysis with latent variables: User's guide (version 8)




L
K
Muthén






B
O
Muthén








Muthén & Muthén












A multilevel mixture IRT framework for modeling response times as predictors or indicators of response engagement in IRT models




G
Nagy






E
Ulitzsch




10.1177/00131644211045351








Educational and Psychological Measurement




82


5


















S
Nakajima






K
Watanabe






M
Sugiyama




Variational Bayesian learning theory


















10.1017/9781139879354






Cambridge University Press












PISA












OECD






Technical Report


Retrieved








JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling




M
Plummer








The 3rd International Workshop on Distributed Statistical Computing






124














Using response times to model not-reached items due to time limits




S
Pohl






E
Ulitzsch






M
Davier




10.1007/s11336-019-09669-2








Psychometrika




84


3
















R: A language and environment for statistical computing. R Foundation for Statistical Computing




R Core Team










Vienna, Austria












Combining clickstream analyses and graph-modeled data clustering for identifying common response processes




E
Ulitzsch






Q
He






V
Ulitzsch






H
Molter






A
Nichterlein






R
Niedermeier






S
Pohl








Psychometrika




86


1


















10.1007/s11336-020-09743-0














Model meets reality: Validating a new behavioral measure for test-taking effort




E
Ulitzsch






C
Penk






M
Von Davier






S
Pohl








Educational Assessment




26


2


















10.1080/10627197.2020.1858786














A hierarchical latent response model for inferences about examinee engagement in terms of guessing and item-level non-response




E
Ulitzsch






M
Von Davier






S
Pohl
























British Journal of Mathematical and Statistical Psychology




73


S1
















10.1111/bmsp.12188














A hierarchical framework for modeling speed and accuracy on test items




W
J
Van Der Linden




10.1007/s11336-006-1478-z








Psychometrika




72


3
















Mathematical theory of Bayesian statistics




S
Watanabe








Chapman and Hall/CRC












Using JAGS for Bayesian cognitive diagnosis modeling: A tutorial




P
Zhan






H
Jiao






K
Man






L
Wang








Journal of Educational and Behavioral Statistics




44


4


















10.3102/1076998619826040














The four models are the dependent latent class IRT model with single-level relationships of response times (DLC-SL-IRT), dependent latent class IRT model with two-level relationships of response times (DLC-TL-IRT), independent latent class IRT model with a random effect of the latent class variable on response times model (ILC-RE-IRT), and independent latent class IRT model with a random intercept of response times






ILC-RI-IRT) model













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]