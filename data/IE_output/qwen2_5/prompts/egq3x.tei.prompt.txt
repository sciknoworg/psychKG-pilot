You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
A wealth of data suggests that the brain includes multiple systems capable of shaping behavior 
[1,
2,
3,
4,
5,
6,
7,
8]
. A longstanding theory in psychology proposes that behavior can be governed by a goal-directed action planning system, or by a stimulus-driven reactive response system 
[9]
. Modelbased (MB) and model-free (MF) reinforcement learning (RL) have been proposed as algorithmic implementations that capture the spirit of these learning and decision making systems 
[10,
11,
12]
. In this framework, model-based RL learns an internal model of the environment, and uses this model to compute the value of candidate action trajectories via on-line planning. Conversely, model-free RL learns cached values that represent the long-run expected value of a particular state and/or action, and compares these scalar value to facilitate choice 
[13]
.
Model-free and model-based RL rely on different learning signals to shape their respective representations of the decision problem. Model-free RL employs a reward prediction error (RPE) signal that quantifies the discrepancy between expected and actual rewards, and is used to refine cached reward predictions to align with what is observed in the environment. Notably, the midbrain dopamine system has been functionally characterized as broadcasting RPE signals to afferent neural targets in support of model-free RL (among other functions) 
[14]
. Along similar lines, model-based RL utilizes a state-prediction error (SPE) to learn a model of the environment's state transition structure, where state-action-state transition probabilities are learned in order reliably predict the transition dynamics encountered in the world. Behavioral correlates of both model-based and model-free control have been reported in humans using sequential decision-making tasks, and neural correlates of these strategies have been identified by a number of neuroimaging studies 
[15,
16,
17,
18,
19,
20]
. Nonetheless, major open questions remain about how to best characterize the RL algorithms implemented in the human brain 
[21]
.
Emerging from a dual-process theory of behavioral control is the issue of how the brain allocates control among multiple candidate systems. Several key variables have been proposed to play a role in the arbitration of control, such as response conflict 
[22]
, the value of information 
[23]
, the trade-off between speed and accuracy 
[24]
, and the expected value of control 
[25]
. Here, we examine and expand on a normative framework that relies on uncertainty-based competition for behavioral control whereby the predictive accuracy of candidates systems is used to arbitrate among them 
[10]
.
Experimental work has shown that uncertainty-based competition for control may be realized in the human brain by a computational mechanism that considers the predictive reliability associated with candidate systems to arbitrate among them 
[26]
. In this framework, the average magnitude of the model-free system's RPEs index the degree to which that system is able to reliably predict reward. Simultaneously, the average magnitude of the model-based system's SPEs provide a measure of that system's ability to reliably predict the consequences of its actions. Neuroimaging evidence suggests that the brain relies on the accumulation of RPE and SPE signals to track the relative uncertainty within model-free and model-based systems respectively to arbitrate between them 
[26]
. Here, we consider the limitations of this approach. We reasoned that the predictive accuracy of a model-based system ought to incorporate not only the reliability of state predictions indexed by SPEs, but also the reliability of reward predictions used to embed value into the planned course of action. Thus, a major goal of the current study was to examine the additional unique effects of model-based RPEs on the arbitration process.
However, a major obstacle to understanding the arbitration of control stems from the difficulty of isolating the generative processes underlying behavior. Analytical and computational approaches have typically relied on the assumption that behavior emerges from either a model-based or a modelfree system, where the level of model-free control is quantified by the degree to which behavior is inconsistent with model-based behavior 
[15,
25,
27,
16]
. However, 'non-model-based' behavior could include unstructured random choices, degenerate behavior (e.g. gambler's fallacy, or other non-RL choice heuristics) 
[28,
29]
, correlations among actions and events 
[30]
, or model-based control that relies on an unconventional representation that does not correspond to the 'correct' model of the task 
[31]
. A central goal of the present study is to better characterize and delineate phenotypes of behavioral control associated with model-based/-free RL by considering measures that extend beyond a propensity to engage in a particular flavor of RL.
Here, we examine uncertainty-based arbitration of control by exposing a large cohort of participants to a novel variant of a multi-step RL decision task designed to enhance signal associated with modelbased/-free control. We identify considerable behavioral heterogeneity across our sample population and show that, although it is not appropriate to assume that behavior is driven by a model-free system simply because is not consistent with model-based planning, model-free control remains a relevant component of human behavior. We also report evidence consistent with uncertainty-based arbitration that relies on system-specific prediction errors, including model-based RPEs, to identify and deploy the most reliable strategy. Finally, exploratory analyses suggest that some participants flexibly arbitrate between model-based and model-free systems of control, whereas others persist in deploying a model-based planning strategy while arbitrating between representations of the task itself. Together, these findings provide evidence that predictive reliability contributes to the impact a control system will have on behavior, and that there are considerable individual differences both in terms of the dominant system and the flexibility with which each system may be deployed. Our findings highlight the challenge of distinguishing model-based from model-free control, but suggests that this challenge can be mitigated with the inclusion of additional task measures aimed at drawing out a richer characterization of individual behavioral profiles 
[21]
.


Results


General Behaviors
Two-stage decision-making tasks are designed to dissociate model-based and model-free response patterns by embedding probabilistic state transitions and reward outcomes into the task design. In these tasks, participants make a choice between two options at the first stage of each trial, with each firststage option leading to one of two second-stage states more reliably than the other (70% common and 30% rare), from which a rewarded outcome may be subsequently encountered. Critically, the reward obtained following a rare transition should impact first-stage choices on the ensuing trial differently according to the strategy in use. For example, when a first-stage choice initiates a rare transition leading to a rewarding outcome, the principals of model-free RL dictate that the first-stage choice is more likely to be repeated as it has just been paired with a reward. Conversely, prospective model-based planning considers the task's transition structure; and as such, is more likely to choose the alternative option that has a better chance of carrying the agent to the previously rewarded state. Thus, whereas model-free RL predicts a main effect of reward, model-based RL predicts a crossover interaction between reward and transition. Prior studies using the standard two-step task have consistently reported that humans express a mixture of both model-free and model-based strategies, as highlighted by both a main effect of reward, and an interaction effect between reward and transition 
[15,
16,
32,
33]
.
We modified the the standard two-stage decision task to augment the distinct behavioral signatures that dissociate model-based and model-free behavior. First, we simplified the task by only asking participants to make a single first-stage choice on each trial (i.e. they did not make a second-stage choice) since second-stage choices typically used in two-stage task designs do not contribute to the dissociation of model-based and model-free control. We reasoned that this modification could help to alleviate misattribution of outcomes to actions 
[31]
, but more importantly, including a second-stage choice would compromise key manipulations used to modulate the arbitration of control (discussed later). Second, in contrast to the random walk typically employed in a two-stage task design, we structured the reward probability time-course to include periods where one option had a higher expected value than the other in order to enhance the behavioral signals indicative of general RL through periodic preference shifts toward a highly rewarding option. Importantly, the reward probability time-course also included periods where both options had approximately equivalent expected values, which served to minimize the effect of learned preference that can dominate choice and can hamper the dissociation of model-based and model-free control 
[25]
.
Data collected from N=452 online participants using mTurk indicate that participants understood and were motivated by the task. Overall, participants encounter more rewards than would be expected from random choice (p = 4.6632e − 95, paired t-test) as participants were rewarded on average on 48.16% (sd = 0.0371) of the total trials compared to 43.11% (sd = 0.0172) with random choices. Replicating previous reports, participants expressed a mixture of model-based and model-free strategies. Consistent with model-free control, a mixed-effects logistic regression revealed a significant main effect of previous reward (β = 0.52, p < 2e − 16); as well as a significant interaction between previous reward and previous transition indicative of model-baed control (β = −0.65, p < 2e − 16). Posthoc analysis of behavior following a common transition shows that participants were significantly more likely to repeat a rewarded choice relative to an unrewarded choice (β = 1.18, p < 2e − 16). Conversely, participants were less likely to repeat a rewarded action relative to a non-rewarded action following a rare transition (β = −0.12, p = 0.003), indicating a modest model-based bias overall consistent with numerous previous studies. is predominantly associated with one of two second-stage states (planets), landing there 70% of the time. Each planet is associated with two visually distinct terminal-states (landing pads) where reward outcomes are delivered. B) Timeline of trial events. A first-stage choice leads to a second-stage state (e.g the red planet), which subsequently transitions to one of two terminal states (here, the North mine's landing pad) where a reward of 75 points is delivered. C) Block design. Examples of conditional block structure for reward magnitude reliability, transition reliability, and reward contingency manipulations. D) Reward probability timecourse. The probability of reward delivery follows a prescribed timeline. For a short period of time one option has a high probability of reward while the alternative does not, which encourages reliable preferences that facilitate model fitting. This is followed by a period during which both options share approximately equal probability of reward. This discourages the development of strong preferences and enhances model-based/free dissociability. E) Reward contingency manipulation. Reward probability is coupled to the chosen stimulus, or to the terminal state. Both model-based and model-free controllers produce significantly larger RPEs during periods of stimulus contingent rewards, but the model-based RPEs are significantly more pronounced. F) Reward magnitude manipulation. When a reward is delivered, its magnitude is drawn from a uniform distribution with either low or high mean and variance. The model-free controller encounters significantly larger RPEs when reward magnitudes are large and have high variance. G) State transition manipulation. The transition from planet to landing pad can occur with high (90%/10%) or low reliability (50%/50%). The model-based controller encounters significantly larger SPEs when state transitions are unreliable.


Behavioral evidence of reliability-based adaptive of control
We aimed to examine the computational variables driving the arbitration of control by augmenting the two-stage task with three key manipulations. As demonstrated by 
[26]
, explicitly devaluing specific outcomes selectively undermined the model-free system's ability to reliably predict reward. Here, we interrogate the influence of model-free RPEs further by systematically manipulating the magnitude of reward, embedding periods of relatively consistent (high reliability) and inconsistent (low reliability) reward magnitude (see 
Figure 1C
,F). Participants were explicitly informed that reward magnitude was inherently random and therefore unlearnable. We reasoned that a model-free system, naive to the task's structure, would be susceptible to reward magnitude variability whereas a well-informed modelbased representation should not, resulting in diminished model-free control during periods of low reward magnitude reliability. As hypothesized, we observed more prominent expression of model-based control during periods of low reward magnitude reliability, as indicated by a significant Outcome (O t−1 ) by Transition (T t−1 ) by Magnitude reliability (M r ) interaction (O t−1 : T t−1 : M r : β = −0.13, p < 2e − 16, see 
Figure 2F
, H). We summarize this effect in 
Figure 2H
, which depicts the change in transition sensitivity across conditions of reward magnitude reliability, where transition sensitivity captures the differential response to reward outcomes following common and rare transitions (see Methods for details).
Our task design also systematically varied state transition reliability to investigate the effect of model-based SPEs on the arbitration of control. Having arrived at a second-stage state, participants automatically transitioned to one of two terminal states associated with that second-stage state with either high (90%) or low (50%) probability, resulting in periods of relatively low or high SPEs respectively (see 
Figure 1C
, G). According to the reliability-based theory of arbitration, elevated SPEs should promote MF control. Indeed, low transition reliability induced a significant reduction in model-based control as indicated by a significant Outcome (O t−1 ) by Transition (T t−1 ) by Transition reliability (T r ) interaction (O t−1 : T t−1 : T r : β = −0.02, p = 0.01, see 
Figure 2G
,H).
Finally, we question whether the brain's arbitration mechanism considers the model-based controller's reward prediction accuracy. We do so by corrupting the task's reward contingency structure to include periods of stimulus-contingent reward into our experimental design, where the probability of reward was coupled to the first-stage stimuli instead of the terminal states. Here, more frequent RPEs are encountered when reward is determined by the first-stage stimulus because the terminal state is no longer a reliable predictor of reward (see 
Figure 1E
). According to the original reliability-based theory of arbitration 
[26]
, elevated RPEs should down-weight the model-free system in favor of model-based control. However, MF control is in fact advantageous under these circumstances, as it aptly reinforces actions regardless of intermediary states encountered en route to reward. More generally, this speaks to one of the strengths of model-free RL, as it offers a learning algorithm that does not depend on a potentially flawed and misleading model of the environment.
We reasoned that tracking separate model-free and model-based RPEs would allow the relative reward reliability of each system to be considered for the purpose of arbitration. Indeed, simulations show that the model-based system's reward predictions are significantly less reliable than those of the model-free system when rewards are coupled to first-stage stimuli (see 
Figure 1E
). Under this rubric, we hypothesized that elevated model-based RPEs induced during periods of stimulus-contingent rewards would promote model-free control. And indeed, as illustrated in 
Figure 2E
,H, participants were significantly more model-free in their choices, as reflected by a significant Outcome
(O t−1 ) by Transition (T t−1 ) by Reward contingency (R c ) interaction (O t−1 : T t−1 : R c : β = 0.19, p < 2e − 16).
In summary, each of the three manipulations under investigation prompted behavioral adaptation in line with our hypotheses, and provides additional evidence for the reliability-based arbitration of control. Variation in the task's reward contingency induced a dramatic shift in behavior, with participants moving from a predominantly model-based planning strategy when rewards were coupled to the terminal state to a model-free strategy when rewards were contingent upon the chosen firststage stimulus. Furthermore, variability in both reward magnitude and transition reliability fostered behavioral adaptations as hypothesized, with participants expressing greater model-based control when reward magnitudes were unreliable and when the task's transition structure was more predictable.


Variables pertinent to adaptive arbitration interact
Further analyses revealed that the effects of reward contingency, reward magnitude reliability, and transition reliability interact. A significant 5-way interaction indicates that adaptation driven by one manipulation is not independent of the others (β = 0.01, p = 0.005). 
Figure 3
 unravels this interaction, showing that when a given system is preferentially promoted by two of the task conditions simultaneously, the effect of the third task manipulation is reduced. For example, when reward magnitude reliability is low and rewards are state-continent (i.e. both conditions favor model-based control), the effect of transition reliability diminished (t(451) = 22.28, p < 2e − 16). Similar dampened effects are observed for reward magnitude reliability (t(451) = 11.00, p < 2e − 16) and reward contingency conditions (t(451) = 5.53, p = 5.44e − 8, see 
Figure 3C
).
Together, these patterns of adaptation suggest that the variables contributing to the arbitration of control are non-linearly aggregated. We reasoned that these effects are consistent with saturation in a non-linear adaptive control process, whereby the effect of a single variable is reduced if other variables have already drawn the control process toward one of its boundaries. Behavioral adaptation to task variables. A) Basic model-free RL predicts that a rewarded first-stage choice is more likely to be repeated on the subsequent trial, regardless of whether that reward occurred after a common or rare transition. The bar-plot inset illustrates this effect in line with conventional reports from the two-step task. The line plot illustrates the same response pattern but highlights the effects of transition type. B) Model-based prospective evaluation predicts that a rare transition should impact the value of the alternative first-stage option, leading to a predicted interaction between reward and transition probability. C) The observed proportion of trials in which the previous trial's response was repeated, averaged across subjects, reflects a mixture of both modelbased and model-free control. D) The mean proportion of rewarded trials expected from random agents performing the same task (gray), and the mean proportion of rewarded trials experienced by participants (blue). E) The proportion of trial in which the previous choice was repeated during periods of the task in which rewards were state-or stimulus-contingent. Dashed lines depict choice following common transitions, solid lines illustrate choice following rare transitions. F) The proportion of repeat trials during periods of the task when reward magnitude reliability was high or low. G) The proportion of repeat trials during periods of the task when transition reliability was high or low. H) Shift in transition sensitivity indicative of greater model-based control when i) rewards were statecontingent (relative to stimulus-contingent), ii) when reward magnitude reliability was low (relative to high), and iii) when state transition reliability was high (relative to low). Points represent average population mean, error bars are standard error over the population means


Variables contributing to the arbitration of control replicate in an independent sample
We conducted a replication study (study 2; N=226) to assess the generalizability and robustness of the effects observed in our original study. Using the same experimental paradigm and analyses, we collected data from a cohort of participants through the Prolific online subject pool (N=160), and a cohort of participants recruited locally through advertisements (N=66).
As with the M-Turk sample, participants in study 2 performed significantly better than what would be expected from random-choice (p < 1.05e − 22, paired t-test). On average, participants' choices were rewarded on 48.26% (sd = 0.04) of the time as compared to 43.03% (sd = 0.02) with random choices, indicating good task comprehension and engagement. Replicating previous reports and effects from Study 1, participants expressed a mixture of model-free and model-based strategies. Consistent with model-free RL, choice was significantly impacted by reward outcome (β = 0.51, p < 2e − 16), and in accordance with model-based control, we also observed a significant outcome by transition interaction (β = −1.03, p < 2e − 16). Post-hoc analysis of behavior following common transitions showed that participants were significantly more likely to repeat a rewarded action relative to a non-rewarded action 
Figure 3
: Variables pertinent to the arbitration of control interact. A) Shift toward model-based control induced by variation in transition reliability as a function of reward magnitude reliability (low vs. high), and reward contingency (terminal state: blue or chosen stimulus: red). Participants are influenced to a lesser degree when both reward magnitude reliability and contingency combine to promote either model-based or model-free control. B) Shift toward model-based control induced by variation in reward magnitude reliability as a function of transition reliability (high vs. low) and reward contingency (terminal state: blue or chosen stimulus: red). Participants are influenced to a lesser degree when both transition reliability and reward contingency combine to promote either model-based or model-free control. C) The shift toward MB control induced by each variable when the two remaining variables promote different strategies (high conflict) relative to when the two-remaining variables promote the same strategy (low-conflict). Each of the three variables manipulated during the task have a stronger influence over behavior when the other variables are not both promoting the same strategy. Points represent mean proportions across participants, and errorbars depict standard error of the participant mean scores.
(β = 1.45, p < 2e − 16). Furthermore, participants were significantly less likely to repeat a rewarded action relative to a non-rewarded action following a rare transition (β = −0.50, p = 2.3e − 10), indicating a somewhat larger MB bias overall than reported in our original study.
Mirroring the effects reported from study 1, response patterns were sensitive to variation in the task's reward magnitude reliability, with participants expressing greater model-based control when reward magnitude reliability was low (O t−1 : T t−1 : M r : β = −0.09, p = 1.26e − 12). We also identified a trending effect of transition reliability, with more prominent model-based planning when state transitions were more reliable (O t−1 : T t−1 : T r : β = −0.02, p = 0.09). Finally, replicating reward contingency effects observed in study 1, participants were more model-based when rewards were coupled to the task's terminal states relative to when they were contingent upon the chosen stimulus (O t−1 : T t−1 : R c : β = −0.07, p = 0.005).
The higher-order interaction identified in the original data set also emerged in study 2. Behavioral adjustments associated with variability in reward magnitude, reward contingency and state transition reliability were not independent of one another, as demonstrated by a significant 5-way interaction (β = 0.03, p = 0.005). Further replicating effects observed in the original study, the effect of transition reliability was diminished when both reward magnitude and contingency promoted the same strategy (t(225) = 11.68, p < 2.2e − 16). These effects were also reflected in the effects of reward contingency (t(225) = 5.97, p = 9.35e − 9), and reward magnitude reliability (t(225) = 10.46, p = 2.2e − 16).
In summary, variation in the task's reward magnitude, state transition, and reward contingency induced consistent behavioral adjustments across both data samples. This offers further evidence for the role of model-free RPE, model-based RPEs and SPEs in guiding a reliability-based mechanism for the arbitration of control.


Individual differences in control and behavioral adaptation
Given the consistency of effects across Studies 1 and 2, we pooled both data sets so as to achieve a sufficiently large sample size for the purpose of investigating individual differences in control and the arbitration of control. Our task design offers multiple measures through which behavioral control can be considered. We leverage this rich dimensionality by conducting a cluster analysis that considers the influence of the previous trial's events on choice and response time, in addition to summary measures characterizing overall sensitivity to outcomes and transitions (see supplement for details). As illustrated in 
Figure 4A
, this analysis identified four groups of participants. A projection of cluster membership onto model-based and model-free dimensions of control as extracted from the regression model's random effects reveals a cluster dominated by model-based control (blue), model-free control (purple), a mixture of both (red), and a group that did not principles of RL (green).
Participants within each cluster expressed distinct choice patterns (see 
Figure 4B
). Participants captured within the Model-Free cluster repeat rewarded choices and switch away from non-rewarded choices regardless of the previous trial's transition type. Conversely, participants assigned to the Model-Based cluster repeat non-rewarded actions and switch away from rewarded actions following a rare transition. Participants within the bounds of the Mixture cluster fall somewhere in-between the Model-Based/-Free groups, while the Non-RL group does not exhibit a robust response to reward outcomes in general. Consistent with this illustration, random effect estimates of model-free (main effect of Outcome) control increase linearly across groups (Non-RL < MB < Mix < MF β = 0.48, p < 2e − 16) whereas measures of model-based control increase linearly across groups (Non-RL
< MF < Mix < MB, β = −0.69, p < 2e − 16).
Having identified populations with distinct propensities to engage in model-based/-free control, we next examined the arbitration of control. As illustrated in 
Figure 4C
, the Model-Based group exhibited robust behavioral adjustment in response to changes in the task's reward contingency (State vs. Stimulus, β = −0.16, p < 2e − 16), reward magnitude reliability (Low vs. High, β = −0.14, p < 2e − 16), and transition reliability (High vs. Low, β = −0.03, p = 0.02). Thus, behavioral evidence suggests that the Model-Based group was sensitive to all three task manipulations, expressing greater model-based control when model-based RPEs or SPEs were minimal, and when model-free RPEs were largest.
Analysis of the Mixture group revealed similar patterns of behavior. Variation in the task's reward contingency (State vs. Stimulus, β = −0.22, p < 2e − 16), reward magnitude reliability (Low vs. High, β = −0.12, p < 2e − 16), and transition reliability (High vs. Low, β = −0.03, p = 0.02) all induces significant modifications to behavior. Thus, as observed in the Model-Based group, Mixture group participants adapted in accordance with a reliability-based arbitration mechanism, expressing greater model-based control when model-based RPEs or SPEs were minimal, and when model-free RPEs were largest.
Next, we assessed adaptive control in the Model-Free group. Unlike Model-Based and Mixture groups, the Model-Free group was insensitive to variation in both the task's reward contingency (β = −0.01, p = 0.38) and transition reliability (β = −0.01, p = 0.45). However, they did adjust their response patterns in accordance to variation in the task's reward magnitude reliability (β = −0.05, p = 0.002). Thus, in contrast to Model-Based and Mixture groups, the Model-Free group only tuned their response pattern according to variation in model-free RPEs by adopting a more pronounced model-free strategy when reward magnitude was most reliable.
Finally, we consider behavioral adaptation in the Non-RL group. Participants in the Non-RL group were sensitive to variation in both the task's reward contingency (β = −0.05, p = 2.44e − 5), and reward magnitude reliability (β = −0.04, p = 1.22e − 4), but did not respond to changes in transition reliability (β = 0.01, p = 0.23). We can only speculate on the computational mechanisms motivating adaptive behavior in this group of participants; however, these patterns hint at a selective sensitivity to reward which could be driven by impulsive behavior that is not well characterized by RL models 
[34]
.
In summary, we identify a range of behavior across our sample population. Some individuals deployed a strategy consistent with model-based planning, while others adopted a strategy in line with model-free response. Still others exhibit a mixture of both model-based and model-free control, while a subset of participants do not appear to deploy a consistent RL-based strategy. The arbitration of control differed across groups. Model-Free participants were only sensitive to variability in the task's reward magnitude reliability, suggesting that model-based prediction error signals were either incapable of biasing the arbitration process or they were not generated in the first place. Conversely, both Model-Based and Mixture group participants adapted their behavior in response to all three manipulations embedded in the task design indicating that both model-based and model-free system reliability are factored into the arbitration scheme. 


Validation of a reinforcement learning account of behavior
Prior to probing the computational origins of behavioral adaptation induced by variation in task parameters, we first validate the use of a RL framework to model human behavior during this task. Previous reports have typically employed mixed RL models to characterize behavior on variants of the two-step task 
[15,
16,
32,
25,
27]
. However, concerns have recently been raised suggesting that this RL approach does not provide a satisfactory description of behavior because it relies on an assumed characterization of the task that may not reflect the representation employed by participants 
[30,
31]
. This line of argument has been supported by findings showing that RL models do not explain task behavior as well as logistic regression models that lack any assumption about the learning mechanisms or task representation 
[31]
.
In light of this, we interrogated our data-set to assess the degree to which behavior could be better explained by RL models over and above a non-learning logistic regression model. To do so, we considered two logistic regression models: 1) Baseline: A logistic regression model that predicts the probability of repeating the previous trial's choice according to the previous trial's reward outcome (Reward vs Non-Reward), transition type (Rare vs Common), and reward magnitude, and 2) Baseline + Condition: Augments the Baseline model to include reward contingency, reward magnitude reliabil-  
Table 1
: Model comparison using AIC. Participant data were fit using a baseline logistic regression model that considered the previous trial's reward outcome (win/no-win), reward magnitude , and transition type (rare/common). Baseline + condition model augmented the Baseline model to allow effects of the previous trial's events to differ across levels of reward contingency (state/stimulus), reward reliability (high/low), and transition reliability (high/low). Data were also fit using a Fixed hybrid model-based/-free RL model, and a Flexible mixture RL model that allowed the relative balance of model-based/-free control to vary across task conditions. Lower AIC scores indicate better model fits.
ity, and state transition reliability as interaction terms to allow for conditional variation in the effects of reward and transition on behavior. We also consider two computational model-based/model-free mixture RL models that mirror the spirit of the regression models: 3) Fixed mixture RL: Includes model-based and model-free learning agents that are both able to contribute to behavior according to a fixed mixture between the two, and 4) Flexible mixture RL: augments the Fixed mixture model to allowed the model-based/-free mixture to differ across levels of reward contingency, reward magnitude and transition reliability. Overall, the Akaike Information Criterion (AIC) favored the flexible mixture RL model over the regression models, as well as the fixed mixture RL model (see 
Table 1
). The flexible mixture model was superior for all sub-groups (MB, Mixture, MF, and Non-RL), indicating that participant's choices were more consistent with a flexible learning mechanism than a regression model that only considers a limited scope of events from the preceding trial. Interestingly, the flexible mixture model offers the best fit to the Non-RL group, suggesting that despite an idiosyncratic response pattern overall, there is nonetheless some sub-structure that resonates with the general principals of RL.
It is important to note that our mixed RL model includes two learning time-scales. Both the modelfree and model-based systems have the capacity to learn over short and long time-scales simultaneously, allowing the model to accommodate both rapid adaptation and efficient statistical integration of past experience 
[35]
. Indeed, a mixture model limited to only fast or slow learning mechanisms did not fit the data as well as a model that included both (Laplacian BIC approximations for: fast+slow learning model=104587, slow learning only model=114508, fast learning only model=105017. Lower score indicates better fit). These results indicate that computational RL offers a better explanation of the data than a generic non-learning regression model once different time scales are accounted for, affirming that the model-based/-free RL framework is a valid tool for interrogating and explaining human learning and decision making during this task.


Behavioral adaptation reflects explicit shifts in control beyond shifts in policy preference
Variation in the task's reward contingency, reward magnitude, and state transition reliability induced shifts in behavior consistent with re-weighting the relative balance of model-based/-free control according to each system's predictive reliability. However, systematic changes in the task environment also impact the policies (i.e. preferences) of the controllers themselves. In some circumstances, this could cause one system to have stronger preferences than the other. For example, simulations show that a model-based controller's preferences are more pronounced when rewards are coupled to the terminal states than when rewards are contingent upon the first-stage options (β = 0.05, p < 2e − 16), opening the possibility that behavior consistent with reliability-based arbitration may simply reflect shifts in each system's preferences. We use a model comparison approach to address this question. We reasoned that a mixed RL model in which the balance of model-based/-free control was fixed across all conditions (but was freely fit for each individual) would capture variance associated with conditional effects on the controller's policies alone. For comparison, we defined a flexible mixed RL model to capture additional variance attributable to a conditional re-weighting of model-based/-free control. This flexible mixture model augmented the fixed mixture model with three additional parameters, each offering a conditionally specific controller re-weighting as a function of reward contingency (ω Rc ), reward magnitude reliability (ω Rr ), and state transition reliability (ω T r , see methods for details). This model does not include an arbitration mechanism per se; rather, it accommodates dynamic arbitration by allowing separate parameters to re-balance model-based/-free control as a function of each reliability manipulation in the task.
As illustrated in 
Figure 5A
, both Fixed and Flexible mixture models reproduced the average effects of reward outcome and transition type well. However, model comparison (χ 2 test for nested models), showed that 74% of participants were best fit by the Flexible mixture model over the Fixed mixture model (see 
Figure 5B)
, demonstrating that the majority of participants adapted their behavior in response to variation in the task's conditions above and beyond what could be accommodated by changes in the controller's preferences themselves.
The limitations of a Fixed mixture RL model can be clearly seen when considering the model's ability to reproduce conditional patterns of adaptations observed in the data (see 
Figure 5C
). Although the Fixed mixture model exhibits some degree of conditional adaptation (red points), the model does not capture the conditional adaptation reflected in the behavioral data as well as the Flexible mixture model (blue points). We quantify the predictive discrepancy between models by considering each model's ability to reproduce the transition sensitivity expressed by participants. This was accomplished by summarizing each participant's mean transition sensitivity across the three task conditions, then assessing the behavioral variance explained by the Fixed model and the Flexible model. We also consider a set of reduced Flexible models that drop one of the conditional re-weighting parameters while retaining the remaining two in order to assess the specific role of each arbitration parameter in isolation. The full Flexible model had a clear predictive advantage, better capturing the transition sensitivity variance associated with the task's reward contingency (Full R 2 = 0.82; Full-ω Rc R 2 = 0.13; Fixed R 2 = 0.06), reward magnitude reliability (Full R 2 = 0.41; Full-ω Rr R 2 = 0.11; Fixed R 2 = 0.06), and transition reliability (Full R 2 = 0.42; Full-ω T r R 2 = 0.08; Fixed R 2 = 0.05).
Given that the Flexible mixture model offered the best fit to the majority of participants, we examined the model's parameter estimates to better understand how the model fostered adaptive control. The model's baseline mixture parameter (ω M F ) captures individual differences in overall tendency toward model-free control, and thus is expected to vary as a function of which cluster an individual belonged to. As illustrated in 
Figure 5D
, the baseline mixture parameter aligned well with the group clusters previously identified, and was highly correlated with regression model estimates indexing levels of model-free response (main effect of outcome: β = 0.7, p = 8.02e−10) and model-based control (Outcome × Transition: β = 0.98, p < 2e − 16). Pairwise group comparisons of the baseline mixture parameter (ω M F ) show that estimates for the Model-Free group were significantly higher than the Mixture group (β = 1.17, p = 4.18e − 15), which were in-turn significantly higher than those of the Model-Based group (β = 1.13, p < 2e − 16). In sum, the estimated model mixtures aligned well with the computationally-agnostic measures of model-based/-free control, lending additional support to the grouping and the validity of the computational RL model.
The parameters responsible for adapting the controller mixture according to the task's conditions aligned with model comparison results and hypothesized effects of the reliability-based arbitration (see 
Figure 5D
). The parameter responsible for modulating the model-based/-free mixture as a function of the task's reward contingency (ω Rc ), was significantly less than zero overall (t(677) = −6.73, p = 3.47e − 11), indicating greater emphasis on model-based control when rewards were coupled to the task's terminal states. Parameter estimates for both Model-Based (t(187) = −3.82, p = 1.81e − 4) and Mixture groups (t(229) = −6.23, p = 2.22e − 9) were significantly less than zero (more model-based during state-contingent rewards), whereas the Model-Free group estimate did not differ from zero (MF: t(122) = −0.97, p = 0.33).
The model parameter responsible for adapting the MB/MF mixture according to reward magnitude reliability (ω Rr ) was also significantly less than zero overall (t(677) = −5.8, p = 1.0e − 8), indicating greater emphasis on MB control when reward magnitudes were less reliable. All three RL groups had a mean parameter estimate significantly less than zero, suggesting that all individuals engaged in RL were sensitive to variation in model-free RPEs and adapted their behavior accordingly (MB: t(187) = −5.49, p = 1.3e − 7, Mix: t(229) = −5.36, p = 2.1e − 7, MF: t(122) = −2.62, p = 0.01 ). The transition reliability mixture parameter (ω T r ) did not differ significantly from zero (t(667) = −0.37, p = 0.71) overall, nor did it differ from zero for any of the sub-groups (all p-values > 0.1). Finally, we note that the Misc group's arbitration parameter estimates did not differ from zero (all p-values > 0.05)), offering further evidence that these participants are not well described by RL models, and did not appear to adapt their behavior consistently according to model-based/-free system reliability.
In summary, these results suggest that behavioral adaptation is driven in part by a system's predictive reliability. A proportion of this adaptation can be traced back to changes in the preferences expressed by model-based/-free controllers themselves. However, our results also show that conditionally induced biases in each system's preferences cannot accommodate the observed behavioral changes alone. Model comparison, captured variance and computational parameter estimation both support the presence of a reliability-based arbitration mechanism that adapted behavior to emphasize the most reliable system of control in accordance with the task at hand.


Do individuals switch between systems of control or representation of the task?
Our results show that individuals adapt their behavior in response to changes in the environment, wherein behavior is modified according to the predictive reliability of both model-based and modelfree systems of control. However, we also consider the possibility that behavioral adaptation reflects the persistent use of model-based planning that relies on different representations of the task. For example, rather than opting to deploy a model-free strategy during periods of stimulus contingent rewards, participants could update their representation of the task to better reflect the environment's reward structure while continuing to use a model-based strategy. Should this be the case, model-based and model-free control would be indistinguishable when considering choices alone; however, reaction time (RT) data offer potential insight into the underlying computational mechanisms driving behavior. Given the generally conceived notion that model-based planning is effortful, deliberate, iterative and therefore slower than model-free response, it would be expected that RTs associated with modelbased control would be slower than model-free RTs. However, previous studies using similar multi-step trial based RL tasks have revealed that model-based and model-free decisions are made at different timepoints 
[36]
. Model-free choices exhibit gaze patterns consistent with a decision-making process that is initiated once stimuli have been presented. In contrast, model-based planning is initiated at the time of outcome, meaning that a choice has already been computed by the time the subsequent trial's stimuli are presented. As such, we hypothesized that acting on a MB plan would be associated with faster RTs relative to relying on a stimulus-driven MF response. Given that first-stage stimulus locations were consistent across trials, this pattern of behavior is further facilitated in our task design as both choice and motor preparations could begin at the time of outcome delivery.
We conducted an exploratory analysis investigating the relationship between RTs and behavioral adaptation by first examining individual differences in RT and strategies of control. We quantify individual measures of model-based and model-free control using random effect estimates extracted from the mixed-effects regression used to model choice behavior (Outcome and Outcome × Transition random effect estimates). As hypothesized, and consistent with previous work 
[36]
, greater expression of model-based planning was associated with faster RTs (β = 0.04, p = 2.25e − 9) and more prominent expression of model-free response was associated with slower RTs (β = −0.02, p = 0.05). As illustrated in Figure6A, the Model-Based group exhibited faster RTs consistent with action planning initiated at the time of the previous trial's reward outcome, whereas the Model-Free group exhibited slower RTs consistent with stimulus driven value comparison.
Building on this, we aimed to assess whether patterns in the RT data provide further insight as to whether participants adapted their behavior by shifting to a different system of control or by maintaining a model-based strategy while adjusting their model of the task. We focused on the reward contingency manipulation, which induced prominent behavioral adaptation in both Model-Based and Mixture groups, and could be rationally motivated by either the arbitration of control or by revising the task representation. Relative to periods of state-contingent rewards that require planning over the environment's entire transition structure, we reasoned that persistent use of model-based planning during periods of stimulus-contingent rewards would be expedited by the shallow planning depth required to inform a decision. Conversely, we reasoned that shifting to adopt a stimulus-driven modelfree decision strategy during periods of stimulus-contingent rewards would result in slower RTs. Thus, 
Figure 5
: Computational mechanisms contributing to adaptive of control. A) Average effect of reward outcome (win/loss) following common/rare transitions observed in behavior (black) and as reproduced by Fixed (red) and (Flexible) mixed RL models. B) The population distribution χ 2 likelihood ratio test and critical value (red-line) showing that the inclusion of explicit conditional mixture re-weightings offers a better fit to the data for 74% of the sample population. C) Conditional shifts in transition sensitivity reproduced by the Fixed and Flexible mixture models relative to behavioral data. The Fixed mixture model fails to capture behavioral adaptation as well as the Flexible mixture model that is capable of explicitly re-weights the relative emphasis of model-based/-free control. D) Parameter estimates from the Flexible arbitration model illustrating the average baseline mixture of model-based/free control (ω M F : emphasis given to model-free control), the shift in model-free control induced by state-contingent rewards (ω Rc ), by low reward reliability (ω Rr ), and by more reliable state transitions (ω T r ). Points represent the group mean, and bars depict standard error.
we interrogate RT adjustments induced by variation in the task's reward contingencies as indicators of either the arbitration of model-based/-free control or adjustments to the model of the task.
Examination of Model-Based and Mixture group RTs across levels of reward contingency data did not reveal a consistent change in RT overall (β = 0.01, p = 0.9). However, a significant Group x Condition interaction indicated that conditional shifts in RTs differed between groups (β = 0.47, p = 0.02). Post-hoc analysis of the Mixture group participants revealed a modest trend toward slower RTs when rewards were coupled to first-stage stimuli, consistent with increased reliance on stimulusdriven model-free control (β = 0.22, p = 0.12). By contrast, the Model-Based group were significantly faster when rewards were stimulus contingent, consistent with planning over a simplified task model (β = −0.24, p = 0.05). Consequently, our findings suggest that both the arbitration of control and model adaptation phenomena are at play in our data. Mixture participants exhibit RT patterns consistent with switching between systems of control as a function of changes in system reliability induced by variation in the task's reward contingency. On the other hand, Model-Based participants exhibit RT patterns consistent with persistent adaptive planning over distinct representations of the task at hand.
Although we have interpreted changes in RT induced by variability in the task's reward-contingency as indicating the arbitration of control in the Mixture group, it is also possible that these patterns could simply indicate some degree of confusion or disengagement induced by volatility in the task structure. However, further analysis of RT patterns suggests that this is not the case. We examined RTs on trials following a rare transition where participants switched to the alternative option as an opportunity to dissociate model-based/-free strategies while minimizing potential confounds associated with response hysteresis 
[25,
29]
. The Model-Based group was consistently faster to switch to the alternative option following a rewarded relative to an unrewarded rare transition regardless of the task's reward contingency (Outcome: β = −0.05, p = 2.37e−7; Outcome × Condition: β = 2.0e−3, p = 0.8). Conversely, the Model-Free group was consistently faster to switch to the alternative option following an unrewarded rare transition (Outcome: β = 0.04, p = 3.57e − 5; Outcome × Condition: β = −0.01, p = 0.26). Thus, both Model-Based and Model-Free groups were faster to respond when executing the more likely action according to their respective strategies. Critically, as illustrated in 
Figure 6C
, the Mixture group adopted distinct RT patterns across levels of the task's reward contingencies, as highlighted by a significant Outcome x Condition interaction (Outcome: β = −1.07e − 2, p = 0.16; Outcome × Condition: β = −2.30e − 02, p = 0.0025). Mixture group participants mirrored the MF group's RT pattern during periods of stimulus-contingent rewards, and were significantly faster to switch their response after an unrewarded rare transition (t(168) = 2.28, p = 0.02), but switched faster after rewarded rare trials during periods of state-contingent rewards (t(163) = −1.8, p = 0.07). Thus, Mixture group participants express the telltale signatures of model-based and model-free control during periods of State and Stimulus contingent rewards respectively, suggesting that response slowing is indicative of increased model-free control over behavior and not simply the result of disengagement or confusion.
In summary, although both Model-Based and Mixture group participants adapted their behavior according to the task's reward contingencies, their response time patterns suggest that they did so for different reasons. Consistent with shallow planning, the Model-Based group were faster to respond when only first-stage stimuli had to be considered. Mixture group participants, on the other hand, appear to have adapted by dynamically shifting control between model-based planning when rewards were coupled to the task's terminal states and a model-free stimulus-driven strategy when rewards were determined by the chosen stimulus.


Phenotypes of control
Clustered participant groups expressed distinct choice patterns and response times consistent with variation in the deployment of model-based/-free control. To further gauge dimensions of commonality within these groups and dissimilarity across them, we examined clinically relevant self-report scores that have been linked to variability in RL mechanisms and behavior 
[27,
34]
. Participants were asked to complete a set of self-report surveys, which included measures of obsessive/compulsive behavior (OCI-R: Obsessive-Compulsive Inventory-Revised 
[37]
), impulsivity (BIS: Barratt Impulsivity Scale 
[38]
, anxiety (STAI: State-Trait Anxiety Inventory 
[39]
), and depression (BDI: Beck's Depression Inventory 
[40]
). Importantly, these self-report scores were not included as features in the clustering process; and thus, provide an opportunity to investigate group delineations as cognitive phenotypes that extend beyond individual differences in RL quantified by our multi-step decision task.
We focus on impulsivity and compulsivity as measures that have been associated with variation in RL processes 
[27,
34]
, and leverage additional measures to control for variance associated with depression and anxiety. Replicating previous findings, one-way ANOVA revealed significant group differences in self-reported compulsivity (F(3, 546) = 8.10, p=2.81e-5), and impulsivity (F(3, 546) = 3.60, p=0.01). Follow-up analyses expose revealing differences across clustered groups of participants. Consistent with previous findings that linked compulsive behavior and deficits in goal-directed planning 
[41,
42]
, groups that relied on a model of the task (e.g. Model-Based and Mixed) reported signifi- 
Figure 6
: Cluster group variation in RT and self-report scores. A) Average RTs of each group. Individuals that adopt a dominant model-based strategy respond faster, on average, than individuals exhibiting behavior consistent with model-free control. B) RT on trials with a response switch following a rare transition. Consistent with model-based planning, negative Win-Loss response time differences indicate that Model-Based group participants were faster to switch following a rewarded rare transition. In line with model-free response, positive Win-Loss response time differences show that Model-Free group participants were faster to switch following an unrewarded rare transition. Mixture group participants were faster to switch following unrewarded rare trials, mirroring the Model-Free group during periods of stimulus-contingent rewards; however, they were faster to switch following a rewarded rare transitions when rewards were state-contingent, consistent with differential engagement of modelbased and model-free control across reward contingency conditions. C) Compulsivity and impulsivity self-report scores across clustered groups. Model-Free and Non-RL groups reported significantly levels of compulsive behavior, whereas Non-RL participants report significantly higher impulsivity scores. Points represent the group mean, and bars depict standard error. cantly lower compulsivity scores than both Model-Free and Non-RL groups (see table for statistical comparisons). Furthermore, investigation of individual differences via regression analysis examining the relationship between compulsivity scores and measures of model-based/-free control shows that compulsivity was related to reduced model-based planning (β = −2.66, p = 0.002), but not model-free response (β = 0.69, p = 0.59). Thus, elevated compulsivity may not only be associated with impaired model-based control, but may also indicate a reliance on esoteric non-RL strategies as well.
Additionally, the Model-Free group reported significantly lower impulsivity scores than the Non-RL group (β = −3.28, p = 0.01), suggesting that Model-Free participants constitute a phenotype distinct from the Non-RL group. Analysis of individual difference in the relationship between impulsivity scores and measures of model-based/-free control shows that impulsivity was associated with reduced model-based planning (β = −1.9, p = 0.03) and reduced model-free response (β = −4.1, p = 0.002), consistent with previous reports suggesting that impulsivity is coupled with non-RL behavior 
[34]
. This underlines the importance of not designating model-free behavior as simply 'non model-based', as doing so risks inappropriately conflating model-free control with other strategies not well characterized by model-based RL 
[31]
.
In summary, self-report measures point to consequential distinctions among groups clustered according to RL attributes. These findings show that the Model-Free group can be distinguished from the Model-Based and Mixture groups in terms of associated compulsivity measures, and that the Model-Free group can also be differentiated from Non-RL via impulsivity measures. Together, this offers evidence supporting a delineation between model-based, model-free and Non-RL strategies, and  


Discussion
Using a novel variant of a multi-step reinforcement learning decision task that systematically manipulated the predictive accuracy of both model-based and model-free decision making strategies, our results offer further evidence in support of uncertainty-based arbitration among systems vying for behavioral control. Although our findings align with prior results from more restricted studies indicating that variation in model-free RPEs and model-based SPEs alter the balance of control 
[26,
43]
, the present findings place these previous results on a more robust footing. By leveraging a large sample to identify individual differences in behavioral control, our results demonstrate that increased expression of model-free behavior driven by either increased model-free reliability or decreased modelbased reliability, is not merely a side-effect of elevated choice stochasticity, confusion, or the adoption of enigmatic task representations. Although a sub-population within our sample do indeed express behavior inconsistent with RL learning, the arbitration process clearly moderates behavioral control across model-based and model-free strategies in a substantial proportion of individuals.
Our results provide new insights into the computational mechanism through which the brain arbitrates between multiple systems competing for control by demonstrating that model-based RPEs also induce strategic adaptation. We examined this by including periods where the probability of reward was coupled to the chosen stimulus (not the terminal state), and thus reflects circumstances where the wrong model may be brought to the task at hand. Behavior adapted in circumstances where the rewards anticipated by the model-based planning system were not realized, with some participants adopting a more appropriate model-free response strategy and others adopting a more appropriate model of the task. Importantly, the predictive reliability of the model-free system was also degraded when rewards were coupled to the chosen stimulus (see 
Figure 1E
). In the absence of an error signal indicating the shortcomings of the model-based system, this reduction in model-free reliability would indirectly promote maladaptive model-based planning. Thus, evidence suggests that participants adapted to variation in the environment according to the predictive reliability of both model-based and modelfree controllers, which suggests that arbitration considers not only how well the state-space transitions have been learned within the model-based system, but also the value expectations of model-based planning.
This finding offers potential insight into the computational function of a model-based RPE signal that has been identified in the human brain 
[15,
31,
44]
. Reward prediction error signals within a tempral-difference RL framework support a value learning mechanism that relies on the history of observed rewards to integrate and track value, and as such, they are not well suited to the rapid flexibility commonly ascribed to model-based control. Our findings suggest that model-based RPEs may not be used for the sake of value learning per say; rather, they could offer a signal indicative of misalignment in the model's value expectations. This signal would provide a measure of model fidelity that goes beyond state transitions to also include motivationally relevant aspects of the environment.
As exemplified by the stimulus-reward contingency condition in our task, model-based planning does not always offer the best course of action, particularly when model misalignment promotes maladaptive behavior. This underlines one of the pitfalls associated with model-based planning and control; namely, that it depends on an accurate model of world. These results suggest that the brain track multiples dimensions of model accuracy not only in support of learning, but also as a means to adaptively adjust behavioral control.
Our findings also addressed the important issue of distinguishing between the dynamic arbitration of control and the impact of uncertainty associated with a controller's policy. A behavioral controller (e.g. the model-based system) will not exert a strong influence on behavior if it is uncertain about what choice to make next, particularly if an alternative controller has a strong preference. This imbalance in system uncertainty could allow the more confident system to dominate behavioral control without requiring system-wide monitoring. Indeed, our finding show that some degree of adaptive control emerges naturally without the need for dynamic arbitration, as exemplified by adaptation expressed by the Fixed mixture model (see 
Figure 5
)C. However, a comparison between Fixed and Flexible models of arbitration showed that the majority of participants are best fit by a model that dynamically arbitrates between controllers. This raises the question of what advantage an explicit arbitration mechanism offers. One possible explanation is that systematically biasing control to favor the most reliable system helps to diminish the impact of corruptive noise contributed by unreliable controllers. In other words, a purely passive arbitration mechanism would produce noisier decisions overall, whereas dynamic reliability-based arbitration promotes decision signals from the most reliable system and thus sharpens the overall behavioral output of the system. Second, circumstances may arise where a controller is uncertain which option is best but has high predictive accuracy nonetheless (e.g. candidate options are equally good or bad). Unless an alternative system is both reliable and more certain of the best option, it would seem a poor course of action to demote an otherwise reliable strategy simply because of a difficult decision was encountered. Thus, we propose that dynamic arbitration contributes to a more robust and adaptive overall system of control better suited to a complex and volatile environment.
Examining the variables pertinent to the arbitration of control allowed us to identify four categories of strategic behavior in our sample population. A proportion of individuals relied heavily on a modelbased planning strategy. These participants were sensitive to changes in both the task's transition and reward structure as expressed by more robust model-based behavior during periods of statecontingent rewards and when state transitions were reliable, indicating that they made use of an internal model of the task. Furthermore, these participants were faster to respond, indicative of outcome-induced response planning 
[36]
. In contrast, a distinct group of participants deployed a strategy consistent with model-free control. This was characterized by a robust sensitivity to reward outcomes but an insensitivity to variation in the task's transition and reward contingency structure. These participants were significantly slower to respond, consistent with a stimulus-induced comparative value-based decision making process. However, we also identified a group of participants that bear the hallmarks of both model-based and model-free control. This is characterized by robust sensitivity to both reward and transition structure, and response times that indicate these participants alternated between outcome-induced response planning when rewards were contingent upon the terminal states and stimulus-driven MF value comparison when rewards were determined by the chosen stimulus. Finally, our analysis also identified a fourth group of participants that were not well characterized by RL models. These participants expressed a marginal sensitivity to reward outcomes, were characterized by a propensity to switch between options regardless of previous events, and were the slowest to respond on average. These patterns are consistent with general task disengagement, and/or a misconstruing the task structure. Importantly, identifying and isolating these participants that were poorly described by either model-based or model-free RL models allowed us to examine behavior more readily explained by RL models.
These distinct patterns of control speak to broader behavioral phenotypes. Individuals in the Model-Free group reported higher compulsivity scores consistent with previously reported findings 
[42,
27]
. However, individuals identified as Non-RL also reported high compulsivity scores, suggesting that more compulsive individuals may be less likely to deploy behavior consistent with the appropriate model-based planning strategy either because of a dominant model-free system or because their model-based plans rely on idiosyncratic representations of the task at hand. Importantly, measures of impulsivity show that Model-Free individuals are clearly distinguishable from Non-RL participants, suggesting that the robust reward learning and and value-guided behavior necessary for RL is compromised in individuals with high-impulsivity 
[34]
. These findings help to validate the distinction between the Model-Free RL, Non-RL, and individuals that rely on an accurate model of the task (i.e Model-Based and Mixed strategy groups) as they appear to map onto meaningful and stable individual differences.
Clear evidence of model-based and model-free control has proven to be difficult to pin down. For example, a model-based system may rely on an inappropriate task representation to yield behavior more consistent with model-free stimulus-driven response 
[31]
, or a model-free system could utilize a rich state-space comprised of temporally extended compound stimuli to give the illusion of modelbased planning 
[30]
. Our results highlight this issue, but also offer some clarification. Specifically, a subset of the sampled population relied heavily on a model-based planning strategy to the extent that they maintain model-based control in circumstances where even a simple model-free strategy would suffice. This is reflected by response speeding during periods when rewards are coupled to the chosen stimulus, suggesting that these individuals adapt to variation in the environment by updating their model of the task, and not by arbitrating among systems for control. On the other hand, a group of individuals were characterized across multiple dimensions as expressing behavior consistent with model-free control. They were insensitive to state transitions (rare vs common) but highly sensitive to past rewards, were insensitive to changes in the task's transition and reward contingency structure, and their response times suggests that they employed stimulus-driven value comparison to guide their behavior. Thus, we conclude that the most parsimonious explanation of our data is that Model-Free participants are indeed engaged in a decision making process that relies on stimulus-driven model-free value comparison, and are distinct from individuals engaged in MB control, be it using a sound or flawed representation (e.g. Non-RL) of the task at hand. Leveraging additional points of variability across individuals by considering both choice and response time data from a task that systematically varied conditions to favor either model-based or model-free control allowed us to segregate and identify groups of individuals expressing distinct response strategies across multiple dimensions simultaneously.
Although this study describes multiple convergent behavioral patterns consistent with a reliabilitybased arbitration mechanism in the brain, a number of open questions remain. Firstly, our findings did not rely on a computational model of the arbitration mechanism itself; rather, we employed a conditionally prescribed controller re-weighting to quantify behavioral adaptation without specific assumptions about the implementation of the arbitration process itself. In future work it will be important to adapt and augment existing dynamic models of arbitration (e.g. 
[26]
) to accommodate the role of modelbased RPEs along with other measures of system reliability, as well as variation across individuals in the degree to which these different arbitration processes manifest, as found here. Secondly, it will be important to look in the brain to examine the relationship between the individual differences in behavioral strategies and the neural signals underpinning them. For instance, if a participant relies predominantly on model-based control, can we find evidence for a model-free signal in the brain, or would we only observe model-based computations? Finally, arbitration should be considered within the larger scope of cognitive control. A broader consideration of the cognitive-behavioral phenotyping alluded to in our results could help to shed light on the neural mechanisms underlying individual differences and their relation to mental health.
In summary, our findings help to catalog the variables used to switch between different strategies of behavioral control, advancing our understanding beyond the existing literature by providing evidence for the role of model-based reward prediction errors in the arbitration process alongside model-free reward prediction errors and model-based state-prediction errors. Furthermore, we identify considerable behavioral heterogeneity in the population by including multiple points of variation in our task design to provide a more nuanced characterization of individual differences than previous studies. Our study shows that a singular approach is not appropriate for understanding human reinforcement learning and decision making. Rather, it is necessary to consider variation in strategies used across individuals and in the form of arbitration deployed to mediate flexible behavioral control. The heterogeneity in our findings also helps to resolve an on-going debate in the literature regarding the extent to which behavior is model-based, model-free or a mixture thereof. Our findings show that each of these positions can be true, as some individuals are heavily model-based and appear to switch between different task models as a consequence of changes in the environment, others dynamically switch between a model-based and a model-free strategy, and yet others are model-free and show only slight changes in their behavior as a function of changes in task structure. Embracing this complexity can present methodological challenges, but also offers up new opportunities to better understand how humans adapt to meet changes in their environment and why they may occasionally fail to do so.


Methods


Participants
For the main study, 1028 participants were recruited through Amazon Mechanical Turk (M-Turk), of which 502 participants completed the full two sessions of the task. In order to participate, individuals must live in the United States and be fluent English readers, and have a 95% job success rate from previous M-Turk recruitment. For the replication study, 169 participants were recruited through Prolific, of which 160 participants completed both sessions of the task. The same screening rules used with M-Turk were applied to Prolific recruitment. Finally, 80 participants were recruited from the local Pasadena community to form a virtual 'in-lab' cohort, of which 76 participants completed both sessions of the task. Prior to starting the experiment, all participants gave informed consent for their participation. Experiments were conducted under protocol 19-0916, approved by Caltech's IRB. All participants were paid in monetary format (either through an M-Turk or Prolific account or via a peer-to-peer payment app).


Exclusion Criteria
We define a set of exclusion criteria inspired by 
[45]
 and applied to participants that completed both sessions of the task that are designed to identify careless and/or malicious behavior. This includes:
• Not responding on over 10% of trials.
• Selecting the same option excessively, quantified as response variance less than 3 standard deviations below the sample mean.
• Switching between options excessively, quantified as a response auto-correlation less than 3 standard deviations below the sample mean.
• Poor task comprehension, quantified by repeating the task instructions more than 5 times.
These criteria captured 50 participants in the main M-Turk study, resulting in an analysis sample size of N=452. No participants in the Prolific or virtual in-lab studies were removed due to poor response (total N=169 and N=76 respectively).


Experimental Procedure
All M-Turk, Prolific and virtual 'in-lab' participants completed the same version of the 2-step task through a web-based portal, which consisted of two sessions (154 trials per session). In order to maximize the similarity between the online and virtual 'in-lab' experience, participants in the virtual 'in-lab' group performed the experiments using the same online platform, but were also asked to communicate with experimenters directly in one-on-one communications via email or phone calls in order to guarantee that data was not produced by online bots and that participants were focused on the task at hand. At an arranged time an experimenter would contact the participant via phone to review the online consent form and talk through the task instructions. Upon completion of the task, the experimenter and participant would call once more to debrief.
All participants first read through the task instructions, which were followed by a set of comprehension test questions. If any questions were answered incorrectly participants were forced to repeat the instructions and comprehension test questions until all questions were answered correctly. Comprehension questions assessed participant knowledge on A) the general task structure, B) the transition structure, and C) reward outcome and magnitude structure. Upon correct response, participants were given further confirmatory information regarding the task design (e.g. 'You will earn some number of points for each gem, but that number of points is completely random and should not influence your choice'). After successfully completing the instructions, participants were given 20 practice trials to provide some exposure to the range of transition and reward probabilities in the task. The set of comprehension questions probing the participants' knowledge included:
• How many planets are there?
• How many mines are there on each planet?
• Which planet does the yellow ship usually land on?
• Which planet does the blue ship usually land on?
• How many points is a mined rock worth? (0 points vs. 1-100 points)
• How many points is a gem worth? ( 0 points vs. 1-100 points)
Participants completed self-report questionnaires through Qualtrics online surveys. Participants were randomly assigned a sub-set of questionnaires to complete following each session of the task. This includes the Obsessive-Compulsive Inventory-Revised (OCI-R, 
[37]
), Barratt Impulsivity Scale (BIS 
[38]
), State-Trait Anxiety Inventory, with state-measures collected at each session (STAI: 
[39]
), and Beck's Depression Inventory (BDI: 
[40]
).


Task design
Participants were asked to earn as many points as they could by selecting one of two spacecrafts (firststage options) to visit one of four different mining operations (terminal states) located on two asteroids (second-stage states). Points earned could be exchanged for a performance bonus awarded once the task was complete. Participants were provided with a background story that mining has begun on two nearby asteroids. Participants were instructed that each asteroid had two mines, one located to the North, and the other located in the South, which was depicted visually by presenting the mine eventually landed at above (North) or below (South) the second-stage asteroid. Participants were informed that mining conditions are known to vary, meaning that sometimes a mine might reliably produce valued gems, but other times the same mine might only produce worthless rock.
Participants could choose one of two first-stage options using the keyboard ('d' to select the ship on the left, and 'k' to select the ship on the right). Participants were explicitly told that the yellow spaceship typically landed on the red planet, and the blue spaceship usually landed on the green planet (70% of the time). They were also told that debris fields surrounding the asteroids can make travel somewhat unreliable. Thus, on some occasions the yellow ship will be forced to land on the green planet and the blue spaceship will be forced to land on the red planet (30% of the time). On a given trial, once participants choose a spaceship, they would observe the chosen spacecraft being highlighted and taking off. After a short delay, participants were shown the second-stage state (red or green asteroid), and after another short delay, they would be shown that their craft had landed at either the Northern or the Southern mine. Once at the terminal state, the production of the mine was presented (gem or stone) along with the corresponding value of that outcome (1-100 points for a gem, 0 points for a stone). Participants were explicitly informed that the value of a gem varied unpredictably and could therefor not be learned and should not influence their choice.
The probability of reward followed a structured pattern of strong and weak value differentiation. Periods of strong value differentiation acted to provide clear signatures indicative of task engagement and learning, where one option could be reliably preferred over the other. Conversely, periods of weak value differentiation serve to dissociate model-based and model-free control by allowing more pronounced effects of previous reward and transition to manifest in behavior in the absence of strong preferences for a given option (see 
[46]
 for additional details on the compromising effects of learned preferences). Periods of high and low value differentiation spanned approximately 4-7 and 15-20 trials respectively (see 
Figure 1D
).
Reward magnitude reliability was manipulated by introducing periods of high reliability (reward magnitude drawn from a uniform distribution spanning 10-20 points), and low reliability (reward magnitude drawn from a uniform distribution spanning 30-100 points). Simulations show that significantly larger model-free RPEs are encountered during periods of low magnitude reliability (see 
Figure 1F
). Each period of magnitude reliability lasted 20-26 trials, and condition order was randomized across participants.
Transition reliability was manipulated by varying the probability of transitioning from secondstage state into a given terminal state. During periods of high transition reliability, participants transitioned into a given terminal state 90% of the time, whereas periods of low transition reliability saw participants transition at random (i.e. 50% of transitioning into either of a second-stage state's two terminal states). Simulations show that model-based SPEs were significantly smaller when transition reliability was high (see 
Figure 1G
). Each period of transition reliability lasted 20-26 trials, condition order was randomized across participants, and conditional boundaries were offset from the boundaries of the reward magnitude manipulation to ensure approximate orthogonality across conditions.
Reward contingency was manipulated across sessions of the task. Both sessions of the task included reward magnitude and transition reliability manipulations, with the first session including stimuluscontingent rewards and the second session including state-contingent rewards. This session specific reward-contingency segregation allowed for long stable periods of each reward contingency condition, which was required to distinguish one reward contingency condition from the other due to the sparsity of information unique to each condition (e.g. the multi-trial reward feedback indicative of the task's reward contingency is primarily exposed following a rare transition where the chosen stimulus and the unexpected terminal state may make different predictions according to the underlying reward contingency condition). During the state-contingent rewards condition, the probability of reward was equal for both terminal states associated with a given second-stage state (i.e. p(1|s iT 1 ) = p(1|s iT 2 )). Given that participants did not make a second-stage choice, this ensured that participants would encounter terminal states of approximately equal value having arrived at a particular second-stage state, and therefore, would not experience different levels of model-based RPEs across transition reliability conditions. Simulations show that model-based RPEs were significantly smaller when rewards were contingent upon the terminal state, and that model-based RPEs were more dramatically impacted than model-free RPEs (see 
Figure 1E
).


Statistical Methods


Transition Sensitivity Measure
We quantify the overall effect of reward on behavior according to an Outcome Sensitivity (OS) measure, defined as the difference in the proportion of trials in which the previously-chosen option is repeated following rewarded and unrewarded trials:
OS = p(a t = a t−1 |R t−1 = 1) − p(a t = a t−1 |R t−1 = 0)
(1)
Transition Sensitivity (TS) offers a a measure of model-based control according to the effect of transition type on Outcome Sensitivity. TS is defined in terms of the OS for trials following Common and Rare transitions:
T S = OS C − OS R
(2)
where OS C and OS R are the outcome sensitivity measures pooled across trials following Common and Rare transitions respectively. Thus, all else being equal, a model-free controller will exhibit equal outcome sensitivity regardless of transition type (i.e. OS C ≈ OS R ), and as such, T S ≈ 0. However, model-based control is more inclined to switch to the alternative option after a rewarded rare transition (and more likely to repeat a response after an unrewarded rare transition), meaning OS R < 0, and as such, T S > 0. Thus, T S offers a model and computationally agnostic summary of behavioral adjustments to rewarding outcomes as a function of transition type.


Regression Models
Mixed-effect logistic regressions analysis was conducted using the glmer function in the lme4 R package. We assess the basic effects of reward and transition type as:
Stay t ∼ (R × T ) t−1 + (1 + (R × T ) t−1 |Subject)
(3)
where Stay t ← a t−1 = a t denotes whether the choice from the previous trial was repeated or not, R t−1 is a binary flag indicating the previous trial's reward (+1: rewarded, -1: unrewarded), and T t−1 is a binary flag indicating the previous trial's transition type (+1: Rare, -1: common). All contrasts were implemented using deviation coding (i.e. sum contrasts in R), and random slope and intercepts were estimated for each participant (Subject).
with free parameters ϑ M F −G and ϑ M F −L capturing differential sensitivity to gains and losses 
[49,
50]
. As such, the MF value ultimately used to guide choice is derived according to:
Q M F (s 1 , a i ) net = Q M F (s 1 , a i ) + resp(a i ) * β M F (9)
where resp(a i ) is an indicator function defined as 1 if a i is the same first-stage action chosen on the previous trial (and zero otherwise), and β M F = ϑ M F −G if a reward was encountered on the previous trial, or β M F = ϑ M F −L otherwise. Thus, resp(a i ) * β M F acts as a bias weight that promotes/demotes the previously chosen action according to whether or not a reward was encountered on the previous trial.
The model-based system learns a state transition function and the immediate reward values for each state, which in this case, are zero for all but the terminal states. These transition and reward functions are integrated to compute cumulative state-action values by iterative planning. Given the structure of the current task, this amounts to considering the probability with which a given first-stage action will arrive at either of the two second-stage states, and from there, considering the expected reward and probability of encountering each of the possible terminal.
Participants are explicitly informed of the first-stage choice transition probabilities (e.g. p(s 2a |a 1 ) = p(s 2b |a 2 ) = 0.7, and p(s 2b |a 1 ) = p(s 2a |a 2 ) = 0.3), and that the magnitude associated with a reward outcome is random and should therefore not be learned. Thus, the model-based agent derives value for each second-stage state according to: 
Q M B (s
where term(s T ) is an indicator function defined as 1 if s T is the terminal state visited on the previous trial (and zero otherwise), and β M B = ϑ M B−G if a reward was encountered on the previous trial, or β M B = ϑ M B−L otherwise. Thus, term(s T ) * β M B acts as a bias weight that promotes/demotes the value of the previously encountered terminal state according to whether or not a reward was delivered. Finally, to ensure that behavioral variance wasn't misattributed to either the model-based or the model-free system because of an individual's sensitivity to reward magnitude (despite being instructed that magnitudes are irrelevant), we also include a magnitude bias to define the net model-based value as:
Q M B (s 2i , a) net = Q M B (s 2i , a) * + ρ * mag(s T ) * r mag
(12)
where mag(s T ) is an indicator function defined as 1 if s T is the terminal state visited on the previous trial (and zero otherwise), r mag is the magnitude of the previous trial's reward outcome, and ρ is a free parameter that determines the influence of reward magnitude on the expected value. Given these second-stage expected values, model-based choice values are defined as the expected values of each second-stage state weighted according to the known transition structure:
Q M B (s 1 , a i ) net = p(s 2a |a i ) * Q M B (s 2a , a) net + p(s 2b |a i ) * Q M B (s 2b , a) net
(13)
where p(s 2a |a 1 ) = p(s 2b |a 2 ) = 0.7, and p(s 2b |a 1 ) = p(s 2a |a 2 ) = 0.3. The probability of transitioning from one of the second-stage states (s) into one of the two associated terminal states (s ′ ) is learned according to state prediction error:
δ SP E = 1 − T (s ′ , a, s)
(14)
where a is a no-op action, and the transition probability is updated according to:
T (s ′ , a, s) = T (s ′ , a, s) + α SP E * δ SP E 
(15)
 where the state learning rate is fixed at α SP E = 0.5. We note that the task design did not allow α SP E to be reliably estimated, and varying α SP E across a range of values had no appreciable influence on the model's behavior. To ensure a normalized probability distribution, the probability of transitioning into the terminal state not visited (s ′′ ) is updated:
T (s ′′ , a, s) = T (s ′′ , a, s) × (1 − α SP E )
(16)
The model-based controller learns the probability of reward at each terminal state according to the standard update rule:
Q M B (s i , a i ) ← Q M B (s i , a i ) + α M B * δ i (17) δ i = r i − Q M B (s i , a i )
(18)
where r i is a binary outcome variable denoting win or loss ([1/0]), and α M B is the model-based controller's reward learning rate. The fixed mixture model maintained a static relative weighting between the two systems that was constant across trials and was controlled by a free parameter ω M F that determined the proportion of control granted to the MF system:
Q(s 1 , a i ) net = ω M F * Q M F (s 1 , a i ) net + (1 − ω M F ) * Q M B (s 1 , a i ) net
(19)
The flexible mixture model included three additional parameters that adjusted the relative weighting between the two systems according to the conditions of the task. Specifically, ω Rc allowed the mixture to be adjusted according to the task's current reward contingency condition (terminal state: β Rc = 1 * ω Rc or chosen stimulus β Rc = −1 * ω Rc ). Adjustments to the task's reward magnitude and transition reliability manipulations follow similarly according to ω Rm (high magnitude: β Rm = 1 * ω Rm or low magnitude: β Rm = −1 * ω Rm ), and ω T r (high reliability: β T r = 1 * ω T r or low reliability: β T r = −1 * ω T r ). The mixture weight for a given trial was determined according to:
ω = 1/(1 + exp(−(ω M F + β Rc + β Rm + β T r )
(20)
Thus, ω M F acts as a fulcrum point around which β Rc , β Rm , and β T r can adjust the proportion of model-free control according to the task's current conditions.


Parameter estimation and model comparison
All model parameters were estimated using the Computational Behavioral Modeling (CBM) toolkit, a hierarchical Bayesian inference method that allows for both empirical and prescribed priors to influence parameter estimation 
[51]
. Parameter estimation and model comparison proceeded by fitting each model of interest to each subject separately (i.e non-hierarchically) as we were primarily interested in individual differences in task performance. All parameters were fit by drawing uniformly distributed samples under a normally distributed but uninformative penalization prior (µ = 0, σ 2 = 6.25).
Figure 1 :
1
Task design and dynamics. A) Two-stage decision making task structure. Each first-stage choice (space ship)


Figure 2 :
2
Figure 2: Behavioral adaptation to task variables. A) Basic model-free RL predicts that a rewarded first-stage choice is more likely to be repeated on the subsequent trial, regardless of whether that reward occurred after a common or rare transition. The bar-plot inset illustrates this effect in line with conventional reports from the two-step task. The line plot illustrates the same response pattern but highlights the effects of transition type. B) Model-based prospective evaluation predicts that a rare transition should impact the value of the alternative first-stage option, leading to a predicted interaction between reward and transition probability. C) The observed proportion of trials in which the previous trial's response was repeated, averaged across subjects, reflects a mixture of both modelbased and model-free control. D) The mean proportion of rewarded trials expected from random agents performing the same task (gray), and the mean proportion of rewarded trials experienced by participants (blue). E) The proportion of trial in which the previous choice was repeated during periods of the task in which rewards were state-or stimulus-contingent. Dashed lines depict choice following common transitions, solid lines illustrate choice following rare transitions. F) The proportion of repeat trials during periods of the task when reward magnitude reliability was high or low. G) The proportion of repeat trials during periods of the task when transition reliability was high or low. H) Shift in transition sensitivity indicative of greater model-based control when i) rewards were statecontingent (relative to stimulus-contingent), ii) when reward magnitude reliability was low (relative to high), and iii) when state transition reliability was high (relative to low). Points represent average population mean, error bars are standard error over the population means


Figure 4 :
4
Individual difference in control. A Cluster assignments projected onto estimated indices of model-free and model-based control. Small points represent individuals, and larger circles indicate each cluster's center of mass. The number of cluster was determined using the gap statistic method, which identified four groups as the offering the best description of the data (see inset). B) Propensity to repeat the previous action as a function of the previous reward outcome (Reward / No-Reward) and transition type (Common / Rare) across individuals captured by each cluster. C) Change in transition sensitivity for each cluster group as a function of reward contingency (State -Stimulus), reward reliability (Low -High), and transition sensitivity (High-Low). Points represent mean proportions across participants, and errorbars depict standard error of the participant mean scores.


2i , a) = p(s iT 1 |s i ) * Q M B (s iT 1 , a) + p(s iT 2 |s i ) * Q M B (s iT 2 , a) (10) where Q M B (s iT 1 , a) & Q M B (s iT 2 , a) are the learned probability of reward for each terminal state associated with their respective second-stage states, and p(s iT 1 |s i ) & p(s iT 2 |s i ) are the learned probabilities of transitioning into either of those terminal states from their respective second-stage parent. Like the model-free sub-component, the model-based system is augmented to include a fast-learning system capable of rapid single-trial adaptation, with free parameters ϑ M B−G and ϑ M B−L capturing differential sensitivity to gains and losses respectively. As such, the model-based value ultimately used to guide choice is derived according to: Q M B (s 2i , a) * = Q M B (s 2i , a) + term(s T ) * β M B


47608.30 62334.70 37506.80 53098.60 Baseline + condition regression 198426.40 47076.10 61391.40 37089.50 52365.30 Fixed RL mixture 198507.00 47044.52 60670.99 37487.06 53304.39 Flexible RL mixture 191860.30 45743.92 58476.90 36040.60 51598.88
Group
Model
All
MB
Mix
MF
Misc
Baseline regression
202275.30


Table 2 :
2
Post-hoc comparison using Tukey's HSD. Group mean and standard error shown along the diagonal, group differences and 95% confidence intervals shown in off-diagonals. * indicates significance at p < 0.05 level.
that model-free RL provides a plausible computational account of a substantial proportion of human
participants on this task.








Acknowledgment
This work was supported by grants from the National Institutes of Mental Healthy to JPOD and JC (R21MH120805 and R01MH121089).






We augmented the baseline regression model to examine individual differences in the effects of variation in reward contingency, reward magnitude reliability, and transition reliability as:
where R c is a binary flag indicating the previous trial's reward contingency (+1: state, -1 stimulus), R r is a binary flag indicating the previous trial's reward magnitude reliability (+1: low, -1: high), and T r is a binary flag indicating the previous trial's transition reliability (+1: high, -1: low). We examined potential interactions among conditions using a simplified random-effects model to avoid convergence issues as:
Effects on response time were modeled as:
where the log-transformed response time for a given trial (log(RT t )) is modeled according to whether the choice made repeated the previous trial's choice (Stay t ), the previous trial's reward (R t−1 ) and transition type (T t−1 ).


Cluster analysis
K-means clustering analysis was performed using a feature set that included: 1) All random effect estimates from the regression model used to quantify the effects of conditional manipulations on modelbased/-free control
All random effect estimates from the regression model used to quantify response time effects
, and 3) Summary statistics quantifying the overall sensitivity to rewards and transitions (T S, OS, and |OS|. We use the Gap statistical method to identify an optimal number of four clusters 
[47]
. Plotting individuals along model-based/-free random effect estimates reveals groups that we label as Model-Free (purple), Model-Based (cyan), a Mixture of model-based/-free (red), and Non-RL group that do not express strong signatures of either strategy (green) (see 
Figure 4A inset
).


Computational Modelling
Behavior on the two-step task was modeled using an extended form of the established dual-system reinforcement-learning model 
[15]
. The model learns action values via both model-based RL and by model-free SARSA(λ) temporal-difference learning 
[13]
, and assumes that choices were driven by the weighted combination of these two learning algorithms. The task consists of an initial state (S 1 ) where the learning agent must choose between two candidate actions (a 1 , and a 2 ), which transition to one of two second stage states (S 2a , and S 2b ). Upon arrival at one of the second-stage states, participants will automatically transition into one of two possible terminal states where reward is delivered, with S 2a transitioning to either S aT 1 , or S aT 2 , and S 2b transitioning to either S bT 1 , or S bT 2 . Both model-free and model-based sub-components of the mixture model aim to learn a state-action value function Q(s, a) that maps each state-action pair to its expected future value.
The MF sub-component was modeled using the SARSA(λ) temporal difference learning algorithm 
[48]
. At each stage i, the value for the visited state-action pair was updated according to:
where:
and α M F is a free learning-rate parameter. Given that participants were only offered the opportunity to choose an action at the first stage, the set of action values learned at subsequent states (S 2a , and S 2b ) map onto a single no-op action. The integrative MF system was augmented to include a fast-learning system that embodied a simple win-stay/lose-switch strategy capable of rapid single-trial adaptation,
 










Animal spirits: Affective and deliberative processes in economic behavior




George
Loewenstein






Ted O'
Donoghue








Available at SSRN




539843














An fmri investigation of emotional engagement in moral judgment




Brian
Joshua D Greene






Leigh
E
Sommerville






John
M
Nystrom






Jonathan
D
Darley






Cohen








Science




293


5537
















Dual-process theories of higher cognition: Advancing the debate




Jonathan
St






B
T
Evans






Keith
E
Stanovich








Perspectives on psychological science




8


3
















The role of learning in the operation of motivational systems




Anthony
Dickinson






Bernard
Balleine








Stevens' handbook of experimental psychology




3
















Different time courses of learning-related activity in the prefrontal cortex and striatum




Anitha
Pasupathy






K
Earl






Miller








Nature




433


7028
















Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning




Barbara
J
Henry H Yin






Bernard
W
Knowlton






Balleine








European journal of neuroscience




19


1
















Inactivation of the infralimbic prefrontal cortex reinstates goal-directed responding in overtrained rats




Etienne
Coutureau






Simon
Killcross








Behavioural brain research




146


1-2
















The role of the basal ganglia in habit formation




H
Henry






Barbara
J
Yin






Knowlton








Nature Reviews Neuroscience




7


6
















Goal-directed instrumental action: contingency and incentive learning and their cortical substrates




W
Bernard






Anthony
Balleine






Dickinson








Neuropharmacology




37


4
















Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control




Nathaniel
D
Daw






Yael
Niv






Peter
Dayan








Nature Neuroscience




8


12
















Multiple Model-Based Reinforcement Learning




Kenji
Doya






Kazuyuki
Samejima






Ken-Ichi
Katagiri






Mitsuo
Kawato








Neural Computation




14


6
















Multiple Forms of Value Learning and the Function of Dopamine




W
Bernard






Nathaniel
D
Balleine






John
P
Daw






O'doherty








Neuroeconomics




Elsevier
















Reinforcement learning: An introduction




Richard
S
Sutton






Andrew
G
Barto








MIT Press


526


Cambridge, MA, US






Reinforcement learning: An introduction. 2nd ed. The. Pages: xxii








A Neural Substrate of Prediction and Reward




Wolfram
Schultz






Peter
Dayan






P. Read
Montague








Science




275


5306
















Model-Based Influences on Humans' Choices and Striatal Prediction Errors




Nathaniel
D
Daw






Samuel
J
Gershman






Ben
Seymour






Peter
Dayan






Raymond
J
Dolan








Neuron




69


6
















Model-based choices involve prospective neural activity




B
Bradley






Katherine
D
Doll






Dylan
A
Duncan






Daphna
Simon






Nathaniel
D
Shohamy






Daw








Nature neuroscience




18


5
















States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning




Jan
Gläscher






Nathaniel
Daw






Peter
Dayan






John
P
O'doherty








Neuron




66


4
















Bold responses reflecting dopaminergic signals in the human ventral tegmental area




Kimberlee D'ardenne






M
Samuel






Leigh
E
Mcclure






Jonathan
D
Nystrom






Cohen








Science




319


5867
















Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis




W
Paul






Glimcher








Proceedings of the National Academy of Sciences


the National Academy of Sciences






108














Dissociable roles of ventral and dorsal striatum in instrumental conditioning




O'
John






Peter
Doherty






Johannes
Dayan






Ralf
Schultz






Karl
Deichmann






Raymond J
Friston






Dolan








science




304


5669
















Beyond dichotomies in reinforcement learning




G
E
Anne






Jeffrey
Collins






Cockburn








Nature Research Journals Number: 10 Primary atype: Reviews Publisher: Nature Publishing Group Subject term: Cognitive neuroscience;Learning algorithms;Learning and memory;Psychology Subject term id: cognitive-neuroscience;learning-algorithms;learning-and-memory;psychology






21








Bandiera abtest: a Cg type








Conflict monitoring and cognitive control




M
Matthew






Botvinick






S
Todd






Deanna
M
Braver






Cameron
S
Barch






Jonathan
D
Carter






Cohen








Psychological review




108


3


624














The Mixed Instrumental Controller: Using Value of Information to Combine Habitual Choice and Mental Simulation




Giovanni
Pezzulo






Francesco
Rigoli






Fabian
Chersi








Frontiers in Psychology




4














Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes




Mehdi
Keramati






Amir
Dezfouli






Payam
Piray








PLoS Computational Biology




7


5


1002055














Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems




Wouter
Kool






Samuel
J
Gershman






Fiery
A
Cushman








Psychological Science




28


9
















Neural Computations Underlying Arbitration between Model-Based and Model-free Learning




Shinsuke
Sang Wan Lee






John
P
Shimojo






O'doherty








Neuron




81


3
















Characterizing a psychiatric symptom dimension related to deficits in goal-directed control. eLife




Michal
Claire M Gillan






Robert
Kosinski






Elizabeth
A
Whelan






Nathaniel
D
Phelps






Daw








5


11305












Human dorsal striatal activity during choice discriminates reinforcement learning behavior from the gambler's fallacy




K
Ryan






John P O'
Jessup






Doherty








Journal of Neuroscience




31


17
















Active reinforcement learning versus action bias and hysteresis: control with a mixture of experts and nonexperts




T
Jaron






Colas






P
John






Scott T
O'doherty






Grafton








PLOS Computational Biology




20


3


1011950














Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task




Thomas
Akam






Rui
Costa






Peter
Dayan








PLOS Computational Biology




25














Rethinking modelbased and model-free influences on mental effort and striatal prediction errors




Carolina Feher Da
Silva






Gaia
Lombardi






Micah
Edelson






Todd
A
Hare








Nature Human Behaviour




7


6
















Habitual control of goal selection in humans




Fiery
Cushman






Adam
Morris








Proceedings of the National Academy of Sciences




112


45
















Cognitive control predicts use of model-based reinforcement learning




Ross
Otto






Anya
Skatova






Seth
Madlon-Kay






Nathaniel
D
Daw








Journal of cognitive neuroscience




27


2
















Impulsivity relates to multi-trial choice strategy in probabilistic reversal learning




Daniela E Muñoz
Amy R Zou






Sheri
L
Lopez






Anne Ge
Johnson






Collins








Frontiers in Psychiatry




13


800290














Deviation from the matching law reflects an optimal strategy involving learning over multiple timescales




Kiyohito
Iigaya






Yashar
Ahmadian






P
Leo






Greg
S
Sugrue






Yonatan
Corrado






Loewenstein






T
William






Stefano
Newsome






Fusi








Nature communications




10


1


1466














Gaze data reveal distinct choice processes underlying modelbased and model-free reinforcement learning




Arkady
Konovalov






Ian
Krajbich








Nature communications




7


1


12438














The obsessive-compulsive inventory: development and validation of a short version




Jonathan
D
Edna B Foa






Susanne
Huppert






Robert
Leiberg






Rafael
Langner






Greg
Kichic






Paul
M
Hajcak






Salkovskis








Psychological assessment




14


4


485














Anxiety and impulsiveness related to psychomotor efficiency




S
Ernest






Barratt








Perceptual and motor skills




9


3
















The state-trait anxiety inventory




D
Charles






Fernando
Spielberger






Angel
Gonzalez-Reigosa






Martinez-Urrutia






F
S
Luiz






Diana
S
Natalicio






Natalicio








Revista Interamericana de Psicologia/Interamerican journal of psychology




5


3 & 4














Beck depression inventory




T
Aaron






Robert
A
Beck






Gregory
K
Steer






Brown


















Disruption in the balance between goal-directed behavior and habit learning in obsessive-compulsive disorder




Martina
Claire M Gillan






Sharon
Papmeyer






Barbara
J
Morein-Zamir






Naomi
A
Sahakian






Trevor
W
Fineberg






Sanne
Robbins






Wit
De








American Journal of Psychiatry




168


7
















Disorders of compulsivity: a common bias towards learning habits




Valerie
Voon






Katherine
Derbyshire






Chistian
Rück






A
Michael






Yulia
Irvine






Jesper
Worbe






Enander






R
N
Liana






Claire
Schreiber






Naomi
A
Gillan






Barbara
J
Fineberg






Sahakian








Molecular psychiatry




20


3
















Task complexity interacts with state-space uncertainty in the arbitration between model-based and model-free learning




Dongjae
Kim






Geon Yeong
Park






P
John






Sang
Wan
O'doherty






Lee








Nature communications




10


1


5738














Model-based predictions for dopamine




J
Angela






Melissa
J
Langdon






Geoffrey
Sharpe






Yael
Schoenbaum






Niv








Current opinion in neurobiology




49
















Inattentive responding can induce spurious associations between task behaviour and symptom measures




Samuel
Zorowitz






Johanne
Solis






Yael
Niv






Daniel
Bennett








Nature human behaviour




7


10
















When does model-based control pay off?




Wouter
Kool






A
Fiery






Samuel
J
Cushman






Gershman








PLoS computational biology




12


8


1005090














Estimating the number of clusters in a data set via the gap statistic




Robert
Tibshirani






Guenther
Walther






Trevor
Hastie








Journal of the Royal Statistical Society: Series B (Statistical Methodology)




63


2
















Q-learning




Jch
Christopher






Peter
Watkins






Dayan








Machine learning




8
















By carrot or by stick: cognitive reinforcement learning in parkinsonism




J
Michael






Lauren
C
Frank






Randall C O'
Seeberger






Reilly








Science




306


5703
















A reinforcement learning mechanism responsible for the valuation of free choice




Jeffrey
Cockburn






G
E
Anne






Michael J
Collins






Frank








Neuron




83


3
















Hierarchical bayesian inference for concurrent model fitting and comparison for group studies




Payam
Piray






Amir
Dezfouli






Tom
Heskes






J
Michael






Nathaniel
D
Frank






Daw








PLoS computational biology




15


6


1007043















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]