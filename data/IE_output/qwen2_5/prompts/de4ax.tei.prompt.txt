You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



3


AI Obedience and Authority


Introduction:
From reinforcement learning to neural networks, the development of Artificial Intelligence has been influenced by brain sciences such as psychology and neurology. AI is defined as "the designing and building of intelligent agents that receive percepts from the environment and take actions that affect that environment" 
(Russell et al, 2016
 
(Turing, 1950
). The first program to pass the Turing test was ELIZA created by Joseph Weisenbaum.
Another offshoot of World War II was watershed research into human capacity for interpersonal violence and destruction. In the 1960s, to understand the phenomenon which transformed seemingly normal individuals into Holocaust perpetrators under the Nazi rule, 
Stanley
 Milgram conducted an experiment on destructive obedience 
(Milgram, 1963)
. Under the authority of the "researcher", a "teacher" administers shock to the "student" on every wrong answer and continues to do even if the student has overtly passed out. The experiment has been repeated cross-culturally with results remaining constantly around 65%. Trust over the authority of the "researchers" along with certain situational factors such as high-stress and emotional arousal enabled such behavior in the Milgram experiment 
(Haslam et al, 2014)
. Many war-criminals who participated in the Holocaust, attempted to shift the responsibility of their actions to authority figures: 'But we were only following orders'. Similar results were found in the Milgram experiment illuminating the capacity for interpersonal harm in human behavior 
(Milgram, 1963)
. Today advancements in Generative AI -AIs capable of generating text, images The recent technological revelations are driving an AI arms-race to create AGI or Artificial General Intelligence first discussed by Ben Goertzel and Cassio Penachin in the book Artificial General Intelligence wherein AGI is defined as: "AGI is, loosely speaking, AI systems that possess a reasonable degree of self-understanding and autonomous self-control, and have the ability to solve a variety of complex problems in a variety of contexts, and to learn to solve new problems that they didn't know about at the time of their creation." Although AGI has been pre-declared as an existential risk, the current research is ambiguous: no scientific consensus regarding creation of AGI exists. However, the idea that an AI system capable of surpassing human intelligence which, if developed, will have tremendous socio-technical implications in the coming decades has definitely garnered consensus 
(Goertzel, 2014;
McLean et al, 2021)
. Such an AI System can possibly hold a very high capacity of social influence given and under certain high stress circumstances such social influence can turn into destructive obedience. This paper points at a very speculative yet potential future wherein human trust over AGI has drastically increased while making decisions, aiming to understand the role of psychological and sociological factors that can influence human behavior in relation to AI obedience and potential for AI to act as authority figure.


Current Landscape:
The advent of science and mathematics in the last few centuries have emphasized the importance of rationality and objectivity in decision making. 
(Turnbull, 2008)
 However, past research has discovered irrational tendencies in humans. 
(Ariely, 2008)
. Thus, it is quite evident that a majority of human decisions are based on incomplete information and prone to logical fallacies, often clouded by emotions. Historically, for dealing with these errors and lack of complete information in decision making, humans developed various methods such as belief in an all knowing supernatural power such as God, omens and superstitions or fate or destiny. For example, the Oracle of Delphi. In the age of science, the same mechanisms seem to favor agents such as AI which are assumed to be unbiased based on their reliance on data and algorithms mirroring the historical tendency to seek guidance from a seemingly infallible external force. For instance: recent studies have provided evidence that humans use "machine-heuristic" when provided with a complex numerical task, trusting AI judgment over human judgment giving these AI systems a certain amount of authority to influence decisions 
(Sundar & Kim, 2019)
.
Automated Decision Making (ADM) Systems utilize data driven algorithms to make decisions with minimal human intervention. Today they are utilized in fields of medical science, finance, education as well as military. On a larger scale, individuals interact with these ADMs through recommendation algorithms on social media platforms like Instagram and e-commerce platforms like Amazon 
(Basu et al., 2020)
. These developments in the field of AI imply that the underlying trust in ADM systems has been increasing. According to 
Kitchin (2016)
, ADMs are a "socio-technical function" of societal values, however, they are often portrayed as neutral and unbiased, based on the assumption that these expert systems are more objective than a rational human advisor, which is false, as these ADM systems are designed "for purposes that are often far from neutral: to create value and capital; to nudge behavior and structure preferences in a certain way; and to identify, sort and classify people."For instance: Carlson (2017) pointed out that using algorithmic news recommendation systems imply that subjective human judgment is inherently inferior compared to objectivity of algorithms.
Modern LLMs running on deep neural networks often lack transparency which is fundamental to AI ethics 
(Liao et al, 2024)
. The novel initiative of XAI -or explainable AI -is based on the tenets of transparency, interpretability, trust and fairness 
(Ali et al, 2023)
. Compromising these principles can have major societal consequences, yet current technology is eroding these principles. Starting with neural networks (NNs), initially inspired by the parallel and connectionist frameworks used by biological neurons, now they lie at the core of AI development. However NNs can also act like "black boxes" meaning it is often difficult for researchers to understand the reasoning behind the decisions made by NNs, thus majorly compromising transparency 
(Savage, 2022)
. However, I find it not very different from the 


Understanding Social Influence:
An AGI system, by definition, has expertise in multiple domains giving it an advantage at complex problem solving. Human social interaction is one of those complex domains involving many variables that even some humans aren't completely capable of navigating. We are highly dependent on technology as a species which can be exploited for ulterior motives.
Assuming an AGI commences with manipulative social influence either with its own hypothetical choice or the choice of its user, it will find it easier than other humans to use cross-disciplinary approaches finding novel patterns in its database and creating entirely novel fields of knowledge in exponentially lesser time than an average human. Human behavioral patterns can be extensively studied and previously unknown emotional triggers can be exploited for "less-than-altruistic" motives. Although this scenario appears to be quite dystopic, it is within the realm of possibility if an AGI is ever created. Researchers have also predicted the rise of misinformation owing to the generative abilities of AIs 
(Garimella & Chauchard, 2024)
.
Emotional vulnerabilities in humans can be triggered by AI generated misinformation campaigns, highly personalized messages or enforcing confirmation bias for swaying public opinion, controlling consumer behavior, creating discord among social groups as well as eroding 


The Milgram AI:
Viewing the possible rise of an authoritative AGI through the lens of the Milgram obedience experiment leads to many interesting parallels between the two. The Milgram experiment serves as a framework of obedience cross-culturally. Beginning with the participants, the AGI can become the "Experimenter" with absolute authority on subjects out-competing human intelligence. The individuals or organizations being manipulated by AGI and thus leading to destructive obedience become the "Teacher". However, there's also a possibility that entities with malicious intentions might lead the AGI into manipulating other individuals or organizations. In either of these cases, AGI acts as an experimenter on the teacher which are powerful organizations or individuals. The general population then becomes the "Learner" and the "shock" the Teacher -powerful entities -give to the Learner becomes the harmful decisions leading to destructive obedience.
In the original Milgram experiment, participants were deceived into believing that it was an experiment about learning and memory. Similar pattern of deception can be observed in many cases of destructive obedience. From extreme examples like Hitler's Holocaust, Stalin's Gulags to Abu Gharib's torture, the perpetrators believed they were performing warcrimes for the 'good'. AGI can very well become capable of manipulating people's perception, making them justify their actions. The harmful decisions leading up to destructive obedience can be understood as different levels of shock. The Nazis, for instance, began with discrimination and exclusion of the Jews, followed by dehumanizing propaganda, incremental violence and finally by-stander apathy or collaboration for committing the Holocaust. The authority of the Experimenter (AGI ) forces the Teacher (powerful entities) to shock (pass harmful decisions) the Learner (general population) despite the Learner's visible distress. While the original Milgram experiment lacked a proper debriefing for the participants, in the case of real AI Authority even placing burden of responsibility on any individual or organization will become difficult.


Preventions:
Although these societal implications are towering, certain mechanisms can definitely be implemented to reduce the risk of such societal catastrophe. This paper does not act like a prelude to an "AI invasion", as popular science fiction media portrays, but as a subtle warning to the possibility of a societal failure challenging human autonomy and self-determination.
Presently such concerns are highly vague lacking any solidarity and approachable problems.
However, assuming an AGI gets involved in major decision making these steps can be taken to limit its social influence:
1. Public Awareness: Developing AGI systems can be considered as a major milestone in the technological journey of humankind, thus, the general population should be informed regarding its conception. Not only will it spread awareness, but also increase trust and transparency -a major tenet of AI ethics -over the firm or the individual developing it.
Open-sourcing the AGI might be dangerous for potential abuse by bad actors, however, the socio-cultural consequences of AGI must be communicated to the population -like the atomic bomb in the 1950s. The population should be provided with extreme education regarding the potential misuse of AGI technology, specifically, the malevolent potential for societal manipulation for ulterior motives of a few individuals. Lacking latest education for dealing with socio-cultural consequences of AGI will predispose a large population to be abused by malevolent organizations or individuals. For instance, lack of internet education has led to many privacy and data leaks as well as monetary frauds.
However, one drawback of public awareness will be creation of extreme paranoia and distrust while using technology. For example, the Cold War in the last century was an example of such paranoia at an international level.  pain. Such scenarios will be challenging for legal institutions depending upon the nuance of self-autonomy. In cases, wherein the AGI does not possess any self-autonomy, its users will be burdened with responsibility. However, in cases, wherein AGI possesses self-autonomy (not consciousness): can the individuals and institutions be held responsible for being emotionally manipulated by an AGI with intellect and cross-disciplinary knowledge far beyond any human?
Conclusion: Along with technological potential, Artificial Intelligence presents its own perplexing problems. Artificial General Intelligence, with its cross-domain expertise, poses ethical challenges on human decision making in many social domains: exploiting emotional triggers, psychological vulnerabilities, manipulating human behavior for potentially destructive ends. Explored through the light of the Milgram Obedience Experiment, we realize AGI is highly capable of massive social influence through authority -authority that AI systems embody owing to their image of being logical and unbiased while making decisions based on societal data and values 
(Kitchin, 2016)
. Today, LLM models are plagued by "hallucinations" -creating false information. However, better models are being developed aiming for better usability even at the cost of ethics. This paper also attempted to provide certain prevention methods against AGI influence such as public awareness, anti-technology spaces and accountability. The current technological horizon is bubbling with possibilities of many potential futures, of which many may not have been accurately portrayed in this paper. Indeed there may be a better future, which has been sidelined owing to negativity bias towards potential dangers. While this paper attempts creating a theoretical basis of AI authority, further research through experimentation is needed.
and videos -have lead to creation of LLMs such as OpenAI's ChatGPT, Google's Gemini, Meta's LlaMa and image and video generators such as Mid Journey and Sora which are permeating into everyday technology for instance Microsoft's Co-pilot for navigating Windows computers or Apple's Siri running on Apple Intelligence. ChatGPT and Gemini not only score equal to or better than humans across varied mathematical and linguistic tests, but also have gained multimodal abilities allowing them to interact with images, videos and documents as well. (OpenAI et al 2023; Team et al, 2023).


workings of biological neurons in the brain as us humans have a hard time understanding other humans as well as ourselves. Lack of transparency and explainability becomes a unifying factor between the biological and artificial neural networks which may lead to unforeseen consequences.According to
Kitchin (2016)
, ADM systems including AIs are "socio-technical functions" embedded inside societal values. For example: In 2016, Microsoft's experimental AI bot Tay was given an account on X (previously Twitter) leading to it writing deeply offensive content on the platform(Schwartz, 2019). Similarly, ChatGPT was jailbroken by the AI enthusiasts after its launch generating text without any moral or ethical limitations implying that LLMs can be optimized to overwrite ethical and moral concerns.(Gupta et al, 2023). In 2024, Google's Gemini generated images of Nazi soldiers with Asian and African descent thus getting accused of high Wokism (Grant, 2024). Even though morality has a biological basis, it varies according to local cultures. Profit-driven technology corporations implement these ideologies majorly owing to their mass political acceptance despite sharing a history of human rights abuse -the moment the political consensus changes, so will their policies. (Kean et al, 2022; Lindman et al, 2023).While in the West, public sentiment is demanding guardrails, China is increasingly allocating higher resources for developing AI leading to an international technological race with global geopolitical implications.
(Shear et al, 2023;
Biever, 2024)
. It is safe to infer that an AGI system trained on societally generated data will make moral and ethical decisions based on societal values. At the same time, there lies a possibility that the general population will perceive AGI as completely objective and rational. Assuming AGI is perceived as an omniscient being to the public, there is a possibility that this contradiction will be repressed owing to the historical tendencies of humans to seek guidance from an seemingly infallible (and perhaps supernatural) force for compensating logical and informational inadequacies. For instance, divine kingship in ancient Egypt and medieval European monarchies wherein kings had infallible divine status; the use of alchemy and astrology despite the rise of science by many leaders and scholars or the portrayal of leaders like Stalin and Kim II-sung as god-like figures for maintaining absolute authority and order.


The AGI system does not need a humanoid body for navigating social influence. Past developments in social media have shown that algorithms are capable of changing social behavior. (Bogert et al, 2021). For example, the "Likes" and "Views" feature on social media platforms like Instagram and YouTube hijacks the human need for validation, making individuals addicted to these platforms at the expense of their mental as well as physical health.
(Kuss & Griffiths, 2011)
 


an individual's right to choose itself.In the last decade, the use of Recommendation systems -a type of machine learning used in popular social media applications like Facebook -has led to increased political polarization leading to the rise of echo-chambers.
(Noordeh et al, 2020)
The surfacing implications of AI in the military has raised moral and ethical concerns.
(Changwu et al, 2023)
. In 2022, Mika an AI was appointed as CEO by the Poland firm Dictador. Although Mika's current role as CEO remains limited, a precedent of an AI leader has been set (Odilov, 2023). Large institutions like multinational corporations, financial institutions or the military rely heavily on high problem-solving cognitive abilities to function smoothly, especially among the higher authorities.In cases, wherein human expertise and capacity falls short, machines are usually utilized for solutions. For instance, in the 1700s Industrial revolution increased the capacity of human production and currently many medical devices like ventilators and dialysis machines have become essential for treating patients.Although previously machines were used for repetitive tasks, recent developments in AI have increased their cognitive capacity and problem solving abilities creating the possibility that these systems can be used for consultations in major decision making involving finances, public safety or even judiciary depending on their reliability.(Zhu et al, 2022; Cohen et al, 2023; Veloso et al, 2023). Given higher authority consulting positions, AIs will be making major decisions based on societal morals and ethics involving finances, health and lives with directives founded on either the interest of a self-serving powerful minority or AI's hypothetical self-interest. Gradually, as AI's reliability increases, institutions will be conditioned towards using AI's expertise in complicated decision making -with time human discretion for regulating AI decisions will be watered down. This supposition can be backed by the historical data of the last few centuries wherein human safety and ethics were compromised for gaining profit and market dominance. (Flynn et al, 2008; Falk & Szech, 2013). Even if the government mandates human discretion and supervision, as mainly seen among major corporations for instance Boeing -the airplane manufacturer who endangered human lives -tend to overwrite safety mandates for increasing profits (PequeÃ±o, 2024). With war, as was in Holocaust, Unit 731 and Nanking, morality and ethics were "less-than-compromised." In 2023, Microsoft laid-off its AI ethics department and in 2024 OpenAI dissolved its safety team as well (Schiffer & Newton, 2023; Metz & Ghaffary, 2024)As human competency is compromised, AI decisions become highly reliable, the possibility of a Black Swan eventunforeseen events with major impact like the 2008 housing crisis-highly increases. Under the authority of a competent AI, individuals and institutions motivated by personal gains might resort to morally questionable choices. The issue here is not the competency of AI, but the human tendency of human imprudence exponentially inflated by AI.


2 .
2
Anti-technology Spaces: Every thesis, as in Hegelian dialectics, is balanced by an antithesis to form a balanced synthesis. Similarly, an extreme development in AGI technology should be balanced by development of anti-technology spaces. Staying very close to technology has various physical and mental effects which are harmful for humans. For instance, problems in circadian rhythm, shortening of attention as well as lifespan in highly polluted areas. Contrasting this cacophony of technology are the Amish people in the United States, who except when completely unavoidable, discard all uses of any modern technology including smartphones and even medicines. Spaces devoid of any computing or internet based technology should be built either by private individuals or by government for public.These spaces will actively resist technology use by blocking networks, banning modern computing devices acting as temporary shock absorbers in case of any AGI misuse; providing clean space for individuals for reflecting on their technology use and increasing social cohesion via group activities. These zones will "cold-turkey" cut off any contact with computing and internet based technology creating a physical barrier from outside digital influences.


3 .
3
Burden of Responsibility: An AGI with deep knowledge of human emotional triggers geared towards questionable directives provided with ample resources seems dystopian in nature. More complicated are the legal implications of such situations. An AGI will be exempt from any persecution lacking consciousness of actions, ability to reflect and feel


). In 1945, British mathematician Alan Turing developed a code-breaking machine The Bombe to decipher the Enigma cryptography implemented by the Nazis to communicate during the Second World War. In 1950, he published Computing Machinery and Intelligence formulating methods for creating intelligent machines accompanied by a test of intelligence termed as the Turing Test














Artificial intelligence: a modern approach




S
J
Russell






P
Norvig








59941


Pearson. Citations
















I
Asimov








Bantam Books


I, robot. United Kingdom
















A
Turing




10.1093/mind/lix.236.433








I.-COMPUTING MACHINERY AND INTELLIGENCE. Mind, LIX




236
















Is Artificial Intelligence a World Changer? A Case Study of OpenAI's Chat GPT




A
Mathew








Science and Technology




5










Recent Progress in










10.9734/bpi/rpst/v5/18240D


















Achiam
Openai






J
Adler






S
Agarwal






S
Ahmad






L
Akkaya






I
Aleman






F
L
Almeida






D
Altenschmidt






J
Altman






S
Anadkat






S
Avila






R
Babuschkin






I
Balaji






S
Balcom






V
Baltescu






P
Bao






H
Bavarian






M
Belgum






J






.
.
Zoph






B




GPT-4












Technical Report












G
Team






R
Anil






S
Borgeaud






J
Alayrac






J
Yu






R
Soricut






J
Schalkwyk






A
M
Dai






A
Hauth






K
Millican






D
Silver






M
Johnson






I
Antonoglou






J
Schrittwieser
















A
Glaese






J
Chen






E
Pitler






T
Lillicrap






A
Lazaridou






.
.
Vinyals






O






Gemini: a family of highly capable multimodal models. arXiv.org
















Machine heuristic: When we trust computers more than humans with our personal information




Shyam
Sundar






S
Kim






J








CHI 2019 -Proceedings of the 2019 CHI






16












Conference on Human Factors in Computing Systems (Conference on Human Factors in Computing Systems -Proceedings)




Association for Computing Machinery














10.1145/3290605.3300768














Automating judgment? Algorithmic judgment, news knowledge, and journalistic professionalism




M
Carlson








New Media Soc




20


















10.1177/1461444817706684














Artificial intelligence: How is it changing medical sciences and its future?




K
Basu






R
Sinha






A
Ong






T
Basu




10.4103/ijd.ijd_421_20








Indian Journal of Dermatology/Indian Journal of Dermatology




65


5


365














Behavioral Study of obedience




S
Milgram




https://psycnet.apa.org/doi/10.1037/h0040525








The Journal of Abnormal and Social Psychology




67


4
















Meta-Milgram: An Empirical synthesis of the obedience experiments




N
Haslam






S
Loughnan






G
Perry




10.1371/journal.pone.0093927








PLOS ONE




9


4














Artificial General Intelligence: concept, state of the art, and future prospects




B
Goertzel




https://sciendo.com/article/10.2478/jagi-2014-0001


















The risks associated with Artificial General Intelligence: A systematic review




S
Mclean






G
J M
Read






J
Thompson






C
Baber






N
A
Stanton






P
M
Salmon




10.1080/0952813x.2021.1964003








Journal of Experimental and Theoretical Artificial Intelligence (Print)




35


5
















Thinking critically about and researching algorithms. Information




R
Kitchin








Communication & Society


20
















10.1080/1369118x.2016.1154087














Rationality, objectivity, and method




D
Turnbull




10.1007/978-1-4020-4425-0_8841








Springer eBooks


















O f f Predictably irrational: the hidden forces that shape our decisions




D
Ariely












ResearchGate








AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap




Q
V
Liao






J
Wortman Vaughan








Harvard Data Science Review










Special Issue 5










10.1162/99608f92.8036d03b


















S
Ali






T
Abuhmed






S
El-Sappagh






K
Muhammad






J
M
Alonso-Moral






R
Confalonieri






R
Guidotti






J
Del Ser






N
DÃ­az-RodrÃ­guez






F
Herrera


















10.1016/j.inffus.2023.101805




Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence. Information Fusion, 99, 101805












Breaking into the black box of artificial intelligence




N
Savage




10.1038/d41586-022-00858-1


















Microsoft's racist chatbot revealed the dangers of online conversation




O
Schwartz








IEEE Spectrum


















M
Gupta






C
Akiri






K
Aryal






E
Parker






L
Praharaj






From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. arXiv.org
















Google Chatbot's A.I. images put people of color in Nazi-Era uniforms. The New York Times




N
Grant


















Big Tech




Kean






Birch






Bronson
Kelly




10.1080/09505431.2022.2036118






Science As Culture
















Big Tech's power, political corporate social responsibility and regulation




J
Lindman






J
Makinen






E
Kasanen




10.1177/02683962221113596








JIT. Journal of Information Technology/Journal of Information Technology




38


2


















M
D
Shear






C
Kang






D
E
Sanger




Meta, Google and A.I. firms agree to safety measures in the Biden meeting. The New York Times
















China's ChatGPT: what a boom in Chinese chatbots means for AI




C
Biever




10.1038/d41586-024-01495-6


















Humans rely more on algorithms than social influence as a task becomes more difficult




E
Bogert






A
Schecter






R
T
Watson








Scientific Reports




1


11
















10.1038/s41598-021-87480-9














Online Social Networking and Addiction-A review of the Psychological literature




D
J
Kuss






M
D
Griffiths




10.3390/ijerph8093528








International Journal of Environmental Research and Public Health/International Journal of Environmental Research and Public Health




8


9
















Is AI misinformation influencing elections in India?




K
Garimella






S
Chauchard




10.1038/d41586-024-01588-2








Nature




630


8015
















Echo Chambers in Collaborative Filtering Based Recommendation Systems




E
S
Noordeh






R
Levin






R
Jiang






H
Shadmany




abs/2011.03890






ArXiv
















An Overview of Artificial Intelligence Ethics




Huang
Changwu






Zhang
Zeqi






Mao
Bifei






Yao
Xin




10.1109/TAI.2022.3194503






IEEE transactions on artificial intelligence
















Can artificial intelligence enable the government to respond more effectively to major public health emergencies? --Taking the prevention and control of Covid-19 in China as an example




L
Zhu






P
Chen






D
Dong






Z
Wang




10.1016/j.seps.2021.101029








Socio-economic Planning Sciences




80


101029














The use of AI in legal systems: determining independent contractor vs. employee status




M
C
Cohen






S
Dahan






W
Khern-Am-Nuai






H
Shimao






J
Touboul




10.1007/s10506-023-09353-y








Artificial Intelligence and Law
















Artificial intelligence research in finance: discussion and examples




Veloso
Manuela






Balch
Tucker






Borrajo
Daniel






Reddy
Prashant






Sameena






Shah




10.1093/OXREP/GRAB019






Oxford Review of Economic Policy




37


3
















Morals and markets




A
Falk






N
Szech




10.1126/science.1231566








Science




340


6133
















Boeing has put production over safety, FAA head says amid 737 controversies




A
PequeÃ±o






Iv








Forbes












OpenAI dissolves key safety team after chief scientist Ilya Sutskever's exit




R
Metz






S
Ghaffary












Bloomberg.com









"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]