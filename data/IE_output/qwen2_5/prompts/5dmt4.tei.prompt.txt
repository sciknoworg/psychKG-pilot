You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Content that evokes strong moral and emotional responses tends to receive greater social media engagement 
(1)
. False information often elicits more negative emotions than true content 
(2)
, causing it to spread rapidly across social media platforms. In an ideal world, factual refutations would reverse the harmful effects of misinformation. In actuality, even when people learn of the falsehood, inaccurate information can still influence their later judgments, a phenomenon known as "continued influence effects" (CIEs) 
(3)
. Past work has documented some cognitive mechanisms as well as correction strategies for CIEs 
(4,
5)
. Still, CIEs typically persist even when best practices for corrections are followed, thus it remains crucial to understand the lingering impacts of false information on behavior. We present a novel approach to measuring CIEs in the context of accusations of political misconduct and examine demographic and psychometric predictors of the magnitude of these effects in political decisionmaking.
CIEs were first studied in the domain of causal reasoning (3), for example, about the causes of a fire. Studies in this domain have proposed potential cognitive mechanisms for CIEs.
One possible mechanism is a failure to integrate new information into one's mental model (e.g., 
3
). An alternate mechanism is selective retrieval (5, 6)-automatically remembering misinformation but not remembering its refutation, and failing to engage in strategic retrieval sufficient to associate the two. Individual differences in CIE vulnerability have been linked to cognitive abilities like vocabulary knowledge 
(7)
, working memory capacity 
(8)
, and long-term memory for stimulus details 
(9)
. However, working memory's predictive power disappears when controlling for long-term memory 
(9)
. Although the present studies do not focus primarily on these mechanisms, we measure recognition memory in part to test a simple selective retrieval account of CIEs. Political misinformation has gained significant societal attention in recent years, but there are challenges to integrating the cognitive psychology literature on CIEs with research on political misinformation. One key question is whether and when CIEs occur when forming impressions about other people. CIEs were observed in one study using hypothetical political candidates, in which refuted misconduct allegations still negatively impacted candidate evaluations 
(10)
. Classic social psychology research has shown that impressions of task ability, both for oneself and an observed other person, remain influenced by false feedback even after debriefing 
(11)
. Similarly, evidence presented but then declared inadmissible tends to impact real-world jury verdicts 
(12)
. However, in more recent work presenting a series of short narratives about a hypothetical non-political target, a retraction fully countered the negative effects from accusations of immoral behavior 
(13)
. A second set of studies using a similar approach likewise found no evidence for CIEs and saw some evidence of overcorrection, even under conditions where other presented information could support the truth of the retracted accusation 
(14)
. Our study will help to resolve these conflicting findings by testing whether CIEs limit effectiveness of corrections when political candidates are targeted by false misconduct accusations, across various candidate stimuli, across both single evaluations and binary choices, and in a context resembling real-world political information environments.
Another missing connection between CIEs and other approaches to researching misinformation is that many individual differences that predict vulnerability to misinformation and conspiracy theories in the real world have not been examined with respect to CIEs. Some work has found that CIEs in a causal inference paradigm are unchanged when excluding those who forget the retraction but are notably larger in people who report not believing the retraction, relative to those who claim to believe it 
(15)
. This suggests that individual differences in how people process and trust information could affect CIEs towards political candidates. Thus, predictors of real-world vulnerability to misinformation may also be relevant to understanding which people are more vulnerable to persistent effects of refuted misinformation when making sociopolitical decisions.
One factor that can impact a person's vulnerability to misinformation is their tendency to think analytically. Specifically, higher scores on the Cognitive Reflection Test (CRT), which measures the degree to which people use analytical reasoning to overcome intuitive but incorrect responses, predicts greater accuracy discernment, i.e., success in identifying false information 
(18)
. Digital literacy-factual knowledge about digital and legacy media-is another factor that predicts improved accuracy discernment 
(19,
20)
; this knowledge may enhance analytic thinking about online content and responsiveness to cues about the reliability of information. A self-report measure of epistemic beliefs also correlates with belief in conspiracy theories 
(21,
22)
 and with measures of headline accuracy discernment and willingness to share false content 
(23)
. Of the three subscales on this measure, having faith in one's intuitive feelings and believing that truth is political (i.e., defined by those in power) are associated with stronger endorsement of conspiracy theories, poorer accuracy discernment, and greater willingness to share false information.
Requiring evidence as a basis for beliefs is associated with reduced willingness to share false content, less endorsement of conspiracy theories, and better accuracy discernment.
In contrast, other work has suggested that social and affective factors like partisanship and ideological biases play an important role in vulnerability to misinformation 
(16,
17,
24)
.
Republicans and those scoring lower on actively open-minded thinking (AOT) show lower accuracy discernment, suggesting a tendency for ideological bias known as "myside bias." Partisanship and AOT are stronger predictors of accuracy discernment than analytic thinking measures 
(25)
, supporting an "integrative account" of misinformation vulnerability. Affective polarization-a strong preference for one's party over opposing partisans-is associated with a greater likelihood of sharing false content on Twitter, particularly among Republicans 
(26)
. Note that because our stimuli are largely devoid of partisan cues, our intention is not to test whether people are more influenced by false information that aligns with their own partisan identify (cf., 
27
). Our design instead allows us to test whether political orientation and/or polarization are associated with being more strongly influenced by compelling scandal narratives even when these are factually refuted.
Another potential vulnerability of practical interest is age. It is unclear whether older adults are more or less vulnerable to misinformation. Older adults tend to show declines in working memory 
(28)
 and associative memory 
(29)
, suggesting a possible impairment in processes associated with CIEs: integrating refutations into mental models and recalling that an accusation has been refuted. Consistent with this expectation, older adults consume and share higher levels of false content on real social media platforms 
(30,
31)
. Aging has also been associated in the lab with reduced explicit memory for trivia knowledge having been identified as false 
(6)
. However, in other laboratory research, older adults have shown either similar 
(32)
 or even reduced CIEs compared to young adults in the context of causal inference in a classic CIE paradigm 
(33)
, and performed better than young adults at explicitly discerning true from false news headlines 
(34,
35)
. These disparate findings could be explained by differences in the cognitive and/or socioemotional processes engaged in some laboratory tasks versus real-world social media platforms, or by differences in the older adult populations that are being studied.
Regardless, additional evidence on how age affects CIEs for accusations against hypothetical political candidates may help clarify these seemingly conflicting findings between real-world and laboratory settings.
Finally, we aim to achieve a preliminary understanding of differences in processing accusations versus factual refutations. Specifically, we examined whether recognition memory varies between accusations and refutations. Prior work suggests that negative information is motivationally salient in a variety of evaluative contexts, including in social impression formation 
(36)
, and that negative emotion enhances memory for associated stimuli 
(37)
. We hypothesized that the negative emotionality evoked by accusations causes accusation stimuli to be prioritized in memory and decision-making relative to matched neutral stimuli, while refutations will receive less or no prioritization. Beyond any direct effect of memory modulation on decision making, prioritized processing of accusation stimuli is a possible mechanism by which refuted accusations may continue to bias decisions (cf., 
38)
.


Results
We ran two large online behavioral experiments with American participants, aiming to recruit 500 participants per experiment. The first study was exploratory, while the second was a preregistered replication in which candidate judgements were also measured two days later. Note that, to address points raised during peer review, some of the analyses highlighted here differ from the preregistered analysis plan; deviations from the preregistration are detailed in the Methods. In both experiments, participants first read introductory bios for all candidates ( 
Figure   1A
), and then saw two mock social media posts for each candidate ( 
Figure 1B
). Each set of social media posts was presented in one of three formats, which were varied within-participants: Refutation Control). Accusation control and refutation control stimuli were topically and visually matched to their respective accusation and refutation stimuli but were designed to have little impact on candidate ratings. Participants saw an equal number of candidates in each of the three formats, and we counterbalanced across participants which candidates appeared in which format.
Immediately after reading the posts, participants rated the candidate on a feeling thermometer ( 
Figure 1C
). The analyses reported here focus largely on continued influence effects (CIEs), subtracting mean ratings for candidates with no accusation from mean ratings for candidates with corrected accusations. Note that more negative scores on CIEs indicate a larger decline in ratings for candidates exposed to accusations. Analyses of secondary outcomes, which look at the effect of accusations and corrections separately, are reported in SI Results.
After reading posts about each candidate and providing a rating, participants then completed a series of self-report questionnaires (see Methods), as well as the MIST-20 headline accuracy judgment measure 
(40)
. These were followed by delayed ratings about each candidate, following the same procedure as the immediate ratings, as well as a delayed choice task ( 
Figure   1D
), in which participants completed a series of binary choices about which of two candidates they would vote for in an election. In the choice task, we computed CIEs as the proportion of trials where a corrected accusation candidate was chosen over a no accusation candidate when candidates of those two types were being compared. Finally, memory was assessed via a recognition memory test ( 
Figure 1E
). In Experiment 1, all of these measures were collected in a single testing session, while in Experiment 2, memory and a second round of delayed ratings and choices were collected two days later.


Behavioral main effects
Immediate and delayed CIEs. As expected based on pilot testing (see SI Results), CIEs were present in the aggregate in Experiment 1 on immediate ratings, t(436) = -11.73, p < .0001, d = -.56 ( 
Figure 2A
). CIEs persisted on ratings made after a short (~20-30 minute) delay, t(436) = -5.63, p < .0001, d = -.27 ( 
Figure 2B
). CIEs were also evident on the choice task at a short  
Figure 2F
). In Experiment 2, we also examined CIEs after a two-day delay; these effects were present in preregistered analyses for both the delayed rating task, t(401) = -5.45, p < .0001, d = -.27, and the delayed choice task, t(401) = -4.32, p < .0001, d = -.22 (see 
Figure S1
). Contrary to our preregistered prediction, CIEs in Experiment 2 did not differ between short-delay and long-delay measurements for the rating task, t(401) = 1.17, d = .06, or for the choice task, |t|(401) < 1, d = -.04 (this issue is discussed further in SI Results). Analyses of CIEs that show similar results for alternative exclusion criteria are reported in 
SI Table S1
.


Comparison of initial and immediate ratings.
To control for any effects of candidate demographic and biographical details, assignment of candidate to condition was counterbalanced between individuals. Mixed effects models that account for initial ratings made prior to reading the social media posts, presented in SI Results, confirm that CIEs were robust to differences in these initial ratings. Another approach to measuring effects of the mock social media posts is to compare immediate ratings after reading each social media post to the initial rating for that candidate. This approach also showed a clear decline from initial ratings to immediate post-story ratings for candidates with corrected accusations in Experiment 1, t(436) = -8.24, p < .0001, d = -.39, and in Experiment 2, t(516) = -8.19, p < .0001, d = -.36 (see SI 
Figure S2
).


CIEs and memory.
We additionally find that CIEs do not appear to be a direct consequence of trials in which the accusation was remembered while the refutation was not, as would be implied by a simple version of the selective retrieval account of CIEs. That is, CIEs were still present when limiting the analysis to trials in which all four stimulus types (see 
Fig.
 1B) were correctly categorized on the later recognition test, though many participants needed to be excluded from these analyses due to having no such trials in a given condition. On this recognition test, familiarity with a candidate was not sufficient to yield a correct response, as all stories involved candidates who participants had seen before; instead, participants needed to remember which two specific social media posts (accusation, accusation control, refutation, or refutation control) they had seen previously for a given candidate and which two they had not. Data from the choice task were less clear. Here, we limited the analysis to candidate pairings in which all four stimulus types were remembered correctly for both candidates, and to individuals for whom at least three choice trials relevant to CIEs met this criterion. This analysis shows a significant CIE at a short delay in Experiment 1, t(186) = -3.44, p = .0007, d = -.25, but not in Experiment 2, whether at a short delay, t(48) = -1.57, p = .124, d = -.22, or at a long delay, |t|(48) < 1, d = -.11. Still, all effects were in the same direction as in the analysis not conditionalized on memory, with corrected accusation candidates less likely to be chosen than no accusation candidates, and with effect sizes that are largely comparable to the main analysis. To more systematically test whether the observed null effects may be due to a failure to remember the refutation, we compared whether the proportion of corrected accusation candidates chosen differs within the same individuals when the analyses are or are not conditionalized on memory.
No evidence for such a difference was found, whether in Experiment 1, t(186) = -1.49, d = -.11, p = .14, in Experiment 2 at a short delay, |t|(48) < 1, d = -.06, or in Experiment 2 at a long delay, |t|(48) < 1, d = -.06. Thus, the null effects for choices in Experiment 2 are more likely due to a lack of power when conditionalizing on successful memory after a two-day delay; we find no direct evidence supporting a role for memory failures in producing CIEs. Analyses of CIEs conditional on memory that show similar results for alternative exclusion criteria are reported in 
SI Table S2
.


Individual Difference Predictors of CIEs
We examined bivariate correlations to determine which factors predict CIEs on the immediate feeling thermometer measure. We focus on immediate ratings because CIEs are larger and more reliable for this measure than for delayed ratings or choices. Specifically, as an estimate of reliability, we calculated CIEs for three subsets of the candidates, each composed of all candidates running for one of the three political offices, and examined whether CIEs were correlated across these three subsets. As described further in SI Results and SI 
Table S3
, these correlations were consistently and significantly greater than zero for immediate ratings in both experiments, but no significant positive correlations were observed for delayed ratings or choices. As might be expected based on this lack of internal consistency, no individual difference measures reliably predicted CIEs on delayed ratings and choices in either experiment (see SI Results).
Three variables were significant predictors of immediate CIEs in both Experiment 1 and Experiment 2 (see 
Figure 3
; SI 
Table S4
). Higher faith in intuition predicted stronger CIEs (Exp. We next examined whether any of the tested individual difference measures show reliably different correlation coefficients with CIEs compared to headline accuracy discernment. To address this question, we used tests of dependent correlation coefficients 
(39)
, accounting for the correlation between CIE and accuracy discernment measures. The most notable finding from these analyses (see 
SI Table S4
) was that the effect of political party differed between measures 
(Figure 4
). Specifically, Republicans scored worse on headline accuracy discernment (Exp. 1: r = -0.244, pFDR < 0.0001; Exp. 2: r = -0.314, pFDR < 0.0001), but there was no effect of party on CIEs (Exp. 1: r = -0.034, p = 0.48; Exp. 2: r = -0.013, p = 0.77), and this difference was significant in both experiments (Exp. 1: t = 3.60, pFDR = 0.003; Exp. 2: t = 5.61, pFDR < 0.0001).
The effects of political party on CIEs and MIST were also significantly different from each other in Experiment 2, and marginally different in Experiment 1, when using an SUR regression approach (41) that we had originally preregistered as our analysis for this test for Experiment 2 (SI 
Tables S5-S6
).
There were other differences between the correlations with CIEs and MIST, but these reflected a difference in degree of relationship rather than in whether a relationship is present at all. Specifically, digital literacy and all three epistemic belief subscales were more strongly correlated with the MIST than CIEs, but these variables showed at least marginal relationships with both CIEs and MIST. Individual differences analyses that show largely similar results (i.e., all significant effects reported above are at least marginal) for alternative exclusion criteria are reported in SI Table S7-12.


Memory
Finally, we examined whether explicit memory differs for accusations versus refutations.
Although CIEs are still present when participants remember all information, memory differences could nonetheless provide insight into how accusations and refutations are processed differently.
We specifically examined whether the categorical benefit to memory for stimuli that should impact social impressions (i.e., accusations and refutations), relative to matched control stimuli lacking such impact, would differ for accusations vs. refutations. To do so, we ran a 2 (Impactfulness: Accusation/Refutation vs. Control) x 2 (Post: Post 1 vs. Post 2) repeatedmeasures ANOVA. In Experiment 1, this analysis showed a main effect of impactfulness, F 
(1,
 403) = 23.89, p < .0001, hp 2 = .056, with impactful stimuli generally being remembered better than control stimuli. There was no main effect of post, F(1, 403) < 1, nor was there a reliable interaction between impactfulness and post, F(1, 403) = 1.90, p = .17, hp 2 = .005, indicating that accusations benefit about as much as refutations in memory relative to matched control stimuli ( 
Figure 5A
).
We examined this effect again in Experiment 2, in which memory was tested 2 days after encoding rather than ~30-40 minutes after encoding ( 
Figure 5B
). Because emotional stimuli tend to affect memory consolidation more than immediate memory 
42
 In this study, we report a novel approach studying how false accusations can harm politicians' reputations, even after those allegations have been factually refuted. We found that hypothetical candidates targeted by false but refuted accusations were rated more negatively than those never accused. The negative impression persisted regardless of whether people rated the candidates immediately, after a short delay, or even after two days, consistent with prior work (e.g., 33). We also discovered reliable individual differences in CIEs on immediate ratings. The tendency to maintain negative impressions is stronger for those who report relying more on intuition, people with lower digital literacy skills, and younger individuals. Interestingly, political affiliation did not affect this tendency. Finally, although we did not find direct evidence that CIEs result from selective retrieval of accusations, we did find that people have better memory for accusations than refutations after a two-day delay. We speculate that emotionally charged information (like accusations) simultaneously impacts both memory consolidation processes and decisions about political candidates.
Our findings replicate and extend a prior study testing impressions of mock political candidates accused of misconduct 
(10)
, while differing from other recent studies examining the formation of social impressions about ordinary individuals 
(13,
14)
. It has been suggested (43), based on the finding in (10) that CIEs may be eliminated when decisions are both preceded by explicit deliberation and are about a same-party political candidate, that CIEs towards politicians may be limited to when there is a reluctance to update negative views about opposing-party politicians. In contrast, we conclude that in a non-partisan political context, CIEs towards politicians are robust, and that CIEs apply more broadly in sociopolitical impression formation than other recent work has suggested 
(13,
14,
43)
. Our paradigm does differ in other ways, such as in having two stimuli about each of a large number of candidates rather than many stimuli about one targeted individual. Our design may be more ecologically valid, and under these conditions the cognitive bandwidth available for impression updating may be reduced, increasing the relative impact of emotionally salient accusations. In any case, we find strong evidence that people maintain negative opinions about politicians even when allegations are demonstrably false.
We find as well that the degree to which retracted information continues to influence social impressions varies reliably between individuals. Specifically, those who report relying more on intuition in choosing what to believe were more affected by debunked information.
These results build on previous work suggesting that an increased reliance on emotion, whether pre-existing or induced by task context, can lead to increased belief in false information 
(44)
. In contrast, those with higher digital literacy-an acquired understanding of how social media platforms work on a mechanistic level-are less influenced by false accusations. Understanding how social media works might make people more skeptical of the content they encounter online, making them more analytical about what they read, and potentially more willing to trust refutations from a verified source (cf., 15). Another question for future work is to examine how cognitive abilities, like working memory, can affect people's ability to spot false information, particularly in political contexts (cf., 
7 ,8, 9)
. Ultimately, these findings are most notable in showing that CIEs vary systematically with measures (i.e., reliance on intuition and digital literacy) that also predict the ability to explicitly discern true from false headlines (e.g., 
19,
20,
21,
44)
.
We find that partisan orientation is unrelated to the magnitude of CIEs. These results are interesting in the context of work emphasizing the role of partisan orientation and other socioemotional factors in headline accuracy discernment, in addition to analytic thinking, forming the "integrative account" of misinformation vulnerability 
(24,
25)
. We find that while Republicans were more likely to believe false headlines in the MIST, they were just as willing as Democrats to change their impressions of novel political candidates after an accusation was debunked. Our results suggest that Republicans are not inherently more vulnerable to making decisions based on debunked false information. On the MIST, true headlines reference specific recent issues and events, and false headlines are generated to resemble real-world misinformation. In recent years, American conservatives have been exposed to more false information than similarly situated liberals 
(30,
45,
46)
. In contrast, all participants in our study hear the exact same information about the fictional politicians. Thus, our data imply that the conservative worldview is not inherently more credulous or dogmatic (cf., 47).
We also observed that older adults consistently show reduced CIEs, replicating another recent study that used non-political stimuli 
(33)
. Older adults have previously been shown to have worse explicit memory that a piece of information had been tagged as a myth (6); however,
given that we find age predicts reduced CIEs on immediate judgments (where the stimuli are likely still in working memory), and that CIEs are present even when people remember all stimuli later, it seems reasonable to assume that the social impression judgments in our study rely to a substantial degree on processes other than explicit memory. Our results align with research on the age-related positivity bias 
(48)
, including recent work showing that older adults are more forgiving of selfish behavior than young adults 
(49)
. Older adults may be less inclined to judge a politician harshly based solely on an accusation, especially after evidence has refuted the allegation. Our result also aligns with prior work demonstrating better headline accuracy discernment in older adults 
(34,
35)
. However, it is possible that older adults who participate in online studies may differ from the general population, whether in cognitive ability (e.g., 50), digital savviness, or some other factor. Future work will need to address this limitation. It is also possible that at a delay longer than two days, memory differences with age would make older adults more vulnerable to CIEs than what we observed. Regardless, it remains unclear why older adults share more false information on social media than their younger counterparts 
(30,
31)
. Finally, we found evidence that people process accusations preferentially relative to refutations. In our second experiment, participants remembered accusations better than refutations after a two-day delay. These results differed from our first experiment, which used a shorter retention interval and showed no significant difference in memory between accusations and refutations. This result is consistent with accusations triggering greater emotional arousal, thereby enhancing memory consolidation 
(42)
. We also found tentative evidence, based on a correlation between CIEs and memory for accusations in Experiment 2 (see SI Results; SI 
Figure   S3
), that would more directly support the interpretation that a common mechanism such as emotional arousal strengthens both CIEs and memory consolidation. Still, more research is needed to establish a role for preferential processing of emotionally evocative accusations in decision making about political candidates.
One limitation of this study is that our newly developed digital literacy measure has not been rigorously validated from a psychometric standpoint. Nonetheless, our measure offers a more relevant assessment of digital literacy than other existing options. In one prior study 
(20)
, only one of four media literacy measures-specifically, information literacy-predicted how well people judged headline accuracy. This measure was notable for being the only one of the four measures to assess factual knowledge, but it does not focus on digital media. To our knowledge, the only measure relating factual digital literacy knowledge to misinformation vulnerability is a single question asking about the Facebook news feed 
(19)
. We used this question as a starting point but added additional questions, as we believe digital literacy is a broad enough construct to require multiple questions to assess robustly, following 
(51)
. As described further in SI Results, we confirmed using Cronbach's alpha that our measure had acceptable reliability in a pilot sample, but more work is needed to further validate and refine this measure. Still, we expect that an ongoing challenge in measuring digital literacy is keeping factual knowledge questions accurate and relevant as the online media landscape rapidly evolves. We expect this work to inspire new avenues for future research. Our work is a proof of concept that it is possible to measure how strongly refuted false information continues to affect decisions about political candidates at the individual level. Moreover, our method focuses on how misinformation influences more consequential political decisions than the judgment of whether a piece of information is accurate, as even information known to be inaccurate can still impact decisions. Finally, this approach allows for measurement of misinformation's impact on political decisions over time, not just immediately after exposure. Our method can be adapted to study misinformation in other areas, including false claims about the COVID-19 vaccine or medical treatments. This work is a critical step towards achieving a better understanding of how misinformation affects decisions and developing interventions that mitigate the real harms of false information by considering not just beliefs but also tangible behaviors.


Methods


Stimuli
We originally developed 36 fictional political candidates but reduced this number to 27 for the current set of studies to keep the experiment under 60 minutes. Both versions are available in our OSF repository (https://osf.io/gjpr9/). Each candidate's story was inspired by a real politician targeted by a false accusation that was later debunked by media sources, e.g., FactCheck.org. The stories covered nine scandal categories: bribery, electoral fraud/interference, embezzlement/self-dealing, racism, abuse of power or discretion, sexual harassment, foreign influence, financial fraud, and pedophilia/bestiality.
Our stimulus set included 18 men and 9 women. Of these candidates, 18 were white, and 9 were non-white (equally split between black, Hispanic, and Asian). This distribution roughly matches the actual demographics of U.S. political officeholders while allowing each of the three counterbalancing groups to have an equal proportion of candidates across race and gender. Candidates were evenly split between running for U.S. Senate, state governor, and U.S. House positions. We tried to match the fictional candidates to their real-life inspirations in terms of race, gender, age, and political office. However, some deviations were necessary because white men were overrepresented in the real-life sample relative to our intended distribution.
We constructed a set of core stimuli for each candidate, consisting of an introductory biography and four social media posts: accusation, refutation, accusation-matched control, and refutation-matched control. The bios were loosely based on the real politician who inspired the story and typically included vague policy positions but avoided identifiable details. Each bio featured a campaign-style banner with the candidate's face, name, and prospective political office. The social media posts included this banner, along with a blurred stock photo image as a thumbnail with a blurred poster name next to it, a brief description of the story, and a headline and stock photo image previewing a hypothetical linked article on Facebook.
Each participant saw two social media posts about a given political candidate. The first post was either an accusation or accusation-matched neutral post, while the second was either a refutation or a refutation-matched neutral post. To minimize possible confounds, the meaningful posts (accusations or refutations) were matched to the paired control stimuli on general topic, article preview image (but with different text captions), and the blurred author thumbnail. For the second post (refutation or neutral), the thumbnail was a blurred logo and blurred name of a reputable news source intended to be politically neutral (e.g., Reuters) with an unblurred blue checkmark next to it. When refuting the accusation, these posts gave a clear causal account of the allegation by explaining how it came about via either a misunderstanding or deliberate fabrication.
CIEs were apparent in the first version of our stimulus set (see SI Results). However, we pilot-tested a series of modifications to ensure that for each candidate, uncorrected accusations negatively impacted post-story ratings, refutations countered this negative impact relative to refutation controls, and neutral posts did not change opinions much in either direction. In this process, we did not modify refutations to make them less effective at correcting false allegations. See SI Methods for additional details about stimulus design and pilot testing.


Experiment 1 -Exploratory study


Participants
We planned to recruit 500 participants using the CloudResearch MTurk Toolkit. We chose the sample size based on our budget and preliminary power analysis, which showed that we could detect a bivariate correlation of at least r > .125 with 80% power. Recruitment was stratified by age, with equal samples targeted from five age brackets 
(18-29, 30-39, 40-49, 50-59, 60+
). Participants were paid $8.00 for completing the study.
We excluded participants if they: failed simple attention checks at the start of this study, were removed from the CloudResearch Approved Participants list by July 2024, reported an age that did not match their birth year on file with CloudResearch (i.e., off by more than one year), or had a median time of less than 1. We also report data from our primary analyses with minimal exclusions in SI Results.
Based on peer review feedback, we modified the initial exclusion criteria noted in our preregistration. Data using our original preregistered exclusion criteria (below-chance performance on either the MIST or digital literacy measures) are also reported in SI Results. Note that in Experiment 2, only those who met our original exclusion criteria were invited back for the delayed judgments and memory test. To enable comparisons in memory performance across the two experiments, we excluded from analyses of memory performance in both experiments individuals who failed our original exclusion criteria. This yielded an additional 24 exclusions in Experiment 1 -22 individuals who were below-chance on the MIST, 1 who was below-chance on the digital literacy measure, and 1 who failed on both measures. Results were similar without making these additional exclusions. Finally, 9 additional individuals were excluded from analyses of memory performance because they scored below chance on the memory test.  
Figure 1A
) for all 27 candidates in random order. Immediately after each bio, they rated how much they liked each candidate on a 0-100 feeling thermometer scale (initial ratings).


Procedure
Participants then saw two mock social media posts about each candidate ( 
Figure 1B
). Candidates were evenly divided between the three conditions (corrected accusation, uncorrected accusation, or no accusation), with candidate assignment to these conditions counterbalanced across participants. This is the only stimulus feature that was varied systematically and counterbalanced. After seeing each pair of posts, participants again rated each candidate on the 0-100 feeling thermometer scale (immediate post-story ratings; 
Figure 1C
).
After viewing posts about all 27 candidates, participants completed several questionnaires: the Cognitive Reflection Test (CRT-2) (52); epistemic beliefs measure 
(21)
; two affective polarization measures -a dictator game, following 
(53)
, and a partisan feeling thermometer, following (54); a belief superiority measure 
(55)
; and the MIST-20 headline accuracy discernment scale 
(40)
. We also included a novel digital literacy measure, building on 
(19,
20)
; the digital literacy measure included 6 multiple-choice questions with 4 response options each to examine factual knowledge of user experience and content moderation on social media platforms. More specifically, the digital literacy measure includes three questions about specific platforms (Facebook, Twitter, and TikTok) and three questions about specific concepts (phishing, blocking, and tagging). We computed digital literacy scores as the percent correct across all six items. See additional details in SI Results and Appendix. All measures that were collected are listed here.
Following these questionnaires, participants made two judgments about each candidate. 


Data analysis
On feeling thermometer rating measures, we computed a CIE score for each individual by subtracting the individual's mean score for no accusation candidates from the mean score for corrected accusation candidates. Note that this means that negative scores indicate the presence of CIEs, with larger CIEs yielding more negative scores. This approach was chosen so that negative values indicate greater influence of false information for both CIEs and headline accuracy discernment. For choices, CIEs were computed as the proportion of trials for which a corrected accusation candidate was chosen, from trials in which the choice was between a corrected accusation candidate and a no accusation candidate. Here again, lower scores indicate greater CIEs.
We also computed an affective polarization measure for those who expressed a preference between the Republican and Democratic parties; those with a weak preference were included, but this measure could not be computed for those who expressed no partisan preference. We computed the difference in the amount shared (out of $10) with in-party vs. outparty targets in the dictator game, as well as the difference between in-party vs out-party feeling thermometer ratings. The Z-score of the in-party vs. out-party difference for each measure was computed across the sample, and the final measure of affective polarization was the participant's average Z-score across the two measures. Based on the finding in Experiment 1 that greater affective polarization predicted larger CIEs, our preregistration included a mixed-effects analysis examining whether such an effect interacted with perceived candidate ideology and participants' political ideology on the item level. The overall effect of affective polarization did not replicate in Experiment 2, however, so this analysis is reported only in SI Results.
For demographic variables of education and income, we converted categorical responses to ordinal numbers for inclusion in regressions, as described in our preregistration. Education was converted to years of education such that "Doctoral degree" = 20, "Master's degree" = 18, "Bachelor's degree" = 16, "Associate's Degree" = 14, "Some college" = 13, "High school diploma" = 12, "Have not finished high school" = 11. A small number of respondents chose "Other", and these responses were coded on an ad hoc basis, with vocational/technical school graduates and those currently in college coded as 13, and a respondent indicating a professional degree coded as 19. Income was converted to a category, consistent with our preregistered analysis plan, with "Under $20,000" = 1, "$20,000-$40,000" = 2, "$40,000 -$75,000" = 3, "$75,000-$100,000" = 4, "$100,000-$500,000" = 5, and "Over $500,000" = 6.
We examined the correlations between CIE and MIST and the In addition to these bivariate correlations, we also examined the significant predictors of CIEs and MIST in multiple linear regressions that included all predictor variables, as well as in regression models selected through a stepwise procedure via the stepAIC algorithm in R. Note that the examination of the bivariate correlations and full multiple regression model was a deviation from our preregistered analysis plan, which only specified stepwise regression (see SI
Results for further explanation of this change made in response to peer review feedback).
In the memory test, for any given candidate and participant, two stimuli were actually "old" and two were actually "new", with the specific assignment of candidate to condition varying based on counterbalancing. The memory test was structured such that only one of the accusation or the accusation control stimuli, and only one of the refutation or refutation control stimuli, would appear in the first half of the memory test, with other stimuli presented in the second half of the memory test. Specific stimuli presented in each half of the test were counterbalanced. For analyses of memory data by condition, we only used data from the first half of the test, to avoid contamination of memory estimates when participants had already seen a matched stimulus earlier in the memory test. Both "Definitely old" and "Probably old" were counted as "old" responses, while "Definitely new" and "Probably new" were counted as "new" responses, and "Not sure" responses were excluded from analysis. Hit rates and false alarm rates were computed for each of the four types of stimuli, with up to 54 trials per condition, and d' scores for each stimulus type were calculated with the log-linear correction applied (57). In the separate set of analyses in which candidate impressions were computed based only on trials for which all stimuli were remembered accurately, data from the full memory test data were used, with four stimuli per candidate.


Experiment 2 -Replication study


Participants
We planned to recruit 500 participants again using CloudResearch MTurk Toolkit, stratifying by age across five brackets (18-29, 30-39, 40-49, 50-59, 60+). Due to a data collection error, we recruited an additional set of ~100 participants in the 30-39 age bracket and a smaller than anticipated sample in the 18-29 age bracket. Participants received $7.50 for completing the first part of the study and an additional $5.00 if they returned for the second part of the study two days later.
Of the 561 participants, we excluded data from 44, yielding a sample of 517 participants.
Our exclusions mirrored those from our first study, with some participants again failing multiple criteria: 1 failed simple attention checks, 11 were removed from the CloudResearch Approved list, 17 reported their age inconsistently, 12 had below-threshold viewing time on Post 1, and 12 had below-threshold viewing time on Post 2. As noted above, these exclusion criteria represent a change from our preregistered plan. However, for the memory test and other long-delay measures, we did not invite back 46 participants who did not meet our original exclusion criteria.
This included 42 individuals who scored below chance on the MIST, 3 who scored below chance on the digital literacy measure, and 1 who scored below chance on both. After these exclusions and natural attrition, we had 402 individuals who completed both study sessions. For memory performance measures, we excluded another 15 participants who scored below chance on the memory test.      


Supplemental Information (SI)


SI Methods


Stimulus construction
Face stimuli were selected from AI-generated novel faces available on the Web site https://generated.photos. We aimed to get an even distribution of perceived ages from about 30 to 75 years old. In a pilot study, participants (100 from Amazon MTurk, 44 from the UPenn Psychology participant pool) rated each of 72 faces on attractiveness, warmth, competence, threateningness, likelihood that they would vote for that individual for a political office (e.g., for Congress), and estimated age. Faces were chosen to represent a range of ages and were selected to avoid outliers that were rated either particularly high or particularly low in attractiveness, warmth, competence, threateningness, or likelihood of voting for the person. The final full set of 36 candidate faces is composed of 2/3 male and 1/3 female faces. Of these, 27 are perceived as non-Hispanic white, 3 as Black, 3 as Asian, and 3 as Hispanic, with perceived race initially determined by the study team and later confirmed in pilot data.
Candidate names were generated from a Web-based tool at https://www.namegenerator.org.uk/. We began with 99 names, 63 with surnames which could be either White or Black, 18 with Asian surnames, and 18 with Hispanic surnames, as determined by the name generator tool and confirmed by our team. An equal number of names were generated with approximate year of birth in 1989, 1974, and 1959. In a pilot study, 150 participants from Amazon MTurk and 36 participants from the UPenn Psychology participant pool rated a randomly selected sampling of 66 names on the same questions used to evaluate faces. As for the faces, these ratings were used to guide selection of the final set of names so that perceived ages 2 matched between faces and names, and to avoid names that were outliers on other measured features.
Once we had constructed introductory bios, these were presented along with names and faces. These stimuli were pilot-tested on perceived race/ethnicity and judged political party, as well as on the other measures described above (attractiveness, warmth, competence, threateningness, likelihood of voting for that individual for a political office, and estimated age).
An initial group of 100 pilot participants were recruited from Amazon MTurk. Bios that were outliers on rating measures were revised and the modified set was then tested on another group of 101 pilot participants from Amazon MTurk. Ratings of judged political party from this final sample were used for the analysis examining how alignment between candidate political orientation and political orientation affected candidate ratings.
Finally, we added the accusations, refutations, and matched control stimuli to the paradigm. In this phase of pilot testing, we examined the difference between feeling thermometer ratings made immediately after reading the two critical story stimuli (post-story) and an initial feeling thermometer rating made after only the introductory bio (pre-story) to determine whether each item was behaving as expected.
CIEs were apparent in the aggregate even in the first full version of our stimulus set (see SI Results). Still, we made further modifications to the stimuli to ensure that for each item, poststory ratings shifted in the expected direction relative to pre-story ratings. The 36-item stimulus set used in pilot testing included 3 additional types of accusations (illegal campaign contributions, sexual misconduct, and murder) and a fourth political office (state legislature). We 


SI Results


Note on exclusions
Key analyses (specifically, main effects of stimulus condition, main effects of condition restricted to trials with successful memory, and analyses of individual differences in CIEs derived from immediate ratings via both correlation and regression approaches) are reported in SI Results using three different sets of criteria for excluding inattentive responders. Supporting the need for exclusions is one recent analysis (Douglas et al., 2021) finding that only 63% of participants on CloudResearch MTurk Toolkit met stringent data quality checks. This was certainly better than the 26% who met the same threshold with a base Amazon MTurk sample, but still not ideal. Other work has shown that in at least some cases, these inattentive responders can produce spurious effects (Zorowitz et al., 2023).
Our primary exclusion criteria were chosen in response to peer review feedback. These exclude all participants who failed a basic attention check, were removed from the CloudResearch approved list prior to July 2024, reported their age inconsistently, or had a median reading time of less than 1.5 s for at least one of the two articles, as described in the main text. Analyses that refer to minimal exclusion criteria only exclude a small number of individuals who failed the basic attention check (n = 3 in Exp. 1 and n = 1 in Exp. 2), as well as, in Experiment 2, the second instance of a participant who apparently participated twice (with the same MTurk ID). Analyses that refer to the preregistered exclusion criteria exclude these participants who failed the minimal exclusion criteria and also exclude anyone who scored below chance on either of our measures with objective answers, the MIST and digital literacy. This led to the exclusion of 38 additional participants in Experiment 1 (31 who scored below chance on the MIST, 2 who scored below chance on digital literacy, and 5 who scored below chance on both measures), yielding 458 total participants. In Experiment 2, this led to the exclusion of 59 additional participants (50 who scored below chance on the MIST, 4 who scored below chance on the digital literacy measure, and 5 who scored below chance on both measures), yielding 500 total participants.


Analyses controlling for initial ratings
Comparisons between initial ratings and immediate post-story ratings for corrected accusation candidates are described in the main text. When comparing immediate post-story ratings with initial ratings for candidates in the uncorrected accusation condition, there was a An alternative approach to accounting for initial ratings is to use a linear mixed effects model controlling for initial ratings, counterbalancing condition, and random effects of participant. In Experiment 1, with immediate ratings for each candidate as the outcome measure, this analysis confirms that ratings were lower for both corrected accusation candidates (b = - 


Assessment of reliability of CIEs
As an estimate of internal consistency, we computed CIEs separately for the three groups of candidates running for different political offices. Within each of these three groups, there were 9 candidates, with between 2 and 4 candidates assigned to each condition. See 
Table S3
 for the correlation coefficients for each pairing of these three groups.
The average correlation for immediate ratings was significantly greater than zero in both 


Individual Difference Analyses
Bivariate correlations between CIEs and all individual difference measures are reported below 
(Table S4
, S7, S10) under all three sets of exclusion criteria. For data from Experiment 2, data for additional exploratory variables are also reported; these variables are separated out because they are not included in the multiple comparison correction for the primary set of 12 variables. Instead, multiple comparison correction for these variables only is applied across the expanded set of 16 variables. Race and ethnicity are also analyzed separately, as we felt that these could be analyzed most effectively as dummy coded variables for Black, Asian, and Hispanic ethnicity, so we ran separate regression models to test these variables.
In our preregistration, we planned to examine individual differences using stepwise regression to include only the predictor variables that yield the minimal AIC value for the model as a whole. As noted in the main text, we now also report multiple linear regression analyses that include all predictor variables common to both experiments to avoid pitfalls of stepwise regression (e.g., 
Smith, 2018)
. We also decided that bivariate correlations might be of greater interest to some readers, as these show the relationships between two variables without controlling for other variables as in multiple regression. In addition, more participants could be included when calculating the bivariate correlations, addressing a reviewer concern about the fact that a large number of participants needed to be excluded from the regressions since the affective polarization measure could not be computed for participants reporting no partisan preference. Full multivariate regressions using CIEs and MIST as outcome measures using all 3 sets of exclusion criteria are reported below 
(Table S5
, S8, S11), as are stepwise regressions using all 3 sets of exclusion criteria 
(Table S6
, S9, S12) Additionally, to compare whether these regressions produce significantly different coefficients when MIST vs. CIE are used as the outcome measure, we used SUR models (ref.


41)
, as specified in our preregistration. Coefficients were compared for all predictor variables that were either significant or marginal for either CIE or MIST outcome measures.
In our preregistration, only these SUR regressions required multiple comparison correction, and we specified the Bonferroni-Holm method this correction, given its simplicity of computation. However, when we added several more analyses that required multiple comparison correction, we opted to apply FDR correction in all analyses, as it provides a better balance between statistical power and control for Type I errors.
In Experiment 2, exploratory individual difference measures were added. These exploratory results are reported here. One notable result was with respect to actively openminded thinking (AOT). As shown in 
Table S4B
, high AOT predicted better headline judgment accuracy but not CIEs. This suggests that AOT can help identify misinformation when that information connects to a real-world media environment, while apparently not affecting the tendency to believe or disbelieve refuted scandal narratives. In addition, cognitive empathy was a reliable predictor of CIEs, with higher cognitive empathy producing larger CIEs. There was no such relationship observed with headline judgment accuracy scores, but the difference between measures was not significant. This result is consistent with prior work suggesting that increasing perspective-taking increases condemnation of and punishment towards a transgressor when malevolent intentions are attributed to the transgressor 
(Lucas et al., 2016)
. Those who are better at mentalizing may similarly have a stronger negative reaction to the initial reported transgression here (as is also apparent for uncorrected accusations; see 
Table S15
), which would then reduce the effectiveness of the subsequent correction.


Relationship between affective polarization, perceived candidate ideology, participant ideology, and CIEs
In our preregistration, we planned to examine whether the effect of affective polarization on CIEs (observed in Experiment 1) was associated with an accentuation of a potential interaction between candidate ideology and participant ideology on CIEs. Because no effect of affective polarization was apparent in Experiment 2, this analysis was no longer of central interest, and instead is reported here (see SI 
Table S13
). Measures of perceived candidate ideology were obtained from a separate pilot sample that only saw the bios. These perceptions of candidate ideology and participants' self-reported ideological orientation, as well as each individual's score on the affective polarization measure, were regressed onto candidate ratings in exploratory linear mixed effects analyses, controlling for initial ratings made after reading the candidate bios. Consistent with the effects reported in the main text and in 
Table S4
, Experiment 1 showed an interaction between affective polarization and the degree to which ratings were lower for candidates with corrected accusations (b = -0.034, t = -2.92, p = .003), but no such effect was observed in Experiment 2 (b = -0.009, t = -0.86, p = .39). Some other effects were apparent, of possible interest for future work, beyond those involving affective polarization.
Specifically, in both experiments, ratings in the no accusation condition were higher when candidate ideology and participant ideology were more aligned, as evidenced by positive twoway interaction effects (Experiment 1: b = 0.057, t = 3.97, p < .0001; Experiment 2: b = 0.085, t = 6.51, p < .0001). In Experiment 2, three-way interactions indicate that this effect was reliably reduced for candidates in the corrected accusation (b = -0.026, t = -2.51, p = .012) and uncorrected accusation (b = -0.029, t = -2.74, p = .006) conditions. In Experiment 1, these effects were not significant, but the trends were towards a reduced effect of ideological alignment in both the corrected accusation (b = -0.016, t = -1.35, p = .18) and uncorrected accusation (b = -0.013, t = -1.16, p = .24) conditions. These results suggest that in the relatively non-partisan political context that our paradigm simulates, ideological alignment is not protective against CIEs and may in fact worsen them.


Relationship between memory and CIEs
In exploratory follow-up analyses, we find evidence potentially linking the initial processing of accusations with increased persistence of refuted accusations in decision making.
Specifically, in Experiment 2, better memory two days later for accusation stimuli, relative to accusation control stimuli, correlated with greater immediate CIEs, r(385) = -.142, p = .005 (see 
Figure S2
). An analogous analysis showed no reliable effect for refutation stimuli, r(385) = -.079, p = .12. However, the difference between these effects for accusation and refutation stimuli was not significant, p = .41. In Experiment 1, when memory was measured at a short delay, there was no relationship between CIEs and memory for accusation stimuli, r(421) = .043, p = .38, or for refutation stimuli, r(421) = .00, p = .95. The difference between experiments is likely due to effects of selective consolidation that were apparent by the time of the memory test in Experiment 2, two days after encoding, but had not yet emerged at the time of the Experiment 1 memory test, similar to what we observe in the analysis of memory by condition.
Note that because memory for accusations two days later correlated with CIEs that were computed from immediate ratings, a causal link from memory to ratings is temporally implausible. Instead, we interpret this relationship as tentative evidence for a common mechanism by which prioritized initial processing of accusations, perhaps due to an increase in emotional arousal, strengthens both memory and CIEs. The case for this common mechanism would be stronger if the relationship between accusations and CIEs were significantly stronger than the relationship between refutations and CIEs. As it is, an alternate explanation, that individuals who are more engaged in the task show both better memory for impactful stimuli at a two-day delay and larger CIEs, cannot be ruled out. Future work in which emotional arousal is measured more directly than in the present studies would be better positioned to address these issues.


Individual Differences in CIEs after a Delay
To examine the effects of individual difference measures on candidate assessments at a delay, because CIEs were strongly correlated between delayed ratings and delayed choices, we computed Z-scores for delayed rating and delayed choice measures and averaged these Z-scores to create a single measure for delayed CIEs 
(Table S14
). None of the individual difference measures reliably predicted CIEs, after correcting for multiple comparisons, at a short delay in either experiment or after a long delay in Experiment 2.


Accusation and Correction Effects
We also report here results for the secondary outcomes referenced in our preregistration, accusation effects and correction effects. Individual difference analyses for accusation effects and correction effects are reported in 
Table S15 and Table S16
.


Initial Pilot data
In order to demonstrate that CIEs were present with these stimuli prior to any modifications, we focus on the 150 participants recruited from CloudResearch MTurk for pilot 


Pilot data comparing short vs. long delay assessments
While in most aspects our pilot data were highly similar to the main study with respect to main effects, one set of findings from the pilot data requires more detailed explanation, as these data inspired a prediction in our preregistration that was not confirmed. Specifically, we expected to find that the degree to which corrections ameliorated the impact of accusations on choices would be reduced after a longer delay. This expectation was based largely on data from rounds 5 and 6 of pilot testing, in which assessments after a 48-hour delay were included.
Both round 5 and round 6 of the pilot study included a choice task at both short-delay (5-10 min) and long-delay (2 days later) time points. We report those data combined across these two experiments. We found that CIEs on the choice task grew from the short-delay assessment to the long-delay assessment, t(222) = 2.95, p = .004, d = .20. Specifically, the proportion of trials between a corrected accusation and a no accusation candidate for which the corrected accusation candidate was chosen dropped from a mean of 48.1% (SE = 0.6%) to a mean of 46.4% (SE = 0.7%). Short-delay feeling thermometer ratings were not included in round 5, so differences between short-delay and long-delay ratings could only be examined on data from round 6. Here, CIEs did not reliably differ from the short-delay assessment (mean ratings of 55.3 for corrected accusation stimuli vs. 57.2 for no accusation stimuli) to the long-delay assessment (mean ratings of 54.1 for corrected accusation stimuli vs. 56.1 for no accusation stimuli), t(94) < 1, d = .04.
Thus, our pilot work suggested that corrections might lose effectiveness over time, leading to greater CIEs on long-delay vs. short-delay measures. We did not find such a result in the main studies. Of course, it is possible that the result in our pilot studies was a Type I error, or that the failure to find this result in the main study was a Type II error. However, one difference that might be important between the pilot and main studies is that the delay interval before the short-delay assessments was somewhat longer in the main studies, because additional questionnaire measures were placed after the initial presentations of the core stimuli and immediate ratings but before the delayed rating phase. This difference may have made the shortdelay and long-delay ratings more similar to each other in terms of cognitive processing. Further work will be necessary to examine these questions.


Digital literacy measure
The digital literacy measure was constructed using an initial set of 8 questions. One question was taken directly from 
Sirlin et al., 2021 (ref. 19
 in the main text), while others were written by our team. We ran a pilot test on this first version using 78 participants recruited from questions yielded accuracy well above chance, while three were at chance-level accuracy. For the second pilot version, these three questions were modified, and the modified scale was tested on another pilot sample of 80 participants from CloudResearch MTurk Toolkit. Here, all stimuli were above chance accuracy. Additionally, we examined Cronbach's alpha for the second pilot version, which for the full 8-item scale was .66. There were two questions for which Cronbach's alpha increased if the item was deleted. Thus, we removed those two items to yield the final sixitem scale described below. Cronbach's alpha among the pilot sample for these 6 items was .72, which we determined to be acceptable reliability.    
Table S2
. Main effect analyses limited to trials in which all stimuli for a given candidate were successfully identified on the recognition test. Results are presented using the main exclusion criteria, the original preregistered exclusion criteria, and a minimal set of exclusion criteria.   
corrected accusation (Post 1: Accusation; Post 2: Refutation), uncorrected accusation (Post 1: Accusation; Post 2: Refutation Control), or no accusation (Post 1: Accusation Control; Post 2:


delay in Experiment 1, t(436) = -5.48, p < .0001, d = -.26 (Figure 2C). In preregistered analyses for Experiment 2, CIEs were present on immediate ratings, t(516) = -11.35, p < .0001, d = -.50 (Figure 2D), on ratings made after a short delay, t(516) = -6.17, p < .0001, d = -.27 (Figure 2E), and on choices made after a short delay, t(516) = -5.12, p < .0001, d = -.23 (


When conditionalizing on memory, CIEs were present on immediate ratings in Experiment 1, t(319) = -5.94, p < .0001, d = -.33, and in Experiment 2, t(217) = -6.06, p < .0001, d = -.41. CIEs conditionalized on memory were also present on short-delay ratings in Experiment 1, t(319) = -5.79, p < .0001, d = -.32, on short-delay ratings in Experiment 2, t(217) = -4.48, p < .0001, d = -.30, and on long-delay ratings in Experiment 2, t(217) = -4.99, p < .0001, d = -.34.


1 :
1
r = -0.277, pFDR < 0.0001; Exp. 2: r = -0.144, pFDR = 0.013), while higher digital literacy (Exp. 1: r = 0.129, pFDR = 0.018; Exp. 2: r = 0.111, pFDR = 0.044) and older age (Exp. 1: r = 0.147, pFDR = 0.008; Exp. 2: r = 0.123, pFDR = 0.030) predicted weaker CIEs. Correlation tables showing all bivariate correlations with immediate CIEs, as well as regressions examining effects of race/ethnicity, are presented in SI Table S4. Regression analyses suggest that these three variables have largely independent effects on CIEs. Faith in intuition and age remain significant predictors of CIEs in both experiments in a multiple regression that includes all individual difference variables simultaneously, and both are also selected as significant predictors in stepwise regressions, which we had originally preregistered as our individual differences analysis for Experiment 2 (SI Tables S5-S6). Digital literacy remains a predictor of CIEs in a multiple regression including all variables simultaneously, and is retained as a significant predictor in stepwise regressions, in Experiment 2 but not Experiment 1 (SI Tables S5-S6).


, we preregistered a prediction of a reliable interaction effect in Experiment 2. Indeed, in Experiment 2, we found a main effect of impactfulness, F(1, 386) = 21.24, p < .0001, hp 2 = .052, a main effect of post, F(1, 386) = 28.61, p < .0001, hp 2 = .069, and critically, an interaction between these variables, F(1, 386) = 29.06, p < .0001, hp 2 = .070. The interaction indicates a strong memory benefit for accusation stimuli vs. accusation control stimuli, t(386) = 7.22, p < .0001, d = .37, but no advantage for refutation stimuli vs. refutation control stimuli, |t|(404) < 1, d = -.05. Finally, we ran an additional 2 x 2 x 2 (Impactfulness x Post x Experiment) mixed ANOVA to test whether the interaction between impactfulness and post differed as a function of the retention interval. This analysis showed a 3-way interaction, F(1, 789) = 10.17, p = .001, hp 2 = .013, indicating that the memory benefit for accusation stimuli was greater with a longer retention interval, as well as a main effect of experiment, F(1, 789) = 141.74, p < .0001, hp 2 = .15, reflecting poorer memory at the longer retention interval, an interaction between post and experiment, F(1, 789) = 11.53, p <.0007, hp 2 = .014, and other lower-order effects repeating those reported above. Discussion


5 seconds reading either the first or second post, which would imply an implausibly fast reading speed of about 1800 words per minute. A total of 499 participants completed Experiment 1. In total, we excluded 62 participants from all analyses, some of whom failed on multiple criteria: 3 failed simple attention checks, 15 were removed from the CloudResearch Approved list, 35 reported their age inconsistently, 11 had belowthreshold viewing time on Post 1, and 18 had below-threshold viewing time on Post 2. After exclusions, we had a final sample of 437 participants.


Study procedures were reviewed and approved by the University of Pennsylvania IRB #8 (protocol 844066) as consistent with the Declaration of Helsinki and the U.S. federal Common Rule (45 CFR part 46) regulating human subjects research. Informed consent was obtained from participants at the beginning of each study session. Participants first saw introductory bios (


First was a choice task. Here, each candidate was paired with each of the other candidates running for the same political office (Governor, U.S. Senate, or U.S. House), and participants chose on each trial which candidate they preferred by clicking on the banner for that candidate (Figure 1D). Participants made 36 choices per office, totaling 108 choice trials. Second, participants completed a delayed feeling thermometer rating for each candidate, in random order, again prompted only by the candidate banner. Third, in a recognition memory test (Figure 1E)they saw all four stimuli (accusation, accusation control, refutation, refutation control) for each of the 27 candidates, and provided ratings using a 5-point scale ranging from "Definitely new" to "Definitely old." Finally, participants answered demographic questions about their age, race/ethnicity, political affiliation and ideology (both following 7-point ANES survey format), education, income, household size, city, state, and ZIP code, and an open-ended feedback question.


following measures: CRT % correct, Digital literacy score, Epistemic beliefs (Faith in Intuition), Epistemic beliefs (Faith in Evidence), Epistemic beliefs (Truth is political), Political party (1-7 scale), Affective polarization, Belief superiority, Age, Gender (Male = 0, Female = 1), Education, and Income. Each set of correlation coefficients was corrected across these 12 measures using a False Discovery Rate (FDR) correction (56), implemented with the R p.adjust function. Effects of race and ethnicity were examined (using dummy codes for Black, Hispanic, and Asian identity) in separate multivariate regressions reported in SI Results. To test whether specific predictor variables had a different relationship with CIEs vs. MIST, we used the Steiger's Z test for dependent correlation coefficients (39), accounting for the correlation between CIE and MIST measures, and applying FDR correction across all 12 tests.


Procedure
Data collection and analysis for Experiment 2 were similar to Experiment 1, with the preregistered plan for Experiment 2 available at https://osf.io/ubw6m. We used the same measures as in Experiment 1, except we removed city and state demographic questions that duplicated ZIP code. We collected three exploratory individual difference measures: theQuestionnaire of Cognitive and Affective Empathy (58), which includes separate cognitive empathy and affective empathy components, the 12-item abbreviated Intolerance of Uncertainty (IUS) scale (59), and the 10-item Actively Open-Minded Thinking scale (25, 60). Multiple comparison correction for correlation coefficients with the 12 measures repeated between the two studies was applied only across those 12 measures. In analyses of the additional exploratory measures in Experiment 2, we applied multiple comparison correction across all 16 measures.Delayed choice and delayed rating measures were collected both at the end of the first experimental session and the beginning of the second experimental session. The recognition memory test was shifted to the end of the second session and was not administered in the first session.


Figure 1 .
1
Stimulus/task design for (A) introductory bios, (B) core stimuli, (C) ratings (immediate and delayed), (D) delayed choices, (E) memory test.


Figure 2 .
2
Main effects of condition on (A) Experiment 1 immediate ratings, (B) Experiment 1 short-delay ratings, (C) Experiment 1 short-delay choices, (D) Experiment 2 immediate ratings, (E) Experiment 2 short-delay ratings, and (F) Experiment 2 short-delay choices. Effects shown in Experiment 2 constitute a preregistered replication of effects observed in Experiment 1. Error bars represent +/-1 SE.


Figure 3 .
3
Relationship between CIEs showing that (A, D) higher self-reported Faith in Intuition is associated with larger CIEs, i.e., a larger drop in ratings in the corrected accusation condition relative to the no accusation condition, (B, E) higher digital literacy is associated with reduced CIEs, and (C, F) older adults show reduced CIEs, in (A-C) Experiment 1 and (D-F) Experiment 2. Shaded regions represent 95% confidence intervals.


Figure 4 .
4
Relationship between political party and both MIST-20 and CIE scores, showing that Republicans perform more poorly on the MIST-20 than Democrats, but do not show a larger CIE, in (A) Experiment 1 and (B) Experiment 2. Shaded regions represent 95% confidence intervals.


Figure 5 .
5
Memory by stimulus condition. (A) Experiment 1, memory after a short delay (~30 minutes) (B) Experiment 2, preregistered replication showing memory after a longer (two-day) delay. Error bars represent +/-1 SE. 56. Benjamini, Y., and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series B, 57, 289-300. 57. Hautus, M. J. (1995). Corrections for extreme proportions and their biasing effects on estimated values of d'. Behavior Research Methods, Instruments, & Computers, 27, 46-51. 58. Reiners, R. L. E. P., Corcoran, R., Drake, R., Shryane, N. M., & VÃ¶llm, B. A. (2011). The QCAE: A questionnaire of cognitive and affective empathy. Journal of Personality Assessment, 93, 84-95. 59. Carleton, R. N., Norton, P. A., & Asmundson, G. J. G. (2007). Fearing the unknown: A short version of the Intolerance of Uncertainty Scale. Journal of Anxiety Disorders, 21, 105-117. 60. Baron, J., Isler, O., & Yilmaz, O. (2022). Actively open-minded thinking and the political effects of its absence. PsyArXiv. https://psyarxiv.com/g5jhp/ 61. [dataset] Cohen, M.S., Halewicz, V., Yildirim, E., & Kable, J. W. (2024). Continued influence of false accusations in forming impressions of political candidates. OSF repository. DOI: 10.17605/OSF.IO/GJPR9


ran 7
7
rounds of pilot testing, with edits made to the stimuli after each round. Participant counts were as follows: Round 1 included a sample of 62 participants from the UPenn Psychology subject pool in addition to the 150 participants from CloudResearch MTurk Toolkit whose data are described below; Round 2 included 81 participants from UPenn Psychology subject pool; Round 3 included 185 participants from CloudResearch MTurk Toolkit; Round 4 included 145 participants from CloudResearch MTurk Toolkit; Round 5 included 150 participants from CloudResearch MTurk Toolkit; Round 6 included 151 participants from CloudResearch MTurk Toolkit; Round 7 included 152 participants from CloudResearch MTurk Toolkit.


significant decline in Experiment 1, t(436) = -26.91, p < .001, d = -1.29, and in Experiment 2, t(516) = -30.36, p < .001, d = -1.34. For candidates in the no accusation condition, there was a small increase in ratings after reading the stories, in Experiment 1, t(436) = 2.29, p = .023, d = .11, and in Experiment 2, t(516) = 2.54, p = .011, d = .11.


5 0
5
.119, t = -14.88, p < .0001) and for uncorrected accusation candidates (b = -0.458, t = -57.24, p < .0001) relative to a baseline of no accusation. A similar mixed effects model with delayed ratings as the outcome measure also shows lower ratings relative to the no accusation baseline for both corrected accusation candidates (b = -0.044, t = -5.21, p < .0001) and for uncorrected accusation candidates (b = -0.099, t = -11.60, p < .0001). In Experiment 2, analogous linear mixed effects models replicate the effects observed in Experiment 1. Immediate ratings were lower for both the corrected accusation (b = -0.122, t = -16.36, p < .0001) and uncorrected accusation candidates (b = -0.503, t = -67.46, p < .0001). Delayed ratings after a short delay were also lower for candidates shown with corrected accusations (b = -0.042, t = -5.18, p < .0001) and uncorrected accusations (b = -0.106, t = -13.07, p < .0001). Finally, delayed ratings made after a two-day delay were lower after both corrected accusations (b = -0.045, t = -4.85, p < .0001) and uncorrected accusations (b = -0.115, t = -12.30, p < .0001).


Experiment 1 (
1
r = 0.267, p < .0001) and Experiment 2 (r = 0.360, p < .0001). The same metric was not significantly above zero for delayed ratings (Exp. 1: r = -0.077, p = .108, Exp. 2: r = -0.076, p = .084), or delayed choices (Exp. 1: r = -0.098, p = .040; Exp. 2: r = -0.079, p = .073). These average correlation coefficients were computed by applying the Fisher's Z transformation to each correlation coefficient, averaging them together, then applying a reverseFisher's Z transformation to obtain a final metric. For delayed measures from Experiment 2, values are averaged across short-delay and long-delay measures. Given the poor reliability of delayed measures among different sets of candidates, we also examined whether the magnitude of CIEs correlated at all between immediate and delayed measures, and between different delayed measures. For this analysis, we used the full set of 27 candidates. In Experiment 1, there was a significant correlation between CIEs on immediate ratings and CIEs on delayed ratings (r = 0.262, p < .0001), as well as between CIEs on immediate ratings and CIEs on delayed choices (r = 0.213, p < .0001). Similarly, for Experiment 2, CIEs computed from immediate ratings correlated significantly with CIEs from ratings made at a short delay (r = 0.315, p < .0001), from ratings made at a long delay (r = 0.305, p < .0001), from choices made at a short delay (r = 0.274, p < .0001), and from choices made at a long delay (r = 0.158, p = .001). Furthermore, CIEs computed from delayed ratings and delayed choices were highly correlated with each other at a short delay in Experiment 1 (r = 0.574, p < .0001), at a short delay in Experiment 2 (r = 0.586, p < .0001), and at a long delay in Experiment 2 (r = 0.605, p < .0001). Finally, in Experiment 2, CIEs computed at a short delay and at a long delay correlated with each other for ratings (r = 0.730, p < .0001) and for choices (r = 0.646, p < .0001).


Accusation effects are computed by subtracting mean ratings for candidates with no accusation from mean ratings for candidates with uncorrected accusations. Note that accusation effects, like CIEs, are computed such that negative scores indicate larger effects. Correction effects are computed by subtracting mean ratings for candidates with uncorrected accusations from mean ratings for candidates with corrected accusations. Note that correction effects are computed such that positive scores indicate larger effects. Accusation effects and correction effects are of interest because they separate out two potential contributors to CIEs, stronger effects of accusations or weaker effects of corrections. Strong accusation effects were apparent on immediate ratings in Experiment 1, t(436) = -29.02, p < .0001, d = -1.39. Accusation effects remained present on ratings measured after a short delay, t(436) = -12.03, p < .0001, d = -.58, and were also present on choices after a short delay, t(436) = -13.51, p < .0001, d = -.65. In preregistered analyses in Experiment 2, the accusation effect replicated on immediate ratings, t(516) = -33.92, p < .0001, d = -1.49, on ratings made after a short delay, t(516) = -12.90, p < .001, d = -.57, and on choices made after a short delay, t(516) = -15.08, p < .0001, d = -.66. Finally, accusation effects remained after a 48hour delay in Experiment 2 on ratings, t(401) = -12.79, p < .0001, d = -.64, and on choices, t(401) = -13.02, p < .0001, d = -.65. Accusation effects did not differ in Experiment 2 between short delay and long delay measures for ratings, t(401) = 1.47, p = .14, d = .07. There was a difference between short delay and long delay choice measures, however, t(401) = -1.99, p = .048, d = -.10, as candidates with an uncorrected accusation were slightly more likely to be chosen after a long delay (M = 42.12%, SE = 0.61%) versus after a short delay (M = 41.35%, SE = 0.57%). Correction effects were strongly apparent in immediate ratings in Experiment 1, t(436) = 24.12, p < .0001, d = 1.15. These effects remained significant after a short delay for both ratings, t(436) = 6.49, p < .0001, d = .31, and choices, t(436) = 7.88, p < .0001, d = .38. Preregistered analyses in Experiment 2 show that correction effects on immediate ratings replicated, t(516) = 28.27, p < .0001, d = 1.24, as did correction effects measured at a short delay for both ratings, t(516) = 9.05, p < .0001, d = .40, and choices, t(516) = 7.89, p < .0001, d = .35. Finally, correction effects remained significant after a 48-hour delay in Experiment 2 for both ratings, t(401) = 8.71, p < .0001, d = .43, and choices, t(401) = 6.44, p < .0001, d = .32. There were no differences between short-delay and long-delay correction effects, for ratings, |t|(401) < 1, d = -.03, or for choices, t(401) < 1, d = .03.


Experiment 1
1
as a comparable sample to that in the final experiments. Here, we found that immediate ratings for corrected accusation stimuli (M = 52.9, SE = 1.22) were lower than immediate ratings for no accusation stimuli (M = 62.2, SE = 1.01), indicating a significant continued influence effect, t(150) = -9.72, p < .001, d = -.79. Similarly, on a short-delay choice task, we found evidence for a significant CIE on choices between corrected accusation and no accusation stimuli, M = 46.7%, SE = 0.8%, t(150) = -4.28, p < .001, d = -.35 .


Figure S2 .
S2
Mean change between initial candidate ratings (prior to reading any social media posts) and ratings immediately after reading each set of posts in (A) Experiment 1 and (B) Experiment 2. Error bars represent +/-1 SE.


Figure S1 .
S1
Aggregate behavioral effects by condition in Experiment 2 after a long (2-day) delay on (A) ratings and (B) choices. Error bars represent +/-1 SE.


Figure S3 .
S3
Relationship between CIEs computed from immediate ratings and the memory benefit for accusation stimuli relative to neutral stimuli in Experiment 2. Shaded regions represent 95% confidence interval.


A. Experiment 1 B
1


posted it and click on the bell symbol at the top right corner b. Click on the arrow menu next to the post and select "report post" c. Click on the arrow menu next to the post and select "not interested" d. Go to the account that posted it, click on the three-dot menu at the top, and click "block" 4. What is phishing? a. Sending fraudulent messages pretending to be from reputable companies in order to get individuals to reveal personal information b. Luring someone into a relationship online through a fictional online persona c. When cyber-criminals hack into a computer network to extract sensitive information d. Embedding ads into website through layering them on top of each other so that they are not visible, but you may accidentally click on them 5. When you block someone on a social media site, what effect does it NOT have? a. They can't see your posts on the platform b. You can't see their posts on the platform c. They are banned from the platform for a period of time d. They can't see your engagement with accounts that you both follow 6. What is the term for when someone references you in a post and it sends a notification to you and/or your followers?


Table S3 .Table S4 .Table S5 .Table S6 .Table S7 .Table S8 .Table S9 .Table S12 .Table S13 .Participant Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus -0.026 -2.288 0.022*Affective Polarization Score x Corrected Accusation (vs. No Accusation) Stimulus -0.034 -2.923 0.003**Corrected Accusation (vs. No Accusation) Stimulus -0.129 -15.440 < 0.0001*** Uncorrected Accusation (vs. No Accusation) StimulusBy computer analysis of what stories might interest you. Tweets from accounts that follow you, but you don't follow back
S3S4S5S6S7S8S9S12S13
Reliability calculated as correlations between scores across the arbitrary distinction of candidates for different political offices, for immediate and delayed measures Bivariate correlations relating all predictor variables with immediate CIEs, MIST (headline accuracy discernment), and tests for the difference between two dependent correlations (Steiger's Z) using the primary exclusion criteria, and regression analyses for race/ethnicity demographic variables. Multiple linear regression results showing effects of all predictor variables on immediate CIEs, MIST (headline accuracy discernment), and SUR regressions comparing these two outcome variables for all predictors with significant or marginal effects, applying the primary exclusion criteria Stepwise linear regression results showing effects of all predictor variables on immediate CIEs, MIST (headline accuracy discernment), and SUR results comparing these two outcome variables, applying the primary exclusion criteria Alternate version of analyses presented inTable S4but applying exclusion criteria as originally preregistered Alternate version of analyses presented inTable S5 but applying exclusion criteria as originallypreregistered Alternate version of analyses presented inTable S6but applying exclusion criteria as originally preregistered Alternate version of analyses presented inTable S6but applying minimal exclusion criteria Mixed effects models examining effects of perceived candidate ideology, participant ideology, stimulus condition, and affective polarization on immediate ratings, applying the primary exclusion criteria How are decisions about what stories to show people on Facebook made? 1 a. At random b. By editors and journalists that work for news outlets c. By editors and journalists that work for Facebook d. 2. On Twitter, which kind of tweets will you NOT see in your feed? a. Retweets shared by people you follow b. Sponsored advertisements c. Tweets replied to by people that you follow d3. If you do not want to see a certain category of content on TikTok (not just from one account), what can you do to make similar content less likely to show up in your feed? a. Go to the account that
Short-delay Choice CIE Long-delay Choice CIE Experiment 1 Immediate Rating CIE -1.57 48 -0.79 48 Experiment 2 Immediate Rating CIE Experiment 1 Short-Delay Rating CIE Experiment 2 Short-Delay Rating CIE Experiment 2 Long-Delay Rating CIE Experiment 1 Short-Delay Choice CIE Experiment 2 Short-Delay Choice CIE Experiment 2 Long-Delay Choice CIE A. Experiment 1 Regression for race/ethnicity, Experiment 1 (n = 429) 0.12 -0.22 -1.47 49 0.43 -0.11 -0.88 49 Governor -U.S. Senate Governor -U.S. House r p r p 0.278 < 0.0001 0.320 < 0.0001 0.200 0.15 0.39 U.S. Senate --0.21 -1.47 49 -0.12 -0.88 49 U.S. House Average r p r < 0.0001 0.267 < 0.0001 0.15 0.39 p 0.315 < 0.0001 0.362 < 0.0001 0.398 < 0.0001 0.359 < 0.0001 -0.008 0.87 -0.037 0.44 -0.184 0.0001 -0.076 0.11 -0.061 0.17 -0.141 0.001 -0.080 0.071 -0.094 0.033 -0.020 0.68 -0.067 0.18 -0.085 0.087 -0.057 0.25 -0.085 0.075 -0.092 0.056 -0.116 0.015 -0.098 0.04 -0.060 0.17 -0.092 0.037 -0.086 0.051 -0.079 0.073 -0.019 0.71 -0.159 0.0014 -0.057 0.25 -0.079 0.11 MIST CIE Difference n r p FDR corr p r p FDR corr p p FDR corr p -0.21 -0.12 CRT 437 0.225 < 0.0001 < 0.0001*** 0.116 0.016 0.032* 0.064 0.11 Digital Literacy 437 0.293 < 0.0001 < 0.0001*** 0.129 0.0069 0.018* 0.0046 0.014* Epistemic Beliefs (Faith in Intuition) 437 -0.425 < 0.0001 < 0.0001*** -0.277 < 0.0001 < 0.0001*** 0.0066 0.016* Epistemic Beliefs (Evidence) 437 0.286 < 0.0001 < 0.0001*** 0.102 0.033 0.057 ~ 0.0016 0.006** Epistemic Beliefs (Truth is Political) 437 -0.347 < 0.0001 < 0.0001*** -0.148 0.002 0.008** 0.0005 0.003** Affective Polarization 387 0.019 0.71 ---0.136 0.0073 0.018* 0.014 0.028* Belief Superiority 437 -0.089 0.062 ---0.077 0.108 --0.84 --Political Party 437 -0.244 < 0.0001 < 0.0001*** -0.034 0.48 --0.0004 0.003** A. Experiment 1 (n = 377) Predictor variable MIST CIE Difference b t p b t p c 2 p pFDR CRT 0.090 2.01 0.045* 0.005 0.094 0.92 1.48 0.22 --Digital Literacy 0.189 4.21 < 0.0001*** 0.063 1.16 0.25 3.23 0.072 --Epistemic Beliefs (Faith in Intuition) -0.193 -3.90 0.0001*** -0.184 -3.09 0.002** 0.01 0.91 --Epistemic Beliefs (Evidence) 0.106 2.32 0.021* 0.049 0.90 0.37 0.63 0.43 --Epistemic Beliefs (Truth is Political) -0.144 -3.10 0.002** -0.044 -0.79 0.43 1.90 0.17 --Affective Polarization -0.002 -0.05 0.96 -0.128 -2.37 0.018* 3.21 0.073 Belief Superiority -0.073 -1.64 0.101 -0.043 -0.81 0.42 ------Political Party -0.237 -5.14 < 0.0001*** -0.033 0.60 0.55 8.03 0.0046 0.055 ~ Age 0.182 3.90 0.0001*** 0.116 2.07 0.039* 0.81 0.37 --Gender (M = 0, F = 1) -0.084 -1.97 0.0496* -0.092 -1.80 0.072 ~ 0.02 0.90 --Education 0.035 0.79 0.43 0.091 1.70 0.090 0.64 0.42 --Income 0.073 1.67 0.097 ~ -0.002 -0.04 0.97 1.21 0.27 --Race (Black) -0.181 -4.14 < 0.0001*** -0.050 -0.95 0.34 3.68 0.055 --Race (Asian) -0.020 -0.47 0.64 -0.063 -1.24 0.21 ------Ethnicity (Hispanic) -0.053 -1.23 0.22 -0.026 -0.50 0.62 ------B. Experiment 2 (n = 433) Predictor variable MIST CIE B. Experiment 2 (n = 433) B. Experiment 2 (n = 469) Participant Political Orientation x Affective Polarization Score x 0.016 1.298 0.194 Affective Polarization Score x -0.009 -0.864 0.39 Corrected Accusation (vs. No Accusation) Stimulus Difference b t p b t p c 2 p pFDR CRT 0.035 0.87 0.39 -0.026 -0.53 0.60 ------Digital Literacy 0.253 5.81 < 0.0001*** 0.109 2.06 0.040* 4.45 0.035 0.07 ~ Epistemic Beliefs (Faith in Intuition) -0.113 -2.49 0.013* -0.126 -2.27 0.024* 0.03 0.87 --Epistemic Beliefs (Evidence) 0.062 1.333 0.18 0.038 0.67 0.50 ------Epistemic Beliefs (Truth is Political) -0.108 -2.45 0.015* 0.01 0.18 0.86 2.88 0.090 --Affective Polarization -0.026 -0.61 0.55 -0.013 -0.25 0.80 ------Belief Superiority 0.042 1.01 0.32 -0.003 -0.06 0.95 ------Political Party -0.273 -5.89 < 0.0001*** 0.007 0.13 0.90 14.79 .0001 0.0008* Age 0.114 2.57 0.011* 0.117 2.18 0.030* 0.00 0.96 --Gender (M = 0, F = 1) -0.017 -0.41 0.69 -0.059 -1.17 0.24 ------Education 0.083 1.82 0.07 ~ 0.043 0.77 0.44 0.32 0.57 --Income 0.035 0.81 0.42 0.000 0.01 0.99 ------Race (Black) -0.168 -3.91 0.0001*** -0.004 -0.07 0.94 5.93 0.015 0.048* Race (Asian) -0.066 -1.64 0.10 -0.062 -1.27 0.20 ------Ethnicity (Hispanic) -0.148 -3.62 0.0003*** 0.003 0.07 0.95 5.55 0.018 0.048* A. Experiment 1 (n = 377) Stepwise regression SUR regression comparing all variables with significant or marginal effects in stepwise regression for MIST or CIE Predictor variable MIST CIE Difference b t b t c 2 p FDR corr p CRT 0.089 1.99* 0.008 0.16 1.34 0.25 --Digital Literacy 0.195 4.34*** 0.065 1.22 3.41 0.065 --Epistemic Beliefs (Faith in Intuition) -0.188 -3.81*** -0.179 -3.02* 0.01 0.91 --Epistemic Beliefs (Evidence) 0.102 2.25* 0.048 0.89 0.57 0.45 --Epistemic Beliefs (Truth is Political) -0.143 -3.08** -0.044 -0.80 1.84 0.17 --Affective Polarization -0.002 -0.039 -0.129 -2.38* 3.26 0.07 --Belief Superiority -0.072 -1.62 -0.040 -0.75 0.21 0.64 --Political Party -0.237 -5.16*** -0.031 -0.57 8.19 0.0042 0.055 ~ Age 0.197 4.37* 0.132 2.43* 0.86 0.35 --Gender (F = 1) -0.080 -1.88 -0.087 -1.71 0.01 0.91 --Education 0.030 0.68 0.083 1.56 0.59 0.44 --Predictor variable b t p Digital Literacy 0.074 1.430 0.154 Epistemic Beliefs (Faith in Intuition) -0.224 -4.31 < 0.0001*** Affective Polarization -0.126 -2.57 0.011* Age 0.143 2.82 0.005** Gender (M = 0, F = 1) -0.087 -1.732 0.084 ~ Education -0.085 1.708 0.089 ~ Stepwise regression SUR regression comparing whether outcome measures differ for all variables with significant or marginal effects in stepwise regression for MIST or CIE Predictor variable MIST CIE Difference b t b t c 2 p FDR corr p Digital Literacy 0.263 6.14*** 0.106 2.05* 5.48 0.019 0.043* Epistemic Beliefs (Faith in Intuition) -0.129 -2.94** -0.131 -2.48* 0 0.98 --Epistemic Beliefs (Truth is Political) -0.113 -2.65** 0.004 0.07 3.04 0.081 --Political Party -0.278 -6.26*** 0.009 0.16 17.00 0.00004*** 0.0003*** Age 0.116 2.71** 0.125 2.41* 0.02 0.90 --Gender (M = 0, F = 1) -0.037 -0.92 -0.065 -1.33 0.19 0.66 --Education 0.096 2.31* 0.043 0.87 0.66 Race (Black) -0.168 -3.98*** 0.002 0.03 6.57 0.010 0.036* Ethnicity (Hispanic) -0.152 -3.77*** 0.007 0.14 6.35 0.012 0.036* Education 456 0.128 0.006 0.009** 0.083 0.078 0.117 0.46 --Income 451 0.057 0.23 ---0.026 0.58 --0.18 --Predictor variable MIST CIE Difference b t p b t p c 2 puncorr pFDR Black -0.205 -4.39 < 0.0001*** -0.075 -1.58 0.11 3.84 0.050 .10 Asian -0.052 -1.10 0.27 -0.117 -2.47 0.014* 0.97 0.33 Hispanic -0.001 -0.018 0.99 -0.042 -0.89 0.37 ------Gender (F = 1) -0.051 -1.20 -0.087 -1.73 0.30 0.59 --Education 0.036 0.82 0.092 1.76 0.67 0.41 --Gender (F = 1) -0.054 -1.43 -0.090 -1.85 0.33 0.56 --Education 0.002 0.04 0.088 1.76 1.86 0.17 --Uncorrected Accusation (vs. No Accusation) Stimulus -0.067 -5.710 < 0.0001 Participant Political Orientation x Affective Polarization Score 0.009 0.380 0.704 Affective Polarization Score x --Affective Polarization 0.085 1.88 -0.182 -3.41*** 14.49 0.00014 0.002** Belief Superiority -0.115 -2.61* -0.041 -0.80 1.17 0.28 --Political Party -0.209 -4.57*** 0.006 0.11 9.28 0.0023 0.016* Age 0.100 2.16* 0.138 2.53* 0.28 0.60 --Affective Polarization 0.056 1.40 -0.145 -2.83** 9.55 0.002 Belief Superiority -0.081 -2.06* -0.033 -0.65 0.58 0.45 --Political Party -0.176 -4.35*** -0.022 -0.43 5.51 0.019 0.089 ~ Age 0.169 4.18*** 0.114 2.21* 0.71 0.40 --Participant Political Orientation x Uncorrected Accusation (vs. No Accusation) Stimulus 0.002 0.200 Candidate Political Orientation x Uncorrected Accusation (vs. No Accusation) Stimulus 0.020 1.783 0.075 ~ Counterbalance2 0.013 0.573 0.567 Participant Political Orientation x Candidate Political Orientation 0.085 6.514 Uncorrected Accusation (vs. No Accusation) Stimulus < 0.0001*** Affective Polarization Score x -0.012 -1.140 0.254 Candidate Political Orientation x 0.842 Counterbalance1 0.032 1.440 0.151 Participant Political Orientation x 0.014* -0.512 -61.398 < 0.0001*** Corrected Accusation (vs. No Accusation) Stimulus Regression for race/ethnicity, Experiment 1 (n = 447) Epistemic Beliefs (Evidence) 0.109 2.38* 0.021 0.39 1.54 0.21 --Epistemic Beliefs (Truth is Political) -0.165 -3.56*** -0.045 -0.83 2.80 0.094 --Epistemic Beliefs (Evidence) 0.109 2.71** 0.027 0.53 1.57 0.21 Epistemic Beliefs (Truth is Political) -0.180 -4.25*** -0.036 -0.66 4.41 0.036 --Candidate Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus -0.012 -1.054 Affective Polarization Score 0.008 0.354 0.723 Affective Polarization Score x -0.001 -0.110 0.912 Candidate Political Orientation x 0.292 Candidate Political Orientation -0.011 -0.881 0.379 Participant Political Orientation x --Participant Political Orientation -0.079 -3.445 0.0006*** Uncorrected Accusation (vs. No Accusation) Stimulus 0.42 Predictor variable b t Digital Literacy 0.108 2.161 0.031* Epistemic Beliefs (Faith in Intuition) -0.134 -2.709 0.007** Age 0.129 2.656 0.008* Gender (M = 0, F = 1) -0.071 -1.478 0.14 MIST CIE Difference n r p FDR corr p r p FDR corr p p FDR corr p CRT 458 0.250 < 0.0001 < 0.0001*** 0.150 0.0013 0.005** 0.091 0.156 Digital Literacy 458 0.296 < 0.0001 < 0.0001*** 0.132 0.0047 0.014* 0.0053 0.011* Epistemic Beliefs (Faith in Intuition) 458 -0.368 < 0.0001 < 0.0001*** -0.203 < 0.0001 0.0001*** 0.0039 0.009** Epistemic Beliefs (Evidence) 458 0.287 < 0.0001 < 0.0001*** 0.062 0.18 --0.0002 0.0005** Epistemic Beliefs (Truth is Political) 458 -0.322 < 0.0001 < 0.0001*** -0.090 0.054 0.093 ~ < 0.0001 0.0003** Affective Polarization 402 0.140 0.005 0.008** -0.169 0.0007 0.0004* < 0.0001 < 0.0001*** Belief Superiority 458 -0.073 0.12 ---0.056 0.23 --0.78 --Political Party 458 -0.246 < 0.0001 < 0.0001*** 0.018 0.71 --< 0.0001 < 0.0001*** Age 457 0.091 0.053 0.064 ~ 0.115 0.014 0.034* 0.69 --Gender (M = 0, F = 1) 455 -0.111 0.018 0.024* -0.091 0.052 0.093 ~ 0.74 --Predictor variable MIST CIE Difference b t p b t p c 2 p pFDR CRT 0.119 2.69 0.0074** 0.045 0.86 0.39 1.18 0.28 --Digital Literacy 0.206 4.60 < 0.0001*** 0.106 2.02 0.044* 2.07 0.15 --Epistemic Beliefs (Faith in Intuition) -0.168 -3.42 0.0007*** -0.138 -2.40 0.017* 0.15 0.70 --Epistemic Beliefs (Evidence) 0.111 2.42 0.016* 0.024 0.44 0.66 1.51 0.22 --Epistemic Beliefs (Truth is Political) -0.165 -3.56 0.0004*** -0.045 -0.83 0.40 2.79 0.095 --Affective Polarization 0.085 1.87 0.062 -0.183 -3.40 0.0007*** 14.48 0.00014 0.002** Belief Superiority -0.114 -2.60 0.0098** -0.041 -0.78 0.43 1.18 0.28 --Political Party -0.209 -4.57 < 0.0001*** -0.006 0.11 0.91 9.27 0.0023 0.016* Age 0.094 2.01 0.045* 0.130 2.35 0.019* 0.24 0.62 --Gender (M = 0, F = 1) -0.054 -1.25 0.21 -0.090 -1.79 0.074 ~ 0.31 0.58 --Education 0.037 0.84 0.40 0.094 1.79 0.074 ~ 0.68 0.41 --Income 0.079 1.82 0.069 ~ -0.022 -0.43 0.66 2.28 0.13 --Race (Black) -0.194 -4.48 < 0.0001*** -0.018 -0.36 0.72 6.92 0.0085 0.040* Race (Asian) -0.006 -0.14 0.89 -0.097 -1.93 0.054 ~ 1.92 0.17 --Ethnicity (Hispanic) 0.030 -0.70 0.48 -0.042 -0.83 0.41 ----Digital Literacy 0.209 4.68*** 0.110 2.09* 2.05 0.15 --Epistemic Beliefs (Faith in Intuition) -0.165 -3.38*** -0.134 -2.34* 0.16 0.69 --Digital Literacy 0.279 6.77*** 0.050 0.95 11.78 0.0006 0.008** Epistemic Beliefs (Faith in Intuition) -0.175 -3.91*** -0.174 -3.05** 0.00 0.99 Initial Rating 0.143 15.854 < 0.0001*** Affective Polarization Score x 0.000 -0.021 0.983 --Candidate Political Orientation x Affective Polarization Score 0.009 0.654 Predictor b t p Candidate Political Orientation x 0.513 Uncorrected Accusation (vs. No Accusation) Stimulus --Stepwise regression SUR regression comparing whether outcome measures differ for all variables with significant or marginal effects in stepwise regression for MIST or CIE Predictor variable MIST CIE Difference b t b t c 2 p FDR corr p CRT 0.119 2.69** 0.045 0.86 1.18 0.28 --Predictor variable b t p Digital Literacy 0.124 2.429 0.016* Epistemic Beliefs (Faith in Intuition) -0.165 -3.279 0.0011** Affective Polarization -0.191 -3.930 0.0001*** Age 0.154 2.977 0.003** Gender (M = 0, F = 1) -0.089 -1.94 0.074 ~ Education -0.092 1.883 0.060 ~ Asian -0.095 -1.942 0.053 ~ CRT 0.063 1.63 0.10 -0.019 -0.40 0.69 ------Digital Literacy 0.291 6.98 < 0.0001*** 0.094 1.81 0.071 8.86 .003 Epistemic Beliefs (Faith in Intuition) -0.107 -2.48 0.014* -0.111 -2.08 0.038* 0.00 .95 --(Evidence) 0.093 2.15 0.032* 0.013 0.24 0.81 1.35 .24 --Epistemic Beliefs (Truth is Political) -0.115 -2.73 0.0065** 0.019 0.36 0.72 3.99 .046 0.094 ~ Affective Polarization -0.022 -0.54 0.59 0.026 0.51 0.61 ------Belief Superiority 0.019 0.49 0.62 -0.017 -0.36 0.72 ------Political Party -0.243 -5.61 < 0.0001*** -0.016 -0.30 0.76 10.85 .0010 0.010* Age 0.135 3.27 0.0012** 0.091 1.78 0.076 ~ 0.44 .51 Gender (M = 0, F = 1) -0.019 -0.48 0.63 -0.094 -1.93 0.055 ~ 1.43 .23 --Education 0.054 1.27 0.20 0.040 0.76 0.45 ------Income 0.020 0.48 0.63 -0.026 -0.51 0.61 ------Race (Black) -0.130 -3.23 0.0013** -0.003 -0.05 3.96 .047 0.094 ~ Race (Asian) -0.065 -1.70 0.089 ~ -0.040 -0.85 0.40 0.17 .68 --Ethnicity (Hispanic) -0.141 -3.65 0.0003** 0.024 0.49 0.62 7.17 .0074 0.025* Predictor variable MIST CIE Difference b t b t p FDR corr p CRT 0.089 2.22* 0.004 0.09 1.69 0.19 --Participant Political Orientation x Affective Polarization Score 0.005 0.193 Experiment 2 (n = 449) Affective Polarization Score x -0.011 -1.071 0.284 0.847 Participant Political Orientation x c 2 Counterbalance2 -0.013 -0.515 0.607 Participant Political Orientation x Candidate Political Orientation 0.057 3.965 Uncorrected Accusation (vs. No Accusation) Stimulus Uncorrected Accusation (vs. No Accusation) Stimulus < 0.0001*** Candidate Political Orientation x Affective Polarization Score x -0.013 -1.107 Candidate Political Orientation x -0.029 -2.741 0.006** 0.268 Participant Political Orientation x 0.96 Counterbalance1 0.007 0.270 0.788 Participant Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus marginal effects in stepwise regression for MIST or CIE Uncorrected Accusation (vs. No Accusation) Stimulus -0.460 -50.543 < 0.0001*** Corrected Accusation (vs. No Accusation) Stimulus Affective Polarization Score x -0.007 -0.682 0.495 SUR regression comparing whether outcome measures differ for all variables with significant or Affective Polarization Score 0.019 0.702 0.483 Corrected Accusation (vs. No Accusation) Stimulus -0.122 -13.471 < 0.0001*** Candidate Political Orientation x Affective Polarization Score x 0.002 0.198 Candidate Political Orientation x 0.843 Corrected Accusation (vs. No Accusation) Stimulus --Affective Polarization -0.140 -3.004 Age 0.109 2.221 Gender (M = 0, F = 1) -0.080 -1.689 Education -0.081 1.727 Black -0.069 -1.447 -0.089 -1.882 0.061 ~ Candidate Political Orientation -0.016 -1.165 0.244 Participant Political Orientation x Affective Polarization Score x -0.017 -1.629 0.103 Asian Participant Political Orientation -0.064 -2.481 0.013* Uncorrected Accusation (vs. No Accusation) Stimulus Participant Political Orientation x 0.149 Initial Rating 0.168 17.188 < 0.0001*** Affective Polarization Score x -0.009 -0.759 0.448 Corrected Accusation (vs. No Accusation) Stimulus 0.085 ~ Predictor b t p Candidate Political Orientation x Candidate Political Orientation x -0.026 -2.512 0.012* 0.092 ~ Participant Political Orientation x Uncorrected Accusation (vs. No Accusation) Stimulus 0.027* Affective Polarization Score x 0.014 1.124 0.261 Affective Polarization Score Experiment 1 (n = 385) 0.0028** Participant Political Orientation x Candidate Political Orientation x 0.006 0.498 0.62 Epistemic Beliefs Predictor variable b t p Epistemic Beliefs (Faith in Intuition) -0.223 -4.71 Counterbalance1 + Counterbalance2 + (1|Subject) Uncorrected Accusation (vs. No Accusation) Stimulus Participant Political Orientation x < 0.0001*** Participant Ideology * Candidate Ideology * Affective Polarization * Corrected Accusation + Participant Ideology * Candidate_Ideology * Affective_Polarization * Uncorrected Accusation + Candidate Political Orientation x -0.013 -1.164 0.244 Affective Polarization Score x Uncorrected Accusation (vs. No Accusation) Stimulus -0.039 -3.771 0.0002*** Participant Political Orientation x 0.015* Stepwise regression Model specification: Immediate Post-Story Rating ~ Initial Pre-Story Rating + Affective Polarization Score x Corrected Accusation (vs. No Accusation) Stimulus -0.004 -0.343 0.732 Uncorrected Accusation (vs. No Accusation) Stimulus 0.022 2.155 0.031* Candidate Political Orientation x p A. Experiment 1: A. Experiment 1 (n = 390) A. Experiment 1 (n = 390) Predictor variable MIST CIE Difference b T p b t p c 2 puncorr pFDR A. Experiment 1 (n = 422) Corrected Accusation (vs. No Accusation) Stimulus Candidate Political Orientation x Participant Political Orientation x Uncorrected Accusation (vs. No Accusation) Stimulus 0.022 2.097 0.036* 1.
Income Race (Black) Income Race (Black) Race (Asian) Income Race (Black) Participant Political Orientation x 0.077 -0.173 0.080 -0.191 -0.002 0.084 -0.164 Candidate Political Orientation x Candidate Political Orientation x Affective Polarization Score Race (Asian) 0.008 Affective Polarization Score Participant Political Orientation x Participant Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus 1.78 0.002 -4.01*** -0.040 1.84 -0.021 -4.44*** -0.014 -0.05 -0.092 2.19* -0.007 -4.28*** -0.068 0.20 -0.089
0.04 -0.76 -0.41 -0.27 -1.84 -0.14 -1.38 0.028 0.012 -1.86 0.005
1.23 3.91 2.27 7.11 1.89 2.14 2.41 1.937 0.951 2.53 0.511
0.27 0.048 0.13 0.008 0.17 0.14 0.12 0.11
--0.23 --0.037* ----0.053 ~ 0.342 ----0.609
Candidate Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus Candidate Political Orientation x Corrected Accusation (vs. No Accusation) Stimulus
-0.016 -0.021
-1.351 -2.036
0.177 0.042*


Question taken from Sirlin et al. (2021)








Acknowledgments and Funding Sources
This work was funded by two grants (in 2020 and 2021) from the Facebook/Meta Research Foundational Integrity research program. Facebook/Meta had no control over the research design, analysis, or publication decisions related to this work. We acknowledge Thomas Hogeboom and Aysha Gsibat for assistance in stimulus development and pilot testing. Preliminary versions of this article were posted on the PsyArXiv preprint server at https://osf.io/preprints/psyarxiv/5dmt4 .






Data Availability
Stimulus materials, anonymized raw data, processed data, and analysis code are available via our Open Science Foundation (OSF) repository (61).


Competing Interest statement
Authors declare no competing interests. 
Table S1
. Main effect analyses showing the overall magnitude of CIEs, using the main exclusion criteria, the original preregistered exclusion criteria, and a minimal set of exclusion criteria.


A. Experiment 1 B. Experiment 2
Note: Samples are identical for long-delay measures using the preregistered and minimal exclusion criteria because participants who did not meet the preregistered exclusion criteria were not invited back to participate in Part 2 of the study, where long-delay measures were collected.    
= 0, F = 1)
 -0.095 -2.049 0.041* 
Table S14
. Bivariate correlations between individual difference measures and CIEs computed from average Z-score of delayed ratings and choices, and regression analyses for race/ethnicity demographic variables. These exploratory analyses are only reported for the primary exclusion criteria.


Main Exclusion Criteria Preregistered Exclusion Criteria
Minimal


A. Experiment 1:
Regression for race/ethnicity, Experiment 1 (n = 429) 
 










Emotion shapes the diffusion of moralized content in social networks




W
J
Brady






J
A
Wills






J
T
Jost






J
A
Tucker






J
J
Van Bavel








Proceedings of the National Academy of Sciences




114
















The spread of true and false news online




S
Vosoughi






D
Roy






S
Aral








Science




359
















Sources of the continued influence effect: When misinformation in memory affects later inferences




H
M
Johnson






C
M
Seifert








Journal of Experimental Psychology: Learning, Memory, and Cognition




20
















Debunking: A metaanalysis of the psychological efficacy of messages countering misinformation




M
S
Chan






C
R
Jones






K
Hall Jamieson






D
AlbarracÃ­n








Psychological Science




28
















Explicit warnings reduce but do not eliminate the continued influence of misinformation




U
K H
Ecker






S
Lewandowsky






D
T W
Tang








Memory & Cognition




38
















The role of familiarity in correcting inaccurate information




B
Swire






U
K H
Ecker






S
Lewandowsky








Journal of Experimental Psychology: Learning, Memory, and Cognition




43
















Fake news': Incorrect, but hard to correct. The role of cognitive ability on the impact of false information on social impressions




J
De Keersmaecker






A
Roets








Intelligence




65
















Working memory capacity, short-term memory capacity, and the continued influence effect: A latent-variable analysis




C
R
Brydges






G
E
Gignac






U
K H
Ecker








Intelligence




69
















Working memory capacity, removal efficiency and event specific memory as predictors of misinformation reliance




J
A
Sanderson






G
E
Gignac






U
K H
Ecker








Journal of Cognitive Psychology




33
















Belief echoes: The persistent effects of corrected misinformation




E
Thorson








Political Communication


33














Perseverance in self-perception and social perception: Biased attributional processes in the debriefing paradigm




L
Ross






M
R
Lepper






M
Hubbard








Journal of Personality and Social Psychology




32


5
















The impact on juror verdicts of judicial instruction to disregard inadmissible evidence: A meta-analysis




N
Steblay






H
M
Hosch






S
E
Culhane






A
Mcwethy








Law and Human Behavior




30
















Do false allegations persist? Retracted misinformation does not continue to influence explicit person impressions




U
K H
Ecker






A
E
Rodricks








Journal of Applied Research in Memory and Cognition




9




















A
Mickelberg






B
Walker






U
Ecker






P
D L
Howe






A
Perfors






N
Fay


















Does mud really stick? No evidence for continued influence of misinformation on newly formed person impressions






Collabra: Psychology




10


92332












Failure to accept retractions: A contribution to the continued influence effect




A
E
O'rear






G
A
Radvansky








Memory & Cognition




48
















Human-algorithm interactions help explain the spread of misinformation. Current Opinion in Psychology




K
L
Mcloughlin






W
J
Brady








101770












Updating the identitybased model of belief: From false belief to the spread of misinformation




J
J
Van Bavel






S
Rathje






M
Vlasceanu






C
Pretus












Current Opinion in Psychology








Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning




G
Pennycook






D
G
Rand








Cognition




188
















Digital literacy is associated with more discerning accuracy judgments but not sharing intentions




N
Sirlin






Z
Epstein






A
A
Arechar






D
G
Rand




10.37016/mr-2020-83








Harvard Kennedy School Misinformation Review
















Does media literacy help identification of fake news? Information literacy helps




S
M
Jones-Jang






T
Mortensen






J
Liu












but other literacies don't














American Behavioral Scientist




65














Epistemic beliefs' role in promoting misperceptions and conspiracist ideation




R
K
Garrett






B
E
Weeks








PLoS ONE




12


184733














Epistemic motivations, political beliefs, and misperceptions of COVID-19 and the 2020 U.S. presidential election




D
G
Young






E
K
Maloney






A
Bleakley






J
B
Langbaum








Journal of Social and Political Psychology




10










I feel it in my gut








When truthiness trumps truth: Epistemic beliefs predict the accurate discernment of fake news




J
P
Rudloff






M
Appel








Journal of Applied Research in Memory and Cognition




12
















Political psychology in the digital (mis)information age: A model of news belief and sharing




J
J
Van Bavel






E
Harris






P
Parnamets






S
Rathje






K
C
Doell






J
A
Tucker








Social Issues and Policy Review




15
















Susceptibility to misinformation is consistent across question framings and response modes and better explained by myside bias and partisanship than analytical thinking




J
Roozenbeek






R
Maertens






S
M
Herzog






M
Geers






R
Kurvers






M
Sultan






S
Van Der Linden








Judgment and Decision Making




17




















M
Osmundsen






A
Bor






P
B
Vahlstrup






A
Bechmann






M
B
Petersen


















Partisan polarization is the primary psychological motivation behind political fake news sharing on Twitter






American Political Science Review




115














Identity concerns drive belief: The impact of partisan identity on the belief and dissemination of true and false news




A
Pereira






E
Harris






J
J
Van Bavel








Group Processes & Intergroup Relations




26
















Aging and verbal memory span: A meta-analysis




K
L
Bopp






P
Verhaeghen








The Journals of Gerontology: Psychological Sciences




60
















Differential effects of age on item and associative measures of memory: A meta-analysis




S
R
Old






M
Naveh-Benjamin








Psychology and Aging




23


1
















Fake news on Twitter during the 2016 U.S. presidential election




N
Grinberg






K
Joseph






L
Friedland






B
Swire-Thompson






D
Lazar








Science




363
















Less than you think: Prevalence and predictors of fake news dissemination on Facebook




A
Guess






J
Nagler






J
Tucker




eaau4586. 40






Science Advances




5














The persistence of inferences in memory for younger and older adults: Remembering facts and believing inferences




J
J
Guillory






L
Geraci








Psychonomic Bulletin & Review




17


1
















The continued influence effect: Examining how age, retraction, and delay impact inferential reasoning




A
L
Miller






K
T
Wissman






D
J
Peterson








Applied Cognitive Psychology




36
















Aging in an era of fake news




N
M
Brashier






D
L
Schacter








Current Directions in Psychological Science




29
















Understanding and reducing online misinformation across 16 countries on six continents




A
A
Arechar






J
Allen






A
J
Berinsky






R
Cole






Z
Epstein






K
Garimella






A
Gully






J
G
Lu






R
M
Ross






M
N
Stagnaro






Y
Zhang






G
Pennycook






D
G
Rand








Nature Human Behavior
















Negativity bias, negativity dominance, and contagion




P
Rozin






E
B
Royzman








Personality and Social Psychology Review




5
















Two routes to emotional memory: Distinct neural processes for valence and arousal




E
Kensinger






S
Corkin








Proceedings of the National Academy of Sciences




101
















Feeling and thinking: Closing the debate over the independence of affect




R
B
Zajonc




J. P. Forgas






Cambridge University Press








Feeling and thinking: The role of affect in social cognition








Tests for comparing elements of a correlation matrix




J
H
Steiger








Psychological Bulletin




87


















R
Maertens






F
M
GÃ¶tz






H
F
Golino






J
Roozenbeek






C
R
Schneider






Y
Kyrychenko






J
R
Kerr






S
Stieger






W
P
Mcclanahan






K
Drabot






J
He






S
Van Der Linden




The Misinformation Susceptibility Test (MIST): A psychometrically validated measure of news veracity discernment. Behavior Research Methods
















An efficient method of estimating seemingly unrelated regressions and tests for aggregation bias




A
Zellner








Journal of the American Statistical Association




57
















The amygdala modulates the consolidation of memories of emotionally arousing experiences




J
Mcgaugh








Annual Review of Neuroscience




27
















The psychological drivers of misinformation belief and its resistance to correction




U
K H
Ecker






S
Lewandowsky






J
Cook






P
Schmid






L
K
Fazio






N
Brashier






P
Kendeou






E
K
Vraga






M
A
Amazeen








Nature Reviews Psychology




1
















Reliance on emotion promotes belief in fake news




C
Martel






G
Pennycook






D
G
Rand








Cognitive Research: Principles and Implications




5


47














Measuring exposure to misinformation from political elites on Twitter




M
Mosleh






D
G
Rand








Nature Communications




13


7144














Social media sharing of low-quality news sources by political elites




J
Lasser






S
T
Aroyehun






A
Simchon






F
Carrella






D
Garcia






S
Lewandowsky








PNAS Nexus




1
















The paranoid style in American politics revisited: An ideological asymmetry in conspiratorial thinking




S
Van Der Linden






C
Panagopoulos






F
Azevedo






J
T
Jost








Political Psychology




42
















The influence of a sense of time on human development




L
L
Carstensen








Science




312
















Aging is associated with maladaptive episodic memory-guided social decision-making




K
M
Lempert






M
S
Cohen






K
A
Macnear






F
M
Reckers






L
A
Zaneski






D
A
Wolk






J
W
Kable








Proceedings of the National Academy of Sciences




119


2208681119














How do older adults recruited using MTurk differ from those in a national probability sample




A
M
Ogletree






B
Katz








The International Journal of Aging and Human Development






93


















A
Diamantopoulos






M
Sarstedt






C
Fuchs






P
Wilczynski






S
Kaiser


















Guidelines for choosing between multi-item and single-item scales for construct measurement: a predictive validity perspective






Journal of the Academy of Marketing Science




40














Investigating an alternate form of the cognitive reflection test




K
S
Thomson






D
M
Oppenheimer








Judgment and Decision Making




11


















S
Iyengar






S
J
Westwood








Fear and Loathing across Party Lines: New Evidence on Group Polarization






59














What Do We Measure When We Measure Affective Polarization




J
N
Druckman






M
S
Levendusky








Public Opinion Quarterly






83














Feeling superior is a bipartisan issue: Extremity (not direction) of political views predicts perceived belief superiority




K
Toner






M
R
Leary






M
W
Asher






K
P
Jongman-Sereno








Psychological Science




24
















Data quality in online human-subjects research: Comparisons between MTurk, Prolific, CloudResearch, Qualtrics, and SONA




P
D
Si References Douglas






P
J
Ewell






M
Brauer








PLoS ONE




18


279720














An intention-based account of perspective-taking: Why perspective-taking can both decrease and increase moral condemnation




B
J
Lucas






A
Galinsky






K
J
Murnighan








Personality and Social Psychology Bulletin




42
















Step away from stepwise




G
Smith








Journal of Big Data




5


32














Inattentive responding can induce spurious associations between task behavior and symptom measures




S
Zorowitz






J
Solis






Y
Niv






D
Bennett








Nature Human Behavior




7

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]