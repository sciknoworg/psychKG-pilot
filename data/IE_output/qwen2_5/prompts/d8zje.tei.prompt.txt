You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
The ability to learn adaptive decision policies over the course of our lifetime is a core aspect of human cognition. Much progress has been made by studying learning using computational theories derived from constrained laboratory studies. Tasks employed in such studies aim to elicit specific behaviors that are amenable to precise analysis and modeling. The ultimate goal of such a research program is to gradually build towards a complete understanding of how people learn by understanding the various components that comprise this cognitive process. However, this approach does not capture the complexity of learning in the real world, as it is limited to relatively simple tasks that are not intrinsically motivating and can be learned in short periods of time.
Traditional experiments also result in small data sets, limiting the potential to analyze complex behavior. To address these limitations, cognitive scientists have gradually begun transitioning towards large-scale online experiments to study learning, using them to test theories over many more data points in naturalistic environments 
(Stafford and Dewar, 2014;
Steyvers and Schafer, 2020
; 
Kuperwajs and Ma, 2022;
Allen et al., 2023;
Wise et al., 2023)
.
Faced with complex learning problems, people seem to learn in two ways: from their environment and from each other 
(Yoeli and Hoffman, 2022)
. The former is known as reinforcement learning, which can be broadly defined as learning from one's own experience with a task. More specifically, reinforcement learning describes the process of learning to select actions that maximize long-term reward, and has emerged as a dominant framework for explaining human learning and decision-making 
(Sutton and Barto, 2018;
Dayan and Daw, 2008;
Daw and Tobler, 2014)
. Despite the vast number of different reinforcement learning algorithms, a key signature of reinforcement learning is that past actions and outcomes influence future choices. A second method by which people learn is through observing or interacting with others. Social learning is supported by a large body of work in psychology and anthropology, including the idea that it enables cultural evolution at the population level 
(Henrich, 2016;
Laland, 2017;
Thompson et al., 2022)
. At the individual level, people likely integrate the actions and outcomes of others into their LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 4 own reinforcement learning algorithm 
(Olsson et al., 2020)
. To truly ascertain how people make use of such sophisticated learning strategies outside of the laboratory, it is necessary to empirically investigate whether prior results scale to more complex tasks.
The ideal paradigm for studying the interplay between reinforcement and social learning should be complex enough that there is opportunity for disparate types of learning to occur simultaneously. Additionally, it should allow for interaction between participants, be highly engaging, and produce substantial amounts of behavioral data collected over a long period of time. Given these desiderata, one of the few tasks that has been historically used to characterize complex decision-making becomes a promising candidate: chess (De Groot, 2014; 
Chase and Simon, 1973)
. Improving one's play requires learned strategies, the two-player nature of the game provides a social component, and players are intrinsically motivated to solve the challenging problem of selecting good moves. Further, the recent popularity of online chess platforms has generated billions of matches played across multiple years that are available to the public, already leading to newfound insights regarding the cognitive processes underlying chess play 
(Anderson et al., 2017;
Holdaway and Vul, 2021;
Russek et al., 2022)
. For our use case, a massive data set allows us to validate the mechanisms by which people learn strategic behaviors in a naturally incentivized setting.
To summarize our results, we leveraged large-scale chess gameplay data to find signatures of reinforcement and social learning. Importantly, we constrained our analysis in two ways. First, we investigated key positions in different opening lines in order to mirror traditional laboratory-based learning experiments with states that repeat across trials. Second, we focused on novice players in their first several months of experience with each opening in our data set where they are likely to learn the most and still be developing strategies. We begin by using the Queen's Gambit as a case study, and show across 480, 932 games that players' choices are influenced by both prior outcomes as predicted by the reinforcement learning literature and moves made by LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 5 their opponents as predicted by the social learning literature. We then highlight various additional cognitive factors that interact with these mechanisms for learning, and show that our results generalize to 2, 018, 851 games consisting of other opening sequences such as the Sicilian Defense, the King's Knight, and the Caro-Kann Defense. Taken together, our findings suggest that people learn strategies via sophisticated mechanisms even in complex, naturalistic environments.


Results
We looked for signatures of reinforcement and social learning in a large-scale data set of human decisions from the online chess platform Lichess (https://lichess.org). Specifically, we investigated key moves from different openings lines. Previous studies have similarly focused on openings to quantify chess performance and expertise effects 
(Chowdhary et al., 2023;
De Marzo and Servedio, 2023)
. Since our goal is to study long-term learning of strategies, we selected players within a lower playing strength range as measured by their Elo rating 
(Elo, 1978)
 and examined games between October 2020 and March 2021. Additionally, we filtered for players who had never encountered the particular opening we were analyzing between March 2020 and September 2020 to control for prior learning of the opening via experience (see Materials and methods for more details). In the following sections, we show that people learn which move to make in a single opening through the outcomes associated with their own decisions as well as those of their opponents. We then turn to other openings to validate that these results generalize.
The Queen's Gambit as a case study in strategic learning The Queen's Gambit provides an intriguing case study in learning strategic behavior. This is in part due to the famous Netflix miniseries of the same name released in October 2020, which caused the opening to suddenly increase in popularity and forced players who had not previously encountered it to quickly learn a response. For reference, the Queen's Gambit jumped from being part of 1 in 24.32 of all games played in October 2020 to 1 in 23.00 of all games played just one month later. A gambit is also a specific kind of chess opening in which a player attempts to LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR sacrifice material with the aim of achieving a positional advantage. This means that the response to the Queen's Gambit reduces to a binary choice between accepting or declining the gambit, simplifying analyses and mirroring traditional laboratory-based investigations of learning within the broader context of a complex task. In other words, we isolate a single decision embedded in a larger sequence of decisions which mimics a two-alternative forced choice. 
Figure 1A
 shows this binary decision, where black can choose to accept by taking white's pawn on c4 or decline by defending their own pawn on d5.
To get a sense for which of the options presented in the Queen's Gambit is better for the black LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 7 player, we computed the win rate for each choice in our data set. We valued wins, draws, and losses at 1, 0.5, and 0 points respectively. Empirically, declining the gambit has a higher win rate compared to accepting it (t(480, 930) = 15.96, p < 0.0001, 
Figure 1B
). For this analysis, we only considered declining the gambit as including the two most commonly played responses. We note that, despite the empirical win rate among players in our data set, accepting the gambit is a known line of play that can be studied such that it becomes a good strategy and is used in high-level play.
In 
Figure 1C
, we calculated the win rates for 28 different choices available to black when selecting a move, showing that defending the pawn on d5 is the decision with the highest overall win rate. Considering the broader distribution of available moves, there are a number of alternatives that lead to higher win rates than accepting the gambit, as well as many moves that lead to lower win rates than accepting the gambit. In all future analyses, we utilize responses reduced to the binary choice of accepting or declining the gambit. Additionally, when using the term games, we always specifically refer to Queen's Gambit games.


Signatures of reinforcement learning
Given this binary decision problem, our goal was to investigate whether people make decisions in consecutive Queen's Gambit games, regardless of the number of other games that occur between them, that align with prior results from reinforcement learning. The simplest form this can take is a win-stay, lose-shift policy, where repeating the choice to accept or decline the gambit is more likely if the outcome of the game associated with that choice was a win as opposed to a loss.   game, we enumerated every combination of choice and outcome for two games and computed the probability of repeating the second choice as a function of this history ( 
Figure 2B
). An immediately striking result was the difference between stay and shift sequences, where the LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 9 probability of repetition was always much higher if the two choices were congruent regardless of outcomes. This indicates that people who have already repeated their choice will likely continue to do so, a concept often referred to as choice perseveration in reinforcement learning 
(Aarts et al., 1998;
Miller et al., 2019)
. Among shift sequences, the probability of repetition was higher overall when choosing to accept after declining compared to the opposite, confirming that players have a baseline preference for accepting the gambit. We also found that given the same sequence of choice-outcome-choice, the probability of repetition was consistently higher if the outcome of the second game was a win rather than a loss. In other words, people's choices seem to be modulated most strongly by the outcome of their most recent game as predicted by reinforcement
learning, suggesting that choice values reflect a recency weighted average of experienced reward.
Finally, the largest difference in repetition probability between a loss and win in the second game occurred in the shift sequences in which the first game resulted in a loss. This indicates that people's behavior is most impacted by receiving a negative reward signal, changing their decision, and then receiving a positive reward signal. Importantly, sequences in which players transitioned from accepting to declining the gambit given the above scenario resulted in the largest overall difference in repetition probability, highlighting that reinforcement learning plays a critical role in discovering the better decision. The statistics and significance values for this analysis are available in 
Tables S1 and S2
.
Ultimately, we were interested in the long-term learning of strategies via reinforcement learning beyond one or two game histories. To investigate this, we correlated previous game outcomes and current choice up to 30 Queen's Gambit games ago across all players in the data set. Theoretical and experimental findings regarding reward prediction errors hypothesize that the effects of previous rewards on current associative strength effectively decline as an exponential function of number of experiences 
(Lau and Glimcher, 2005;
Glimcher, 2011)
. monotonically decreasing impact on current decisions, and are qualitatively well fit by an exponential function.


Signatures of social learning and adaptive decision-making
Shifting from reinforcement to social learning, we wanted to confirm that players were more likely to decline the gambit the more games they played. Indeed, people exhibited baseline learning as they were gradually more likely to make the choice that led to a higher empirical win rate as a function of total experience ( 
Figure 3A
). Then, we investigated situations in which a player encounters positions, in this case the Queen's Gambit, that their opponent has already played against them. Many players use the same opening repertoire with white and black, and these positions occur naturally in our data set. To do so, we plotted the same learning curve as before, but only for games played as white. This analysis revealed that players learned to decline the gambit at a high rate more quickly when observing their opponent decide whether to accept or decline. This suggests that learning from other people's actions is a key factor in making good LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 11 decisions.
We then characterized two additional mechanisms of social learning. The first is a copy effect, where players are more likely to decline the gambit themselves if they observed their opponent doing so in their most recent game as white (t(74, 785) = 3.27, p < 0.01, 
Figure 3B
). This occurs irrespective of game outcome. Second, we were interested in the interaction between reinforcement and social learning. A relatively straightforward social learning strategy would be to incorporate color-reversed data as additional data in one's own reinforcement learning algorithm and treat it similarly to firsthand experience. This would predict correlations between the outcomes of past games and the tendency for players to repeat their opponent's moves in future games. 
Figure 3C
 demonstrates that people were more likely to repeat their opponent's decision to accept or decline the gambit if the outcome of that game led to their own loss as opposed to their own win, although the effect was not statistically significant (t(71, 299) = 1.26, p = 0.21). Regardless, people may take the outcomes of their opponent's actions into account, incorporating them into their own reinforcement learning policy.
In the next part of our analysis, we wanted to test whether people adhere to general psychological principles of adaptive decision-making in this naturalistic context. The first is exploration: do players always make the best move according to their current beliefs, or do they strategically test alternatives in order to gain broader experience? Navigating this exploration-exploitation tradeoff has been extensively studied in multi-armed bandit tasks 
(Cohen et al., 2007;
Gershman, 2018)
 as well as in real-world settings 
(Schulz et al., 2019;
Brändle et al., 2023)
, with various reinforcement learning algorithms developed to solve this problem 
(Gittins, 1979;
Auer et al., 2002)
. Perhaps the simplest indicator of exploration is whether people change their choices from one instance of the gambit to the next. Exploring less as the values associated with actions are better known is a defining property of uncertainty-driven exploration 
(Wilson et al., 2014)
. In 
Figure 4A
, we show that players are more likely to shift their decision to accept or decline when  they have not played many games. This slowly decreases with experience, indicating that players eventually settle on a preferred strategy.
Response times are a common measure of cognitive resources spent in a task, where executing additional computations incurs a cost but can be beneficial in discovering better decisions.
Following this, we measured the probability of declining the gambit as a function of time taken to make a move ( 
Figure 4B
). In general, longer response times led to a higher probability of declining the gambit, suggesting that thinking for more time allowed players to select a better move more often. The exception to this is at very short response times, which we interpret as players also being more likely to decline if they have a preferred move in mind before their turn begins. One possible mechanism driving this result is planning, or the process of evaluating the future consequences of currently available options. Deeper forward search is typically correlated with better action selection 
(van Opheusden et al., 2023)
, and response times have previously LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 13 been utilized as a proxy for number of planning computations 
(Chabris and Hearst, 2003;
Russek et al., 2022;
Kuperwajs et al., 2023)
. As such, players may be more likely to decline the gambit because they plan further into the future and evaluate the outcomes associated with doing so.
Since people can decide to play games at-will on online chess platforms, another important influence on decision-making is the delay between games that contain the Queen's Gambit.
Learning mechanisms have been shown to be dependent on timescale, an example being that shorter delays between trials lead to faster learning due to the interaction between working memory and reward-based learning (van de Vijver and Ligneul, 2020; Wimmer and Poldrack, 2022). In our case, we found that players were less likely to decline the gambit the more days elapsed between games in which they encountered the position ( 
Figure 4C
). Taken in the context of our prior results on reinforcement learning, it is reasonable to assume that these effects are more pronounced in games that are played in close proximity to each other.
Lastly, we were interested in expertise effects. A growing body of literature has investigated how expert chess players differ from their less-skilled counterparts, explaining their superior performance due to contributions from both improved pattern recognition and deeper search 
(Ericsson and Smith, 1991;
Campitelli and Gobet, 2004)
. In our data set, we computed the probability of declining the gambit for different Elo ratings, finding that stronger players were more likely to decline ( 
Figure 4D
). This result also includes a social learning component:
stronger players are themselves paired with stronger opponents ( 
Figure S1
), meaning that they can selectively learn from players with higher Elo ratings. Since our data set is composed of novice players, even stronger players are still developing towards expertise. Importantly, though, they likely exhibit higher expertise than the majority of subjects performing completely novel tasks in traditional laboratory studies.  Defense. As before, when using the term games we always refer to games of each opening respectively.


LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR
In the Sicilian Defense, players most often choose to play closed by moving one of their knights to c3 or play open by moving their other knight to f3 ( 
Figure 5A
). When we computed the empirical win rates for these alternatives, playing open had a higher win rate compared to playing closed (t(714, 216) = 12.68, p < 0.0001). A broader distribution of available moves is shown in 
Figure S2A
, with playing open only having the second highest win rate and many options between playing closed and open and below playing closed. We replicated our main reinforcement learning finding, where recent outcomes are more highly weighted in their correlation with current choices when compared to outcomes further in the past that have an exponentially decreasing effect on decisions in the present. In 
Figure S3A
 we show the reinforcement learning effect for a single game, and in 
Figure S4
 the reinforcement learning effect for a two game history. The main effects all trend in the same direction as in the Queen's Gambit, and the two game history analysis shows that the probability of repetition was higher  
Figure 5B
). The win rates for a broader distribution of available moves are shown in 
Figure S2B
, with only one move having a higher win rate than the two moves we considered. We then replicate our previous reinforcement learning findings, again showing a strong correlation between current choices and past outcomes. The single and two game history analyses are available in 
Figures S3B and S5
 respectively, with the statistics and significance values for the two game history analysis available in 
Tables S5 and S6
. While we find the same copy effect in the King's Knight (t(773, 510) = 10.84, p < 0.0001), the opponent reinforcement learning effect is actually reversed (t(745, 746) = 4.25, p < 0.0001). That is to say, players in the King's Knight are more likely to repeat their opponent's decision if it leads to their own win rather than their own loss. Overall, while the Sicilian Defense captured aspects of both reinforcement and social learning equally, the King's Knight more strongly displays reinforcement learning effects.
The last opening we analyzed is the Caro-Kann Defense, where players most often choose to exchange by capturing the pawn on d5 or advance by moving their pawn to e5. Similarly to the Queen's Gambit, the Caro-Kann Defense has gained in popularity in recent years because one of the most prolific chess content creators, Levy Rozman, advertised the opening often while streaming. For this opening, advancing has a higher empirical win rate when compared to exchanging (t(78, 721) = 9.65, p < 0.0001, 
Figure 5C
). The win rates for a broader distribution of available moves are shown in 
Figure S2B
, with several moves having higher and lower win rates than the two moves we consider. We then replicate our previous reinforcement learning findings, again showing a correlation between current choices and past outcomes. Note that for the Caro-Kann Defense, this correlation decreases much more rapidly when compared to other openings, suggesting that only the most recent outcomes impact decision-making. The single and two game history analyses are available in 
Figures S3C and S6
 respectively, with the statistics and significance values for the two game history analysis available in Tables S7 and S8. We found that both the copy (t(5, 349) = 0.80, p = 0.42) and opponent reinforcement learning (t(5, 174) = 2.95, p < 0.01) effects replicated, although the former was not significant. In contrast to the King's Knight, the Caro-Kann Defense more strongly displays social as opposed to reinforcement learning effects.


Discussion
Humans have a remarkable ability to learn flexible decision policies. In this work, we tested whether two mechanisms underlying this hallmark of cognition, reinforcement and social learning, apply in the context of naturalistic strategic behavior. Specifically, we showed that theories developed from constrained laboratory studies of learning scale to complex environments, namely selecting which move to make in the opening of a chess match. We used the Queen's Gambit as a case study to show that the decisions made by novice players are influenced by the associations between prior choices and their outcomes. Then, we found that observing an opponent's behavior also affected decision-making. Finally, we examined additional cognitive processes that impact learning, and validated that signatures of reinforcement and social learning generalized to other openings. Ultimately, our results help advance our understanding of the algorithms that people employ during strategic behavior in complex settings.
While we uncovered clear evidence for reinforcement learning across multiple opening lines, there are several remaining questions to be addressed in this domain. Despite evidence that game outcome is a reward signal, it is certainly not the only rewarding signal present during gameplay.
For example, another signal might be the estimated value of the board position immediately following the decision or at any point until the end of the game. This type of signal represents an opposing extreme of temporal difference learning 
(Tesauro et al., 1995
 
Dayan, 1992)
.
In terms of social learning, our analysis was limited to the individual level. However, social learning also enables new ideas to spread throughout a community. Studying this kind of cultural transmission requires identifying a new idea or strategy and tracking the mechanisms that permit it to propagate through people's play. One promising source of new ideas in chess is artificial intelligence, and in particular AlphaZero 
(Silver et al., 2018)
. AlphaZero was trained entirely from self-play and developed a novel understanding of the game. Thus, one promising research direction is to identify instances where individual players used a theme derived from AlphaZero and then examining whether social learning mechanisms can account for the spread of these themes throughout the community. This approach has already been successfully applied to Go, another complex task where strategic behavior is essential 
(Shin et al., 2023)
.


LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 19
To conclude, we hope that our findings serve to highlight the necessity of moving beyond traditional laboratory-based studies for understanding how people learn. The increasing availability of large-scale, naturalistic data sets such as the one used here will pave the way for exciting advances in characterizing human cognition. If we ultimately seek to construct detailed, process-level models that are capable of describing and predicting behavior in complex environments such as chess, then gaining insight into the mechanisms by which people reason in such tasks is a valuable step towards that goal.


Materials and methods


Data
The raw data consists of standard rated games played on the online chess platform Lichess (https://lichess.org) stored as Portable Game Notation (PGN) files. We downloaded all games between October 2020 and March 2021, filtering for games that contained the moves corresponding to the opening line we were investigating. We used all games in which the sum of the time control setting and the increment multiplied by 60 was between 120 and 300.
Additionally, we limited our analysis to players with an Elo rating between 800 and 1200 as computed on Lichess using the Glicko 2 system. To enable this preprocessing, we used Scoutfish, a program that allows for high-speed, flexible queries from very large chess databases. We then extracted the desired information from each game: player names for white and black, the Coordinated Universal Time for the game, the move played in response to the board position we were analyzing along with its corresponding response time, the outcome of the game, Elo ratings for white and black, and number of previously played games as white and black. Note that to compute response times, we take the difference in clock between a player's successive moves. We also downloaded all games between March 2020 and September 2020, repeating the process above to extract the set of unique player names who had played each particular opening line. This allowed us to remove all users who had previously encountered the opening from our analysis.
Finally, we reduced the data set further by only analyzing moves that were included in our defined van Opheusden, B., 
Kuperwajs, I., Galbiati, G., Bnaya, Z., Li, Y., & Ma, W. J. (2023)
. Expertise increases planning depth in human gameplay. Nature, 618 
7967
 


Additional analyses
We conducted a variety of additional analyses to support the claims in the main text. To validate that stronger players tend to play against, and therefore selectively learn from, stronger opponents, we correlated player and opponent Elo ratings in each game we used in the data set ( 
Figure S1
). Lichess pairs players based on their ratings, but we test this empirically nonetheless.
As expected, ratings were correlated in Queen's Gambit (r = 0.796), Sicilian Defense (r = 0.800), King's Knight (r = 0.785), and Caro-Kann Defense (r = 0.797) matches.
Next, we further characterized behavior in the Sicilian Defense, King's Knight, and Caro-Kann
Defense. We first computed the empirical win rates across all different choices that were present in the data set for each opening ( 
Figure S2
). In the Sicilian Defense, playing open is the most common response for white but has only the second highest win rate. There are then a number of
alternatives between playing open and closed, and many moves that lead to lower win rates than playing closed. In the King's Knight, playing an Italian Game or the Ruy Lopez have the second and third highest win rates while being the two most common responses for white. In the Caro-Kann Defense, advancing is the most common response for white despite having the third highest win rate. There are then a number of alternatives between advancing and exchanging, and many moves that lead to lower win rates than exchanging.
In terms of reinforcement learning, we investigated the simplest win-stay, lose-shift policy that players exhibited in the Queen's Gambit in these three openings. 
Figure S3 shows
   learning ones. As a whole, these results confirm that game outcome generalizes as a reward signal that drives learning.
Similarly to the Queen's Gambit, we extended our reinforcement learning analysis to a two game history of all possible choices and outcomes. In the Sicilian Defense, we found relatively similar results ( 
Figure S4)
. Specifically, the difference between stay and shift sequences, preference for shifting from closed to open compared to the opposite, and effect of the second game resulting in a win as opposed to a loss all replicated. The exceptions to this were smaller differences between stay and shift sequences when the first choice was to play closed, and an additional insignificant value in the second game outcome analysis. We replicated this in the King's Knight as well, where the effects of stay versus shift sequences and the second game resulting in a win as opposed to a loss were even stronger than in the Queen's Gambit ( 
Figure S5)
. Here, the baseline preference seemed to be for playing an Italian Game due to the probability of repetition being higher overall when choosing to play an Italian Game after playing the Ruy Lopez compared to  
Figure S4
. the opposite. In the Caro-Kann Defense, the effects of stay versus shift sequences were present as well as a baseline preference for advancing as opposed to exchanging ( 
Figure S6
). Additionally, we found almost no significant effects in the second game outcome analysis, although this could be due to the size of the data set since the effects themselves trend in the same direction. Overall, this set of analyses shows that gameplay history modulates decision-making in consistent ways.
LEARNING


Statistics for the two game history analyses
In almost all of our analyses, we provide the necessary statistics and significant values along with the results. The exception to this is the two game history analysis, as there are simply too many combinations to include in the main text. Thus, we provide these for the stay versus shift sequences and second game losses versus wins results here. 
Tables S1 and S2 correspond
 
Figure S5
. Probability of repeating the most recent choice to play an Italian Game or the Ruy Lopez in the King's Knight as a function of every combination of choice and game outcome for a two game history.
Data are displayed as the mean across all players and error bars indicate the standard error of the mean.
Queen's Gambit 
, Tables S3 and S4 correspond to the Sicilian Defense, Tables S5 and S6
 correspond to the King's Knight, and 
Tables S7 and S8
  accept-win-accept-loss accept-win-decline-loss 65.13 <0.0001 33,454
accept-win-accept-win accept-win-decline-win 62.62 <0.0001 27,278
decline-loss-accept-loss decline-loss-decline-loss 51.37 <0.0001 
29,
198
 decline-loss-accept-win decline-loss-decline-win 44.65 <0.0001 25,067
decline-win-accept-loss decline-win-decline-loss 48.66 <0.0001 25,072
decline-win-accept-win decline-win-decline-win 44.78 <0.0001 22,098 
Table S1
. Statistics and significance values for stay versus shift sequences in the two game history analysis for the Queen's Gambit.


Sequence 1 Sequence 2 t p df
accept-loss-accept-loss accept-loss-accept-win 1.91 0.06 64,178 accept-loss-decline-loss accept-loss-decline-win 3.47 <0.01 12,551 accept-win-accept-loss accept-win-accept-win 2.41 <0.05 51,423
accept-win-decline-loss accept-win-decline-win 0.44 0.66 9,309 decline-loss-accept-loss decline-loss-accept-win 3.05 <0.01 11,693
decline-loss-decline-loss decline-loss-decline-win 2.96 <0.01 42,572 decline-win-accept-loss decline-win-accept-win 1.45 0.15 9,493 decline-win-decline-loss decline-win-decline-win 2.84 <0.01 37,677   
Table S4
. Statistics and significance values for second game losses versus wins in the two game history analysis for the Sicilian Defense.


Sequence 1 Sequence 2 t p df
exchange-loss-exchange-loss exchange-loss-advance-loss 9.67 <0.0001 1,757 exchange-loss-exchange-win exchange-loss-advance-win 7.44 <0.0001 1,745 exchange-win-exchange-loss exchange-win-advance-loss 10.34 <0.0001 1,717
exchange-win-exchange-win exchange-win-advance-win 7.86 <0.0001 1,703
advance-loss-exchange-loss advance-loss-advance-loss 31.04 <0.0001 3,780
advance-loss-exchange-win advance-loss-advance-win 29.23 <0.0001 3,910
advance-win-exchange-loss advance-win-advance-loss 29.22 <0.0001 3,900
advance-win-exchange-win advance-win-advance-win 28.71 <0.0001 4,293 
Table S7
. Statistics and significance values for stay versus shift sequences in the two game history analysis for the Caro-Kann Defense.


Sequence 1 Sequence 2 t p df
exchange-loss-exchange-loss exchange-loss-advance-loss 1.11 0.27 2,629 exchange-loss-exchange-win exchange-loss-advance-win 0.97 0.33 873 exchange-win-exchange-loss exchange-win-advance-loss 2.64 <0.01 2,552 exchange-win-exchange-win exchange-win-advance-win 3.39 <0.001 868 advance-loss-exchange-loss advance-loss-advance-loss 1.15 0.25 811 advance-loss-exchange-win advance-loss-advance-win 0.63 0.53 6,879 advance-win-exchange-loss advance-win-advance-loss 0.64 0.52 820 advance-win-exchange-win advance-win-advance-win 0.07 0.94 7,373 
Table S8
. Statistics and significance values for second game losses versus wins in the two game history analysis for the Caro-Kann Defense.
Figure 1 .
1
The Queen's Gambit. (A) Board position for the Queen's Gambit, which follows the opening moves d4, d5, and c4. Black can choose to accept the gambit by capturing the pawn on c4, or decline by protecting the pawn on d5. (B) Empirical win rates for black when accepting and declining the gambit, where declining includes the two moves shown in (A). Here and in (C) data are displayed as the mean across all players and error bars indicate the standard error of the mean. (C) Empirical win rates for 28 different moves available to black when faced with the Queen's Gambit.


Figure
Figure 2A demonstrates that players' choices exhibit this behavioral signature


Figure 2 .
2
Reinforcement learning in the Queen's Gambit. (A) Probability of repeating the choice to accept or decline the gambit as a function of game outcome. Here and in (B) data are displayed as the mean across all players and error bars indicate the standard error of the mean. (B) Probability of repeating the most recent choice to accept or decline the gambit as a function of every combination of choice and game outcome for a two game history. (C) Correlation between previous game outcomes and current choice to accept or decline up to 30 Queen's Gambit games in the past. The shading indicates an exponential fit to the correlation data.


LEARNINGFigure 3 .
3
Human behavior in the Queen's Gambit is consistent with this framework, as recent outcomes are the most highly weighted in terms of current choice (Figure 2C). Meanwhile, outcomes further in the past have a Social learning in the Queen's Gambit. (A) Probability of declining the gambit as a function of number of all games played or games played as white. Here and in all subsequent panels, the data are displayed as the mean across all players, with shading or error bars indicating the standard error of the mean. (B) Probability of declining the gambit as a function of whether the opponent chose to accept or decline in the previous game. (C) Probability of repeating the opponent's choice to accept or decline the gambit as a function of game outcome.


LEARNING


Figure 4 .
4
Adaptive decision-making in the Queen's Gambit. (A) Probability of shifting from a decision to accept the gambit to a decision to decline the gambit or vice versa as a function of total games played. Here and in all subsequent panels the data are displayed as the mean across all players and shading indicates the standard error of the mean. (B) Probability of declining the gambit as a function of the time taken to make a move measured in seconds. Here and in all subsequent plots, results are binned into 10 bins. (C) Probability of declining the gambit as a function of the time in days that has elapsed between games. (D) Probability of declining the gambit as a function of the player's current playing strength, as measured by their Elo rating.


Figure 5 .
5
Generalization of reinforcement and social learning. (A) The Sicilian Defense, which follows the opening moves e4 and c5. White can choose to play closed by moving their knight to c3 or play open by moving their other knight to f3. The remainder of the column contains the empirical win rates for white when playing closed or open, the correlation between previous game outcomes and current choice to play closed or open up to 30 Sicilian Defense games in the past, the probability of playing open as a function of whether the opponent chose to play closed or open in the previous game, and the probability of repeating the opponent's choice to play closed or open as a function of game outcome. In all panels the data are displayed as the mean across all players with error bars indicating the standard error of the mean. (B) The same as in (A), but for the King's Knight, which follows the opening moves e4, e5, Nf3, and Nc6. White can choose to play an Italian Game by moving their bishop to c4 or play the Ruy Lopez by moving their bishop to b5. (C) The same as in (A), but for the Caro-Kann Defense, which follows the opening moves e4, c6, d4, and d5. White can choose to exchange by capturing the pawn on d5 or advance by moving their pawn to e5.


, 1000-1005.Watkins, C. J.,& Dayan, P. (1992).Q-learning. Machine Learning, 8, 279-292.   Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., & Cohen, J. D. (2014). Humans use directed and random exploration to solve the explore-exploit dilemma. Journal ofExperimental Psychology: General, 143(6), 2074. Wimmer, G. E., & Poldrack, R. A. (2022). Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period. Memory & Cognition, 50(2), 312-324. Wise, T., Emery, K., & Radulescu, A. (2023). Naturalistic reinforcement learning. Trends in Cognitive Sciences. Yoeli, E., & Hoffman, M. (2022). Hidden games: The surprising power of game theory to explain irrational human behavior. Basic Books.


Figure S1 .
S1
Validating that players are paired with opponents of similar playing strength. (A) Correlation between player and opponent Elo ratings in all Queen's Gambit games. (B) The same as in (A), but for all Sicilian Defense games. (C) The same as in (A), but for all King's Knight games. (D) The same as in (A), but for all Caro-Kann Defense games.


Figure S2 .Figure S3 .
S2S3
Empirical win rates for different moves available to white when faced with the (A) Sicilian Defense, (B) King's Knight, and (C) Caro-Kann Defense. Data are displayed as the mean across all players and error bars indicate the standard error of the mean. Win-stay, lose-shift in the Sicilian Defense, King's Knight, and Caro-Kann Defense. (A) Probability of repeating the choice to play open or closed as a function of game outcome. Data are displayed as the mean across all players and error bars indicate the standard error of the mean. (B) The same as in (A), but for playing an Italian Game or the Ruy Lopez. (C) The same as in (A), but for exchanging or advancing.


Probability of repeating the most recent choice to play open or closed in the Sicilian Defense as a function of every combination of choice and game outcome for a two game history. Data are displayed as the mean across all players and error bars indicate the standard error of the mean.


LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 15Generalization of learning strategies across openingsUntil now, all of our results have been limited to the Queen's Gambit. While we believe that this is an illustrative case study in how people learn naturalistic strategic behavior, it is crucial to know whether these signatures of reinforcement and social learning generalize to other openings. When selecting new opening lines to test, we aimed to maintain the reduction to a binary choice exhibited by the Queen's Gambit while focusing on openings that were sufficiently popular to ensure enough games were available to allow us to test our hypotheses. Thus, we investigated strategies in three new openings: the Sicilian Defense, the King's Knight, and the Caro-Kann


when choosing to play open after playing closed compared to the opposite. This indicates that people have a baseline preference for playing open. The statistics and significance values for this We found that players LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 16 were more likely to play open if they observed their opponent doing so in their most recent game as black (t(200, 228) = 8.08, p < 0.0001), and more likely to repeat their opponent's decision if the outcome of that game led to their own loss as opposed to their own win (t(192, 489) = 6.63, p < 0.0001). Thus, game outcomes associated with decisions people observe their opponents make seem to be another reward signal.In the King's Knight, players most often choose to play an Italian Game by moving one of their bishops to c4 or play the Ruy Lopez by moving the same bishop to b5. This is a classic opening
analysis are available in Tables S3 and S4. For social learning, we replicated both the copy and the color-reversed win-stay, lose shift analyses in the Sicilian Defense.line in chess, cited as perhaps the oldest first move in the modern version of the game. Both alternatives we consider are well known openings that see regular play at all levels. For this opening, playing the Ruy Lopez has a higher empirical win rate when compared to playing an Italian Game, although this difference is not statistically significant (t(1, 225, 908) = 1.92, p = 0.05,


Table 1 .
1
LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 20 binary choice response for each opening. The number of unique players and games included in the subset of the data corresponding to each opening is provided inTable 1. Number of unique players and games included in the analyses corresponding to each opening.Source code for all analysis in the paper as well as the filtered data is available at htttps://github.com/ionatankuperwajs/learning-openings. The raw data is available from the Lichess open database at https://database.lichess.org, while source code for preprocessing the raw data is available at https://github.com/mcostalba/scoutfish.Thompson, B., van Opheusden, B., Sumers, T., & Griffiths, T. (2022). Complex cognitive algorithms preserved by selective social learning in experimental populations.
Opening
Number of players Number of games
Queen's Gambit
146,721
480,932
Sicilian Defense
165,698
714,218
King's Knight
167,331
1,225,910
Caro-Kann Defense
39,825
78,723
Code and data availability


Probability of repeating the most recent choice to exchange or advance in the Caro-Kann Defense as a function of every combination of choice and game outcome for a two game history. Data are displayed as the mean across all players and error bars indicate the standard error of the mean.
correspond to the Caro-Kann Defense. LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR Outcome 1 6 Choice 2 Choice 1 Outcome 2 Exchange Loss Win Exchange Advance Exchange Advance Loss Win Loss Win Loss Win Loss Win Advance Loss Win Exchange Advance Exchange Advance Loss Win Loss Win Loss Win Loss Win Caro-Kann Defense 7 Sequence 1 Sequence 2 t p df accept-loss-accept-loss accept-loss-decline-loss 77.96 <0.0001 42,336 Figure S6. LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR accept-loss-accept-win accept-loss-decline-win 67.60 <0.0001 34,393


Table S2 .
S2
Statistics and significance values for second game losses versus wins in the two game history analysis for the Queen's Gambit.
Sequence 1
Sequence 2
t
p
df
closed-loss-closed-loss closed-loss-open-loss 13.42 <0.0001 19,750
closed-loss-closed-win closed-loss-open-win 13.14 <0.0001 19,915
closed-win-closed-loss closed-win-open-loss 15.06 <0.0001 19,442
closed-win-closed-win closed-win-open-win 15.97 <0.0001 19,867
open-loss-closed-loss
open-loss-open-loss 166.23 <0.0001 84,506
open-loss-closed-win
open-loss-open-win 165.24 <0.0001 86,156
open-win-closed-loss
open-win-open-loss 166.13 <0.0001 84,472
open-win-closed-win
open-win-open-win 168.10 <0.0001 90,598


Table S3 .
S3
Statistics and significance values for stay versus shift sequences in the two game history analysis for the Sicilian Defense.
Sequence 1
Sequence 2
t
p
df
closed-loss-closed-loss closed-loss-closed-win 0.06 0.95
25,684
closed-loss-open-loss
closed-loss-open-win 0.36 0.72
13,981
closed-win-closed-loss closed-win-closed-win 2.91 <0.01 25,889
closed-win-open-loss
closed-win-open-win 1.47 0.14
13,420
open-loss-closed-loss
open-loss-closed-win 1.75 0.08
12,852
open-loss-open-loss
open-loss-open-win
2.06 <0.05 157,810
open-win-closed-loss
open-win-closed-win 2.14 <0.05 12,570
open-win-open-loss
open-win-open-win
3.00 <0.01 162,500








Acknowledgements
This work was made possible by grants from the National Science Foundation (number IIS-2312373) and the NOMIS Foundation.


LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR






 










Predicting behavior from actions in the past: Repeated decision making or a matter of habit




H
Aarts






B
Verplanken






A
Van Knippenberg








Journal of Applied Social Psychology




28


15




















K
R
Allen






F
Brändle






M
Botvinick






J
Fan






S
J
Gershman






T
L
Griffiths






J
Hartshorne






T
U
Hauser






M
K
Ho






J
De Leeuw












Using games to understand the mind








Assessing human error against a benchmark of perfection




A
Anderson






J
Kleinberg






S
Mullainathan








ACM Transactions on Knowledge Discovery from Data (TKDD)




11


4
















Finite-time analysis of the multiarmed bandit problem




P
Auer






N
Cesa-Bianchi






P
Fischer








Machine learning




47
















Empowerment contributes to exploration behaviour in a creative video game




F
Brändle






L
J
Stocks






J
B
Tenenbaum






S
J
Gershman






E
Schulz








Nature Human Behaviour




7


9
















Adaptive expert decision making: Skilled chess players search more and deeper




G
Campitelli






F
Gobet








ICGA Journal




27


4
















Visualization, pattern recognition, and forward search: Effects of playing speed and sight of the position on grandmaster chess errors




C
F
Chabris






E
S
Hearst








Cognitive Science




27


4
















Perception in chess




W
G
Chase






H
A
Simon








Cognitive Psychology




4


1
















Quantifying human performance in chess




S
Chowdhary






I
Iacopini






F
Battiston








Scientific Reports




13


1


2113














Should i stay or should i go? how the human brain manages the trade-off between exploitation and exploration




J
D
Cohen






S
M
Mcclure






A
J
Yu








Philosophical Transactions of the Royal Society B: Biological Sciences




362
















Value learning through reinforcement: The basics of dopamine and reinforcement learning




N
D
Daw






P
N
Tobler








Neuroeconomics




Elsevier
















Decision theory, reinforcement learning, and the brain




P
Dayan






N
D
Daw
























Cognitive, Affective, & Behavioral Neuroscience




8


4


















A
D
De Groot








Thought and choice in chess




4




Walter de Gruyter GmbH & Co KG












Quantifying the complexity and similarity of chess openings using online chess community data




G
De Marzo






V
D
Servedio








Scientific Reports




13


1


5327














The rating of chessplayers, past and present




A
E
Elo








Arco Pub












Toward a general theory of expertise: Prospects and limits




K
A
Ericsson






J
Smith








Cambridge University Press












Deconstructing the human algorithms for exploration




S
J
Gershman








Cognition




173
















Bandit processes and dynamic allocation indices




J
C
Gittins








Journal of the Royal Statistical Society Series B: Statistical Methodology




41


2
















Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis




P
W
Glimcher








Proceedings of the National Academy of Sciences




108


supplement_3
















The secret of our success: How culture is driving human evolution, domesticating our species, and making us smarter




J
Henrich








Princeton University Press












Risk-taking in adversarial games: What can 1 billion online chess games tell us?




C
Holdaway






E
Vul








Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






43












Heuristics for meta-planning from a normative model of information search




I
Kuperwajs






M
K
Ho






W
J
Ma


















A joint analysis of dropout and learning functions in human decision-making with massive online data




I
Kuperwajs






W
J
Ma








Proceedings of the 44th Annual Conference of the Cognitive Science Society


the 44th Annual Conference of the Cognitive Science Society
















Darwin's unfinished symphony: How culture made the human mind




K
N
Laland








Princeton University Press












Dynamic response-by-response models of matching behavior in rhesus monkeys




B
Lau






P
W
Glimcher








Journal of the Experimental Analysis of Behavior




84


3
















Habits without values




K
J
Miller






A
Shenhav






E
A
Ludvig








Psychological Review




126


2


292














The neural and computational systems of social learning




A
Olsson






E
Knapska






B
Lindström








Nature Reviews Neuroscience




21


4
















Time spent thinking in online chess reflects the value of computation




E
Russek






D
Acosta-Kane






B
Van Opheusden






M
G
Mattar






T
Griffiths


















Structured, uncertainty-driven exploration in real-world consumer choice




E
Schulz






R
Bhui






B
C
Love






B
Brier






M
T
Todd






S
J
Gershman








Proceedings of the National Academy of Sciences


the National Academy of Sciences






116














Superhuman artificial intelligence can improve human decision-making by increasing novelty




M
Shin






J
Kim






B
Van Opheusden






T
L
Griffiths








Proceedings of the National Academy of Sciences




120


12


2214840120














A general reinforcement learning algorithm that masters chess, shogi, and go through self-play




D
Silver






T
Hubert






J
Schrittwieser






I
Antonoglou






M
Lai






A
Guez






M
Lanctot






L
Sifre






D
Kumaran






T
Graepel








Science




362


6419
















Tracing the trajectory of skill learning with a very large sample of online game players




T
Stafford






M
Dewar








Psychological Science




25


2
















Inferring latent learning factors in large-scale cognitive training data




M
Steyvers






R
J
Schafer








Nature Human Behaviour




4


11
















Reinforcement learning: An introduction




R
Sutton






A
Barto








MIT Press






2nd edition








Temporal difference learning and td-gammon




G
Tesauro








Communications of the ACM




38


3

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]