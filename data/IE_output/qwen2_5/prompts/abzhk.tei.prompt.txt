You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Decision making is a critical part of everyday life which underpins many types of actions. Even though there is a large range of decisions that individuals regularly engage in, researchers posit that various decision-making processes can be described within the framework of evidence accumulation models (EAMs; 
Donkin and Brown, 2018;
Forstmann et al., 2016)
.
EAMs posit that decision-makers gather information for each choice alternative until sufficient evidence for one alternative has been accumulated to commit to a decision 
(Donkin and Brown, 2018;
Ratcliff and Smith, 2004)
. Although many implementations of EAMs exist, most propose that decision-making is governed by a combination of least three latent cognitive processes. First, the drift rate drives the speed of the evidence accumulation process. Second, the threshold captures the amount of evidence needed to commit to a decision. Third, the non-decision time comprises both the time necessary for sensory processing of the stimulus and the time taken to execute the motor response.
EAMs describe response times and choices simultaneously, providing a model that accounts for both modalities of observed decision-making behavior. Given the success of EAMs in describing decision-making, and their sensitivity to manipulations of the decision process, their use is widespread 
(Ratcliff et al., 2016)
. EAMs are applied to study decision-making across various domains, such as conflict processing, perceptual decision-making, value-based decision-making, learning, working memory, and the speed-accuracy trade-off (e.g., 
Boag et al., 2022;
Hübner and Schlösser, 2010;
McDougle and Collins, 2020;
Miletić et al., 2021;
Polanía et al., 2014;
Rae et al., 2014)
.
To accommodate the differences between these tasks within the framework of evidence accumulation, EAMs are often adjusted for task-specific processes (e.g., 
Boag et al., 2019;
Collins and Frank, 2012;
van Ravenzwaaij et al., 2019)
. Furthermore, cognitive psychologists assume that the type of evidence accumulated differs between types of decisions. For example, the strength of accumulation for decisions involving working memory is assumed to be based on strength of working memory representations 
(Boag et al., 2021)
, whereas the accumulation process for value-based decision making may depend on subjective preference for alternate values 
(Ratcliff et al., 2016)
. Nevertheless, the underlying architecture of accumulating evidence for various choice options until a threshold is met, remains the same. Thus, explicit links between these designs exist in the framework of EAMs. If, for example, a subject is generally fast to respond due to a lack of caution, this is assumed to hold for different tasks 
(Hedge et al., 2019)
. Furthermore, even though the type of accumulated evidence differs between tasks, the accumulation process is still assumed to be based on information processing ability, a trait that should show similarities between tasks ).
Previous studies have tested the assumption that the cognitive processes assumed by EAMs are related between different decision-making tasks 
(Lerche et al., 2020;
Lerche and Voss, 2017;
Ratcliff and Childers, 2015;
Schmiedek et al., 2007;
Schulz-Zhecheva et al., 2016;
Yap et al., 2012)
. These studies mostly found that an individuals' ability to discriminate information efficiently, as measured by the most prominent EAM, the DDM, was related between different decision-making tasks. Still, these studies relied on tasks that engaged similar decision-making processes, for example, cognitive control.
Here, we chose to study tasks a wider range of domains of decision making tasks, keeping in mind that these domains may require the recruitment of different regions across the brain. We were specifically interested in tasks that are known to recruit both cortical and subcortical regions, which include the domains of working memory (Rac-Lubashevsky and 
Frank, 2021)
, value-based decision making and reinforcement learning 
(Gläscher et al., 2010;
O'Doherty, 2004)
, balancing speed and accuracy 
(Bogacz et al., 2010;
Forstmann et al., 2008)
, and cognitive control / conflict tasks 
(Aron and Poldrack, 2006;
de Hollander et al., 2017;
Isherwood et al., 2022;
Miletić, Bazin, et al., 2020)
. Studying these broader domains also allows us to assess whether the parameters in the EAM framework are stable across domains for which the neural underpinnings might partly vary.
To that end we relied on four different decision-making tasks that engage different domains of decision-making: 1. A reversal learning task (RL-Rev; 
Behrens et al., 2007;
Costa et al., 2015;
Miletić et al., 2021)
; 2. The multi-source interference task 
(MSIT;
Bush et al., 2003)
; 3. The reference-back task (RB; Rac-Lubashevsky and Kessler, 2016a, 2016b); 4. A reinforcement learning speed -accuracy trade-off task (RL-SAT; 
Miletić et al., 2021;
Sewell et al., 2019)
.
The four tasks involve different domains of decision-making, however, all four tasks were forced choice speeded decision-making tasks that fit within the EAM framework 
(Donkin and Brown, 2018)
. All task-specific EAMs contained response threshold parameters and non-decision time parameters. Furthermore, for each task we reparameterized the drift rate as a combination of an evidence dependent part of the accumulation process, which is characterized by information processing ability, and an evidence independent part of the accumulation process, which can be characterized as urgency 
(Trueblood et al., 2021)
. Both urgency and response caution allow decision-makers to strategically adjust their response speed, however, they have unique contributions to the decision-process. Increased urgency only leads to an increased probability of making a response with passing time, whereas higher response caution gives more time to choose the correct stimulus 
(Miletić and van Maanen, 2019;
Trueblood et al., 2021)
.
Besides engaging a broader scope of decision-making domains, the current study also uses a new methodology to test the relationships between these domains. Most previous studies that investigated the extent to which EAM constructs were related between different tasks, correlated the parameters of the EAMs to each other in a second, independent, step of analysis. However, recent work has highlighted that in order to best account for any relations that may exist between cognitive measures of different tasks, these relations should be explicitly accounted for in the model itself. Consequently, the model also accounts for measurement error and uncertainty within the estimation of these relations 
(Matzke, Ly, et al., 2017;
Turner et al., 2013;
Wall et al., 2021)
.
To elaborate, cognitive models are often estimated in a Bayesian hierarchical framework, in which the cognitive model parameter estimates of individual subjects are related through an overarching group distribution of each parameter. Besides having desirable effects on the estimation of the parameters of the model 
(Rouder and Lu, 2005;
Scheibehenne and Pachur, 2015)
, such a group level distribution is also beneficial since it facilitates inference at the level which is usually the target for analysis in psychological science -the population. In most studies that aim to investigate links between different decision-making tasks, different EAMs are estimated independently, in that no relationships between the different group level parameters of these EAMs are estimated.
Correlations are then calculated in a second step.
However, recent advances have used a multivariate normal distribution to describe the group level, which allows an explicit account of the covariances between the group level parameters 
(Gunawan et al., 2020;
Turner et al., 2013)
. Including a covariance structure in the group level allows us to estimate the likelihood of multiple models (components) simultaneously, by simply extending the vector of parameters to be estimated, while explicitly allowing the parameters (within and between components) to inform one another through the covariance structure. We refer to this approach that explicitly accounts for relationships between parameters as the joint modeling covariance approach 
(Turner, Forstmann, et al., 2017)
.
Joint modeling of cognitive model parameters of different tasks has two advantages compared to standard correlational approaches in which the correlations are calculated after the models were fit individually. First, estimation precision is improved and less attenuation of the true correlations is achieved by explicitly allowing relationships between parameters within the model, since measurement noise and parameter uncertainty are accounted for 
(Matzke, Ly, et al., 2017;
Rouder et al., 2019;
Wall et al., 2021)
. Second, since the covariances are also estimated in a Bayesian approach it is possible to construct credible intervals of correlation estimates, providing us with an inherent estimate of the uncertainty of our inferences.
Building on the covariance approach, recent work suggested to replace the multivariate normal at the group level, with a factor analysis decomposition 
(Innes et al., 2022;
Kang et al., 2021;
Turner, Wang, et al., 2017)
. This approach reduces the number of estimated parameters in the joint model, which can improve estimation with multiple components included in the joint model, yet still captures the relationships that exist between these parameters. Furthermore, with the hierarchical factor model we estimate latent factors that span the decision-making process across different decision-making domains and can aid interpretation.
In this study, we used the hierarchical factor modeling approach to investigate the relationships between the aforementioned four decision-making tasks. The participants completed each task twice in different sessions. Therefore, we could first test to what extent decision-making was related between different time-points. More importantly, since the four tasks were completed by the same set of participants, we could test whether the latent cognitive processes underlying decision-making, as proposed by EAMs, were related between different decision-making domains. 


Results
Participants each completed four different decision-making tasks twice. We analyzed the data using different joint models that explicitly estimated the relationships between sessions or between tasks in the architecture of the model. Of the task-specific models that formed the components of the joint models, the models of the RL-Rev and RL-SAT were selected based on model comparisons described in 
Miletić et al. (2021)
. The models of the MSIT and RB were newly developed and selected after a model comparison study (see Materials and Methods).


Between sessions
For each task, we tested to what extent the behavior of the participants on session 1 of the task was related to their behavior on session 2 of the task. To that end, we used a joint model of both sessions combined. These four between-session joint models had two components, the first component comprised the EAM parameters corresponding to the first session, and the second component the EAM parameters corresponding to the second session. Our between-session joint models relied on a multivariate normal distribution at the group-level to account for the relationships between the parameters. We included data from 54 participants in the between-sessions joint model of the RL-Rev, 86 participants for the MSIT, 85 for the RB, and 54 for the RL-SAT (see Materials and Methods). We translated the covariances of the multivariate normal group-level into correlations. We examined the within-session correlations from both sessions and the correlations between the parameters of the two sessions. For all tasks we constructed credible intervals from the marginal posterior distributions, for the correlations between parameters of the different sessions that mapped onto the same cognitive construct (e.g., the correlation between t 0 of session 1 and t 0 of session 2). We found that these credible intervals spanned only positive values for most of the parameters of the RL-SAT, RB, and MSIT (see 
Table 1
). These positive correlations indicate that participants who, for example, showed high response caution in session 1, also showed high response caution in session 2.
We also plotted the correlation matrices for the different tasks ( 
Figure 2, Figure 3
, 
Figure 4
, and 
Figure 5
). The within-session correlations are symmetrical along the
diagonal (ρ v ses1 ,b ses1 = ρ b ses1 ,v ses1
) and we therefore only plot the values below the diagonal.
The between-session correlations, which are then found in the lower square of the triangular plots, are, however, not symmetrical (e.g.
ρ v ses1 ,b ses2 ̸ = ρ b ses1 ,v ses2 )
. We found that again for the RL-SAT, RB and MSIT, the visual pattern of correlations observed between the parameters within the same session is replicated between the parameters of the different sessions, albeit weaker in strength (see 
Figure 3
, 
Figure 4
, and 
Figure 5
). This entails that if, for example, information processing ability and response caution were correlated constructs within a session, they were also correlated between sessions.
For the RL-Rev we found that the 95% credible intervals for the correlations between the parameters that mapped onto the same cognitive construct did contain 0 for most parameters 
(Table 1)
, which indicates that the correlations between sessions for the RL-Rev were inconclusive. Furthermore, the same visual pattern of correlations within session was not found between the two sessions ( 
Figure 2
), which again highlights that there was lower similarity between the behavior in session 1 and session 2 of the RL-Rev compared to the other three tasks. 


Figure 2
Mean parameter correlations of the joint model of the two sessions of the reversal learning task. The larger the circles the larger the absolute size of the correlations. The darker the red the more negative the correlations, the darker the blue the more positive the correlations.


Figure 3
Mean parameter correlations of the joint model of the two sessions of the reinforcement learning speed -accuracy trade-off task. The larger the circles the larger the absolute size of the correlations. The darker the red the more negative the correlations, the darker the blue the more positive the correlations.


Figure 4
Mean parameter correlations of the joint model of the two sessions of the reference-back task. The larger the circles the larger the absolute size of the correlations. The darker the red the more negative the correlations, the darker the blue the more positive the correlations.


Figure 5
Mean parameter correlations of the joint model of the two sessions of the multi-source interference task. The larger the circles the larger the absolute size of the correlations. The darker the red the more negative the correlations, the darker the blue the more positive the correlations.


Between tasks
To test to what extent the behavior of the participants was related between tasks,
we constructed a joint model that simultaneously estimated the parameters from all four task-specific models, which we will again refer to as components. In order to reduce the number of parameters we estimated, we pooled the data of both sessions for each task and estimated only one set of parameters per task. Still, the number of estimated group level relationships with a multivariate normal distribution could result in uncertain estimates with this many parameters. Therefore, to reduce the number of estimated group level parameters and improve parameter inference, we used hierarchical factor analysis (see 
Materials and Methods;
Innes et al., 2022)
. Furthermore, we can also examine the latent factors given by the hierarchical factor model for meaningful interpretation.
To determine which number of factors best described the relationships between the parameters of the joint model we estimated the marginal likelihood for a one, two and three-factor models using a recently developed implementation of IS 2 for hierarchical factor models, which relies on importance sampling to obtain estimates of the Bayes factors 
(Innes et al., 2022;
Tran et al., 2021)
. These importance samples can subsequently also be used to obtain standard errors of the Bayes factor estimates. We found that the group-level relationships were best described with 2 factors, standard errors in brackets
[BF 2−1 = 35.97(SE = 2.27), BF 2−3 = 57.12(SE = 2.90)]. We did not estimate a four-factor model, since the increased complexity of a three-factor model was already not preferred over the two-factor model 
(Innes et al., 2022)
.


Figure 6
Mean factor loadings of the winning two-factor joint model of the four decision-making tasks. The larger the circles the larger the absolute size of the factor loadings. The darker the pink the more negative the correlations, the darker the green the more positive the correlations. The mean group-level factor loadings of our two-factor model are plotted in 
Figure  6
 and the 95% credible intervals and the median for all loadings are presented in 
Table 2
.
We found that all information discrimination ability (δ) parameters, except for the RL-SAT, loaded positively onto the first factor. Furthermore, other evidence accumulation parameters of the MSIT and the RB also loaded positively onto the first factor. Thus, our first factor captured elements of the ability to discriminate information and of general evidence accumulation across tasks. Note that we also found weak negative loadings on the threshold parameters on the first factor for the MSIT and RB task, which suggests that participants who were good at the tasks also set lower thresholds for these tasks, as they could discriminate evidence more quickly. Furthermore, we found that parameters that related to threshold (B) or urgency (V 0 ) from the reinforcement learning tasks loaded highly on the second factor. Thus, our second factor was related to time management in the reinforcement learning tasks. Note that the first parameter (δ of the RL-Rev) on the second factor was fixed to 0 to satisfy estimation constraints 
(Ghosh and Dunson, 2009;
Innes et al., 2022)
.


Discussion
In this study, we investigated to what extent decision-making behavior is related between different types of decisions. To that end, we tested participants on four different decision-making tasks: a reversal learning task (RL-Rev; 
Behrens et al., 2007;
Costa et al., 2015
), a reinforcement learning speed -accuracy trade-off task (RL-SAT; 
Miletić et al., 2021;
Sewell et al., 2019
), a working memory task called the reference-back task (RB;
Rac-Lubashevsky and Kessler, 2016a, 2016b), and a cognitive control task, called the multi-source interference task (MSIT; 
Bush et al., 2003;
Isherwood et al., 2022)
. We relied on evidence accumulation models (EAMs) tailored to each task to to account for both modalities of decision-making behavior, responses and response times. Furthermore, EAMs facilitate interpretation of the behavior in terms of latent cognitive constructs (the parameters in the EAMs) underlying the data 
(Donkin and Brown, 2018;
Ratcliff et al., 2016
).
For the two learning tasks, we relied on previously developed and tested models that capture the interplay between learning and decision-making 
(Miletić et al., 2021)
. However, for the RB and MSIT, we developed two novel modelling approaches that provided a parsimonious account of the different experimental effects in the respective designs.
To test relationships between the parameters of our different EAMs, we employed the joint modelling framework, where relationships between parameters are measured with less attenuation compared to standard approaches, using a hierarchical model that explicitly estimates the relationships between the parameters as an integral part of the model 
(Matzke, Ly, et al., 2017;
Turner, Forstmann, et al., 2017;
Turner et al., 2013;
Wall et al., 2021)
. First, for each task, we constructed a between-session joint model of the two sessions participants had completed to test to what extent the cognitive constructs, as proposed by EAMs, were consistent between different time points. Then we constructed a between-task joint model to investigate to what extent these constructs were related between different decision-making domains.


Between sessions
The between-session joint models of the RL-SAT, RB and MSIT showed high correlations between the same latent constructs of the first and second session.
Furthermore, parameters that were correlated within session for these three tasks were predominantly also correlated between the two sessions, which indicates that strategic components of the decision-making process were consistent between the two sessions. For example, we found that participants who showed higher non-decision times in session one not only showed lower response caution within that same session to compensate for the time taken, but also in session two. The correlations across sessions between the same cognitive constructs and between strategic components, together provide evidence that
EAMs capture a high degree of similarity in behavior of the RL-SAT, RB and MSIT.
In contrast to the three other tasks, for the RL-Rev, we did not find these patterns of high between-session correlations between the same constructs or strategic components.
In the RL-Rev, participants learned to identify to most rewarding stimulus within a pair of stimuli, however, roughly halfway through each block the least rewarding stimulus of the pair became the most rewarding and vice versa 
(Behrens et al., 2007;
Costa et al., 2015
).
This reversal is not explicitly instructed to the participants. Thus, the dissimilarity between the two sessions is potentially caused by the participants slightly adjusting their behavior in the second session in anticipation of the reversal, and the reversal learning task might not be suitable for between-session aims.


Between tasks
Our primary interest was, however, to test to what extent the latent cognitive constructs as proposed by EAMs were related between different decision-making domains.
To that end, we constructed a joint hierarchical factor model that captured the relationships between the parameters of four different decision-making tasks using latent factors 
(Innes et al., 2022)
. We found that the relationships between the parameters of the joint model were best described using two factors.
Our first factor indicated that an individuals' ability to discriminate information quickly was related between most of the different types of decisions they faced in our four tasks. Furthermore, we also found that for the cognitive control task and working memory task, individuals who were good at information processing subsequently also demonstrated less response caution, potentially because of their ability to discriminate the correct from incorrect choices more quickly. Thus, similar information processing was engaged regardless of the type of decision, and in two tasks this lead to lower response caution.
The second factor in our hierarchical factor model provided additional support that although information processing ability was related between all four tasks, strategic components were not fully consistent across all tasks. Namely, the second factor indicated that participants who demonstrated a stronger sense of urgency, an evidence independent part of the evidence accumulation process 
(Trueblood et al., 2021)
, also showed higher response caution, but only in the two reinforcement learning tasks. The concept of urgency as defined in our models has comparable effects on predicted behavior as the concept of collapsing bounds in EAMs 
(Cisek et al., 2009;
Hawkins et al., 2015;
Miletić and van Maanen, 2019;
Thura and Cisek, 2016)
. Thus, presumably participants that exhibited a stronger sense of urgency had to compensate this urgency by setting higher thresholds.
Note that urgency and thresholds have unique contributions to the decision-process. High urgency paired with high response caution yields lower accuracy with increasing response times compared to the combination of low urgency and low response caution 
(Miletić and van Maanen, 2019;
Trueblood et al., 2021)
. Thus, we found evidence that a strategic component of time-management was related across the two reinforcement learning tasks, but not the other two decision-making tasks.
These results are in keeping with earlier work showing that the drift rate, which maps onto the latent cognitive process of information processing ability or cognitive efficiency, is consistent across decision-making domains 
(Lerche et al., 2020;
Schmiedek et al., 2007;
Schubert et al., 2016;
. The current work builds on the aforementioned approaches, first by taking into account more complex decision-making tasks from various domains. Second, by employing the joint modelling framework, specifically a joint hierarchical-factor analysis 
(Innes et al., 2022)
, that explicitly takes into account relationships between the parameters of our models to reduce attenuation of the estimated relationships and to therefore obtain more accurate estimates.
Although we did find information processing ability to be related between different types of decisions across our tasks, we found that other strategic components were only consistent between some of our tasks. A limitation of the current work is that we cannot rule out whether these inconsistencies are caused by the dissimilarity in behavior between these tasks, or by the dissimilarity in the modelling architecture, since we could not apply the exact same model to all four tasks. Although we attempted to keep the task-specific models as similar in architecture as possible, the inherent differences between the different tasks also required distinct modelling choices. Within the definition of an EAM psychologists attribute meaning to parameters of the model. However, with complex tasks we could only keep these parameters as similar in mathematical definition as possible and we were limited by the differences in the designs of the tasks. Nevertheless, this limitation holds for any analysis aimed at comparing relationships of cognitive processes between different types of decisions. Even in studies that performed subtraction analysis of response times, these subtractions are mapped onto inferred latent states, which can show even weaker consistency compared to the latent constructs as proposed by EAMs 
(Price et al., 2019;
.
The current study highlights that interpretation of inferred latent states between decision-making tasks must be done with proper caution, since what can be referred to as urgency in one task, could map onto a different cognitive process in another. Possibly, to take response caution for example, people could employ different response caution mechanisms for different types of decisions, which is why we did not find strong relationships between response caution across all four tasks. Alternatively, people do employ the same response caution mechanism across different types of decisions, but our models failed to isolate this mechanism in our response caution parameters, and different cognitive processes partially mapped onto our response caution parameters. The absence of a one-to-one mapping of processes to parameters is of course to be expected, since models are inherently simplifications 
(Guest and Martin, 2021;
Marr and Poggio, 1977)
. However, it becomes problematic when the processes-to-parameter mapping differs between different models, which could explain the inconsistencies in relationships estimated between our parameters. Future work could structurally explore to what extent the cognitive constructs proposed by EAMs differ between tasks with varying degrees of similarity, both in terms of modelling architecture and design.
In summary, in the current work, we found that an individuals ability to process information quickly and accurately was related between the different types of decisions they faced in our four tasks. Furthermore, three of our four tasks showed high consistency in the proposed cognitive processes across individuals, and the fourth task had an element of surprise that was potentially lost in the second session. Because of the flexibility of the proposed framework, our methods can be easily extended to include models of neural data to study cortical and sub-cortical networks involved in decision-making. Therefore, we believe that the joint modelling framework should be utilized to study cognitive processes at the core of decision-making.


Materials and Methods


Subjects and Procedure
150 students from the University of Leiden, the Netherlands, participated in an experiment consisting of five tasks that were each to be completed twice online from their own computers. The study was approved by the local ethics committee. For between session analysis of each task we included all participants that completed both sessions for that task above chance level accuracy. Out of 150, we included 86 participants for the MSIT, 85 for the RB, 54 for the RL-SAT and 54 for the RL-Rev. For analysis between tasks, we included all participants that completed each task twice (with both attempts above chance level accuracy), which included 53 participants. The high drop-out rate is mostly caused by participants not completing all sessions or having low accuracy, which was likely due to the online, rather than in-lab, participation.
The experiment served as a pilot study for a functional magnetic resonance imaging (fMRI) experiment including 5 tasks. One of these tasks was the stop-signal task. The model architecture needed to account for the behavioral data of the stop-signal task is vastly different to the other tasks 
(Matzke, Love, et al., 2017;
Verbruggen et al., 2019)
.
Therefore, we chose not to include the stop-signal task in our analyses. In depth methods including task background and design can be found in Appendix A.


Cognitive Modelling
We first tested the extent to which the behavior of the participants was related between sessions (within-tasks). For this, we constructed a joint model for each task where the components of the joint model were the EAMs applied to each session (i.e., EAM for session 1 vs EAM for session 2 for each task). Second, we tested to what extent the behavior of the participants was related between the four decision-making tasks (between-task analysis). To that end we constructed a joint model of all four tasks combined, where the components of the joint model were the EAMs for each task. In this between-task joint model we aggregated across the sessions, rather than having separate parameters for each session, to limit the number of relations between the parameters estimated. Here we outline the task-specific models that will function as the components in our between-sessions and between-tasks joint models.
To keep our task-specific models as similar as possible in architecture, we applied variants of the racing diffusion model (RDM; 
Tillman et al., 2020;
Zandbelt et al., 2014)
 to all four decision-making tasks. The RDM is an EAM that proposes a race between competing choices that each have separate accumulators. The first accumulator to reach the threshold determines the choice made, and the time taken to reach the threshold determines the response time along with the non-decision time. The RDM combines aspects of the DDM as it posits within trial noise in the accumulation process 
(Ratcliff and Smith, 2004)
, and the LBA as it comprises separate accumulators for each choice 
(Brown and Heathcote, 2008)
. The choice of EAM was based on the model comparisons in Miletić et al. (2021) for the two reinforcement learning tasks, and the architecture was extended to the other two decision-making tasks.


Reinforcement learning tasks
We replicated the reinforcement-learning speed -accuracy trade-off task (RL-SAT) and a reinforcement learning reversal task (RL-Rev) from 
Miletić et al. (2021)
, in which participants were tasked to learn the reward probabilities associated with different pairs of symbols. That experiment studied reciprocal influences of learning and decision-making, by
integrating EAMs with reinforcement learning models (RL-EAMs; 
Fontanesi, Gluth, et al., 2019;
Fontanesi, Palminteri, et al., 2019;
Frank et al., 2015;
Miletić, Boag, and Forstmann, 2020;
Miletić et al., 2021;
Pedersen et al., 2017;
Sewell et al., 2019)
. RL-EAMs propose that people make decisions by gradually integrating information of value representations associated with each available choice option. When enough evidence has been accumulated, they commit to a choice and the associated feedback is used to update the value representations. In turn, these value representations drive the speed of evidence accumulation the next time the decision-maker is faced with the same choice.
In the previous study 
(Miletić et al., 2021)
, the authors relied on the delta learning rule to model the learning process:
Q t+1 = Q t + α(r − Q t )
(1)
This entails that the difference between reward r and the current expected value Q t is scaled by the learning rate α to determine how much the current value representation is updated to form the new expected value representation following feedback Q t+1 . These value representations drive the drift rate, the speed of evidence accumulation. That study showed that the choice of EAM in the RL-EAM greatly influenced the extent to which the RL-EAM could describe the data. That study showed that the advantage framework in combination with a racing diffusion model (ARD), could best describe the learning related increase in response accuracy and response speed. The advantage framework posits that the speed of evidence accumulation is a weighted sum of three components. First, a baseline, evidence independent component, which can be interpreted as urgency 
(Miletić and van Maanen, 2019;
Trueblood et al., 2021)
. Second, the difference in perceived value between the available options. Third, the sum of the perceived values of all options 
(van Ravenzwaaij et al., 2019)
. Additionally, the evidence accumulation process is subject to Gaussian noise W , with standard deviation s, which was fixed to 1 to satisfy scaling constraints 
(Donkin et al., 2009;
van Maanen and Miletić, 2020)
. When faced with two choices as in our instrumental learning tasks, the accumulators associated with each choice can thus be described as:
dx 1 = [V 0 + δ(Q 1 − Q 2 ) + Σ(Q 1 + Q 2 )] + sW dx 2 = [V 0 + δ(Q 2 − Q 1 ) + Σ(Q 1 + Q 2 )] + sW (2)
Furthermore, non-decision time (t 0 ) and response caution (B) were also estimated.
The same model architecture, a combination of the advantage framework racing diffusion model with a reinforcement learning algorithm, was applied to both learning tasks. Below we outline task-specific changes to the modelling architecture, if any, for the two reinforcement learning tasks.


Reinforcement learning reversal task
The reinforcement learning reversal task (RL-Rev) is an instrumental learning task in which the associated reward probabilities within a pair are switched roughly halfway through a block 
(Behrens et al., 2007;
Costa et al., 2015)
. This tests the ability of the participant to update their representation of the most valuable stimulus within a stimulus pair. We used the above described RL-ARD to account for the behavioral data of this task.
The model predicts that following the reversal, the reward prediction errors will increase, which in turn will lead to updating of the Q values. The RL-ARD we used for the RL-Rev has six free parameters (α, V 0 , δ, Σ, b, t 0 ).


Reinforcement learning speed -accuracy trade-off task
In the reinforcement learning speed -accuracy trade-off task (RL-SAT), participants complete an instrumental learning task in which they are instructed to emphasize response accuracy on half of the trials, and response speed on the other half of the trials 
(Sewell et al., 2019)
. On these speed trials, participants also have less time to respond, forcing them to leverage speed for caution, which is referred to as the speed -accuracy trade-off 
(Bogacz et al., 2010;
Ratcliff and Rouder, 1998)
. Similar to other recent papers 
(Arnold et al., 2015;
Heathcote and Love, 2012;
Rae et al., 2014;
Sewell et al., 2019)
, 
Miletić et al. (2021)
 found that such SAT manipulations are best described by both drift rate adjustments (in this case separate urgency components, V 0,spd V 0,acc ), as well as threshold adjustments (different thresholds b spd and b acc 
;
Miletić et al., 2021)
. In total, the model we used for the RL-SAT was a RL-ARD with eight free parameters (α, V 0,spd , V 0,acc , δ, Σ, b spd , b acc , t 0 ).


Reference-back task
The reference-back task (RB) is a working memory task in which participants have to compare the current stimulus to a stimulus held in working memory (the reference) to make a binary "same" or "different" response. On comparison trials, the stimulus needs only to be compared to the reference stimulus held in working memory, whereas on reference trials, the stimulus also becomes the reference for subsequent trials 
Kessler, 2016a, 2016b)
. Stimuli were presented as either the letter "X" or "O", encircled by a red frame for reference trials and a blue frame for comparison trials. Using the RDM we can disentangle the cognitive costs of working memory updating (reference trials) from working memory comparison (comparison trials).
Previous studies showed that participants performed better when responding "same" compared to "different" and when there was a comparison rather then a reference trial.
Furthermore, these studies also found a sequencing effect in that participants were faster when previous trial's trial type (i.e., reference or comparison) was repeated compared to a switch in the trial type sequence (e.g. 
Boag et al., 2021;
Jongkees, 2020;
Rac-Lubashevsky and Frank, 2021;
Rac-Lubashevsky and Kessler, 2016b
). In the current study we also found that participants performed better following stimulus-identity (e.g., X or O) repetitions between trials compared to switches in stimulus identity. We aimed to construct the most parsimonious model that still accounted for the aforementioned effects and was similar in architecture as the RL-ARD described above. Therefore, instead of letting drift rates vary across response, trial type, trial type sequence, and stimulus sequence, we used a sensitivity parametrization, in which the drift rates for two options can be written as a combination of an urgency term and a difference term (similar to Luke 
Strickland et al., 2018)
, such that:
v correct = V 0 + δ 2 , v error = V 0 − δ 2 (3)
Thus, we can estimate some of the aforementioned effects as differences in V 0 (urgency) and some as differences in δ (processing ability), effectively reducing the number of parameters estimated. This parameterization is essentially a simplification of the advantage framework. To elaborate, the V 0 parameter in equation 3, captures both the urgency and the sum term of the advantage framework (equation 2), since these cannot be disentangled in a task where there are no inherent stimulus values. Analogously, the δ in the advantage framework weights the difference of stimulus values, whereas the δ in equation 3 captures the difference in evidence for each stimulus. Even though this parameterization is a simplification of the advantage framework it still results in a range of models that capture all effects associated with the RB task. Since the above described model was newly developed we performed a model comparison study to test which combination of parameters could best account for our behavioral data (see appendix B). In the winning model, the evidence accumulation process for the correct and incorrect choice can be described as:
dx correct = [V 0,type−trans + δ type,response ]dt + sW dx incorrect = [V 0,type−trans − δ type,response ]dt + sW
(4)
This entails that the δ parameter varied depending on each combination of what would be the correct response ("same" or "different"; same choices are generally easier) and trial type ("reference" or "comparison"; comparison trials are generally easier). We also estimated an urgency term that differs for type transitions, e.g. repetitions in trial type or switches in trial type. Additionally, the winning model comprised a difference in threshold between repetitions in stimulus-identity and switches in stimulus-identity. In total, our RDM for the RB comprised 10 parameters
(V 0,type−trans 1 V 0,type−trans 2 , δ resp 1 ,type 1 , δ resp 2 ,type1 , δ resp 1 ,type 2 , δ resp 2 ,type2 , b stim−trans 1 , b stim−trans 2 , t 0 ).
We numbered the types of trials, transitions of trials, transitions of stimulus identity and responses for brevity.


Multi-Source Interference Task
The multi-source interference task (MSIT; 
Bush et al., 2003)
 combines two types of interference with the relevant (target) visual information, either from irrelevant location information (the Simon effect; Hommel, 2011) or irrelevant stimuli spatially adjacent to the target (the Flanker effect; 
Eriksen, 1995)
. In the current experiment we use an adjusted version of the MSIT 
(Isherwood et al., 2022)
, where Flanker and Simon interference are presented both separately and combined in different types of trials (for example trials see 
Figure 1
; for a more in depth design description see Appendix A). Together, these different trial types allowed us to test the extent to which decision-making processes are influenced by Flanker or Simon interference and a possible combination of both. We rely on an EAM framework that introduces separate drift rates for both Flanker and Simon interference. In contrast to our other three decision-making tasks, participants always had three response options rather than two.
To the best of our knowledge, there have been no previous studies that used a
model-based approach to analyze the MSIT. We therefore constructed a process-oriented model of decision making that could best describe the observed behavioral effects. We hypothesized that the drift rate for each choice is jointly driven by an urgency component and the evidence supporting that choice. To quantify the evidence in support for each choice, we described the drift rate for choice A as a combination of possible Flanker support, Simon support, and target support (the correct response). Additionally, the evidence accumulation process is subject to Gaussian noise W , with standard deviation s, which again was fixed to 1 to satisfy scaling constraints. Consequently, the drift rates in our MSIT model can be desribed as:
dx = [V 0 + v F lank + v Simon + δ]dt + sW (5)
Where v F lank and v Simon were 0 if there was no Flanker support or Simon effect respectively. Similarly, δ was only added to the accumulator that matched the correct response. Furthermore, we found that response time and accuracy were influenced by the position of the target, possibly due to left to right reading effects. We therefore modified the starting point of the accumulator corresponding to the target, based on position of the target. We fixed start pos 3 to 0 to set a base positional startpoint modifier to which the other positional modifiers were relative, to satisfy scaling constraints. In total, our MSIT model comprised 8 parameters
(v F lank , v Simon , δ, start pos 1 , start pos 2 , V 0 , B, t 0 ).
The above described model was selected after model comparison against competing models, for details see appendix B.


Joint model
For each task, we used the above described models to first test to what extent the behavior of the participants on session 1 of a task is related to their behavior on session 2 of a task. To that end we used a joint model of both sessions combined. These four between-session joint models had two components, the first component was the parameter set corresponding to the first session, and the second component the parameter set corresponding to the second session.
We concatenated the parameters from the components into a single vector of parameters that we estimated per participant (random effects) and for each of those parameters we also estimated a mean at the group level that describes the parameters of all participants as a group. We were specifically interested in the relationships that existed between these group level parameters. Therefore, we used a hierarchical model that allows for covariances between the group-level parameters 
(Gunawan et al., 2020;
Turner et al., 2013)
. We applied this joint modelling architecture to construct four joint models of the different sessions for the different tasks.
Besides the between-session joint models, we similarly constructed a joint model that simultaneously estimated the parameters from all four task-specific models, which we will again refer to as components. In order to reduce the number of parameters we estimated, we pooled the data of both sessions for each task and estimated only one set of parameters per task. Nevertheless, if we combined the parameters estimated per participant from each task-specific model component, we would need to estimate 6 (RL-Rev) + 8 (RL-SAT) + 9 (RB) + 8 (MSIT) = 31 parameters per participant. If we then applied a multivariate normal distribution as a group level distribution it would result in 31 means + 31*(31-1)/2 variances and covariances, thus a total of 496 group level parameters. To reduce the number of estimated group level parameters and improve parameter inference, we used hierarchical factor analysis rather than a multivariate normal 
(Innes et al., 2022)
. Furthermore, we can also examine the latent factors given by the hierarchical factor model for meaningful interpretation. To answer what number of factors optimally described our data, we estimated multiple joint factor models each with different number of factors and interpreted the model with the highest marginal likelihood (the gold standard in Bayesian model comparison; 
Kass and Raftery, 1995)
, as estimated by the newly developed approach, importance sampling squared (IS 2 ; 
Innes et al., 2022;
Tran et al., 2021)
. IS 2 uses importance sampling on both the individual level and the group level of the hierarchical model to obtain an estimate of the marginal likelihood. The importance samples can subsequently also be used to calculate Bayes factors (the ratio of two marginal likelihoods) and a bootstrapped standard error of these Bayes factors.


Bayesian MCMC sampling using PMwG
All models were estimated using particle Metropolis within Gibbs sampling 
(PMwG;
Gunawan et al., 2020)
, which comprises three phases: burn-in, adaptation, and sampling. The group-level distributions, in our case the multivariate normal or the factor decomposition of it, is described using Gibbs sampling 
(George and Mcculloch, 1993)
. For the group-level mean parameters we used a multivariate normal prior with variance 1 and covariance 0. The prior mean for the group level means was set to 2 for threshold 
(B)
 parameters, 2 for processing ability (δ) parameters, 1 for urgency (V 0 ) parameters, 0.2 for non-decision time (t0) parameters and 0 for all other parameters. For the prior on the group level covariance and factor structure we relied on the default priors described in 
Gunawan et al. (2020)
 and 
Innes et al. (2022)
 respectively.
The parameters at the individual level are estimated using particle Metropolis-Hastings sampling 
(Chib and Greenberg, 1995)
, which uses different proposal distributions in combination with importance sampling. In the burn-in phase, these proposals are drawn jointly from the group-level distribution at that Markov chain Monte Carlo (MCMC) iteration and a multivariate normal distribution centered on the previous set of random effects of that individual in the MCMC chain. Following burn-in, the parameters have converged towards their posterior, therefore in the adaptation stage we draw samples that approximate the posterior distribution. We subsequently use these samples to create a distribution that mimics the posterior to efficiently draw proposals from. In the sampling stage, this efficient distribution, together with the group-level distribution and the distribution centered on the previous set of parameters of that individual are used to draw proposals for the particle Metropolis-Hastings step for each individual 
(Gunawan et al., 2020)
.


Code Availability
All code and samples are publicly available on OSF (https://osf.io/c3d75/).


Competing interests
None of the authors declare having competing interests.
After each choice, participants received feedback consisting of two components: an outcome and a reward. The outcome refers to the outcome of the probabilistic gamble, whereas the reward refers to the number of points the participant actually received. If the participant responded in time, the reward was equal to the outcome.


Reinforcement learning reversal learning task
In the reinforcement learning reversal task (RL-Rev) we used the above described paradigm with the following settings. Participants completed two sessions, each with two blocks and 128 trials per block. Two pairs of symbols with associated reward probabilities .8/.2 and .7/.3 were presented in a block, randomly interleaved. Between trial 61 and 68
(sampled from a uniform distribution) of each block, the associated reward probabilities within a pair were switched, such that what used to be the most rewarding stimulus within a pair, now became the least rewarding of the two stimuli. No symbols were repeated between the blocks. Participants were not informed of the reversal in the instructions of the experiment. On each trial, the pair of symbols was presented until a participant made a response, but with a maximum of 2000 ms. Following stimulus presentation, the choice was highlighted for 500 ms after which feedback was presented for 750 ms. One session of the RL-Rev was always paired with a session of the reference-back task. The order of the tasks within session and between the two sessions was counterbalanced between participants.
On each trial, the pair of symbols was presented until a participant made a response, but with a maximum of 2000 ms. Following stimulus presentation, the choice was highlighted for 500 ms after which feedback was presented for 750 ms.
One session of the RL-Rev was always paired with a session of the reference-back task. The order of the tasks within session and between the two sessions was counterbalanced between participants.


Reinforcement learning speed -accuracy trade-off task
In order to manipulate the speed -accuracy trade-off, a cue and a deadline manipulation were added to the reinforcement learning paradigm 
(Frank et al., 2004)
.
Prior to each trial a cue instructed participants to emphasize response speed ('SPD') or accuracy ('ACC'). Participants did not earn a reward in speed trials if they responded slower than 600ms. Speed and accuracy trials were randomly interleaved.
Participants completed two sessions with each 324 trials with 108 trials per block.
In each block 3 different pairs of symbols were each presented 36 times, with associated reward probabilities 0.8/0.2, 0.7/0.3, and 0.6/0.4. No symbols were repeated between the blocks. On each trial, the cue was presented on screen for 1500 ms, after which the pair of symbols was presented until a participant made a response, but with a maximum of 1500 ms. Following stimulus presentation, the choice was highlighted for 250 ms after which feedback was presented for 1000 ms.


Reference-back
Our third task was a replication of the reference-back task as described in 
Rac-Lubashevsky and Kessler (2016a)
. Participants compared the stimulus presented on screen to a stimulus held in working memory (the reference stimulus). Two types of trial exist. In comparison trials the participant only compared the stimulus to the previous reference trial. In reference trials, the participant not only compared the stimulus to the reference stimulus, but subsequently also updated the current stimulus as the new reference stimulus in working memory. Stimuli could be either a white "X" or "O". A blue frame around the stimulus indicated that the current trial was a comparison trial. A red frame around the stimulus indicated that the current trial was a reference trial. Reference and comparison trials were randomly interleaved. An example trial sequence is shown in 1.
At the start of each trial, a blank screen was presented for 1000 ms, after which a fixation cross was presented for 1000 ms. Then the stimulus was presented until a response was made. Participants completed 192 trials per session. One session of the RB task was always paired with a session of the RL-Rev task. The order of the tasks within session and between the two sessions was counterbalanced between participants.


MSIT
The multi-source interference task is a cognitive control task in which participants have to identify the unique number out of three numbers presented on screen 
(Bush et al., 2003;
Isherwood et al., 2022
).
In the "Flanker" condition, the two non-target stimuli have the identity of another valid response. Furthermore, in the "Simon" condition, the position of the stimulus and the identity of stimulus are incongruent (Simon).
In total, we can derive 5 different conditions from our MSIT design 
(Figure 1)
. In condition 1, the identity of the target is congruent with its position and the other two stimuli are equal to 0. Since 0 is not a possible response option, there is no response conflict in these trials. In condition 2, the identity of the target and its position differ, resulting in Simon interference. In condition 3, there are Flankers that suggest a different response than the target stimulus, but the position and identity of the target are congruent. In condition 4, there is both Flanker and Simon interference, however, the Flankers point towards a different response than the Simon. Lastly, in condition 5 there is again both Flanker and Simon interference, however, both the Flanker and the Simon interference point towards the same incorrect response.
Participants completed 265 trials per session. On each trial after the response window, feedback of either 'in time' (responses less than 600ms), 'too slow' (responses between 600 -900ms) or 'very slow' (for response more than 900ms) was shown. One session of the MSIT was always paired with a session of the stop-signal task. The order of the tasks within session and between the two sessions was counterbalanced between participants.


Appendix B: Model comparisons
Here we describe the model comparison study we performed for the reference-back (RB) task and multi-source interference task (MSIT). For both tasks we pooled the data of the two sessions together and estimated different models to that data. Model comparisons


Figure 8
Posterior predictives for the reinforcement-learning speed -accuracy trade off task. Left, the data for all the trials where the speed cue was presented. Right, the trials where the accuracy cue was presented. Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas in the middle and right column correspond to the 95% credible interval of the posterior predictive distribution.
In the posterior predictives of the multi-source interference task we found that the correct RTs across all conditions and the accuracy accross the position of the stimulus and the identity of the Flanker matches the behavioral data. We also found that the model underestimated the accuracy when the target stimulus was position 1 and 3. Furthermore, the error response time distributions were consistently overestimated in the model. This misfit is most likely because of the low percentage of errors in the behavioral data. In the posterior predictives of the reference-back task we observed slight underestimations in the accuracy across the experimental conditions. The correct RT distributions generally matched the behavioral data. However, the error RT distributions were mostly underestimated. Again, this is likely because of the low percentage of errors in the behavioral data. 


Figure 10
Posterior predictives for the reference-back task. A) the data aggregated accross the two different correct possible responses. B) The data aggregated across the two different trial types C) The data aggregated across the different transitions in trial type compared to the trial type of the previous trial. D) The data aggregated across the different transitions in stimulus identity compared to the stimulus identity of the previous trial. Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas in the middle and right column correspond to the 95% credible interval of the posterior predictive distribution.
Figure 1
1
Visual descriptions of the four decision-making tasks. A) The reversal learning task (RL-Rev), the reward contingencies are the reward probabilities associated with the two pairs of symbols presented per block. B) The multi-source interference task (MSIT). C) The reference-back (RB). D)The reinforcement learning speed -accuracy trade-off.


( a )Figure 9
a9
Identity of the target (b) Position of the target (c) Identity of the Flanker distractor Posterior predictives for the multi-source interference task. A) the data aggregated accross the three different target identities. B) The data aggregated across the three different positions where the target could be presented, which is the Simon distractor if it does not have the same identity as the target. C) The data aggregated across the different possible identities of the Flanker distractors. Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas in the middle and right column correspond to the 95% credible interval of the posterior predictive distribution.


Correct response (b) Type of the stimulus (c) Transition of the trial type (d) Transition of the identity of the stimulus


Table 1
1
95% credible intervals of the correlations between the same parameters of session 1 and 2 for the different tasks. Correlations with credible intervals not containing 0 are marked in bold.
Correlation estimates
Variable
2.5%
50%
97.5%
RL-Rev
t 0
-0.14
0.16
0.42
V 0
0.11
0.41
0.66
B
-0.21
0.11
0.41
δ
-.40
-0.15
0.12
α
-.46
-0.16
0.19
Σ
-.38
-0.08
0.26
MSIT
t 0
0.06
0.39
0.63
V F l
0.09
0.36
0.60
V Si
0.33
0.55
0.73
strt p 1
0.15
0.44
0.66
strt p 2
0.14
0.40
0.63
δ
0.33
0.55
0.72
V 0
-0.06
0.25
0.55
B
-0.14
0.24
0.54
RB
t 0
0.06
0.37
0.62
V 0tt 1
0.20
0.48
0.69
V 0tt 2
0.17
0.42
0.65
δ tr 1
0.18
0.48
0.69
δ tr 2
0.29
0.55
0.74
δ tr 3
0.27
0.55
0.72
δ tr 4
0.20
0.47
0.68
B st 1
0.23
0.53
0.75
B st 2
0.29
0.56
0.75
RL-SAT
t 0
0.19
0.46
0.66
V 0c 1
0.26
0.55
0.76
V 0c 2
0.42
0.64
0.79
B c 1
0.52
0.72
0.86
B c 2
0.58
0.76
0.87
Σ
-.15
0.20
0.50
δ
0.13
0.48
0.72
α
0.34
0.60
0.78


Table 2
2
95% credible intervals of the factor loadings of the winning two-factor joint model of the four decision-making tasks. Loadings with credible intervals outside of 0 are marked in bold.
Factor 1 loadings
Factor 2 loadings
Variable
2.5%
50%
97.5%
2.5%
50%
97.5%
RL-Rev
δ
0.23
0.30
0.40
0
0
0
B
-0.18
-0.05
0.11
0.22
0.28
0.37
t 0
-0.03
0.02
0.7
-0.07
-0.02
0.03
V 0
-0.14
0.015
0.20
0.02
0.18
0.36
α
-0.14
-0.05
0.05
-0.05
0.05
0.16
Σ
-0.05
-0.08
0.21
-0.19
-0.06
0.07
MSIT
t 0
-0.02
0.03
0.08
-0.04
0.02
0.07
v F l
0.01
0.18
0.34
-0.08
0.08
0.23
v Si
0.01
0.18
0.34
-0.29
-0.12
0.04
strt p 1
-0.08
-0.01
0.05
-0.09
-0.01
0.05
strt p 2
-0.07
-0.02
0.04
-0.08
-0.01
0.06
δ
0.08
0.34
0.61
-0.08
0.21
0.52
V 0
-0.38
-0.17
0.03
-0.34
-0.14
0.06
B
-0.25
-0.14
-0.04
-0.17
-0.03
0.11
RB
t 0
-0.02
0.03
0.08
-0.05
0.00
0.05
V 0tt 1
0.16
0.28
0.40
-0.07
0.12
0.29
V 0tt 2
0.13
0.25
0.36
-0.03
0.14
0.28
δ tr 1
0.45
0.66
0.89
-0.22
0.22
0.51
δ tr 2
0.42
0.66
0.91
-0.10
0.28
0.61
δ tr 3
0.42
0.63
0.89
-0.18
0.25
0.53
δ tr 4
0.30
0.45
0.63
-0.18
0.12
0.36
B st 1
-0.31
-0.19
-0.06
-0.01
0.13
0.27
B st 2
-0.37
-0.22
-0.07
0.02
0.17
0.33
RL-SAT
t 0
-0.04
0.02
0.07
-0.10
-0.05
0.00
V 0c 1
-0.44
-0.13
0.30
0.39
0.61
0.90
V 0c 2
-0.55
-0.18
0.30
0.42
0.68
0.94
B c 1
-0.43
-0.16
0.20
0.43
0.60
0.81
B c 2
-0.48
-0.17
0.21
0.44
0.65
0.86
Σ
0.01
0.15
0.31
-0.19
-0.01
0.16
δ
-0.14
0.06
0.26
-0.32
-0.12
0.07
α
-0.09
0.05
0.20
-0.02
0.11
0.27








Appendix A: design


Reinforcement learning tasks
Two of the four decision-making tasks were replications of probabilistic instrumental learning tasks 
(Frank et al., 2004)
 described in 
Miletić et al. (2021)
. Here we describe the general paradigm, after which we will highlight the differences between the two tasks. On each trial participants chose between two abstract symbols that were both associated with a fixed probability of returning reward when chosen. Within that pair of stimuli, one of the choice options always had a higher chance of returning reward than the other. The aim for the participant was to maximize returns, by learning through trial and error which of the two symbols was associated with a higher chance of returning reward. If a choice was rewarded, the reward was equal to a 100 points.
were made using the Bayesian predictive information criterion (BPIC; 
Ando, 2007)
, rather than Bayes factor estimates using IS 2 , since in contrast to the joint models, the individual likelihoods were of main interest rather than the group-level distributions, in which case the BPIC is far more computationally efficient. Note that contrary to Bayes factors and marginal log-likelihood estimates, for the BPIC lower is better.


Reference-back
For the different RB models we limited our model search space by only comparing models where all manipulations and sequence effects as described in the Cognitive Modelling sectioned were captured. Furthermore, we limited ourselves to 10 parameter models, which was the minimal amount of parameters to describe all facets of the data, while considering the cognitive plausibility and assumptions of the effect-to-parameter mapping within the framework of evidence accumulation modelling. The BPIC scores are summarized in 
Table 3
. 
Table 3
 BPIC results for the different models compared for the reference-back experimental data. Each column in the table corresponds to a manipulation in the data for which there were two conditions. The parameter mentioned in that column, entails that for the model corresponding to that row that parameter was varied between those two conditions. Type refers to reference vs comparison trial. Response refers to same versus different trials. Stim trans refers to whether the current trial was a switch or repetion of stimulus ('X' or 'O') compared to the previous trial. T yp trans refers to whether the current trial was a switch or repetition of trial type 
('reference' or 'comparison')
 compared to the previous trial.


M odel T ype Response Stim trans T yp trans BP IC


Multi-source interference task
For the different MSIT models the behavioral effects our model was to capture where the Flanker manipulation, the Simon manipulation and the observed differences between left and right presentation of the target stimulus. Again we limited our model search by excluding mappings of these effects to EAM parameters that violated assumptions about these parameters. The BPIC scores are summarized in 
Table 4
. 


Appendix B: Model fits
Here we present posterior predictives for the different EAMs we constructed. We summarize accuracy and correct and error response time (RT) distributions of the four different decision-making tasks. For each task we plot these quantities for the conditions of interest in the experiment. Throughout we plot the 10th, 50th and 90th percentile of the RT distributions. The 50th percentile corresponds to the central tendency and the smaller difference between the 10th and the 50th percentile compared to the difference between the 90th and the 50th percentile summarizes the positive skew generally found in RT distributions. For the RL tasks we visualize the effect of learning by dividing the trials in bins and plotting the bins along the x-axis. All data are collapsed across participants. To asses overlap between the predictions of the joint model and the models fit individually, we plot the posterior predictives of the joint model in green and of the single, or individually fit, model in blue. We find that the joint model generally predicts very similar behavior as the models fit individually.
For the reinforcement learning reversal task we found that similar to 
Miletić et al.,
 
 










Bayesian Predictive Information Criterion for the Evaluation of Hierarchical Bayesian and




T
Ando








Biometrika




94


2
















Empirical validation of the diffusion model for recognition memory and a comparison of parameter-estimation methods




N
R
Arnold






A
Bröder






U
J
Bayen




10.1007/s00426-014-0608-y








Psychological Research




79


5
















Cortical and subcortical contributions to stop signal response inhibition: Role of the subthalamic nucleus




A
R
Aron






R
A
Poldrack




10.1523/JNEUROSCI.4682-05.2006








Journal of Neuroscience




26


9
















Learning the value of information in an uncertain world




T
E
Behrens






M
W
Woolrich






M
E
Walton






M
F
Rushworth




10.1038/nn1954








Nature Neuroscience




10


9
















Cognitive control of working memory: A model-based approach




R
J
Boag






N
Stevenson






R
Van Dooren






A
C
Trutti






Z
Sjoerds






B
U
Forstmann




10.3390/brainsci11060721








Brain Sciences




11


6




















R
J
Boag






L
Strickland






Heathcote












Evidence accumulation modelling in the wild: Understanding safety-critical decisions




A
Neal






A
Palada






H
Loft






S








Trends in Cognitive Sciences




2














Strategic attention and decision control support prospective memory in a complex dual-task environment




R
J
Boag






L
Strickland






S
Loft






A
Heathcote




















10.1016/j.cognition.2019.05.011








Cognition














The neural basis of the speed-accuracy tradeoff




R
Bogacz






E
J
Wagenmakers






B
U
Forstmann






S
Nieuwenhuis








Trends in Neurosciences




33


1


















10.1016/j.tins.2009.09.002














The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive Psychology




57


3


















10.1016/j.cogpsych.2007.12.002














The multi-source interference task: Validation study with fMRI in individual subjects




G
Bush






L
M
Shin






J
Holmes






B
R
Rosen






B
A
Vogt




10.1038/sj.mp.4001217








Molecular Psychiatry




8


1
















Understanding the Metropolis-Hastings Algorithm




S
Chib






E
Greenberg








The American Statistician




49


4
















Decisions in changing conditions: The urgency-gating model




P
Cisek






G
A
Puskas






S
El-Murr








Journal of Neuroscience




29


37


















10.1523/JNEUROSCI.1844-09.2009














How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis




A
G
Collins






M
J
Frank








European Journal of Neuroscience




35


7


















10.1111/j.1460-9568.2011.07980.x














Reversal learning and dopamine: A Bayesian perspective




V
D
Costa






V
L
Tran






J
Turchi






B
B
Averbeck








Journal of Neuroscience




35


6


















10.1523/JNEUROSCI.1989-14.2015














Comparing functional MRI protocols for small, iron-rich basal ganglia nuclei such as the subthalamic nucleus at 7 T and 3 T




G
De Hollander






M
C
Keuken






W
Van Der Zwaag






B
U
Forstmann






R
Trampel




10.1002/hbm.23586








Human Brain Mapping




38


6
















The overconstraint of response time models: Rethinking the scaling problem




C
Donkin






S
D
Brown






A
Heathcote




10.3758/PBR.16.6.1129


















Response Times and Decision-Making




C
Donkin






S
D
Brown




10.1002/9781119170174.epcn509


















The Flankers Task and Response Competition: A Useful Tool for Investigating a Variety of Cognitive Problems




C
W
Eriksen








Visual Cognition




2


2-3


















10.1080/13506289508401726














A reinforcement learning diffusion decision model for value-based decisions




L
Fontanesi






S
Gluth






M
S
Spektor






J
Rieskamp




10.3758/s13423-018-1554-2








Psychonomic Bulletin and Review




26


4
















Decomposing the effects of context valence and feedback information on speed and accuracy during reinforcement learning: a meta-analytical approach using diffusion decision modeling. Cognitive




L
Fontanesi






S
Palminteri






M
Lebreton








Affective and Behavioral Neuroscience




19


3


















10.3758/s13415-019-00723-1
















B
U
Forstmann






R
Ratcliff






E.-J
J
Wagenmakers




Sequential Sampling Models in Cognitive Neuroscience: Advantages, Applications, and Extensions






















Annual Review of Psychology




67


1
















10.1146/annurev-psych-122414-033645














Striatum and pre-SMA facilitate decision-making under time pressure




B
U
Forstmann






G
Dutilh






S
Brown






J
Neumann






D
Yves Von Cramon






K
R
Ridderinkhof






E.-J
Wagenmakers










Proceedings of the National Academy of Sciences




105


45
















FMRI and EEG predictors of dynamic decision parameters during human reinforcement learning




M
J
Frank






C
Gagne






E
Nyhus






S
Masters






T
V
Wiecki






J
F
Cavanagh






D
Badre








Journal of Neuroscience




35


2


















10.1523/JNEUROSCI.2036-14.2015














By carrot or by stick: Cognitive reinforcement learning in Parkinsonism




M
J
Frank






L
C
Seeberger






R
C
Reilly








Science




306


5703


















10.1126/science.1102941














Variable Selection Via Gibbs Sampling




E
I
George






R
E
Mcculloch








Journal of the American Statistical Association




88


423
















Default prior distributions and efficient posterior computation in Bayesian factor analysis




J
Ghosh






D
B
Dunson




10.1198/jcgs.2009.07145








Journal of Computational and Graphical Statistics




18


2
















States versus rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning




J
Gläscher






N
Daw






P
Dayan






J
P
Doherty








Neuron




66


4


















10.1016/j.neuron.2010.04.016














How Computational Modeling Can Force Theory Building in Psychological Science




O
Guest






A
E
Martin




10.1177/1745691620970585








Perspectives on Psychological Science




16


4
















New estimation approaches for the hierarchical Linear Ballistic Accumulator model




D
Gunawan






G
E
Hawkins






M
N
Tran






R
Kohn






S
D
Brown




10.1016/j.jmp.2020.102368








Journal of Mathematical Psychology




96














Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making




G
E
Hawkins






B
U
Forstmann






E
J
Wagenmakers






R
Ratcliff






S
D
Brown








Journal of Neuroscience




35


6


















10.1523/JNEUROSCI.2410-14.2015














Linear deterministic accumulator models of simple choice




A
Heathcote






J
Love




10.3389/fpsyg.2012.00292








Frontiers in Psychology




3














Slow and steady? Strategic adjustments in response caution are moderately reliable and correlate across tasks




C
Hedge






S
Vivian-Griffiths






G
Powell






A
Bompas






P
Sumner








Consciousness and Cognition




75


















10.1016/j.concog.2019.102797














The Simon effect as tool and heuristic




B
Hommel




10.1016/j.actpsy.2010.04.011








Acta Psychologica




136


2
















Monetary reward increases attentional effort in the flanker task




R
Hübner






J
Schlösser








Psychonomic Bulletin and Review




17


6


















10.3758/PBR.17.6.821














Using group level factor models to resolve high dimensionality in model-based sampling




R
J
Innes






N
Stevenson






Q
F
Gronau






S
Miletić






A
Heathcote






B
U
Forstmann






S
D
Brown






I
Or




10.31234/osf.io/pn3wv


















Intra-Individual Networks of Response Inhibition




S
J S
Isherwood






P
L
Bazin






S
Miletić






A
C
Trutti






D
H Y
Tse






N
R
Stevenson






A
Heathcote






D
Matzke






R
J
Innes






S
Habli






D
R
Sokolowski






P
E
Goa






A
Alkemade






A
K
Håberg






B
U
Forstmann












and Interference Resolution at 7T. Under Review








Baseline-dependent effect of dopamine's precursor L-tyrosine on working memory gating but not updating




B
J
Jongkees




10.3758/s13415-020-00783-8








Cognitive, Affective and Behavioral Neuroscience




20


3
















A regularization method for linking brain and behavior




I
Kang






W
Yi






B
M
Turner




10.1037/met0000387








Psychological Methods
















Bayes factors




R
E
Kass






A
E
Raftery




10.1080/01621459.1995.10476572








Journal of the American Statistical Association




90


430
















Diffusion modeling and intelligence: Drift rates show both domain-general and domain-specific relations with intelligence




V
Lerche






M
Von Krause






A
Voss






G
T
Frischkorn






A
L
Schubert






D
Hagemann




10.1037/xge0000774








Journal of Experimental Psychology: General




149


12
















Retest reliability of the parameters of the Ratcliff diffusion model




V
Lerche






A
Voss








Psychological Research




81


3


















10.1007/s00426-016-0770-5














A Theory of Decision Control in Event-Based Prospective Memory




Luke
Strickland






Shayne
Loft






Roger
W
Remington






Andrew
Heathcote




10.1037/rev0000113.supp








Psychological Review
















From understanding computation to understanding neural circuitry




D
Marr






T
Poggio








Neurosciences Research Program Bulletin




15
















A Bayesian approach for estimating the probability of trigger failures in the stop-signal paradigm




D
Matzke






J
Love






A
Heathcote




10.3758/s13428-015-0695-8








Behavior Research Methods




49


1
















Bayesian inference for correlations in the presence of measurement error and estimation uncertainty




D
Matzke






A
Ly






R
Selker






W
D
Weeda






B
Scheibehenne






M
D
Lee






E
J
Wagenmakers




10.1525/collabra.78








Collabra: Psychology




3


1
















Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning




S
D
Mcdougle






A
G
Collins








Psychonomic Bulletin and Review


















fMRI protocol optimization for simultaneously studying small subcortical and cortical areas at 7 T. NeuroImage, 219




S
Miletić






P
L
Bazin






N
Weiskopf






W
Van Der Zwaag






B
U
Forstmann






R
Trampel




10.1016/j.neuroimage.2020.116992


















Mutual benefits: Combining reinforcement learning with sequential sampling models




S
Miletić






R
J
Boag






B
U
Forstmann




10.1016/j.neuropsychologia.2019.107261








Neuropsychologia




136














A new model of decision processing in instrumental learning tasks. eLife




S
Miletić






R
J
Boag






A
C
Trutti






N
Stevenson






B
U
Forstmann






A
Heathcote




10.7554/elife.63055








10












Caution in decision-making under time pressure is mediated by timing ability




S
Miletić






L
Van Maanen








Cognitive Psychology




110


















10.1016/j.cogpsych.2019.01.002














Reward representations and reward-related learning in the human brain: Insights from neuroimaging




J
P
O'doherty




10.1016/j.conb.2004.10.016


















The drift diffusion model as the choice rule in reinforcement learning




M
L
Pedersen






M
J
Frank






G
Biele








Psychonomic Bulletin and Review




24


4


















10.3758/s13423-016-1199-y














Neural Oscillations and Synchronization Differentially Support Evidence Accumulation in Perceptual and Value-Based Decision Making




R
Polanía






I
Krajbich






M
Grueschow






C
C
Ruff








Neuron




82


3


















10.1016/j.neuron.2014.03.014














Computational Modeling Applied to the Dot-Probe Task Yields Improved Reliability and Mechanistic Insights




R
B
Price






V
Brown






G
J
Siegle




10.1016/j.biopsych.2018.09.022








Biological Psychiatry




85


7
















Analogous computations in working memory input, output and motor gating: Electrophysiological and computational modeling evidence




R
Rac-Lubashevsky






M
J
Frank








PLoS Computational Biology




6


17
















10.1371/journal.pcbi.1008971














Dissociating working memory updating and automatic updating: The reference-back paradigm




R
Rac-Lubashevsky






Y
Kessler








Journal of Experimental Psychology: Learning Memory and Cognition




42


6


















10.1037/xlm0000219














Decomposing the n-back task: An individual differences study using the reference-back paradigm




R
Rac-Lubashevsky






Y
Kessler




10.1016/j.neuropsychologia.2016.07.013








Neuropsychologia




90
















The hare and the tortoise: Emphasizing speed can change the evidence used to make decisions




B
Rae






A
Heathcote






C
Donkin






L
Averell






S
Brown




10.1037/a0036801








Journal of Experimental Psychology: Learning Memory and Cognition




40


5
















Individual differences and fitting methods for the two-choice diffusion model of decision making. Decision




R
Ratcliff






R
Childers








2














10.1007/978-3-319-04879-6{\_}3














Modeling Response Times for Two-Choice Decisions




R
Ratcliff






J
N
Rouder




10.1111/1467-9280.00067








Psychological Science




9


5
















A Comparison of Sequential Sampling Models for Two-Choice Reaction Time




R
Ratcliff






P
L
Smith








Psychological Review




111


2


















10.1037/0033-295X.111.2.333














Diffusion Decision Model: Current Issues and History




R
Ratcliff






P
L
Smith






S
D
Brown






G
Mckoon








Trends in Cognitive Sciences




20


4


















10.1016/j.tics.2016.01.007














Why Most Studies of Individual Differences With Inhibition Tasks Are Bound To Fail




J
N
Rouder






K
Aakriti






J
M
Haaf








PsyArXiv




4


5
















An introduction to Bayesian hierarchical models with an application in the theory of signal detection




J
N
Rouder






J
Lu








Psychonomic Bulletin & Review




12


4
















Using Bayesian hierarchical parameter estimation to assess the generalizability of cognitive models of choice




B
Scheibehenne






T
Pachur




10.3758/s13423-014-0684-4








Psychonomic Bulletin and Review




22


2




















F
Schmiedek






K
Oberauer






O
Wilhelm






H
M
Süß






W
W
Wittmann


















Individual Differences in Components of Reaction Time Distributions and Their Relations to Working Memory and Intelligence


10.1037/0096-3445.136.3.414








Journal of Experimental Psychology: General




136


3














Trait characteristics of diffusion model parameters




A
L
Schubert






G
T
Frischkorn






D
Hagemann






A
Voss








Journal of Intelligence




4


3


















10.3390/jintelligence4030007


















Y
Schulz-Zhecheva






M
C
Voelkle






A
Beauducel






M
Biscaldi






C
Klein


















Predicting fluid intelligence by components of reaction time distributions from simple choice reaction time tasks






Journal of Intelligence




4


3
















10.3390/jintelligence4030008














Combining error-driven models of associative learning with evidence accumulation models of decision-making




D
K
Sewell






H
K
Jach






R
J
Boag






C
A
Van Heer








Psychonomic Bulletin and Review




26


3


















10.3758/s13423-019-01570-4














Modulation of premotor and primary motor cortical activity during volitional adjustments of speed-accuracy trade-offs




D
Thura






P
Cisek




10.1523/JNEUROSCI.2230-15.2016








Journal of Neuroscience




36


3
















Sequential sampling models without random between-trial variability: the racing diffusion model of speeded decision making




G
Tillman






T
Van Zandt






G
D
Logan








Psychonomic Bulletin and Review




















10.3758/s13423-020-01719-6














Robustly estimating the marginal likelihood for cognitive models via importance sampling




M
N
Tran






M
Scharth






D
Gunawan






R
Kohn






S
D
Brown






G
E
Hawkins








Behavior Research Methods




53


3


















10.3758/s13428-020-01348-w














Urgency, leakage, and the relative nature of information processing in decision-making




J
S
Trueblood






A
Heathcote






N
J
Evans






W
R
Holmes




10.1037/rev0000255








Psychological Review




128


1
















Approaches to Analysis in Model-based




B
M
Turner






B
U
Forstmann






B
C
Love






T
J
Palmeri






L
Van Maanen




10.1016/j.jmp.2016.01.001.Approaches








Cognitive Neuroscience. Mathematical Psychology




76


B
















A Bayesian framework for simultaneously modeling neural and behavioral data




B
M
Turner






B
U
Forstmann






E
J
Wagenmakers






S
D
Brown






P
B
Sederberg






M
Steyvers








NeuroImage




72


















10.1016/j.neuroimage.2013.01.048














Factor analysis linking functions for simultaneously modeling neural and behavioral data




B
M
Turner






T
Wang






E
C
Merkle




10.1016/j.neuroimage.2017.03.044








NeuroImage




153
















The interpretation of behavior-model correlations in unidentified cognitive models




L
Van Maanen






S
Miletić








Psychonomic Bulletin and Review


















10.3758/s13423-020-01783-y














Accumulating Advantages: A New Conceptualization of Rapid Multiple Choice




D
Van Ravenzwaaij






S
D
Brown






A
A
Marley






A
Heathcote




10.1037/rev0000166








Psychological Review




















F
Verbruggen






A
R
Aron






G
P
Band






C
Beste






P
G
Bissett






A
T
Brockett






J
W
Brown






S
R
Chamberlain






C
D
Chambers






H
Colonius






L
S
Colzato






B
D
Corneil






J
P
Coxon






A
Dupuis






D
M
Eagle






H
Garavan






I
Greenhouse














A consensus guide to capturing the ability to inhibit actions and impulsive behaviors in the stop-signal task




A
Heathcote






R
J
Huster






.
.
Boehler






C
N




10.7554/eLife.46323






















L
Wall






D
Gunawan






S
D
Brown






M
N
Tran






R
Kohn






G
E
Hawkins


















Identifying relationships between cognitive processes across tasks, contexts, and time






Behavior Research Methods




53


1
















10.3758/s13428-020-01405-4














Cognitive efficiency beats top-down control as a reliable individual difference dimension relevant to self-control




A
Weigard






D
A
Clark






C
Sripada




















10.1016/j.cognition.2021.104818








Cognition




215














Task-General Efficiency of Evidence Accumulation as a Computationally Defined Neurocognitive Trait: Implications for Clinical Neuroscience




A
Weigard






C
Sripada








Biological Psychiatry Global Open Science




1


1


















10.1016/j.bpsgos.2021.02.001














Individual differences in visual word recognition: Insights from the English Lexicon Project




M
J
Yap






D
A
Balota






D
E
Sibley






R
Ratcliff








Journal of Experimental Psychology: Human Perception and Performance




38


1


















10.1037/a0024177














Response times from ensembles of accumulators




B
Zandbelt






B
A
Purcell






T
J
Palmeri






G
D
Logan






J
D
Schall








Proceedings of the National Academy of Sciences of the United States of America




111


7


















10.1073/pnas.1310577111















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]