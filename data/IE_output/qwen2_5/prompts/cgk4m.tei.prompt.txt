You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
In recent years, the use of artificial intelligence (AI) systems has proliferated. AI now routinely assists humans in tasks that require fast and efficient analysis of 'big data' toward prediction of future outcomes. Machine-learning algorithms that crunch vast amounts of data promise to improve our quality of life by quickly 'learning' people's preferences and habitual decision patterns-though, unfortunately, also their biases (e.g. 
Eubanks, 2018)
. Although much attention is currently given to large language models and generative AI (e.g. ChatGPT, Claude, Gemini, Llama), the vast majority of currently available AI technologies consists in so-called decision support systems (DSSs), meant to improve the efficiency of human decision-making by providing them with (ideally) relevant information about viable alternatives in varied domainsfrom medical diagnostics, insurance, and finance to the criminal justice system. Crunching such vast amounts of data would have otherwise taken a human a long time-perhaps even many lifetimes. Here we focus on these DSSs.
However, as was made abundantly clear during the Covid-19 pandemic, when many rejected the protective and sometimes life-saving vaccines that were developed at record speed and high cost via novel technology, even the most efficient and useful technology will only help humans who are willing to engage with it. Similarly, in the case of AI, these new and powerful technologies can only bring about their benefits if a critical mass of people are interested in using them. We argue that, to fulfill their 'mission' of helping humans make better decisions (and thus increasing human productivity) in various contexts, including high-stakes ones, DSSs must have the ability to provide explanations for their outputs that play the role of reasons when communicating these outputs to human users 4 . After we clarify what we mean by "reasons" and motivate our claim that providing reasons for recommendations is important in the context of productive collaboration between humans and DSSs, we will discuss two possible objections to our proposal. In replying to the objections, we will further defend our suggestion that a DSS capable of providing reasons for its outputs has the potential to increase users' willingness to utilize the system as a decision-making aid. We leave a systematic empirical investigation of our suggestion to future work. Instead, we set ourselves a more modest goal here: to make the case for work in explainable AI (XAI) and human-computer interaction to incorporate more explicitly the idea of reasons (alongside other kinds of explainability-enhancing strategies) and for XAI researchers to take on the challenge of developing strategies to provide their models with reasongiving capabilities.


Explainable AI and the challenge of intelligibility for users
As DSSs become more powerful, a key obstacle that remains for their integration into human decision-making is that state-of-the-art models employ deep neural networks, which are notoriously 'black boxes' 
(Guidotti et al., 2018b;
Liang et al., 2022;
von Eschenbach, 2021;
Weld and Bansal, 2019)
. In other words, such networks are highly interconnected and complex, containing up to billions of parameters to train. Therefore, given a trained and working DSS, it is typically very difficult, if not impossible, for humans to completely understand how the system 'decided' to issue a specific output given a certain input by tracking the many layers and myriad connections leading from the input to the output. Though, importantly, such understanding is critical when the purpose is to utilize the machine to make good, evidence-based decisions.
By way of an example, imagine a medical DSS trained to analyze patients' charts and predict whether a patient is a good candidate for a certain surgery. A typical DSS would likely output a label (e.g., "surgery recommended" or "surgery not recommended") and a degree of confidence that the label is correct (e.g., 72%). Now let us further imagine a surgeon that independently examines the chart and concludes that the patient is not a good candidate for that surgery. This surgeon then goes on to consult with the DSS (say, as a second opinion), only to get the output "surgery recommended" with 72% confidence and no additional explanation.
Without additional information-such as the specific features of the patient that drove the decision (e.g. age, sex, medical history, etc.) and how those features relate specifically to the surgery being considered-the surgeon may well be disinclined to seriously consider the DSS's output as a legitimate 'opinion' worth considering. So, perhaps after another look at the chart, they may decide that the DSS's recommendation is wrong and be less inclined to consult it in the future. The situation might become even worse if a patient were exposed to the conflicting diagnoses of their surgeon ("surgery not recommended") and the DSS ("surgery recommended", 72% confidence). The patient is then likely to lower their trust in both diagnoses, resulting in negative repercussions for the perceived competence of the surgeon.
A well-built and well-trained DSS is likely to have been trained or huge amounts of relevant data and to have relied on the patient's demographics, medical history, and much more for its diagnosis and confidence score. But if the AI cannot relay how that information and that knowledge has led it to its diagnosis in any particular case, the user's willingness to trust the DSS's diagnosis and use it again may greatly diminish.
To make up for this lack of intelligibility, researchers have tried to fashion DSS with features to make the machine's outputs and internal processes more explainable to humans. XAI can take many forms, especially since the very meaning of the term seems to vary across research fields, depending on the type of AI architecture and overall explanatory goals 
(Adadi and Berrada, 2018;
Doran et al., 2018;
Gunning and Aha, 2019;
Kasirzadeh, 2021;
Langer et al., 2021;
Vilone and Longo, 2021)
. To some, explainability in AI coincides with interpretability (sometimes also known as 'transparency'), that is, a way for people who are familiar with the technical details of the model to figure out how an output was derived from an input. This type of explainability is importantly different from a less detail-oriented and more user-centered one, which might be termed 'comprehensibility' 
(Doran et al., 2018)
. A comprehensible system issues its output together with additional information (keywords, visualizations, etc.) which, when properly interpreted by a human, reveal how a certain input was transformed into a certain output. 
5
 Although the latter kind of explainability does not require a lot of technical knowledge about the inner workings of the model, in current systems it still requires a human interpreter-in particular, their reasoning abilities and background knowledge-for the actual explanation to be explicitly articulated. The additional information issued by such an explainable machine might thus be likened to a cartographic map: it contains an abundance of details, but humans still must know how to read the map and use it to navigate the world for it to fulfill its purpose.
If the purpose is for end-users-especially those with no technical expertise in machine learning-to obtain intelligible, valuable, and pertinent information to support their decisionmaking processes, especially in potentially high-stakes situations (e.g., doctors in medical diagnostics), are interpretability and comprehensibility as defined above enough? Do interpretability and comprehensibility sufficiently encourage non-expert users to integrate machine recommendations into their deliberations? For both questions, we think that the answer is 'No'. Interpretability and comprehensibility as described above are both passive notions, that is, they focus on providing the user with additional information about the machine under the assumption that the user already knows how to use that information to make decisions. In other words, interpretability and comprehensibility are not designed to capture the essentially interactive nature of human decision-making and tool use (see below)-both key components of the phenomenon we are interested in, i.e., decision-making informed by artificially intelligent tools.
Humans make the majority of their decisions either directly or indirectly for the sake of coordinating their behavior with other humans, achieving common goals, planning joint actions, etc. The social context greatly influences, both ontogenetically and phylogenetically, preferences and values, which in turn are what guides and motivates decisions. Additionally, humans learn to use tools largely in social contexts and through cooperation 
(Sterelny, 2012)
. If DSSs are to become tools capable of enhancing the quality of human decision-making writ large, they must be designed to 'fit' with what we already know about how and why humans make decisions as smoothly as possible. Therefore, explainable decision support systems should be designed with particular attention to the social and interactive aspects of human decision-making and, relatedly, to how humans explain their actions to other humans 
(De Graaf and Malle, 2017;
Lombrozo, 2016;
Lombrozo and Carey, 2006;
Miller, 2019)
. But what are these aspects, exactly?
Empirical evidence seems to suggest that users prefer interactive AI systems, which can directly answer questions and elaborate on their suggestions on-demand over more passive systems that simply lay out information 
(Kunkel et al., 2019;
Lim et al., 2009a)
. Moreover, several studies in human-computer interaction and related fields suggest that humans tend to adopt cognitive attitudes similar to those underlying interactions with other humans or animals when interacting with advanced machines 
(Abercrombie et al., 2023;
Chaves and Gerosa, 2021;
Nass and Moon, 2000;
Rapp et al., 2021;
Rosenthal-von der PÃ¼tten et al., 2013)
. It has also been argued that the 'perceived humanness' of an artificial agent positively correlates with measures of user satisfaction, trust, engagement, etc. 
(Christoforakos et al., 2021;
Go and Sundar, 2019;
Lu et al., 2022;
Rapp et al., 2021;
Rheu et al., 2021;
Toader et al., 2020)
. Thus, for example, increasing the interactivity of explanations to better resemble human-to-human interaction, at least to an extent, could be beneficial to users' willingness to incorporate AI in their decision-making


processes. 6
As a second example, consider a DSS designed to give personalized insulin intake recommendations to patients with type-1 diabetes 
(Buch et al., 2018)
. Van der Waa and colleagues 
(Van Der Waa et al., 2021)
 found that such a system's ability to articulate the 'rule' it used to come up with particular predictions in an easily understandable way (even at the expense of precision with respect to technical details) had a positive effect on users' self-reported understanding of the system and significantly improved users' ability to identify which factors 
6
 In this respect, for instance, it is not surprising that AI systems trained on human-produced text or speech which can directly answer questions and can have prolonged 'conversations' with users (e.g. OpenAI's ChatGPT or Google's Gemini) have become one of the most popular ways to conduct internet searches (which these days are the starting point for a lot of decisions). Humans value the possibility to refine their queries, ask for more detail, and, more generally, 'personalize' their information-gathering practices so that decisions can be made as quickly and seamlessly as possible.
most influenced a specific recommendation. These results are consistent with what studies on earlier-generation AI recommenders found 
(Stumpf et al., 2009)
. Namely, that 'rule-based' explanations positively affect users' willingness to contribute feedback to improve the performance of the system, suggesting that 'rule-based' explanations are considered conducive to productive human-machine collaboration. Moreover, users seeking to make real-life decisions with the help of AI models might not have much use for the exact, technical details regarding how the model transforms an input into an output. In other words, the specific values of the 'weights' (the connections between the nodes of the neural network, which are the typical parameters on which the learning of modern AI models depends) would be of little use for the average surgeon when deciding whether to agree with a system that just told them their patient should undergo surgery with 72% confidence. Indeed, such fine-grained details are usually quite removed from the user's goals and desiderata, which mostly focus on practical usefulness, on maintaining autonomy and control over the machine, and on understanding how the machine's overall objective (i.e. maximizing prediction accuracy) relates to the specifics of the context of application 
(Herlocker et al., 2000;
Kulesza et al., 2013;
Langer et al., 2021;
Lim et al., 2009b)
.
So: assuming that the goal of DSSs like our fictional 'surgery predictor' or the diabetes monitoring system is to aid, rather than completely replace, human decision-makers, what is it about certain explanation 'styles', and in particular rule-based explanations, that makes them suitable to support and encourage the use of DSSs? Our answer draws on insights from philosophy of mind and the philosophy of action: explainable DSSs should be able to provide users with reasons for their recommendations, since reason-giving is a central aspect of how humans develop an understanding of a decision problem, which is in turn a necessary step towards finding a solution, that is, making the decision. We also argue that rule-based explanations are particularly suitable to be interpreted as reasons by users, thus supporting efforts in XAI that focus on this 
(Guidotti et al., 2018a;
Rudin and Shaposhnik, 2023)
.


Cooperation, rules, and reasons
In order to integrate the machine's recommendations into our decision-making processes, that is, to appropriately cooperate with the machine during deliberation, we must understand (i) why the machine issued that specific recommendation in a particular instance, e.g., why it issues the label "surgery recommended" with 72% confidence, (ii) what the machine is generally designed to do (i.e., its 'objective function'), and (iii) what it is 'trying' to do in the current context of application (e.g. predict whether a patient is a good candidate for a certain surgery based on their current and past relevant medical conditions). Only with this information in place can a user-a surgeon and even more so a patient-properly consider the machine's output as a possible decision alternative and determine whether they agree with it or not. In other words, an AI system that can properly serve as a decision-making aid for humans must display what Zhou and Danks 
(Zhou and Danks, 2020)
 call "function-based intelligibility". They argue that, despite its inability to "answer all possible why-questions, particularly those that focus on the internal operations of the system," this type of intelligibility is the most useful to users in the context of cooperative tasks (like the type in which AI can help with making decisions), as it "provides 
[users]
 with exactly the abilities that they require to efficiently integrate, appropriately use, and accurately interpret the performance of the autonomous system so that they can reach their goals" (p. 197).
And explanations that afford function-based intelligibility do so in virtue of playing the role of reasons, or so we argue. First, notice that the 'reasoning' behind specific outputs, which is what function-based intelligibility is all about, can be also understood as the set of 'rules' or 'principles' the machine used to come up with that specific recommendation (think about how a mathematical proof consists of several inferential steps based on the application of mathematical 'rules', i.e. axioms and theorems). So, an explanation that affords function-based intelligibility is an explanation that redescribes a certain input-output relation as the instantiation of a (likely simplified and idealized) rule. 7
Philosophers like 
Wittgenstein (1953)
 and 
Sellars (1954)
 have argued that describing a behavior as the instantiation of a rule is a way of describing the behavior as if it were an intentional action, and to represent it as an instance of a more general and reproducible action-type. By way of a simplified example, one of the rules of basketball is that players must dribble the ball whenever they want to move around with it. Through the rule, a new action-type is codifiedi.e., the action-type "moving around dribbling the ball"-together with a series of normative criteria to assess whether a particular action is a token of that type and thus a concrete instantiation of the rule. Now notice that, in the appropriate context, mentioning the rule an action instantiates can be considered an acceptable explanation for the action, that is, an appropriate answer to the question "Why?". To illustrate, let's keep using the same basketball example (while however keeping in mind that it is a highly artificial one, which differs from real life in important ways).
If during a game of basketball someone were to ask you "why are you moving around dribbling the ball?", "because one of the rules of basketball is that players must dribble the ball whenever they want to move around with it" would be a legitimate answer, serving at the same time as an articulation of the rule and an explanation of your action. Sellars distinguishes between merely 7 State-of-the-art AI systems are now typically based on artificial neural networks and therefore do not have rules explicitly programmed into them. However, it is still the case that, during training, a neural net 'comes up' with some kind of systematic ways of representing information-learning rules from the training data. Because these representations are distributed all the way throughout the network, they are often too complex and convoluted to be clearly interpreted by humans as a small set of rules. Nevertheless, they are, for all intents and purposes, rules.
conforming to a rule and following a rule 
(Sellars, 1954)
, and argues that the latter requires an agent to have the explicit intention of following the rule. If one describes one's action in terms of the rule that codifies the corresponding action-type, this in itself is proof that one either performed that action intentionally, or is at least capable of performing that action intentionally, whether or not one did so in that particular case.
If we now apply this argument to the case of AI systems and the explanations of their recommendations, we can say that an explanation that describes what the machine is doing in intentional terms is basically articulating the (machine equivalent of the) intention with which the machine issues its outputs. Another philosopher, G. E. M. Anscombe 
(Anscombe, 2000)
, argued that intentions are special interpretations/descriptions of actions which capture the sense in which an action is done for a reason. As a consequence, to reveal the intention with which an action is done is equivalent to articulating the reason why the action is done. 8 By describing the machine's behavior in a particular case, i.e., a particular input-output relation, as the instantiation of a rule, therefore, an explanation of why the behavior occurred that can be interpreted by users as if the behavior were intentional, thus allowing the explanation to work as a reason. 9
We say "can work as reasons" instead of "are reasons" because, first, we do not want to commit to the claim that machines genuinely have intentions just yet, though we leave open the possibility that future empirical and philosophical investigations into AI behavior will establish that machines can (and perhaps do) have intentions. For our present purposes, it is enough that humans act as if machines have intentions for the purpose of explaining the machine's behavior in a way that allow us to rely on familiar 'mental' concepts 
(Dennett, 1987)
. Second, the very notion of a reason denotes a functional role (more precisely, several different functional roles 10 ) rather than a monadic property, that is, explanations do not possess, strictly speaking, the property of being reasons, even when issued by humans. Rather, explanations with certain characteristics may or may not be suitable to fulfil the functional role of a reason; a role which, in turn, exists only within the specific human social practice of giving and asking for reasons 
(Brandom, 2000
(Brandom, , 1995
.
In the next section, we discuss two potential objections to the idea that AI systems should have the ability to issue explanations that can play the role of reasons. Replying to these objections will give us the opportunity to elaborate on why exactly we think that having AI explanations play the role of reasons is the best way to encourage the use of AI systems as decision-making aids by people with no technical machine learning expertise.


Objections and Replies
The first objection we consider states that since AI systems don't really have intentions and don't really act for reasons, making them issue explanations that play the role of reasons might lead to misplaced trust and might thus mislead users, especially those with little technical knowledge about AI, regarding the actual cognitive abilities of these systems. The risk is that users will anthropomorphize the AI system and will thus be 'morally confused' about how to properly interact with it (e.g. 
Schwitzgebel, 2023;
Turkle, 2024)
. And if so, our proposal would be in explicit tension with one of our main motivations for writing this paper -namely, to find ways to convince more and more people with no machine-learning expertise to use AI systems.
These broadly ethical worries are legitimate, and other work by one of the authors of this paper explicitly considers them (citation deleted). However, it should be noted that very often humans don't have a way to check whether the reasons provided by another human as explanations of why they did something are correct, or whether the action really was done for those reasons, or whether it was done for some reason at all. Nevertheless, it is a well-established idea in organizational psychology that the mere availability of a reason-like explanation for a behavior has a positive effect on at least initial willingness to engage in some cooperative task (e.g. 
Edgar et al., 2021;
Jonker et al., 2010;
Mathieu et al., 2000)
. And a willingness to engage is the necessary first step: without it, any discussion about trust, whether misplaced or not, would be highly premature.
Moreover, we are not discussing a context in which a human completely delegates and gives up control over a decision. And because the cases we are interested in here are not cases in which the human lets the machine decide on its own, but rather the human uses the machine as an 'advisor', a 'second opinion' that ensures a more rigorous deliberation process, excessive anthropomorphization does not seem as worrisome or ethically problematic. A more balanced attitude towards the anthropomorphization and consequent misplaced trust worry, perhaps, would be to see it as a bullet that must be bitten in order to make machines less alien or intimidating to non-experts. Indeed, anthropomorphization allows users to rely on familiar cognitive attitudes and communicative patterns -though we should be aware of the possibility of an 'uncanny valley effect' 
(Cheetham et al., 2015;
Ciechanowski et al., 2019;
Mori et al., 2012)
, especially with systems equipped with a conversational interface powered by LLMs.
Findings from an online survey on trust and explainability in machine learning (N=100 participants) run by our group on the Prolific platform also suggest that non-expert users see the ability to provide reasons as conducive to trust regardless of whether the reasons provided by the machine indeed accurately track the low-level algorithmic details. In the survey, people with little to no technical expertise were asked whether they agreed with the claim 'In order for me to trust an AI, that AI must be able to explain why it makes decisions'. On a Likert scale from 1 (Strongly disagree) to 5 (Strongly agree), the average answer was 3.72, which is significantly higher than the 'neutral' answer (3 on a scale of 1 to 5). 11 Similarly, participants seemed to reliably agree with the claim 'If I do not know that an AI's internal algorithm is working properly, but that AI is able to explain why it makes decisions, I trust that AI' (average response on the Likert scale was 3.25). 12 Collectively, these two sample-questions from our survey suggest that being able to have at least some understanding of why the machine issues an output is generally perceived as having a positive effect on trust, thus potentially counteracting the negative effects of excessive anthropomorphization. Lastly, since the goal is for DSSs to assist humans in their decisions instead of making decisions autonomously, thus maintaining a 'human in the loop' at all times, reason-giving capabilities seem to be a good way to ensure that the human in question maintains accountability for a decision made with the assistance of a DSS by ensuring that the human understands why a certain output was issued 
(Baum et al., 2022)
.
The second objection we consider states that requiring explanations at the level of reasons prevents us from taking full advantage of AI tools' power, because it forces the AI to communicate with us at a higher level of abstraction, thus possibly lowering its accuracy in favor of more intelligibility for humans 13 . For instance, it is easy to imagine a system being 'forced' to take into account fewer variables so as to make the post-hoc explanation easier to understand, or altogether producing 'made up' explanations that do not accurately reflect the system's functioning but still 'sound plausible'. These situations are not ideal, as they would clearly diminish the quality of the recommendations. The latter situation in particular is representative of a more general and widely discussed problem vexing many AI systems (and especially generative AI systems), known as the problem of 'machine hallucinations', which is often mentioned as a core ethical issue that should discourage users from relying on AI systems to make important decisions, since these systems may have been trained on false or misleading information that then leads them to issue erroneous outputs (e.g. 
Farquhar et al., 2024;
Ghassemi and Nsoesie, 2022;
Noble, 2018;
Rawte et al., 2023)
. Therefore, the objection goes, requiring AI systems to have reason-giving capabilities cannot possibly be the best way to encourage nonexperts to utilize these systems as decision-making aids.
This objection echoes what some critics have been saying about the role of even lowerlevel forms of AI explainability, such as interpretability/transparency, not just reason-giving 
(Humphreys, 2004;
Lipton, 2018;
Robbins, 2019)
. These critics emphasize an apparent tension between two ideas. On the one hand is the idea, generally accepted even by the public, that AI systems do some things better and faster than humans. A chatbot like ChatGPT, for example, can generate a better-than-average college essay or write an OK-sounding reference letter starting from a random CV in a few seconds. Even less advanced systems, such as the DSS we are concerned with here, can still analyze a large dataset in a fraction of the time needed by a human while taking into account many more variables. It is quite easy to see what makes these technologies attractive in contexts in which time and fine-grained classifications are critical (e.g., medical diagnosis).
On the other hand, there is the idea, popular among computer ethicists and philosophers of technology, that AI systems should explain themselves to all 'stakeholders', not just expert engineers 
(Kasirzadeh, 2021;
Langer et al., 2021;
Zhou and Danks, 2020)
. Thus, many ask: How can we expect AI models to perform 'super-humanly' and provide us with novel insights, if they are forced to 'dumb things down' or even just translate things in terms and concepts familiar to humans all the time? If the purpose is to utilize AI for faster and better data analysis and prediction compared to what humans are capable of, wouldn't enhancing its non-expert human intelligibility essentially defeat this very purpose? Shouldn't we focus on creating AI tools that are so accurate and reliable that the need for explanations becomes less pressing?
We reply by pointing out that, though AI systems may well fall short of agency and may well not really have the mental states we usually associate with agency (see our reply to the first objection). their complexity makes them importantly different from other technologies to which the narrow notion of reliability and the (allegedly) consequent lack of need for explainability most clearly apply. AI systems, in other words, are not like humans (or many other non-human animals), but they are not like, e.g., toasters, either. If they were, the question of how to have humans most productively collaborate with AI systems in high-stakes decision-making contexts would not even arise. You probably do not collaborate with your toaster; at most, you consider it a reliable piece of equipment. And indeed you would probably not ask the toaster 'why?' if it malfunctions, neither would you expect it to answer you by disclosing its intentions and the adverse circumstances that prevented it from functioning correctly (e.g., you would not expect the toaster to explain that it really wanted to toast your bread, but that a short circuit in its heating element made toasting impossible). A toaster either works or it doesn't, it either toasts bread properly or it doesn't, and a failure to toast bread properly is a failure to be useful as the tool that it is. And since there is nothing else to it, that is, there is no other sense in which a toaster can be useful even in a case in which it malfunctions, reliability is enough, and explanations are in this context clearly superfluous.
But today's AI systems are not like toasters. Some of them (e.g., the experimental.
AutoGPT and likely more so its future instantiations) can plan their own course of action based on the user's requests and execute it largely autonomously by applying the 'knowledge' they acquired during training; and it seems likely that systems like that will become the norm soon enough. Moreover, even in cases in which the AI system's recommendation is considered wrong by the user, there is still something to be gained by investigating why it was wrong. Sometimes, by probing the system further through a request for explanation, users can discover that in fact the system was not wrong, thus convincing the user to take on the recommendation (cf. 
Baum et al., 2022)
. What it means for an AI system to 'malfunction' has additional layers to it which do not apply to toasters 14 , and that is why the ability to explain why an output was issued is a necessary component of what makes AI systems the tools that they are, and a necessary condition for their usefulness as tools, in science and beyond 
(Creel, 2020)
. Combining this with what we said previously about the kinds of explanations that are more likely to play a beneficial role in enhancing cooperation given how humans act and deliberate, we reach our desired conclusion: AI systems meant to be used as decision-making aids should be able to provide explanations that can play the role of reasons for their users.
14 One way to think about this claim is by analogy with the discussion around intentionality in the philosophy of mind, and in particular around 'naturalistic' theories of intentionality such as teleosemantics (e.g. 
Millikan, 2004
Millikan, , 1984
Neander, 2007)
. Although we can describe an intentional state with false or inaccurate content as a case of 'malfunctioning', it is still a contentful state which plays a cognitive role. Being sometimes inaccurate is part of the function of intentional states, while in non-intentional systems to perform a function inaccurately just is to fail to perform the function altogether. The functional complexity of AI systems makes it possible for us to take the 'intentional stance' 
(Dennett, 1987)
 on them, and, as others have argued, that is a key aspect of what makes them useful decision-making aids for humans 
(Zerilli et al., 2019)
.


Conclusion
In this paper, we argued that current XAI methods that aim at interpretability and comprehensibility alone are not sufficient for humans with no technical machine learning expertise to effectively use DSSs as decision-making aids. Explaining the technical features of the algorithm or creating map-like visualizations of the features that most prominently influenced a specific output are worth the effort, but an additional 'layer' of explainability is necessary if we want AI-supported decision-making to become widespread among non-experts.
Indeed, a non-expert user collaborating with a DSS to solve a decision-problem (e.g. a doctor who wants to 'consult' with a DSS for medical diagnosis) needs intelligible access to information about what the system is 'doing' in the particular case the user is interested in, that is, information about why the system issued the recommendation it did. To give users access to such information, we argued that AI systems should be able to provide explanations that function as reasons, that is, explanations that redescribe the system's input-output relation as an instance of an actiontype specified by a rule, which in turn is a way of understanding the system's behavior at least as if it were intentional.
Of course, treating something which is not an agent as if it were an intentional agent, who can act for reasons, can be dangerous and ethically problematic. However, we argued that the mere ability to rephrase explanations so that they can function as reasons is still compatible with users being very aware that the AI system is only a tool. While in some cases anthropomorphizing AI can generate misplaced trust, there is another facet to consider. Namely, users would be able to employ conceptual categories and cognitive attitudes they are already familiar with (i.e., those employed when collaborating with other humans), thus making using AI less intimidating and potentially more effective-again, keeping in mind the risk of 'uncanny valley effects' 
(Cheetham et al., 2015;
Ciechanowski et al., 2019;
Mori et al., 2012)
.
Finally, we also acknowledged the objection stating that, since AI is a tool, perhaps the goal should not even be thought of in terms of genuine collaboration, but rather more similarly to how we use toasters. This might be a good point if DSSs were like toasters, but they are importantly not. DSSs' behavior can productively be described as intentional and, even (and perhaps especially) when they are 'wrong', they still provide something relevant for the user to consider in their deliberations, even if only by making the possibility of disagreement salient 
(Baum et al., 2022)
. Thus, we think that making DSSs capable of issuing explanations that function as reasons is not a hindrance, but it should be seen as part of what these systems are currently designed to do, that is, instrumental to their success as tools-at least for now.
In conclusion, novel technologies can only be revolutionary if enough people 'buy in', and AI-powered DSSs are no different. If we want DSSs to become the potentially disruptive decisionmaking aid that tech companies have promised us, it is paramount that explainability is not only kept at the forefront of AI research agendas, but that it is also tailored specifically towards nonexpert users and their needs. Making DSSs able to issue reason-like explanations accomplishes just that.


Statements and Declarations
 
As clarified above, we focus here on DSSs giving reasons, not on generative AI. Some generative AI system, especially large language models (or LLMs), will output text that reads like reasons when prompted to do so by their users. However, it does not seem that this reason-like text output by LLMs plays the motivating role that we would expect reasons to play for the LLMs. We offer a detailed discussion of reasons below. But a full discussion whether the text output by generative AI should be viewed as reasons is beyond the scope of this manuscript.


For example, a tool like LIME (Local Interpretable Model-agnostic Explanations) when applied to any AI recommender system aims to issue keywords corresponding to the features the classifier considered most 'important' in driving its outputs
(Ribeiro et al., 2016)
.


It should be clarified that, for Anscombe, intentions are not the only things that can play the role of reasons. Motives, for example, can also be interpreted as reasons in certain contexts. Her famous example is someone answering the question "Why did you kill him?" with "He killed my father". In a sense, the latter seems to be an acceptable answer to the question, and the answer, again in a sense, explains why the action was carried out. However, motives importantly differ from intentions, and the different ways in which these two notions play the role of reasons is an important aspect of what grounds the distinction for Anscombe. 9 In the study by Stumpf and colleagues
(Stumpf et al., 2009)
, rule-based explanations are expressed through the phrase "the reason the system [issued prediction A] is because the rule wasâ€¦"(Ibid., 7). This suggests that, in most cases, the relationship between explanations that redescribe the machine's functioning in broadly intentional terms and reasons is assumed rather than justified. In this section, we provide some justification for this relationship based on how philosophers think about intentional actions and rules.


Sinnott-Armstrong (2022)
, for instance, distinguishes between epistemic, practical, and explanatory reasons as three different functional roles which, indeed, can be played by the very same proposition at the same time (like in his example of "Because I knew I would win a lot of money" as an answer to the question "Why did you bet so much on the game?" (p. 129)), but they need not be.


Two-sided paired t test against the neutral rating (i.e. '3' on the Likert scale), t = 7.09, p < 0.001. 12 Two-sided paired t test against the neutral rating, t = 2.39, p = 0.019.


A trade-off has been previously demonstrated between accuracy and interpretability in machine-learning models, see for example
Liang et al. (2022)
.








Funding
This work was supported by The John Templeton Foundation (grant #: 61283), the John E. Fetzer Institute (grant #: 4189.00), and Templeton World Charity Foundation (grant #: TWCF-2022-30259). The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation, Templeton World Charity Foundation, Inc., or the Fetzer Institute.


Competing Interests
The authors have no relevant financial or non-financial interests to disclose. 


Statement of AI use
Google Gemini was used to compose an early draft of the "Conclusion" section.
 










On Anthropomorphism in Dialogue Systems




G
Abercrombie






A
Curry






T
Dinkar






V
Rieser






Z
Talat




10.18653/v1/2023.emnlp-main.290








Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Presented at the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing


the 2023 Conference on Empirical Methods in Natural Language Processing. Presented at the the 2023 Conference on Empirical Methods in Natural Language Processing
Singapore




Association for Computational Linguistics










Mirages








Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)




A
Adadi






M
Berrada




10.1109/ACCESS.2018.2870052








IEEE Access




6
















Intention, Library of philosophy and logic




G
E M
Anscombe








Harvard University Press












From Responsibility to Reason-Giving Explainable Artificial Intelligence




K
Baum






S
Mantel






E
Schmidt






T
Speith




10.1007/s13347-022-00510-w








Philos. Technol




35














Articulating Reasons: An Introduction to Inferentialism




R
Brandom








Harvard University Press


Cambridge, MA












Knowledge and the Social Articulation of the Space of Reasons




R
Brandom




10.2307/2108339








Philosophy and Phenomenological Research




55














Artificial intelligence in diabetes care




V
Buch






G
Varughese






M
Maruthappu




10.1111/dme.13587








Diabet. Med




35
















How Should My Chatbot Interact? A Survey on Social Characteristics in Human-Chatbot Interaction Design




A
P
Chaves






M
A
Gerosa




10.1080/10447318.2020.1841438








International Journal of Human-Computer Interaction




37
















Arousal, valence, and the uncanny valley: psychophysiological and self-report findings




M
Cheetham






L
Wu






P
Pauli






L
Jancke




10.3389/fpsyg.2015.00981








Front. Psychol




6














Connect With Me. Exploring Influencing Factors in a Human-Technology Relationship Based on Regular Chatbot Use




L
Christoforakos






N
Feicht






S
Hinkofer






A
LÃ¶scher






S
F
Schlegl






S
Diefenbach




10.3389/fdgth.2021.689999








Front. Digit. Health




3














In the shades of the uncanny valley: An experimental study of human-chatbot interaction




L
Ciechanowski






A
Przegalinska






M
Magnuski






P
Gloor




10.1016/j.future.2018.01.055








Future Generation Computer Systems




92
















Transparency in complex computational systems




K
A
Creel




10.1086/709729








Philosophy of Science




87
















How people explain action (and autonomous intelligent systems should too)




M
M
De Graaf






B
F
Malle








Presented at the 2017 AAAI Fall Symposium Series
















The intentional stance




D
C
Dennett








MIT Press


Cambridge, MA, US












What does explainable AI really mean? A new conceptualization of perspectives




D
Doran






S
Schulz






T
R
Besold








CEUR Workshop Proceedings
















Better Decision-Making: Shared Mental Models and the Clinical Competency Committee




L
Edgar






M
D
Jones






B
Harsy






M
Passiment






K
E
Hauer




10.4300/JGME-D-20-00850.1








Journal of Graduate Medical Education




13
















Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor




V
Eubanks












St. Martin's Publishing Group












Detecting hallucinations in large language models using semantic entropy




S
Farquhar






J
Kossen






L
Kuhn






Y
Gal




10.1038/s41586-024-07421-0








Nature




630
















In medicine, how do we machine learn anything real? Patterns




M
Ghassemi






E
O
Nsoesie




10.1016/j.patter.2021.100392








3














Humanizing chatbots: The effects of visual, identity and conversational cues on humanness perceptions




E
Go






S
S
Sundar




10.1016/j.chb.2019.01.020








Computers in Human Behavior




97
















Local Rule-Based Explanations of Black Box Decision Systems




R
Guidotti






A
Monreale






S
Ruggieri






D
Pedreschi






F
Turini






F
Giannotti




10.48550/ARXIV.1805.10820


















A survey of methods for explaining black box models




R
Guidotti






A
Monreale






S
Ruggieri






F
Turini






F
Giannotti






D
Pedreschi




10.1145/3236009








ACM Computing Surveys




51














DARPA's Explainable Artificial Intelligence Program




D
Gunning






D
W
Aha




10.1609/aimag.v40i2.2850








AI Magazine




40
















Explaining collaborative filtering recommendations




J
Herlocker






J
E
Konstan






J
Riedl








Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work


the 2000 ACM Conference on Computer Supported Cooperative Work


















Extending Ourselves: Computational Science, Empiricism, and Scientific Method




P
Humphreys








Oxford University Press
















C
Jonker






M
Riemsdijk






B
Vermeulen












Shared Mental Models -A Conceptual Analysis








Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence




A
Kasirzadeh




10.1145/3442188.3445866








Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency


the 2021 ACM Conference on Fairness, Accountability, and Transparency






14












Too much, too little, or just right? Ways explanations impact end users' mental models




T
Kulesza






S
Stumpf






M
Burnett






S
Yang






I
Kwan






W.-K
Wong




10.1109/VLHCC.2013.6645235








2013 IEEE Symposium on Visual Languages and Human Centric Computing


















Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems




J
Kunkel






T
Donkers






L
Michael






C.-M
Barbu






J
Ziegler




10.1145/3290605.3300717








Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems


the 2019 CHI Conference on Human Factors in Computing Systems


















What do we want from Explainable Artificial Intelligence (XAI)?-A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research




M
Langer






D
Oster






T
Speith






H
Hermanns






L
KÃ¤stner






E
Schmidt






A
Sesing






K
Baum








Artificial Intelligence




296


103473














Examining the utility of nonlinear machine learning approaches versus linear regression for predicting body image outcomes: The U.S. Body Project I




D
Liang






D
A
Frederick






E
E
Lledo






N
Rosenfield






V
Berardi






E
Linstead






U
Maoz




10.1016/j.bodyim.2022.01.013








Body Image




41
















Why and why not explanations improve the intelligibility of context-aware intelligent systems




B
Y
Lim






A
K
Dey






D
Avrahami








Proceedings of the SIGCHI Conference on Human Factors in Computing Systems


the SIGCHI Conference on Human Factors in Computing Systems


















Why and why not explanations improve the intelligibility of context-aware intelligent systems




B
Y
Lim






A
K
Dey






D
Avrahami








Proceedings of the SIGCHI Conference on Human Factors in Computing Systems


the SIGCHI Conference on Human Factors in Computing Systems


















The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery




Z
C
Lipton








Queue




16
















Explanatory Preferences Shape Learning and Inference




T
Lombrozo




10.1016/j.tics.2016.08.001








Trends in Cognitive Sciences




20
















Functional explanation and the function of explanation




T
Lombrozo






S
Carey




10.1016/j.cognition.2004.12.009








Cognition




99
















Measuring consumer-perceived humanness of online organizational agents




L
Lu






C
Mcdonald






T
Kelleher






S
Lee






Y
J
Chung






S
Mueller






M
Vielledent






C
A
Yue




10.1016/j.chb.2021.107092








Computers in Human Behavior




128














The influence of shared mental models on team process and performance




J
E
Mathieu






T
S
Heffner






G
F
Goodwin






E
Salas






J
A
Cannon-Bowers








Journal of applied psychology




85


273














Explanation in artificial intelligence: Insights from the social sciences




T
Miller




10.1016/j.artint.2018.07.007








Artificial Intelligence




267
















Varieties of Meaning: The 2002 Jean Nicod Lectures




R
G
Millikan








The MIT Press


Cambridge, MA, US












Language, Thought, and Other Biological Categories: New Foundations for Realism




R
G
Millikan








The MIT Press


Cambridge, MA, US












The Uncanny Valley




M
Mori






K
F
Macdorman






N
Kageki




10.1109/MRA.2012.2192811








IEEE Robotics & Automation Magazine




19
















Machines and Mindlessness: Social Responses to Computers




C
Nass






Y
Moon




10.1111/0022-4537.00153








Journal of Social Issues




56
















Teleological Theories of Mental Content: Can Darwin Solve the Problem of Intentionality?




K
Neander




Ruse, M.






Oxford University Press






The Oxford Handbook of Philosophy of Biology








Algorithms of oppression: How search engines reinforce racism. xv




S
U
Noble








229






Algorithms of oppression: How search engines reinforce racism








The human side of human-chatbot interaction: A systematic literature review of ten years of research on text-based chatbots




A
Rapp






L
Curti






A
Boldi




10.1016/j.ijhcs.2021.102630








International Journal of Human-Computer Studies




151














A survey of hallucination in large foundation models




V
Rawte






A
Sheth






A
Das




arXiv:2309.05922










arXiv preprint








Systematic Review: Trust-Building Factors and Implications for Conversational Agent Design




M
Rheu






J
Y
Shin






W
Peng






J
Huh-Yoo




10.1080/10447318.2020.1807710








International Journal of Human-Computer Interaction




37
















Why Should I Trust You?": Explaining the Predictions of Any Classifier




M
T
Ribeiro






S
Singh






C
Guestrin




10.48550/ARXIV.1602.04938


















A Misdirected Principle with a Catch: Explicability for AI. Minds and Machines




S
Robbins




10.1007/s11023-019-09509-3








29














An Experimental Study on Emotional Reactions Towards a Robot




A
M
Rosenthal-Von Der PÃ¼tten






N
C
KrÃ¤mer






L
Hoffmann






S
Sobieraj






S
C
Eimler




10.1007/s12369-012-0173-8








Int J of Soc Robotics




5
















Globally-Consistent Rule-Based Summary-Explanations for Machine Learning Models: Application to Credit-Risk Evaluation




C
Rudin






Y
Shaposhnik








Journal of Machine Learning Research




24
















AI systems must not confuse users about their sentience or moral status. Patterns 4, 100818




E
Schwitzgebel




10.1016/j.patter.2023.100818


















Some Reflections on Language Games




W
Sellars








Philosophy of Science




21
















What are reasons?




W
Sinnott-Armstrong




10.1093/oso/9780197572153.003.0015








Free Will: Philosophers and Neuroscientists in Conversation


Maoz, U., Sinnott-Armstrong, W.




Oxford University Press
















The Evolved Apprentice: How Evolution Made Humans Unique




K
Sterelny




10.7551/mitpress/9780262016797.001.0001








The MIT Press












Interacting meaningfully with machine learning systems: Three experiments




S
Stumpf






V
Rajaram






L
Li






W
K
Wong






M
Burnett






T
Dietterich






E
Sullivan






J
Herlocker




10.1016/j.ijhcs.2009.03.004








International Journal of Human Computer Studies




67




















D.-C
Toader






G
Boca






R
Toader






M
MÄƒcelaru






C
Toader






D
Ighian






A
T
RÄƒdulescu




10.3390/su12010256








The Effect of Social Presence and Chatbot Errors on Trust. Sustainability




12














Who Do We Become When We Talk to Machines? An MIT Exploration of Generative AI




S
Turkle




10.21428/e4baedd9.caa10d84


















Evaluating XAI: A comparison of rulebased and example-based explanations




J
Van Der Waa






E
Nieuwburg






A
Cremers






M
Neerincx




10.1016/j.artint.2020.103404








Artificial Intelligence




291














Notions of explainability and evaluation approaches for explainable artificial intelligence




G
Vilone






L
Longo




10.1016/j.inffus.2021.05.009








Information Fusion




76
















Transparency and the Black Box Problem: Why We Do Not Trust AI




W
J
Von Eschenbach




10.1007/s13347-021-00477-0








Philosophy and Technology




34
















The challenge of crafting intelligible intelligence




D
S
Weld






G
Bansal




10.1145/3282486








Communications of the ACM




62
















Philosophical investigations. Philosophische Untersuchungen., Philosophical investigations. Philosophische Untersuchungen




L
Wittgenstein








Macmillan, Oxford, England












Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?




J
Zerilli






A
Knott






J
Maclaurin






C
Gavaghan




10.1007/s13347-018-0330-6








Philos. Technol




32
















2020. Different "intelligibility" for different folks




Y
Zhou






D
Danks




10.1145/3375627.3375810








AIES 2020 -Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]