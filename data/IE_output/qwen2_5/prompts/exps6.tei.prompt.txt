You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Two distinct and age-varying motivations may underlie peoples' valuation of agentic choice: its instrumental and intrinsic value. In controllable environments, in which choices influence experienced reward outcomes, agentic choice has instrumental value because making choices may yield greater reward than forgoing opportunities to choose 
(Katzman & Hartley, 2020;
Ly et al., 2019;
Moscarello & Hartley, 2017)
. Across real-world environments, the instrumental value of choice varies, and people must learn the extent to which their choices influence the rewards they experience. They can then leverage this knowledge to determine whether the benefits of agentic choice outweigh the potential time and effort costs of choosing 
(Boureau et al., 2015;
Shenhav et al., 2017;
Westbrook & Braver, 2015)
. Multiple aspects of these reward-learning and choice processes may change with age -children, adolescents, and adults may vary in the extent to which they update their beliefs about how rewarding an action is following wins and losses 
(Habicht et al., 2022;
Nussenbaum et al., 2022;
Rosenbaum et al., 2022)
, or following outcomes that were or were not elicited by their own choices 
(Cockburn et al., 2014)
, which may lead to systematic developmental differences in estimates of the value of agentic choice. In addition, the use of learned values to guide decisions about whether to seek or forgo agentic choice may require prospective simulation of potential outcomes, which itself may engage cognitive control and working memory processes that improve through adolescence 
(Luna et al., 2015)
. Thus, the influence of instrumental value on agentic choice may be supported by multiple cognitive processes that undergo marked change across development.
People may also seek opportunities to make choices because agentic choice is intrinsically rewarding, meaning choice is valued in and of itself beyond its efficacy in promoting the acquisition of reward 
(Blain & Sharot, 2021;
Bown et al., 2003;
Cockburn et al., 2014;
 In press: Psychological Science & Delgado, 2011; 
Leotti et al., 2010)
. Even when one's choices do not affect the rewards one experiences, opportunities to choose may still be valued because they enhance beliefs that one's actions have causal influence 
(Ly et al., 2019)
. Across age, people report greater feelings of wellbeing with higher levels of perceived control 
(Bandura et al., 2003;
Véronneau et al., 2005;
Weinberg et al., 1979;
Weinstein & Mermelstein, 2007)
 and prefer environments in which they have more opportunities to choose 
(Bown et al., 2003;
Katzman & Hartley, 2020;
Leotti & Delgado, 2011;
Sran & Borrero, 2010;
Suzuki, 1997)
, even when their choices do not influence the reward outcomes they experience.
The intrinsic value of choice may also vary across development, with prior research in related areas suggesting multiple potential patterns of age-related change. Across species, sensitivity to diverse types of rewards, including extrinsic reinforcers like money, sucrose, and drugs 
(Cauffman et al., 2010;
Doremus et al., 2005;
Galvan et al., 2006;
Galván & McGlennen, 2013;
Smith et al., 2012)
, and intrinsic, cognitive rewards, like novelty and social interaction 
(Douglas et al., 2003
(Douglas et al., , 2004
, varies non-linearly with age, with the greatest sensitivity often occurring in adolescence. The intrinsic reward that people experience when they make their own choices may exhibit a similar pattern of developmental change, peaking earlier in adolescence and declining through adulthood. Another possibility is that the intrinsic value of choice monotonically decreases across development. Indeed, relative to adults, children tend to seize opportunities to effect consequences in the world, showing stronger biases toward actions that have greater causal influence 
(Liquin & Gopnik, 2022;
McCormack et al., 2016;
Meng et al., 2018;
Raab & Hartley, 2020)
. It may be the case that increasing autonomy from childhood to adulthood confers more opportunity for agentic choice, leading to a decline in its intrinsic value. Finally, a third competing possibility is that the intrinsic value of choice increases over developmental time -as people learn to make better choices that more consistently lead to positive outcomes across contexts, the act of choosing itself may acquire greater value.
The present study investigates the relative contributions of intrinsic and instrumental value to agentic choice across development. Across two experiments, participants aged 8 to 25 years completed a novel task that enabled us to measure their preferences for agentic choice across conditions in which the instrumental value of choice varied. The task was coupled with a computational model that characterized how age-related differences in reward-learning mechanisms contribute to differential sensitivity to the intrinsic and instrumental value of choice from middle childhood to early adulthood. We hypothesized that participants across our age range would demonstrate some degree of agentic choice bias, seeking opportunities to choose even when doing so had no instrumental value, but that developmental changes in value-guided
In press: Psychological Science learning and decision-making would lead to an increasing influence of the instrumental value of choice on decisions from childhood to adulthood.


Open Practices Statement
For both experiments, all task code and stimuli, task data, and analysis code are publicly 
Figure 1
. A) Example trial of agency task. Each trial began with the (1) Agency Decision stage, in which participants viewed the upcoming arcade room and slot machine pair. Participants had to choose between accepting a variable offer amount (0 -6 tokens) and forgoing agency (i.e., allowing a coin flip to randomly determine their machine selection), or rejecting that offer and choosing agency (i.e., selecting one of the machines for themselves). After participants made their agency decision, the task proceeded to the (2) Machine Selection stage. If the participant had chosen to forgo agency, they viewed an animated coin flip and were instructed to select the machine that matched the color on which the coin landed. If the offer was rejected, participants selected between the machines for themselves. Once a machine was selected, participants viewed the (3) Outcome (either 10 or 0 tokens). If participants chose to forgo agency, they additionally received the tokens offered in the Agency Decision stage. B) Each of the three arcade rooms contained a pair of slot machines that paid out 10-token rewards with different probabilities on each trial.
Throughout the task, participants encountered three pairs of slot machines, each of which were housed in different arcade rooms. On every trial, participants completed a 'Machine Selection' stage in which they entered one of the rooms and selected between two machines.
Participants could learn through trial and error which machines were more likely to pay out tokens.
Critically, the Machine Selection stage was always preceded by an 'Agency Decision' stage, during which participants saw a door previewing which room they would enter. Participants had to decide whether to choose between the two machines themselves (i.e., 'choose agency') or whether to let the computer randomly pick one of the two machines for them (i.e., 'forgo agency'). Importantly, on every trial, participants were offered a variable number of tokens (between zero and six) that they would receive if they chose to forgo agency. If they chose to forgo agency, then in the Machine Selection stage, an animated coin flip determined which machine they had to play. All decisions were self-paced; outcome screens during the machine selection stage were displayed for 1.5 seconds.
We manipulated the value of choice by varying the reward probabilities of the three pairs of machines in the different rooms ( 
Figure 1B
). In the 50/50 condition, both machines had a 50%
In press: Psychological Science probability of paying out tokens. In this 'uncontrollable' environment, choice had no instrumental value -participants' own selections would not yield more reward than the computer's random choices. In the 70/30 and 90/10 conditions, the machines paid out tokens on 70% and 30%, and 90% and 10% of trials, respectively. In these two 'controllable' conditions, choice had instrumental value because participants could learn to select machines that would reliably yield more reward than random selections.
Participants completed 315 trials total. Trials were divided into 15 blocks of 21 trials, though these blocks were not signaled to participants. The 21 trials within each task block comprised each pair of slot machines (i.e., 50/50, 70/30, and 90/10 pair) coupled with each of the seven possible token offers (0 -6 tokens) one time. Within each block, the order of the trials was randomized for each participant. Prior to beginning the task, all participants completed an extensive, child-friendly tutorial with both written and auditory instructions (see Supplement).
Post-task assessments. Immediately following the agency task, participants completed two additional tasks assessing their knowledge of the machines' reward values. In addition, participants also completed a series of questionnaires assessing various individual difference measures. We include more details of these measures and related findings in the supplement.


Analysis approach
We analyzed participants' behavior in two ways. First, we ran regression analyses to determine how features of the task, including the reward probabilities of the machines, and the offer amounts presented, influenced the choices participants made at both decision stages.
Second, we fit participants' data with reinforcement-learning models that enabled us to examine how machine reward probabilities were learned over the course of the experiment and how learning influences choice. We describe both these approaches in more detail in the results. We focused on participants' decisions, not their reaction times, but include additional reaction time analyses in the supplement.


Results


Learning machine reward probabilities
We first confirmed that participants learned to select the more rewarding machines in the 70/30 and 90/10 conditions. We fit a mixed-effects logistic regression (see supplement for analysis methods) to participants' machine selections (only on free choice trials; coded as 1 if they selected the higher value machine and 0 if they selected the lower value machine) with
In press: Psychological Science condition (70/30 or 90/10), within-condition trial number, continuous age, and their interactions as predictors. Because we were interested in whether participants learned to select the better machines, we did not include trials in the 50/50 condition (in which there was no better machine) or trials in which the computer made machine selections in this analysis. Across conditions, participants made optimal choices at above-chance levels (Intercept = 2.4, SE = .17, z = 13.8, p < .001; 
Figure 2
). Participants made more optimal machine selections in the 90/10 condition relative to the 70/30 condition (β = -.42, SE = .08, X 2 (1) = 20.86, p < .001). Participants also made increasingly optimal choices across trials (β = .71, SE = .08, X 2 (1) = 60.59, p < .001). There was not a significant main effect of age on optimal choices (β = .21, SE = .17, X 2 (1) = 1.46, p = .227), nor was there a significant age x trial interaction effect (β = .10, SE = .08, X 2 (1) = 1.46, p = .226).
No other effects or interactions reached significance (ps > .22; see supplement for full results).
These findings indicate that across age, participants learned to select the better machines. Across trials (here plotted across blocks of 21 trials for visualization) and conditions, participants learned to select the more rewarding slot machines. Learning performance did not significantly vary across age. Smaller points indicate participant-level averages, large points indicate age-group averages, lines indicate the best-fitting linear regression (fitted to age-group averages) with shaded regions representing 95% confidence intervals. The dashed lines indicate chance-level performance. Participants were discretized into three age groups for visualization; age was treated continuously in statistical analyses.


Sensitivity to the intrinsic and instrumental value of choice
After establishing that participants learned to select the more rewarding slot machines, we examined whether they used the machine reward probabilities to guide their agency decisions.
We formalized the comparison between choosing and forgoing agency by computing their expected values (EV). On each trial, we defined !"##$% as the maximum EV of the two machines (i.e., 5, 7, or 9 tokens), assuming that participants would select the higher-value machine. We defined &#'(# as the average EV of the two machines (because the computer has a 50% chance of selecting each machine) plus the offer amount. We defined the 'value of choice' (VoC) as the difference between !"##$% and &#'(# . Positive values indicate that choosing agency is optimal, while negative values indicate that forgoing agency is better. To examine how VoC influenced participants' agency decisions, we fit a mixed-effects logistic regression with VoC, within-condition trial, continuous age, and their interactions as predictors. Here, we included all trials. Because our VoC variable incorporates information about the machine reward probabilities, we did not include a separate condition variable in the model. Participants demonstrated sensitivity to VoC, such that they were more likely to choose agency when doing so had higher expected value (β = 1.42, SE = .07, X 2 (1) = 144.39, p < .001; 
Figure 3A
). Moreover, we observed an age x VoC interaction (β = .15, SE = .07, X 2 (1) = 4.17, p = .041), such that older participants demonstrated greater sensitivity to VoC ( 
Figure 3B)
. A VoC x trial interaction effect (β = .33, SE = .04, X 2 (1) = 50.21, p < .001) revealed that sensitivity to VoC also increased across the task. Further, the extent to which sensitivity to VoC increased across trials varied across age, with older participants demonstrating the greatest increases in the effect of VoC on agency decisions across the experiment (VoC x age x trial effect: β = .12, SE = .04, X 2 (1) = 9.59, p = .002; 
Figure 3C
).
There was not a significant main effect of age on agency decisions (β = -.02, SE = .14, X 2 (1) = 0.01, p = .906) -participants across age demonstrated a strong bias toward choosing agency, even when doing so was not beneficial. When we restricted our analysis to trials in which VoC was 0, meaning the EVs of choosing versus forgoing agency were equivalent, we similarly did not observe a significant effect of age on agency decisions (β = .01, SE = .12, X 2 (1) = 0.00, p = .961). Across age, when VoC was 0, participants chose agency on 73.2% of trials (SE = 2%) ( 
Figure 3A
).
In press: Psychological Science Participants across age demonstrated a bias toward choosing agency even when its expected value was negative. In addition, participants were increasingly likely to choose agency as the value of choice increased, with older participants demonstrating a stronger influence of VoC on their agency decisions. Moreover, the effect of VoC on agency decisions increased across trials. In panel A) points show the average proportion of trials in which participants across age chose agency at each VoC level and error bars show standard errors of the mean. Trials were discretized into two bins and participants into three age groups for visualization purposes. Both variables were treated continuously in statistical analyses. An 'optimal' participant should always choose agency when VoC is positive and to forgo agency when VoC is negative. In panels B) and C), points show the fixed effects plus participant-specific random slopes of VoC (B) and VoC x trial (C) from a mixed-effects logistic regression model examining the influence of VoC, trial, and their interaction on agency decisions. The line shows the best-fitting linear regression through the points, with the shaded region showing 95% confidence intervals.


Reinforcement-learning modeling results
Our analyses of participants' agency decisions indicate that sensitivity to VoC increased both across development and across trials. However, our VoC measure assumes that participants had perfect knowledge of the machines' reward probabilities from the beginning of the task. As such, age differences in how participants learned the values of the machines across trials may influence decisions about choosing versus forgoing agency.
To examine how participants learned to make choices throughout the task, we fit variants of a reinforcement-learning model to our choice data. All model variants assumed that the participant learned the values of the six slot machines through experience. After selecting (or seeing the computer select) a machine and observing the outcome (r), the estimated value of the machine is updated, such that:
)*!"+,% !"# = )*!"+,% ! + * ( − )*!"+,% ! ) ⬚
where is a participant-specific learning rate. The model then uses these estimated machine values to compute the value of choosing and forgoing agentic choice at the Agency Decision stage on each trial. Following the same logic we used to compute VoC for our regression analysis, if the participant chooses agency, then at the Machine Selection stage, they will then be able to
In press: Psychological Science select the machine that they believe is better; as such, the value of choosing agency is the maximum estimated value of the two upcoming machines. Here, we add to this value a participantspecific 'agency bonus' that captures the intrinsic reward that each participant places on agentic choice. Positive and negative values of the agency bonus parameter reflect inflated and deflated valuation of agentic choice, respectively. Thus, on each trial, the value of agentic choice is computed as: ( )*!"+,%. , )*!"+,%/ ) + Where )*!"+,%. reflects the estimated value of the machine on the left and )*!"+,%/ reflects the estimated value of the machine on the right. The model maintains six value estimates, corresponding to the six machines experienced throughout the task, and uses those corresponding to the two presented machines on each trial.
If the participant chooses to forgo agentic choice, the computer will select randomly between the two machines. Thus, the value of forgoing agentic choice is the average of the estimated values of the two upcoming machines, plus the token offer amount the participant will receive on that trial:
( )*!"+,%. , )*!"+,%/ ) +
Although the agency bonus and token offer amount take similar forms, unlike the agency bonus, which is a free parameter fitted to each participants' choices that remains stable across choices, the token offer is a hard-coded feature of the experimental design that varies across trials.
At both decision stages, estimated values are converted into choice probabilities via softmax functions, where decision 'noise' or stochasticity is determined by participant-specific inverse temperature parameters. Higher inverse temperatures reflect decisions that are more deterministically governed by estimated values, whereas lower inverse temperatures reflect decisions that are less sensitive to value estimates.
We fit variants of this model with different numbers of learning rate, inverse temperature, and agency bonus parameters. We fit models with a single learning rate for all trials, two learning rates that allowed for differential value-updating following self-versus computer-made machine selections, two learning rates that allowed for differential value-updating following wins and losses, and four learning rates that allowed for differential value-updating across both self-versus computer-made selections and wins and losses. We also fit models with a single inverse temperature parameter that governed the extent to which both first-stage Agency Decisions and
In press: Psychological Science second-stage Machine Selections were value-driven, as well as two inverse temperatures that allowed the 'noisiness' of decisions to vary across stages. Finally, we fit models with and without the 'agency bonus' parameter. In total, we fit 16 model variants, comprising all combinations of one, two, and four learning rates, one and two inverse temperatures, and zero and one agency bonuses. Models were fit individually to each participant's data, using common priors. We include more details about our model-fitting procedure as well as model recoverability and validation in our supplement.
Both at the whole-group level, and within each age group, the best-fitting reinforcementlearning model included seven free parameters ( 
Figure 4
): four learning rate parameters ( !"#+!%0 , !"#+!%1 , !#)20 , !#)21 ) that govern the extent to which participants update their beliefs about the values of the machines following their own versus the computer's choices after wins and losses; two inverse temperature parameters ( *(%,!3 and )*!"+,% ) that determine the extent to which participants' value estimates influenced their Agency Decision and Machine Selection choices, and an agency bonus that was added to the value of choosing agency to account for individual biases toward choosing or forgoing agency. The second-best fitting model, which had only one inverse temperature parameter, similarly well-captured participants' choices (AIC difference from best-fitting model: 1.9). However, because our best-fitting model nested this simpler model (and all other simpler models) within it, we chose to focus our analyses on estimated parameter values from the better fitting, more complex model only. We confirmed via simulations that parameter estimates from this model were recoverable -in simulations, correlations between true, generating parameter values and estimated parameter values ranged from .79 to .96 (See supplement for details).
Parameter estimates from this winning model can help clarify whether age differences in learning the machine values, using value estimates to guide choice, or both, contributed to agerelated change in sensitivity to VoC. We first analyzed how learning rate parameters, which reflect how participants updated their beliefs about the value of the machines across trials, differed across agency decisions (i.e., after choosing or forgoing agency), outcome valence (i.e., wins or losses), continuous age, and their interactions via a mixed-effects linear regression. We observed an agency decision x outcome valence interaction (β = -.04, SE = .01, F(1, 270) = 10.0, p = .002; 
Figure 5
). Post-hoc paired t-tests indicated that participants demonstrated a confirmation bias, such that they weighted recent wins more heavily than recent losses following their own machine selections (t(91) = 3.2, p = .002; Mean !"#+!%0 : .24 (SE = .03), Mean !"#+!%1 : .11 (SE = .02)), but not following selections made by the computer (t(91) = -.87, p = .386; Mean !#)20 : .15 (SE = .03), Mean !#)21 : .18 (SE = .03)). Age did not relate to learning rates (β = .01, SE = .01, F(1, 90) = .52, p = .473), nor were there significant interactions between age and either agency decisions or outcome valence on learning rate magnitudes (ps > .24). A) The best-fitting reinforcement-learning model included seven free parameters that captured individual differences in how participants learned the values of the slot machines and used those values to guide their first-and second-stage choices. Points show individual participants' parameter estimates; the lines represent the best-fitting linear regression through the points, and the shaded regions show 95% confidence intervals. B) Participants demonstrated a confirmation bias, in which they updated their beliefs about the values of the machines more following positive versus negative outcomes, but only when they selected between the machines on their own. Error bars show standard errors across participant means.
Next, we analyzed age differences in the parameters that influenced participants' agency decisions. To test whether biases toward choosing agency varied with age, we ran a linear regression to examine the relation between age and agency bonuses. We found that agency bonuses did not vary significantly with age, b = 0.01, SE = 0.01, p = .269. In accordance with our behavioral findings, participants' average agency bonus was .32 (SE = .04), indicating that the intrinsic reward of agentic choice was approximately 3.2 tokens.
To test for age-related change in the use of learned value to guide agency decisions, we examined the extent to which participants' agency decisions were sensitive to their own, subjective valuation of choice. Within the model, the *(%,!3 parameter characterizes the extent to which participants' agency decisions were guided by their own estimates of the values of choosing and forgoing agentic choice -estimates that reflected both their own, idiosyncratic value-learning processes and their own agency biases. Higher values of *(%,!3 reflect a greater use of one's own subjective valuation of choice to guide agency decisions. We found that *(%,!3 marginally increased with age, b = .24, SE = 0.12, p = .056 (Mean *(%,!3 : 9.33 (SE = .59)). This analysis corroborates our model-free regression results and suggests that participants' use of the subjective value of choice to guide agency decisions increased across development.
When examined in isolation, we did not observe an effect of age on )*!"+,% values (b = .09, SE = .11, p = .410; Mean )*!"+,% : 7.44 (SE = .52)), indicating that stochasticity in participants' machine selections did not significantly vary across age. In an additional exploratory analysis (conducted after we submitted our Experiment 2 preregistration), we tested whether age effects on *(%,!3 and )*!"+,% differed by including them as dependent variables in the same linear-mixed effects model, with decision stage and age as interacting predictors. Here, we did not observe a significant decision stage x age interaction effect on estimates of values, b = .34, SE = .29, F(1, 90) = 1.41, p = .238. Thus, even though )*!"+,% did not vary with age when examined in isolation (while *(%,!3 showed a marginal relation), we cannot reject the null hypothesis that both inverse temperatures followed similar trajectories of age-related change.


Experiment 1 Discussion
Findings from Experiment 1 revealed an age-varying influence on instrumental value and an age-invariant influence of intrinsic value on agentic choice. That is, from middle childhood to early adulthood, participants were increasingly likely to decide to make their own choices when doing so would lead to more reward gain. Across age, however, participants consistently overvalued the opportunity to make choices. Together, these results suggest that distinct
In press: Psychological Science cognitive and motivational mechanisms may influence when and how people seek opportunities to control their environments. Nevertheless, across both analytic approaches, the interaction between age and instrumental value on agentic choice was modest. Thus, to ensure this finding was replicable, we conducted an additional, online study in which a larger sample of participants completed the same reinforcement-learning task.


Experiment 2: Preregistered Replication


Methods
After analyzing data from Experiment 1, we conducted an online, preregistered replication study (N = 150; participants ages 8 -25 years). In prior work, we have shown that with appropriate precautions, the developmental decision-making data we collect online looks largely similar to the data we collect in in-person laboratory experiments 
(Nussenbaum, Scheuplein, et al., 2020)
 Before beginning data collection, we specified a target sample size of 150 participants based on the size of the effect (adjusted / = .052) of age on *(%,!3 in the Experiment 1 data. Based on our original analysis, we determined that including 150 participants would give us > 80% power to detect an effect of the same size. After completing data collection for Experiment 2, we discovered that our original analysis code had a minor bug; when we fixed it (new adjusted / = .029), we determined that with a sample size of N = 150, we had 57% power to detect an effect of age on *(%,!3 . However, while we only recruited participants as young as 10 years old for our in-person experiment, we a priori decided to extend our age range down to 8 years for the online replication in order to characterize developmental changes in agentic choice across a wider age range. This decision likely also increased our power to detect age-related changes in our measures of interest.
Though we made several minor modifications to the task in order to administer it remotely and asynchronously (described in detail in the supplement), all relevant manipulations and task statistics (e.g., number of trials, reward probabilities, token offer amounts) remained identical to those used in the Experiment 1 task. Participants were recruited from across the United States via ads on social media, in-person science outreach events, flyers, and word-of-mouth. Adult participants and parents of minors provided informed consent; participants under eighteen years of age assented to participate. Participants were paid $10 for participation, plus a $5 performance bonus. Though we treated age continuously in all statistical analyses, we discretized participants into age groups for recruitment and data visualization purposes. In total, we collected data from 164 participants; after applying preregistered exclusion criteria to filter out inattentive participants (see Supplement), our final sample comprised 150 children (N = 50, ages 8 -12 years, M = 10.5
In press: Psychological Science years, SD = 1.4 years, 25 females, 25 males), adolescents (N = 50, ages 13 -17, M = 15.4, SD = 1.5, 26 females, 24 males), and adults (N = 50, ages 18 -25, M = 22.0, SD = 2.1, 22 females, 23 males, 5 other). Participants' self-identified race and ethnicity were as follows: 53.3% White, 26.7% Asian, 11.3% More than one race, 8.0% Black, and 1% Native American. In addition, 12.7% of participants identified as Hispanic. Research procedures were approved by New York University's Institutional Review Board (ID: 2021-5210).


Results
We followed the same analytic approach as in Experiment 1 to examine participants' machine selections and agency decisions. In brief, we replicated our prior findings. Because the primary goal of this replication study was to examine evidence for age-related change in the influence of instrumental value on agentic choice and age-invariance in the influence of intrinsic value, we focus on key tests of these hypotheses, and include a detailed description of all Experiment 2 findings in the supplement.
As in Experiment 1, to examine the influence of instrumental value on agency decisions, we examined both the effect of VoC on first-stage agency choices, as well as estimates of *(%,!3 derived from our reinforcement-learning model. Critically, we replicated our original finding of an age x VoC interaction effect on agentic choice (β = .22, SE = .06, X 2 (1) = 12.28, p < .001), such that older participants demonstrated greater sensitivity to VoC ( 
Figure S9
) when making their firststage agency decisions. As in Experiment 1, we also found that sensitivity to VoC increased across the task (VoC x trial interaction effect: β = .20, SE = .03, X 2 (1) = 51.94, p < .001) to the greatest extent at older ages (VoC x trial x age effect: β = .06, SE = .03, X 2 (1) = 5.26, p = .022).
Here too, we found that *(%,!3 -which captures the extent to which participants' agency decisions were guided by their own, learned estimates of the value of agentic choice -increased with age, b = .25, SE = .09, p = .008. As in Experiment 1, while we did not observe a significant effect of age on )*!"+,% when examined in isolation, b = .14, SE = .08, p = .081, in an exploratory, non-preregistered analysis examining both )*!"+,% and *(%,!3 estimates together, we also did not observe a significant interaction between age and decision stage on inverse temperatures, b = .27, SE = .21, F(1, 148) = 1.55, p = .216.
To examine the influence of intrinsic value on agentic choice, we examined the influence of age on agency decisions, as well as model-derived estimates of participants' 'agency bonuses.'
We did not observe evidence for a significant influence of age on agency decisions (β = .01, SE = .17, X 2 (1) = 0.00, p = .965) -participants across age demonstrated a strong bias toward choosing agency, even when doing so was not beneficial. Across age, when VoC was 0,
In press: Psychological Science participants chose agency on 75.8% of trials (SE = 2%) ( 
Figure S9
). Corroborating these findings, agency bonuses similarly did not vary significantly with age, b = -0.01, SE = 0.01, p = .242.
Finally, as in Experiment 1, we found that learning from the outcomes of the slot machines varied depending on whether the computer or participant made the machine selection and as a function of outcome valence. Specifically, we again observed an agency decision x outcome valence interaction (β = -.06, SE = .01, F(1, 444) = 28.34, p < .001). As in Experiment 1, post-hoc paired t-tests indicated that participants demonstrated a confirmation bias following their own machine selections (t(149) = 7.0, p < .001; Mean !"#+!%0 : .30 (SE = .02), Mean !"#+!%1 : .12 (SE = .02)), but not following those made by the computer (t(149) = -1.1, p = .269; Mean !#)20 : .18 (SE = .02), Mean !#)21 : .22 (SE = .03)). Here, we also observed decreasing learning rates with increasing age (β = -.03, SE = .01, F(1, 148) = 5.38, p = .022). Taken together, findings from our preregistered, online replication study mirrored those from our original, in-person experiment, providing additional evidence for distinct developmental trajectories of the influence of instrumental and intrinsic value on agentic choice.


General Discussion
Here, we investigated how intrinsic and instrumental value shape agentic choice preferences across development. Across two experiments, we found that from childhood to adulthood, participants demonstrated a consistent agentic choice bias, such that they preferred to make choices even when forgoing agency would lead to greater reward. Both computational model-based analyses and a simpler regression revealed that this bias -which we interpret as reflecting participants' intrinsic valuation of choice -did not significantly vary in magnitude from middle childhood into early adulthood. Moreover, we found that participants' agency decisions were also sensitive to the instrumental value of choice, such that participants were increasingly likely to choose agency on trials in which doing so would lead to more reward. An interaction between age and VoC on agency decisions revealed that this sensitivity increased with age.
Critically, parameter estimates from our fitted reinforcement-learning model revealed that agerelated increases in the calibration of agency decisions to different contexts were not solely due to age-related changes in learning the rewards that different actions were likely to yield; even when accounting for individual and developmental differences in participants' learning, the use of beliefs about the instrumental value of choice to guide agency decisions increased with age.
Multiple cognitive mechanisms may have contributed to age-related change in sensitivity to the instrumental value of choice. Older participants' greater use of VoC to guide their agency decisions may have been driven by developmental improvements in the ability to accurately
In press: Psychological Science compute the expected value (EV) of choosing versus forgoing agency. Despite demonstrating effective learning of the machine values, younger participants may have had more difficulty than older participants in integrating learned machine values with explicit token offer amounts to compute the overall expected values of choosing versus forgoing agency. Prior work has demonstrated that EV estimation improves into adulthood (G. 
Rosenbaum & Hartley, 2019)
, as mathematical and probabilistic reasoning abilities develop 
(Donati et al., 2014;
Geary, 2007;
Schlottmann & Anderson, 1994)
. In addition, in this task, to effectively use VoC to guide their agency choices, participants had to think one step into the future, determining their first-stage agency decisions based on the rewards they expected to earn from their second-stage machine selections. The ability to effectively plan multiple steps into the future improves across childhood and adolescence 
(Albert & Steinberg, 2011;
Decker et al., 2016;
Ma et al., 2022;
Nussenbaum, Scheuplein, et al., 2020;
Potter et al., 2017)
, and may also contribute to the use of instrumental value to guide agency choices.
Age-related change in sensitivity to the instrumental value of choice may also reflect general, age-related decreases in 'decision noise,' as has been observed in prior studies 
(Nussenbaum & Hartley, 2019)
. Indeed, we observed mixed-evidence for the specificity of the age effect of the value of choice on agency decisions -older participants in our task may have made more value-driven decisions overall, not just at the first stage in which they made their agency choice. Age-related decreases in decision noise could reflect improvements in value computation or shifts from a more exploratory to a more exploitative choice strategy 
(Giron et al., 2023;
Gopnik, 2020)
. Here, our primary aim was to examine whether sensitivity to the instrumental and intrinsic value of choice varied with age. However, future work could tease apart the various potential influences on age-related change in sensitivity to the instrumental value of choice by more directly manipulating the complexity of the value computation involved in agency versus other types of decisions. For example, a variant of this experiment could present participants with explicit information about the value of choice on some trials, and examine how this reduction in computational complexity influences agency decisions from childhood to adulthood. Future work could also include trials in which participants have to make equivalently complex two-stage decisions, but where the first stage does not involve choosing or forgoing agency. Differences in value-guided behavior across agency versus non-agency decisions could further elucidate whether the developmental trajectories of value computations that involve explicit consideration of oneself as an agentic being differ from those that do not; it may be the case that this explicit consideration is particularly demanding earlier in life, as children learn to weigh their beliefs about
In press: Psychological Science their own efficacy in bringing about desired outcomes with the potential costs of exerting control 
(Shenhav et al., 2021)
.
Across age, we observed a consistent preference for agentic choice, such that on average, participants sacrificed more than three tokens to select between the machines themselves. This bias toward agentic choice has been observed in many prior studies (Ackerlund 
Brandt et al., 2015;
Bobadilla-Suarez et al., 2017;
Bown et al., 2003;
Cockburn et al., 2014;
Leotti & Delgado, 2011;
Munuera et al., 2023;
Wang et al., 2021)
, and may be generally adaptive, particularly early in development: Choice provides the opportunity to learn whether actions are causally efficacious, promoting knowledge of environmental structure and estimates of one's agency in the world. However, people do not always value opportunities to choose. Unlike the relatively simple and low-stakes choices participants faced in our task, other decisions can be difficult or anxiety-provoking 
(Iyengar & Lepper, 2000;
Patall, 2012;
Shenhav & Buckner, 2014;
Sidarus et al., 2019)
, potentially reducing the hedonic properties of choice. In addition, some choices may be relatively insignificant and may thus not warrant cognitive effort 
(Boureau et al., 2015)
. Making decisions can also be unpleasant when choosing between aversive, mundane, or numerous options 
(Botti et al., 2009;
Iyengar & Lepper, 2000;
Leotti & Delgado, 2014;
Shenhav et al., 2018)
. The learning context of our task may have also introduced additional age-varying motivations for agentic choice -participants may have wanted to resolve epistemic uncertainty about specific machine options, leading them to choose agency so that they could explore strategically, even when they knew it was likely to lead to less reward on any given trial 
(Meder et al., 2021;
Molinaro et al., 2023;
Nussenbaum et al., 2023;
Somerville et al., 2017)
.
A more complete and general account of age-related change in agentic choice preferences will require replicating and extending our findings to varied contexts in which different properties of choice, like its difficulty, valence, and utility in resolving uncertainty systematically vary.
Here, we also found that making choices influenced how participants learned about the value of different actions. Our analysis of model-derived learning rates revealed that across age, when participants selected between the machines themselves, they updated their beliefs to a greater extent following wins versus losses. However, they did not demonstrate this learning rate asymmetry when the computer selected between the machines. This type of 'confirmation bias' 
(Palminteri & Lebreton, 2022)
, may cause participants to persistently overestimate the rewards they earn by making their own choices. It is possible that over long timescales, such learning distortions contribute to agentic choice acquiring intrinsic value. While the present study focused on examining the relative contributions of instrumental and intrinsic value to agentic choice preferences across age, future work could further decompose and assess the factors that give
In press: Psychological Science rise to the intrinsic value of choice in the first place. It may be the case that the age-invariance of the intrinsic value of choice that we observed emerges from combinations of different factors (e.g., learning biases, reward sensitivity) that change with experience over developmental time.
Future work will also be required to test the generalizability of our findings. Participants in this study comprised a community sample of children, adolescents, and young adults from the New York City area (Experiment 1) and from the United States (Experiment 2). Prior analyses of participants in a different behavioral experiment in our lab  who were drawn from the same database as participants in Experiment 1 revealed that in-person participants tended to come from homes with two to three times the average annual income of the surrounding community (Average annual household income of lab sample: $153,137; NYC average annual household income: $55,191), and with higher levels of parental education (Average years of parental education of lab sample: 16.7 (i.e., college degree); percentage of NYC adults over age 25 with a bachelor's degree: 36.2). Similarly, online study participants in Experiment 2 came from homes with higher annual household incomes than the U.S. population from which they were sampled (Median annual household income of study participants: $100,000 -$200,000); USA median annual household income: $74,580 (2022)). Prior research has revealed that the development of basic learning and choice mechanisms are influenced by the presence of both stress 
(Hanson et al., 2017;
Harms et al., 2018)
 and enrichment opportunities 
(Amso et al., 2019;
Sheridan et al., 2017)
 in early-life environments, which may vary systematically with socioeconomic status. It may be the case that developmental trajectories of sensitivity to the instrumental value of choice are similarly influenced by early-life exposure to stress and enrichment. Further, the development of agentic choice preferences themselves may be highly dependent on early experiences during which people learn the efficacy of their own actions -an extensive cross-species literature has revealed that exposure to uncontrollable stressors may lead to 'learned helplessness' 
(Maier & Seligman, 2016)
, such that organisms stop seeking opportunities for agentic choice, even in controllable environments. Finally, there may also be cultural differences in agentic choice preferences, such that individuals who grow up in different sociocultural environments may form different beliefs about self-efficacy 
(Oettingen, 1995)
 and the value of making their own choices.
Here, by developing a novel task, we sought to address how preferences for agentic choice -and critically, the flexibility of those preferences to adapt to different contexts -change across development. From early in life, people learn to act as agentic beings in the world, tailoring their behavior to exert causal effects on their environments. Critically, each instance of acting in the world is preceded by a decision about whether to act freely. Here, we found evidence for
In press: Psychological Science distinct developmental trajectories of sensitivity to the intrinsic and instrumental value of agentic choice -a decoupling that may be adaptive. The early-emerging, age-invariant positive, intrinsic value of choice instills a bias toward action that may promote greater opportunity for individuals to learn about their sphere of influence in the world. Later, this simple bias toward choice may be increasingly augmented by a more complex algorithm that informs when to forgo such opportunities. Together, bias and flexibility in peoples' decisions to seek or forgo opportunities for choice may underlie the development of adaptive agentic action.


Supplementary information for:
Sensitivity to the instrumental value of choice increases across development


Task tutorial
As noted in the main text of the manuscript, prior to beginning the task, participants completed an extensive, child-friendly instruction phase. The full instructions are available on the Open Science Framework (https://osf.io/69rs8/). In Experiment 1, instructions were presented on the screen and an experimenter sat in the room and read them aloud to each participant. In Experiment 2 (which was administered online), instructions were presented on the screen while a pre-recorded voice read them aloud. In Experiment 2, participants were not able to advance each instruction screen until the voice recording had finished playing. Throughout the instructions, participants also practiced each trial component separately and together to learn the mechanics of the task.
The instructions explained each component of the task. First, participants were told that throughout the task, they would encounter rooms with different pairs of slot machines, with each machine sometimes paying out 10 tokens and sometimes paying out 0 tokens. Participants were told that some machines were 'lucky,' meaning they would pay out 10 tokens more than half the time, some machines were 'regular,' meaning they would pay out 10 tokens about half the time, and some were 'unlucky,' meaning they would pay out 10 tokens less than half the time. Participants were told that the luckiness of each machine would stay consistent for the whole game.
Next, participants were told that the arcade owners like to test the machines, and that they would offer the participant tokens to let a computer randomly select which machine to play. They were explicitly told that the luckiness of the machine would not change depending on whether they chose it or the computer chose it. If they chose to accept the offer, they would receive the token offer amount and any tokens the played machine yielded. Participants were told that to earn the most tokens, they should consider both the offer amount and the luckiness of the machines. They were also told that their total token amount would be converted to bonus money at the end of the experiment.


Mixed-effects modeling methods
We used R version 4.3.1 (R Core Team, 2023) to conduct data processing and statistical analyses. Mixed-effects models were fit using the 'afex' package 
(Singmann et al., 2023)
. For all mixed-effects models, we included subject-level random intercepts and slopes across within-subjects fixed effects 
(Barr et al., 2013)
, unless otherwise noted. We set the number of model iterations to one million and used the 'bobyqa' optimizer. Age was treated as a continuous variable in all analyses. All continuous variables were z-scored prior to their inclusion in mixed-effects models.


Agency decision times
To further examine participants' sensitivity to the instrumental value of choice, we conducted an additional, exploratory (non-preregistered) analysis of how long they spent making their agency decisions. We expected that participants' decision times would reflect the engagement of effortful deliberation, including computing and comparing the values of choosing and forgoing agency. We fit a mixed-effects linear regression to log-transformed agency decision times with the absolute value of VoC, within-condition trial, continuous age, and their interactions as predictors. Across both experiments, we observed a main effect of the absolute value of VoC (E1: β = -.02, SE = .01, F(1, 90.3) = 17.0, p < .001; E2: β = -.01, SE = .003, F(1, 148.6) = 17.1, p < .001), indicating that participants were indeed slower to make agency decisions on difficult trials in which choosing and forgoing agency had more similar expected values ( 
Figure S1
). In both experiments, while agency decision times decreased across trials (E1: β = -.13, SE = .01, F(1, 89.4) = 83.8, p < .001; E2: β = -.10, SE = .01, F(1, 148.0) = 71.6, p < .001), the effect of VoC on decision times did not significantly vary across the experiment (E1: p = .16; E2: p = .647).
The effect of age on agency decision times varied across experiments. In Experiment 1, younger participants made slower decisions overall (β = -.08, SE = .04, F(1, 90.0) = 4.1, p = .045), but the effect of VoC on decision times did not vary significantly with age (β = -.01, SE = .01, F(1, 90.9) = 1.16, p = .285). In Experiment 2, however, we observed an age x absolute value of VoC interaction effect, (β = -.01, SE = .003, F(1, 148.4) = 4.4, p = .037), with older participants demonstrating a stronger influence of VoC on decision times ( 
Figure S1B
).
Taken together, these results indicate that across age, participants' agency decision times tracked VoC, potentially reflecting the increased engagement of effortful deliberation on trials in which agency decisions were, from a normative standpoint, more difficult. We observed inconclusive evidence for how age modulates this process -it may be the case that younger participants' reduced sensitivity to VoC both emerges from and in turn leads to reduced modulation of effortful deliberation during value computation. 


Reinforcement-learning modeling methods


Models
As noted in the main text of the manuscript, to examine how participants learned to make choices throughout the task, we fit variants of a reinforcement-learning model to our choice data. In total, we fit our data with 16 different models. The 16 models comprised all combinations of four parameterizations of the learning rate, two parameterizations of the 'agency bonus,' and two parameterizations of the inverse temperature, all of which we describe in more detail below.
Learning rates. All model variants assumed that the participant learned the values of the slot machines through experience. After selecting (or seeing the computer select) a machine and observing the outcome (r), the estimated value of the machine is updated, such that:
ℎ +1 = ℎ + α * ( − ℎ )
where is a participant-specific learning rate. Across the four value functions, the number of α α parameters varied. We fit models with:
1.) a single parameter that governed value-updating on all trials. α 2.) two parameters that varied based on the first-stage choice:
, which governed α α ℎ value-updating on trials in which the participant chose between the machines themselves and , which governed value-updating on trials in which the computer determined the machine α selection.
3.) two parameters that varied based on the valence of the machine's outcome: for α α + wins and for losses. α Agency bonus. The model then uses these estimated machine values to compute the value of forgoing and choosing agentic choice at the Agency Decision stage on each trial. On each trial, the value of forgoing choice is computed as the average of the estimated values of the two upcoming machines, plus the token offer amount the participant will receive on that trial:
= ( ℎ 1 , ℎ 2 ) +
The value of choosing agency is simply the highest estimated value of the two upcoming machines:
ℎ = ( ℎ 1 ,
ℎ 2 )
To account for any biases participants may have toward choosing or forgoing agency, we also tested versions of the model with an additional 'agency bonus' free parameter. This agency bonus parameter was added to the value of , such that:
ℎ ℎ = ( ℎ 1 , ℎ 2 ) +
Unlike the token offer amount, which changes every trial and reflects the true number of tokens a participant can gain by forgoing agency, the agency bonus is stable across trials, and is fitted to each participants' choices to capture the intrinsic value they place on choosing agency. Because it is added to the value of , its units are in tokens and therefore easily ℎ interpretable -positive values reflect the number of tokens at which a participant values an agency choice, while negative values reflect the number of tokens at which a participant values forgoing agency. Inverse temperatures. At both decision stages, estimated values are converted into choice probabilities via softmax functions. At the first stage, the probability of choosing agency is:
( ℎ ) = β * ℎ β * ℎ + β *
where is an inverse temperature free parameter that captures the extent to which each β participants' first-stage choices are driven by the estimated values of choosing and forgoing agency.
At the second stage, the probability of choosing machine 1 (the machine on the left) is:
( ℎ ℎ 1 ) = β ℎ * ℎ 1 β ℎ * ℎ 1 + β ℎ * ℎ 2
where is an inverse temperature free parameter that captures the extent to which each β ℎ participants' second-stage machine selection choices are driven by the estimated values of the two machines.
We tested variants of the model in which and were included as separate, β β ℎ independent free parameters, to capture potential differences in how people used value to guide agency decisions versus machine selection decisions. We also fit variants of the model with a single inverse temperature parameter (i.e., = ) to test whether each participants' β β ℎ decision noise was similar across decision stages. All model combinations. As noted above, our different parameterizations yielded 16 total models in our test set 
(Table S1)
.  Model-fitting procedure For each participant and each model, we identified the fitted parameter values that maximized the log posterior of their choices using the fmincon function in the optimization toolbox in Matlab 2020b (The Mathworks Inc., 2020). Prior to model-fitting, we rescaled rewards and offer amounts so that they were bounded between 0 and 1. We applied the following bounds and priors to each parameter: : bounds = [0, 1], prior~(1.1, 1.1); : bounds = [0, 30], α β β prior~(2, 3), agency bonus: bounds = [-5, 5], prior~(0, 3). We randomly initialized each γ parameter, drawing uniformly from within their bounds. We initialized and ran fmincon 100 times per participant, and took the parameter estimates that maximized the log posterior across iterations.


Model recoverability
To ensure our models were distinguishable from one another, we conducted recoverability analyses. We simulated data from 1,000 participants for each model. To generate parameters for each simulated participant, we sampled values from uniform distributions with the following bounds: : [0, 1], [1, 10], agency bonus: 
[-1, 1]
. In addition, we applied additional α β constraints to ensure that and parameters from models with two or two parameters were α β α β sufficiently different from one another; if, for example, the random samples of and ℎ + +
were highly similar, then we knew a priori that data generated from this model would not be well-captured with a model with separate learning rates across agency decisions. As such, if α parameters from models with two or four learning rates were less than .3 apart, they were re-sampled until they met that spacing constraint. Similarly, if parameters from models with β two inverse temperatures were less than 3 apart, they were also re-sampled. Finally, if the absolute value of the agency bonus was less than .1, it was also resampled to ensure that it was sufficiently different from 0.
After simulating datasets from each of the 16 models, we fit these simulated datasets with all 16 models and examined the proportion of simulations from each generating model best fit by each model according to AIC ( 
Figure S2A
). In addition, we computed the proportion of simulations that were generated from each model that were best fit by a given model ( 
Figure  S2B)
.


A.
B. The model recoverability results indicate that simpler models were sometimes confused for more complex models, and vice versa. Thus, it is not always clear whether participants were using different learning rates or different inverse temperatures across the task. We nevertheless focused our analyses on the most complex model because it both a.) best fit the empirical data across age groups and b.) nested the simpler models.


Parameter recoverability
For the model, the model that best fit our empirical data, we α_ β_ examined the correlation between simulated and fitted parameter values. In general, correlations between simulated and fitted parameter values were high (r > .79; 
Figure S3
). One potential concern for parameter recoverability is that almost all participants in our experiment demonstrated a bias toward choosing agency, on average valuing agency at more than 3 tokens and choosing agency on more than 70% of trials. It may be the case that in causing participants to frequently choose to choose, this bias toward agency impairs our ability to accurately estimate how participants learn from the outcomes of the computer's choices. To test whether this was the case, we additionally tested how well we could recover parameters when we restricted our analyses only to simulated agents with agency bonuses greater than .3 (i.e., at more than 3 tokens). Here, we continue to observe strong correlations between simulated and recovered parameter values, indicating that we can characterize how participants learned on trials in which they chose to forgo agency, even when they choose agency on most trials ( 
Figure S4
). 
Figure S4
. Parameter recoverability results when the agency bonus is > .3. Correlations between simulated parameter values and fitted parameter values were high for all model parameters, even when we restricted our analyses to simulated participants with a high agency bonus.


Model validation
We also examined whether simulated choice data generated by the model well-captured qualitative features of participants' choices α_ β_ during the task. To do so, we generated 50 simulated datasets for each participant, using their best-fitting parameters and trial sequence. In total, we generated 4,600 simulated datasets (92 participants x 50 repetitions). We then examined the proportion of optimal Machine Selection decisions that participants made across task blocks ( 
Figure S5
), as well as how the value of choice related to agency decisions ( 
Figure S6
). In general, our winning model captured many features of participants' choices, though we observed some differences between the empirical data and the simulated data. Most notably, the winning model failed to capture adults' increasing sensitivity to VoC in their agency decisions across the task. This suggests that increases in VoC sensitivity over the course of the task may not arise from learning the values of the slot machines (which the model can capture), but rather from the increasing use of VoC estimates across the task (which the model cannot account for). It may be the case that participants learned to integrate the values of the machines with the token offer amounts through experience. For example, participants may have initially relied on cheaper heuristic strategies that only consider the machine values or the token offer amounts, before learning to effectively integrate them. Through experience, participants -and particularly older participants -may have learned to adjust their choice strategy. Strategy adjustments may be supported in part, by participants' metacognitive awareness of the efficacy of their own decision-making, which may improve from childhood to early adulthood 
Weil et al., 2013)
. Future work could assess how individuals learn to calibrate their agency decisions across trials above and beyond how they learn the value of the machines. A) The best-fitting reinforcement-learning model well-captured participants' patterns of machine selections across trials. Points represent age-group averages and the line shows the best-fitting linear regression through the points. The shaded region represents 95% confidence intervals. B) The model could not fully account for older adolescents' attenuated difference in performance across the two conditions. Points represent the difference in values derived from the empirical vs. simulated data; we first averaged over the 50 simulations for each participant to determine the mean performance of each simulated participant. We then computed the difference between the simulation average and the true, empirical performance for the corresponding participant. Error bars represent the standard error of participant means.


F igure S6. Posterior predictive check: Agency decisions.
A) The best-fitting reinforcement-learning model captured many of the qualitative features of participants' agency decisions, including their increasing sensitivity to VoC with increasing age, and bias toward choosing agency across age. B) The model could not fully account for participants' reduction in the proportion of trials in which they chose agency from the first to the second half of the experiment when VoC was less than 0. Points represent the difference in values derived from the empirical vs. simulated data; we first averaged over the 50 simulations for each participant to determine the mean performance of each simulated participant. We then computed the difference between the simulation average and the true, empirical performance for the corresponding participant. Error bars represent the standard error of participant means.
Finally, to ensure that our model-derived parameter estimates were interpretable, in an exploratory, non-preregistered analysis, we examined how they related to random effects estimates from a logistic regression analyzing participants' agency decisions. To extract a measure of each participants' bias toward agency as well as the influence of VoC on agency decisions, we ran a mixed-effects logistic regression examining the influence of VoC, within-condition trial, and their interaction on agency decisions. Our model included random participant intercepts as random participant slopes across VoC, trial, and their interaction. From this model, we extracted random participant intercepts as a measure of individual differences in the bias toward (or away from) choosing agency, and random participant VoC slopes as a measure of individual differences in the influence of VoC on agency decisions. We hypothesized that participant intercepts should relate to model-derived agency bonuses, and participant slopes should relate to model-derived estimates of . β
In line with our hypothesis, linear regressions revealed that agency bonuses strongly predicted random participant intercepts (E1: b = 2.3, SE = .23, t(90)= 9.9, p < .001; E2: b = 2.4, SE = .17, t(148)= 14.4, p < .001), and estimates of strongly predicted VoC slopes (E1: b β = .09, SE = .01, t(90) = 10.4, p < .001; E2: b = .10, SE = .01, t(148) = 19.4, p < .001). Further, we confirmed that and VoC slopes captured shared variance in participant behavior β above and beyond their relations with age; when we included age in our regression, we continued to observe a robust effect of on VoC slopes (E1: b = .09, SE = .01, t(89)= 10.0, β p < .001; E2: b = .10, SE = .01, t(147)= 18.7, p < .001). Finally, we confirmed that and β VoC slopes were uniquely related; when we included as an additional predictor in our β ℎ regression, we continued to observe a relation between and VoC slopes, and we did not β observe a significant effect of (E1: p = .239; E2: p = .808). Taken together, these β ℎ analyses further validate our interpretation of model-derived estimates of the agency bonus as capturing participants' bias toward choosing agency, and as capturing the influence of β estimated values of choice on participants' agency decisions.


Post-task assessments
Choice-based value estimation task Immediately following the agency task, participants completed an additional task designed to assess the learned value of each machine. On each trial of this task (adapted from 
(Frank et al., 2004)
), participants viewed a pair of slot machines that may or may not have been paired together during the agency task. Participants were instructed to select which of the two machines was more likely to pay out tokens. They did not receive any feedback. Participants completed 90 trials, such that they saw every possible machine pairing six times in a pseudorandom order. Importantly, the novel pairings in this task enabled assessment of the degree to which participants learned sufficiently precise subjective value estimates to be able to distinguish the two higher-value (i.e., 70% vs. 90%) options and the two lower-value (i.e., 30% vs. 10%) options from different choice pairs.
Our analyses excluded trials with both 50% machines. Across participants, mean accuracy was 0.769 (SD = 0.153), indicating that in general, they learned the relative values of the slot machines. A mixed-effects logistic regression examining trial-level accuracy as a function of age, the absolute difference in reward probability between the two machines, and their interaction revealed that accuracy increased with the difference in reward probability (X 2 (1) = 79.9, p < .001). There was not a significant effect of age nor was there a reward probability difference x age interaction effect (ps > .24).


Declarative value estimation task
After the choice-based value estimation task, participants completed a task in which they explicitly indicated the reward probability of each machine. In this task, they saw each slot machine one at a time in a random order and were instructed to indicate its probability of paying out tokens on a 9-point scale from 10% to 90%. We calculated participants' error as the absolute difference between the true probability and their response. Overall, participants acquired explicit knowledge of each machine's reward probability, with a mean error of 14.3% (SD = 8.7%) ( 
Figure S7)
. A linear mixed-effects regression examining trial-level error as a function of participant age, true reward probability, and their interaction revealed that participants were more accurate in assessing the machines with higher reward probabilities (X 2 (1) = 21.7, p < .001), likely due to participants choosing the higher-value machines on more trials than the lower-value machines. There was not a significant effect of age nor was there a reward probability x age interaction effect (ps > .54). Participants across age demonstrated explicit knowledge of the machine's reward probabilities at the end of the task. Participants were more accurate in reporting the reward probabilities of the higher-value machines. The horizontal black lines within the boxplots indicate median reported reward probabilities for each age group and each slot machine. The lower and upper edges of the boxes indicate the first and third quartiles of the grouped data, and the vertical lines extend to the smallest value no further than 1.5 times the interquartile range. Black dots indicate data points outside those values.


Questionnaires
Following the experimental tasks, participants completed a series of online questionnaires assessing various individual difference measures. All participants completed the Desirability of Control scale 
(Burger & Cooper, 1979)
. Adults additionally completed the Locus of Control questionnaire 
(Nowicki & Duke, 1974)
, the State-Trait Anxiety Index 
(Spielberger, 1989)
, and Beck's Depression Inventory 
(Beck et al., 1996)
. Participants ages 10-17 completed analogous child-friendly versions of the questionnaires: the Locus of Control 
(Nowicki & Strickland, 1973)
, State-Trait Anxiety Inventory for Children 
(Spielberger & Edwards, 1973)
 and the Children Depression Inventory 
(Kovacs, 1992)
, respectively. Participants also completed a verbal debriefing questionnaire conducted by the experimenter to assess self-reported strategy use during the Agency Task.
We conducted exploratory analyses to examine the relation between scores from these questionnaires (DOC, LOC, STAI/STAIc, BDI/CDI) and individual differences in decision-making during the task. Specifically, we ran a series of linear regressions examining the relation between each questionnaire score and age, model-derived and agency bonus parameter β values, and their interactions. We did not observe any significant effects of these variables on DOC, LOC, STAI, or BDI/CDI scores (ps > .07). 
Figure S8
. Experiment 2 task design. The reinforcement-learning task used in Experiment 2 followed an identical structure to the task used in Experiment 1. Visuals were changed for online administration. Each trial began with the (1) Agency Decision stage, in which participants viewed the upcoming arcade room and slot machine pair. Participants had to choose between accepting a variable offer amount (0 -6 tokens) and forgoing agency (i.e., allowing a coin flip to randomly determine their machine selection), or rejecting that offer and choosing agency (i.e., selecting one of the machines for themselves). After participants made their agency decision, the task proceeded to the (2) Machine Selection stage. If the participant chose to forgo agency, only one machine was made available for them to select. If the offer was rejected, participants selected between the machines for themselves. Once a machine was selected, participants viewed the (3) Outcome (either 10 or 0 tokens). If participants chose to forgo agency, they additionally received the tokens offered in the Agency Decision stage.


Results


Machine selection decisions
Replicating our previous findings, we found that participants learned to select better slot machines across trials (β = .50, SE = .05, X 2 (1) = 69.5, p < .001; Filtered sample: β = .53, SE = .06, X 2 (1) = 66.5, p < .001; 
Figure S9
), and made more optimal machine selections in the 90/10 condition relative to the 70/30 condition (Full sample: β = .41, SE = .07, X 2 (1) = 29.27, p < .001; Filtered sample: β = .39, SE = .07, X 2 (1) =25.05, p < .001), increasingly so across trials (β = .09, SE = .04, X 2 (1) = 5.47, p = .019; Filtered sample: β = .08, SE = .04, X 2 (1) = 3.44, p = .064), 
Figure S8
). In contrast to Experiment 1, here, we did observe that older participants made more optimal machine selections (β = .46, SE = .11, X 2 (1) = 15.49, p < .001; Filtered sample: β = .53, SE = .12, X 2 (1) = 18.88, p < .001), likely due to our inclusion of younger participants. 


Agency decisions
Also replicating our Experiment 1 results, we found that participants integrated learned machine reward probabilities with token offer amounts to guide their agency decisions, demonstrating sensitivity to VoC, such that they were more likely to choose agency when doing so had higher expected value (β = 1.09, SE = .06, X 2 (1) = 166.6, p < .001; Filtered sample: β = 1.15, SE = .06, X 2 (1) = 168.42, p < .001; 
Figure S10
). As reported in the main text of the manuscript, we replicated our original finding of an age x VoC interaction effect on agentic choice (β = .22, SE = .06, X 2 (1) = 12.28, p < .001; Filtered sample: β =.21, SE = .06, X 2 (1) = 10.92, p < .001), such that older participants demonstrated greater sensitivity to VoC when making their first-stage agency decisions ( 
Figure S10
). Sensitivity to VoC increased across the task (VoC x trial interaction effect: β = .20, SE = .03, X 2 (1) = 51.94, p < .001; Filtered sample: β = .20, SE = .03, X 2 (1) = 48.77, p < .001), to the greatest extent at older ages (VoC x trial x age effect: β = .06, SE = .03, X 2 (1) = 5.26, p = .022; Filtered sample: β =.06, SE = .03, X 2 (1) = 5.31, p = .021). There was not a significant main effect of age on agency decisions (Full sample: β = .01, SE = .18, X 2 (1) = 0.00, p = .965; Filtered sample: β = -.03, SE = .13, X 2 (1) = 0.04, p = .840) -participants across age demonstrated a strong bias toward choosing agency, even when doing so was not beneficial. Across age, when VoC was 0, participants chose agency on 75.8% (Filtered sample: 74.5%) of trials (SE = 2%; Filtered sample SE: 1.7%). Participants across age demonstrated a bias toward choosing agency even when its expected value was negative. In addition, participants were increasingly likely to choose agency as the value of choice increased, with older participants demonstrating a stronger influence of VoC on their agency decisions. Moreover, the effect of VoC on agency decisions increased across trials, increasingly so with age. In panel A) points show the average proportion of trials in which participants across age chose agency at each VoC level and error bars show standard errors of the mean. Trials were discretized into two bins and participants into three age groups for visualization purposes. Both variables were treated continuously in statistical analyses. In panels B) and C), points show the fixed effects plus participant-specific random slopes of VoC (B) and VoC x trial (C) from a mixed-effects logistic regression model examining the influence of VoC, trial, and their interaction on agency decisions. The line shows the best-fitting linear regression through the points, with the shaded region showing 95% confidence intervals.


Reinforcement-learning modeling results
Results from our reinforcement-learning models similarly suggest different developmental trajectories for the influence of instrumental and intrinsic value on agentic choice. The same winning model from Experiment 1 best captured participants' choices in Experiment 2 ( 
Figure S11
). Here too, we found that -which captures the extent to which participants' β agency decisions were guided by their own, learned estimates of the value of agentic choiceincreased with age, b = .25, SE = 0.09, p = .008 (Filtered sample: b = .25, SE = 0.10, p = .014; 
Figure S12
). As in Experiment 1, however, agency bonuses did not vary significantly with age, b = -0.01, SE = 0.01, p = .242 (Filtered sample: b = -.01, SE = 0.01, p = .099). There was also not a significant effect of age on values (b = .14, SE = .08, p = .08; Filtered sample: b = .15, β ℎ SE = 0.09, p = .083).
Finally, we replicated our finding that participants learned differently from valenced outcomes when they made their own choices versus when the computer selected a slot machine for them (agency decision x outcome valence interaction effect on learning rates: β = -.06, SE = .01, F(1, 444) = 28.3, p < .001; Filtered sample: β = -.06, SE = .01, F(1, 399) = 25.75, p < .001; 
Figure S12B
). Participants weighted recent wins more heavily than recent losses following their own machine selections (t(149) = 6.97, p < .001; Filtered sample: t(134) = 6.47, p < .001), but not following selections made by the computer (t(149) = -1.11, p = .269; Filtered sample: t(134) = -1.18, p = .239). In contrast to Experiment 1, here we did observe a main effect of age on learning rates, β = -.03, SE = .01, F(1, 149) = 5.38, p = .022 (Filtered sample: β = -.03, SE = .01, F(1, 133) = 3.75, p = .055). Post-hoc regressions examining each learning rate individually revealed that decreased with increasing age, b = -0.01, SE = 0.003, p = .001 α ℎ − (Filtered sample: b = -0.01, SE = 0.004, p = .003).  participants' parameter estimates; the lines represent the best-fitting linear regression through the points, and the shaded regions show 95% confidence intervals. B) Participants demonstrated a confirmation bias, in which they updated their beliefs about the values of the machines more following positive versus negative outcomes, but only when they selected between the machines on their own. Error bars show standard errors across participant means.


Explicit knowledge of machine reward probabilities
After the reinforcement-learning task, participants completed a task in which they explicitly indicated the reward probability of each machine. In this task, they saw each slot machine one at a time in a random order and were instructed to indicate its probability of paying out tokens on a 9-point scale from 10% to 90%. Participants used a slider to make their responses. We calculated participants' error as the absolute difference between the true probability and their response. Overall, participants acquired explicit knowledge of each machine's reward probability, with a mean error of 15.9% (SD = 6.4%; 
Figure S13
) (Filtered sample: Mean = 15.6%; SD = 6.4%). A linear mixed-effects regression examining trial-level error as a function of participant age, true reward probability, and their interaction revealed that participants were more accurate in assessing the machines with higher reward probabilities (F(1, 748) = 23.42, p < .001; Filtered sample: F(1, 673) = 22.78, p < .001), likely due to participants choosing the higher-value machines on more trials than the lower-value machines. In contrast to Experiment 1, here we additionally observed a main effect of age, such that younger participants' reports were less accurate, (F(1, 148) = 7.59, p = .007; Filtered sample: F(1, 133) = 7.01, p = .009). This discrepancy with our Experiment 1 findings may be due in part to our inclusion of younger participants in Experiment 2. 
Figure S13
. Explicit knowledge of slot machine reward probabilities. Participants across age demonstrated explicit knowledge of the machine's reward probabilities at the end of the task. Participants were more accurate in reporting the reward probabilities of the higher-value machines. The horizontal black lines within the boxplots indicate median reported reward probabilities for each age group and each slot machine. The lower and upper edges of the boxes indicate the first and third quartiles of the grouped data, and the vertical lines extend to the smallest value no further than 1.5 times the interquartile range. Black dots indicate data points outside those values.     
Figure 2 .
2
Optimal choices at the Machine Selection stage.


Figure 3 .
3
Agency Decisions across the experiment.


Figure 4 .
4
Model comparison results. AIC values indicated that across age groups, the model with four learning rates, two inverse temperatures, and an agency bonus best fit the data. A) Bars show average AIC values for each model within each age group. B) Bars show the difference between the overall mean AIC of the best-fitting model and the mean AIC values of the other models with an agency bonus in the comparison set.


Figure 5 .
5
Model parameters.


Figure S1 .
S1
Agency decision times in Experiment 1 (A) and Experiment 2 (B). Participants' agency decisions were slower on trials in which the expected values of choosing and forgoing agency were more similar (VoC values closer to 0). The effect of the absolute value of VoC on agency decision times increased significantly with age in Experiment 2. Points indicate average agency decision times at each level of VoC, and error bars indicate standard errors of participant means.


− 4 .
4
) four parameters that varied based on both the first-stage choice and the valence of α the


Figure S2 .
S2
Model recoverability results. A) Confusion matrix showing the proportion of datasets from each simulated model that were best fit by each of the 16 models. B) Inversion matrix showing the proportion of datasets simulated from each model that were best fit by each of the 16 models.


Figure S3 .
S3
Parameter recoverability results. Correlations between simulated parameter values and fitted parameter values were high for all model parameters.


Figure S5 .
S5
Posterior predictive check: Machine Selection decisions.


Figure S7 .
S7
Explicit knowledge of slot machine reward probabilities.


Figure S9 .
S9
Optimal choices at the Machine Selection stage. Across trials (here plotted across blocks of 21 trials for visualization) and conditions, participants learned to select the more rewarding slot machines. Smaller points indicate participant-level averages, large points indicate age-group averages, lines indicate the best-fitting linear regression (fitted to age-group averages) with shaded regions representing 95% confidence intervals. The dashed lines indicate chance-level performance. Participants were discretized into three age groups for visualization; age was treated continuously in statistical analyses.


Figure S10 .
S10
Agency Decisions across the experiment.


Figure S11 .
S11
Model comparison results. AIC values indicated that across age groups, the model with four learning rates, two inverse temperatures, and an agency bonus best fit the data. Bars show average AIC values for each model within each age group.


Figure S12 .
S12
Model parameters. A) The best-fitting reinforcement-learning model included seven free parameters that captured individual differences in how participants learned the values of the slot machines and used those values to guide their first-and second-stage choices. Both , which β reflected the extent to which participants used the estimated values of choosing and forgoing agency to guide their agency decision, and varied significantly with continuous age. Points show individual α ℎ −


Table


Table S7 :
S7
Experiment 2: learning rates
Predictor
Estimate
SE
df
F
p
intercept
0.20
0.01
age
-0.03
0.01
1, 148
5.38
.022
valence
-0.04
0.01
1, 444
11.17
<.001
agency
0.00
0.01
1, 444
0.10
.753
age x valence
-0.01
0.01
1, 444
0.51
.477
age x agency
-0.00
0.01
1, 444
0.09
.762
valence x agency
-0.06
0.01
1, 444
28.34
<.001
age x valence x agency
-0.02
0.01
1, 444
3.20
.074
Model 4: Inverse temperatures
estimate~1 + age * decision_stage + (1 | participant_id)


Table S8 :
S8
Experiment 1: inverse temperatures
Predictor
Estimate
SE
df
F
p
intercept
8.39
0.46
age
1.09
0.46
1, 90
5.56
.021
decision stage
0.94
0.29
1, 90
10.73
.001
age x decision stage
0.31
0.29
1, 90
1.16
.284


Table S9 :
S9
Experiment 2: inverse temperatures
Model 5: Agency decision times
log(agency_decision_time)~1 + age * VoC_magnitude * condition_trial +
(1 + VoC_magnitude * condition_trial | participant_id)
Predictor
Estimate
SE
df
F
p
intercept
7.32
0.37
age
0.97
0.38
1, 148
6.75
.010
decision stage
0.14
0.21
1, 148
0.45
.503
age x decision stage
0.27
0.21
1, 148
1.55
.216


Table S10 :
S10
Experiment 1: agency decision times
Predictor
Estimate
SE
df
F
p
intercept
0.15
0.04
age
-0.08
0.04
1, 89.98
4.14
.045
VoC magnitude
-0.02
0.01
1, 90.30
16.96
<.001
trial
-0.13
0.01
1, 89.36
83.83
<.001
age x VoC magnitude
-0.01
0.01
1, 90.86
1.16
.285
age x trial
0.01
0.01
1, 89.48
0.48
.490
VoC magnitude x trial
-0.00
0.00
1, 449.77
1.96
.162
age x VoC mag. x trial
0.00
0.00
1, 455.28
0.01
.931


Table S11 :
S11
Experiment 2: agency decision times
Predictor
Estimate
SE
df
F
p
intercept
-0.05
0.03
age
-0.02
0.03
1, 147.99
0.63
.427
VoC magnitude
-0.01
0.00
1, 148.61
17.05
<.001
trial
-0.09
0.01
1, 147.97
71.56
<.001
age x VoC magnitude
-0.01
0.00
1, 148.39
4.43
.037
age x trial
-0.00
0.01
1, 147.97
0.09
.767
VoC magnitude x trial
-0.00
0.00
1, 1560.95
0.21
.647
age x VoC mag. x trial
0.00
0.00
1, 1562.56
0.61
.435








Acknowledgments
We thank Cara Zampino and Faya Movchan for help with data collection. This work was supported by the Klingenstein-Simons Foundation (Fellowship Award to C.A.H.), the National Science In press: Psychological Science






Experiment 2: Preregistered Replication


Methods
As noted in the main text of the manuscript, after analyzing data from Experiment 1, we conducted an online, preregistered replication study (N = 150; participants ages 8 -25 years). We preregistered our hypotheses, data collection, and analytic plan prior to beginning data collection. Our preregistration is available on the Open Science Framework: https://osf.io/q5zfj.
We made several minor modifications to the task in order to administer it remotely and asynchronously. First, we made minor changes to the visual stimuli and their animations to make them more dynamic and engaging in the online context ( 
Figure S8
). Second, to make the experimental session shorter, we removed the reward sensitivity task, and included only the primary reinforcement-learning task and secondary test of explicit knowledge. Third, because the online task was administered remotely without an experimenter present, we recorded audio instructions; participants had to listen to each page of instructions prior to moving to the next. We also included two short comprehension checks; If participants answered any of the true/false comprehension questions incorrectly, the instructions repeated. Finally, a priori we defined several exclusion criteria to filter out inattentive participants from our final analyses: Four participants were excluded for interacting with their browser (entering or exiting full screen, clicking in and out of the task window) more than 20 times throughout the experiment, and nine additional participants were excluded for making first-stage agency decisions in fewer than 150 ms on more than 10% of trials. In addition, we excluded trials in which participants did not make a first-stage agency decision within the time limit (10 s) or in which they responded in fewer than 150 ms.
We did not preregister any additional exclusions based on participant performance. However, in examining our data, we noticed that 15 participants made the same agency decision on 300 or more of the 315 trials (6 children, 7 adolescents, and 2 adults). While this behavior could reflect a strong agentic choice bias, it could also reflect general inattention or disengagement from the task. As such, we report results from all analyses including both the full dataset (in line with our preregistration) and with the filtered dataset, from which these participants are excluded. Importantly, patterns of statistical significance are the same regardless of whether we exclude these participants (except for the context by trial interaction effect on machine selections, which we note below).


Full regression model results
For all mixed-effects regression models reported in the manuscript, we report here the full random-effects structure of the model and effect estimates for both Experiment 1 and Experiment 2.      
 










The value of choice as a reinforcer for typically developing children




Ackerlund
Brandt






J
A
Dozier






C
L
Juanico






J
F
Laudont






C
L
Mick






B
R








Journal of Applied Behavior Analysis




48


2
















Age differences in strategic planning as indexed by the tower of London




D
Albert






L
Steinberg








Child Development




82


5
















The relationship between cognitive enrichment and cognitive control: A systematic investigation of environmental influences on development through socioeconomic status




D
Amso






C
Salhi






D
Badre








Developmental Psychobiology




2
















Role of affective self-regulatory efficacy in diverse spheres of psychosocial functioning




A
Bandura






G
V
Caprara






C
Barbaranelli






M
Gerbino






C
Pastorelli








Child Development




74


3
















Intrinsic reward: potential cognitive and neural mechanisms




B
Blain






T
Sharot








Current Opinion in Behavioral Sciences




39
















The intrinsic value of choice: The propensity to under-delegate in the face of potential gains and losses




S
Bobadilla-Suarez






C
R
Sunstein






T
Sharot








Journal of Risk and Uncertainty




54


3
















Tragic Choices: Autonomy and Emotional Responses to Medical Decisions




S
Botti






K
Orfali






S
S
Iyengar








The Journal of Consumer Research




36


3
















Deciding How To Decide: Self-Control and Meta-Decision Making




Y.-L
Boureau






P
Sokol-Hessner






N
D
Daw








Trends in Cognitive Sciences




19


11
















The lure of choice




N
J
Bown






D
Read






B
Summers








Journal of Behavioral Decision Making




16


4
















Age differences in affective decision making as indexed by performance on the Iowa Gambling Task




E
Cauffman






E
P
Shulman






L
Steinberg






E
Claus






M
T
Banich






S
Graham






J
Woolard








Developmental Psychology




46


1
















A reinforcement learning mechanism responsible for the valuation of free choice




J
Cockburn






A
G E
Collins






M
J
Frank








Neuron




83


3
















Intrinsic motivation and the process of learning: Beneficial effects of contextualization, personalization, and choice




D
I
Cordova






M
R
Lepper








Journal of Educational Psychology




88


4
















From Creatures of Habit to Goal-Directed Learners: Tracking the Developmental Emergence of Model-Based Reinforcement Learning




J
H
Decker






A
R
Otto






N
D
Daw






C
A
Hartley








press: Psychological Science






27














A mediation model to explain decision making under conditions of risk among adolescents: the role of fluid intelligence and probabilistic reasoning




M
A
Donati






A
Panno






F
Chiesi






C
Primi








Journal of Clinical and Experimental Neuropsychology




36


6
















Factors influencing elevated ethanol consumption in adolescent relative to adult rats. Alcoholism




T
L
Doremus






S
C
Brunell






P
Rajendran






L
P
Spear








Clinical and Experimental Research




29


10
















Novel-object place conditioning in adolescent and adult male and female rats: effects of social isolation




L
A
Douglas






E
I
Varlinskaya






L
P
Spear








Physiology & Behavior




80


2-3
















Rewarding properties of social interactions in adolescent and adult male and female rats: impact of social versus isolate housing of subjects and partners




L
A
Douglas






E
I
Varlinskaya






L
P
Spear








Developmental Psychobiology




45


3




















G
Dunlap






M
Deperczel






S
Clarke






D
Wilson






S
Wright






R
White






A
Gomez


















Choice making to promote adaptive behavior for students with emotional and behavioral challenges






Journal of Applied Behavior Analysis




27


3














On the relative reinforcing effects of choice and differential consequences




W
W
Fisher






R
H
Thompson






C
C
Piazza






K
Crosland






D
Gotjen








Journal of Applied Behavior Analysis




30


3




















A
Galvan






T
A
Hare






C
E
Parra






J
Penn






H
Voss






G
Glover






B
J
Casey


















Earlier development of the accumbens relative to orbitofrontal cortex might underlie risktaking behavior in adolescents






The Journal of Neuroscience: The Official Journal of the Society for Neuroscience




26


25














Enhanced striatal sensitivity to aversive reinforcement in adolescents versus adults




A
Galván






K
M
Mcglennen








Journal of Cognitive Neuroscience




25


2
















Development of mathematical understanding




D
C
Geary




10.1002/9780470147658.chpsy0218








Handbook of Child Psychology




John Wiley & Sons, Inc














Developmental changes in exploration resemble stochastic optimization




A
P
Giron






S
Ciranka






E
Schulz






W
Van Den Bos






A
Ruggeri






B
Meder






C
M
Wu








Nature Human Behaviour




7


11
















Childhood as a solution to explore--exploit tensions




A
Gopnik








Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences




375














Children are full of In press: Psychological Science optimism, but those rose-tinted glasses are fading -reduced learning from negative outcomes drives hyperoptimism in children




J
Habicht






A
Bowler






M
E
Moses-Payne






T
U
Hauser








J. Exp. Psychol. Gen




151


8
















Early adversity and learning: implications for typical and atypical behavioral development




J
L
Hanson






W
Van Den Bos






B
J
Roeber






K
D
Rudolph






R
J
Davidson






S
D
Pollak








Journal of Child Psychology and Psychiatry, and Allied Disciplines




58


7
















Instrumental learning and cognitive flexibility processes are impaired in children exposed to early life stress




M
B
Harms






K
E
Shannon Bowen






J
L
Hanson






S
D
Pollak








Developmental Science




21


4


12596














A motivational theory of life-span development




J
Heckhausen






C
Wrosch






R
Schulz








Psychological Review




117


1
















When choice is demotivating: can one desire too much of a good thing




S
S
Iyengar






M
R
Lepper








Journal of Personality and Social Psychology




79


6
















The value of choice facilitates subsequent memory across development




P
L
Katzman






C
A
Hartley








Cognition




104239














The inherent reward of choice




L
A
Leotti






M
R
Delgado








Psychological Science




22


10
















The value of exercising control over monetary gains and losses




L
A
Leotti






M
R
Delgado








Psychological Science




25


2
















Born to choose: the origins and value of the need for control




L
A
Leotti






S
S
Iyengar






K
N
Ochsner








Trends in Cognitive Sciences




14


10
















Children are more exploratory and learn more than adults in an approach-avoid task




E
G
Liquin






A
Gopnik








Cognition




218














An integrative model of the maturation of cognitive control




B
Luna






S
Marek






B
Larsen






B
Tervo-Clemmens






R
Chahal








Annual Review of Neuroscience




38
















A Reward-Based Framework of Perceived Control




V
Ly






K
S
Wang






J
Bhanji






M
R
Delgado








Frontiers in Neuroscience




13


65














The component processes of complex planning follow distinct developmental trajectories




I
Ma






C
Phaneuf






B
Van Opheusden






W
J
Ma






C
Hartley




















Learned helplessness at fifty: Insights from neuroscience




S
F
Maier






M
E P
Seligman








Psychological Review




123


4
















Children's use of interventions to learn causal structure




T
Mccormack






N
Bramley






C
Frosch






F
Patrick






D
Lagnado








Journal of Experimental Child Psychology




141








In press: Psychological Science 1-22








Development of directed and random exploration in children




B
Meder






C
M
Wu






E
Schulz






A
Ruggeri








Developmental Science




24


4


13095














Children's causal interventions combine discrimination and confirmation




Y
Meng






N
Bramley






F
Xu










Proceedings of the 40th Annual Conference of the Cognitive Science Society


the 40th Annual Conference of the Cognitive Science Society
















Multifaceted information-seeking motives in children




G
Molinaro






I
Cogliati Dezza






S
K
Bühler






C
Moutsiana






T
Sharot








Nature Communications




14


1


5505














Agency and the Calibration of Motivated Behavior




J
M
Moscarello






C
A
Hartley








Trends in Cognitive Sciences




21


10
















Intrinsic motivation for choice varies with individual risk attitudes and the controllability of the environment




J
Munuera






Ribes
Agost






M
Bendetowicz






D
Kerebel






A
Chambon






V
Lau






B








PLoS Comput Biol




19


8


1010551














Causal Information-Seeking Strategies Change Across Childhood and Adolescence




K
Nussenbaum






A
O
Cohen






Z
J
Davis






D
J
Halpern






T
M
Gureckis






C
A
Hartley








Cognitive Science




44


9


12888


















K
Nussenbaum






R
E
Martin






S
Maulhardt






Y
Yang






G
Bizzell-Hatcher






N
S
Bhatt






M
Koenig






G
M
Rosenbaum






J
P
O'doherty






J
Cockburn






C
A
Hartley


















Novelty and uncertainty differentially drive exploration across development. eLife, 12, e84260


10.7554/elife.84260














Reinforcement learning across development: What insights can we draw from a decade of research?




K
Nussenbaum






C
A
Hartley








Developmental Cognitive Neuroscience




40


100733














Memory's reflection of learned information value increases across development




K
Nussenbaum






E
Prentis






C
A
Hartley








Journal of Experimental Psychology. General




10
















Moving Developmental Research Online: Comparing In-Lab and Web-Based Studies of Model-Based Reinforcement Learning




K
Nussenbaum






M
Scheuplein






C
V
Phaneuf






M
D
Evans






C
A
Hartley








Collabra: Psychology




6


1


















K
Nussenbaum






J
A
Velez






B
T
Washington






H
E
Hamling






C
A
Hartley


















Flexibility in valenced reinforcement learning computations across development






Child Development




93


5














Cross-cultural perspectives on self-efficacy. Self-Efficacy in Changing Societies




G
Oettingen


























Psychological Science






In press








The computational roots of positivity and confirmation biases in reinforcement learning




S
Palminteri






M
Lebreton








Trends in Cognitive Sciences




26


7
















The motivational complexity of choosing: A review of theory and research. The Oxford Handbook of Human Motivation




E
A
Patall








579














Cognitive components underpinning the development of model-based learning




T
C S
Potter






N
V
Bryce






C
A
Hartley








Developmental Cognitive Neuroscience




25
















Adolescents exhibit reduced Pavlovian biases on instrumental learning




H
Raab






C
A
Hartley








Scientific Reports




10


1


15770














Developmental perspectives on risky and impulsive choice




G
Rosenbaum






C
A
Hartley








Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences




374














Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory




G
M
Rosenbaum






H
L
Grassie






C
A
Hartley








64620












Memory enhancements from active control of learning emerge across development




A
Ruggeri






D
B
Markant






T
M
Gureckis






M
Bretzke






F
Xu








Cognition




186
















Children's judgments of expected value




A
Schlottmann






N
H
Anderson








Developmental Psychology




30


1
















Neural correlates of dueling affective reactions to win-win choices




A
Shenhav






R
L
Buckner








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






111














The evil of banality: When choosing between the mundane feels like choosing between the worst




A
Shenhav






C
K
Dean Wolf






U
R
Karmarkar








Journal of Experimental Psychology. General




147


12
















Decomposing the motivation to exert mental effort




A
Shenhav






M
P
Fahey






I
Grahek








Current Directions in Psychological Science




30


4
















Toward a Rational and Mechanistic Account of Mental Effort




A
Shenhav






S
Musslick






F
Lieder






W
Kool






T
L
Griffiths






J
D
Cohen






M
M
Botvinick








Annual Review of Neuroscience




40
















Dimensions of childhood adversity have distinct associations with neural systems underlying executive functioning




M
A
Sheridan






M
Peverill






A
S
Finn






K
A
Mclaughlin








Development and Psychopathology




29


5
















Cost-benefit trade-offs in decision-making References




N
Sidarus






S
Palminteri






V
Chambon


















Random effects structure for confirmatory hypothesis testing: Keep it maximal




D
J
Barr






R
Levy






C
Scheepers






H
J
Tily








Journal of Memory and Language




3


68
















A
T
Beck






R
A
Steer






G
K
Brown




Beck Depression Inventory (BDI-II): Manual and Questionnaire. The Psychological Corporation
















The desirability of control




J
M
Burger






H
M
Cooper








Motivation and Emotion




3


4
















By carrot or by stick: cognitive reinforcement learning in parkinsonism




M
J
Frank






L
C
Seeberger






R
C
Reilly








Science




306


5703
















Children's depression inventory: Manual. Multi-Health Systems North Tonawanda




M
Kovacs








NY












MATLAB version: 9








The MathWorks Inc




9


0




The MathWorks Inc












A locus of control scale for noncollege as well as college adults




S
Nowicki






M
P
Duke








Journal of Personality Assessment




38


2
















A locus of control scale for children




S
Nowicki






B
R
Strickland








Journal of Consulting and Clinical Psychology




40


1
















Causal Information-Seeking Strategies Change Across Childhood and Adolescence




K
Nussenbaum






A
O
Cohen






Z
J
Davis






D
J
Halpern






T
M
Gureckis






C
A
Hartley








Cognitive Science




44


20


12888














R: A language and environment for statistical computing. R Foundation for Statistical Computing




R Core Team










Vienna, Austria












Afex: analysis of factorial experiments




H
Singmann






B
Bolker






J
Westfall






F
Aust






M
S
Ben-Shachar












R package version 1.3-0










C
D
Spielberger




State-Trait Anxiety Inventory: Bibliography


Palo Alto, CA




Consulting Psychologists Press








2nd ed.








Preliminary Test Manual for the State-trait Anxiety Inventory for Children:("How-I-feel Questionnaire")




C
D
Spielberger






C
D
Edwards








Consulting Psychologists Press












The development of metacognitive ability in adolescence




L
G
Weil






S
M
Fleming






I
Dumontheil






E
J
Kilford






R
S
Weil






G
Rees






R
J
Dolan






S.-J
Blakemore








Consciousness and Cognition




22


1


1007326








PLoS Computational Biology








Decision making in children and adolescents: impaired Iowa Gambling Task performance in early adolescence




D
G
Smith






L
Xiao






A
Bechara








Developmental Psychology




48


4
















Charting the expansion of strategic exploratory behavior during adolescence




L
H
Somerville






S
F
Sasse






M
C
Garrad






A
T
Drysdale






N
Abi Akar






C
Insel






R
C
Wilson








Journal of Experimental Psychology. General




146


2
















Assessing the value of choice in a token system




S
K
Sran






J
C
Borrero








Journal of Applied Behavior Analysis




43


3
















Effects of number of alternatives on choice in humans




S
Suzuki








Behavioural Processes




39


2
















An evaluation of the value of choice with preschool children




J
H
Tiger






G
P
Hanley






E
Hernandez








Journal of Applied Behavior Analysis




39


1
















Intrinsic Need Satisfaction and Well-Being in Children and Adolescents: An Application of the Self-Determination Theory




M
Véronneau






R
F
Koestner






J
R Z
Abela








Journal of Social and Clinical Psychology




24


2
















Corticostriatal Circuits Encode the Subjective Value of Perceived Control




K
S
Wang






M
R
Delgado








Cerebral Cortex




29


12
















The influence of contextual factors on the subjective value of control




K
S
Wang






M
Kashyap






M
R
Delgado








Emotion




21


4
















Expectations and Performance: An Empirical Test of Bandura's Self-efficacy Theory




R
Weinberg






D
Gould






A
Jackson








Journal of Sport and Exercise Psychology




1


4
















Relations between daily activities and adolescent mood: the role of autonomy




S
M
Weinstein






R
Mermelstein








Journal of Clinical Child and Adolescent Psychology




36


2
















Cognitive effort: A neuroeconomic approach




A
Westbrook






T
S
Braver








Cognitive, Affective & Behavioral Neuroscience




15


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]