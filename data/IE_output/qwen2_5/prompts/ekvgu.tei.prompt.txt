You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



The idea that memory retrieval involves an evidence accumulation decision process was proposed by 
Ratcliff (1978)
, who characterized memory retrieval in two-choice decision tasks as a Brownian motion, or Wiener, diffusion process, with normally-distributed drift rates. Ratcliff assumed that the drift rate, which determines the rate at which the diffusion process accumulates evidence, is normally distributed across trials, representing variability in the quality of the information entering the decision process from memory. An implication of his analysis is that distinct sources of variability contribute to the speed and accuracy of retrieval decisions: across-trial variability in memory representations and within-trial variability in the process of matching the contents of memory to the retrieval cue. The first of these sources of variability is represented by drift rate variability; the second is represented by moment-to-moment diffusive variability in the evidence accumulation process. Prior to Ratcliff's work, signal detection theory 
(Green & Swets, 1966
) was used to model both perceptual and memory-based decisions -as it continues to be used today 
(Wixted, 2020)
. However, signal detection models are unable to distinguish between memory variability and decision-process variability and, critically, are unable to predict empirically observed decision times.
Recent studies using continuous-outcome tasks to study retrieval from visual working memory (VWM) have raised new questions about the memory and decision processes that determine the speed and accuracy of retrieval. In VWM tasks, subjects make decisions about members of sets of briefly-presented perceptual stimuli that have been stored in memory. Continuous-outcome VWM tasks use continuously-distributed perceptual attributes like color and orientation as stimuli 
(Wilken & Ma, 2004)
 and the decisions about them are expressed on continuous spatial scales. Studies of VWM using both two-choice and continuous-outcome decisions have shown that VWM is sharply capacity-limited 
(Phillips, 1974)
, possibly to as few as three or four items 
(Luck & Vogel, 1997;
Pashler, 1988;
Cowan, 2001)
. It therefore represents a significant bottleneck in the translation of perception into action. Understanding the capacity limitations of the human processing system has been one of the main aims of cognitive psychology since the foundational work of 
Broadbent (1958)
 and 
Kahneman (1973)
 and the study of VWM capacity has accordingly been one of the most heavily researched areas during the last two decades.
Our aim in this article is to use a successful model of continuous-outcome decisions, the circular diffusion model (CDM) 
(Smith, 2016
(Smith, , 2019
Smith et al., , 2022
 to study the speed and accuracy of VWM retrieval and to provide a theoretical characterization of the memory and decision processes involved and of the relationship between them. Like the original diffusion model, the CDM models the decision process as evidence accumulation by a Wiener diffusion process, but in two evidence dimensions (2D), which define the Cartesian axes of a circular space, rather than one. Also like that model, it assumes that across-trial variability in evidence entering the decision process and within-trial variability in evidence accumulation both contribute to the speed and accuracy of retrieval. We combine the CDM with a flexible model of across-trial variability in memory representations, the Jones-Pewsey distribution 
(Jones & Pewsey, 2005)
, to characterize how memory precision varies with set size and stimulus discriminability.  used the Jones-Pewsey distribution to model across-trial variability in stimulus encoding in a continuous-outcome decision task that used noise-perturbed color patches as stimuli. They showed that the distribution characterized drift rates in the CDM as a function of stimulus noise, including trials on which noise caused perceptual encoding to fail and responses to be very inaccurate. We show that the combination of the CDM and a Jones-Pewsey memory distribution (the JP-CDM model) provides a good account of the joint distributions of accuracy and decision times in four experiments that varied the cognitive load of the memory task (Experiments 1 and 2) and the difficulty of the decision task (Experiments 3 and 4).
We show that the estimated parameters of the JP-CDM model behave in simple and interpretable ways as both memory load and the difficulty of the perceptual matching task are varied, and provide evidence for a sample-size model of VWM capacity 
(Palmer, 1990;
Schneegans et al., 2020
, Sewell et al., 2014
Smith et al., 2016
Tomić & Bays, 2024)
. The sample-size model conceives of VWM as a limited-capacity resource made up of noisy evidence samples, which can be interpreted as activity in a population of statistically variable Poisson neurons 
(Bays, 2014;
Schneegans et al., 2020;
Smith, 2015;
Smith & Sewell, 2013)
. 
Sewell et al. (2014)
 showed that the sample-size model provided a parameter-free account of retrieval accuracy as a function of set size and exposure duration in a two-choice VWM task that used decisions about oriented grating patches.  subsequently showed that a diffusion process model in which drift rates follow the sample-size model successfully characterized the speed and accuracy of decisions in this task at the level of the response time (RT) distributions for correct responses and errors.  showed that a sample-size process in which the evidence samples are allocated unequally among items led to an increase in the set-size effect that could be described by a more general power law. In this article we show that the same combination of a sample-size memory representation and a diffusion decision process can account for retrieval performance in continuous-outcome VWM tasks. Our article therefore provides a theoretical bridge between, and unification of, two complementary approaches to the study of VWM retrieval.


Concepts and Controversies in Visual Working Memory
The catalysts for the contemporary study of VWM include: the study of 
Phillips (1974)
, who provided evidence for a purely visual form of short-term memory that does not rely on verbal, or articulatory, codes; the study of 
Luck and Vogel (1997)
 who showed that the capacity of VWM for such material depends on the number of stored objects irrespective of their complexity (although limits on this property were later identified by 
Alvarez and Cavanagh (2004)
); and the integrative review of 
Cowan (2001)
, who argued that, for most people, the capacity limit of VWM is around four items. Cowan suggested that the number four constitutes a "magical number" in 
Miller's (1956)
 sense of the term in recognition of this fact. Most early studies of VWM followed Phillips and used some version of a change detection task, in which subjects decided whether there was a change in the second of two consecutively presented visual arrays, either anywhere in the array, or at a location probed at the time of the second array, and made a yes/no decision 
(Donkin et al., 2016;
Luck & Vogel, 1997;
Vogel et al., 2006;
Wilken & Ma, 2004)
. A smaller number of studies have used a two-alternative forced-choice (2AFC) task in combination with 
Downing's (1988)
 poststimulus probe method, which she developed to study attention in briefly-presented, near-threshold, visual displays, in which a decision is made about a single location that is probed after display offset 
(Palmer, 1990;
Lilburn et al., 2019;
Sewell et al., 2014;
. Changes in accuracy as a function of set size in both change detection and 2AFC tasks can be characterized using signal detection methods. Some studies have used confidence ratings combined with a receiver operating characteristic (ROC) analysis, which can again be characterized using signal detection methods 
(Donkin et al., 2014;
Rouder et al., 2008)
.
The first study to use a continuous-outcome decision task to investigate VWM was that of 
Wilken and Ma (2004)
, who used the task, along with a change-detection task, to investigate VWM for color, orientation, and spatial frequency. The continuous-outcome task is a contemporary version of the method of adjustment of classical psychophysics, in which sensory thresholds were measured by asking subjects to adjust the intensity of a variable stimulus until it matched a standard 
(Woodworth & Schlossberg, 1954)
. It was reintroduced to modern psychology by 
Blake et al. (1997)
 and 
Prinzmetal et al. (1998)
 to study VWM for motion and the effects of attention on perceptual variability, respectively, and was adapted by Wilken and Ma to study VWM. Unlike two-choice tasks, which yield a single error proportion in each condition, continuous-outcome tasks yield entire distributions of errors, which characterize the distance between the true stimulus value and the value recalled by the subject on each trial as a function of memory load. The typical distribution of errors is a peaked, heavy-tailed distribution, with a bell-shaped central region flanked by flat tails. The usual experimental finding is that the precision of the central region -defined as the reciprocal of the standard deviation -decreases with increasing set size while the height of the tails increases 
(Wilken & Ma, 2004;
Zhang & Luck, 2008)
. Many VWM researchers have come to prefer continuous-outcome tasks to two-choice tasks because they offer increased data degrees of freedom for model testing.
Since the introduction of the continuous-outcome task to VWM research, a rich body of theory has grown up around it, which has tried to explain why distributions of errors have the form they do and why they change in systematic ways with set size. The first wave of theories contrasted item-capacity limited models (colloquially, "slot models") and resource models. Slot models assume that the distribution of errors comprises a mixture of high-accuracy responses, associated with items in memory, and uniformly-distributed guesses, associated with items that are not 
(Awh et al., 2007;
Rouder et al., 2008)
. These two components make up the bell-shaped central region and the heavy tails of the error distribution, respectively. As the set size increases relative to the item-capacity limit, the probability of guessing increases, which reduces the height of the central peak and increases the height of the tails. Resource models represent memory as an allocatable resource, which can be concentrated on a few items or distributed among many 
(Bays et al., 2009)
, as first proposed in 
Kahneman's (1973)
 capacity model of attention. As set size increases and the share of the resources available to each item decreases, memory precision also decreases. In pure resource models the share of resources allocated to any item never goes to zero 
(Bays & Husain, 2008)
, regardless of set size, but in working versions of the model, like that of 
Bays et al. (2009)
, resource allocation is augmented with guessing and swap errors. The latter arise from misreports of distractor items at unprobed locations and are interpreted as failures of feature binding. Guessing and swap errors both increase the probability of very inaccurate responses on some trials and contribute probability mass to the tails of the error distribution. Importantly, slot models and resource models are not mutually exclusive: Resource limitations may exist within slot limits and are theoretically compatible with them. For example, the slots-plus-averaging model of 
Zhang and Luck (2008)
, which is related theoretically to the sample-size model discussed below, conceives of memory as a set of noisy, allocatable slots, and so can be viewed as a form of hybrid slots-plus-resources model.
The second wave of theories, comprises the variable precision models of 
Fougnie et al. (2012)
 and van den 
Berg et al. (2012
Berg et al. ( , 2014
 and the interference model of 
Oberauer and Lin (2017)
. These models sought to move beyond the basic slots versus resources dichotomy and to capture features of the error distributions that simpler forms of the models could not. Instead of assuming that items in memory are represented with a constant precision, the variable-precision models assume that precision varies randomly across items and trials. Fougnie et al. modeled variable precision with a wrapped t-distribution; van den Berg et al. modeled it with a power-law with a randomly varying exponent. Variable precision is closely related to the idea, mentioned earlier and developed in relation to the JP-CDM model below, that the drift rate in the diffusion model varies randomly across trials. Van den Berg et al.'s model replaces the idea of fixed slots with a probabilistic encoding process, such that the number of items encoded into VWM on any trial varies randomly according to a Poisson distribution with a given mean. Poisson encoding imposes a form of "soft slots" constraint that can be interpreted as expressing the metabolic cost of VWM encoding, as distinct from earlier models in which the number of slots was treated as a hard-wired property of the cognitive system. Van den 
Berg et al. (2014)
 reported that the variable precision model with Poisson item encoding performed better than competitor slot and resources model, including the slots-plus-averaging model, on 10 published data sets.
The interference model assumes that performance decreases with increasing set size, not because of limits in the storage capacity of VWM, but because items in memory interfere with one another, as assumed in other areas of working memory research 
(Oberauer et al., 2012)
, such that the amount of interference increases with the number of stored items.
Response accuracy in the model is represented by a Luce choice rule 
(Luce, 1959)
, in which the response strength of the cued item is divided by the sum of the strengths of competitor items. Although it is formalized in a different way, the response rule in the interference model endows it with similar properties to those of normalization models 
(Carandini & Heeger, 2012;
Reynolds & Heeger, 2009;
Smith & Sewell, 2013;
Smith et al., 2015)
, in which competitive interactions among items lead to divisive normalization of the item strengths, so that the strength of any item representation is divided by the sum of the strengths of other, active items. Smith and Sewell showed that a suitably-scaled competitive interaction model with divisive normalization implements the sample-size model. In one sense, then, models like the sample-size model, which is a resource model, and divisive normalization or interference models are not mutually exclusive, but can instead be seen as complementary ways to characterize the same phenomena.
The third wave of theories comprises the population code models, including the target confusability competition (TCC) model of 
Schurgin et al. (2020)
, the neural resource model of Bays and colleagues 
(Bays, 2014;
Schneegans & Bays, 2016;
Schneegans et al., 2016)
, and a miscellany of models that share properties of these models that 
Oberauer (2023)
 termed "measurement models." An illuminating analysis of these models was provided by 
Tomić and Bays (2024)
. Population code models assume that the memory representation of a stimulus consists of a uniformly-distributed array of noisy detectors, together with a tuning function that describes how strongly a detector at a particular location responds to stimuli at other locations. The location of the peak of the tuning function corresponds to the detector's preferred stimulus and the shape of the tuning function describes how its response decreases as a function of distance in stimulus space. For commonly-used stimuli like isoluminant colors, orientation, and direction of motion, the stimulus space is circular and the detectors are arranged around the perimeter of a circle ±π rad. (±180 • ). The tuning function in the TCC model was derived theoretically from 
Shepard's (1987)
 exponential law of generalization and estimated from perceptual similarity judgments using the scaling method of 
Maloney and Yang (2003)
. The similarity function is well described by a Laplace distribution, which consists of a pair of back-to-back exponential distributions, suitably scaled 
(Oberauer, 2023;
Tomić & Bays, 2024)
. (The Laplace distribution is sometimes termed the "double exponential" distribution, but this terminology is also used for the Gumbel distribution, which has distribution function exp(−e −x ), so is probably best avoided.) The outputs of the detectors in the TCC model are independently perturbed by Gaussian (normally-distributed) noise, and the response on any trial is given by the most strongly responding detector. These kinds of maximum-of-outputs, or, more briefly, MAX, models -which may be thought of as generalized signal detection models -are widely used to model decision accuracy in sensory psychophysics and attention 
(Eckstein et al, 2000;
Palmer et al., 2000;
Graham et al., 1987)
.
Schurgin et al. showed that the VWM error distribution can be predicted from the empirical tuning function and an amplitude parameter, which determines the peak response of the detector, which varies with set size. The theoretical interest of their work is they showed empirically that the shape of the tuning function and the amplitude are separable (our terminology, not theirs): The amplitude of the tuning function varied with set size but its shape, as characterized by the exponential similarity parameter, remained constant. A related result was reported earlier for a two-choice task by 
Lilburn et al. (2019)
, who studied VWM for the orientations of sinusoidal gratings. They found that accuracy was well-described by a combination of a separable tuning function, modeled, as is common in visual psychophysics, as a Gaussian density 
(Thomas & Gille, 1979)
, and a samplesize-dependent amplitude parameter. As in Schurgin et al.'s study, the tuning parameter (the Gaussian standard deviation σ) remained constant across stimulus exposure duration and set size while the amplitude parameter varied. It was not the case, as might have been predicted, that channel tuning worsened with increasing set size. Rather, across three different stimulus exposure durations and four different set sizes the channel tuning remained constant and only the amplitude, expressed as signal detection d ′ , changed.
The demonstrations of separable tuning and amplitude parameters in the studies of 
Schurgin et al. (2020)
 and 
Lilburn et al. (2019)
 are important theoretically because they identify invariants of the underlying memory representations -that is, properties that remain unaltered as set size or perceptual encoding changes. Outside of psychology, the search for invariants is an important part of the discovery of physical laws, because they identify properties of a physical system that remain unaltered as its superficial features, like the coordinates in which it is represented, change. Indeed, a deep theorem in classical mechanics, Noether's theorem 
(Noether, 1918)
, asserts that associated with every symmetry (i.e., invariant) of a physical system is a conservation law, and vice versa. The search for invariants of VWM is one of the main themes of this article, which we characterize via the parameters of the Jones-Pewsey distribution, as described below.
The neural resource model of Bays and colleagues is similar to the TCC model, in that it also consists of an array of noisy detectors and a tuning function, but the latter is a bellshaped, von Mises, distribution rather than a Laplace distribution and the noise is Poisson distributed rather than normally distributed and is assumed to reflect the firing rates in a population of Poisson neurons. As in the TCC, the tuning function describes how the firing rate of a neuron tuned to a particular stimulus attribute (color, orientation, direction, etc.) changes as a function of distance in stimulus space. The response rule in the model is described as "maximum likelihood decoding," which essentially means it is based on the point of maximum activity in the neural population, like the signal detection MAX model in the TCC model. There is a finite pool of neurons available to represent items in memory, which means the number of neurons available to represent an item is inversely proportional to set size. Because of the statistics of Poisson processes, the model predicts that decoding precision will follow a sample-size relationship (Schneegans et al., 2020). Smith 
2015
derived a diffusion decision model for two-choice VWM based on pairs of excitatory and inhibitory Poisson processes and showed that the drift rate follows a sample-size model while the diffusion coefficient, which describes moment-to-moment variability in evidence accumulation, remains constant. The model provides a neural resource interpretation of the sample-size diffusion model used to fit RT and accuracy data from a two-choice VWM task by 
Sewell et al. (2016)
. The work of Bays and colleagues implies that the same scaling relationship can be expected to hold in continuous-outcome VWM decisions as well. 
Tomić and Bays (2024)
 compared the neural resource model to several versions of the TCC model on four VWM tasks: a color task; an orientation task; a source memory task, in which subjects were cued with a colored dot and recalled the location at which it had previously been presented; and a task that used circularly-distributed abstract shapes developed by 
Li et al. (2020)
. They compared the neural resource model to two different versions of the TCC model, one in which the exponential tuning parameter was estimated from a perceptual similarity task, as was done by 
Schurgin et al. (2020)
, and another in which it was estimated directly from the VWM data. In both cases, the neural resource model performed better than the TCC model and they showed that the reason for its better performance was because of the difference in the tuning functions rather than the noise distributions. A version of the TCC model with a von Mises tuning function, Gaussian noise, and a signal detection MAX rule performed better than the original Laplace-tuning form of the model. Oberauer (2023) generalized these findings by considering population code models in which he factorially compared different tuning functions, noise distributions, and response rules. Specifically, he compared models with Laplace and von Mises tuning functions, and models with Gaussian and Gumbel noise, and models with either a signal detection MAX or a Luce choice model decision rule -the latter as assumed in the interference model. Like 
Tomić and Bays (2024)
, he found the best model combined a von Mises tuning function and a MAX decision rule, while the choice of noise function mattered relatively little. He highlighted the theoretically interesting special case of the Gumbel noise model, for which the MAX rule and the Luce choice model become identical. Proof of the equivalence of the two decision rules with Gumbel noise is due to Holman and Marley 
(Luce, 1994)
 and was given by 
Yellott (1977, Theorem 5)
. (Yellott calls the Gumbel distribution the "double exponential.") Oberauer reported that, to a good approximation, the width or precision of the tuning function appears to remain relatively constant across set size, in agreement with the findings of 
Schurgin et al. (2020)
 and 
Lilburn et al. (2019)
. As we show below, a somewhat different picture emerges when we combine a model of VWM representation with a diffusion decision process in the JP-CDM and we consider why this might be the case in the General Discussion.
An attractive features of the population code models, shared with the variable precision and interference models, is that they move the field beyond the slots versus resources debate, which has remained unresolved after more than two decades of research. A recent attempt to settle the issue was made by Adam et al. (2017) who used a free-recall, wholereport task in which subjects tried to recall every item in the display in whatever order they preferred. On average, recall was in order of inverse item strength (best to worst) and, for six item displays, accuracy for the last and the next-to-last recalled items was indistinguishable from chance, which Adam et al. interpreted as evidence for a fouritem slot limit. However, Oberauer (2022) tried to replicate 
Adam et al. (2017)
 (his Experiment 4) and found better-than-chance recall for the last two recalled items from six item displays. The reason for the difference between the two studies remains unclear, but may simply reflect different amounts of time on task: Oberauer tested his subjects over four one-hour sessions whereas Adam et al. tested them in a single 90 min session and it may be that subjects become better at encoding larger set sizes with practice. 1 
Bays et al. (2011)
 showed that VWM encoding is slower for large set sizes, implying it is not effortless, and we know from automaticity studies that effortful tasks can become less so with practice 
(Shiffrin & Schneider, 1977)
. Graded slot-like behavior as a function of changes in encoding efficiency would only be possible if the number of items encoded reflects resource-dependent properties of the cognitive system rather than hard slot limits, which should be immune to practice effects.
Graded slot-like behavior is consistent both with variable precision models with Poissondistributed item encoding, as proposed by van den Berg et al. 
2014
, and with population code models. The latter assume that performance depends on the height of the peak of the encoding function relative to the background noise in the detector array. Poorly-encoded stimuli will be associated with lower amplitudes and their memory representations will tend to become submerged in the noise floor of the detector array, leading to near-chance performance. This is a graded rather than an all-or-none phenomenon, and if encoding quality varies randomly among items in a display on individual trials, as proposed by variable precision models, then individual differences in encoding efficiency could either produce chance performance on the last-recalled items from large displays, as found by 
Adam et al. (2017)
 or poor, but better-than-chance performance, as found by 
Oberauer (2022)
.
Evidence for graded slot-like behavior was provided by a study by 
Donkin et al. (2016)
, who investigated performance on a change-detection task in which set sizes of 2, 4, 6, and 8 items were either blocked across sessions or randomized across trials. They found stronger evidence for item-capacity limits when set sizes were randomized than when they were blocked and argued that blocked set sizes made subjects more "resource-like" whereas unblocked set sizes made them more "slot-like." The message from their study seems to be that subjects are more likely to be able to encode large set sizes if they are predictable than if they are unpredictable. The estimated item-capacity parameter in Donkin et al.'s model was in the range of three to four items, suggesting some degree of underlying slot-like 1 Partial support for this conjecture comes from an unpublished study from our laboratory by King (2022), which used the procedure of Adam et al. 
2017
and Oberauer's (2022) Experiment 4. King collected data over three one-hour sessions and, like Oberauer, found better-than-chance performance for the last two recalled items from six-item displays.
behavior. Its context-dependence is inconsistent with a hard-slots limit, but is consistent with a soft-slots constraint like the one in the variable precision model, in which the average number of items in VWM reflects the cognitive effort of encoding. We report results related to those of Donkin et al. in our Experiments 1 and 2, in which we found that VWM precision depends not only on the set size on a trial but also on the range of set sizes within a block.


The Current State of the VWM Literature
VWM research over the last few decades has yielded a body of well-replicated findings and sophisticated theory, but also shows some significant limitations. For our purposes, the most important of the established findings is that heavy-tailed error distributions do not necessarily imply a guessing process and do not require an item-capacity model to explain them -although they are broadly consistent with such models. Variable precision models, interference models, and population code models all have mechanisms that can predict heavy-tailed error distributions without item-capacity limits. Resource models can also predict these distributions if they are augmented with swap errors and guessing. The next most important finding is that, if there are item-capacity limits, then they are more likely to be soft limits, as characterized by Poisson encoding in van den Berg et al.'s (2014) model, rather than hard limits, as the colloquial usage "slot model" would imply. Item-capacity limits, when they appear, are more likely to reflect the cognitive effort of the encoding process rather than any hard-wired property of the memory system and are likely to be affected by variables like practice and features of the context, like predictability, as shown by 
Donkin et al. (2016)
.
The first limitation is that the literature has become increasingly paradigm-bound. Contemporary theories of VWM have become, in practice, theories about the shapes of error distributions in continuous-outcome tasks. An exception is 
Schurgin et al. (2020)
, who showed that signal detection d ′ from a 2AFC VWM task using perceptually nonconfusable stimuli scales linearly with the TCC amplitude parameter from a continuous-outcome VWM task. Apart from them, however, most studies have not tried to characterize the relationship between different kinds of decision task or to analyze its theoretical implications. In his original paper on the diffusion model, 
Ratcliff (1978)
 deprecated the paradigm-bound memory research of the era and proposed the diffusion model as a retrieval mechanism that could unify multiple paradigms. We propose the JP-CDM model in the spirit of the ideas in his article.
The second limitation is that existing theories do not distinguish the independent contributions made by memory and decision processes to retrieval errors. Some studies have acknowledged the role played by variability in either perceptual or motor processes and have attempted to correct the distribution of retrieval errors in some way. Van den Berg et al. (2014) assumed a component of motor error is added to the retrieval error to determine the total error on any trial. Tomić and Bays (2024) assumed that perceptual variability during stimulus encoding affects the similarity function and used an empirical estimate of perceptual variability to correct for it. However, many other studies have more or less tacitly proceeded as if the empirical error distribution were a veridical representation of the variability of memory. These approaches suffer from the limitation that they lack a well-articulated theory of the decision process that specifies how the decision process and memory jointly determine observed precision.
The third limitation, which is intimately related to the preceding two, is that existing models of VWM retrieval explain only one margin, or dimension, of what is in reality a 2D joint distribution of decision outcomes and RTs. Studies of the RT and accuracy of continuous-outcome decisions have shown the joint distributions possess a rich empirical structure that is highly constraining for model testing 
(Kvam, 2019;
Kvam et al., 2022;
Ratcliff, 2018;
Smith et al., , 2022
. Models like the JP-CDM we develop here assume that across-trial variability in memory representations determines the drift rate of a diffusion process. This combines with diffusive variability in the evidence accumulation process to determine both the speed and accuracy of responding. The critical difference from existing theories of VWM is that, even if there were no variability in VWM -that is, if the value of the stimulus attribute retrieved from memory were a perfectly faithful representation of the presented stimulus -then the JP-CDM would still predict trial-to-trial variability in the speed and accuracy of responding. Indeed, under these circumstances it predicts von Mises distributions of decision outcomes 
(Smith, 2016
(Smith, , 2019
Smith et al., , 2022
, as discussed below. This property of the model is striking because many existing VWM models represent the error distribution as a mixture of von Mises components. Consequently, there is a natural interlock between models like the variable precision model that assumes a continuous distribution of von Mises error components and the JP-CDM, which predicts von Mises components from the properties of the evidence accumulation process. The ability of the JP-CDM to distinguish independent contributions of memory variability and decision process variability comes from the additional constraints provided by the joint distributions of error and RT.


The Sample-Size Model
The sample-size model can be thought of as a form of resource model in which the resource is a set of independent, noisy, evidence samples that are recruited to represent stimuli. It can be given a neural interpretation by assuming that the evidence samples characterize firing rates in a population of Poisson neurons, as proposed by 
Smith (2015)
 and Bays and colleagues 
(Bays, 2014;
Schneegans et al. 2020)
. In VWM applications, memory is resource-limited because the population of samples (neurons) available to represent items is finite and fewer samples are available to represent each item as the number of items in memory is increased. The predictions of the model follow from elementary sampling theory, specifically, from the fact that the standard error of the mean of a sample is inversely proportional to the square root of the sample size. In signal detection applica-tions to VWM, the model predicts that d ′ for an m-item display will be proportional to d ′ for a single-item display, with a constant of proportionality equal to the inverse square-root of the set size, because the samples must be distributed among m items. Symbolically,
d ′ m = d ′ 1 √ m .
(1)
The model has found a diverse range of applications in psychology, including cross-modal attention 
(Bonnel & Hafter, 1998;
Bonnel & Miller, 1994;
Lindsay et al., 1967;
Taylor et al., 1967)
, divided attention 
(Eckstein et al., 2000;
Palmer, 1990;
Shaw, 1980
Shaw, , 1982
Shaw, , 1984
, and object-based attention 
(Corbett & Smith, 2017)
. Corbett and Smith, for example, found a √ 2 cost in an object-based attention task when decisions required attentional selection of a pair of simultaneous targets into VWM, compared to when they required only one target. The earliest account of the sample-size idea we are aware of was given by 
Swets et al. (1959)
 who reported a square-root law improvement in auditory detection sensitivity in a "multiple look" task as a function of the number of presentations of a stimulus, which they attributed to integration of information across samples. 
Lilburn and Smith (2020)
 adapted this idea in a multiple-look VWM experiment in which some stimuli were presented in either the first, the second, or both of two observation intervals, each of 50 ms or 100 ms duration, presented 400 ms apart. VWM sensitivity, as measured by d ′ m , showed a √ 2 improvement for stimuli that were presented in both intervals as compared to those that were presented in only one of them, while performance as a whole followed the sample size model, where m was defined as the total number of filled display locations across the two intervals. These results are striking in showing that the sample-size model successfully predicts both the costs of increasing set size and the benefits of redundancy in the same experimental task. 
Palmer (1990)
 found evidence for a sample-size model in a divided attention task using the poststimulus probe method, in which the probe both identified the target and acted as a reference standard for a 2AFC decision about it. He found that difference thresholds, defined as the minimum stimulus difference needed to achieve 79% accuracy using an adaptive staircase procedure, followed a sample-size model for displays of 1, 2, and 4 items. Palmer attributed the set size effect to divided attention rather than VWM capacity, but noted that divided attention sets a limit "on how much visual information is remembered from a brief presentation." 
Sewell et al. (2014)
 argued that the sample-size relationship is a measure of VWM capacity rather than attentional capacity per se and showed that Equation 1 holds both for simultaneous presented items, like those in 
Palmer's (1990)
 study, and also for successively presented items, in which only one item is present in the display at a time. Indeed, there was a slight statistical advantage for simultaneously presented items, but the effect was very small (the error bars were almost completely overlapping), which is the opposite to what would be predicted if the set size effect were due to divided attention. There has been some debate about whether the simultaneous versus successive manipulation can effectively distinguish the capacity limitations of attention from those of VWM 
(Bundesen, 2018;
Sewell et al., 2018)
, because of the confounding effects of attentional dwell times 
(Duncan, 1994)
, which may result in capacity being allocated to empty display locations. However, the virtually identical estimates of d ′ m found for simultaneous and successive presentation by 
Sewell et al. (2014)
 makes an attentional capacity explanation unlikely. The dwell-time argument seems to equate the attentional load to the number of potentially filled rather than actually filled display locations, in which case the model would not predict a set size effect at all.
The slots-plus-averaging model of 
Zhang and Luck (2008)
 is related theoretically to the sample-size model, and appears to have been inspired in part by 
Palmer's (1990)
 article, but the two models should not be confused. The slots-plus-averaging model is like the sample-size model except that the number of samples is identified with the number of slots, estimated by Zhang and Luck to be around four. When there are fewer items in the display than there are slots, the model assumes that redundant copies of some display items will be stored in the available vacant slots, leading to a √ 2 improvement in precision for items that are stored twice. We can think of the model as a "coarse-quantized sample-size model," to distinguish it from the original. Zhang and Luck made the coarse-quantization assumption because of the way in which VWM precision changed (or failed to change) with a predictive precue. They expected that precision for cued items would be high because all of the samples would be concentrated on the cued item, but they found only a small increase compared with neutral (uncued) items. That cues did not improve precision in the expected way suggested to them there must be some resolution limit on precision, which they argued was captured by the coarse quantization assumption, which limits the precision with which any item can be represented. From the perspective of the theory developed in this article, what is missing from Zhang and Luck's analysis is a recognition of the role played by decision processes. As described above, in the JP-CDM the effect of noise in the decision process means that errors will follow a von Mises distribution, even if mnemonic fidelity is perfect. To put this in other terms, the slots-plus-averaging model equates mnemonic precision with the empirical precision of the error distribution whereas the JP-CDM views mnemonic precision and empirical precision as different things.
The slots-plus-averaging model has been tested empirically and found to perform more poorly than competitor models 
(Fougnie et al., 2012;
Oberauer & Lin, 2017;
van den Berg et al. 2014
), as we noted above. However, these tests should not be mistaken for tests of the sample-size model in general, but only of the coarse-quantized version of it. Apart from the comparatively poor empirical performance of the slots-plus-averaging model, 
Sewell et al. (2014)
 highlighted an unattractive theoretical feature of it, namely, that it seems to require a complex bookkeeping system to implement the proposed storage and retrieval process. To obtain the proposed benefits of redundant coding, the bookkeeping system would need to tag each slot with the identity of the item stored there, keep track of the relationship between slots and items during the retention interval, and then reassemble a representation of redundantly coded items at recall by combining the contents of the slots that contain the item. It is difficult to envisage the kind of neurocomputational mechanism that could implement such a storage and retrieval scheme.


The Power Law Generalization
The sample-size model provides a parsimonious account of the set-size effect in many 2AFC VWM studies, but does not hold universally. 
Bays and Husain (2008)
, for example, found that the set-size effect in a change discrimination task was better described by a power law relationship, which predicts a larger set-size effect than the one predicted by the sample-size model. Bays and Husain used the inverse standard deviation of the psychometric function relating stimulus strength to accuracy as a measure of precision instead of d ′ . The two measures should scale in the same way with memory load because the standard deviation of the psychometric function corresponds (at least approximately) to the standard deviation of the noise distribution in the denominator of d ′ . Expressed in signal detection terms, the power law model holds that
d ′ m = d ′ 1 m −β ,
(2)
where β is a free exponent, which Bays and Husain estimated to be β = 0.74. The samplesize model is a special case of the power law model with β = 0.5 because m −0.5 = 1/ √ m.
Bays and Husain's estimate of the exponent is appreciably greater than this. There have a been numerous reports of set-size effects using 2AFC, change detection, and continuous-outcome tasks, some of which have found a sample-size relationship and some of which have found effects that are better described by a more general power law. As noted above, 
Lilburn et al. (2019)
 found that the set-size effect for 1 to 4 items in a 2AFC fine orientation discrimination task with grating stimuli was well-described by a tunedchannel model with a separable memory strength parameter that followed the sample-size model. 
Sewell et al. (2014)
 reanalyzed data from a change detection task by 
Vogel et al. (2006)
 and again found the set-size effect for sets of 1 to 4 items was well-described by the sample-size model. 
Donkin et al. (2016)
 fitted a resources-within-slots model to data from a change-detection task that used displays of up to eight items and found the set-size effect in d ′ was well-described by the power law model of Equation 2. They did not report an exponent in their article, but the estimated value was very close to 0.5, which is the sample-size model 
(Donkin, personal communication, April 2, 2016)
. In contrast, 
Smith et al. (2016)
 ran a version of the 2AFC VWM task of 
Sewell et al. (2016)
 using a phase discrimination judgment in which subjects decided whether Gaussian-filtered, luminous disks were darker on the left or the right side. Unlike the orientation judgment used by Sewell et al., visual search for phase targets is attention-demanding and likely requires serial search 
(Thornton & Gilden, 2007)
. Smith et al. characterized the set size effect in the phase discrimination task using a generalization of Equation 1 in which they treated m in the denominator as a free parameter that they estimated from the data, which they termed the "effective set size." For displays of 1, 2, 3, and 4 items the estimated effective set sizes were 1, 3.2, 5.5, and 7.7, which corresponds fairly closely to a power law with β = 0.75, in agreement with 
Bays and Husain (2008)
.
Estimates of the set-size effect in continuous-outcome tasks, using a range of different set sizes, have yielded values that tend to be larger than the one predicted by the samplesize model. 
Schurgin et al. (2020)
 commented that the set-size effect in their data, as characterized by the d ′ m amplitude of the similarity function in the TCC model, appear to follow a power law. They did not report an exponent but the values of d ′ m for sets sizes 1 to 8 are well-described by a power law with an exponent of around β = 0.75. Van den Berg et al. 
2014
used Fisher information to characterize the set-size effect in their analysis of the variable precision and related models. Fisher information is a variance rather than a standard deviation measure, so the predicted set-size effect should scale as 1/m rather than 1/ √ m. For the better models analyzed by van den Berg et al., the exponents were in the range 1.25 to 1.67, which corresponds to β ≈ 0.6 to 0.8 when expressed in standard deviation instead of variance units. As pointed out by 
Oberauer and Lin (2017)
, unlike the sample-size model, which provides a process account of the set-size effect, the power law model is unattractive because the exponent is a free parameter with no clear theoretical interpretation. Oberauer and Lin discussed previous attempts to explain why the empirical exponent should be greater than the sample-size model and argued they are unconvincing. As an alternative,  proposed that the exponent is an index of the attentional demands of the stimuli used in the task and showed that larger exponents are predicted when processing resources, expressed as a proportion of the available evidence samples, are allocated unequally across items in VWM rather than equally.  reported data from five 2AFC VWM experiments, rank-ordered according to the attentional demands of the stimuli they used. They found the magnitude of the set-size effect across the five experiments was well-predicted by the a priori attentional demands of the stimuli and spanned a range from β = 0.5 for the least demanding to β = 0.75 for the most demanding. They showed that larger exponents were predicted theoretically by an attention-weighted sample-size model. The model, which is related to the weighted sample-size model of divided attention of Bonnel and colleagues 
(Bonnel & Hafter, 1998;
Bonnel & Miller, 1994)
, assumes that when stimuli are attentionally demanding one of them captures attention and receives some fraction, c, of the available n samples. The remaining fraction, 1 − c, is divided equally among the other m − 1 items. When c = 1/m the samples are allocated equally and the model reduces to the sample-size model and predicts β = 0.5. As c increases above this, the exponent also increases. If stimuli in a VWM task are presented simultaneously, then attention capture occurs essentially at random. If stimuli are presented sequentially, as in one of the phase-discrimination VWM experiments reported by Smith et al. 
2016
, then the first item presented captures attention and receives a disproportionate share of the samples resulting in a large primacy effect.  showed that, with simultaneous presentation, deviations from the sample-size model were found only for exposure durations longer than about 150 ms, which is the time course of the exogenous attentional orienting system. In the attention weighted sample-size model, this is assumed to be the minimum time needed for attention capture to occur. Unlike the general power law model, which is atheoretical, the attention-weighted sample-size model provides a process interpretation of why power law deviations from the sample-size model should occur. The deviations are predictable from the attentional demands of the stimuli used in the VWM task, rank-ordered according to external criteria, and exhibit a time course consistent with the time course of the attentional mechanism assumed to underlie the allocation of samples to stimuli. In the modeling work described below we consider both sample-size and power law models of the set size effect, which we relate to the properties of the Jones-Pewsey memory distribution. 
Figure 1
 shows the main elements of our modeling framework. 
Figure 1a
 shows the CDM, which we use to model the retrieval of evidence accumulation from VWM. The model was developed in articles by 
Smith (2016
Smith ( , 2019
 and 
Smith et al. ( , 2022
Smith et al. ( , 2023
. The articles by  and 
Smith et al. (2022)
 describe applications to continuousoutcome decisions about noise-perturbed color patches and direction of coherent motion in random-dot kinematograms, respectively. 
Figure 1b
 shows the Jones-Pewsey distribution, which we use to model across-trial variability in encoded stimulus representations in VWM. We discuss these components of the model and the way in which they fit together in the next two sections.


Diffusion Model of Continuous-Outcome Decisions


The Circular Diffusion Model
In the CDM, evidence is accumulated by a two-dimensional Wiener diffusion process, with Cartesian coordinates, X t = (X 1 t , X 2 t ) ′ , on the interior of a disk of radius a. In what follows, we use bold-face symbols to denote vector or matrix valued quantities, primes to denote matrix transposition, and superscripts to index coordinates. We denote random variables by upper-case Roman symbols, and constants and deterministic functions by Greek or lower-case Roman symbols. The growth of evidence over time is characterized by a vector-valued stochastic differential equation,
dX t = µ dt + σ dW t ,
(3)
where
dW t = (dW 1 t , dW 2 t ) ′
, is the differential of a 2D Wiener, or Brownian motion, process. The two components of dW t describe the horizontal and vertical parts of the random change in the process X t during a small interval of length dt. The information in the stimulus is represented by a vector-valued drift rate, µ = (µ 1 , µ 2 ) ′ , shown in the figure as an arrow at π/4 rad (45 • ). The rate at which the process diffuses towards the boundaries is represented by a dispersion matrix σ = σI, where I is a 2 × 2 identity matrix. The dispersion matrix describes a Wiener process composed of two independent components, each with infinitesimal standard deviation σ (diffusion coefficient σ 2 ). In polar coordinates, the drift rate has a length or norm, µ = µ 2 1 + µ 2 2 , and a phase, or polar, angle, θ µ = arctan(µ 2 /µ 1 ). The polar angle determines the average direction in which evidence diffuses and the norm determines the average rate at which it does so. Psychologically, the polar angle represents the encoded stimulus identity and the norm represents the encoded stimulus strength or quality. of radius a by a 2D Wiener process, X t , with components (X 1 t , X 2 t ) ′ . The point at which the process hits the bounding circle, X θ , is the decision outcome and the time, T , taken to hit it is the decision time. The drift rate is vector-valued, µ, with components (µ 1 , µ 2 ) ′ , with norm µ and direction, or polar angle, θ µ . The polar angle represents the encoded stimulus identity and the norm represents the quality of the encoded representation. (b). Jones-Pewsey distribution of across-trial variability in drift rate polar angles. The curves in the figure, from least to most peaked, are for ψ = 0, -0.5, -1.0, -1.25, all with κ µ = 2. The ψ = 0 case is a von Mises distribution.
At the beginning of a trial, the process starts at the center of the disk, X 0 = (0, 0) ′ , and on presentation of a stimulus, diffuses until it hits a point on the bounding circle. The radius of the circle, a, represents the decision criterion for the task and is assumed to be under the decision-maker's strategic control, like boundary separation in the two-choice diffusion model. The irregular trajectory in 
Figure 1
 represents the sample path of the process (i.e., the accumulating evidence) on a single experimental trial. The hitting point on the boundary is denoted by X T , where the use of capitalized symbols for both the process and the time subscript indicates that the hitting point and hitting time are both random variables. This is represented in the notation in the figure as a pair, (X θ , T ), which denotes the hitting time and the hitting point as independent variables, where θ is the polar angle of the hitting point. The polar and Cartesian representations of the hitting point are related by the expressions X 1 T = a cos θ and X 2 T = a sin θ. To characterize the joint distribution of decision outcomes and decision times explicitly, we denote by dP t (θ) the joint probability density that a 2D Wiener diffusion process with drift rate µ hits the bounding circle a at the point X T = (a cos θ, a sin θ) ′ at a random time T . This density is the product of two components, denoted Z t (X T ) and dP t (a),
dP t (θ) = Z t (X T )dP t (a).
(4)
The component dP t (a) is the probability density that a 2D, zero-drift process, started at the center of the circle at time zero, hits a point on its circumference at time t, and takes the form
dP t (a) = σ 2 2πa 3 ∞ k=1 j 0,k J 1 (j 0,k ) exp − j 2 0,k σ 2 2a 2 t .
(5)
Equation 5 is obtained from the first-passage time probability density function for the 1D Euclidean distance, or Bessel, process, which characterizes the Euclidean distance of a zero-drift Wiener process from its starting point 
(Hamana & Matsumoto, 2013)
. If a zerodrift Wiener process starts at the origin then its first-passage time distribution through a circular boundary will be circularly symmetrical and identical, up to a scaling factor of 2πa, to that of the 1D Bessel process. The denominator of Equation 5 includes a factor 2πa that transforms the first-passage time function for the 1D Bessel process into that of a 2D zero-drift Wiener process whose probability mass around the circumference of the response circle integrates to unity when it is combined with Z t (X T ) in Equation 4. When the 2D model is evaluated by integrating over outcomes, θ, in polar coordinates, the integral involves an area element of the form adθ. In previous presentations of the model we have absorbed the polar scaling factor into the implementation in part or in full, but here we write it explicitly. In Equation 5, J 1 (x) is a first-order Bessel function of the first kind 
(Abramowitz & Stegun, 1965, p. 360)
 and the j 0,k terms are the zeros of a Bessel function of the first kind of order zero, J 0 (x), that is, they are the points at which the function crosses the x-axis. The terms J 1 (j 0,k ) in the denominator represent values of J 1 (x) evaluated at the zeros of J 0 (x). Note that dP t (a) is a function of time only and is independent of the hitting point on the criterion circle. Mathematically, it depends only on the diffusion coefficient, σ 2 , and the radius of the criterion circle, a. The distribution of hitting points on the criterion circle is characterized by the second term, Z t (X T ), given by the Girsanov change-of-measure theorem 
(Karatzas & Shreve, 1991)
, which takes the form
Z t (X T ) = exp 1 σ 2 (µ • X T ) − 1 2σ 2 µ 2 t ,
(6)
where (µ • X T ) = aµ 1 cos θ + aµ 2 sin θ is the dot product of the drift rate vector and the vector of coordinates of hitting points on the boundary. The function Z t (X T ) factorizes into a product of two exponential terms: one that depends on time and is independent of the hitting point and another that depends on the hitting point and is independent of time.
As noted above, the CDM predicts that the decision outcomes are von Mises distributed. The von Mises distribution has a density function f (θ; κ, ϕ), where κ is a precision or concentration parameter and ϕ is the distribution center,
f (θ; κ, ϕ) = e κ cos(θ−ϕ) 2πI 0 (κ) ,
(7)
where I 0 (κ) is a modified Bessel function of the first kind of order zero. The relationship between the CDM and the von Mises distribution is obtained by writing the Cartesian coordinates of the hitting point on the bounding circle in Equation 6 as X 1 T = a cos θ and X 2 T = a sin θ and the coordinates of the drift rate vector as µ 1 = µ cos ϕ and µ 2 = µ sin ϕ. This allows us to write the outcome-
dependent part of Z t (X T ) in Equation 6 as exp 1 σ 2 (µ • X T ) = exp a µ σ 2 (cos θ cos ϕ + sin θ sin ϕ) .
(8)
The elementary trigonometric formula for the difference of cosines shows that the righthand side of Equation 8 is the same as the numerator of the von Mises density in Equation 7 with precision parameter
κ = a µ σ 2 .
(9)
The modified Bessel function in the denominator of Equation 7, multiplied by 2π, makes the function a proper probability distribution and can be obtained by integrating the remaining terms in Equation 6 over outcomes and time as described by 
Smith (2016)
. Equation 9 provides a theoretical decomposition of the von Mises precision parameter into components of the decision process. The predicted precision is equal to the decision criterion multiplied by the drift rate norm divided by the diffusion coefficient. Otherwise put, it is equal to the amount of evidence used to make a decision multiplied by the quality of evidence in the stimulus divided by the noisiness of the evidence accumulation process. When there is across-trial variability in drift rate, Equation 6 is replaced by an expression whose form depends on whether the components of the distribution of drift rates are independent or correlated. When the components are independent and normally-distributed, with means ν 1 and ν 2 and standard deviations η 1 and η 2 , the expression has the form 
(Smith & Corbett, 2019;
Smith et al., , 2022
. A related expression was derived by 
Tuerlinckx (2004)
 for a two-choice diffusion process with normally-distributed drift rates. As in Equation 6, X T denotes the locus of hitting points on the criterion circle, with components X 1 T = a cos θ and X 2 T = a sin θ. The overbar notation suggests averaging or marginalization and conveys the idea that the distribution of hitting points and hitting times is obtained by marginalizing Equation 6 across a distribution of drift rates. When the components of drift rate are correlated rather than independent Equation 6 is replaced by a more complex expression given by 
Smith (2019)
. The distribution of drift-rate variability in Equation 10 is a 2D generalization of the distribution of drift rates in the two-choice model and, as in that model, represents trial-totrial variability in the evidence entering the decision process. When η 1 = η 2 , the contours of equal likelihood of the drift-rate distribution are circular and the CDM is circularly symmetrical. If η 1 = η 2 then the contours of equal likelihood are ellipses rather than circles. When the mean drift-rate vector aligns with the positive x-axis, such that ν = (ν 1 , 0), we say the model is in canonical orientation. In canonical orientation, η 1 is the radial component of drift rate variability and η 2 is the tangential component. The radial component characterizes variability along the length of the mean drift rate vector while the tangential component characterizes variability perpendicular to its length. Psychologically, the radial component characterizes variability in the encoded stimulus quality and the tangential component characterizes variability in the encoded stimulus identity. Even when the model is not in canonical orientation, the first and second components of η = (η 1 , η 2 ) can still be identified with the radial and tangential components of drift rate variability by using the rotationally invariant form of the model in 
Smith (2019)
, in which a general expression forZ t (X T ) is obtained by a coordinate transformation of Equation 7. The reader is referred to that article for details of the resulting, somewhat complex, expression.
Z t (X T ) = 2 i=1 1 (η i /σ) 2 t + 1 exp − ν 2 i 2η 2 i + [X i T (η i /σ) 2 + ν i ] 2 2η 2 i [(η i /σ) 2 t + 1] ,
(10)


Jones-Pewsey Distribution of Memory Representations
The second component of the JP-CDM is a model of across-trial variability in memory representations. We follow  and use a distribution proposed by 
Jones and Pewsey (2005)
. Their distribution is a three-parameter family, defined on a circular space, with probability density
f (θ; κ µ , ϕ, ψ) = [(cosh(κ µ ψ) + sinh(κ µ ψ) cos(θ − ϕ)] 1/ψ 2πP 1/ψ (cosh(κ µ ψ)) ,
(11)
examples of which are shown in 
Figure 1b
. In this equation, θ is the polar angle of the stimulus, ϕ is its center, κ µ is the precision, and ψ is a parameter that controls the distribution shape. The term P 1/ψ (•) in the denominator, which makes the function a probability distribution, is an associated Legendre function of the first kind of degree 1/ψ and order 0 
(Gradshteyn & Ryzhik, 2007, 8.7-8.8)
. When the degree of the function is not an integer, as here, P 1/ψ (•) may be evaluated in terms of gamma functions and hypergeometric functions 
(Gradshteyn & Ryzhik, 2007, 8.702)
. In CDM applications of the JP distribution, it is used to describe across-trial variability in the encoded stimulus identity. Computationally, the normally-distributed tangential component of drift rate variability in Equation 10 is replaced by a Jones-Pewsey (JP) distribution. The reason for wanting the greater generality of the JP distribution is it provides a way of characterizing how memory representations change across trials as a function of set size and other experimental manipulations.  used the JP distribution to model encoding failures in a continuousoutcome, eye-movement decision task using colored patches as stimuli in which the colors were perturbed by chromatic noise. Response errors increased with increases in noise and, when noise levels were high, heavy-tailed distributions of errors were obtained, reminiscent of those obtained in VWM studies with large set sizes. An example of this kind of distribution, together with the JP-CDM fit for one condition for one subject (the high noise condition for S1), is shown in 
Figure 2a
. See 
Figure 9
 of Smith et al. for other examples. 
Figure 1b
 shows examples of the JP distribution with fixed precision κ µ = 2.0 and the shape parameter, ψ, varying from 0 to -1.25. When ψ = 0 the distribution reduces mathematically to the von Mises distribution 
(Jones & Pewsey, 2005)
, which is the least peaked, bell-shaped density in 
Figure 1b
. As ψ becomes more negative the distribution becomes more leptokurtic, with a central peak of probability mass and a heavy tail. When ψ becomes very large and negative (e.g., ψ = −2.0) and precision is high, the distribution approximates a single spike of probability mass (i.e., a Dirac delta function), concentrated at ϕ.
Smith et al. (2020) evaluated two models of encoding failures. Their first model was a mixture model comprising a high-precision central component and a uniformly-distributed component to represent trials on which encoding fails. The mixture model is analogous to VWM models comprising a mixture of memory-based responses and guessing. The second model was a JP model that described the drift rates on encoding failure trials and regular trials using a single distribution. Because of its three-parameter form the JP distribution can describe performance on both low-noise trials, in which encoding failures are negligible, and high-noise trials, in which encoding failures occur regularly and the distribution of errors has high tails extending out to ±π rad.
In VWM applications of the JP distribution, the normally-distributed, tangential component of drift rate, represented by the η 2 term in Equation 10, is replaced by the JP distribution of Equation 11. We have written the precision parameter of the JP distribution as κ µ to emphasize the fact that it represents tangential, or angular, variation of the drift rate, µ, in the CDM; it is not the precision of the empirical error distribution. The JP distribution is not intended to represent any particular theoretical model of VWM but it expresses properties shared by most if not all of them -namely, that the distribution of memory representations has a central peak flanked by high tails. In slots models the tails arise from guessing; in variable precision models they arise from across-trial variability in item representations; in population code models they arise from background noise in the detector array. We use the JP distribution in order to avoid making a commitment to any of these competing models, which allows us to focus on the role of decision processes in VWM retrieval.
In addition to tangential, or angular, variability in drift rate, the JP-CDM has a second component of drift rate variability, represented by the parameter η 1 , which describes radial variability in the drift-rate vector. It can be viewed, with some theoretical looseness, as variability in the drift-rate norm, µ , represented by the length of the arrow in 
Figure 1a
, with the proviso that the norm cannot be negative but the radial component of drift-rate variability can be so if it is normally distributed. We have retained the normality assumption for the sake of computational efficiency, because Equation 10 provides an explicit expression for the associated component ofZ t (X T ), which would otherwise need to be integrated numerically. For the parameter estimates we report here this property creates no interpretive difficulties. 
Smith et al. (2022)
 described an application of the CDM to a random-dot motion task in which a normally-distributed radial component of drift-rate variability predicted the bimodal distributions of errors found for some participants. In general terms, the radial component of drift-rate variability characterizes across-trial variability in the quality of stimulus information entering the decision process. This is distinct from variability in the tangential component of drift rate which describes across-trial variability in the identity of the stimulus information. The two components of drift-rate variability have different meanings and different theoretical interpretations. Even if mnemonic precision were perfect, which would be represented in the model by zero tangential variability (i.e., a JP distribution with ψ approaching -2.0 in the JP-CDM), there may nevertheless be variability in the quality of the representation, which is characterized by radial variability.
In models that only seek to characterize error distributions, the distinction between the radial and tangential components of drift-rate variability does no predictive work, but be-comes important when we are seeking to model RT. As shown in 
Figure 1
, the decision time is the time required for the accumulating evidence to hit the bounding circle, a (technically, the first-passage time of the process X T through a). When there is no drift rate variability, the model predicts that the decision time will be independent of the decision outcome -a property shared with the two-choice Wiener process and sequential-probability ratio random walk models 
(Luce, 1986)
. Under these circumstances, the RT properties of the model are completely characterized by the marginal distribution of decision times, an example of which is shown in the right-hand panel in 
Figure 2a
. The predicted distribution of RTs in 
Figure 2a
 was obtained as the sum of the decision time and a uniformly-distributed nondecision time with mean T er and range s t .
When there is variability in the radial component of drift rate, the model predicts a continuous-outcome version of the slow-error property of the two-choice Wiener diffusion model 
(Ratcliff & McKoon, 2008)
. In difficult two-choice tasks in which accuracy is stressed, error RTs are typically longer than correct RTs 
(Luce, 1986;
. In the two-choice model, errors are more likely to occur when the absolute magnitude of the drift rate is low. RTs on such trials are likely to be longer than on trials on which drift rate is high, so the model predicts slow errors. The continuous-outcome form of the slow-error property is represented in 
Figure 2b
, which shows a Q × Q (quantile-byquantile) plot of the joint distribution of errors and RTs. In the plot, selected quantiles of the conditional distributions of RTs (the .1, .3, .5, .7, and .9 quantiles) have been plotted against quantiles of the error distribution. The plot has a characteristic dish-shaped form, in which RTs for inaccurate responses (i.e., responses close to ±π rad.) are longer than those for accurate responses (responses close to 0 rad.). As in the two-choice diffusion model, the slow-error property is most evident in the tails of the RT distribution (the .7 and .9 quantile RTs); the leading edge (the 0.1 quantile RTs) shows only a small effect. The radial component of drift rate variability determines the magnitude of the predicted slow-error effect. When η 1 = 0 the predicted Q × Q plot is flat and the RT properties of the model can be characterized completely by the marginal distributions. The fit shown in 
Figure 2b
 was obtained with a moderate value of η 1 . One of the attractive features of the CDM, which results from the fact that the model predicts both decision outcomes and decision times, is that it provides separate estimates of across-trial variability in the quality and the identity of stimulus representations in VWM.


Stimulus Anisotropies
As well as predicting marginal and joint distributions of errors and RT, the third point of contact between the model and data is to variations in RT and accuracy as a function of the location of the stimulus in stimulus space (here color space), which we term "stimulus anisotropy." In the case of color, anisotropies have been linked theoretically to the categorical properties of nameable colors. Color category effects have been found in long-term memory (Persaud & Hemmer, 2016), short-term memory (Bae et al., 2015; Hardman et al., 


2017)
, and in perceptual decisions with no retention interval 
(Bae et al., 2015)
. Categorical effects have also been found in memory for random dot motion 
(Blake et al., 1997)
. Such effects have typically been ignored in the VWM literature, although they are compatible with the theoretical framework of variable precision models, which assume variability in precision across items and trials. However, these model do not distinguish between systematic and random sources of across-trial variability. Although categorical effects are not our main focus here, we believe it is principled to include them in the model to try to distinguish them from other sources of variability. To that end, we use the color category model for continuous-outcome decisions about noisy color patches described by  and adapted for decisions about random dot motion in a 3D (spherical) diffusion model by 
Smith et al. (2022)
. The properties of the category model are shown in 
Figure  3
 and an example of its predictions are shown in 
Figure 2c
.
The model assumes that categorical effects in continuous-outcome decisions reflect differences in the rate of evidence accumulation for stimuli in different parts of the stimulus space and are modeled as drift-rate biases. As shown in 
Figure 3a
, the biases are represented by a set of bias vectors, b i , i = 1, 2, . . ., in stimulus space. The number, locations, and norms of the bias vectors are assumed to be subject-specific. A stimulus presented at a particular location, θ, is represented by a mean drift-rate vector, ν θ , which has an associated norm and polar angle. The drift rate for the stimulus is the sum of the stimulus-driven component, ν θ , and its nearest neighboring bias vector, b ϕ . The vectors add nose-to-tail in the usual manner of vector addition as shown in 
Figure 3b
. This relationship means, first, that evidence accumulation will be biased in either a clockwise or anticlockwise direction towards the nearest bias vector, and second, when the polar angles of the stimulus and bias vector coincide, the vectors will be collinear and the norm of the sum will be maximized. Evidence will accumulate more rapidly under these circumstances and the RTs will be shorter and precision will be greater. 
Smith et al. ( , 2022
 reported evidence for effects of this kind.
The full bias model assumes that the effect of a bias vector located at ϕ on a stimulus at θ is given by Proj(ν θ |b ϕ ), the projection of the stimulus on the bias vector, exponentially attenuated by the difference in their polar angles measured in the 1 − cos(•) circular distance metric, as shown in 
Figure 3c
. The attenuation of bias with distance is equal to
ν θ exp{−α[1 − cos(θ − ϕ)}b ϕ ,
where α is a generalization parameter 
(Shepard, 1987)
 that characterizes the spatial extent of the bias. The full model assumes that the resulting distance-dependent biases are summed over all bias vectors in the stimulus space and that the θ-component of the biased drift rate vector is
ν θ, bias = ν θ + ν θ ϕ exp {−α[1 − cos(θ − ϕ)]} b ϕ .
(12)
Because the effects of bias decrease exponentially with distance, for most reasonable choices of α the sum in Equation 12 will be dominated by the bias vector that is closest to the stimulus. 
Figure 2c
 shows the effect of stimulus biases on the signed angular response errors and the median RT for stimuli located in different parts of the space for the condition whose marginal and joint distributions are shown in the other panels. We have found that for both color and direction of motion judgments the number and the locations of the bias vectors are subject-specific and idiosyncratic. 
Blake et al. (1997)
 likewise reported that biases in memory for motion direction were similarly idiosyncratic and did not align, for example, with the cardinal or subcardinal axes of the stimulus space. The data for the subject shown in 
Figure 2
 were best-described by a model with three bias vectors, but other data are better described with two, four, or five bias vectors. The bias model of Equation 12 captures some of the stimulus-dependent effects on errors and RT in 
Figure 2c
, but not completely. In the fitted model, the same set of bias vectors was used to represent anisotropies at three different levels of stimulus noise simultaneously and, for the subject shown, the data and the fitted values coincided more closely in the other two conditions (see Subject S1 in 
Figure 13
 of . Although the bias model does not capture stimulus anisotropies in their entirety we think it is important to retain them in the overall model because they allows us to distinguish systematic from nonsystematic variability in drift rates -at least partially. Being able to do so allows us to draw sharper inferences about representations of stimuli in VWM than would otherwise be possible.
To apply the bias model of Equation 12 to VWM we made one critical change to it, based on preliminary fits to data. The model of Equation 12 predicts that the effects of bias on errors and RTs will be highly correlated. Errors will be least biased and RTs will be shortest for stimuli that coincide with a bias vector. For single-stimulus identification decisions this model performs fairly well, but in VWM the RT effects are attenuated while error biases are nevertheless present. In the bias model of Equation 12, anisotropies in mean drift rate and the drift rate radial variability, η 1 , both contribute to the slow-error effect. The drift-rate bias and slow error predictions are tightly coupled: large stimulus biases imply large bias vectors, which in turn lead to a large slow-error effect. In the VWM tasks we present below the slow-error effect is appreciably smaller than comparable effects in single-stimulus identification tasks. Often there is a slow-error effect only for set size m = 1 but not for larger sets, but stimulus bias effects are nevertheless present at all set sizes. The modified bias model we used here was designed to reflect this fact.
The slow-error effect predicted by the CDM depends only on the radial component of drift-rate variability, but is independent of its tangential component. Random variability in the polar angle of the stimulus has no effect on the rate of evidence accumulation and, consequently, no effect on RT if the drift-rate norm remains constant. However, angular variability can have an appreciable effect on the signed angular errors, as shown in the left-hand panel of 
Figure 2c
. To capture this property we used an extended version of the drift-bias model, in which the bias effect (the vector labeled "bias" in 
Figure 3b
 and ν θ, bias in 
Figure 3c
) is decomposed into its radial and tangential components and in which the contribution of the radial component varies parametrically as ν θ, bias = ν θ, tan. + δν θ, rad. , where 0 ≤ δ ≤ 1.0. When δ = 1.0 the bias model is the same as the model of Equation 12; when δ = 0 the radial component of bias is set to zero and only the tangential component is retained. When fitting the model to data, the value of δ combines with the radial component of drift-rate variability, η 1 , to determine the magnitude of the slow-error effect. If both η 1 = 0 and δ = 0, all of the drift-rate vectors for a given mean drift-rate ν will have the same norm and there will be no slow-error effect.
We do not have strong theoretical grounds for the decomposition of the bias vectors in the extended drift-bias model. Rather, we have adopted it in the pragmatic spirit that it provides a way to decouple the predicted error-bias and slow-error effects in a way that is compatible mathematically with the structure of the JP-CDM model. 
Panichello et al. (2019)
 proposed another model of category effects in VWM that has nice theoretical properties, in which color categories are identified with the attractor states of a dynamical system. This model has similar properties to our extended bias model. Potentially, their model could provide an alternative model of drift-rate bias in the CDM, but harmonizing the two models computationally would be challenging, in part, because their model makes no RT predictions. We have chosen instead to use the generalized version of Equation 12 because it is comparatively straightforward and has the advantage that it allows our analysis of VWM data to be related theoretically to previously published analyses of single-stimulus identification tasks. 


Method Stimuli and Design
We applied the JP-CDM to data from four color VWM experiments in which subjects encoded a variable number of colored patches into VWM and, after a short retention interval, reported the color of a probed patch by indicating its position on a surrounding color wheel. 
Figure 4
 gives an example of the stimuli used in our experiments. Experiments 1 and 2 followed  and used an eye-movement decision task, in which subjects made saccadic eye movements from a central home position to points on the color wheel to indicate their decisions; Experiments 3 and 4 used mouse movements, in which subjects moved a mouse from a home position to points on the color wheel. Ratcliff (2018) compared continuous-outcome decisions expressed using eye movements, mouse movements, and finger-pointing responses and reported that the response modality had only minor effects on performance and did not alter any of the inferences about the decision process. Full details of the experiments can be found in Appendix A; we restrict ourselves here to those features of them that are relevant to the modeling of data and evaluation of the theory.
All experiments used small-N psychophysical designs in which a large number of experimental trials were collected from each of a small number of highly-practiced subjects (five in each of our experiments) and all of the modeling was done at the individual subject level. As discussed by 
Smith and Little (2018)
, these kinds of designs, which are widely used in visual and auditory psychophysics and in single-cell studies in neuroscience with awake, behaving animals, have several advantages. Each subject is treated as an independent replication of the experiment and the consistency of the results (or otherwise) across subjects provides an indication of the reproducibility of the findings. The focus of studies using these designs is on measurement rather than population inference, specifically, on the question of whether a model and data agree with one another within the limits of measurement error. The use of large samples of data from highly-practiced subjects and stimuli materials calibrated to individual subjects' performance are effective ways to reduce measurement errors. The properties of theoretical interest in the JP-CDM are expressed in the joint distributions of decision outcomes and RT (e.g., 
Figure 2b
) and these distributions require large samples (around 400 trials in each condition) to estimate reliably. Small-N designs provide the right kind of test bed to evaluate such models.
Experiments 1 and 2 were identical to one another except for the range of set sizes used. Experiment 1 used set sizes of 1, 2, 3, and 4 items; Experiment 2 used set sizes of 1, 2, 4, and 6 items. The studies of the sample-size model and the attention-weighted sample-size model by 
Sewell et al. (2014)
 and 
Smith et al. (2016
 restricted the maximum set size to four items to limit the effects of guessing and we restricted the range of set sizes in Experiment 1 for the same reason. However, much of the recent work in VWM has focused specifically on what happens at large set sizes because performance under these conditions is regarded as central to resolving the slots versus resources debate. We therefore replicated the experiment using a larger range of set sizes, using some of the same subjects in Experiment 2.
Experiments 3 and 4 manipulated the difficulty of the decision task, which required subjects to match a memory representation of a stimulus color to the corresponding hue on the color wheel. In Experiment 3, we did this by manipulating the color saturation of the stimulus patches (Appendix A). Most color VWM studies have used saturated colors to ensure stimuli are highly distinguishable from one another. Color saturation has sometimes been used in studies of visual search to manipulate search efficiency 
(Santhi & Reeves, 2004)
, but it has not, to our knowledge, been used in VWM. Matching the memory of a desaturated color to a saturated display is more challenging than matching a saturated color and leads to lower accuracy and longer RTs. By simultaneously varying the memory load and the difficulty of the decision task we were able to characterize the independent contributions of memory variability and decision process variability to speed and accuracy. In Experiment 4, we extended this logic, by factorially varying the saturation of the stimuli and the color wheel. In this experiment, the saturation of the stimuli and the color wheel could be independently high or low. The saturation manipulation was crossed with set size, with all variables randomized across trials within experimental blocks. Experiment 3 used set sizes of m = 1, 2, 4, 6 items; Experiment 4 used only set sizes of m = 1, 2, 4 items to ensure there were enough experimental trials in each condition when set size was crossed with stimulus and color wheel saturation. Data and code associated with the experiments are publicly available at https://osf.io/6rakj/ 
(Smith et al., 2024)
. The studies were not preregistered. T er Nondecision time variability s t Note. σ 2 = 1 for all conditions


Fitting the JP-CDM
We fit the JP-CDM to the individual subjects' error and RT data using maximumlikelihood estimation (MLE) using the methods described by 
Smith et al. ( , 2022
. The predictions were generated using a combination of Matlab and C, again as described by 
Smith et al. ( , 2022
, but making use of a correction factor to stabilize them numerically for large drift rates and decision criteria, as described by 
Smith et al. (2023)
. Because the model predicts joint distributions of RT and errors, conditional on the locations of the stimuli in color space, it unavoidably requires a large number of parameters, and the focus of our modeling was to find a psychologically meaningful, reduced form of the model that identifies invariants of the memory representations and the decision process. 
Table  1
 lists the parameters of the JP-CDM that we estimated to fit the model to data. The numbers of parameters of each kind varied with the particular designs in each experiment and are described below. For each experiment we fitted the model by minimizing minus the log-likelihood ratio (−LL) using the Matlab implementation of the Nelder-Mead simplex algorithm (fminsearch).
The parameters of the models fall into four general classes: parameters of the stimulus representation in memory; parameters of the decision process; parameters of the stimulus bias model, and nondecision time parameters. Each stimulus condition is represented by a distribution of drift rates with a mean drift rate (drift rate norm) ν , which determines how rapidly, on average, evidence accumulates in the CDM. The drift rate norms could either vary freely across conditions or be theoretically constrained, as described below. The polar, or angular, components of drift rate variability, which characterize across-trial variability in the encoded stimulus identity, were described by the shape and precision parameters of the JP distribution, ψ and κ µ , which could also freely vary or be theoretically constrained. The radial components of drift rate variability, which characterize across-trial variability in encoded stimulus quality, were described by the normal standard deviation, η 1 in Equation 10. Subsequently, we write the radial component of drift-rate variability as η i , where the subscript i denotes the experimental condition.
The only unique parameter of the decision process is a, the radius of the criterion circle. As the set sizes in our experiments were randomized across trials within blocks we made the selective influence assumption, common in studies of two-choice and continuous-outcome decisions, that the same decision criterion is used for all stimuli and, in particular, for all set sizes. Recently, Fennell and Ratcliff (2023) reported a continuous-outcome VWM study in which they compared slot, resource, and guessing models, using 
Ratcliff's (2018)
 spatially-continuous diffusion model (SCDM) to characterize the decision process. They found they needed to use different criteria for each set size to account for their RT and accuracy data and justified this choice by arguing that people adjust their criteria on a trial-to-trial basis in response to the properties of the display before they make a decision about it. Although this argument is defensible it requires one of the core selective influence assumptions of evidence accumulation models be relinquished. In contrast, we found we were able to model the set size effect in RT and accuracy in our experiments using the JP-CDM using a single decision criterion. We consider the relationship between our results and those of Fennell and Ratcliff in the General Discussion.
Anisotropies in stimulus representations were described by the stimulus bias model of Equation 12 and 
Figure 2
, which characterizes bias by a set of bias vectors, b ϕ , at locations ϕ. The bias vectors are characterized by a set of bias norms, B ϕ , and bias angles, A ϕ , which specify their lengths and locations in stimulus space (±π rad.). For simplicity, we used a four-component bias model for all subjects in all four experiments. Because the bias norms were allowed to vary freely in our fits this model subsumes models with fewer bias vectors as special cases. We have rarely found that data are better fitted with more bias vectors (e.g., Smith et al., 2020, 
Figure 13
) and in these cases the individual vectors are not wellidentified because they trade off with the stimulus similarity parameter. The remaining components of the bias model are the similarity parameter α, which describes the spatial extent of the bias and, in the case of the extended bias model, the parameter δ, which allows bias to be decomposed into its radial and tangential components.
We modeled nondecision time as an independent, uniformly-distributed, random variable with mean T er and range s t , as is common in studies of evidence accumulation models. Recent theoretical and empirical work on nondecision times has advocated other representations, like cascaded decision and motor stages, or nonuniformly-distributed nondecision times 
(Bompas et al., 2024;
Dendauw et al., 2024;
Verdonck & Tuerlinckx, 2016;
Verdonck et al., 2021)
. For our purposes, however, the additive, independent decomposition of decision and nondecision times has the advantages of being simple, of involving the fewest auxiliary assumptions, and has been shown to work well in a range of applications. Many studies have made the selective influence assumption that T er should remain fixed across experimental conditions because it represents processes outside the decision process whose durations should be constant, but other studies have found this assumption breaks down in various ways. Discussions can be found in 
Dutilh et al. (2019)
 and . In a two-choice VWM study using the diffusion decision model with set sizes 1, 2, 3, and 4, 
Sewell et al. (2016)
 found T er progressively increased with set size. Short values of T er are to be expected with set sizes of m = 1 because the relevant stimulus is known before the report cue and the decision process can begin before it is presented. Sewell et al. linked the increase in T er with set sizes of m > 1 to 
Oberauer's (2002)
 idea that an item in working memory must be brought into the focus of attention before it can be retrieved and this takes longer when there are more items in memory. We followed Sewell et al. and allowed T er to vary with set size, but constrained it to be constant across other experimental manipulations, such as the saturation manipulations in Experiments 3 and 4. We held s t constant across all conditions in all cases.


Linking Sample-Size and Power Law Models to the JP Distribution
For two-choice VWM tasks, in which performance is characterized using the Gaussian signal detection model, the sample-size model follows straightforwardly from elementary sampling theory. If d ′ is computed from the mean of a set of n noisy evidence samples that are distributed equally among m items, then an inverse square law in set size is predicted because the denominator of d ′ depends on the standard deviation of the mean of the set of samples (i.e., the standard error of the mean), which increases as √ m, while the numerator is a consistent estimator of the population mean for all m. For continuous-outcome decisions about stimuli on a circular space the situation is not so straightforward because the statistics of distributions on circular spaces do not behave like those of distributions on the real line. The circular standard deviation (CSD) of a collection of n randomly-oriented unit vectors with polar angles R i , i = 1, 2, . . . n, is defined as (− lnR 2 ) 1/2 , where the resultant,R, is the length of the average of the vectors. It is obtained by decomposing the vectors into their sine and cosine components, averaging the components, ( i cos R i )/n and ( i sin R i )/n, and then computing the length of the average by Pythagoras's theorem 
(Mardia & Jupp, 2000)
. The intuition behind this measure is that the length of the averaged vectors gets closer to zero as their distribution becomes more uniform, which is reflected inR. Bays and colleagues have used the reciprocal of the CSD as a measure of recall precision in continuous-outcome VWM tasks 
(Bays et al., 2011)
. As an alternative, as noted earlier, van den Berg et al. (2014) characterized the set-size effect in their variable precision model using Fisher information. Fisher information describes the curvature of the likelihood function in the vicinity of its maximum and, for normally-distributed random variables equals the reciprocal of the variance, which is another definition of precision. For a von Mises distribution with precision κ, given by Equation 7, the Fisher information is equal to κI 1 (κ)/I 0 (κ), where I 1 (κ) and I 0 (κ) are modified Bessel functions of the first kind of order one and zero, respectively. The Fisher information of the von Mises distribution is approximately linear in κ, so it provides a natural way to characterize the set-size effect.
In the case of the JP distribution, the expression for the Fisher information has an unattractively complex form 
(Jones & Pewsey, 2005, Appendix A.
2) that has little intuitive content. Instead, we considered two simpler alternatives. The first was the JP precision parameter, κ µ , in Equation 11. The precision parameter provides a natural characterization of the dispersion of the distribution and, in the special case of ψ = 0, in which the JP distribution reduces to the von Mises, is asymptotically proportional to the Fisher information used by van den Berg et al (2014). The second was to use 1/CSD, the reciprocal of the circular standard deviation of the JP distribution. The theoretical CSD is defined analogously to the empirical CSD for a sample, in terms of the resultantR, but with expectations replacing averaging. For a distribution like the JP, which is symmetrical around θ = ϕ (zero in our applications), the resultant reduces to E θ [cos(θ − ϕ)], where the expectation is taken with respect f (θ), the density function of the random variable θ, because the product f (θ − ϕ) sin(θ − ϕ) is an odd function, so E θ [sin(θ − ϕ)] = 0. Values of E θ [cos θ] for the JP distribution are easily computed by numerical integration. The two characterizations of the set-size effect gave similar results, but we found 1/CSD gave more consistent and regular results across subjects and experiments, so we have confined ourselves to reporting sample-size and power law relationships using this measure.


Model Evaluation
Model comparison in cognitive psychology is most often performed using model selection methods like the Akaike information criterion (AIC; 
Akaike, 1973)
 and the Bayesian information criterion (BIC; 
Schwarz, 1978)
, which penalize models for their numbers of free parameters. The BIC uses a more conservative sample size dependent penalty and is preferred by some researchers because it is less prone to selecting more complex models than the AIC, although 
Oberauer and Lin (2017)
 reported that the BIC had poorer model recovery in simulated data. These methods are easy to apply but they have limitations and should be interpreted with caution. One limitation is that they penalize models based on their numbers of free parameters but do not take into account differences in model complexity 
(Pitt et al., 2002)
. A second, more pervasive, problem is that they assume the fit statistic is a true likelihood, derived from a sample of independent, identically-distributed random variables -an assumption unlikely to ever be true in cognitive experiments, because of the effects of learning, fatigue, attentional fluctuations, and other sources of trial-to-trial heterogeneity 
(Luce, 1986, Section 6.6
). These kinds of variability lead to a phenomenon called overdispersion, in which data are more variable, and computed likelihoods for otherwise well-fitting models are reduced, compared to what would be obtained if the assumption of independent, identically-distributed random variables were true. When overdispersion is present, 
Burnham and Anderson (2002, p. 69
) recommend use of a modified AIC, called the QAIC, defined as QAIC = −2 log LL max /c + 2k,
where LL max is the maximized log-likelihood, k is the number of free parameters in the model, "log" is the natural logarithm, and c is an estimate of overdispersion. The QBIC is defined similarly, as QBIC = −2 log LL max /c + k log(N),
where N is the total sample size. To estimate overdispersion, Burnham and Anderson follow 
Cox and Snell (1989)
 who recommended using the estimator c = χ 2 /df , where χ 2 is a Pearson chi-square statistic computed on the most complex model of the class of models under consideration and df is its associated degrees of freedom. The logic of this estimator is that the expected value of χ 2 for a well-fitting model is equal to its degrees of freedom, so the expected value of c will be 1.0. Burnham and Anderson report that values of c estimated in the way typically range from about 1 to 4, with values greater than 4 indicating systematic misfit of the model rather than overdispersion. The formal justification for dividing the log-likelihood in Equation 13 by c to correct for overdispersion was given by 
Kim et al. (2013)
, who noted that the rescaled log-likelihood in Equation 13 leads to a Fisher information matrix in which the variance terms are rescaled in the same way as the log-likelihood ratio, allowing c to be interpreted as a variance inflation factor. Smith and Corbett (2019) used 
Cox and Snell's (1989)
 procedure with a G 2 likelihoodratio statistic and, consistent with Burnham and Anderson's observation, found that most c values were clustered around 2 with one of 12 estimates ranging as high as 4. The idea that fit statistics in choice tasks are systematically inflated by overdispersion accords with the findings of 
Smith (1998)
 and the observations of 
Ratcliff and Childers (2015)
. Smith used a method proposed by 
McCullagh and Nelder (1989)
 based on the heterogeneity of multiple variance estimates computed across blocks and sessions, and found that individual two-choice psychophysical data were overdispersed by a factor of 1.2 to 2.5 relative to the theoretical binomial variance. Ratcliff and Childers reported that chi-square fits of the two-choice diffusion model tend to vary between the critical value of χ 2 (roughly df + 2 √ 2 df ) and twice the critical value, implying overdispersion of around 1.3 to 2.6, in agreement with Smith's estimates and Burnham and Anderson's observations.
To implement Cox and Snell's procedure, we computed a minimum Pearson χ 2 based on a 5 × 5 discretization of the predicted joint distributions for the most highly-parameterized CDM for each subject in each experiment, summed across conditions, using the 0, .2, .4, .6, .8 and 1.0 quantiles of the theoretical distributions of RT and error as bin boundaries. The estimates of c obtained in this way agree fairly well with estimates obtained using other methods and in other settings and are reported below. The effect of using the QAIC and the QBIC rather than the standard AIC and BIC (with c = 1) is that they are less prone to selecting the most complex model in a set of candidate models. Although we computed both the continuous maximized log-likelihood, LL max , for all models, and the minimum χ 2 for the most complex model, we prefer the log-likelihood for model evaluation because it does not lose information.


Variable Range Set-Size Experiments
JP-CDM Modeling, Experiments 1 and 2 Experiments 1 and 2 used an eye-movement decision task to characterize VWM for set sizes m = 1, 2, 3, and 4, and m = 1, 2, 4, and 6, respectively. Data were collected from five experienced subjects in each experiment, each of whom served in 10 experimental sessions, following one or two sessions of practice. Three of the subjects (DF, PG, and YW) served in both experiments; one of them (PG) is an author. Each subject provided around 400 trials of experimental data at each of the four set sizes, which were randomized within experimental blocks, allowing estimation of the joint distributions of error and RT. As described in Appendix A, RT was defined as the first time that the eyes made an excursion of more than 90% of the distance from a central home circle to the surrounding color wheel. The response was defined as the location of the first corrective eye movement after the initial saccade. A similar procedure was used by  in their eye-movement study of continuous-outcome decisions.  
(4ν, 4κ, 4ψ)
 4 drift rates, 4 precisions, 4 shapes (4ν, 4κ, 1ψ) 4 drift rates, 4 precisions, 1 shape (4ν, 1κ, 1ψ) 4 drift rates, sample-size precision, 1 shape (1ν, 1κ, 1ψ) 1 drift rate, sample-size precision, 1 shape (2ν, 1κ, 1ψ) 2 drift rates, sample-size precision, 1 shape (2ν, 1κ, 1ψ, β) 2 drift rates, power law precision, 1 shape The questions of theoretical interest to us were, first, whether the JP-CDM, equipped with the extended bias model, can account for the joint and marginal distributions of RT and error and the anisotropies of the stimulus space and, second, to identify the most parsimonious characterization of the VWM distributions. Our specific aim was to identify lawful regularities in the memory distributions as a function of set size, as characterized by sample-size or power law models. The JP-CDM makes a sharp distinction between the distribution of memory representations and the distribution of responses errors and the question therefore arises as to the appropriate level to seek regularities of this kind. Most published analyses, mainly using the power law model, have focused on the empirical distribution of errors, but the more relevant level of analysis in the JP-CDM is the level of the latent distribution of memory representations. This distribution contributes only one of the sources of variability in the distribution of errors, along with variability in the decision process. Note. Bold face type indicate the best models in a QAIC or QBIC sense.
To identify lawful changes in the memory distributions we compared models that were more or less constrained across set-size conditions. 
Table 2
 summarizes the notation we used to distinguish among models. The models differed according to the number of free drift-rate norms and JP shape parameters and in whether the JP precision varied freely or was constrained to follow a sample-size or power law model in 1/CSD. The constrained models are characterized by a single JP κ µ parameter (written without the subscript for brevity) for set size one that determined the precision for all other set sizes. For sample-size models, the m = 1 precision wholly determines the precision at larger set size; for power law models, the precision at larger set sizes was characterized jointly by the m = 1 value of κ µ and a free exponent, β.
The remaining models in 
Table 2
 parameterize the drift-rate norms in different ways. In addition to models in which the drift rate norm either varied freely as a function of set size or was the same for all set sizes, we considered models with two drift rate norms, one for set size m = 1 and another for all other set sizes. These models investigated whether set size one is in some way special because the identity of the target stimulus is known earlier than it is for larger set sizes. All six models in the table had the same extended bias model and made the same assumptions about nondecision times. The extended bias required a total of 10 parameters: four bias vector locations, four bias vector norms, a similarity parameter, α, and a bias decomposition parameter, δ. All of the models assumed that mean nondecision time, T er , varied across set size, while nondecision time variability, s t , remained the same. We defer discussion of drift-rate bias effects until after presentation of all four experiments. The results of the analysis are summarized in 
Tables 3 and 4 and Figures 5 to 8. Table  3
 summarizes the mean fit statistics averaged across subjects for each experiment. The entries in the table are minus the maximized log-likelihood and the QAIC and QBIC, together with the number of estimated parameters for each model (k). Readers are reminded that log-likelihoods cannot be compared across different data sets to assess the relative fit to different subjects or in different experiments because the maximum achievable LL depends on the variance of the distribution being fitted. Values of −LL are greater for Experiment 2 than Experiment 1 because the variance of the empirical distributions in Experiment 2 was greater. For Experiment 1, the individual overdispersion estimates from the χ 2 /df method ranged from 1.84 to 3.65 with a mean of 3.02; for Experiment 2 they ranged from 1.17 to 2.92 with a mean 2.20. 
Table 4
 shows estimated parameters for selected models, averaged across subjects, as discussed below. 
Figures 5 and 6
 show examples of fitted marginal and joint distributions of errors and RT. 
Figures 7 and 9
 characterize the sample-size relationship in the latent memory distributions of the fitted models. Although the results for the two experiments in 
Table 3
 are not identical, they are more similar than might appear at first glance, with the differences attributable to a minority of subjects, as discussed below. Overall, the JP-CDM did a good job of accounting for the marginal and joint distributions of errors and RTs for all subjects, but to conserve space, we have reproduced the fits for one subject for each experiment (YW in Experiment 1 and OS in Experiment 2) only. We chose these subjects because their data were well-described by the simple 
model, (2ν, 1κ, 1ψ)
. The patterns for these subjects are typical of those shown by the remaining subjects, apart from small deviations from the sample-size model for a minority of subjects, discussed below. 
Figure 5
 shows the fitted marginal distributions of errors and RTs for YW for Experiments 1 and 2. 
Figure 6
 shows the joint distributions in the form of a Q × Q plot, as discussed in relation to 
Figure 2c
. The slow-error pattern again appears as a dishing of the upper quantiles of the Q×Q plot. The two subjects in the figure both show small-to-moderate slow error patterns at all set sizes; for other subjects it was most apparent at small set sizes, particularly m = 1. The attenuated slow-error effect contrasts with the robust effects reported for a single-stimulus identification task using noise-perturbed color patches by . The following inferences can be made from 
Table 3 and the accompanying figures:
 1. The shape of the memory distribution, as characterized by the JP parameter ψ, remained invariant as a function of set size while the precision of the distribution changed. The invariance is characterized by the better performance of model (4ν, 4κ, 1ψ) than (4ν, 4κ, 4ψ), in both a QAIC and a QBIC sense, for both experiments. The shape invariance was true on average, as shown in 
Table 3
, and for all five individual subjects in both experiments.
2. The JP memory distributions of the best-fitting models were strongly leptokurtic, typically with a ψ parameter of around -1 or less (cf. 
Figure 1b)
. These distributions resemble the perceptual similarity functions in Schurgin et al.'s (2020) TCC model, which can be approximated by a Laplace distribution. Unlike their model and related models by 
Oberauer (2023)
 and 
Tomić and Bays (2024)
, however, the shape of the memory distribution does not directly determine the shape of the empirical error distribution. Rather, in the absence of memory variability the CDM predicts pure von Mises error distributions, as noted above. Variability in memory changes the shape of the error distribution, as it does in variable precision models, and allows the JP-CDM to predict increased probability mass in the tails of the error distribution.
3. For Experiment 1, the changes the JP precision parameter κ µ were well-described by a sample-size model in 1/CSD. To generate predictions for the sample-size model, we estimated a value of κ µ for set size m = 1 from fits to the JP-CDM to data. We computed the CSD of the JP distribution for set size m = 1 numerically and used it to predict 1/CSD for m > 1 using the sample-size model. We then inverted these values numerically to obtain the corresponding values of κ µ and used them to predict JP distributions for m > 1. For Experiment 2, the JP precision was again welldescribed by the sample-size model, although it was not the best model statistically. Instead, on average, the QAIC-best model was the free-κ model, (4ν, 4κ, 1ψ), and the QBIC-best model was the power law model, (2ν, 1κ, 1ψ, β). These results reflect the performance of two subjects whose κ µ parameters were, surprisingly, a little better than sample size, as discussed below.
4. The rate of evidence accumulation in the CDM depends on the drift rate norm, ν , which is theoretically independent of the precision of the encoded stimulus representation, characterized by the JP distribution. The comparison of the models (4ν, 1κ, 1ψ) and (1ν, 1κ, 1ψ) tested whether the rate of evidence accumulation and the precision of the memory representation are separable, that is, whether the rate is the same for precise and imprecise representations. The first model allowed the drift-rate norm to vary with set size; the second model constrained it to be the same for all set sizes. The remaining model, (2ν, 1κ, 1ψ), and its power law generalization, 
(2ν, 1κ, 1ψ, β)
, tested whether set size m = 1 is special, for the reasons described above -namely, that the identity of the probed item is known at the time of stimulus presentation. For both Experiments 1 and 2 models with two drift-rate norms (one for set-size one and another for all other set sizes) performed better in both a QAIC and QBIC sense than models with either fewer or more drift rate norms. Drift rate norms for models with two and four ν parameters are compared in 
Table  4
. Their magnitudes decrease with set size, but the largest difference is for m = 1 versus the others. The idea that the decision process starts earlier on m = 1 trials and is associated with a higher rate of evidence accumulation is supported by the nondecision times, T er , in 
Table 4
. Nondecision times for the JP-CDM increased progressively with set size, as previously reported for the two-choice diffusion model by 
Sewell et al. (2016)
. However, the largest differences in nondecision times were again for m = 1, which were 70 ms to 100 ms shorter than those for the other set sizes, consistent with the idea that the decision process on m = 1 trials begins earlier. The important relationships theoretically for the JP-CDM are those at the level of latent memory distributions, as noted above. 
Figures 7 and 8
 show pairs of JP memory distributions estimated from fits of the JP-CDM to the individual subject data from the two experiments. One set of distributions is from the unconstrained model with four κ µ precision parameters and four ν parameters. The second set are from a model with sample-size constrained κ and two ν parameters. In most cases, the constrained and unconstrained estimates are in very good agreement, consistent with the idea that the quality of the memory representation depends on the allocation of neural resources to items, as predicted by the sample-size model. There is no evidence of any additional decline for m = 6 in Experiment 2 of the kind associated with a guessing process, as predicted by slot models. The estimated JP distributions imply that, for some subjects in some conditions, particularly in Experiment 1, there is comparatively little across-trial variability in the encoded stimulus identity. The difference between the JP memory distributions and the empirical error distributions (e.g., 
Figure 5
) is due to the variability added by the decision process. The effects of decision process variability are characterized by Equation 9, which says that, for a single encoded stimulus value with drift rate µ, the precision of the von Mises error distribution will be a µ /σ 2 , the product of the decision criterion and the drift-rate norm divided by the diffusion coefficient. Importantly, however, the CDM distinguishes between variability in the encoded stimulus identity and variability in the encoded stimulus quality, so it is not the case that the distributions in 
Figure 7
 and 8 mean there is little or no acrosstrial variability in the encoded stimulus representation at small set sizes. In the JP-CDM, variations in identity are characterized by the JP distributions in 
Figures 7 and 8,
 whereas variations in quality are characterized by the radial component of drift rate variability (η in 
Table 4
), which were moderate. In intuitive terms, the JP distributions in 
Figures 7 and  8
 imply that someone might be unlikely to cognitively represent a pink stimulus in memory as a red stimulus, but the quality of their representation of pink could nevertheless vary across trials, leading to variability in the retrieved values of pink on different occasions. We develop this point in relation to Experiments 3 and 4 in which we varied the difficulty of the decision task by varying the color saturation of the stimuli and the response wheel.
Although there is a good correspondence between the estimated JP distributions for the sample-size model and the free model in 
Figures 7 and 8
, there are small discrepancies for set size m = 6 for subjects LF and YW in Experiment 2. For these subjects the precision of the free JP distribution was a little better than predicted by the sample-size model (i.e., the peak of the continuous curve is higher than the broken curve in 
Figure 8
). The model selection results in 
Table 3
 reflect the results for these subjects. For both subjects, the free-κ model, (4ν, 4κ, 1ψ), and the power law model, (2ν, 1κ, 1ψ, 1β), were preferred to the constrained model 
(2ν, 1κ, 1ψ)
, whereas the constrained model was preferred for the remaining subjects, in agreement with Experiment 1. The exponents for these subjects were β = .40 and β = .38, respectively. These values differ from previous reports of power law relationships in which the exponents are typically greater than 0.5.


Discussion of Small Range and Large Range Experiments
The main finding from Experiments 1 and 2 is that the CDM can be extended successfully from continuous-outcome, stimulus identification decisions to VWM retrieval decisions. The extension follows the work of  who showed that stimulusdriven trials and encoding failure trials could both be characterized by the JP distribution. They found that the CDM with a JP distribution of polar drift-rate angles performed about as well as a two-process model in which performance was a mixture of CDM processes with different precisions. One process was a stimulus-driven process with high precision, described by a normal distribution with small variance; the other was an encoding failure process with zero precision. We proposed the JP distribution as a model of the memory distribution in a similar spirit, as a distribution that can flexibly capture the memory representations of models that assume a narrow central peak flanked by heavy tails. Strikingly and fortuitously, the estimated JP memory distributions resemble the similarity-scaled stimulus distributions in the TCC model, which can be well-described by a Laplace distribution, as noted above.
There was no evidence in our data of a slot-like limit, which should have been most evident in the m = 6 condition of Experiment 2. Rather, the set-size effect in both experiments was, for the most part, well described by the sample-size relationship out to the largest set sizes used. Any deviations from the sample-size model at large sets in Experiment 2 were small and in the direction of slightly better than sample-size performance. We consider a possible mechanism that might produce better than sample-size performance in relation to Experiments 3 and 4. There was no need to include a guessing component in our models. Rather, the JP distributions of drift rates in the CDM predicted heavy-tailed error distributions without explicit guessing. These distributions predict large deviations from the true stimulus value, and hence in the polar angle of the drift rate, on a small proportion of trials.
The main contribution of the JP-CDM is that it provides a unified account of the mechanisms that underlie the speed and accuracy of continuous-outcome VWM outcome decisions, not just their accuracy. The theoretical framework we used to do this was the same as the one we have used to account for performance in stimulus identification tasks with noisy color stimuli. Our approach emphasizes that the continuous-outcome VWM task is a decision task, and so should be amenable to the same kind of theoretical analysis as other decision tasks. What such tasks have in common, whether they involve twochoices or continuous-outcome choices, and whether they involve perception or memory, is that they require an encoded perceptual representation be matched against a set of alternatives in memory. The matching process is both noisy and time-consuming, and has been successfully represented in many settings using diffusion models. The JP-CDM links the continuous-outcome VWM task to this broader literature.
One of the implications of this change in viewpoint, of representing continuous-outcome VWM retrieval as a decision task, is it predicts that performance will be limited both by the quality of the memory representation and the noisiness of the decision process, which in the CDM is determined by the statistical properties of the diffusion process. If this idea has theoretical merit, then it should be possible to experimentally manipulate the difficulty of the decision task independently of the memory representation, and show how manipulations of the two processes jointly affect performance. This idea motivated our next two experiments, which manipulated the color saturation of the stimuli and, in Experiment 4, also of the color wheel.


Variable Color Saturation Experiments JP-CDM Modeling, Experiments 3 and 4
Experiments 3 and 4 used a mouse-movement decision task to characterize VWM for set sizes m = 1, 2, 4, 6 and m = 1, 2, 4, respectively. Full details of the experiments can be found in Appendix A. In Experiment 3, the set-size manipulation was crossed with a manipulation of stimulus saturation, using color patches similar to those in Experiments 1 and 2. In Experiment 3, the color patches were either saturated, within the limits of the monitor gamut, or desaturated. Desaturated colors appear duller, or more washed-out, than their saturated counterparts. The desaturated colors were constructed by mixing the saturated colors with a gray of equal luminance, which shifted them towards the center of the isoluminant plane. Set size and color saturation were randomly varied across trials within a block. (There were no mixed-saturation trials: Stimuli on any trial were either all saturated or all desaturated.) Subjects made their responses by moving a mouse from a central home circle and to a point on a surrounding, saturated, color wheel. The RT was defined as the first time the mouse crossed the inner border of the color wheel and the response was defined as the point of the inner border crossing. In Experiment 4 the saturation of the stimuli and the saturation of the color wheel were varied factorially and crossed with set size. Set size, stimulus saturation, and wheel saturation were randomly varied within a block and the stimuli on a given trial were, again, all saturated or all desaturated. Data were collected from five subjects in each experiment, including two of the authors (PG, ES), each of whom served in 10 experimental conditions. Subjects in Experiment 3 each completed a total of 3,200 trials, 400 in each of eight conditions; subjects in Experiment 4 each completed a total of 4,080 trials, 340 in each of 12 conditions.  
(8ν, 1κ, 1ψ, 1β) 31 2,838 1,997 2,171 (8ν, 1κ, 1ψ, 1β) 29 2,728 2,126 2,285  (8ν, 1κ, 1ψ, 2β) 30 2,823 1,993 2,174 (8ν, 1κ, 1ψ, 2β) 30 2,723 2,123 2,288  (4ν, 1κ, 1ψ, 1β) 27 2,894 2,014 2,163 (4ν, 1κ, 1ψ, 1β
) 25 2,740 2,127 2,263 (4ν, 1κ, 1ψ, 2β) 28 2,884 2,010 2.165 (4ν, 1κ, 1ψ, 2β) 26 2,736 2,126 2,269 Note. Bold face type indicate the QAIC and QBIC best models.
Our primary interest in the analysis was in whether, and to what extent, the effects of stimulus and wheel saturation and memory set size were separable. The JP-CDM assumes that set size affects the precision of the JP distribution, which characterizes the polar variability of memory, while saturation affects the strength of the match between the color wheel and memory, which should affect the drift-rate norms. The simplest model is one in which JP precision follows either the sample-size model or a power law model and is wholly prescribed by precision for set size one, while drift rate norm varies with stimulus saturation, or the combination of stimulus and wheel saturation in Experiment 4. The most complex model is a nonseparable model in which JP precision and drift rate norms vary with all combinations of set size and saturation. In between those extremes are models in which JP precision varies freely with set size but is independent of saturation and models in which drift rate norm varies with set size in a restricted way, including the "m = 1 is special" models which allow drift rates to differ for m = 1 and m > 1 trials. Based on the results of  and our conjecture that encoding may be more demanding for desaturated than saturated stimuli, we considered versions of the power law model in which the exponent varied with stimulus saturation. 
Table 5
 summarizes the main models we considered. We also considered models in which the JP ψ shape parameter varied across conditions, which we used to compute overdispersion from χ 2 /df in the QAIC and QBIC statistics. For Experiment 3 the overdispersion estimates ranged from 2.52 to 4.89; for Experiment 4 they ranged from 2.12 to 3.46. Models with a single ψ parameter were preferred by the QBIC for both experiments, so we focus here on fixed-shape models. When ψ was allowed to vary freely it did not do so in a predictable, monotonic way, implying any advantages of shape-varying models were likely due to model flexibility rather than systematic effects.
The models in 
Table 5
 are organized factorially with JP precision varying within drift rate norm. The numbers of parameters of the models for the two experiments reflects differences in the experimental conditions: Experiment 3 had eight conditions (two saturation conditions crossed with four set sizes); Experiment 4 had 12 conditions (four saturation conditions crossed with three set sizes). The 8κ and 12κ models for Experiments 3 and 4, respectively, allowed JP precision to vary with both set size and saturation. The 4κ and 3κ models allowed JP precision to vary with set size but remain constant across saturation conditions, while the 1κ models assumed JP precision followed the sample-size model. The (1κ, 1β) and (1κ, 2β) models are power law models; in the latter, the exponent varied with stimulus saturation. The models with varying numbers of drift-rate norms were: (a) norms varied freely with both saturation and set size (8ν and 12ν models for Experiments 3 and 4, respectively); (a) norms varied with saturation only (2ν and 4ν models); and (c) "m = 1 is special" (4ν and 8ν models).
The main inferences from the fits in 
Table 5
 and the accompanying figures are as follows:
1. The best models in a log-likelihood sense were the most highly parameterized, nonseparable, models. These models also performed well in a QAIC, but not a QBIC sense. In an absolute sense, however, all of the models in 
Table 5
 performed fairly well. For Experiment 3, the log-likelihoods of the most complex and the simplest models, (8ν, 8κ, 1ψ) and (2ν, 1κ, 1ψ), differed by only 7%. For Experiment 4, the log-likelihoods of the most complex and the simplest models, 
(12ν, 12κ, 1ψ)
 and (4ν, 1κ, 1ψ), differed by only 2%. Qualitatively, the fits of the simplest and most complex models to the marginal and joint distributions and to the stimulus-dependent error and median RTs showed only minor differences that are difficult to discern in plots.
2. The separable models in which JP precision varied freely with set size performed uniformly better than the sample-size models in a QAIC sense but worse in a QBIC sense for Experiment 3; for Experiment 4 the separable models also performed better than the sample-size model in a QBIC sense for some drift-norm models. The differences most likely reflect the fact that the free-κ models had more parameters in Experiment 3 than in Experiment 4 because there were more set-size conditions, and so were penalized more heavily.
3. For both Experiments 3 and 4, drift rate norms varied with both set size and saturation. Although models in which the norms varied with saturation and were independent of set size performed well in a log-likelihood sense, they were not the best models in either a QAIC or QBIC sense, nor were the "m = 1 is special" models, at least not consistently. These results point to a more complex dependency of mean drift rates on both set size and saturation, which we characterize in more detail below.
4. The results of both experiments favored a power law model with a large exponent over a sample-size model and, for some subjects, favored a model in which the exponent varied with stimulus saturation. These results contrast with those of Experiments 1 and 2 and support the idea that the exponent indexes attentional engagement. Exponents were greater for desaturated than for saturated stimuli, consistent with the idea that encoding desaturated stimuli is more attention demanding.
Figures 9 and 10 show the JP memory distributions for individual subjects for Experiments 3 and 4 as a function of set size estimated from the fitted models. Each panel contrasts plots of a free-κ model with a large number of free drift rate norms and the 2β power law model with a small number of drift rate norms. In most cases, the correspondence is very good, although the distributions for the free-κ models are a little more sharply peaked than those for the constrained models. To construct these models we have used the most restrictive power law models in which there was only one norm per saturation condition (i.e., two for Experiment 3 and four for Experiment 4). The implication of these figures is that the power law model successfully captures the across-trial variability in the encoded stimulus identity, as expressed in the JP distribution, as a function of set size. The similarity of the individual JP distributions for simple and complex models shows graphically why the differences among the log-likelihoods for the fitted models in 
Table 5
 are all comparatively small: Within the limits of estimation variability, the simple and complex models converge on similar characterizations of the underlying memory distributions. 
Figures 11 and 12
 show, respectively, marginal and joint distributions of error and RT for Experiment 4, for one subject only, again to conserve space. The patterns reproduce those for Experiments 1 and 2 and for Experiment 3, which we have not shown. The model  
-κ model (8ν, 4κ, 1ψ)
; the broken dark curves are for the power law model (4ν, 1κ, 1ψ, 2β) with separate exponents for low and high saturation stimuli (β − and β + , respectively). The green curves are for low saturation stimuli (S − ); the purple curves are for high saturation stimuli (S + ). The curves for the two sets of stimuli have been shown offset from zero for clarity.
does a good job of capturing the way in which the error distributions and RT distributions vary as a function of stimulus and wheel saturation and set size. The RT distributions show the characteristic shape of distributions predicted by diffusion process models and the JP memory model captures the way in which the shapes of the error distributions vary with set size and the change in probability mass in the tails. As with Experiments 1 and 2, there is a comparatively small slow error effect in the joint distributions. For this subject, 
Figure 10
: Estimated Jones-Pewsey distributions of memory variability for individual subjects for Experiment 4. The continuous light curves are for the free-κ model 
(12ν, 12κ, 1ψ)
; the broken dark curves are for the power law model (4ν, 1κ, 1ψ, 2β) with separate exponents for low and high saturation stimuli (β − and β + , respectively). The mustard curves are for low stimulus and low wheel saturation (S − W − ); the green curves are for low stimulus and high wheel saturation stimuli (S − W + ). The red curves are for high stimulus and low wheel saturation (S + W − ); the purple curves are for high stimulus and high wheel saturation stimuli (S + W + ). The curves for the four sets of stimuli have been shown offset from zero for clarity. the model predicts a small slow error effect for m = 1 and m = 4, but essentially none for m = 2.
Although there were individual differences in drift-rate norms there were some clear and robust effects shown by all subjects. 
Figure 13
 shows average drift rate norms as a function of set size and saturation in the two experiments when the norms were free to vary. For Experiment 3, drift rate norms were systematically lower for desaturated than for saturated colors, implying that rates of evidence accumulation were lower when making decisions about desaturated colors. However, there was also a set size effect, similar to the one in Experiment 2, in which norms decreased with set size, which interacted with saturation, such that the difference between saturated and """

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]