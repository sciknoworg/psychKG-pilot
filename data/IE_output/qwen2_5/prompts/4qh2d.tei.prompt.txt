You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
The view that perceptions, sensations and evaluations depend on their context was already a central tenant of the late 19th century's Gestalt psychology theory 
[1]
 and of early Utility theory 
[2]
. A century later, the pervasiveness of perceptual illusions and decision-making biases, combined with decades of research in psychology, economics and neurosciences, consolidated the notion that perceptual and economic decisions are highly susceptible to contextual effects 
[3]
. A significant fraction of contextual effects seems to result from two fundamental computations: reference-point centring and range adaptation 
[4]
[5]
[6]
.
In most ecological and real-life situations, decisions are arguably strongly influenced by the retrospective recollection of past outcomes experienced in similar situations 
[7]
. Yet, in these experience-based decisionsrealm of the reinforcement-learning framework -, the notion of outcome context-dependence has been mostly neglected, until recent times 
[8,
9]
. Here, we review recent experimental work demonstrating that, in human reinforcement learning, outcomes are encoded and remembered as a function of the learning context.
By building on earlier work in perceptual decision-making, we consider outcome contextdependence as a manifestation of adaptive coding. Adaptive coding formalizes the idea that the (neural) representation of a variable is constrained by its underlying statistical distribution (the context 
[4,
5]
). Analogously, in reinforcement learning, outcome encoding is influenced by the distribution of outcomes experienced in the same or similar contexts.


Outcome reference point-dependence in reinforcement learning
Harry 
Helson (1898
Helson ( -1977
's adaptation-level (AL) theory constitutes the first systematic empirical investigation and theoretical formalization of the reference point-dependence of perceptual judgments 
[10]
. AL theory postulates that perceptual features (such as luminosity, loudness and weight) are evaluated relative to a norm (or adaptation level) as follows:
= − ̅
where is the judgement of a particular stimulus on a specific attribute, is the objective value of the same stimulus in the perceptual attribute under consideration, and ̅ is the norm, namely the arithmetic mean of all stimuli relevant to defining the context. The norm constitutes a reference point, usually defined as the running average of similar stimuli recently or simultaneously sampled, which is used as a point of comparison to judge the currently experienced stimulus (centring). By importing the AL core intuition into the realm of economic judgment and decision-making, Kahneman and Tversky proposed that the utility of an expected outcome does not reflect its objective value, but rather a sense of gain or loss, relative to a reference point. Reference-point dependence is therefore an intrinsic feature of prospect theory (PT 
[11,
12]
).
In a recent study, we tested if reference point-dependence affects the way outcomes are encoded (and stored in memory) in human reinforcement learning [**13]. Our behavioral paradigm joins a learning phase with a transfer phase 
[14,
15]
. Initially, during the learning phase, participants had to choose between options presented as fixed pairs of cues that were associated with a probabilistic outcome. The type of outcome defined the learning context: 'gain' (i.e. reward maximization) or 'loss' (i.e. punishment minimization) ( 
Figure 1A)
. In the transfer phase, participants were required to express their option preference for each pairwise possible combination, including hybrid combinations of options from different learning contexts ( 
Figure 1B)
. Two key behavioral results emerged: i) during learning phase, accuracy was well above chance and remarkably similar in the gain and the loss contexts; ii) option preferences in the transfer phase violated the strictly monotonic ranking dictated by their expected values ( 
Figure 1A-B)
. More specifically, we found a significant preference for the small-loss option over the small-gain option. Crucially, these two key effects violate the predictions of outcome encoding by a standard Q-learning algorithm. In the learning phase, the standard model predicts lower performance in the loss condition: a phenomenon due to an intrinsic asymmetry in reinforcement rate in the gain and loss contexts (a.k.a. the punishment learning paradox 
[16]
[17]
[18]
. In the transfer phase the standard model predicts a strictly monotonic ranking of option preferences as a function of their objective values (see Box 1). By following the intuition of AL and PT theories, we proposed a model that learns the value of a reference-point and uses it to dynamically center the outcomes before computing the optionspecific prediction error ( 
Figure 1C)
. We refer to this model as the REFERENCE model. This model successfully explains symmetrical gain-loss performance in the learning phase and the suboptimal preference pattern in the transfer phase. Moreover, it outperforms the standard Qlearning model in a broad range of conditions, arguing in favor of outcome reference-point dependence in reinforcement-learning. This result has been replicated not only in our laboratory, but also in other studies and featuring different designs, including social learning 
[19]
 and different option contingencies, arrangements and manipulations [*20 
-22]
. combinations. Here, we focus on the most informative combinations (AC and BC). The hallmark of outcome reference-point dependence is the preference for C over B in the BC comparison (green bar). While these behavioral signatures observed in both the learning and the transfer phase strikingly contrast with a model assuming objective outcome encoding (white dots), they are well captured by the REFERENCE model (black dots). Of note, choice pattern in the AC is also informative and indicates that the centering is only partial. (C) Evolution of the contextual variables (top panel) and subjective outcomes (bottom panel). The top panel illustrates the canonical temporal evolution of the reference points in the gain and loss contexts. Halfway through the learning phase, the reference points cross the expect value of the small gain/loss options. The bottom panel illustrates the resulting evolution of the average subjective outcomes for each option. Symmetrically to the top panel, roughly halfway through the learning phase, the subjective value of the outcomes of the EV 25 and EV -25 options started to be subjectively 'perceived' as negative and positive, respectively.


Outcome range-adaptation in reinforcement learning
In the late 20th century, Allen Parducci revealed the presence of context-dependence in affective judgements of happiness, pleasure and pain, and formalized his findings in the range frequency (RF) theory 
[23]
. Of particular interest to our review is Parducci's 'range principle', which describes the subjective judgement of a stimulus as:
= − −
where is the objective value of the stimulus in the perceptual attribute under consideration, while and are the highest and lowest values presented in the relevant context, bounding the range of possible outcomes. Essentially, the range principle states that subjective valuation is adapted to the underlying distribution of stimuli through a normalization rule. Recently, 
Kontek
 and Lewandoswky translated this idea into description-based decision-making by proposing the range-dependent utility model as an alternative to PT [*24]. The model assumes that the prospective valuation of the expected payoff of lotteries is range-adapted and accounts for several known behavioral paradoxes 
[25]
.
In a couple of recent studies, we tested if the range principle also applies to outcome encoding and retrospective retrieval from memory in reinforcement learning [**26,**27]. We built upon the previous behavioral paradigms to include systematic manipulation of outcome magnitudes, generating learning contexts with different outcome ranges. As in the previous study, the learning phase was followed by a transfer phase, which included new combinations of options 
(Figure 2A-B)
. Again, two key results emerged from these studies: i) accuracy was very similar in the small and the big magnitude contexts; ii) in the transfer phase, participants' choice-elicited preferences were not consistent with the objective outcome values. Notably, options that were locally correct in the small magnitude contexts were systematically preferred to options that were locally incorrect in the big magnitude contexts, despite their objective expected values having the opposite ranking. A standard Q-learning model (with objective outcomes and softmax decision rule 
[28]
) fails to predict this pattern, because its choice probabilities (and therefore accuracy) are strongly affected by the relative magnitudes of the option values. In line with RF theory, we proposed a model that learns the range of possible outcomes and uses it to dynamically rescale the outcomes before computing the option-specific prediction error ( 
Figure 2C)
. This model, referred to as the RANGE model (see Box 1), satisfactorily captures the key behavioral effects. In our last study 
[27]
, we also modulated the difficulty of the learning phase in two ways: by manipulating outcome information (partial vs. complete feedback) and by manipulating the task structure (blocked vs interleaved design). We found that outcome range adaptation was more pronounced in the easiest settings (block design, complete feedback), consistent with the idea that these manipulations enabled the participants to identify the context-relevant variables more easily. Crucially, as predicted by the RANGE model, this result was accompanied by a reduction in the subjects' ability to successfully extrapolate option values in the transfer phase. This finding is in striking opposition to the almost universally shared intuition that reducing task difficulty should lead, if anything, to more accurate and rational behavior 
[29,
30]
.
Another recent study investigated choices in a reinforcement learning paradigm featuring repeated choices between a deterministic (i.e. risk-free) and a probabilistic (i.e. risky) option. Results showed that the outcome range matters in subjective outcome values [*31]. Specifically, the authors convincingly demonstrated that risk preferences were strongly driven by an increased saliency of the extreme (i.e. the highest and the lowest possible) outcomes presented locally, in a given context, rather than being attached to any specific objective outcome value. 


Dependence of irrelevant alternatives in human reinforcement learning
In the first part of this review, we provided evidence generalizing two manifestations of contextual influences to reinforcement learning: reference-point dependence and range normalization. However, the notion of context in psychology and economics is much richer, encompassing many other dimensions 
[32]
. Particularly relevant for economic decision-making is the notion of the choice set or menu. In fact, standard normative theories assume that decision-makers (should) evaluate options in a way that the relative probability of choosing, say, option A over B, should be independent of the presence of a third alternative, say C (the so-called independence of irrelevant alternatives axiom, IIA 
[28]
). Despite being intuitive, IIA is quite often violated in descriptionbased decisions 
[33]
. A recent study actually demonstrated that such contextual effects induced by the choice set also occur in reinforcement learning [**34]. The authors designed several behavioral paradigms, where participants choose between three options whose expected values were designed to elicit classic violations of IIA. However, the expected values were to be learned by trial-anderror. Their results reveal that similar outcomes 'inhibit' each other, whereas the most dissimilar (hence salient) outcomes 'stand out'. This bottom-up attentional bias can be captured by a computational model that decreases the subjective value of an outcome as a function of the cumulative sum of the perceived similarity between a given outcome and others concomitantly presented 
1
 . As a result, this study illustrates that contextual effects created by the choice set also extend to reinforcement learning scenarios. In these scenarios, contextual effects produce preference patterns that sometime oppose those observed in decision among lotteries, thus providing a new instance of the experience-description gap 
[8,
9,
35]
.


Relation to behavioral economics research and alternative computational frameworks
We reviewed converging evidence in support of the idea that the subjective value of an outcome is strongly influenced by the learning context, derived from the distribution of other and past outcomes 
[13,
19,
20,
26,
27,
31,
34]
. This body of work suggests that context plays a role in virtually all types of decision-making, possibly via the recycling of similar neural computational processes and constraints 
[3]
[4]
[5]
.
Contextual factors, such as the reference point, are central to theories of description-based decisions (such as PT). Although experimental evidence suggests that the reference point is dynamically updated by the choice history, the exact algorithm and mechanisms remain to be specified 
[36,
37]
, thus weakening the theory 
[38]
. Importing learning models into the decision-bydescription framework and leveraging functional neuroimaging methods could prove useful in bridging this gap, both at the normative and descriptive levels 
[13,
39]
. We also proposed that range adaptation can be implemented via a range normalization mechanism, based on the learned maximum and minimum possible outcomes 
[27,
31]
. Although reinforcement learning traditionally relies on behavioral paradigms featuring unidimensional outcomes (the numeric reward), multi-attribute choice is another canonical situation in which the choice menu has been shown to be critical 
[33]
. In this context, range normalization could apply to each attribute separately, generating and explaining the decoy effects observed in classical description-based decisions with a computationally tractable mechanism 
[40]
. Context effects in choice can be understood through an alternative computational formalism, the decision-by-sampling (DbS) framework 
[41]
. The DbS framework supposes that the subjective value of an option comes from a series of ordinal comparisons with outcomes drawn from memory. Since subjective values come from comparisons with other options, context-effects arise naturally from the DbS formalism 
[42]
. Furthermore, DbS could provide a unified framework for description-(sampling from distributions) and experience-based (sampling from memory) decision-making. Particularly relevant for our treatment, a recent elegant study showed that DbS concomitantly generates range effects and achieves efficient coding of information [*43].


What are the functional roles of outcome context-dependence in reinforcement learning?
Converging evidence shows that outcome context-dependence systematically induces suboptimal choices when options are extrapolated beyond their original learning contexts in the transfer phase 
(Figure 1-2)
. This is particularly striking as similar behavioural findings have been found in distant species, such as rodents 
[44]
 and birds [ 
*45,46]
. Identifying predictable sources of biases is always puzzling, because evolutionary forces should have, in principle, negatively selected for processes leading to suboptimal choices. Our work shows that context dependency can, of course, improve learning performance in specific conditions (loss avoidance, small magnitude). However, most of these beneficial learning effects could be achieved by normalizing value signals at the choice phase, rather than at the learning and memorization phase, without bearing the costs of irrational preferences in the transfer phase. We speculate two possible functional roles for this learning bias. First, outcome context-dependence could simply result from adaptive and efficient (neural) coding principles, thereby optimizing information processing during learning 
[4,
5]
. Alternatively, while context-dependent learning induces suboptimal choices in our laboratory setting, they may be evolutionarily rational, meaning that they generate, on average, optimal performance in the environments where they evolvede.g. in environments where the resources are highly volatile 
[47,
48]
.


Option value learning or direct policy learning?
A whole spectrum of models exists in the reinforcement learning framework, ranging from models assuming that expected values are learned for individual options (such as Q-learning), to models assuming that choice policies are learnt without intermediate option values representations (such as direct policy learning) [*49]. The latter hypothesis was backed up by evidence from a couple of studies on humans, where direct policy learning methods better explained subjects' choices in complete feedback tasks, at least in some critical trials 
[20,
50,
51]
. However, while the empirical data reviewed here clearly falsify the Q-learning's assumption that option-values are learned on a context-independent (or objective) scale, they also reject the equally extreme predictions of direct policy learning, by showing residual effect of outcome valence and magnitude in option value preferences 
(Figure 1 and Figure 2)
 
[13,
26,
27]
. We therefore propose a hybrid scenario where option-specific values are computed, but based on subjective outcomes that are encoded in a context-dependent manner.


Open questions
The present demonstration of context-dependent outcome encoding 
(Figure 1 and Figure 2
) relies on a combination of an instrumental learning phase and of a transfer phase eliciting preference as instrumental choices (e.g., in a procedural manner). Whereas recent evidence suggests that the Pavlovian learning system presents similar outcome encoding constraints 
[52]
, future studies should investigate address whether the same mechanism generalizes to other learning (Pavlovian, instrumental, goal directed) and representational (declarative, episodic) systems 
[53,
54]
. Finally, although we focused our review on situations, where context-dependent reinforcement learning concurrently benefits the learning phase and undermines generalization, an exhaustive investigation of learning and transfer environments could potentially identify situations where this trade-off can be tipped in favor of better generalization.
To conclude, investigating the effects of past outcomes on learning opens up a promising window, not only to define and formalize contextual effects (Box 2), but also to understand how the subjective, hedonic perception of outcomes shapes preferences. Deciphering the mechanisms and properties of reference-point dependence and range adaptation may also be key to appreciating the neurobiological encoding of learning and decision-related variables [** 
13,39,*55,56]
.
Box Reinforcement-learning models with outcome context dependence.
Both the REFERENCE 
[13]
 and the RANGE 
[27]
 models build on a standard Q-learning model, applied to a two-armed bandits task with complete feedback information 
[57]
. For each pair of cues (i.e. state ), the REFERENCE model learns a reference point ( ), often referred to as context-or state-value ( ), which is updated as follows:
( ) ← ( ) + * ( ( ) + ( ) 2 −
(
)) ( ) and ( ) are the outcome of the chosen and unchosen option respectively, while is a learning rate (0 < < 1) 2 (see 
Figure 1C
 for the temporal evolution of the variable). ( ) is then used to calculate the subjective outcome for each option as follows:
( ( )) = ( ) − ( ) The RANGE model infers two context-level variables:
( ) and ( ), which are updated as follows:
( ) ← ( ) + * (max ( (: )) − ( )), max( (: )) > ( ) ← ( ) + * (min ( (: )) − ( )), min( (: )) < where ( (: )) and ( (: )) are respectively the highest and lowest possible outcomes observed in a given trial, while is a learning rate (0 < < 1) (see 
Figure 2C
 for the temporal evolution of the variable). In this formulation, ( ) / ( ) can only increase / decrease: it suits only tasks where the range does not change over time. The model could easily accommodate situations where the range changes over time, by simply assuming that ( ) is updated at a smaller rate when the observed outcome is smaller than the current estimation of ( ) (the opposite holds true for ( )) 
[23]
. This variable is then used to calculate the subjective outcome for each option as follows 
3
  2 Of note, the model proposed by Klein et al. (2017) is a special case of the REFERENCE model described (when = 1). 
3
 In the denominator '+1' is added for computational convenience. It could be replaced by a free-parameter to account for task-specific differences, akin to a semi-saturation term governing the efficiency of the normalization 
[5]
.
Where is a learning rate (0 < < 1). Both models make decisions with a standard 'softmax' decision rule with a fixed temperature parameter. These models have been shown to satisfactorily account for the behavioral patterns in both the learning phase and transfer phase (see 
Figure 1C
 and 
Figure 2C)
, which falsify several plausible alternative formulations in reinforcement learning (actor-critic, habit learning 
[17,
58]
) and in neuroeconomics (subjective utility, divisive normalization 
[2,
59]
). As a final remark, even if the two models are not mathematically nested, the subtraction of ( ) at the numerator of the range normalization rules indicates that the RANGE model also implies the possibility that objectively negative outcomes can be reframed as subjectively positive.


Box 2: multiple definitions of context
This quote from Parducci clearly illustrates how broadly the term context can be interpreted 
[23]
:
"The term context refers to a conceptual representation of a set of events, real or imaginary, determining the dimensional judgement of any particular event" (p. 36). In this box we summarize the meaning of context in the main studies reviewed here, building on an analogy to its definition in perceptual and value-based decision-making 
[6,
60]
. In visual cognition, 'spatial' context refers to objects simultaneously presented with a target stimulus. Since spatial contextual effects do not require inferring a hidden contextual variable, as all relevant information is immediately given, in reinforcement learning, we can see this concept as the simultaneous presentation of outcomes affecting the subjective value of an outcome. While this definition of context becomes ambiguous when the outcome of the chosen option is displayed alone, it has been used to build models of complete feedback paradigms [*20,**34]. 'Temporal' context refers to objects presented in the (more or less recent) past. This is how contextual effects are defined in the REFERENCE and the RANGE models [**13,**27]. In this case, outcome context-dependence is driven by hidden contextual variables (such as reference point, or the maximum possible reward), whose values are inferred from the history of past outcomes. This definition of context applies to both partial and feedback tasks, since it does not require simultaneous presentation of all feedback information. A corollary question concerns the time horizon for temporal context integration. Evidence suggests that contextual variables can be computed simultaneously over different time scales 
[61,
62]
. Both the 'spatial' and the 'temporal' definitions of context given above are implicit, i.e. they are not attached to any particular cue. However, evidence suggests that contextual information can be attached to explicit value-neutral stimuli (e.g. visual cues, background colors) that can then be used to generalize contextual information to new options, without the need to experience the relevant outcomes 
[63,*64]
. Finally, we mainly focused on external contexts (i.e. derived from stimuli). However, the internal state of an organism can also contribute to define a context. There is ample evidence that the level of satiation or the current energetic budget strongly influences memory and decision-making 
[65]
. Accordingly, the recently developed framework of homeostatic reinforcement learning postulates that the subjective value of an outcome is determined by whether a given outcome moves toward (or away from) a homeostatic set-point 
[66,*67]
, defining an alternative formulation of outcome context-dependence in reinforcement learning. It is important to disambiguate the term 'state', which has different meanings in different fields. In ethology and foraging research 'state' refers to the internal (physiological) status of the organism 
[47]
, while in animal and reinforcement learning literature, it mainly refers to a node in a Markov decision-process, roughly synonym of what we refer to as "external context" 
[57]
.


References * of interest
Figure 1 :
1
reference point-dependence in RL: task, results and model variables. (A) Learning phase contexts (top panel) and typical behavior (bottom panel). Subjects are presented for several trials with two learning contexts: AB (gain-maximization context) and CD (loss-minimization context). Feedback is probabilistic. Accuracy typically starts at chance level and progressively increases, reaching a similar plateau in both learning contexts. (B) Transfer phase contexts (top panel) and typical behavior (bottom panel). After the learning phase, symbols are re-arranged in new


Figure 2 :
2
range adaption in RL: task, results and model variables. (A) Learning phase contexts (top panel) a typical behavior (bottom panel).Subjects are presented for several trials with two learning contexts: AB (big-magnitude context) and CD (small magnitude context). Feedback is probabilistic. Accuracy typically starts at chance and progressively increases reaching a quite similar plateau in both learning contexts. (B) Transfer phase contexts (top panel) and typical behavior (bottom panel). After the learning phase, symbols are re-arranged in new combinations. Here, we focus on the most informative combinations (AC and BC). The hallmark signature of outcome range adaptation is the preference for C over B in the BC comparison (green bar). While these behavioral signatures observed in both the learning and the transfer phases strikingly contrast with a model assuming objective outcome encoding (white dots), they are well captured by the RANGE model (black dots). Of note, choice pattern in the AC is also informative, as it indicates that the range adaptation is only partial. (C) Evolution of the contextual variables (top panel) and subjective outcomes (bottom panel). The top panel illustrates the canonical temporal evolution of the ranges in the big and small magnitudes contexts. To the end of the learning phase, the ratio between the expected value of the options and the range values become similar in the big and small magnitude contexts. Crucially, Rmax and Rmin updates are conditional of R > Rmax and R<Rmin, respectively (see Box1). The bottom panel illustrates the evolution of the average subjective outcomes for each option. Notably, approximately halfway through the learning phase, the subjective value of the outcomes of the EV2.5 and EV0.75 cross over.


Finally, both models assume that option values (: , ) are updated following the standard update rule:( , ) +1 = ( , ) + * ( ( ( ) ) − ( , ) )


Of note, while the authors investigated only the complete feedback case, they argue that their model could be easily extended to the partial feedback case, by assuming that outcome comparison occurs between the presented outcome and previous outcomes stored in memory. To our knowledge, this hypothesis remains to be empirically verified.








Conflict of interest statement
Nothing declared
 












G
T
Fechner




Elemente der psychophysik. Breitkopf und Härtel; 1860












Specimen theoriae novae de mensura sortis




D
Bernoulli








Comment Acad Sci Imp Petropolitanae




1738














Maps of Bounded Rationality: Psychology for Behavioral Economics




D
Kahneman








Am Econ Rev




93
















Normalization as a canonical neural computation




M
Carandini






D
J
Heeger








Nat Rev Neurosci




13
















Efficient coding and the neural representation of value




K
Louie






P
W
Glimcher








Ann N Y Acad Sci




1251
















Value normalization in decision making: theory and evidence




A
Rangel






J
A
Clithero








Curr Opin Neurobiol




22
















A framework for studying the neurobiology of value-based decision making




A
Rangel






C
Camerer






P
R
Montague








Nat Rev Neurosci




9
















The description-experience gap: a challenge for the neuroeconomics of decision-making under uncertainty




B
Garcia






F
Cerrotti






S
Palminteri








Philos Trans R Soc B Biol Sci




376














The description-experience gap in risky choice




R
Hertwig






I
Erev








Trends Cogn Sci




13
















Adaptation-level theory: an experimental and systematic approach to behavior




H
Helson








New York












Prospect Theory: An Analysis of Decision under Risk




D
Kahneman






A
Tversky








Econometrica




47


263














Replicating patterns of prospect theory for decision under risk




K
Ruggeri






S
Alí






M
L
Berge






G
Bertoldo






L
D
Bjørndal






A
Cortijos-Bernabeu






C
Davison






E
Demić






C
Esteban-Serna






M
Friedemann








Nat Hum Behav




2020














Contextual modulation of value signals in reward and punishment learning




S
Palminteri






M
Khamassi






M
Joffily






G
Coricelli








Nat Commun




6


8096














This study investigates reference-point dependence human reinforcement learning, showing that the REFERENCE model represents an efficient computational solution to punishment avoidance problem




*








The paper also present neurophysiological evidence supporting the computational assumptions of the REFERENCE model








By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism




M
J
Frank






L
C
Seeberger






O'
Reilly






R
C








Science




306
















Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans




M
Pessiglione






B
Seymour






G
Flandin






R
J
Dolan






C
D
Frith








Nature




442
















A temporal difference account of avoidance learning




M
Moutoussis






R
P
Bentall






J
Williams






P
Dayan








Netw Comput Neural Syst




19
















Two-factor theory, the actor-critic model, and conditioned avoidance




T
V
Maia








Learn Behav




38


















O
H
Mowrer




Learning Theory and Behavior




John Wiley & Sons














Partial Adaptation of Obtained and Observed Value Signals Preserves Information about Gains and Losses




C
J
Burke






M
Baddeley






P
N
Tobler






W
Schultz








J Neurosci




36
















Learning relative values in the striatum induces violations of normative decision making




T
A
Klein






M
Ullsperger






G
Jocham








Nat Commun




8


16033














Transfer phase performance is undermined by outcome context-dependent learning as






* This study investigates context-dependent reinforcement learning in humans coupling behavioral and neural analyses






13,25,26








Neurophysiological data are consistent with the notion of relative outcome encoding












Contextual influence on confidence judgments in human reinforcement learning




M
Lebreton






K
Bacily






S
Palminteri






J
B
Engelmann








PLOS Comput Biol




15


1006973














The elusive effects of incidental anxiety on reinforcement-learning




C-C
Ting






S
Palminteri






M
Lebreton






J
Engelmann




10.31234/osf.io/7d4tc






J Exp Psychol Gen in press












Parducci A: Happiness, pleasure, and judgment: The contextual theory and its applications




Lawrence Erlbaum Associates, Inc














Range-Dependent Utility




K
Kontek






M
Lewandowski








Manag Sci




64
















This study provides an axiomatic basis for a range-dependent utility model, adapting Parducci's range-frequency theory to decision-making under risk. The model is shown to account for several known decision paradoxes






including Allais' paradox








Advances in prospect theory: Cumulative representation of uncertainty




A
Tversky






D
Kahneman








J Risk Uncertain




5
















Reference-point centering and range-adaptation enhance human reinforcement learning at the cost of irrational preferences




S
Bavard






M
Lebreton






M
Khamassi






G
Coricelli






S
Palminteri








Nat Commun




9


4503














This study reveals the co-existence of reference-point dependence and range adaptation in human reinforcement learning. Among other results, it also shows that contextual biases are positively correlated with an explicit understanding of the task structure




*














Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning




S
Bavard






A
Rustichini






S
Palminteri








Sci Adv




7


340














This study investigates range adaptation in human reinforcement learning. It provides evidence in favor of the RANGE model (described in Box 1) over eight experiment and by re-analyzing data from




*








25]. The paper also explores the paradoxical relation between learning and transfer phase performance








Individual Choice Behavior: A Theoretical Analysis




R
D
Luce








Courier Corporation
















Rational choice and economic behavior




R
H
Day








Theory Decis




1
















Rationality for Economists?




D
L
Mcfadden








J Risk Uncertain




19
















Living near the edge: How extreme outcomes and their neighbors drive risky choice




E
A
Ludvig






C
R
Madan






N
Mcmillan






Y
Xu






M
L
Spetch








J Exp Psychol Gen




147
















* By investigating risk preferences in human reinforcement learning, this study found that they are not determined by the value of the outcomes per se, but rather they are driven by over-representation in memory of the contextually more salient outcomes












Chapter 24 -The Neurobiology of Context-Dependent Valuation and Choice




K
Louie






De
Martino






B








Neuroeconomics


Glimcher PW, Fehr E




Academic Press


2014








Second Edition








Cognitive and Neural Bases of Multi-Attribute, Multi-Alternative, Value-based Decisions




J
R
Busemeyer






S
Gluth






J
Rieskamp






B
M
Turner








Trends Cogn Sci




23
















How similarity between choice options affects decisions from experience: The accentuation-of-differences model




M
S
Spektor






S
Gluth






L
Fontanesi






J
Rieskamp








Psychol Rev




126


52














This compelling study investigates classical IIA violations in human reinforcement learning and provides clear evidence for outcome context-dependence in several experiments. The results are consistent with an attentional bias, where similar outcomes are reciprocally inhibited




*














The Effect of Experience on Context-dependent Decisions




E
Ert






T
Lejarraga








J Behav Decis Mak




31
















A cross-cultural study of reference point adaptation: Evidence from China, Korea, and the US




H
R
Arkes






D
Hirshleifer






D
Jiang






S
S
Lim








Organ Behav Hum Decis Process




112
















Reference-Point Formation and Updating




M
Baucells






M
Weber






F
Welfens








Manag Sci




57
















Searching for the Reference Point




A
Baillon






H
Bleichrodt






V
Spinu








Manag Sci




66


















F
Rigoli






K
J
Friston






R
J
Dolan








Neural processes mediating contextual influences on human choice behaviour






7


12416












A Range-Normalization Model of Context-Dependent Choice: A New Model and Evidence




A
Soltani






B
D
Martino






C
Camerer








PLOS Comput Biol




8


1002607














Decision by sampling




N
Stewart






N
Chater






G
D
Brown








Cognit Psychol




53
















Does the brain calculate value?




I
Vlaev






N
Chater






N
Stewart






G
D
Brown








Trends Cogn Sci




15
















Decision by sampling implements efficient coding of psychoeconomic functions




R
Bhui






S
J
Gershman








Psychol Rev




125


985














* This theory paper shows how decision-by-sampling can normatively emerge as a way to efficiently represent information in a noisy channel. The paper also shows that the resulting model displays behavior compatible with Parducci's range-frequency principle














C
F
Flaherty




Incentive Relativity




Cambridge University Press














Context-dependent utility overrides absolute memory as a determinant of choice




L
Pompilio






A
Kacelnik








Proc Natl Acad Sci




107
















* To our knowledge, this study (in European starlings) is the first one to demonstrate contextdependent outcome encoding in an instrumental learning paradigm combining a learning (training) and a test (transfer) phase. This design served as a base for later studies in humans






26,27








Context-Dependent Preferences in Starlings: Linking Ecology, Foraging and Choice




M
Vasconcelos






T
Monteiro






A
Kacelnik








PLOS ONE




8


64934














The ecological rationality of state-dependent valuation




J
M
Mcnamara






P
C
Trimmer






A
I
Houston








Psychol Rev




119


114














An Adaptive Response to Uncertainty Generates Positive and Negative Contrast Effects




J
M
Mcnamara






T
W
Fawcett






A
I
Houston








Science




340
















The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)




B
Hayden






Y
Niv




10.31234/osf.io/7hgup




2020












Thought-provoking opinion paper challenging the common assumption that the brain represents expected value and arguing in favor of heuristic decision-making and direct policy learning












Signals in human striatum are appropriate for policy update rather than value prediction




J
Li






N
D
Daw








J Neurosci Off J Soc Neurosci




31
















Regret in Experience-Based Decisions: The Effects of Expected Value Differences and Mixed Gains and Losses




W
Hayes






D
Wedell




10.31234/osf.io/xaeyn
















Decomposing the effects of context valence and feedback information on speed and accuracy during reinforcement learning: a metaanalytical approach using diffusion decision modeling




L
Fontanesi






S
Palminteri






M
Lebreton








Cogn Affect Behav Neurosci




19
















Chapter 24 -Multiple Forms of Value Learning and the Function of Dopamine




B
W
Balleine






N
D
Daw






J
P
O'doherty






P
W
Glimcher






C
F
Camerer






E
Fehr






R
A
Poldrack








Neuroeconomics




Academic Press
















Memory systems of the brain: A brief history and current perspective




L
R
Squire








Neurobiol Learn Mem




82
















Assessing inter-individual differences with task-related functional neuroimaging




M
Lebreton






S
Bavard






J
Daunizeau






S
Palminteri








Nat Hum Behav




3
















* This perspective papers claims (and mathematically demonstrates) that hypotheses concerning the normalization of the outcome signals bear heavy consequences on how neural data (especially functional magnetic resonance) should be analyzed and interpreted in modelbased fMRI approaches












BOLD Subjective Value Signals Exhibit Robust Range Adaptation




K
M
Cox






J
W
Kable








J Neurosci




34
















Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








Cambridge University Press












Habits without values




K
J
Miller






A
Shenhav






E
A
Ludvig








Psychol Rev




126


292














The Normalization of Consumer Valuations: Context-Dependent Preferences from Neurobiological Constraints




R
Webb






P
W
Glimcher






K
Louie








Manag Sci




67
















Adaptive neural coding: from biological to behavioral decision-making




K
Louie






P
W
Glimcher






R
Webb








Curr Opin Behav Sci




5
















Multiple timescales of normalized value coding underlie adaptive choice behavior




J
Zimmermann






P
W
Glimcher






K
Louie








Nat Commun




9


3206














Adaptive Value Normalization in the Prefrontal Cortex Is Reduced by Memory Load




L
Holper






Ldv
Brussel






L
Schmidt






S
Schulthess






C
J
Burke






K
Louie






E
Seifritz






P
N
Tobler








4












Rational choice, context dependence, and the value of information in European starlings (Sturnus vulgaris)




E
Freidin






A
Kacelnik








Science




334
















Encoding Context Determines Risky Choice




C
R
Madan






M
L
Spetch






Fmds
Machado






A
Mason






E
A
Ludvig








Psychol Sci




32
















*This paper clearly shows that information about outcome range can be attached to external cues (background images in this case), thus influencing the valuation of newly presented options






It also shows that contextual endpoints (Rmax and Rmin) are preferentially remembered








State-Dependent Decisions Cause Apparent Violations of Rationality in Animal Choice




C
Schuck-Paim






L
Pompilio






A
Kacelnik








PLOS Biol




2


402














Where Does Value Come From?




K
Juechems






C
Summerfield








Trends Cogn Sci




23
















Homeostatic reinforcement learning for integrating reward collection and physiological stability




M
Keramati






B
Gutkin








3


4811













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]