You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



oriented and typically a means to another end-most often financial gain, for example, when considering embezzlement, corruption, and fraud.
Here, we review the current state of research on the science of (dis)honesty and suggest directions for future study. Our review focuses on (dis)honest behavior driven by instrumental motivations, such as financial gain, and is grounded in the behavioral ethics literature. In short, we argue that we know a lot about people's (dis)honesty in a very specific context, namely, when people make decisions that affect only themselves, without fear of being sanctioned for lying.
We know much less about more realistic contexts in which lying may lead to sanctions, and in which people's lies may influence, or sometimes depend on, others. Our framework calls for more research on these less well-understood, but very relevant, contexts.


The State of the Art: People Are Rather Honest, But Not Always
Fifteen years of behavioral research on honesty suggests that although people tend to be quite honest, they are not always 
(Abeler et al., 2019;
Stephens-Davidowitz, 2017)
. Two seminal studies 
(Mazar et al., 2008;
Fischbacher & Föllmi-Heusi, 2013)
 have led the way in examining the motives behind people's propensity to lie. Using a variety of experimental tasks, both papers made an important finding: when individuals can lie to increase their own gains without risk of detection or punishment, many still refrain from lying to the maximum extent possible.
Moreover, a significant fraction of individuals will resist lying even for substantial rewards, such as €150 for a single lie .
From a rational economic perspective, this behavior is puzzling. When no risk of detection or punishment exists, one would expect people to seize the opportunity to maximize their returns by lying. An important explanation for why people refrain from lying is their desire to maintain a positive self-image. In other words, even when people lie, they still want to perceive themselves as moral.
According to the psychological framework of self-maintenance (e.g., 
Mazar et al., 2008)
, dishonest behavior creates an internal conflict between two opposing goals. On the one hand, people highly value their morality 
(Greenwald, 1980;
Sanitioso et al., 1990)
 and strive to maintain it as a central aspect of their self-image 
(Aquino & Reed, 2002;
Rosenberg et al., 1995)
.
On the other hand, when the opportunity to benefit from lying arises, resisting the temptation is difficult. Caught between a rock and a hard place, this internal tension-referred to as ethical dissonance-poses a serious threat to people's self-concept and moral identity 
(Barkan et al., 2015)
. The observation that people cheat, but only to a certain extent (e.g., 
Hochman et al., 2016;
Mazar et al., 2008;
Gneezy et al., 2018;
, reflects a solution whereby people benefit from lying while perceiving themselves as moral 
(Shalvi et al., 2015)
.
The extent to which people avoid lying depends largely on the context in which they face the temptation. When lying benefits only those who lie, people limit their lying 
(Abeler et al., 2019;
Gerlach et al., 2019;
Kobis et al., 2019)
. However, a study by 
Leib et al. (2021)
 found that when lying requires collaboration-when two or more people must lie together-about one-fifth of their 2,716 participants lied to the fullest extent possible, engaging in what has been termed "collaborative dishonesty" 
(Weisel & Shalvi, 2015)
. Collaborating with others appears to provide a justification for lying, especially when doing so benefits the group to which one belongs 
(Shalvi & De Dreu, 2014)
. Similarly, when lying benefits others, even when collaboration is not required, people lie more than they would if lying did not benefit others 
(Wiltermuth, 2011;
Klein et al., 2017;
Conrads et al., 2013)
.
Self-serving justifications, such as benefiting another person, shape how much people lie.
Consider, for example, the die-under-cup task used by  to study the role of self-serving justifications (see 
Figure 1
). Building on the task introduced by 
Fischbacher and
 Föllmi-Heusi (2013), participants were asked to privately roll a die under a paper cup to determine their pay.  paid participants according to the number they reported rolling; reporting a "1" meant earning US$1, reporting a "2" meant earning US$2, and so on until reporting a "6" resulted in earning US$6. If participants are honest, they should report the number they observed. With a large enough sample and a fair dice, the distribution of reported outcomes, assuming honesty, should be flat. A similar proportion of participants should report 1s, 2s, 3s, 4s, 5s, and 6s. If all participants are dishonestly trying to maximize their payoff, they should all report "6."
A key variation in the study was the number of dice rolls participants were able to observe before reporting. When participants rolled only once, their lying was minimal. However, when they were allowed to roll three times and report only the result of the first roll, lying increased. The logic is that with multiple rolls, participants could justify lying by reporting a higher number that appeared on one of the rolls that does not count for pay. In this scenario, reporting a counterfactual-an observed higher number that did not count-seemed to provide participants with a moral buffer that made dishonesty feel more justified than simply making up a number. Relatedly, as the baseline probability of winning increases, people are more likely to lie and report winning 
(Schild et al., 2020)
.
Aggregate evidence suggests self-serving justifications significantly influence the extent to which people lie. A meta-analysis by 
Abeler et al. (2019)
 examined the die-rolling task described above, where participants were either instructed to roll the die once before reporting the result or were allowed to roll multiple times but only report the result of the first roll.
Although this variation was not randomly assigned as in , it still provided insight into how observing additional die rolls affects honesty. Meta-analytic results from a large sample (N = 44,390) suggest multiple rolls do indeed lead to higher, more dishonest reports.
Participants who were asked to roll only once were significantly more honest than those who were allowed to roll multiple times.
Further support for the positive influence of self-serving justifications on lying comes from cross-cultural research (N = 2,568 participants), which shows that in societies where rulebreaking is more common, participants' reports on the dice-rolling task (in which lying is assessed at the aggregate level) closely resembled the distribution that would be expected if they had reported the highest of the dice-roll outcomes that they observed, rather than the outcome of the first dice roll that they were supposed to report 
(Schulz & Gächter, 2016)
. This finding suggests that in cultures that are more tolerant of dishonesty, people are more likely to use selfserving justifications.
However, results from online adaptations of the dice-rolling task have been mixed. Some studies, such as Baro, 
Verschuere, and Nieper (working paper)
, found multiple rolls led to more dishonesty than a single roll, whereas others, such as Ay, Berge, and Nodtvedt (2024), did not observe this effect. These inconsistencies suggest potential differences between laboratory and online settings may influence the level of dishonesty and warrant further investigation to understand how context affects lying behavior (see 
Lilleholt et al., 2020)
.
Although the above summary of the current state of the art is not exhaustive, it represents the generally accepted way of conceptualizing and studying (dis)honesty. As we point out in the remainder of the manuscript, over the past 15 years, scholars have mostly (but not exclusively) focused on settings in which individuals can lie or be honest without being monitored and/or punished for lying. Our review suggests many interesting insights have emerged from such work and that several recent lines of work suggest expanding this scope can lead to a better understanding of when and why people lie.


Identifying Gaps in Knowledge: A New Framework
A major gap in our understanding of honesty lies in the extensive focus on studying behavior in a "laboratory context"-specifically, a context in which individuals act in isolation and their lies cannot be detected. Much of the existing behavioral evidence on (dis)honesty comes from such a laboratory setting. A key advantage of such a setting is that it allows researchers to examine how image concerns shape dishonesty. The premise is straightforward: if people refrain from lying even when they know their lies will not be detected or punished, they are likely concerned with maintaining a positive self-image. Accordingly, limiting one's dishonesty likely reflects a desire to maintain an honest self-concept 
(Hochman et al., 2021)
.
Although these findings are intriguing, social psychology, experimental economics, and related disciplines aim to model real-world behavior through laboratory experiments. Outside the lab, the probability of being caught in a lie is rarely zero (although it can sometimes be very low), and people do not know what that probability is 
(Thielmann & Hilbig, 2018)
. Moreover, dishonesty in everyday life is often relational. People's lies often involve others, either through collaboration or by relying on others' tolerance of deceptive behavior.
To address these shortcomings, we introduce a framework that includes two dimensions: enforcement, or the likelihood that lying will be detected and punished (ranging from low to high), and the context in which decisions are made (ranging from solitary to relational), as shown in 
Figure 2
. The figure shows most research focuses on the upper-left quadrant-where enforcement is low and individuals make decisions in isolation-whereas much less is known about the other quadrants. Our proposed framework encourages the field to explore more realistic settings where enforcement mechanisms are in place (and punishment is possible) and
where decisions may involve larger groups. We demonstrate the utility of this framework by reviewing relevant research that provides (preliminary) support for the key ideas we propose in each section, and by identifying open research questions in psychology and related fields.
The remainder of the paper is organized as follows: Section 2 introduces a framework that highlights both well-researched areas and gaps in the literature; Section 3 reviews findings on (dis)honesty in five key areas and shows how the framework can bridge existing knowledge gaps. These areas include the structure of relationships between individuals (Section 3.1:
interdependence, intragroup processes), relationship aspects (Section 3.2: social norms, trust, gossip); individual differences (Section 3.3: personality, beliefs, justifications, dynamic personality theories, care for honesty vs. pro-sociality, gender), cognitive processes (Section 3.4: ethical dissonance, attention, perspective-taking, automatic vs. controlled behavior, embodied cognition), and situational factors (Section 3.5: honesty pledges, artificial intelligence). Finally, Section 4 provides concluding remarks.


Enforcement and Compliance
Enforcement mechanisms-acts that compel compliance with a rule-can reduce dishonesty. Enforcement is typically viewed as the product of the probability of detection multiplied by the severity of the punishment for dishonesty 
(Becker, 1968)
. Although empirical crime studies have found consistent evidence that the probability of punishment is more important than its severity 
(Doob & Webster, 2003;
Nagin, 2013;
Chalfin & McCrary, 2017)
, most experimental studies have found no effect for the probability of detection (e.g., 
Mazar et al., 2008;
Peer & Gamlierl, 2013)
 or have found stronger effects for the severity of punishment than for the probability of detection (e.g., 
Anderson & Stafford, 2003;
Friesen, 2012;
Thielmann & Hilbig, 2018)
. One possible explanation for this discrepancy is the gap between these experimental studies and real-world settings. Most research on dishonesty focuses on tasks in which people have limited opportunities to lie, meaning studies of sustained deterrence are relatively rare 
(Gerlach & Teodorescu, 2022)
. In real-world situations, however, decisions are often repeated, making an understanding of sustained deterrence crucial for predicting dishonest behavior outside of the laboratory.
Focusing on sustained deterrence, 
Teodorescu et al. (2021)
 manipulated both the probability of detection and the amount of punishment, while holding the expected punishment constant. The authors created environments in which participants repeatedly encountered temptations to increase their payoffs by providing dishonest answers. Initially, participants provided answers without external enforcement, allowing a baseline level of honesty to be established. Later, an enforcement mechanism was introduced: participants' answers were checked with a certain probability, and dishonest reports resulted in fines. The study compared two enforcement policies: infrequent large fines versus frequent small fines. Both enforcement strategies increased honesty, but frequent small fines were far more effective, especially among the most dishonest participants in the no-enforcement phase.
These results suggest the frequency of enforcement is a stronger predictor of honesty than the severity of punishment. This pattern of results is consistent with empirical studies of crime, which show crime rates are more sensitive to the frequency of rule enforcement than to the severity of punishment 
(Doob & Webster, 2003;
Nagin, 2013;
Chalfin & McCrary, 2017)
. Thus, studying environments with repeated temptations to lie provides a more ecologically valid setting for understanding how external enforcement shapes honesty.
In real-world contexts, the frequency of enforcement is not constant and may vary across time and place. This scenario can result from adjustments in enforcement resources, such as increasing or decreasing surveillance or shifting focus to "crime hot spots" 
(Weisburd et al., 2023)
. As summarized in 
Table 1
, a critical question for future research is the extent to which dishonest behavior adapts to changes in enforcement frequency. The findings of 
Teodorescu et al. (2021)
 suggest individuals may be highly responsive to very low detection rates, but less adaptive to changes when enforcement is moderate or high. Interestingly, research on tax evasion has shown the experience of being audited and punished shapes people's decisions to cheat in the future. For example, 
Kastlunger and colleagues (2009)
 found compliance decreases after being audited. This behavior is referred to as the "bomb crater effect," according to which people tend to underestimate the likelihood of being punished shortly after enforcement 
(Mittone, 2006
; see also 
Plonsky et al., 2015)
. Furthermore, does the speed at which enforcement changes matter?
Comparing responses to abrupt versus gradual changes in detection probability could shed light on how sustained deterrence evolves. One promising avenue for addressing these questions is studies of decision-making in dynamic environments 
(Hochman & Erev, 2013;
Teodorescu & Erev, 2014;
Plonsky et al., 2015)
, which suggest people adjust their behavior more quickly to abrupt than to gradual changes. Examining how changes in the frequency of enforcement affect dishonesty could provide valuable insights for policymakers and deepen our understanding of the processes underlying dishonest behavior.
Finally, work on enforcement from a legal perspective highlights that in real life, and especially in the context of serious crimes, detection does not necessarily imply punishment (see 
Nagin 2013
, Van Rooij et al., 2025
. In laboratory settings, these factors may be more confounded than in the real world, where the detection of a lie and the determination and execution of punishment are not givens. In addition, criminological research has found a difference between objective and subjective deterrence, as people may over-or underestimate the likelihood of being caught and punished 
(Decker et al., 1993;
Van Rooij, 2016)
. Such research has examined how deviant decisions update information after word of punishment 
(Apel, 2013)
, something laboratory research on honesty and deterrence could also do.
In this context, we should remember deterrence is one of many potential effects of punishment on behavior 
(Van Rooij et al., 2025)
. We know punishment can have both positive and negative effects on illegal or deviant behavior. Positive effects that may reduce deviance include specific and general deterrence 
(Nagin, 2013;
Nagin et al., 2009)
, assurance of compliance 
(Thornton et al., 2005)
, and signaling what the norm is 
(Van Rooij et al., 2025)
.
Negative effects that increase deviance include crowding out prosocial norms 
(Gneezy & Rustichini, 2000)
, activating antisocial norms 
(Cialdini et al., 2006)
, stigmatization 
(Feingold, 2021)
, and undermining legitimacy in the case of procedurally unfair punishment 
(Tyler, 2006;
Walters & Bolger, 2019)
. Future research on honesty that seeks to understand the effects of enforcement should consider these broader effects.


From effective enforcement to compliance
An ongoing challenge for policymakers seeking to improve compliance through effective enforcement is the trade-off between the likelihood of detection and the severity of punishment. This dilemma arises from the potentially counterproductive nature of harsh penalties. If enforcers perceive harsh penalties as unjust, their willingness to consistently enforce rules may decrease 
(Feess et al., 2018)
, leading to reduced enforcement activity and, consequently, a lower probability of detection. In this case, several critical questions arise: Do enforcers actually reduce the frequency of enforcement when faced with severe penalties? If so, do potential violators take advantage of this leniency and increase their dishonest behavior when they expect severe punishment but perceive lax enforcement? The interaction between those who enforce the rules and those who are monitored remains an important area for further research.
Policymakers should prioritize building trust, enhancing legitimacy, and fostering a regulatory environment that encourages voluntary compliance while maintaining effective enforcement mechanisms for those less inclined to comply. Although honesty is often viewed as a personal virtue, compliance with laws and regulations is a social behavior influenced by individual characteristics, social factors, and institutional contexts 
(Feldman, 2018)
. By understanding the complex interplay between personal honesty, institutional trust, and compliance motivations, authorities can develop more effective strategies to promote lawabiding behavior among diverse populations 
(Frey & Jegen, 2001;
Becker, 1968)
.
Adopting a responsive regulatory approach, as suggested by 
Ayres and Braithwaite (1992)
, involves tailoring regulatory responses to the motivations and behaviors of different individuals or groups, which can increase the likelihood of policy success. For example, authorities might begin with a cooperative approach and escalate enforcement only when faced with noncompliance. This strategy helps build trust with those who are inclined to comply voluntarily, while maintaining a deterrent effect on potential violators. In summary, 
Figure 2
 illustrates the knowledge required to understand how enforcement shapes honest behavior. As the figure further illustrates, knowledge of honesty as a relational process is also required, which we discuss in the next section.


Honesty as a Relational Process
Honesty is inherently relational, involving a dynamic interaction between communicators-individuals, groups, or organizations-and the recipients of the communication 
(Cooper et al., 2023)
. What a communicator chooses to share and how they share it significantly influences whether their audience forms accurate beliefs. However, the responsibility for honesty does not rest solely with the communicator, as they are only one side of the interacting parties.
Effective honesty requires not only a communicator (the actor) but also a receiver whose psychology, behavior, knowledge, skills, and abilities also affect the likelihood that all parties involved in the communication will reach a mutual understanding of the truth 
(Ren et al., 2022)
.
In some cases, such as self-deception, the actor and the receiver may be the same person, and the audience of the communication is oneself 
(Schwardmann & Van der Weele, 2019;
Von Hippel & Trivers, 2011)
. This situation is unique. More commonly, the audience includes external parties such as friends, family members (DePaulo & 
Kashy, 1998;
Levine & Cohen, 2018;
Reinhardt & Reinhard, 2023)
, colleagues, clients, and others with whom one interacts professionally 
(Bourdage et al., 2019;
Keep, 2009;
Shell, 2021;
Yagil & Medler-Liraz, 2013
), shareholders (Yuthas et al., 2002
, social media followers, and the general public 
(Pennycook et al., 2020)
.
Viewing honesty as a relational process with its own relational motives and consequences underscores why the dominant methods of studying honesty over the past 15 years may be limited. Stylized experiments that focus on binary decisions, such as lying to an experimenter or telling the truth for a small monetary reward 
(Abeler et al., 2019;
Gerlach et al., 2019)
, fail to capture the complex social dynamics that influence honesty in real-world interactions. These controlled experiments, which emphasize simplicity and clean experimental designs, often ignore the intricate social and relational contexts in which honesty and dishonesty naturally occur. As a result, these studies treat honesty as an isolated individual choice, divorced from the potential for detection and devoid of relational or reputational consequences. This approach is useful for understanding how, when, and why people lie in solitude, but it conspicuously sidesteps large, important questions related to truth-seeking, authentic communication, and fostering mutual understanding of truth in others (as discussed by 
Cooper et al., 2023
; see 
Figure 3
).
The use of the sender-receiver game introduced by Gneezy (2005) may be a first step in this direction. In this stylized game, the sender is given private information and must report it to the receiver, either truthfully or not. The receiver then makes a decision. The payoffs for both the sender and the receiver depend on the receiver's decision. In a common variant, the receiver must guess the true state of the world, with payoffs for both players depending on whether this guess is correct 
(Erat & Gneezy, 2012)
. This game has proven useful for studying some aspects of relational lying, particularly sophisticated deception, in which the sender tells the truth with the intent to deceive and expects the receiver not to believe him 
(Sutter, 2009;
Blazquiz-Pulido et al., 2024)
. However, numerous other aspects of relational lying remain unexplored and could be further explored using this or other games as basic tools.
To advance our understanding of honesty in everyday life, adopting research methods that reflect the relational nature of honesty is critical. Adopting such methods means examining how people navigate truthfulness in ongoing relationships, considering factors such as relationship history, social norms, and the potential long-term consequences of honesty or dishonesty. It also means exploring how honesty is maintained or compromised in different relational contexts, such as families, workplaces, or broader social networks, and how different stakeholders interpret and respond to honest or dishonest communication.
By recognizing honesty as a dynamic, contextual process rather than a static, isolated act, we can gain deeper insights into the factors that promote or inhibit truthful communication. This broader perspective not only enriches our theoretical understanding, but also has practical implications for designing interventions and policies to promote honesty and trust in various social and organizational settings.
A systematic review of 169 empirical studies of honesty and dishonesty in management, organizational behavior, applied psychology, and business ethics conducted between 
(Cooper et al., 2023
 found nearly three-quarters of the articles focused primarily on "the accuracy of the content a communicator shares" (i.e., honest content). Far fewer studies examined other critical dimensions of honesty, such as "the amount and depth of disclosure by the communicator" (i.e., honest disclosure), "the manner in which the communicator shares or discloses information" (i.e., honest delivery), and "the process by which a communicator develops, validates, or updates his or her beliefs" (i.e., intellectual honesty). If a similar review were conducted in the field of social-personality psychology, we would likely observe the same trend: an overwhelming focus on the antecedents of accurate versus inaccurate reporting, with much less attention paid to the nuanced ways communicators can foster false beliefs in their relational partners through both verbal and nonverbal strategies beyond outright lying.
These overlooked strategies include dodging, deflecting, hedging, remaining silent, omitting key information, avoiding certain topics, and neglecting to fact-check. Such behaviors-strategic or inadvertent-can contribute significantly to misunderstandings and misinformation in interpersonal relationships 
(Bitterly & Schweitzer, 2020;
Levine et al., 2020)
.
The role of the recipient of communication is equally important: when recipients fail to ask questions, raise concerns, or exercise due diligence, they may inadvertently contribute to the perpetuation of false beliefs and unchallenged dishonesty.
A growing body of research suggests that whereas communicators often view these indirect tactics as less unethical than outright lying, recipients do not 
(Levine et al., 2018;
Park & Klein, 2024
; see also 
Levine & Duncan, 2020)
. Recipients tend to be highly sensitive to whether a communicator is intentionally fostering misunderstanding, regardless of the specific tactic employed. For example, communicators may view prosocial lies of omission-such as withholding inconvenient truths-as more acceptable than prosocial lies of commission, in which false information is actively provided. Recipients, however, often take the opposite view and perceive omissions as equally, if not more, deceptive.
Similarly, when communicators choose to "ghost" a relationship partner-effectively ending communication without explanation-they may believe they are sparing the other person's feelings by avoiding a difficult conversation. However, those who are ghosted typically do not interpret this action as kind or considerate, and instead may experience increased confusion and distress 
(Park & Klein, 2024)
. These dynamics underscore the important role that relationships and different perspectives play in shaping judgments of (dis)honesty.
In sum, the dominant focus of research on honest content overlooks the complex relational processes through which honesty and dishonesty manifest. By broadening the scope of inquiry to include the subtleties of honest disclosure, delivery, and intellectual honesty, scholars can gain a more comprehensive understanding of how (dis)honesty operates in interpersonal and organizational contexts. This holistic perspective is essential for developing effective interventions and policies to promote honest and transparent communication.


Motives for (dis)honesty
Understanding honesty as a relational process illuminates the diverse motives that drive both honest and dishonest behavior. In controlled laboratory settings, participants often lie for self-interested reasons or to achieve mutual benefits, as seen in corrupt collaboration paradigms 
(Weisel & Shalvi, 2015, pp. 20-22)
 and research on the interindividual-intergroup discontinuity effect 
(Cohen et al., 2009;
Wildschut & Insko, 2007)
. In particular, group decisions tend to be influenced by motivations such as greed and strategic considerations aimed at maximizing collective outcomes. In addition, fear and distrust often lead to deceptive behavior, driven by the belief that one will be met with similar distrust from others 
(Cohen et al., 2009;
Sutter, 2009;
Wildschut & Insko, 2007)
 or simply because cynical beliefs provide a particularly powerful justification 
(Hilbig et al., 2022)
.
Outside the laboratory, however, the motives for lying extend far beyond self-interest and fear. Individuals often engage in dishonest behavior due to complex relational factors, including spite 
(Erat & Gneezy, 2012)
, competitiveness , loyalty 
(Hildreth & Anderson, 2018;
Thielmann et al, 2021)
, prosocial intentions 
(Klein et al., 2017;
Levine & Schweitzer, 2014
Wiltermuth, 2011)
, harm avoidance 
(Levine, 2022)
, and paternalism 
(Lupoli et al., 2018)
. Although these motivations can be studied in experimental settings, fully understanding why people choose to act on them requires acknowledging the role of relationships in shaping communication.
Lies motivated by relational concerns often lead to more nuanced outcomes than those motivated by self-interest. For example, lies that are perceived as unambiguously prosocialhelping the recipient without causing harm-are generally considered ethical 
(Levine & Schweitzer, 2014)
 and may even increase trust 
(Levine & Schweitzer, 2015)
. By contrast, selfserving lies are typically viewed as unethical and detrimental to trust. However, not all prosocially motivated lies are perceived positively. When a communicator lies to benefit the recipient but lacks sufficient understanding of the recipient's preferences, the lie is perceived as paternalistic rather than prosocial. For example, offering false hope to a patient who values optimism over realism might be seen as a clearly prosocial act. However, offering the same false hope to a patient who has not expressed such a preference will be seen as paternalistic.
Paternalistic lies, like self-interested ones, are often judged to be unethical and are more likely to be punished 
(Lupoli et al., 2018)
. This distinction highlights the importance of a communicator's awareness of the recipient's preferences in shaping judgments of dishonesty.
Context also plays a critical role in judgments of dishonesty. Recent research suggests people are more likely to condone dishonesty that prevents unnecessary harm 
(Levine, 2022)
.
Judgments of unnecessary harm are based on two key inferences: the extent to which honesty would cause immediate emotional harm to the recipient, and whether it holds potential for longterm learning and growth (i.e., its instrumental value). When honesty is perceived as causing emotional harm without offering instrumental value, people are more likely to view it as causing unnecessary harm, thereby justifying dishonesty as the more ethical choice. Certain contexts are particularly conducive to judgments of unnecessary harm; for example, when the recipient is unable to understand, respond to, or learn from painful information, providing it may be seen as unnecessarily harmful. Consider the scenario of giving someone negative performance feedback: although critical feedback can cause emotional distress, it also provides an opportunity for the recipient to learn and improve in the future 
(Abi-Esber et al., 2022;
Fulham et al., 2022)
. In cases where the recipient cannot benefit from such feedback, false praise may be seen as the more ethical response 
(Levine, 2022)
.


The case of corrupt collaboration
An important example of the relational aspect of dishonesty is corrupt collaboration, or the use of joint acts of dishonesty to achieve personal gain 
(Weisel & Shalvi, 2015;
Leib et al., 2021)
. The main finding in this line of work is that collaborative settings, in which agents work together toward common goals and can benefit from each other's dishonest behavior, are particularly prone to excessive levels of dishonesty. For many people, collaboration as a noble cause seems to offset the moral costs of dishonest behavior, allowing them to be dishonest while maintaining a reasonable moral self-image. In this view, cooperation and honesty can be seen as moral currencies that can be traded for one another 
(Weisel & Shalvi, 2022)
.
The notion of tradable moral currencies fits well with the relational aspects of (dis)honesty highlighted in the current framework. When multiple people are involved in acts of (dis)honesty, determining the total moral "value" of such acts naturally requires taking into account the welfare of all the agents involved. In this view, even acts that are at face value immoral, for example, because they involve lying or harming others, may ultimately be judged as morally acceptable by both decision-makers and others, depending on the outcome of the moral calculus.
An understudied, relational aspect of corrupt cooperation is the role of collaborators' group affiliations. What are the dynamics of corrupt cooperation among members of different groups? How do group characteristics, whether actual or perceived, and intergroup relations affect corrupt cooperation? 
Dorrough et al. (2023)
 examined similar questions in the context of bribery, and seeing what patterns emerge with respect to corrupt collaboration would be interesting.
One of 
Weisel and Shalvi's (2015)
 findings is that dishonesty is most prevalent when both members of a dyad stand to gain equally from collaborative dishonesty, as opposed to situations where dishonesty is more profitable for one dyad member than the other. The notion that symmetry between dyad (or group) members can encourage dishonesty can be applied and tested in the current framework by examining symmetric and asymmetric enforcement mechanisms (see 
Figure 2
) in the context of corrupt collaboration. Both the probability of detection and the severity of punishment can vary across potential collaborators. A simple hypothesis, based on the results of 
Weisel and Shalvi (2015)
, is that dishonesty will be more prevalent when enforcement is identical for all participating agents than when it is asymmetric. A related avenue for future work can build on the work of 
Teodorescu et al. (2021)
, who systematically varied the frequency and severity of punishments in an individual (dis)honesty task and found frequent small fines were a stronger deterrent to dishonesty than infrequent large fines. Testing whether the corrupting power of cooperation renders such differences between enforcement mechanisms would be interesting.


Using Our Novel Framework to Reevaluate the Science of Honesty
As highlighted in 
Figure 2
, the current review focuses on the interplay between enforcement and the relational aspects of (dis)honesty. In Section 3, we assess how such a perspective can shed new light on five major strands of knowledge in the study of (dis)honesty.
We do so by reviewing the state of the art in each of these strands of knowledge and highlighting open questions. These strands of knowledge about (dis)honesty include relationship structure, relationships features, individual differences, cognitive processes, and situational factors.


Relationship Structure


Interdependence
Interpersonal interdependence refers to "the ways in which two individuals influence each other's outcomes in the course of their interaction" 
(Kelley, 2003, p.
 3). In the context of (dis)honesty, interdependence occurs when two individuals influence each other's decisions and outcomes in situations involving potential deception or truthfulness. According to interdependence theory 
(Kelley et al., 2003;
Van Lange & Balliet, 2015)
, interdependent situations are characterized by six key characteristics: (a) dependence: the extent to which one's outcomes are dependent on the actions of one's interaction partner; (b) power: the extent to which one has control over one's own and one's partner's outcomes; (c) conflict: The extent to which the best outcome for an individual results in a worse outcome for their partner; (d) coordination: how one individual's behavior affects the partner's actions and, consequently, one's own outcomes; (e) future interdependence: the impact of current behaviors on both partners' future interactions and outcomes; and (f) information certainty: the degree to which both partners are aware of each other's preferences and the consequences of their actions.
The two types of dishonesty we consider-individual and relational-differ significantly along these dimensions. Individual dishonesty occurs when a person lies to benefit themself at the expense of others, as in the classic die-rolling task, where participants may misreport the outcome of a private die roll for personal gain 
Gerlach et al., 2019)
. This form of dishonesty is characterized by low interdependence and coordination, because the actions of others do not affect one's own outcomes. It also exhibits high levels of power and conflict, because the individual's financial gain is solely under their control and comes at the expense of others.
By contrast, relational dishonesty, such as collaborative dishonesty, involves two or more individuals jointly engaging in dishonest acts, such as in the dyadic die-rolling paradigm 
(Weisel & Shalvi, 2015;
Leib et al., 2021)
 or bribery behavior (e.g., 
Dorrough et al., 2023)
. This form of dishonesty demonstrates a high degree of interdependence, where the actions of the other influence the outcomes of each participant. It is characterized by high interdependence (the monetary reward in collaborative tasks depends on both participants' reported outcomes, making each person's payoff dependent on the other's behavior), high coordination (success requires the coordination of dishonest actions; see 
Dorrough et al., 2023;
Ścigała et al, 2019;
Zickfeld et al., 2023)
, low conflict (the best outcome for an individual is directly linked to the best outcome for his or her partner, resulting in shared interests and reduced conflict), and shared power (control over the final outcome is shared between both partners, as each influences the other's potential rewards).
Although evidence shows interdependence influences dishonest behavior (e.g., 
Weisel & Shalvi, 2015
; but see 
Zickfeld et al., 2023)
, research is still in its early stages, particularly regarding the role of future interdependence and information certainty. For example, how collaborative dishonesty evolves over longer periods of time, such as months or years, remains an open question. Although experimental settings can simulate collaborative dishonesty, such behavior in real-world contexts may be driven by factors such as mutual trust, partner vulnerability, or the threat of retaliation-elements that typically develop over time.
In addition, how physical proximity versus online interaction affects the propensity to engage in collaborative dishonesty is unclear. Greater trust and rapport may develop in face-toface interactions, potentially increasing the likelihood of dishonest collaboration. Conversely, face-to-face interactions may also increase shame and reputation concerns, limiting the extent of dishonest collaboration. Future research should also examine how individuals' preferences, along with those of their interaction partner, shape the development of collaborative dishonesty over time, and whether these preferences change based on prior experiences with dishonesty.
Finally, examining how factors such as group membership or stereotyping affect individuals' levels of honesty over time would be valuable. Understanding the dynamics of collaborative dishonesty in long-term interactions could inform the development of interventions aimed at reducing dishonest behavior in such contexts. By exploring these dimensions of interdependence, future studies can provide a more nuanced understanding of how and why dishonest behavior emerges and persists in both individual and collaborative settings.


Intragroup processes in dishonesty: From cooperation to culture
Focusing on the intra-group processes that shape collaborative dishonesty, Zanetti and Butera (2023) conducted a series of interviews that revealed individuals often recall acts of collective cheating as complex, highly organized group activities. Such cheating behaviors are often premeditated, with accomplices selected from close associates, making cheating, in some cases, a routine practice. Group members carefully assess potential risks and establish concealment strategies, including codes of silence, to protect their misdeeds. Notably, these cheating events, particularly in academic settings, are remembered not only as acts of dishonesty but also as opportunities for cooperation, loyalty, and group cohesion. This shared experience contributes to the formation of a collective identity, providing a psychological basis for justifying their dishonest actions as part of group solidarity.
The role of shared identity in facilitating dishonesty is further supported by experimental research by 
Rullo et al. (2024)
. Their studies showed participants were more likely to cheat when their group members shared a common identity, such as a national identity, than when they did not. In addition, participants were more likely to cover up the dishonest actions of a group member who shared their identity than those who did not. This finding suggests working with individuals who are perceived as similar encourages not only the act of cheating, but also the enforcement of a collective code of silence to protect fellow cheaters.
The tendency to prioritize and protect similar others in dishonest contexts parallels a broader adherence to certain value systems, as evidenced by 
Pulfrey et al.'s (2018)
  Distinguishing organizational characteristics or practices from organizational culture is important because culture refers to something that is deeply rooted, not easily visible or known, even by organizational members, and very difficult to change (see 
Tobsch et al., 2024)
.
The body of research on intragroup processes underscores the importance of considering not only individual motives, but also the broader social and cultural contexts that facilitate and perpetuate dishonest behavior. Understanding the intricate web of group dynamics, shared identities, and cultural values that underlie collective cheating is critical to developing effective strategies for addressing dishonesty in various institutional settings. This research also draws attention to the understudied area of collective processes in cheating (see 
Figure 2
). In terms of enforcement, the available evidence points to two possible outcomes. On the one hand, cooperation in cheating may reduce enforcement by reducing the likelihood of whistleblowing 
(Rullo et al., 2024)
. On the other hand, in the case of enforcement, the consequences may appear less severe, due to the diffusion of responsibility within the group 
(Zanetti & Butera, 2023)
.


Intergroup processes in dishonesty
Dishonesty occurs not only between different individuals in a group, but also across group boundaries. Although in real life, dishonest acts often involve multiple groups (e.g., bribery across national borders), research on this topic is still scarce. In fact, although an emerging stream of research on collective dishonesty exists (for a meta-analysis, see 
Leib et al., 2021)
, intergroup dishonesty has only recently been studied. In a large-scale multinational study by 
Dorrough et al. (2023)
, participants from different countries interacted in a bribery game in mixed-national pairs. Results show people offer disproportionately more bribes to interaction partners from countries with a high (vs. low) reputation for foreign bribery, as measured by macro-level indicators of corruption perceptions (e.g., the BPI). People largely share countryspecific expectations about the level of acceptance of bribery in other countries. However, these expectations are negatively correlated with actual levels of bribe-taking, suggesting shared but inaccurate stereotypes about bribery propensities. Findings also suggest more than one's own national background and the national background of one's interaction partner shape people's willingness to engage in bribery, highlighting the importance of expectations and stereotypes for dishonest behavior. In this research, the decision context was highly relational with low enforcement (see 
Figure 2
). Future research can implement different punishment institutions (i.e., high enforcement) and test their effectiveness in combating dishonest behavior in the form of bribery and beyond.
In addition, the role of gender in corruption research has received considerable attention (Kubbe & Merkle, 2022). However, similar to the national group membership of the opposing party, the gender composition of the group with which one interacts has not yet been examined.
This task is left for future research. The same is true for other instances of dishonest behavior (e.g., lying, cheating) and other category memberships (e.g., high-status vs. low-status groups).
Psychological science tends to test the generality of phenomena, but showing how phenomena vary as a function of context is equally important. An open research question in (dis)honesty is the role of context and how a culture of cheating manifests itself in different ways at work, in sports, at school, in the arts, and so on. Indeed, research has shown a culture of cheating (vs. not cheating) affects moral disengagement 
(Zanetti & Butera, 2024)
. Showing how routinized cheating leads to reduced feelings of guilt, and how highly competitive environments-for example, in industry or science-can lead to collective moral disengagement 
(Bandura, 2002)
-for example, in the form of beliefs that "the end justifies the means"-would be interesting.


Relationship Features


Social norms
Social norms form the backbone of our societies, guiding both selfish and cooperative behavior and enabling the formation of social bonds. These norms are often maintained through credible enforcement mechanisms and are crucial in influencing individual decisions, particularly in the context of deviant behaviors such as lying and dishonesty. How do social norms affect and are affected by lying?
Norms are properties of an individual's relevant reference group, which are people who are important to the decision-maker in a particular situation 
(Bicchieri, 2006;
Krupka & Weber, 2013;
Fehr & Schurtenberger, 2018;
Bicchieri & Dimant, 2019;
Cohn et al., 2019
Cohn et al., , 2022
. People often observe and imitate these reference groups, interpreting their intentions and sometimes justifying their own actions and the underlying social norms in self-serving ways 
(Bicchieri et al., 2023)
. This process highlights the importance of understanding how norms and compliance evolve, particularly in settings where individuals can choose which norms to learn and follow 
(Dimant et al., 2024a)
.
Peer behavior influences one's choices through at least one of two channels that follow from 
Bicchieri (2006)
: empirical expectations (what others do) and normative expectations (what others approve of). For example, when people know their peers also lie or engage in deviant behavior, they are more likely to behave in a similar way, expressing a form of conditionality 
(Bicchieri et al., 2022)
. Sometimes, however, people are unable to fully observe their peers or are not observed by their peers. Such environments are characterized by the anonymity of actions, which hinders the enforcement of social norms and thus facilitates the spread of dishonesty 
(Kajackaite & Gneezy, 2017;
Dimant et al., 2020)
.
Importantly, the lack of observability and norm enforcement creates moral space through which deviance can spread 
(Bolton et al., 2021)
. As noted above, people use self-interested justifications to determine how much to lie and strive to maintain a positive self-concept even when engaging in dishonest behavior 
(Bicchieri et al., 2023)
. When individuals had multiple opportunities to roll a die and report the outcome, they were more likely to lie by reporting a favorable but unobserved outcome. These findings suggest lying feels more justified when individuals can convince themselves they are reporting a possible outcome rather than fabricating it entirely . At the same time, some wiggle room remains in terms of what social information one is exposed to. In experiments in which participants were allowed to choose their sources of information about prevailing social norms, they showed a bias toward more lenient information 
(Dimant et al., 2024a)
. This bias was stronger among those who had already lied, suggesting individuals actively seek out information that justifies their dishonest behavior. This selection bias in information acquisition can have significant implications for the formation and influence of social norms 
(Dimant et al., 2024a;
see also Maggian & Villeval, 2016
).
Finally, social norms can be a fragile construct. Social-norm enforcement is essential for maintaining social order and reducing the occurrence of dishonesty 
(Buckenmaier et al., 2021)
.
Norm-enforcement behavior can be influenced by norm nudges, which can change individuals' perceptions of what behaviors are acceptable. For example, providing normative information about the inappropriateness of lying can increase the perceived inappropriateness of the behavior, thereby increasing the likelihood of norm enforcement through punishment 
(Bicchieri et al., 2021;
Dimant & Gesche, 2023)
. Similarly, 
Capraro and Vanzo (2019)
 demonstrated the use of morally loaded language can influence people's honesty in decision-making contexts, highlighting the power of framing in ethical behavior.


Reducing dishonesty via norm nudging
Norm nudging leverages social expectations to change behavior by influencing what people believe others will do or approve of 
(Bicchieri & Dimant, 2019)
. This method has been shown to be effective in a variety of related contexts, such as tax compliance and the enforcement of honesty norms (Hallswort et al., 2017). For example, informing taxpayers about the high compliance rates of their peers increased tax compliance rates. These examples illustrate norm nudging can be a powerful tool for promoting honesty.
An important aspect that determines the effectiveness of norm nudging is the relevance of the peer group and the context in which the behavior occurs. For example, one study found hotel guests were more likely to reuse towels when they were informed that previous guests in their room had done so. This finding suggests that the closer the reference group is to the individual's context, the more effective the norm nudge will be 
(Goldstein et al., 2008)
. However, norm nudges do not always produce positive behavior change, and the effectiveness of such interventions can vary significantly depending on how the norm information is framed 
(Bicchieri & Dimant, 2023)
. In a study examining the effects of norm nudges on lying behavior, 
Dimant et al. (2020)
 found framing the norm message positively (e.g., "the majority did not cheat") or negatively (e.g., "a minority did cheat") had no significant effect on participants' dishonesty and was indistinguishable from a no-information control condition. This finding suggests norm nudges may not always be sufficient to change existing norms, especially when those norms are already well established 
(Gelfand et al., 2022;
Dimant, 2024)
.
In sum, social norms play a critical role in guiding behavior and maintaining social order.
Understanding how norms influence deviant behavior and developing effective interventions can foster a more honest and cooperative society. Although existing research has provided valuable insights into the role of social norms in deviant behavior, several gaps remain. For one, future research should examine how norms form endogenously through the selection of information in social contexts and how this process influences behavior. In addition, more research is needed to examine the long-term effects of norm nudges and the conditions under which they are most effective. This examining includes how different framing strategies and the presence of group identity influence the effectiveness of these interventions.
Finally, in addition to direct effects on social norms and behavior, the relationship between deviance, lying, and political polarization is of growing importance in today's political climate. Research suggests political polarization may exacerbate the spread of deviant behaviors such as lying, because individuals may selectively seek out and trust information that is consistent with their political beliefs, thereby reinforcing dishonest behaviors within their ingroups 
(Dimant, 2024;
Dimant et al., 2024b)
. This phenomenon is exacerbated by the existence of echo chambers and filter bubbles, where exposure to homogeneous sources of information can reinforce partisan views and justify dishonest actions toward out-groups 
(Bursztyn et al., 2020;
Levi, 2021)
. Understanding these dynamics is critical to designing interventions that promote honesty and cooperation across political divides. Importantly, to successfully harness the power of social norms and norm nudges to curb dishonesty, we need to develop new measurement approaches that capture the nuances of social norms 
(Charness et al., 2024)
. Existing measurement approaches (e.g., 
Bicchieri & Chavez, 2010;
Krupka & Weber, 2013)
 are useful in capturing the average or most common behavior/opinion in a given context. However, the new generation of social-norm-elicitation techniques (e.g., 
Dimant, 2023;
Panizza et al., 2024
) also capture more nuanced aspects of social norms, such as tightness, looseness, and polarization.
Taken together, understanding the shape of social norms, the mechanisms underlying norm adherence, and their enforcement in different contexts can help design more effective interventions to promote sustained honesty 
(Dimant & Shalvi, 2022)
.


Trust
The previous sections show that although humans often portray themselves as moral beings, their actions often contradict this self-image. As we have seen, people engage in dishonesty, form fraudulent alliances, and disregard social norms. However, the extent of such behavior is influenced by numerous factors, including individual differences, the likelihood of being caught, and cognitive factors such as mental capacity and the (lack of) energy required to deceive. Given the prevalence of dishonest behavior, how can people still trust each other?
Trust is defined in terms of capabilities (the belief that the trusted individual is competent to perform a task) and encapsulated interests (the expectation that the trusted individual will consider the interests of others). Seligman (2000) described trust as "a kind of belief in the other's benevolence, given the opacity of the other's intentions and calculations." Similarly, Russell 
Hardin (2002)
 defined it as the expectation of "encapsulated interest," that is, the understanding that acting on one's behalf is in the trusted person's interest. This logic sounds rather "sophisticated. In fact, some trust decisions that people have to make in their lives are made after extensive thought and careful consideration. Most of the time, however, we make trust decisions in a split second, based on gut feelings rather than rationality 
(Li et al., 2022;
Snowden et al., 2022)
.
Using a trust game, 
Engelmann and Herrmann (2016)
 showed chimpanzees place more trust in their partners when interacting with friends than with non-friends. Based on this and a handful of other studies, we can conclude that trust within close relationships is not a uniquely human trait. But what about trust in complete strangers? Observational studies in the wild show chimpanzees and our other close relatives, namely, bonobos, behave quite differently in encounters with unfamiliar conspecifics, with the former generally being more aggressive than the latter 
(Hare et al., 2012)
, although other socio-ecological factors (e.g., gender, ingroupoutgroup, available resources) somewhat blur this species difference 
(Mouginot et al., 2024)
.
Studies of human hunter-gatherers show that, compared with chimpanzees living in the same Congolese rainforest, humans travel greater distances and trade with other groups 
(Jang et al., 2019
). However, their interactions with strangers are minimal compared with people in WEIRD countries 
(Westernized, Educated, Industrialized, Rich, and Democratic;
Henrigh et al., 2010)
.
Humans naturally organize themselves into groups and subgroups, but our WEIRD society has become extremely layered due to globalization and digitization. Permissions, registrations, benefits, rules, acts, laws, inspections, and repercussions, but also the sharing of experiences via social media, ensure people and institutions strive to maintain their reputations.
In the Netherlands, for example, a daycare provider must have completed training as a childcare worker and clear a criminal record check before being allowed to care for other people's children. The complexity of our society is strange, but on an evolutionary timescale, it has helped reduce aggression and foster trust and large-scale cooperation, even with strangers 
(Pinker, 2012)
. Research on non-human primates and non-WEIRD societies is extremely scarce. But if we are to better understand the precise societal, cultural, ecological, and evolutionary drivers of trust, comparative research is essential. Such a line of research would open up opportunities for cross-disciplinary collaborations between psychologists, anthropologists, ecologists, primatologists, and behavioral economists.
The above has provided some explanations for why humans, although we often cheat each other, still trust on a large scale. The question of why we feel more comfortable sitting next to person A on the train than person B remains open. A crucial factor here is what we see in their faces. To explore which facial features inspire trust, the researchers used a technique called "reverse correlation" 
(Dotsch & Todorov, 2012)
. Participants were shown the same grayscale face on a computer 1,000 times, each time with a different noise filter, namely, a random pattern of pixels. They saw two faces at the same time and had to decide which looked more trustworthy, the left or the right. The pixel patterns changed the faces slightly each time.
Overlaying all the pixel patterns deemed trustworthy created a baby face with a round jawline, large pupils under light eyebrows, a high forehead, and slightly upturned corners of the mouth 
(Dotsch & Todorov, 2012;
Frisanco et al., 2023)
. Conversely, superimposing the untrustworthy pixel patterns resulted in a grumpy blockhead, the stereotypical "untrustworthy person.
Follow-up research showed people were more likely to trust people with a baby face. In fact, people need only 33 milliseconds to categorize a face as "trustworthy" or "untrustworthy" 
(Todorov et al., 2009)
. But the story does not end there. Although we might be more inclined to entrust our child to a person with a childlike face and friendly expression, we tend to trust politicians with features that convey dominance and competence: a square jaw, eyes that are not too wide, high cheekbones, and preferably some attractiveness 
(Laustsen et al., 2018)
. Research shows these facial features significantly influence election outcomes. Our primate brains often struggle in our modern society. The associations, biases, and stereotypes we consciously or unconsciously hold make us less likely to trust a female politician or a male daycare worker. These undesirable evolutionary "leftovers" are reinforced by what we see and become accustomed to on a daily basis. Importantly, we need to remember that although the way humans live today is unique, our genome evolved in small, close-knit groups where encounters with strangers were rare.
In addition to static facial features, emotional expressions are also critical in assessing trust. Many studies have shown people with a smile are more trusted than those with a frown (e.g., 
Scharleman et al., 2001)
, and that even subtle cues such as blushing (Dijk et al., 2011), pupil dilation 
(Kret & de Dreu, 2019)
, or direct gaze 
(Kreysa et al., 2016)
 can make one appear more trustworthy. Emotional expressions are often imitated by others, and people even synchronize their physiological states. Synchronization helps predict each other's behavior, which is why various studies have shown mimicry and synchrony promote trust 
(Kret & Akyüz, 2022)
. After decades of silence following 
Levenson and Gottman's (1983)
 seminal paper on physiological coupling in couples, new statistical methods and affordable equipment have revitalized the field of social neuroscience. Research has shown people mimic pupil size and that mimicking pupil dilation has prosocial consequences, including an increase in trust 
(Kret et al., 2015)
. By combining eye tracking, psychophysiological measures, and fMRI, researchers revealed the importance of the theory of mind network for this link between pupil mimicry and trust 
(Prochazkova et al., 2018)
. Every year, new products, such as wearables that measure physiological responses or brain activity, enter the market. These innovations allow researchers to study the precursors of trust decisions during real-life interactions, with a focus on physiological synchrony and hyper-scanning. One of the most challenging and exciting research questions we face is understanding how the intricate choreography of emotional expressions between interaction partners influences mutual trust.


Gossip
Our framework ( 
Figure 2
) highlights the need to explore understudied areas related to enforcement and the need to examine decision contexts that are relational rather than solitary.
Research on gossip relates to both of these recommendations. Gossip is defined as communication between a sender and receiver about an absent person (i.e., the gossip target;
Dores . When the opportunity to engage in dishonest behavior arises, the possibility that others will gossip about such behavior implies, in the terminology of our framework, that the behavior can be detected and punished by others (i.e., gossip can be viewed as a form of coercion) and that the decision context involves others (i.e., the presence of gossip makes the context relational). Taken together, reviewing research on the influence of gossip on (dis)honesty is valuable here.
Gossip allows individuals to learn about the behavior of others without directly observing them, and as such, researchers have argued it is an essential mechanism explaining human cooperation 
(Nieper et al., 2022;
Wu et al., 2016)
. Through gossip, receivers can learn whom to trust and cooperate with and whom to avoid or punish 
Sommerfeld et al., 2007)
, thus allowing for indirect reciprocity (cooperating with those who cooperated with others; 
Nowak, 2006)
 and partner selection (choosing group members who are known to be reliable partners, 
Capraro et al., 2016)
. Therefore, for potential gossip targets, the possibility of being gossiped about should lead to reputational concerns (i.e., concerns about others' evaluations of one's qualities or characteristics; 
Caldwell, 1986)
, which in turn should make them more likely to behave in ways that lead to a positive reputation (e.g., 
Beersma & Van Kleef, 2011;
Wu et al., 2016)
.
Linking gossip to honesty, the possibility of being the target of gossip should, in principle, lead people to behave more honestly due to reputational concerns. To examine the functioning of gossip as an enforcement mechanism, Nieper et al. (in press) used the aforementioned die-rolling task. Participants who received reports about the behavior of dice rollers and were able to gossip about them to other participants largely portrayed gossip targets as honest or dishonest, consistent with the dice roll reports they received: die roll averages were strongly correlated with gossip describing the gossip target as dishonest. Moreover, gossip recipients used gossip as a valid source of information: those portrayed as dishonest were trusted less than those portrayed as honest. In addition, and central to the framework presented here, dice rollers' honesty increased when they knew the participant observing their dice rolls could gossip about them to another participant with whom they would interact later in the experiment, whereas mere observation without the possibility of gossiping did not increase honesty. The effect of (potential) gossip on honesty was partially mediated by reputational concern.
Although these findings suggest the (possibility of being the target of) gossip can have a positive effect on honesty, and thus on the success of gossip as an enforcement mechanism, results from the second study by 
Nieper et al. (in press)
 show this effect is limited to situations in which gossip receivers can influence the outcomes of gossip targets. Specifically, the ability of observers of dice rolls to gossip to someone who could not influence the participants' monetary outcomes in a follow-up task did not increase the honesty of the dice rollers, whereas it did when observers were able to gossip to someone who could directly influence the dice rollers' monetary outcomes in that task. Similarly, 
Huber et al. (2023)
 found no effect of a manipulation that combined observability and non-consequential gossip on honesty in the dice-rolling task. Thus, the possibility of being the target of gossip appears to affect honesty only when gossip is directed to receivers who can impose consequences on potential gossip targets, suggesting gossip is particularly effective as an enforcement mechanism when it can lead to actual punishment of the gossip target by gossip receivers.
Interestingly, in addition to acting as an enforcement mechanism against dishonesty, gossip itself can reflect dishonesty. People can lie about gossip and is more likely to happen when it is personally beneficial (Dores 
Cruz et al., 2024;
Peters & Fonseca, 2020)
. Therefore, an important question for future research is to what extent gossip needs to be accurate in order to stimulate honest decision-making. If decision-makers know gossip about them may be inaccurate, and if they know gossip recipients know this, will they still behave more honestly?
An agent-based simulation study by 
Testori et al. (2022)
 suggests such inaccuracy can counteract the positive effects of gossip on group cooperation via indirect reciprocity and/or partner selection. Although research has shown that even when receivers knew gossip statements may contain inaccurate information, the gossip still affected their evaluations of targets 
(Baum et al., 2020)
, whether and how (potentially) inaccurate gossip affects the behavior of targets is unclear.
Related to the potential effects of gossip inaccuracy, the influence of the interdependence structure between the three members of the "gossip triad" (the sender, the target, and the receiver) represents another interesting avenue of research. Although the results discussed above show the possibility of being the target of gossip increases honesty, we do not currently know whether it does so under different interdependence structures. Results from the second study by 
Nieper et al. (in press)
 show gossip increases honesty only when the outcomes of gossip targets depend on the decisions of gossip receivers, but the effects of other patterns of interdependence in the gossip triad, such as when an observer who can gossip about one's behavior would benefit from lying, or when a gossiping observer is an ally or an enemy of the gossip receiver, are still unclear.
In a situation where a gossiper could benefit from a target's dishonesty (see 
Weisel & Shalvi, 2015)
, negative gossip might portray the target as dishonest when they lie, but as uncooperative when they behave honestly. Thus, if targets are primarily concerned about their reputation as honest people, gossip in this situation should lead to more honest decisions, but if targets are primarily concerned about their reputation as good cooperators, gossip should lead to more lies. Nieper et al. (unpublished manuscript) investigated how gossip affects the behavior of potential targets in this situation. Their preliminary results, based on Bayesian statistics, showed a null effect of gossip. However, they found the effect of gossip was moderated by the target's personality, such that gossip led to more honest reports for targets who scored low on honesty-
humility. An interesting possibility that future studies could explore is that individuals with nefarious goals may use gossip about others' honesty versus dishonesty as a way to identify potential partners for collaborative cheating. Thus, although gossip generally appears to promote honesty, it may ironically enable dishonesty to the extent that it allows individuals to identify future partners in crime.
Finally, many studies examining the effects of gossip on target behavior have examined the possibility or threat of gossip rather than becoming the victim of gossip (see, e.g., 
Beersma & Van Kleef, 2011;
Fehr & Sutter, 2019)
. Research has shown the experience of being the target of gossip leads to reduced feelings of social inclusion and withdrawal 
(Martinescu et al., 2021;
Xing et al., 2021)
. Testing the effects of actual, rather than anticipated, gossip on honesty would therefore be interesting.


Individual Differences


Personality, beliefs, and justifications
Research has firmly established notable interindividual differences in whether and to what extent people engage in unethical behaviors such as dishonesty, and that this variation can be explained by personality traits (for a review, see 
Hilbig, 2022)
. In particular, the honestyhumility dimension from the HEXACO model of personality structure 
(Heck et al., 2018;
Hilbig & Zettler, 2015;
Zettler et al., 2020)
 and the common core of so-called "dark" traits (e.g., narcissism or psychopathy), the "dark factor of personality" (hereafter "D"; 
Moshagen et al., 2018;
Moshagen et al., 2020)
. These two traits not only predict lying in anonymous, individual settings, but have also been shown to predict how consistently individuals tell the truth or lie in such contexts over time 
(Thielmann et al., in press)
. Similarly, as noted in the introduction, research has well established that people who lie rely on self-preserving justifications to avoid threats to their moral self-image.
Until recently, however, these two approaches to understanding dishonesty-personality traits and justifications-have typically been studied in isolation. With the goal of integrating these lines of research, 
Hilbig et al. (2022)
 proposed that certain beliefs associated with the D factor of personality (explicitly defined as "the general tendency to maximize one's individual utility-disregarding, accepting, or maliciously provoking disutility for others-are accompanied by beliefs that serve as justifications" 
(Moshagen et al., 2018, p. 657
, emphasis added) promote the subjective justification of disutility, essentially acting as an accelerant that operates on the underlying motivation, namely, the trait. In other words, individuals who are more motivated and more likely to lie (those high in D) are expected to hold beliefs that allow them to construct justifications for dishonesty and thereby mitigate threats to their moral self-image.
Prominent among such beliefs inherent in D are those related to superiority, special status, and outstanding deservingness, which is relatively unsurprising given that traits such as grandiosity, narcissism, psychological entitlement, or self-centeredness can all be considered mere manifestations of D 
(Hilbig et al., 2023;
Scholz et al., in press
). However, D is also associated with a much broader set of beliefs that may justify dishonesty. For example, those high in D hold hierarchy-related beliefs (especially social-dominance orientation and/or authoritarianism; 
Moshagen et al., in press)
 and relativism-related beliefs (such as normlessness or negative reciprocity norms), both of which operate on injunctive social norms (called normative expectations in Section 3.2.1), the former justifying dishonesty in terms of its necessity (for the in-group or as demanded by some authority) and the latter in terms of ambiguity and conditionality, and thus the absence of a norm. Most importantly, however, D is strongly associated with various distrust-related beliefs (such as a competitive worldview or cynicism, i.e., "everyone does it") that invoke a descriptive social norm (called empirical expectations in Section 3.2.1) and thus justify dishonesty by its prevalence (e.g., 
Gächter & Schulz, 2016;
López-Pérez, 2012;
Schultz et al., 2007
; see Section 3.2.1). Experiments 
(Hilbig et al., 2022)
 have shown individuals high in D are more likely to predict others will be dishonest (i.e., cynicism, measured in an incentive-compatible manner; i.e., participants received bonus payoffs for correct predictions about others' dishonesty). In turn, these individuals are more likely to judge dishonesty as justifiable, and ultimately more likely to engage in dishonesty themselves, supporting the notion that the (cognitive) mechanism by which personality is expressed (see Section 3.3.2) involves the construction of justifications from beliefs.
In related work using a complex moral-dilemma paradigm, 
Ng et al. (2022)
 found individual differences in deontological norm adherence negatively predicted cheating in a cointoss paradigm, regardless of whether cheating was for personal gain or the greater good. By contrast, individual differences in utilitarian welfare maximization did not predict cheating regardless of whether it served personal gain or the greater good. Although not of primary interest, the two studies also included the honesty-humility subscale of the HEXACO inventory, which negatively predicted cheating for personal gain but not cheating for the greater good.
Perhaps one of the most fundamental open questions in this regard is essentially ontological: do individuals who are initially inclined toward dishonesty successively adopt increasingly strong and varied specific beliefs (e.g., cynicism or social-dominance orientation) to protect their moral self-image, or do individuals initially hold or acquire (e.g., through experience and exposure) certain beliefs that then gradually incline them toward unethical behavior? Whether and which social contexts and experiences shape these beliefs, especially those related to the honesty of others and the likelihood and effectiveness of sanctions, remains unknown (see 
Figure 2
). For example, cynical beliefs include the agreement that "most people aren't really honest for a desirable reason; they're afraid of getting caught" 
(Chowdhury & Fernando, 2013)
, and relativistic beliefs assert that "if something works, it doesn't matter whether it's right or wrong" 
(Kohn & Schooler, 1983)
. Thus, the very beliefs that link personality traits to dishonest behavior appear to be directly related to the two major dimensions identified in the framework above. Thus, an important task for future research is to determine not only how the likelihood of sanctions and the social context shape dishonest behavior (as in 
Figure 2
), but also how related experiences play into the very beliefs that link personality traits to dishonest behavior.


Honesty and personality dynamics
In personality science, an exciting recent line of research on honesty has emphasized personality dynamics 
(Furr & Funder, 2021;
Rauthmann, 2021)
. Whole Trait Theory, for example, posits that traits include both consistent patterns of behavior (a descriptive aspect) and the underlying psychological mechanisms (an explanatory aspect) that cause these patterns 
(Fleeson & Jayawickreme, 2021)
. A dynamic perspective recognizes people regulate honesty based on situational contexts and individual goals, emotions, and cognitive processes.
By considering honesty from this perspective, dynamic personality accounts acknowledge that although some individuals may be inclined to be honest, due to stable personality traits, their honesty may vary meaningfully depending on specific contexts. For example, a person may be generally honest but lie when he or she perceives a threat to his or her self-image or relationships. Such variability arises from psychological processes such as motivations, values, and beliefs that are activated in different contexts. Thus, dynamic accounts of personality provide a perspective for understanding how consistent patterns of honesty emerge over time while also accounting for flexibility in honesty across contexts in daily life.
The framework outlined in 
Figure 2
 can be viewed through the lens of personality dynamics. First, enforcement can be broadly interpreted as "the perceived likelihood of one or more consequences occurring and triggering a sanction" and thus in terms of goal-directed decisions. Actors' beliefs about the likelihood of lie detection and subsequent enforcement appear to be most relevant when actors worry about undesirable interpersonal or relational consequences that may occur if others detect their dishonesty. This view can be extended in at least two ways. First, lie detection might produce desirable consequences (e.g., being "caught" lying to help another person might produce appreciation and praise). Second, dishonesty may produce undesirable consequences that are not interpersonal or relational. For example, if an agent views dishonesty as immoral, her dishonest behavior might produce the undesirable intrapersonal consequence of seeing herself as immoral, even if no one else detects it.
This extended view suggests that when deciding whether to lie in real-life contexts, actors are likely to consider the likelihood that doing so will produce both (a) ≥1 desired consequence and (b) ≥1 undesired consequence. When viewed as goal directed (consistent with dynamic approaches such as Whole Trait Theory, which views flexibility in moral behavior as intentional), decisions to act (in)honestly arise from weighing the likelihood of competing outcomes and the relative desirability of those outcomes. Greater likelihood of detection thus means one or more consequences (most likely, but not only, undesirable interpersonal consequences) are more likely to occur.
Second, the dynamic approach can also frame the "decision setting" facet of 
Figure 2
 broadly in terms of attention to the relational context of the honesty-relevant situation. This approach emphasizes the importance of different contextual factors in shaping (in)honest behavior, consistent with dynamic accounts of personality. Varying relational factors can manifest themselves in a variety of ways, including variation in the types of consequences that are possible, changes in the relative importance of consequences, and shifts in the likelihood that particular consequences will occur. These two facets call for the study of honesty through the lens of goal-oriented personality dynamics outside the laboratory. Goals, which reflect important differences among people but are also highly contextualized, are an important part of dynamic approaches to personality 
(Jayawickreme & Fleeson, 2017)
. Particular goals (e.g., financial reward, avoidance, of interpersonal sanction) are activated more frequently in some situations than in others.
Similarly, certain goals are more highly valued, viewed as more achievable, and perceived as more possible for some individuals than for others. To fully understand an individual's (dis)honesty, we need to understand both the situational factors that shape the availability and achievability of (dis)honesty-relevant goals and the personality factors that shape which goals are considered desirable and attainable (by some people).
This perspective has both methodological and conceptual implications. Methodologically, insights into (dis)honesty will emerge from studying individuals' honesty-relevant goals, perceptions, beliefs, and behaviors as they move from situation to situation. An extremely useful approach to do so is through intensive assessments such as experience sampling methods (ESM, e.g., 
Meindl et al., 2015)
, which provide the opportunity to efficiently and quickly "observe" many individuals in many real-life situations. Indeed, consistent with the dynamic approach, recent work has used intensive measurement to assess the frequency of honest behavior and the psychological processes, such as motivations and beliefs, that drive that behavior 
(Reynolds et al., in revision)
. One challenge here (depending on how dishonesty is defined) is a potentially low base rate of dishonesty in daily life, or at least dishonesty that is more than trivial (e.g., something more than "I'm feeling fine today, thank you" when, in truth, one is feeling slightly less fine than usual that day). If meaningful or significant dishonesty is relatively rare in everyday life, researchers will need to consider which types of dishonesty are of interest and (potentially) how to study a phenomenon with a low base rate.
Conceptually, the dynamic perspective highlights several important questions. First, and most generally, what are the mechanisms underlying moral traits such as honesty 
(Sun & Smillie, 2024)
? Second, what kinds of real-life goals are generally facilitated by honesty, by dishonesty, or potentially by both (depending on the situation)? Third, to what extent do people differ in their valuing, believing in their ability to achieve or avoid, or perceiving the availability of certain (dis)honesty-relevant goals? Fourth, to what extent can person-level differences in (dis)honest behavior be explained by goal-related dynamics?


Care for honesty vs. pro-sociality
As noted above, one of the most established trait predictors of honesty is honesty-humility from the HEXACO personality model, which represents "the tendency to be fair and genuine in dealing with others" 
(Ashton & Lee, 2007, p. 156)
 and thus the link between honesty and
prosociality. An open research question in this context, which has recently received increasing attention, is whether the relationship between honesty-humility and honest behavior remains robust when the latter conflicts with prosociality, that is, when being dishonest is associated with increasing the welfare of others, that is, being prosocial. As defined above, honesty-humility involves both fairness and sincerity; thus, individuals high in honesty-humility should be torn between honesty and prosociality whenever the two behaviors remain in conflict. In a recent attempt to address this question, Thielmann et al. (2024) manipulated who benefited from an individual's dishonesty: the individual, a "non-needy" other (e.g., another participant with an average household income), or a "needy" other (e.g., another participant with a household income below the poverty line). In the latter two conditions, dishonesty is purely prosocial because only another person benefits from an individual's lie. In turn, helping someone in need arguably involves a stronger moral imperative than helping a non-needy stranger; thus, individuals motivated by prosociality should feel particularly drawn to dishonesty when they can help a needy other. However, regardless of how morally imperative lying was, the positive relationship between honesty-humility and honesty remained robust, suggesting dispositionally honest individuals prioritize honesty over prosociality when the two conflict.
Evidence on the relationship between honesty-humility and collaborative dishonesty paints a similar picture. In one study, honesty-humility showed a negative relationship with collaborative dishonesty, regardless of whether individuals faced a fully honest or dishonest counterpart 
(Ścigała et al., 2019)
. Similarly, honesty-humility predicted honest behavior in the Unethical Loyalty Game 
(Seidl et al., 2024;
Thielmann et al., 2021)
. In this game, individuals in the role of witness can lie to cover up a transgressor's dishonesty and thus be unethically loyal.
Unethical loyalty confers a benefit on the transgressor (as well as the witness) and can therefore be considered prosocial. Abstaining from unethical loyalty (i.e., being honest about the transgressor's dishonesty), on the other hand, leaves the transgressor (and the witness) emptyhanded. In both a laboratory 
(Thielmann et al., 2021)
 and an online 
(Seidl et al., 2024)
 version of this game, honesty-humility was negatively related to unethical loyalty, meaning individuals with higher levels of honesty-humility were more likely to refrain from lying to improve another person's outcome.
Overall, the evidence suggests dispositionally honest individuals-as reflected in trait levels of honesty-humility-value honesty over prosociality. On the other hand, in most studies examining individual differences in prosocial lying, the prevalence of dishonesty was above zero even among individuals high in honesty-humility (see also 
Ścigała et al., 2020)
. In other words, even people who tend to be honest sometimes lie when doing sot benefits others, and this tendency seems to be especially true when lying has a collaborative aspect (see, e.g., 
Thielmann, Hilbig, Klein, et al., 2024)
. Future research is needed to provide a more systematic understanding of how different individuals weigh honesty versus pro-sociality in different contexts (e.g., solitary vs. relational settings; see 
Figure 2
) as a function of their personality traits. Moreover, all previous studies implemented paradigms in which lying remained anonymous to others, except for the interaction partner (in some cases). Thus, in line with the above framework 
(Figure 2
), another avenue for future research might be to examine how personality manifests in prosocial lying when enforcement is high, for example, because dishonesty can be detected (and potentially punished) by uninvolved parties.


Gender differences
Since the seminal observation that men are more likely than women to tell self-serving lies in everyday contexts 
(DePaulo et al., 1996)
, behavioral scientists have extensively used economic games to explore potential gender differences in the propensity to lie for personal gain 
(Kennedy & Kray, 2022)
.
Early research by 
Dreber and Johannesson (2008)
 confirmed that, on average, men lie more than women. This finding has been replicated in several studies 
(Friesen & Gangadharan, 2012;
Conrads et al., 2013;
Ruffle & Tobol, 2017;
Muñoz García et al., 2021;
Babin et al., 2024;
Elaad et al., 2024)
. However, other studies have not found significant gender differences in lying 
(Lundquist et al., 2009;
Childs, 2012;
Ezquerra et al., 2018;
Gylfason et al., 2013;
Alfonso-Costillo et al., 2022)
. One study even found women lie more than men 
(Ruffle & Tobol, 2014)
.
Given this variability, considering meta-analyses that aggregate findings across multiple studies to provide a more comprehensive picture is useful. Capraro (2018) conducted a metaanalysis of 4,173 decisions in a sender-receiver game and found men were more likely than women to tell selfish lies. A subsequent, larger meta-analysis by 
Gerlach et al. (2019)
, which included over 40,000 observations from the sender-receiver game, matrix task, die-roll task, and coin-flip task, confirmed this trend. They found men are generally more dishonest than women, with an estimated 4-percentage-point difference in honesty. The experiments included in these meta-analyses were primarily conducted in Western societies. However, a recent working paper suggests gender differences in honesty may be culturally shaped. Whereas women are more honest than men in Western societies, this pattern does not hold in several non-Western contexts, partly due to differing social norms 
(Graf et al., 2024)
.


Gender difference in self-serving dishonesty: Other-regarding preferences or intrinsic cost of lying?
Why are women more honest than men? Is the reason that they have a higher intrinsic cost of lying, reflecting a greater moral aversion to dishonesty? Or is the reason that they are less selfish, that is, they care more about the other person's or the experimenter's payoff?
Dictator game studies have shown women tend to give more than men 
(Engel, 2011;
Rand et al., 2017;
Branas-Garza et al., 2018;
Doñate-Buendía et al., 2022)
. This finding suggests women may indeed be more concerned with the payoffs of others, which may partly explain their lower levels of self-serving dishonesty. Consistent with this view, one study found social-value orientation mediates gender differences in lying 
(Grosch & Rau, 2017)
.
To examine whether the intrinsic cost of lying also plays a role, consulting the literature on gender differences in lying that benefits another person, known as white lies, is useful 
(Erat & Gneezy, 2012)
. White lies can be categorized into altruistic white lies, which help another person at a cost to the liar, and Pareto white lies, which benefit both the liar and the receiver. If women have a higher intrinsic cost of lying, we should observe higher levels of honesty among women in these contexts as well.
In the case of altruistic white lies, initial evidence suggests women tend to tell more altruistic white lies than men 
(Erat & Gneezy, 2012)
. However, Biziou-Van-Pol et al. (2015) did not replicate this finding and instead found women tell fewer altruistic lies than men. Another study found no gender differences in lying when the benefit goes to a charity 
(Babin et al., 2024)
.
A meta-analysis of 2,940 decisions in a sender-receiver game found women are significantly less likely than men to tell altruistic white lies 
(Capraro, 2018)
. These findings suggest women's lower levels of self-serving dishonesty cannot be attributed solely to other-regarding preferences but is partly due to a higher intrinsic cost of lying.
In the context of Pareto white lies, 
Erat and Gneezy (2012)
 found that, on average, men lie more than women. However, 
Cappelen et al. (2013)
 did not replicate this finding and did not observe gender differences. An internal meta-analysis of 1,615 decisions found a marginally significant tendency for men to tell more Pareto white lies than women 
(Capraro, 2018)
.
However, this meta-analysis is limited by its small sample size and internal nature, which reduces its statistical power.
Research on white lies suggests gender differences in self-serving lying cannot be explained solely by gender differences in other-regarding preferences. Two pieces of evidence further support this view. 
D'Attoma et al. (2020)
 found women in five countries tended to report more income than men in tax-compliance experiments, even though no consistent gender differences in prosociality in these countries were present. 
Leib et al. (2021)
 conducted a metaanalysis of cooperative dishonesty and found groups with higher proportions of women tended to be more honest.
In sum, existing research suggests gender differences in self-serving lying are likely due to a combination of factors: men tend to be more self-interested and have a lower intrinsic cost of lying than women.


Heterogeneity in gender differences: Probability of detection and group interactions
The observed gender differences in self-serving lying hold on average, but considerable heterogeneity exists. Given the conceptual framework in 
Figure 2
, an important potential source of heterogeneity may be enforcement mechanisms, specifically the likelihood that a lie will be detected.
Theoretically, the risk of detection might reduce lying especially among risk-averse individuals. Because women are generally more risk-averse than men 
(Byrnes et al., 1999)
, the risk of detection might increase honesty more among women. However, experimental evidence is scarce. To our knowledge, no studies have specifically examined gender differences by experimentally manipulating the probability of detection. One study examined gender differences at a fixed detection probability of 0.5 and found women were 9 percent point more honest than men 
(Lohse & Qari, 2021)
. Given the overall 4-percentage-point gender difference in lying estimated by 
Gerlach et al. (2019)
, this finding could, at best, be seen as preliminary evidence supporting the theoretical prediction that detection probability may increase gender differences in lying. Future research should explore this possibility more thoroughly.
Other important sources of heterogeneity lie in the relationships between actors. As discussed in the previous section, whether gender differences exist in the telling of Pareto white lies remains unclear. Investigating this possibility further is an important direction for future work. Another important aspect to consider is group interactions. Previous research has shown groups are more likely to lie than individuals, primarily due to two mechanisms: communication and learning about norm compliance. Communication can expose individuals to arguments in favor of norm violation, which can change expectations about norm compliance and subsequently affect behavior 
(Kocher et al., 2018)
. These mechanisms may affect women and men differently. Because the intrinsic costs of lying are generally higher for women, group interactions that reduce these costs could lead to increased dishonesty, particularly among women. In support of this view, 
Kocher et al. (2018)
 found group interactions increased dishonesty primarily among women. Furthermore, 
Muehlheusser et al. (2015)
 found all-female groups lie less frequently than all-male and mixed-gender groups, with the latter two groups exhibiting similar levels of dishonesty. Taken together, these findings suggest a possible explanation: group interactions may increase the likelihood that women will lie by exposing them to men's arguments in favor of lying. Future research could test this hypothesis.


Cognitive Processes


Ethical dissonance
Ethical dissonance is the psychological tension between the temptation to act unethically for personal gain and the desire to maintain a moral self-image 
(Barkan et al., 2015)
. This unique form of cognitive dissonance 
(Elliot & Devine, 1994;
Festinger, 1962)
 involves violations of absolute moral values that pose a significant threat to self-integrity. Unlike typical dissonance, ethical dissonance cannot be resolved by adjusting one's attitudes to match the 
(unethical)
 behavior, because doing so would imply a decline in moral standards and further threaten one's self-image 
(Barkan et al., 2015;
Hochman et al., 2016)
.
Ethical dissonance can either be anticipated when contemplating an unethical action, such as lying (pre-violation), or experienced after an unethical action has occurred (post-violation).
Acting as an internal gatekeeper, if the threat of anticipated dissonance is strong enough, it can deter unethical actions. However, people often use preemptive justifications by reframing unethical actions to fit within moral boundaries in a way that avoids the dissonance 
(Ayal, 2020;
Shalvi et al., 2015)
. Examples include blurring ethical boundaries (e.g., claiming "there is no specific victim," Ariely & Jones, 2012), relying on negative social norms (e.g., "everyone does it"), or even justifying actions as altruistic when cheating benefits others (e.g., 
Hochman et al., 2021;
Levine & Lupoli, 2022;
Shuster et al., 2024;
Weizel & Shalvi, 2015
.
When dissonance is experienced following dishonest behavior, it poses a direct threat to people's moral identity 
(Aquino & Reed, 2002)
. In such cases, people can reduce dissonance and restore the paradoxical state of doing wrong while feeling moral 
(Gneezy et al., 2014;
. Frequently, corrections for experienced ethical dissonance involve moral-cleansing strategies aimed at righting the wrong and restoring the self-image 
(West & Zhong, 2015)
.
Examples include donations, confessions, or prosocial activities. These atoning behaviors can lead to two patterns: moral bolstering, in which ethical behaviors continue, or moral licensing, in which the earned "moral credentials" justify future unethical behaviors, as if balancing a moral account 
(Effron & Helgason, 2023;
Gneezy et al., 2014;
Peer et al., 2014;
Sachdeva et al., 2009)
.
Finally, when ethical dissonance persists, people may distance themselves from their wrongdoing by adopting stricter moral standards and judging others harshly to maintain a virtuous public image 
(Berndsen & Gausel, 2015;
Kennedy & Schweitzer, 2018)
.


Ethical dissonance and enforcement
Ethical dissonance is an internal cost of misconduct that is expected to occur independently of external detection. However, important interactions occur between internal ethical dissonance and the external detection and enforcement of disciplinary policies for misconduct.
First, the internal moral gatekeeper created by anticipated ethical dissonance can be combined with external detection to deter dishonesty. To strengthen this internal barrier, blocking the avenues of justification that reduce moral tension is essential 
(Ayal, 2020)
. Moral reminders, increased visibility, and fostering self-involvement and identification are all effective strategies for internal enforcement 
(Ayal et al., 2015;
Peer et al., 2024)
. For example, 
Zickfeld et al. (2024)
 found honesty oaths-particularly those that specify unethical behavior and increase self-engagement-reduced dishonesty. As another example, a field study focused on train passengers in France showed posters with visibility cues (e.g., eye images) combined with social norm messages reduced fare evasion and lying 
(Ayal et al., 2021)
.
Second, increased ethical dissonance may make concealing guilt more difficult for offenders. Folklore suggests guilt is visible ("liar, liar, pants on fire"), and recent studies show lie detectors based on physiological arousal are more effective when ethical dissonance is active (e.g., lying for personal gain without justification) than when dissonance is dulled by altruistic justifications 
(Hochman et al., 2021
).


An open question concerns a possible trade-off between ethical dissonance and detection.
Specifically, external enforcement may reduce ethical dissonance because external incentives often undermine intrinsic motivation 
(Cerasoli et al., 2014;
Deci, 1971;
Ryan & Deci, 2000)
.
This effect suggests that when detection is highly likely, ethical dissonance may be reduced. In addition, frequent detection may facilitate disengagement from internal moral values and replace them with practical and cynical criteria 
(Teodorescu, 2021)
. Similarly, punishment may lead to disengagement from the moral self ("I have paid my debt to society") rather than maintaining responsibility ("I will carry this guilt for the rest of my life") 
(Moore, 2015)
. Understanding the potential trade-off between internal ethical dissonance and external coercion could provide valuable insights.


Relational decisions and ethical dissonance
A critical factor in dishonest behavior is whether an individual acts alone or in concert with others. The involvement of others in the same unethical act can have opposing effects. On the one hand, groups can reduce an individual's ethical dissonance. Groups often provide a sense of anonymity and diffusion of responsibility, both of which can reduce the ethical dissonance (anticipated or experienced) associated with unethical behavior (e.g., 
Choo et al., 2019;
Weisel & Shalvi, 2015)
. Another factor that contributes to reduced ethical dissonance is the group mechanism of persuasion 
(Cialdini, 1991)
. Exposure to others' rationalizations and alignment with negative norms provides additional justifications and reinforces the rationalizations one may already hold for unethical behavior (e.g., 
Janis, 2008;
Bicchieri, 2005;
Cialdini et al., 1991)
.
Conversely, groups may increase an individual's ethical dissonance. The presence of others can increase one's sense of visibility and self-awareness, potentially amplifying ethical dissonance and promoting peer monitoring 
(Cialdini & Goldstein, 2004;
Duval & Wicklund, 1972;
McKimmie, 2015)
.
The contrasting effects of group participation on an individual's ethical dissonance provide a valuable direction for future research. Studies could explore optimal "choice architectures" 
(Thaler & Sunstein, 2021)
 and identify strategies that promote the benefits of cooperation without incurring the costs of increased dishonesty (e.g., 
Bazerman & Tenbrunsel, 2011
).


Attention
Eye-movement research has recently found dishonesty correlates with distinct patterns of attention and information processing (see 
Hochman et al., 2016;
Fosgaard et al., 2020;
Fiedler & Glöckner, 2015)
. One of these patterns is the "ethical blind spot"-a biased attentional tendency to focus on tempting and self-serving information at the expense of honest information 
(Bazerman & Tenbrunsel, 2011;
Leib et al., 2019;
Pittarello et al., 2019;
Pittarello et al., 2015;
Pittarello et al., 2016)
. To overlook what is right without feeling guilty (see 
Mazar et al., 2008)
, people need sufficient cognitive flexibility 
(Ayal et al., 2021;
Hochman et al., 2021;
Kunda, 1990;
Peleg et al., 2021)
, which is facilitated by ambiguous situations where the distinction between right and wrong is unclear (see 
Hochman et al., 2016;
Pittarello et al., 2015;
.
Building on our framework proposed in 
Figure 2
, blind spots have so far mainly been studied in situations where individuals act alone and in private settings. Thus, whether and to what extent blind spots would emerge in relational settings where the presence of others and perceived social norms guide behavior remains unclear (see 
Ayal et al., 2021;
Lois & Wessa, 2021)
. Given that real-life ethical decisions are made in social contexts 
(Peleg et al., 2019)
, answering such a question is critical.
At the macro level, 
Dorrough et al. (2023)
 found more people offer bribes to individuals from countries perceived to be highly corrupt than from countries with lower perceived corruption. 
Gächter and Schulz (2016)
 showed countries with higher levels of corruption tend to have a higher prevalence of rule violations. Finally, a cross-cultural study by 
Aycinena et al. (2022)
 found, perhaps counterintuitively, people cheated the most when the perceived social norm was that a small lie was just as unacceptable as a big lie. Given that norms shape people's moral perceptions 
(Ayal et al., 2021)
, future research should examine whether relational environments in which dishonesty is more prevalent provide individuals with sufficient cognitive flexibility to disregard ethical choices without compromising their self-concept.
At the micro level, Vives et al. 
2022
found unethical behavior perpetrated by in-group members (but not by out-group members) increased immoral behavior. In addition, 
Ayal et al. (2021)
 found that a sign at the entrance of a train station in France stating that the majority pay for their train ticket reduced fare evasion by 50%. Clearly, the presence of others (either implicitly or explicitly), as well as the relational context that highlights people's interactions with each other, is important. But how would this affect blind spots? One possibility is that observing others breaking the rules would license people to cheat and increase blind spots. Another possibility is that it would make morality more salient (see 
Becker, 1968)
, which in turn may counteract blind spots and increase honesty (see 
Pittarello et al., 2019)
. Both alternatives can shed light on how the immediate environment and the nature of relationships between individuals shape attention toward moral norms.
Social relationships can also influence ethical behavior through social pressure and shared incentives (see 
Zanetti & Butera, 2023)
. However, whether people would ignore ethical considerations if they were pressured to cheat without personal gain is unclear. Thus, their attention to ethical norms may not align with their actual behavior, suggesting a disconnect between social influence and individual decision-making, a topic worthy of further investigation.
In addition, shared interests within relationships may exacerbate ethical blind spots. 
Weisel and Shalvi (2015)
 found people engage in "corrupt collaboration" when their incentives align, compared with when they diverge. One can reasonably hypothesize ethical blind spots are more likely to emerge in situations where people's dishonest interests are aligned.
Finally, understanding the attentional process underlying moral behavior is crucial to our ability to detect and deter dishonesty. For example, 
Hochman et al. (2021)
 found people who cheated a lot to benefit others were less likely to be identified as cheaters in a lie-detection test than people who cheated a little to benefit themselves. These findings suggest good "reasons" may lead to guilt-free cheating behavior. As presented in our theoretical framework (see 
Figure   2
), much of this research has been conducted in "low" enforcement settings. An intriguing possibility would be to examine whether imposing fines of different sizes and frequencies for failing to report others' misbehavior would still lead people to fail to detect cues indicating dishonesty, even when such dishonesty is justified (e.g., prosocial lying).
In addition to practical implications, this research may shed light on how ethical blind spots may reduce our ability to detect deceptive behavior. Ultimately, this line of work has the potential to create effective interventions to correctly detect immoral acts by individuals.


Perspective
The common explanation for why people lie is that, while people want to achieve the more advantageous outcome, they also want to maintain their self-moral image. However, this explanation leaves open the question of what underlying cognitive process helps people maintain their self-image when they lie. More generally, one might ask how dishonest people view their behavior. Would a person who lied about a false outcome think of his or her action as "not lying" if that counterfactual outcome had been observed? Or perhaps this lie is a "little lie" -one that is within the norms? Of course, answering these questions requires addressing the question of what constitutes a lie and what is perceived as a lie.
Two lines of research have examined the question of what constitutes a lie. Although these two lines of research have produced consistent results, the underlying mechanisms are more complex than they appear at first glance. On the one hand, research by 
Bicchieri et al. (2022)
 suggests lying is more closely related to distorting beliefs about descriptive norms-what others actually do-than to revising prescriptive or ethical norms about what should be done.
Their findings suggest believing that many people lie does not necessarily mean one now views lying as ethical. Instead, people may engage in dishonest acts based on a biased perception that such behavior is widespread and conforms to a misperceived descriptive norm, even while their beliefs about moral standards remain intact. This finding suggests the underlying psychological mechanism of dishonesty stems more from conforming to perceived social norms than from actively reshaping one's ethical beliefs.
On the other hand, research by 
Mazar et al. (2008)
 on self-concept maintenance theory posits that people use moral flexibility and rationalization to allow dishonest behavior that violates ethical norms, but only to the extent that it still allows them to maintain a positive selfconcept as an honest person. The underlying process is to adjust one's ethical standards through motivated reasoning just enough to rationalize some unethical acts as morally permissible, while avoiding a complete shattering of one's moral self-image. Thus, self-deception about the ethicality of the behavior itself, rather than just perceptions of the descriptive norm, enables people to cheat within reasonable limits.
Several findings can be interpreted as supporting the latter account. 
Schurr et al. (2012)
 examined the effects of different choice procedures on dishonest behavior. In one of their experiments, participants played a 20-question trivia game in which they were shown the correct answers before being asked to report whether they knew the answers, while the experimenters had no way of detecting whether a particular participant was lying. Participants earned money each time they claimed to have known the correct answer, with higher payouts for difficult questions. The experiment manipulated the perspective of choice: participants in the narrowperspective condition chose between an easy and a difficult question before each trial, whereas those in the broad-perspective condition decided in advance how many easy and difficult questions they wanted to answer in the 20 trials. Consistent with the self-concept maintenance account, the study found that in the narrow-perspective condition, when faced with a sequence of 20 repeated continuous choices between easy and difficult questions, participants were more likely to choose difficult (more profitable) questions and more likely to report knowing the correct answers than in the broad-perspective condition. Thus, despite the identical nature of the task in both conditions, the higher rate of dishonesty in the narrow-perspective condition suggests participants did not change their ethical norms, but rather used a flexible perception of lying to justify their behavior. Rilke et al. (2016) have reported similar findings.


Honesty as an automatic versus controlled behavior
Several lines of research have proposed cognitive control is crucial in resolving this conflict between serving self-interest and maintaining an honest self-concept 
(Abe & Greene, 2014;
Greene & Paxton, 2009;
Mead et al., 2009;
Verschuere et al., 2018)
. However, the precise nature of how cognitive control resolves this conflict remains controversial. Scientists and philosophers have long debated the role of cognitive control in (dis)honest decision-making: according to the grace hypothesis, people naturally default to honesty. According to the will hypothesis, people naturally default to self-interest. Honesty requires willpower, so people must exercise cognitive control to override their dishonest default.
In support of the will hypothesis, some studies have reported that impairing participants' capacity for cognitive control, such as when they are exhausted by demanding tasks, sleep deprived, anxious, or under time pressure, makes them less likely to be honest 
(Barnes et al., 2011;
Gunia et al., 2012;
Shalvi et al., 2012;
Zhang et al., 2020)
. However, caution should be exercised in interpreting these findings, because a substantial part of this body of evidence relies on now highly controversial experimental manipulations such as ego depletion 
(Vohs et al., 2021)
, and replicating study findings with uncontroversial manipulations appears to be difficult 
(Van Der Cruyssen et al., 2020)
. A number of fMRI studies show that when regions in the cognitive control network, such as the dorsolateral and ventrolateral prefrontal cortex, are activated, dishonest individuals can suppress selfish impulses and avoid lying 
(Abe et al., 2018;
Abe & Greene, 2014;
Dogan et al., 2016;
Knoch et al., 2006;
Yin et al., 2016)
. Early work also suggests that when activity in these areas is disrupted by transcranial magnetic stimulation (TMS) or lesions, selfish behavior predominates 
(Knoch et al., 2006;
Zhu et al., 2014)
. However, caution is warranted in interpreting TMS results, because a meta-analysis revealed they are inconsistent and effect sizes are small (see 
Ganis, 2014
Ganis, , 2015
, for a critical review).
Evidence in favor of the grace hypothesis comes from studies showing people respond more quickly when asked to tell the truth than when asked to lie (see the meta-analysis of 114 studies with n = 3,307; 
Suchotzki et al., 2017;
Verschuere et al., 2018)
. Moreover, the fact that most people report being truthful most of the time 
(Serota et al. 2010;
Debey et al. 2015;
Halevy et al. 2014
) is also consistent with the "truth default"-the idea that telling the truth is the dominant and automated behavior in human communication 
(Levine, 2014)
. That the truth would come naturally also explains the increased activity in cognitive control regions of the brain when lying (for a meta-analysis, see 
Christ et al., 2009)
. Further support for the grace hypothesis comes from developmental studies showing children begin life completely honest and only gradually learn to lie as their cognitive control capacities develop and they acquire the ability to inhibit (honest) behavior 
(Talwar & Lee, 2002
. Moreover, introducing time pressure in contexts where decision-makers do not have immediate access to their payoff-maximizing strategy makes them more likely to resort to telling the truth. This finding suggests the true state of the world serves as an anchor to guide individuals when they are unable to fully calculate the consequences of their available actions 
(Capraro, 2017;
Lohse et al., 2018;
.


The moral default
The will and grace hypotheses seem to be in direct conflict: they cannot both be correct.
Several explanations have been proposed, including the incentives and self-interest that deception brings , the social consequences of lying 
(Köbis et al., 2019)
, and the accessibility of the payoff-maximizing strategy 
(Capraro, 2024)
. Another factor that may help reconcile these two hypotheses is the recognition of significant individual differences in (dis)honesty 
(Speer et al., 2021
(Speer et al., , 2020
(Speer et al., , 2022b
Yin & Weber, 2019)
. Several personality traits, such as psychopathic traits, impulsivity, creativity, greed, and implicit attitudes toward (dis)honesty, are associated with these variations in unethical behavior and the tendency to cheat 
(Anderman et al., 2009;
Halevy et al., 2014;
Hatta et al., 2022;
Seuntjens et al., 2019;
S.P.H. Speer et al., 2022b)
.
Recent studies have investigated whether these dispositions toward (dis)honesty are encoded in stable patterns of the resting brain. They used resting-state fMRI, which measures fluctuations in connectivity patterns when people are not engaged in a specific task. Variability in resting-state connectivity has been found to be substantial across individuals, but stable within an individual over time 
(Cao et al., 2014;
Finn et al., 2015;
Zuo & Xing, 2014)
. Metaphorically, resting-state connectivity patterns could be compared to a brain "fingerprint." It has been associated with individual differences in personality as well as social decision-making 
(Cai et al., 2020;
Li et al., 2013;
Nostro et al., 2018)
. Resting-state studies of (dis)honesty have found higher connectivity between self-referential thinking and the reward network is associated with more honesty. This finding has been replicated across age groups 
(Yin et al., 2021)
 and cultures 
(Pang et al., 2021)
, and a recent study has also linked similar connectivity patterns to driving violations 
(Ju, 2023)
.
Taken together, compelling evidence shows considerable heterogeneity in (dis)honesty, and emerging evidence suggests this heterogeneity is encoded in stable patterns of connectivity in individuals' brains. Thus, participants may be distributed along a continuum: whereas somearguably most-people are more predisposed to honesty, others-probably a minority-are more predisposed to cheating (meta-analyses suggest this distribution appears to be skewed towards honesty: 
Abeler et al., 2019;
Gerlach et al., 2019)
. On the one hand, participants have a default bias toward honesty, which is associated with more self-referential thinking when faced with the opportunity to cheat. On the other hand, people have a strong predisposition toward dishonesty, and their decisions are driven by reward anticipation 
(Speer et al., 2022a)
. The predisposition toward honesty or (dis)honesty has been termed "moral default" 
(Speer et al., 2022a)
. Thus, whether situational demands (e.g., stress, demanding tasks, or sleep deprivation) make people more or less likely to lie depends on their moral default. Those who are honest would be more honest in the presence (vs. absence) of pressure, whereas those who are dishonest would be more dishonest in the presence (vs. absence) of pressure. Now, this idea of a moral default raises several important questions (see 
Table 1
).
Research in this area could greatly benefit from a measure of moral default that is easy to administer. This approach would allow such a measure to be routinely included in research, extending findings from cognitive neuroscience to psychology, communication science, and behavioral economics. Beginning this quest with established measures, such as self-report measures of psychopathy 
(Muris et al., 2017)
 or honesty 
(Thielmann et al., 2017)
, seems prudent.
The key research question is whether this moral-standard measure reliably explains substantial heterogeneity in self-report, behavioral, and neuroimaging research findings. Does a simpler alternative exist to resting-state connectivity to reliably assess moral default and explain heterogeneity in self-report, behavioral, and neural responses to (dis)honesty? Do current findings, typically obtained by testing individuals in isolation, extend to real-time, dynamic, twoway interactions? And does science on the intuitive nature of (dis)honesty allow for the development of new ways to distinguish lies from truth? Do new insights into the intuitive nature of (dis)honesty allow us to design interventions to promote greater honesty in society? Such a moral-default measure allows us to examine how people with different moral defaults resolve the moral conflict associated with lying. Research suggests conflict is often resolved through compromise, where people behave dishonestly enough to benefit, but honestly enough to maintain a positive self-concept. Recent neuroimaging studies 
(Speer et al., 2021
(Speer et al., , 2020
) examined how cognitive control balances rewards and self-image. The studies found dishonest people needed cognitive control to be honest. Conversely, honest participants used cognitive control to cheat. Thus, the role of cognitive control varies with an individual's moral standard, with honest individuals using it to suppress honesty at times, whereas cheaters use it to act honestly at times. Current research on dishonesty tends to study individuals in isolation-a respondent to a survey, a player in a behavioral economics game, or a person lying in an MRI scanner. But deception is inherently a social behavior. So where is the other? Recently, behavioral economists have changed the games they play, so they can now study interactions between individuals 
(Leib et al., 2021)
. Similarly, neuroimaging research on (dis)honest decision-making has primarily used paradigms that do not allow for real-time, dynamic, two-way interaction. A promising future avenue may be to move away from these conventional paradigms by studying spontaneous and voluntary dishonesty and its neurocognitive underpinnings in naturalistic, real-time interactions such as conflictual conversations. Hyperscanning can be used for this purpose. In hyperscanning, neuroimaging data are collected from dyads engaged in two-way interactions, dynamically shaping the conversation while their neural activity is recorded in real time. The advantage of this approach is that it allows researchers to capture the emergent properties of these interactions. In the context of dishonesty, it allows us to assess (a) what neurocognitive underpinnings precede the decision to cheat in a naturalistic setting, (b) how the interaction partner perceives this dishonest act, and (c) how dishonesty affects the interaction and the underlying neural dynamics between the two interaction partners.


The applied agenda: Detecting deception
Although several basic research questions remain to be answered, considering possible applications of more fundamental deception research is worthwhile. Here, we consider one such application, namely, deception detection.
About a decade ago, 
Vrij et al. (2008)
 proposed a novel approach to deception detection rooted in the current understanding of deception. That people, on average, take longer to lie than to tell the truth 
(Suchotzki et al., 2017)
 supports the view that truth-telling is more automatic and that lying requires cognitive control. The novel idea was to design interventions that further increase the cognitive load of lying so that it becomes apparent and can be used as a cue to lie.
To impose cognitive load, the interviewer could ask unexpected questions, tell the story in reverse order, or have the interviewee perform an additional task. Similarly, 
Speer et al. (2023)
 found exposing people to stress reduces cognitive control and reveals moral deficiencies 
(Speer et al., 2023)
. Whereas some studies have been successful, others have failed to replicate these findings 
(Brimbal et al., 2023)
, and a meta-analysis showed imposing cognitive load does not readily improve deception detection 
(Mac Giolla & Luke, 2021;
Verschuere et al., 2018)
. Again, however, an unexplored area of research is whether individual differences in the propensity to lie or tell the truth might partly explain the divergent research findings.
Are people more likely to be honest or dishonest? Research from several disciplines seems to converge on the idea that substantial individual differences exist. For the vast majority, the moral default is to be honest, but a minority may tend to be dishonest. Research would benefit from a simple measure of the moral default, not only to study how cognitive conflict in moral decision-making is resolved, but also to tailor interventions to promote honesty and perhaps improve deception detection techniques.


Embodied (dis)honesty
Embodied cognition, an approach that has profoundly influenced the neuropsychological study of mental processes over the past two decades, postulates that even low-level, body-related exteroceptive and interoceptive signals mediate high-level, complex processes 
(Barsalou, 2008)
.
Body and brain signatures of "embodiment" have been provided for motor-cognitive 
(Moro et al., 2008;
Urgesi et al., 2007a
, 2007b
), emotional (de Gelder, 2006
, highly abstract (e.g., grammar) 
(Williams et al., 2009;
Liuzza et al., 2011)
, and even ineffable functions such as self-transcendence 
(Urgesi et al., 2010)
, sense of fairness 
(Lenggenhager et al., 2013)
, and observation of immoral acts 
(Liuzza et al., 2014)
. The bodily dimension of morality is also suggested by the need to wash one's body after an immoral act 
(Lee & Schwarz, 2010)
, the increased severity of moral judgments associated with bodily dirtiness 
(Schnall et al., 2008)
 -an effect which, however, was not confirmed in a direct replication study 
(Johnson et al., 2014)
-, and the effect of physical cleanliness in reducing the distressing consequences of unethical behavior and threats to one's moral self-image 
(Zhong & Liljenquist, 2006
).
As noted above, (dis)honesty involves a conflict between the temptation to gain some benefit and the desire to conform to personal and societal moral norms or to avoid aversive social consequences 
(Mazar et al., 2008;
Mead et al., 2009)
. It is also influenced by complex variables related to the individual, such as personality 
(Johnson et al, 2005;
Kashy & DePaulo, 1996;
Panasiti et al., 2011)
, creativity 
(Vincent & Kouchaki, 2015)
, emotional and cognitive intelligence 
(Pittarello et al., 2017;
Sarzyńska et al., 2017), and
affiliation (e.g., religious, Arbel et al., 2014;
Bar-El & Tobol, 2017
) with a social group. Exploring whether individual dishonesty is also influenced by the level of unethical behavior prevalent in a given community has not yielded clear results, with data supporting 
(Gächter & Schulz, 2016)
 or contradicting 
(Mann et al., 2016)
 the primary role of societal habits in promoting individual dishonesty. Interestingly, however, the notion that people may be fundamentally similar across societies in their tendency to deceive others does not imply dishonest behavior is independent of context but calls for studies of the relationships between dishonesty and body-related signals in the brain of people who act unethically, individually or as a group.
A research program on embodied morality has been developed around the notion of bodily self-consciousness (BSC), that is, a basic primitive of the "self" 
(Berlucchi & Aglioti, 1997;
2010)
, which consists mainly of the sense of ownership of one's own body, that is, the sense of ownership 
(Blanke & Metzinger, 2009;
Blanke, 2012)
, and of being in control of one's own actions, that is, the sense of agency, a mental property inextricably linked to the notion of free will and responsibility 
(Haggard, 2017)
. BSC arises from the integration of body-related exteroceptive (e.g., visual and auditory) and interoceptive (e.g., cardiac, respiratory, bowel motility, hunger, thirst, temperature, proprioception, pain, itch, and sensory touch) signals.
Importantly, the BSC is critical not only for defining the sense of self, but also for distinguishing the self from others 
(Berlucchi &Aglioti, 2010;
Salomon, 2017)
, which is a central process in social cognition 
(Decety & Sommerville, 2003)
. The strength of this self-other distinction can be deeply influenced by social dimensions, as evidenced by situations in which individuals feel a sense of fusion (shared BSC) with their in-groups, both in terms of ownership (e.g., feeling like the same entity) and agency (e.g., feeling like acting as a single entity; 
Swann et al., 2012;
Besta et al., 2016)
. This sense of fused identity supports extreme pro-group behavior 
(Swann & Buhrmester, 2015)
, which, although considered moral by some, can be devastating for many (e.g., suicidal terrorist attacks in the name of group identity; 
Swann et al., 2009
Swann et al., , 2010
. In this sense, changes in the BSC may influence individual dishonesty. Several hypotheses related to this issue have been tested using the Temptation to Lie Card Game (TLCG) as a proxy for morality 
(Panasiti et al., 2011)
. The TLCG is an experimental task in which participants spontaneously decide to lie (egoistically or altruistically) to another player (typically a research confederate) who must try to pick the trump card (i.e., the ace of hearts) from two covered cards.
Favorable trials are those in which the chosen card is a loss to the participant, and vice versa in unfavorable trials. Importantly, the opponent cannot see directly whether he has won or lost, but receives this information from the participant, who can decide to lie and thus reverse the outcome of the game (i.e., win if he has lost [self-gain lie]-or lose if he has won [other-gain lie]), or to tell the truth. Because winning involves taking money out of a common pocket, self-gain lies are tantamount to stealing money from another individual, thus committing specific moral violations. The risk of being caught in a deception and thus jeopardizing one's reputation varied across conditions. The moral personality of the participants was also measured.
The TLCG proved to be a suitable tool for studying social deception under wellcontrolled and still rather ecological conditions. Indeed, the number of self-gain lies was influenced by reputation risk, modulated by moral-related personality traits such as Machiavellianism versus social reward dependence, and modulated by the perceived warmth or competence of the interactive partners 
(Panasiti et al., 2014
(Panasiti et al., , 2016
Azevedo et al., 2017)
.
Moreover, bodily or brain signatures of the process of self-gain have been provided 
(Panasiti et al., 2016
(Panasiti et al., , 2014
Dupont et al., 2023)
. In addition to demonstrating a link between the BSC and moral personality 
(Scattolin et al., 2022a)
, using the TLCG as a proxy for morality allowed us to show increasing or decreasing the sense of ownership over an artificial virtual body decreases and increases individual honesty 
(Scattolin et al., 2022b)
. Similarly, a study of people with bodyintegrity disorders shows deceptive choices expressed with a disowned body part reduce body ownership 
(Scattolin et al., 2023)
. Also note that when reputation was at risk (i.e., the other player knew they were lying), participants told significantly fewer egoistic lies than when their decisions were secret, an effect that was moderated by the internal bodily signals associated with cardiac interoception 
(Vabba et al., 2002)
. Finally, mindfulness meditation may reduce dishonest behavior through increased regulation of interoceptive awareness 
(Feruglio et al., 2024)
. This finding support"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]