You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Over the past several decades, evidence accumulation models (EAMs) have become the dominant theoretical framework for rapid decision-making (see 
Evans & Wagenmakers,
 2020 for a review). EAMs propose that the decision-making process involves the noisy accumulation of information for the different response alternatives, until the decision maker's criterion amount of evidence is reached for one alternative, at which point they make a response for that alternative. Importantly, EAMs have helped to further the area of rapid decision-making in both a theoretical and applied sense. In a theoretical sense, EAMs are able to explain a number of key benchmark phenomena in the rapid decision-making literature, such as the speed-accuracy tradeoff 
(Ratcliff & Smith, 2004)
, the ubiquitous positive skew in response time distributions 
(Ratcliff, 1978;
Ratcliff & Rouder, 1998)
; and the relative speed of correct and error response times 
(Ratcliff & Tuerlinckx, 2002)
. In an applied sense, EAMs are able to decompose the response time and accuracy of responses into theoretically meaningful latent cognitive constructs 
(Evans & Wagenmakers, 2019)
, which have helped to answer a range of research questions in areas, such as ageing 
(Ratcliff, Thapar, & McKoon, 2001;
Starns & Ratcliff, 2010;
Servant & Evans, 2020)
, clinical populations 
(Huang-Pollock et al., 2020;
Manning et al., 2022
Manning et al., , 2022
, intelligence 
(Lerche et al., 2020;
Ratcliff, Thapar, & McKoon, 2010)
, personality 
(Evans, Rae, Bushmakin, Rubin, & Brown, 2017)
, early life adversity 
(Knowles, Evans, & Burke, 2019)
, and sleep deprivation 
(Ratcliff & Van Dongen, 2018)
.
The dominance of EAMs has lead to a proliferation of different variants, which intend to either extend the framework theoretically, or to further aid applications. Specifically, the standard diffusion model (M. 
Stone, 1960)
 only contains 4 parameters: the speed of information processing (drift rate), response caution (threshold), initial response bias (starting point), and time spent on other processes (non-decision time). However, variants have been developed in an attempt to further simplify the evidence accumulation framework to improve estimation and computational tractability 
(Wagenmakers, van der Maas, & Grasman, 2007;
Brown & Heathcote, 2008)
, or to propose new theories about how the specifics of the evidence accumulation process operates 
(Usher & McClelland, 2001;
Ditterich, 2006;
Cisek, Puskas, & El-Murr, 2009;
Trueblood, Heathcote, Evans, & Holmes, 2021;
Purcell et al., 2010)
. While the latter, theoretically-driven variants can in principle be contrasted to one another to improve our understanding of decision-making -with models that better explain the empirical data being thought of as more accurate representations of the process -the utility in contrasting models heavily depends on how distinguishable their predictions are about empirical data. Unfortunately, these EAM variants have been shown to display high levels of model mimicry with one another in standard contexts; that is, while the models propose theoretically distinct processes, they make extremely similar predictions about the choice response time distributions in empirical data (though see 
Teodorescu & Usher, 2013
 for experimental approaches to teasing apart the predictions of these models with experimental manipulations). Importantly, this mimicry issue has somewhat stalled theoretical progress on understanding the specifics of the decision-making process, as while different models that propose different processes exist, the issue of model mimicry prevents them from being properly contrasted within standard choice response time data alone 
(Evans, Dutilh, Wagenmakers, & van der Maas, 2020;
Leite & Ratcliff, 2010)
.
One potential method for limiting model mimicry is through constraining the models to account for additional types of data, making it increasingly difficult to successfully account for all trends in the empirical data 
(Servant, White, Montagnini, & Burle, 2015;
Servant, Logan, Gajdos, & Evans, 2021;
Evans, Dutilh, et al., 2020)
. Specifically, while EAMs are typically only required to account for the trends in the choice response time distributions, they can be extended to also account for other types of data (e.g., muscle movements), which may result in different underlying architectures making distinguishable joint predictions across all types of data. Recently, 
Evans, Dutilh, et al. (2020)
 showcased how double responding -a behavioural phenomenon where a second response is made after the first response for a different alternative (see also C. 
Stone, Mattingley, & Rangelov, 2022
 for a review of the change of mind literature) -can provide important constraint on EAMs and help to distinguish between different variants. Specifically, 
Evans, Dutilh, et al. (2020)
 found that models with lateral inhibition -such as the leaky competing accumulator -provided the best joint account of choice response time distributions and double responding, whereas models with feed-forward inhibition -such as the diffusion model -were unable to account for these data, showing a large over-prediction for the proportion of double responses in all participants. These findings lead 
Evans, Dutilh, et al. (2020)
 to conclude that (1) additional sources of data, such as double responses, can help to adequately constrain different EAM variants, and distinguish between their predictions despite the typically high levels of model mimicry, and (2) models with lateral inhibition provide the best account of the decision-making process, and that models with feed-forward inhibition show strong qualitative inaccuracies when constrained to account for double responding behaviour. 
Evans, Dutilh, et al. (2020)
 was whether EAMs should be constrained by double responding behaviour that implicitly occurs or is explicitly requested from participants. Specifically, implicit double responses refer to situations where participants are not told that they can make second responses, meaning that these second responses can be thought of as a spillover from the initial decision that participants naturally act upon in an attempt to correct their previous decision. Alternatively, explicit double responses refer to situations where participants are explicitly told that they can change their responses, meaning that these second responses can be thought of as the combination of both information spillover and intentional corrective responses. Importantly, studying each type of double responding behavioural has its own unique strengths and weaknesses.


One issue discussed by
A strength of studying implicit double responding is that is does not require researchers to alter their paradigm of interest, limiting potential generalisation issues due to changes in the task. However, while previous research has shown that participants do make implicit double responses to correct previous errors 
(Rabbitt, 2002;
Evans, Dutilh, et al., 2020)
, it is likely that participants inhibit some of these second responses as they are not aware that they are even being recorded, meaning that studying implicit double responding likely fails to capture at least some portion of instances where participants change their mind.
Alternatively, studying explicit double responding should ensure that all changes of mind are captured, as participants are explicitly told that they may make a second response in these instances. However, it is possible that the knowledge that they are able to change their mind may result in participants altering how they make initial decisions -such as being less cautious, or paying less attention -meaning that the results from explicit double responding paradigms may not generalise well to standard decision-making paradigms. Importantly, 
Evans, Dutilh, et al. (2020)
 studied implicit double responding due to concerns about the generalisability of explicit double responding. While justifiable, the use of implicit double responding means that the overprediction of double responding behaviour by the diffusion model could actually reflect a large portion of changes of mind being missed in the implicit context, rather than the diffusion model providing a poor account of double responding behaviour, and that the diffusion model may perform better within an explicit double responding paradigm. However, as far as we are aware, no previous studies have attempted to validate that explicitly instructing participants that they can change their mind does not change how they make their initial decisions, making it difficult to currently justify studying explicit double responding as the results may not generalise to standard paradigms, where these EAM variants are most commonly studied. Furthermore, previous studies in decision-making have shown the existence of interference effects 
(Kvam, Pleskac, Yu, & Busemeyer, 2015;
Yearsley & Pothos, 2016)
, where making initial decisions can alter decision-making for later decisions, showing that the incorporation of additional explicit decisions can influence the decision process in some contexts.
The current study aims to provide a validation for the use of explicit double responding.
Specifically, we present two experiments where participants are given the option to correct their responses in one section of trials, and no option to correct their responses in the other section of trials. These experiments provide an assessment of whether people change how they make initial decisions when given the option to correct their decisions afterwards. Importantly, if we do not observe any differences in the decision-making process between these conditions -via assessing changes in the diffusion model parameters of drift rate, threshold, and non-decision time -then these findings would suggest that results from explicit double responding paradigms provide a valid generalisation to standard paradigms without second responses. Experiment 1 includes an additional manipulation of task difficulty, in order to assess whether any potential differences between conditions may be modulated by task difficulty. Experiment 2 provides a conceptual replication of Experiment 1, and includes both task difficulty and speed/accuracy emphasis manipulations to assess whether any potential differences between conditions may be modulated by either/both of these manipulations, which are commonly implemented in rapid decision-making experiments.


Method
For brevity, the methods of both experiments are presented together, with any differences between experiments mentioned explicitly within each sub-section.


Participants
Participants were 79 (Experiment 1) and 60 (Experiment 2) undergraduate students from the University of Queensland, who completed the experiment in lab and were reimbursed with course credit in accordance with the protocol approved by the University of Queensland Human Research Ethics Committee. For Experiment 1, we defined an exclusion criterion of 90% accuracy on the 40% coherence trials, where participants scoring below this criterion on these extremely easy trials were considered to not be performing the task properly. For Experiment 2, as we did not include 40% coherence trials, we instead defined an exclusion criterion of 65% accuracy on the 10% coherence accuracy-emphasis trials. These exclusion criteria resulted in 70 (Experiment 1; 9 excluded) and 42 (Experiment 2; 18 excluded) participants being included in the analysis.


Task and procedure
Before beginning the study, participants were first provided with some basic information about the study, and then indicated their consent to take part by proceeding to the experimental task. Stimuli were presented on a 16-in CRT monitor at a resolution of 1024
x 768 and a refresh rate of 85Hz using a web-browser (using custom-designed Javascript code; Evans & Brown, 2017), with participants reading task specific instructions before commencing.
Participants performed a two-alternative forced choice random dot kinetogram task, which required decisions about the apparent motion of a cloud of dots. Before each trial, participants were presented with a fixation cross for 500ms before stimulus onset. During each trial participants observed a cloud of 40 white dots on a black background, where some percentage (defined by the trial coherence) of these dots appeared to move in a coherent direction according to the white-noise algorithm 
(Pilly & Seitz, 2009)
, either towards the top-left or top-right of the screen (see 
or Evans, Hawkins, & Brown, 2020
 for further specifics on the algorithm used). Participants responded with the left arrow or right arrow key if they believed that the coherent motion was towards the top-left or topright of the screen, respectively. If participants responded within 150ms of the stimulus presentation, they were timed out for 1500ms with the message "Too Fast!" and their response time displaying, to discourage anticipatory responding. If participants responded after 150ms, their response was displayed on screen via the response boxes for 1000ms (with the ability to make another corrective response differing based on the manipulation of instructions), before they were shown feedback for 700ms on whether their response was correct and their time taken on the trial.
Participants completed 16 blocks of 28 trials of the random dot kinetogram task, creating a total of 448 trials per participant. These 16 blocks were split into 2 sections of 8 blocks each, which formed the key manipulation of corrective instructions: whether participants were instructed that they could (correction allowed), or could not (no correction), change their initial decision. In the correction allowed condition, participants were told "After you make your response, there will be a 1000 millisecond period where you may change your response. The direction of your response will be displayed on the screen in blue, and you may change your final answer up to three times within this period of time". Furthermore, the virtual response boxes on the left and right side of the screen -which remained white during the trial, and turned blue when a response was made for the selected responseupdated when participants changed their initial response, and the correct/incorrect feedback at the end of the trial was based on the final response of the participant. In the no correction condition, participants were told "After you make your response, there will be a 1000 millisecond waiting period. The direction of your response will be displayed on the screen in blue, though you will not be able to change your final answer". Furthermore, Schematic representation of task as presented to the participant, with this example assuming the coherent dots are moving upwards and to the right. After the initial response participants have 1000ms in which they can change their response up to three times. Feedback is provided for the final response. the virtual responses boxes did not update if participants made another response, and the correct/incorrect feedback at the end of the trial was based on the initial response of the participant. We counterbalanced the corrective instructions conditions, where participants were randomly assigned to either first complete the correction allowed condition (Experiment 1: 36; Experiment 2: 19) or the no correction condition (Experiment 1: 34; Experiment 2: 23).
Experiment 2 also included an emphasis manipulation, where participants were in-structed to either emphasise speed 1 or accuracy 2 in their decision-making in different blocks. Specifically, the 8 blocks of each corrective instructions condition were split into sub-sections of 4 blocks, where participants received additional emphasis instructions. We counterbalanced the emphasis conditions, where participants were randomly assigned to either first complete the speed emphasis condition (20) or the accuracy emphasis condition (22). Furthermore, participants completed emphasis conditions in the same order in both the correction allowed and no correction sections of the experiment, creating 4 different orders that participants could complete the conditions of Experiment 2 (see 
Table 1
). 


Design and Data Analysis
We treated Experiment 1 as a 2 (corrective instructions: correction allowed, no correction) * 2 (initial corrective instructions: correction allowed, no correction) * 4 (coherence:
0%, 5%, 10%, 40%) mixed design, and Experiment 2 as a 2 (corrective instructions: correction allowed, no correction) * 2 (initial corrective instructions: correction allowed, no correction) * 2 (SAT 3 emphasis: accuracy, speed) * 2 (initial SAT emphasis: accuracy, speed) * 2 (coherence: 5%, 10%) mixed design. In both experiments, the 'initial' condition factors were the only between group factors. We excluded responses faster than 150ms
and slower than 6000ms, assuming that these reflected anticipatory responses and lapses in attention, respectively.
The focus of the current study was that of Model Application (i.e., applying an existing model to empirical data to estimate the latent constructs of interest; 
CrÃ¼well, Stefan, & Evans, 2019)
 We applied the simple diffusion model (M. 
Stone, 1960)
 to estimate the cognitive constructs of interest: drift rate (task ability and effort; v), decision threshold (response caution; a), and non-decision time (encoding and motor time; ter), due to its reliable measurement properties 
(Lerche & Voss, 2016)
. More specifically, we estimated these parameters using Bayesian hierarchical estimation, where the parameters of each individual were constrained to follow group-level distributions. Furthermore, in order to account for potential learning effects in the model parameters that could influence inferences about the corrective instructions conditions (and in the case of Experiment 2, the SAT emphasis conditions), we included group-level parameter(s) that allowed for differences between conditions based on the order that the conditions were completed, removing any changes between conditions that could be directly attributed to order effects that were consistent across counterbalancing groups 4 . In Experiment 1, we only allowed drift rate to vary over coherence, and allowed all parameters except for starting point (including the drift rates of each of the coherence conditions) to potentially vary over corrective instructions. In Experiment 2, we only allowed drift rate to vary over coherence, and also allowed drift rate, threshold, and non-decision time to potentially vary over both corrective instructions and SAT emphasis, with the latter being due to previous findings being mixed on what parameters are influenced by SAT manipulations 
(Voss, Rothermund, & Voss, 2004;
Rae, Heathcote, Donkin, Averell, & Brown, 2014;
Dutilh et al., 2019;
Evans, 2021)
. The exact model specifications (including prior distributions) can be seen in the supplementary materials.
The posterior distributions of the model parameters were estimated using the Differential Evolution Markov chain Monte Carlo algorithm 
(DE-MCMC;
Ter Braak, 2006;
Turner, Sederberg, Brown, & Steyvers, 2013)
, with 3k parallel chains (where k is the number of free parameters per person) that were each run for 7,000 iterations, with the first 2,000 iterations discarded as burn-in 5 . In order to assess whether parameters changed over conditions of interest, we used the Savage-Dickey Ratio approximation 
(Wagenmakers, Lodewyckx, Kuriyal, & Grasman, 2010)
, which provides an approximation of the Bayes factor between the null and alternate hypotheses using the ratio of the posterior and prior densities where the parameter of interest is 0. Specifically, our assessments in Experiment 1 purely focused on whether each of the drift rate, threshold, and non-decision time parameters differed between corrective instructions conditions. Our assessments in Experiment 2 again assessed whether these parameters differed based on corrective instructions, but also provided a manipulation check to ensure drift rate was influenced by our coherence manipulation (as the objective difference in coherence was much smaller than Experiment 1), and an assesswere qualitatively identical (see the supplemental materials).
ment of whether each of the parameters differed based on SAT emphasis. We interpreted the strength of evidence in accordance with van Doorn et al. (2020) (see also 
Jeffreys, 1961)
. Specifically, Bayes factors less than 3 or greater than 1 3 in magnitude indicate weak evidence, between 3 -10 or 1 3 -1 10 indicate moderate evidence, between 10 -100 or 1 10 -1 100 indicate strong evidence, and greater than 100 or less than 1 100 indicate decisive evidence 6 .


Results
As with the method section, the results of both experiments are presented together for brevity. Prior-posterior plots, which showcase the prior and posterior distributions of each parameter of interest, can be seen in 
Figure 2
. As our focus is largely on testing each parameter via Bayes factors, we do not provide further discussion for these plots, but include them for readers who are interested in visually inspecting the exact estimated posterior distributions. Moreover, cumulative density function (CDF) plots, which showcase the goodness-of-fit of the model, can be seen in 
Figure 3
. As our focus is on Model Application, and not on the ability of the diffusion model to account for specific patterns within our data, we do not provide further discussion for these plots, but include them to ensure readers that our model provides an adequate fit to the empirical data.
The Savage-Dickey Ratio estimated Bayes factors for each parameter of interest in Experiment 1 can be seen in 
Table 2
. Importantly, all 6 parameters indicate evidence against an effect of corrective instruction type (i.e., evidence for the null), with the a, v 0 , v 5 , and v 10 parameters all showing strong evidence against the effect, and the ter and v 40 parameters all showing moderate evidence against the effect. A similar trend can be seen for Experiment 2 
(Table 3)
, where all 3 parameters indicate evidence against an effect of corrective instruction type (i.e., evidence for the null), with the a and v parameters showing strong
6 For those who are not familiar with Bayes factors, note that Bayes factors above 1 typically (and in our study) indicate evidence for the alternate hypothesis, and Bayes factors below 1 typically (and in our study) indicate evidence for the null hypothesis evidence against the effect, and the ter parameter showing moderate evidence against the effect. While the evidence is generally slightly weaker in Experiment 2, which might be expected due to the smaller sample size, the pattern of results are highly consistent with Experiment 1. Weak evidence for no effect on non-decision time due to SAT emphasis.
Note: Bayes factors obtained using Savage-Dickey Ratio approximation, with an order effects parameter included in the model to account for potential learning effects.
(a) Experiment 1 (b) Experiment 2 
Figure 2
. Prior-Posterior Density plots. Note: Due to the extremely high density of the posterior densities (red line) near 0, the scaling of the y-axis makes the prior distribution (blue line) appear relatively flat. These results are somewhat in contrast to previous literature that has commonly found an effect of emphasis on at least one of these parameters. Overall, the results of both Experiments 1 and 2 indicate consistent evidence against corrective instructions having an impact on the decision-making process for the initial decision.


Discussion
The aim of the current study was to provide a validation for the use of explicit double responding. Specifically, our study intended to assess whether explicitly instructing participants that they can change their mind influences how they make their initial decisions.
Across two experiments, our findings consistently indicated that the initial decision-making process of participants did not differ when they were explicitly instructed that they could change their mind, relative to when they were not given the opportunity to change their mind. More specifically, in both experiments we found evidence that task ability/attention (drift rate), response caution (decision threshold), and time dedicated to encoding/motor processes (non-decision time) each did not differ between corrective instructions conditions,
showing that these constructs do not appear to differ depending on whether or not people were told that they were allowed to correct their responses or that they were not able to do so. Importantly, these findings provide validation for the use of explicit double responding paradigms, as informing people that they can make corrective responses does not appear to influence initial decisions, and therefore, does not appear to prevent generalisation to standard paradigms.
As an aside, Experiment 2 also involved manipulations of motion coherence and speedaccuracy emphasis. As expected, motion coherence was found to manipulate drift rate and speed-accuracy emphasis was found to manipulate threshold, suggesting that these manipulations were successful. More notably, our findings also indicated evidence against speed-accuracy emphasis manipulating drift rate (strong evidence) and non-decision time (weak evidence), which is in contrast to some previous findings that have suggested that speed-accuracy emphasis also manipulates either one or both of these parameters 
(Voss et al., 2004;
Rae et al., 2014;
Dutilh et al., 2019;
Evans, 2021)
. However, it should be noted that (1) 
Evans (2021)
 found that the diffusion model mostly only showed changes in the threshold and non-decision time parameters, with our findings only showing weak evidence against a difference in non-decision time, and (2) our hierarchical approach does not properly assess individual differences in these effects, which seem to be present in other studies 
(Evans, 2021)
. Therefore, we do not believe that any strong inferences should be made regarding what parameters are influenced by speed-accuracy emphasis based on our results alone, though we believe that our study does add to the growing body of evidence suggesting that speed-accuracy emphasis does not cause differences in the diffusion model parameter of drift rate 
(Voss et al., 2004;
Dutilh et al., 2019;
Evans, 2021)
.
More generally, our findings indicate that researchers in speeded decision-making should be able to incorporate explicit double responding with little concern, as these instructions do not appear to influence the initial decision-making process of participants. We believe that this conclusion has two important implications. First, we believe that the validation of explicit double responding provides a stronger motivation for speeded decision-making researchers to more broadly adopt explicit double responding paradigms. Importantly, based on our findings, including explicit double responding does not appear to influence initial responses, while also providing a avenue for the study of change of mind in speeded decisions to become more mainstream. Second, we believe that our findings invite a reassessment of the study of 
Evans, Dutilh, et al. (2020)
 with explicit double responding.
As discussed earlier, 
Evans, Dutilh, et al. (2020)
 assessed implicit double responding be-haviour based on potential generalisability concerns with explicit double responding, and
found that the diffusion model provided a large overprediction of double responding behaviour. Importantly, this overprediction of double responding behaviour in the diffusion model may have instead reflected an underestimation of changes of mind in implicit double responding paradigms, and therefore, future research should assess whether these findings hold when using explicit double responding paradigms. However, it should be noted that we do not believe that our current data is appropriate for such an assessment, as double responding behaviour is reasonably rare, and our experiments had a limited number of trials per condition due to our 2+ manipulations per experiment and our single session of data collection. Instead, we believe that these theoretical comparisons of models using explicit double responding should focus on 'small N, big n' designs 
(Smith & Little, 2018)
, similar to first data set assessed in 
Evans, Dutilh, et al. (2020)
.


Open Practices Statement
The data and code for both experiments are available at https://osf.io/wud96/, and neither of the experiments were preregistered.


Model Specifications and Priors
Our model for Experiment 1 had the following structure and priors:
y p,i,k âˆ¼ Diffusion(v p,i,k , a p,i , ter p,i , z p v p,1,k + v p,2,k 2 âˆ¼ N (Âµ v.mean k , Ïƒ v.mean k ) v p,1,k âˆ’ v p,2,k âˆ¼ N (Âµ v.dif f k Â± Î´ v.dif f , Ïƒ v.dif f k ) a p,1 + a p,2 2 âˆ¼ N + (Âµ a.mean , Ïƒ a.mean ) a p,1 âˆ’ a p,2 âˆ¼ N (Âµ a.dif f Â± Î´ a.dif f , Ïƒ a.dif f ) ter p,1 + ter p,2 2 âˆ¼ N + (Âµ ter.mean , Ïƒ ter.mean ) ter p,1 âˆ’ ter p,2 âˆ¼ N (Âµ ter.dif f Â± Î´ ter.dif f , Ïƒ ter.dif f ) z p a p,i âˆ¼ T N 0,1 (Âµ z , Ïƒ z ) Âµ v.mean k âˆ¼ N (0.3, 0.3) Âµ a.mean âˆ¼ N + (0.2, 0.2) Âµ ter.mean âˆ¼ N + (0.3, 0.3) Âµ z âˆ¼ T N 0,1 (0.5, 0.2) Âµ v.dif f k , Âµ a.dif f , Âµ ter.dif f âˆ¼ N (0, 0.1) Î´ âˆ¼ N (0, 0.1) Ïƒ âˆ¼ Î“(1, 1)
where the first subscript p indexes participants, the second subscript i indexes corrective Our model for Experiment 2 had the following structure and priors:
y p,i,j,k âˆ¼ Diffusion(v p,i,j,k , a p,i,j , ter p,i,j , z p 2 i=1 2 j=1 2 k=1 v p,i,j,k 8 âˆ¼ N (Âµ v.mean , Ïƒ v.mean ) v p,1,, âˆ’ v p,2,, âˆ¼ N (Âµ v.dif f Â± Î´ v.dif f , Ïƒ v.dif f ) v p,,1, âˆ’ v p,,2, âˆ¼ N (Âµ v.SAT Â± Î´ v.SAT , Ïƒ v.SAT ) v p,,,1 âˆ’ v p,,,2 âˆ¼ N (Âµ v.COH , Ïƒ v.COH ) 2 i=1 2 j=1 a p,i,j 4 âˆ¼ N (Âµ a.mean , Ïƒ a.mean ) a p,1, âˆ’ a p,2, âˆ¼ N (Âµ a.dif f Â± Î´ a.dif f , Ïƒ a.dif f ) a p,,1, âˆ’ a p,,2, âˆ¼ N (Âµ a.SAT Â± Î´ a.SAT , Ïƒ a.SAT ) 2 i=1 2 j=1 ter p,i,j 4 âˆ¼ N (Âµ ter.mean , Ïƒ ter.mean ) ter p,1, âˆ’ ter p,2, âˆ¼ N (Âµ ter.dif f Â± Î´ ter.dif f , Ïƒ ter.dif f ) ter p,,1, âˆ’ ter p,,2, âˆ¼ N (Âµ ter.SAT Â± Î´ ter.SAT , Ïƒ ter.SAT ) z p a p,i,j âˆ¼ T N 0,1 (Âµ z , Ïƒ z ) Âµ v.mean âˆ¼ N (0.3, 0.3) Âµ a.mean âˆ¼ N + (0.2, 0.2) Âµ ter.mean âˆ¼ N + (0.3, 0.3) Âµ z âˆ¼ T N 0,1 (0.5, 0.2) Âµ v.dif f , Âµ a.dif f , Âµ ter.dif f âˆ¼ N (0, 0.1) Âµ v.SAT , Âµ a.SAT , Âµ ter.SAT âˆ¼ N (0, 0.1) Âµ v.COH âˆ¼ N (0, 0.1) Î´ âˆ¼ N (0, 0.1) Ïƒ âˆ¼ Î“(1, 1)
where the first subscript p indexes participants, the second subscript i indexes corrective instructions conditions, the third subscript j indexes SAT emphasis conditions, and the fourth subscript k indexes coherence conditions.


Results for Model Without Order-Effects Parameter
Experiment 1
Results are shown with no initial correction to account for learning effects. As can be seen, they are qualitatively very similar to the main results, and do not alter the conclusions drawn.  As with the first experiment, results are shown with no initial correction to account for learning effects. They are also qualitatively very similar to the main results, and do not alter the conclusions drawn.  
Figure 1.


Figure 3 .
3
CDF plots. The dots indicate the empirical data, whereas the lines with crosses indicate the model predictions. Green reflects correct responses, whereas red reflects incorrect responses.Furthermore, Experiment 2 shows decisive evidence in favour of the coherence manipulation successfully manipulating v and the SAT emphasis manipulation successfully manipulating a, showing that our manipulations were effective. Interestingly, Experiment 2 shows evidence against an effect of SAT emphasis on both v (strong) and ter (weak).


instructions conditions, and the third subscript k indexes coherence conditions. N (m, sd) denotes the normal distribution with mean m and standard deviation sd, N + (m, sd) denotes the positive truncated normal distribution, T N c,d (m, sd) denotes the truncated normal distribution with lower truncation point c and upper truncation point d, and Î“(s, r) denotes the gamma distribution with shape s and rate r.


Figure 1 .
1
Prior-Posterior Density plots and CDF plots for Experiment 1 Experiment 2


Figure 2 .
2
Prior-Posterior Density plots and CDF plots for Experiment 2


Table 1 :
1
Configurations used in Experiment 2, based on the initial condition presented to participants
Task Sections
Initial Condition 1
Initial Condition 2
Initial Condition 3
Initial Condition 4
Section 1
4 Blocks
explicit & accuracy explicit & speed
implicit & accuracy implicit & speed
28 Trials/block
Section 2
4 Blocks
explicit & speed
explicit & accuracy implicit & speed
implicit & accuracy
28 Trials/block
Section3
4 Blocks
implicit & accuracy implicit & speed
explicit & accuracy explicit & speed
28 Trials/block
Section 4
4 Blocks
implicit & speed
implicit & accuracy explicit & speed
explicit & accuracy
28 Trials/block


Table 2 :
2
Results for Experiment 1
Hyperparameter
Description
Bayes Factor
Interpretation
v 0
comparing instruction types at 0% coherence
0.0171
Strong evidence for no effect due to instruction type based on (very high) task difficulty
v 5
comparing instruction types at 5% coherence
0.0334
Strong evidence for no effect due to instruction type based on (high) task difficulty
v 10
comparing instruction types at 10% coherence
0.0836
Strong evidence for no effect due to instruction type based on (medium) task difficulty
v 40
comparing instruction types at 40% coherence
0.1241
Moderate evidence for no effect due to instruction type based on (low) task difficulty
a
comparing instruction types with decision threshold
0.0397
Strong evidence for no effect due to instruction type based on response caution
ter
comparing instruction types with non-decision time
0.1622
Moderate evidence for no effect due to instruction type based on non-decision time
Note: Bayes factors obtained using Savage-Dickey Ratio approximation, with an order effects parameter included in the model to account for potential learning effects.


Table 3 :
3
Results for Experiment 2
Hyperparameter
Description
Bayes Factor Interpretation
v
comparing instruction types with task difficulty
0.0430
Strong evidence for no effect on task difficulty due to instruction type
a
comparing instruction types with decision threshold
0.0873
Strong evidence for no effect on response caution due to instruction type
ter
comparing instruction types with non-decision time
0.1931
Moderate evidence for no effect on non-decision time due to instruction type
vCOH
comparing dot coherence with task difficulty
17609
Decisive evidence there is an effect on task difficulty due to dot coherence
vSAT
comparing SAT emphasis types with task difficulty
0.0570
Strong evidence for no effect on task difficulty due to SAT emphasis type
aSAT
comparing SAT emphasis types with decision type
39440
Decisive evidence there is an effect on response caution due SAT emphasis type
terSAT
comparing SAT emphasis types with non-decision time
0.3523


Table 1 :
1
Results for Experiment 1
Hyperparameter
Description
Bayes Factor
Interpretation
v 0
comparing instruction types
0.0122
Strong evidence for no effect due to instruction type
at 0% coherence
based on (very high) task difficulty
v 5
comparing instruction types
0.0384
Strong evidence for no effect due to instruction type
at 5% coherence
based on (high) task difficulty
v 10
comparing instruction types
0.1148
Moderate evidence for no effect due to instruction type
at 10% coherence
based on (medium) task difficulty
v 40
comparing instruction types
0.1752
Moderate evidence for no effect due to instruction type
at 40% coherence
based on (low) task difficulty
a
comparing instruction types
0.0542
Strong evidence for no effect due to instruction type
with decision threshold
based on response caution
t er
comparing instruction types
0.1742
Moderate evidence for no effect due to instruction type
with non-decision time
based on non-decision time


Table 2 :
2
Results for Experiment 2
Hyperparameter
Description
Bayes Factor
Interpretation
v
comparing instruction types with
0.0528
Strong evidence for no effect on task difficulty due
task difficulty
to instruction type
a
comparing instruction types with
0.1381
Moderate evidence for no effect on response caution
decision threshold
due to instruction type
t er
comparing instruction types with
0.1905
Moderate evidence for no effect on non-decision time
non-decision time
due to instruction type
vCOH
comparing dot coherence with task
15849
Decisive evidence there is an effect on task difficulty
difficulty
due to dot coherence
vSAT
comparing SAT instruction types
0.0559
Strong evidence for no effect on task difficulty due
with task difficulty
to SAT instruction type
aSAT
comparing SAT instruction types
103706
Decisive evidence there is an effect on response caution
with decision type
due SAT instruction type
t erSAT
comparing SAT instruction types
0.3551
Weak evidence for no effect on non-decision time
with non-decision time
due to SAT instructions.


"Furthermore, in these blocks you should place an emphasis on *speed*. Specifically, you should aim to complete these trials quickly, though without resorting to fast guesses. However, don't worry about making a few extra mistakes, as we are interested in fast decisions."2 "Furthermore, in these blocks you should place an emphasis on *accuracy*. Specifically, you should aim to minimise your errors, though without taking a long period of time on any one trial. However, don't worry if your responses are slow, as we are interested in accurate decisions."


"Speed-accuracy trade-off", reflecting whether participants were told to emphasise speed or accuracy. 4 Note that we also applied versions of the model without these order effects parameters, and the results


Also note that within our burn-in phase we ran a migration algorithm every 14 iterations between iterations 500 and 1,500.








Supplementary Material for: Does allowing for changes of mind influence initial responses?
Grant J. Taylor a , Augustine T. Nguyen a,b , and Nathan J. 
 










The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive psychology




57


3
















Decisions in changing conditions: the urgency-gating model




P
Cisek






G
A
Puskas






S
El-Murr








Journal Article]. Journal of Neuroscience




29


37
















Robust standards in cognitive science




S
CrÃ¼well






A
M
Stefan






N
J
Evans




10.1007/s42113-019-00049-8








Journal Article]. Computational Brain Behavior




2


3-4
















Evidence for time-variant decision making




J
Ditterich








European Journal of Neuroscience




24


12
















The quality of response time data inference: A blinded, collaborative assessment of the validity of cognitive models




G
Dutilh






J
Annis






S
D
Brown






P
Cassey






N
J
Evans






R
P P P
Grasman






.
.
Donkin






C












Journal Article










10.3758/s13423-017-1417-2


doi: 10.3758/ s13423-017-1417-2








Psychonomic Bulletin Review




26


4














Think fast! the implications of emphasizing urgency in decision-making




N
J
Evans












Journal Article














Cognition




214














People adopt optimal policies in simple decision-making, after practice and guidance




N
J
Evans






S
D
Brown








Journal Article


















10.3758/s13423-016-1135-1


doi: 10.3758/ s13423-016-1135-1








Psychonomic Bulletin Review




24


2














Double responding: A new constraint for models of speeded decision making




N
J
Evans






G
Dutilh






E.-J
Wagenmakers






Van Der






H
L J
Maas












Journal Article










10.1016/j.cogpsych.2020.101292








Cognitive Psychology




121


101292












The role of passing time in decision-making




N
J
Evans






G
E
Hawkins






S
D
Brown








Journal Article]. Journal of Experimental Psychology: Learning, Memory, and Cognition




725














Need for closure is associated with urgency in perceptual decision-making




N
J
Evans






B
Rae






M
Bushmakin






M
Rubin






S
D
Brown




10.3758/s13421-017-0718-z








Journal Article]. Memory Cognition




45


7
















Theoretically meaningful models can answer clinically relevant questions




N
J
Evans






E.-J
Wagenmakers








Brain




142


5


1172














Evidence accumulation models: Current limitations and future directions




N
J
Evans






E.-J
Wagenmakers




10.20982/tqmp.16.2.p073








The Quantitative Methods for Psychology






16














A diffusion model analysis of sustained attention in children with attention deficit hyperactivity disorder




C
Huang-Pollock






R
Ratcliff






G
Mckoon






A
Roule






T
Warner






J
Feldman






S
Wise












Journal Article










doi: 10.1037/ neu0000636






Neuropsychology




34


6














Theory of probability




H
Jeffreys








Oxford University Press


Oxford, UK












Some evidence for an association between early life adversity and decision urgency




J
P
Knowles






N
J
Evans






D
Burke




10.3389/fpsyg.2019.00243


doi: 10.3389/fpsyg.2019.00243








Journal Article]. Frontiers in Psychology




10














Interference effects of choice on confidence: Quantum characteristics of evidence accumulation




P
D
Kvam






T
J
Pleskac






S
Yu






J
R
Busemeyer




10.1073/pnas.1500688112








Proceedings of the National Academy of Sciences


the National Academy of Sciences






112


10645












Modeling reaction time and accuracy of multiple-alternative decisions




F
P
Leite






R
Ratcliff








Journal Article
















10.3758/APP.72.1.246


doi: 10.3758/APP.72.1.246








Psychophysics




72








Attention












V
Lerche






M
Von Krause






A
Voss






G
T
Frischkorn






A.-L
Schubert






D
Hagemann


















Diffusion modeling and intelligence: Drift rates show both domain-general and domainspecific relations with intelligence






Journal Article]. Journal of Experimental Psychology: General












Model complexity in diffusion modeling: Benefits of making the model more parsimonious




V
Lerche






A
Voss




10.3389/fpsyg.2016.01324


doi: 10.3389/ fpsyg.2016.01324








Journal Article]. Frontiers in Psychology




7














Behavioural and neural indices of perceptual decision-making in autistic children during visual motion tasks




C
Manning






C
D
Hassall






L
T
Hunt






A
M
Norcia






E.-J
Wagenmakers






N
J
Evans






G
Scerif








Scientific Reports




12


1




















C
Manning






C
D
Hassall






L
T
Hunt






A
M
Norcia






E.-J
Wagenmakers






M
J
Snowling














Visual motion and decision-making in dyslexia: Reduced accumulation of sensory evidence and related neural dynamics




N
J
Evans








Journal of Neuroscience




42


1
















What a difference a parameter makes: A psychophysical comparison of random dot motion algorithms




P
K
Pilly






A
R
Seitz








Vision Research




49




















B
A
Purcell






R
P
Heitz






J
Y
Cohen






J
D
Schall






G
D
Logan






T
J
Palmeri


















Neurally constrained modeling of perceptual decision making






Psychological Review




117














Consciousness is slower than you think




P
Rabbitt








Journal Article


















10.1080/02724980244000080






The Quarterly Journal of Experimental Psychology Section A




55


4














The hare and the tortoise: Emphasizing speed can change the evidence used to make decisions




B
Rae






A
Heathcote






C
Donkin






L
Averell






S
Brown








Journal of Experimental Psychology: Learning, Memory, and Cognition




40


5


1226














A theory of memory retrieval




R
Ratcliff








Journal Article]. Psychological review




85


2


59














Modeling response times for two-choice decisions




R
Ratcliff






J
N
Rouder




10.1111/1467-9280.00067








Journal Article]. Psychological Science




9


5
















A comparison of sequential sampling models for two-choice reaction time




R
Ratcliff






P
L
Smith












Journal Article










10.1037/0033-295x.111.2.333






Psychol Rev




111


2














The effects of aging on reaction time in a signal detection task




R
Ratcliff






A
Thapar






G
Mckoon








Psychology and aging




16


2


323














Individual differences, aging, and iq in two-choice tasks




R
Ratcliff






A
Thapar






G
Mckoon








Journal Article]. Cognitive psychology




60


3
















Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability




R
Ratcliff






F
Tuerlinckx




10.3758/bf03196302








Journal Article]. Psychonomic Bulletin Review




9


3
















The effects of sleep deprivation on item and associative recognition memory




R
Ratcliff






H
P A
Van Dongen




10.1037/xlm0000452






Journal Article]. Journal of Experimental Psychology: Learning, Memory, and Cognition




44


2
















A diffusion model analysis of the effects of aging in the flanker task




M
Servant






N
J
Evans








Psychology and Aging
















An integrated theory of deciding and acting




M
Servant






G
D
Logan






T
Gajdos






N
J
Evans




10.1037/xge0001063






Journal Article]. Journal of Experimental Psychology: General , No Pagination Specified-No Pagination Specified
















Using covert response activation to test latent assumptions of formal decision-making models in humans




M
Servant






C
White






A
Montagnini






B
Burle












Journal Article
















Journal of Neuroscience




35


28
















10.1523/JNEUROSCI.0078-15.2015




2015&partnerID=40&md5=12e0bba64eff812d1755ae6d92554e4f












Small is beautiful: In defense of the small-n design




P
L
Smith






D
R
Little








Journal Article


















10.3758/s13423-018-1451-8








Psychonomic Bulletin Review




25


6














The effects of aging on the speed-accuracy compromise: Boundary optimality in the diffusion model




J
Starns






R
Ratcliff




10.1037/a0018022








Journal Article]. Psychology and aging




25


2
















On second thoughts: changes of mind in decision-making




C
Stone






J
B
Mattingley






D
Rangelov








Trends in Cognitive Sciences
















Models for choice-reaction time




M
Stone








Psychometrika




25
















Disentangling decision models: from independence to competition




A
R
Teodorescu






M
Usher








Psychological review




120


1


1














A markov chain monte carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces




C
J
Ter Braak








Journal Article]. Statistics and Computing




16


3
















Urgency, leakage, and the relative nature of information processing in decision-making




J
S
Trueblood






A
Heathcote






N
J
Evans






W
R
Holmes








Psychological Review




128


1


160














A method for efficiently sampling from distributions with correlated dimensions




B
M
Turner






P
B
Sederberg






S
D
Brown






M
Steyvers




10.1037/a0032222








Journal Article]. Psychological Methods




18


3
















The time course of perceptual choice: the leaky, competing accumulator model




M
Usher






J
L
Mcclelland








Journal Article]. Psychological review




108


3


550














The jasp guidelines for conducting and reporting a bayesian analysis




J
Van Doorn






D
Van Den Bergh






U
BÃ¶hm






F
Dablander






K
Derks






T
Draws






.
.
Wagenmakers






E.-J












Journal Article










10.3758/s13423-020-01798-5








Psychonomic Bulletin Review












Interpreting the parameters of the diffusion model: An empirical validation




A
Voss






K
Rothermund






J
Voss








Memory & Cognition




32
















Bayesian hypothesis testing for psychologists: A tutorial on the savage-dickey method




E.-J
Wagenmakers






T
Lodewyckx






H
Kuriyal






R
Grasman








Journal Article]. Cognitive psychology




60


3
















An EZ-diffusion model for response time and accuracy




E.-J
Wagenmakers






H
J L
Van Der Maas






R
P P P
Grasman








Psychonomic Bulletin & Review




14
















Zeno's paradox in decision-making




J
M
Yearsley






E
M
Pothos












Journal Article










10.1098/rspb.2016.0291


doi: 10.1098/rspb.2016.0291








Proceedings of the Royal Society B: Biological Sciences




283















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]