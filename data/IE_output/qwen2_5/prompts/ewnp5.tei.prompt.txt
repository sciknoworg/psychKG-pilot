You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



There used to be a discipline called speculative psychology. It wasn't quite philosophy because it was concerned with empirical theory construction. It wasn't quite psychology because it wasn't an experimental science. But it used the methods of both philosophy and psychology because it was dedicated to the notion that scientific theories should be both conceptually disciplined and empirically constrained. What speculative psychologists did was this: They thought about such data as were available about mental processes, and they thought about such first-order psychological theories as had been proposed to account for the data. They then tried to elucidate the general conception of the mind that was implicit in the data and the theories. Speculative psychology was, by and large, quite a good thing. William James and John Dewey were speculative psychologists and so, in certain of his moods, was Clark Hull. 
[...]
 This book, in any event, is unabashedly an essay in speculative psychology.


Resumo
A Teoria de Duplo Processo tem ganhado fama enquanto um arcabouço para explicar evidências em tarefas de raciocínio e tomada de decisão. Essa teoria propõe que deve haver uma distinção afiada no pensamento para explicar dois aglomerados correlacionais de características. Um aglomerado descreve um processo rápido e intuitivo (Tipo 1), enquanto o outro descreve um devagar e reflexivo (Tipo 2) 
(Evans, 2008;
Evans & Stanovich, 2013;
Kahneman, 2011)
. Entretanto, como 
Samuels (2009)
 notou, existe um problema em determinar o motivo desse grupo de características formarem aglomerados, mais do que os rótulos Tipo (ou sistema) 1 e 2 conseguem capturar, 'o problema da união'. Entendemos que podem haver diferenças nas arquiteturas cognitivas que sustentam cada tipo de processo, assim exigindo arcabouços cognitivos distintos para cada. Argumentamos que a abordagem do processamento preditivo (como apresentada por Hohwy, 2013 e Clark, 2016) é um arcabouço mais adequado para processos do Tipo 1. Tal abordagem propõe que a cognição tem o trabalho de prever o que perturbará os inputs sensoriais em um próximo momento. Essas não são previsões pessoais mas múltiplas previsões subpessoais que até o sistema visual realiza em vários níveis em cada milissegundo que passa. Ao invés de ser baseado em representações simbólicas de cada aspecto do mundo, essas previsões são realizadas com base em informação estatística atualizada a cada momento. 
Kahneman (2011)
 vem argumentando que existe uma ligação entre a percepção e processamento Tipo 1. O que sustentamos é que tal ligação existe pois julgamentos do Tipo 1 na verdade são previsões derivadas de níveis altos de sistemas perceptivos que funcionam por meio do processamento preditivo. Por outro lado, argumentamos que tal arquitetura não funciona para processos do Tipo 2. Em vez disso, esses processos parecem estar baseados em sistemas simbólicos clássicos executando busca heurística como explicado por 
Newell (1980)
. Em conclusão, propomos que uma arquitetura dupla é necessária para explicar por qual motivo existem dois aglomerados de características. Esse arcabouço incluiria processamento preditivo para explicação de processos do Tipo 1 e computações em representações simbólicas para processos do Tipo 2.  
INTRODUCTION.................................................................................…..1
 1


Introduction
This introduction is twofold. The first part presents an outline of how the thesis will look like, introducing problems, our line of argument and hypotheses while providing a first inspection into each of the chapters. The second part presents an introduction to the Fodorian architecture of the mind which will serve as our theoretical starting point.


I Problems, line of argument and hypotheses
When solving math problems, it is interesting to note how some calculations, such as '2+2', almost seem to carry with it its own answer. Thus, when we hear or read such calculation, the number four rushes to mind inevitably. This observation is at odds with other calculations such as '74-37' which are also simple but no answer comes along. One applies a few mental steps in order to reach the answer, it is easy, but the answer does not come to mind as one reads or hears the numbers. Regardless of what the characteristics of the numbers are which evokes this difference in thought, it is notable that there seems to be a difference between springing to mind inevitably and applying mental steps. This sort of observation has led psychologists working under reasoning, judgment and decision making to suppose there are two sorts of processing in thinking which are responsible in one side for faster and effortless judgments and on the other side slower and effortful reasoning.
Of course, the evidence for a distinction in two types of processing is not limited to such simple observation but rather has appeared repeatedly since the 60s when (mainly) the tradition known as heuristics and biases started studying human reasoning, judgment and decision making by crafting tasks in which conflicting responses were common. In Chapter one-Research in Reasoning and Rationality-we will then review some of the most famous tasks in such tradition along with possible interpretations given to the results. Including heuristics, evolutionary psychology, pragmatics and dual process theories.
Dual process theories also are not limited to such simple theorizing and characterizations as we introduced. Thus in Chapter two-Basic Dual Process Theory-we work on fundamental 2 theoretical questions 1 that have been raised for dual process theories, including: if dual process theories can be seen as a complete theory of cognition or only of reasoning; what features characterizes each type of processing and which are defining; how one process communicates with the other and even if there are different processes at all or if distinct features points to dual systems, dual minds or dual modes (the reference problem). 
Samuels (2009)
 notes that even if one considers the evidence to be convincing and the dichotomy of processes (termed Type 1 and Type 2) and their property clusters 
(termed S1 and S2)
 well placed, we still have a basic research question open, which he calls the unity problem:
"though positing mechanisms is a standard strategy for explaining the existence of property clusters, it does not, by itself, constitute a satisfactory explanation. Rather one needs to specify those features of the proposed mechanisms that account for such clustering effects. In the present case, we need to specify those characteristics of type-1 systems that yield S1-exhibiting processes, and those properties of type-2 systems that yield S2-exhibit-ing processes. Again, this does not strike me as a serious objection so much as a challenge for future research-one that requires a more detailed account of the systems responsible for type-1 and type-2 processes." 
(Samuels, 2009, p.141)
.
The unity problem is distinct from the reference problem. The reference problem is the problem of determining what these property clusters refer to in the mind, to which a possible answer would simply be 'distinct systems', for instance. The unity problem asks: even supposing there really are two main systems responsible for the way we reason, what are the characteristics of these systems that explain why they exhibit these distinct features, rationalities and responses? What kind of systems are these that explain why they bind these group of properties together?
The unity problem guides our endeavors as we strive to explain what is underlying our mental architecture that is responsible for computational differences which result in the cognitive and behavioural effects observed by dual process theorists. 
Fodor (1983)
 shares a similar duality of mind, which will be our starting point in the next part of this introduction. He argues the mind can be divided into modular input systems (or vertical faculties more generally) and domain-general central systems (or horizontal faculties). Following the frame problem 2 , 
Fodor (1983
Fodor ( , 1987
Fodor ( , 2001
, one of the philosopher most responsible for 3 developing the computational theory of mind in the first place, dismissed it as a complete picture of mind. He detects the problem originates because such theory cannot apply to central systemswhich in his descriptions are abductive, based on semantics, and open to any aspect of the person's whole web of belief-but only to input systems which proceed syntactically and are informationally encapsulated.
One of the intuitions 3 that started our project is that 
Fodor (2001)
 diagnosed this inversely.
We think rather that it is his characterization of input systems (inflexible) that renders central systems obsolete. If there is something like central systems in our cognition any sort of input to it must already be contextualized and relevant before even reaching it. We are with 
Fodor's starting chapters (in 1975)
 where he started to propose the language of thought as means of explaining our psychology of propositional attitudes for decision and choice. This is the main role for classical architectures in our proposal and not its application to input and perception (like in 
Fodor, 1983)
.
We hold that what he characterized as input systems is better understood under a Hierarchical Bidirectional Predictive Processing (HBPP) framework as proposed by 
Friston (2003
Friston ( , 2005
Friston ( , 2008
Friston ( , 2010
, 
Hohwy (2013)
 and 
Clark (2016)
, which is also a computational theory of mind, but not the computational theory of mind of Fodor. The predictive processing account unifies aspects of different models (such as predictive coding, emulation theory, active inference) about a very similar story. Such story suggests the brain should be thought of as in a restless, active cycle of predicting what will perturb it in a proximal and distal future. Instead of being understood as reading input from the world, the predictive brain story suggests the brain uses statistics to anticipate input before they even arrive. These predictions are based on expectations (or a statistical generative model) which foresees the most likely outcome of stimuli and events and takes a bet that thus and so will obtain. By keeping and polishing expectations, the brain can poise itself ahead of the game when it comes to dealing with the world.
These models suggest the brain is formed by a hierarchy of processing (comprising higher and lower levels) where multiple layers of neurons are organized to compose a network with two major streams of information flow. The top-down flow is understood as conveying multiple 4 predictions, each higher layer attempts to predict the workings of the one underneath it. The bottom-up flow conveys error correction on previously made predictions to each higher layer. If predictions of a given event are on track than lower sensory stimulation will be attenuated, they will not even be considered. On the other hand, if predictions mislead, sensory stimulation will flag the difference between what was predicted and what was sensed and the system will try to overcome such gap. This, prediction error minimization, 
Clark (2016)
 claims, is the brain's major goal. 
Clark (2016)
 notes a very strong and interesting shift the predictive processing approach suggests. It proposes that the forward flow consists not so much of all the features that were detected to be passed onwards to higher levels but only the error necessary to correct and update models. Instead of conveying all information from the environment, rather it provides a natural funnel which guarantees processing economy by focusing on newsworthy information in the form of error correction.
While 
Clark (2016)
 sees predictive processing as a unified framework for the mind, including perception, action, reasoning and language. We will defend in Chapter Three-Intuitive Predictions-that when it comes to thinking and reasoning, predictive processing is suitable to understand only Type 1 processing. We show this by demonstrating how predictive processing fits with each of Type 1 defining and correlational features, and does not with Type 2 features.
Finally, in Chapter Four-Reflecting As Expression Search-we propose that 
Newell's (1980)
 4 architecture for cognition is suited for Type 2 features in reasoning. 
Newell and Simon (1976)
 exposition of heuristic search also plays a good role in this explanation and the GPS 
(Newell and Simon, 1963)
 model for human reasoning serves as an excellent example. We also understand 
Fodor and Pylyshyn (1987)
 make an interesting case when characterizing classical system as based on compositionality. We understand these authors make a good case for the powers of classical architecture which we see explaining Type 2 but not Type 1 features. We show how the reasons Type 1 features are not covered by classical architecture have been demonstrated by issues in artificial intelligence, mainly the frame problem.


5
We understand this thesis can answer the unity problem by explaining how computational differences in mechanisms used in each type of process make these property clusters come about.
Thus, what explains S1 properties are the powers and limitations of predictive processing. On the other hand, what explains the S2 cluster is the powers and limitations of classical architectures.
Simply exposed, these are the main problems, our line of argument and hypotheses that this thesis follows 5 .


II The Fodorian architecture as a starting point
We have now presented the path taken by thesis and the first step is to introduce the Fodorian architecture of mind. The Fodorian architecture is actually a dual process theory in its own, and one that is supposed to apply to all functions of the mind. The duality proposed by 
Fodor (1983)
 is one of input systems and general systems. What is interesting about this proposal is that it is very detailed and specific. Thus, 
Fodor (1983)
 lists very rigid criteria to explain each system.
It is not exaggerated to say that any dual process theory in cognitive science was at least partially influenced by this proposal. 
Fodor (1983)
 enumerates a few features which input systems are likely to have. He argues input systems are domain specific, that is, they respond to stimuli in one specific type of domain and ignore other modalities. Now, Fodor's domain-specificity is strong. That is because a domain specific system in his view is not that which deals with specific kinds of input, it must deal with only such inputs and must be specifically structured as to account for such input, rendering it blind to other domains. The property he mentions as one that helps identify domain-specificity is handling eccentric stimuli domains. These are domains that need specific handling and that differ from more general and usual tasks of the mind. For instance, a usual task is categorizing items according to their class, but to understand sentences one cannot classify tokens of language sentences, it is probable that some specific biases are necessary in order to constrain human 5
The reader is advised to return to this short explanation of the path of the thesis if at any moment he feels lost. 6 sentences understanding as 
Chomsky (1975)
 has noted. Thus, so the argument goes, language might need domain-specific modules that recognize important characteristics that makes it unique.
The second characteristic is that input systems are mandatory, they are obligatorily applied when they encounter their domain stimuli. Another example from language is that one cannot hear language as noise even if she tries. Also, Fodor argues there are few ways in which perception captures the world. In contrast, central processes are flexible and allow for a nearly unlimited amount of thoughts about the same object.
A third characteristic is that central processes (or at least conscious reports), have limited access to intermediate levels of inputs systems processing. That is, people have little or no conscious access and thus cannot report on how input systems might get the details of processing.
That is why people have no intuitions about how the brain treats visual images. In contrast, the final consequences of input processing are perfectly understandable and reportable.
Input systems are fast and faster than thoughtful processes. This is a strange phenomenon since the sort of computations required to process the perceptual stimuli seem to be complex but input systems get them done in fractions of seconds. As 
Fodor (1983, p.63)
 puts is: " 
[...]
 the puzzle about input analysis is precisely that the computational complexity of the problem to be solved doesn't seem to predict the difficulty of solving it; or, rather, if it does, the difference between a 'hard' problem and an 'easy' one is measured not in months but in milliseconds". In contrast, central processes can take months to decide on a chess problem.
A key feature of the Fodorian architecture is that input systems are informationally encapsulated. This means that there is a part of these systems which is blind to contextual information or knowledge information located elsewhere in cognition. Thus, there might be a module for color perception which does not take in suggestions from what subjects know about forests, that they are green. The famous case is the Muller-Lyer illusion, an illusion where two lines that are the same length look as if they are of different lengths. Measuring them and knowing that they are of different length does not help us see them as the same. 
Fodor (1983)
 argues some level of input analyzers are closed from other information in the system. He does concede that contextual information can bias perception, but he thinks this is because perception is not limited to input analyzers, so this bias can be applied by higher areas. Also, when it comes to priming language it 10 process theory alone, it does not imply that the mind has a dual structure explained by a distinction between predictive and classical computations. In other words, we will not try to fix Fodor's dual process theory, rather, we will try to help dual process theories of reasoning by giving them (what Fodor did not) an answer to the unity problem compatible with the realm of cognitive science.


11
CHAPTER ONE:


RESEARCH IN REASONING AND RATIONALITY
13 infer that working memory was successful, while if incorrect they can determine that working memory failed, and possibly why. This is the basic model of psychological research of cognitive processes. When it comes to reasoning and decision making however, determining which should be the correct answer for some task is not so simple. 
Following Samuels' et al. (2004)
 classification, the reasoning and rationality agenda includes three projects. The descriptive 7 project, in which scientists mean to discover how people actually reason and to describe the cognitive processes that underlie human reasoning as it is. In contrast to what is researched in logic, the descriptive project should have little interest in which are the most valid forms of reasoning, or how to optimize argumentation. Rather, the project means to discover exactly what is happening in one's mind when he reasons in everyday situations or in laboratory tasks. The normative project has more of a philosophical aspect as it means to determine what it is to be rational. The goal of such project is to discuss on what grounds we should base our judgments of reasoning or rationality, to specify principles, standards and norms that people might follow in reason. Given such norms and principles, the evaluative project aims to empirically determine, on groups of people, which individual is reasoning correctly, or which is rational. Using tasks such as the descriptive project does and basing judgments on a norm from the normative project, the evaluative project then is supposed to have the tools necessary to determine the rationality of individuals.
For a psychologist to determine if a reasoning strategy is correct or false in his laboratory it is assumed he needs some normative standard from which to evaluate the value of the answer.
There are various candidates for normative theories for rationality, but as we shall see, that is not an advantage, it is precisely the problem for evaluativism.
We argue that descriptive theories of reasoning need not worry about the evaluative project.
The core idea is that by looking at the empirical results themselves, without determining which is correct or false, one can determine how people do in fact reason. The steps of the evaluative project should only be subsequent to an established empirical theory of reasoning. They are not necessary, they are only complementary to researchers who want to know more, or to determine more, or to 14 make specific practical implications for their results. The normative project is necessary for planning tasks and as a possible guide as to how to read the results. As a guide it cannot determine which and how answers should be considered, only help indicate. Difference in task construal by subjects can also be a viewed as a result. If determining which individual gave right or wrong answers is unnecessary, then the need for a debate over difference in task construal is also unnecessary, as long as some people do give modal answers or some expected answer, then the fact that most did not give expected answers because of different task construal is a result that informs us about their cognitive mechanisms as well, not about the normative invalidity of tasks.


Subject interpretation of the task is already a result about cognitive mechanisms.
Depending on what normative principle is chosen, on a classic task, different responses can be analyzed as rational, and there are good philosophical positions backing these principles. The main difficulty for an evaluative project is determining which, out of many candidates, should be the correct normative theory. One can use a general account for all tasks or different accounts depending on which task is analyzed. Either way, there seems to be no solid reason for choosing a normative principle over the other. 
Elqayam and Evans (2011)
 call this the arbitration problem.
Some of the formal theories that are candidates for deriving normative principles are: classical logic, fuzzy logic, many values logic, other non-classical logic, decision theory, game theory, Bayesian probability, information theory or pragmatic accounts. Of course, some theories will not serve as norms for some tasks. But given that more than one theory can serve as a norm for one task, the arbitration problem seems to imply that there seems to be no simple way of determining which to choose. Furthermore, the answer to a task can be 'correct' or 'false' depending on which norm is applied. They are, therefore, essential if one needs an evaluative account of rationality.
After somehow deciding on which theory to base norms, one need also derive these norms in propositions that include 'should' or 'ought', since these formal theories by themselves are not normative principles. 
Stein (1996)
 for example, derives the conjunction principle from probability theory: One ought not to assign a lower degree of probability to the occurrence of event A than one does to the occurrence of A and some (distinct) event B. This is yet another difficulty, since as 
Samuels et al. (2004, p.37)
 note, "if normative principles of reasoning are not logically or probabilistically derivable from formal theories, then in what sense are they derivable?". Which implies: what license is one using to decide which norms to derive from the chosen formal theories?
One strategy could be trying to determine which formal theories should be used on the basis of empirical evidence. Since people can have effective behaviors, and since these behaviors correspond to a given formal theory, then such formal theory could be the one through which we should evaluate rationality. Also, one can try to determine the most rational and correct response based on the answers intelligent individuals give. However, as 
Elqayam and Evans (2011)
 point out, by doing this, researchers are subject to the naturalistic fallacy, as conceived by Hume and Moore. The empirical data allows one to form propositions about how the world is, but there is no reason to believe one could use these propositions to derive conclusions about how the world ought to be. The normative ground is just as the ethical ground, empirical evidence might give us directions on these grounds, but the final word is always a philosophical commitment. And of course, to decide which person is more intelligent you already need some sort of previous evaluation.
A neutral approach for interpretation of evidence, that analyzes the frequency of responses and individual differences, is what we endorse. Instead of trying to determine, as is done in other psychological research, which is the correct or false response, what needs to be done is to explain why responses are given, and which processes are responsible for such responses. Not all possible responses are given in a uniform fashion. Suppose, for example, that in a task there are ten combinations of possible responses. What we tend to see is two or three more frequent combinations of responses. The job of the descriptive theorist is only to explain why these frequent responses happen. The fact that they are frequent means that there is some type of reasoning process behind it, or else these would simply be random choices like the seven other infrequent combinations in the supposed task. One could also try and understand the infrequent responses, but first one would need to show that they are not just random variations. In such attempt, normative theories could help. If few individuals are answering according to normative principles across various tasks, then the psychologist should want to consider their answers more than he would consider random or other infrequent answers. There is no point (for the description) to try and determine one solution to a task as the best or correct choice, there is simply no descriptive gain for reasoning theories in doing so.
The usefulness in normativism for reasoning tasks is that it can help us outline the range of possible reasoning strategies our subjects could take without needing to evaluating them. The experimenter can contrast the answers in a given task with normative principles, as to help them understand what a subject was thinking during the execution of the task.
If rationality theories are used in a non-evaluative manner than the arbitration problem does not come as a burden. If we use these normative principle only to envision what subjects could be thinking by responding in a given way in a reasoning task than having multiple principles is actually positive. That is because none of these principles suggest the 'best way' to reason about the task at hand. They suggest possible ways to reason about a task and as so they serve as guides into understanding why a subject responded to the task in one or the other manner. It is important to be clear that we are not against evaluativism completely, we only stress that it cannot and should not be used in descriptive projects of human reasoning.


Classical reasoning, judgment and decision making tasks
The last half of the last century in reasoning, judgment and decision making research has been marked by a large list of puzzling experiments that started mainly with the works of Wason, Kahneman and Tversky. As examples, we will focus on three classic experiments and discuss their interpretations as well as some similar experiments which they have inspired: The Wason selection task; the Linda problem; and the base-rate Harvard Medical School problem.


Selection
The selection task is possibly the most famous experiment in this trend of reasoning research. In its original version, 
Wason (1966)
 presented students with four cards and told them that every card had a letter on one side and a number on the other side, and that either could be facing upwards. They were asked to decide which cards they would need to turn over to determine if the experimenter was lying 8 in uttering this statement: If a card has a vowel on one side, then it has an even number on the other side. The four cards were: A vowel (P), a consonant (~P), an even number (Q) and an odd number (~Q). Following classical logic, the only two cards that need to be chosen are the vowel (P) and the odd number (~Q), since only these two can logically determine if the statement is true or false, this is seen as the normative response. However, most of the students choose the vowel (P) and the even number (Q). Interestingly, if they can see the two sides of the cards, most students accord with the normative response.
This task has been replicated various times and the modal response is always P and Q, typically less than 10% chose P and ~Q 
(Stanovich & West, 1998a
). 
Wason's (1966)
 original interpretation of this was that subjects were irrationally choosing these cards because of a confirmation bias (which we will call a confirmation effect). A confirmation effect is a tendency to verify only the statement that one has in mind, instead of also trying to falsify it. When the evidence of falsification is ignored, one can believe the confirmation effect has occurred. However, 
Evans & Lynch (1973)
 have demonstrated that this does not always occur. They applied the selection task with a slight modification. Instead of having a positive consequent, the statement was presented with a negative consequent: "if there is a vowel on one side then there is not an even number on the other". If responses were following the confirmation effect, then the choices should be a vowel and a card which is not even. However, that does not occur, people choose cards that match the lexical content of the statement regardless of the negation, a vowel and the even card. This is evidence that in both cases (original task or negative modified task), they are not then trying to confirm the statement, but are rather responding to the lexical content itself. This was termed the matching bias by 
Evans & Lynch (1973
) (which we will refer to as the matching effect).
Notice that while this is a good start as to why people choose these cards in this task, it by itself does not yet describe which cognitive mechanisms are responsible for these responses. 
Evans (1996)
, provided interesting evidence for processing in selection tasks. He tested subjects with a computerized version of the task and instructed subjects to keep the pointer on the card which they were inspecting. The computer recorded the amount of time each card was inspected before the choice. The results showed that people spent most of their time inspecting the cards they choose.
They barely inspect the other non-selected cards. This is evidence that the cards which are not chosen are dismissed by tacit processes.
This original version of the task is called the abstract selection task. The results on it were quite puzzling by themselves; however, more similar tasks were conducted to further investigate interpretations to the selection task. These new results were not clarifying either. The most interesting modification to the task was the insertion of content, instead of using numbers and letters, a story was provided and the cards reflected actions the subjects should take. However, with the insertion of content to the selection task two distinct types of tasks were recognized. The indicative (or non-deontic) type and the deontic type. 
Cheng and Holyoak (1985)
 noticed that on the deontic type, a rule or regulation attempts to guide or govern action or behavior, it suggests what 'ought' or 'must' be done. On the other hand indicative conditionals have no rules to be obeyed or disobeyed, they indicate factual relationships that may be true of false.
The most famous deontic selection task problem was presented by 
Griggs and Cox (1982, p.415)
. It began with a story about the task: "imagine that you are a police officer on duty. It is your job to ensure that people conform to certain rules". The subjects were told the card presented information about people sitting at a table and that on one side of the card was a person's age and on the other side what they were drinking. The rule read "If a person is drinking beer, then the person must be over 19 years of age". So the subjects had to select the cards that they definitely needed to turn over in order to determine whether or not the people were violating the rule. The cards were: Drinking beer (P), Drinking coke (~P), 22 years of age (Q) and 16 years of age (~Q).
This was termed the 'drinking-age problem'. The results were very interesting, 73% of the participants chose the P and ~Q cards. On their control group, none chose the P and ~Q cards on the original abstract selection task.
This led researchers to believe that content was providing a thematic facilitation of the task, and that subjects were now getting it 'right'. However, subsequent research showed that people did not choose the P and ~Q cards in various other tasks with content. In 
Stanovich and West (1998a)
 four of these indicatives tasks did not induce the P and ~Q response: "If 'Baltimore' is on one side of the ticket, then 'plane' is on the other side of the ticket." (destination problem); "If it's a 'USA' postcard, then a '20c' stamp is on the other side of the postcard"; "Whenever the menu has 'fish' on one side, 'wine' is on the other side"; "Every coin with 'Madison' on one side has 'library' on the other side"; but many of these individuals (31.5%) chose the P and ~Q on the drinking-age problem even without the introductory story. When adding an introductory story to the destination problem and to the drinking-age problem, choices of P and ~Q were of only 8% on the destination problem but of 85% on the drinking-age problem. These results show that content is not enough to make people choose the P and ~Q cards, it seems other peculiarities, or some specific type of content, determine results. We will discuss the interpretations of these results throughout the rest of the chapter, but let us first review results on the Linda problem and the base-rate Harvard Medical School problem.


Conjunction
The Linda problem was first developed by 
Kahneman, Slovic and Tversky (1982, p.91
).
These authors analyzed three groups of students, a statistically naive group with undergraduates from the University of British Columbia and Stanford with no background in probability or statistics; an intermediate group with students of psychology, education and medicine from Stanford who had basic training in statistics and probability; and a sophisticated group with graduate students in decision science of Stanford who had all taken advanced courses in probability and statistics. The original task consisted of two characters, Bill and Linda, however, the trends of study following this original research focused mostly on Linda, so we will follow tradition. The test consisted of a description of a woman and some choices about her work and hobbies, the subject had to decide the probability that each choice matched the description.
"Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Please rank the following statements by their probability, using 1 for the most probable and 8 for the least probable.
( ) Linda is a teacher in elementary school. The description of Linda matched a stereotype of an active feminist (F) and not of a bank teller (T). But following formal probability, a conjunction of two events should not be more probable than any of the two individually. To affirm that one of these events individually is less probable than a conjunction of them has been termed the conjunction fallacy (but as 
Kahneman, Slovic and Tversky, 1982
 originally did, we will refer to it as the conjunction effect). However, that rule does not correspond to the choice of individuals. 89% of the naive group, 90% of the intermediate group and 85% of the sophisticated group assigned 'T & F' as more probable than T or F individually. In all groups together, 85% of the subjects assigned F > (T & F) > T. Other (naive) subjects were also asked to compare only the two choices, F and 'T & F', even so, the conjunction effect did not disappear, 92% of subjects chose the compound target. Some from the intermediate group were interviewed about the task, and the majority stated they had selected the compound target because of a similarity or a typicality, but agreed, after some reflection, with the normative probabilistic response.
An interesting altered replication of the Linda problem was performed by 
Fiedler (1988)
 in three experiments. In sum, he wanted to test for alterations of two types: a) Simpler tasks first: if the conjunction effect would persist if first there was exposure of cases where everyone would agree that the normative conjunction principle applies; and b) Frequencies: if the conjunction effect would persist if the task was presented in terms of frequencies, as opposed to probabilities.
For the first type (a) of alteration, 
Fiedler (1988, p.125)
 presented simpler tasks first, such as: "Which event is more frequent: A massive flood somewhere in North America in 1987, in which more than 1000 people drown or an earthquake in California sometime in 1987, causing a flood in which more than 1000 people drown?". The original Linda problem was the last task. On another experiment, conjunction principles with Venn diagrams were exposed before the original Linda problem was introduced. For this 'simpler task first' alteration there was no significant change, the conjunction effect was preserved.
For the second type (b) of alteration, 
Fiedler (1988)
 presented the Linda problem first, but this time subjects had to consider 100 people that fit the description of Linda and to decide how 21 many of them should fit the eight occupation and hobbies from the original task. The alteration was: There are 100 persons who fit the description above (that is, Linda's). How many of them are:
(followed by the original eight choices). In this case, the conjunction effect had a substantial decrease. Only 20% of subjects responded in a way that does not accord to the prescribed normativity, where in the original task this number varied from 70% to 80%. Thus, asking the question in frequency format has a very interesting effect. 
Fiedler (1988)
 further investigated what would happen if the number of 100 subjects who fit Linda's descriptions changed to 168, an amount that is more uncommon and unnatural in our everyday thinking. Even with this last alteration the conjunction effect had no significant impact.
The results seem to show that the difference in response comes not from the fact that the number 100 is easier to deal with, but rather from the exposure of the problem in terms of frequency.


Base-rate
The third set of classic experiments we will discuss has come in various forms, so we chose to highlight the one with the most impacting outcome. This test was administered by 
Casscells et al. (1978, p.999)
 to 60 people from Harvard Medical School. There were three groups of people, 20 were house officers, 20 were fourth-year medical students and 20 attending physicians. It consisted in answering a question about a simple statistics problem applied to a medical context:
"If a test to detect a disease whose prevalence is 1/1000 has a false positive rate of 5 per cent, what is the chance that a person found to have a positive result actually has the disease, assuming that you know nothing about the person's symptoms or signs?''.
The results showed that only 11 participants (18%) chose the statistically normative response, which is '2%'. These 11 were almost equally distributed among the three groups. The modal answer was '95%', which results from considering only the last probability information
given.
This is an example of the so-called base-rate neglect, since in this and various others studies 
(Kahneman et al., 1982;
Stanovich, 1998d)
, people seem to be avoiding the first (general) statistical 22 information in favor of the second (specific). It seems to be a difficulty of reconciling and uniting two separate probabilistic pieces of information.
These results were replicated by 
Cosmides and Tooby (1996)
 when applying the exact same question to 25 undergraduate students of Stanford. However, they also wanted to know if one could eliminate the base-rate neglect if the problem was presented with several possible facilitation effects. They reworded the test in the following way:
"1 out of every 1000 Americans has disease X. A test has been developed to detect when a person has disease X. Every time the test is given to a person who has the disease, the test comes out positive (i.e., the "true positive" rate is 100%). But sometimes the test also comes out positive when it is given to a person who is completely healthy. Specifically, out of every 1000 people who are perfectly healthy, 50 of them test positive for the disease (i.e., the "false positive" rate is 5%). Imagine that we have assembled a random sample of 1000 Americans. They were selected by a lottery. Those who conducted the lottery had no information about the health status of any of these people. Given the information above: on average, How many people who test positive for the disease will actually have the disease? out of___" 
(Cosmides and Tooby, 1996, p.24)
.
This version included various attempts of facilitation: a) it shows the true positive rate, not just the false positive; b) the base-rates are given in frequencies as well as percentages; c) it explains about false positives; d) it includes the information that the sample was random; e) it specifies the size of the sample; and f) it asks for the answer in a frequency format, rather than that of a probability of a single-event. The results showed a large increase in the amount of answers that accorded to the normative standard. In their replication of the original problem 12% of the analyzed Stanford students accorded to the normative principle, whereas in the average of various reworded versions 76% did. The various facilitation attempts were controlled for, and interestingly, the most significant effect was presenting the information in frequencies only (not along with percentages) and asking for the answer in terms of frequencies. With frequency-aimed modifications they managed make responses accord to normative principles in up to 92%. They also found that the other facilitation effects without the frequency format only mildly produced the normative answer (36%).
Together, the results in the selection task, the Linda problem and on base-rate neglect show that in many cases most people do not accord to normative principles in solving simple problems, but with simple rewording of the same problems, most performances do accord with normative 23 principles. For the rest of this chapter, we will attempt to cover some of the interpretations that have been given for these results, bearing in mind that agents are minimal and that 'ought' accounts should not figure in our construction of descriptive theories of reasoning.


Interpretations of results
From here on we will try to cover most of the famous interpretations for these results and results alike. They have been taken as implying facts about our general rationality. But more importantly we are interested in what these experiments tell us about the cognitive processes of human beings. The main interpretations have been presented by the heuristics and biases tradition, evolutionary psychology, pragmatics and dual process theories. We will explain shortly what each proposes and we agree with 
Samuels et al. (2002)
 that the historic quarrel between heuristics and biases tradition and evolutionary psychology is mostly a communication problem, rather than a real dispute. We will propose further that, once the evaluative use of normativism is gone, it is possible to see that the dispute over the extent of human rationality really hides an emerging consensus in most interpretations about descriptive theories of reasoning, and that such consensus points towards dual process theories 9 .


Heuristics and biases
The heuristics and bias tradition was mainly advocated by Daniel 
Kahneman and Amos Tversky. Kahneman et al. (1982)
 believed that the type of reasoning studies presented in the last section indicated that common people reason using basic heuristics that will sometimes get the needed results but will also make them prone to counter-normative responses. These heuristic principles reduce the complex tasks with which people are faced, and in general are quite useful, but since they process on data of limited validity, or because of this reduction done to the analysis of complex matters, they might lead to systematic errors, that we can understand as biases. 
Kahneman et al. (1982, pg.
3) exemplifies this by means of a comparison of these types of judgments with the way in which our perceptual system processes visual information.
"The apparent distance of an object is determined in part by its clarity. The more sharply the object is seen, the closer it appears to be. This rule has some validity, because in any given scene the more distant objects are seen less sharply than nearer objects. However, the reliance on this rule leads to systematic errors in the estimation of distance. Specifically, distances are often overestimated when visibility is poor because the contours of objects are blurred. On the other hand, distances are often underestimated when visibility is good because the objects are seen sharply. Thus, the reliance on clarity as an indication of distance leads to common biases. Such biases are also found in the intuitive judgment of probability." 
Piattelli-Palmarini (1994)
, an heuristics and biases theorist who follows a pessimistic view on rationality, illustrates the meaning of a bias with the example of a lead and a feather. We seem to have a heuristic that associates the weight of an object with the impact it can cause when colliding. We can assume that one would rather have light objects over heavy objects falling on their heads. However, if the objects are a heavy feather and a less heavy lead than this heuristic would not work, since we would prefer that the feather fell in our heads because of its relational properties with the wind. Because of the bias that this heuristic might lead us to, we would even have the incorrect intuition that the feather must be lighter than the lead when in this case it in fact is not.
One heuristic which is repeatedly mentioned by these theorists is Representativeness. When subjects are asked to determine the likelihood that a given person fits a certain characteristic, or the probability that an object A belongs to the class B, they use the representativeness heuristic. This heuristic process works by assuming that A bears resemblance to B given some stereotype. If X is a stereotype, and the person is given tokens A, B and C, they will use a representativeness heuristic to determine which of these tokens best fit X based on resemblance. 
Kahneman et al. (1982)
 believe that this heuristic will work for various cases but will lead to biases since they do not incorporate more rigorous laws of probability.
The interpretation to the Linda problem is based on the representativeness heuristic. They believe the probability calculus implies the conjunction rule and therefore the correct answer to the Linda problem must be to give a smaller probability to the single event (Linda is a bank teller) than to the compound event (Linda is a bank teller and an active in feminist movements). However, given that people make these judgments based on the representativeness heuristic, and do not 25 follow all laws of probability, their answers will be biased. Since the similarity of an object to a stereotype can increase with the addition of more characteristics shared with the stereotype, then people will judge compound events as more probable than single events.
Because of cases such as the Harvard Medical School problem, 
Piattelli-Palmarini (1994)
 argues that humans are probability blind and suggests a general psychological principle: "Any probabilistic intuition by anyone not specifically tutored in probability calculus has a greater than 50 percent chance of being wrong." (p.132). According to the base neglect reading, it happens that we have a heuristic that makes us consider the last shown base-rate as the most important one to be considered. The bias is, as shown by modal responses in these types of tasks, ignoring previously mentioned base-rates. 
Piattelli-Palmarini (1994)
 understands the modal response of the Wason selection task as an example of the confirmation effect. The bias is a consequence of having a confirmation heuristic, by which when people are convinced of a certain positive correlation they will attempt to find new confirmations of such correlation and shun evidence to the contrary. Although as mentioned earlier, the 'matching effect' seems to be a more favorable explanation and could be incorporated as a bias in their interpretation.
A stronger claim proponents of heuristics and biases tradition make is that given these various biases, the general rationality of human beings is inherently flawed. Piattelli-Palmarini (1994) believes these biases are cognitive illusions that human race is subjected to. In many cases we believe to be fully correct about the answer to a problem, however, our answers do not accord to normativity, so these are cases, in this view, where human beings clearly do not listen to reason, and such cases are natural and occur to everyone. In his book 'Inevitable Illusions' he sets out to try and teach us to cope with the "disorganized inventory of human nature" (p.2).
The general conclusion of the pessimistic heuristics and biases tradition is that the mind is composed of a number of simple heuristics which can work most of the time, but these experiments show how in fact our rationality is flawed, by showing how these heuristics are very prone to bias.
So the goal of the project is to map these heuristics as to help learn about how to avoid (or to help others take advantage of) such biases. 
Piattelli-Palmarini (1994)
 also responds to experiments that show how human reasoners can accord to normativity in altered conditions, such as the cases of certain content insertion in the Wason selection task and of describing probability problems in the form of frequencies rather than of percentages. He argues that those results change in nothing the fact that human rationality is inherently flawed, since in various situations we are prone to bias. He goes on to argue that since, of course, the heuristics are not flawed in all cases then the facilitation researchers are only discovering cases in which the said heuristics do work, while the biases are kept untouched in cases where heuristics do not work. The fact that heuristics do work in various situations is acknowledged by everyone including the pessimistic heuristics and bias tradition. 
Piattelli-Palmarini (1994, pg.192)
 argues that such a result " 
[…]
 in no way cancels, or reduces, the remarkable and worrisome fact that the illusions are present and vivid in other common situations […] previously tested [...]".


Evolutionary psychology
The main proponents of evolutionary psychology, Leda 
Cosmides and John Tooby, decided
 to rise up to the challenge of the claims that human irrationality is inherently flawed. Evolutionary psychology is a discipline in cognitive science that proposes a synthesis of such science with principles of evolutionary biology.
Evolutionary psychology (as summarized by 
Cosmides & Tooby, 2013
) is a framework that proposes concepts for comprehending human cognition and motivation and adds new methodology to the discipline of psychology. They understand the brain as an organ with specific functions, just as any other evolved organ, whose main job is to process information as means of generating adaptive behavior. The brain was built by natural selection, which, according to their story, means that the design of its functions is linked with the goal of increasing the benefit of survival and reproduction. Although they recognize that our brains share the same history as various animals, for higher cognition, these scientists emphasize hunter-gatherer societies when humans had to deal with various environmental difficulties in order to survive. That is because, supposedly, relatively new brain functions evolved in response to the repeating elements of the 'Environment of Evolutionary Adaptedness' (EEA) 
(Tooby and Cosmides, 1990)
 and reflect its structure. 
Tooby and Cosmides (1990, p.387
) define the EEA not as a place or period in history but as "[…] a statistical composite of the adaptation-relevant properties of the ancestral environments encountered by members of ancestral populations, weighted by their frequency and fitnessconsequences". Evolutionary psychologists 
(Cosmides and Tooby, 2013)
 inherit their ideas of the EEA from anthropology and biology and use evolutionary game theory as a method to discover adaptive information processing problems, as means of hypothesizing about what characteristics programs would need to have in order to solve them. Game theory is a method from economics for analyzing how agents behave in interactive situations. It models interactions between agents with defined decision rules that produce behavior. In evolutionary game theory decision rules that have successful outcomes will prevail in algorithmic selection. By looking at the consequences of various decision rules, one may hypothesize which is more likely to have been adopted in the EEA.
With this in hand they believe they can have relevant testable hypothesis.
This part of the method shows that solutions depend greatly on which specific adaptive problem is faced. Because of this diverse problem solving structure the EEA had and since according to evolutionary principles the brain, being an organ, should reflect this structure, evolutionary psychologists believe the mind is composed of a collection of domain-specific problem solving mechanisms 10 . 
Cosmides and Tooby (2001)
 explain that adaptive problems have two defining characteristics: They are conditions that various individuals faced constantly in human evolutionary history and are problems whose solution increased reproduction of individual organisms or their relatives (because of a gene-centered view of evolution, see 
Dawkins, 1976)
.
Some adaptive problems are computational. These can be defined as problems that can be solved by mechanisms that monitor some aspect of the environment and regulate the operation of other parts of the organism. 
Cosmides and Tooby (2001)
 argue that to solve the variety of different adaptive problems humans must have developed at least as many domain-specific mechanisms as there are adaptive domains for successful behavior that make a difference. Evolutionary psychologists hope to identify thousands of these domains, such as: parenting, food choice, mate choice, friendship maintenance, language acquisition, predator defense, sexual rivalry, status attainment and kin welfare 11 . 
Cosmides and Tooby (1994)
 intend that cognitive psychologists use the term function of a design in reference only to how it might have contributed to its own propagation in the EEA. They argue functions cannot refer to folk psychological ideas such as contributing to the individual's goals or to one´s well-being. Because natural selection is the designer of our brains, these other kinds of utilities exist as side-effects of other evolutionary functions. Such references of function cannot have explanatory utility as for why they have a specific organization.
According to 
Cosmides and Tooby (1994)
, domain-specific mechanisms are more likely to be selected in a natural process because of their intrinsic qualities. Since there is no need to have an engineered solution to competing tasks in the same mechanism, they display speed, reliability and efficiency. Different functions can be handled by different specialized mechanisms.
These mechanisms can have these intrinsic qualities also because they already know a great deal about the structure of their specific problems. In contrast to blank slate theories, evolutionary psychologists 
(Cosmides and Tooby, 2001)
 propose these mechanisms deal with problems already having solutions in advance. This would make them far more intelligent and efficient than systems that have no innate knowledge. As an example of evidence of such innate tendencies they refer to 
Johnson and Morton's (1991)
 research which shows how babies less than ten minutes old seem to have interest in face-like patterns but no interest in random versions of these patterns.
Evolutionary psychologists 
(Cosmides and Tooby 2013;
Pinker 1997)
 argue that an architecture of multiple domain-specific mechanisms is both more consistent with an evolutionary realistic description and also more efficient at problem solving than a general purpose mind would be. 
Cosmides and Tooby (2001)
 explain that, in engineering, the same device is rarely effective at solving two different problems equally well. A saw is suited for trees but not for the kitchen, and a kitchen knife is useless in a tree. Therefore, it seems to be a basic phenomenon that a single general solution to two different adaptive problems will be inferior to specific solutions to each. A general purpose system, they argue, must necessarily sacrifice efficiency. 
Cosmides and Tooby (1994)
 expose three main reasons for why a domain-general psychological architecture cannot guide behavior in ways that promote fitness. First, a problem is needed to define success, and adaptive problems have specific domains for success. So the first reason is that what counts as fit behavior differs from domain to domain, so there is no domaingeneral criterion of success or failure that could correlate with fitness. Second, general purpose mechanisms depend on perception and inference alone to reach goals. 
Cosmides and Tooby (1994)
 argue that adaptive problem solving cannot be deduced nor learned by general criteria, because they depend on statistical relationships between features of the environment, behavior, and fitness that emerge over many generations and are, therefore, not observable during a single lifetime. This is in line with 
Chomsky's (1975)
 poverty of stimulus argument which proposes our behaviors could not be accomplished if not accompanied by innate structures. Third, because they must evaluate all alternatives they can define, domain-general systems will run into combinatorial explosion when faced with real-world complexity. 12 
Pinker (1997)
 believes that concepts such as general intelligence and multipurpose learning strategies will become mere myths when scientists eventually map down all the domain-specific mechanisms that together were doing the actual work, making it seem like there was a general purpose system ready for use. 
Pinker (1997, pg. 27
) believes such concepts "will surely go the way of protoplasm in biology and of earth, air, fire and water in physics".
General machines are dysfunctional and improbable to have been bought up by evolution, evolutionary psychologists argue. They admit domain specificity has limits for explaining all intelligent behavior, such as using novelty in a particular situation. For such reasons they theorize over how these many domain specific mechanisms could have limited information sharing, some form of network, or a scope delimiting marker (named scope syntax, see 
Cosmides and Tooby 2001
) that would help on these problems. While they are careful enough to recognize such limits of domain-specificity, when it comes to explanation of phenomena, or understanding the evidence 30 in reasoning tasks, they nonetheless evoke domain specific only processes, these are the processes that they emphasize, and that will feature in any more specific theory for explaining specific evidence. Concepts such as the scope syntax are more of a framework level explanation that do not feature in real research predictions.
Given this framework, evolutionary psychologists attempt to reinterpret data and develop new data to show that in fact the pessimism of the heuristics and biases tradition is false, that the mind is actually composed of "elegant designed mechanisms" 
(Cosmides and Tooby, 1996, p.18)
 built to solve problems as they were faced in our evolutionary history. A fundamental claim 
(Cosmides, 1989)
 of this interpretation is that reasoning procedures utilized in a task will vary according to the domain of the task. What is called content-effect for other theorists is evidence for evolutionary psychologist of such procedure use variation.
One of these hypothesized content domain is social contract. Social exchange is a very important evolutionary capacity for humans. 
Cosmides (1989)
 hypothesizes that there are innate social contract rules that follow a specific structured logic. An individual must usually pay a cost to receive a benefit. All participants in an exchange should follow the same cost-benefit rules in order for this to be a naturally selected attribute. So humans need to be able to avoid exchanges in which the cost exceeds the benefit.
Using evolutionary game theoretical procedures, 
Cosmides (1985)
 found that for social exchange to evolve in a specie one must be able to detect individuals who cheat, or fail to follow these established rules. So she defines a social contract as a relation of perceived benefits to perceived costs, where one needs to meet certain requirements to others in order to receive a benefit from them. Cheating is receiving the benefit without having achieved the agreed cost.
Following the social contract structure on a Wason selection task, irrespective of logical category, a cheater detection procedure should follow these choices: Choose the 'cost not paid' card and the 'benefit accepted' card and ignore the 'cost paid' card and the 'benefit not accepted card'. The reason is clear, only the first two could help detect potential cheating. 
Cosmides (1989)
 analyzes two possible social contract rules that can be overlaid on the Wason selection task. On the standard social contract rule, participants need to investigate this rule structure: If you take the benefit, then you pay the cost. The other possibility is the switched social contract rule which reads: If you pay the cost, then you take the benefit.
On the standard rule, the supposedly correct social contract behavior answer matches the normative logical answer: P (benefit accepted), ~P (benefit not accepted), Q (cost paid), ~Q (cost not paid). So the social contract correct answers would also be P & ~Q, since only they would help on cheater detection. The switched rule does not match normativity: P (cost paid), ~P (cost not paid), Q (benefit accepted), ~Q (benefit not accepted). While the logical answer would still be P & ~Q, the social contract answer would have to be ~P & Q.
Evolutionary psychologists understand that on the traditional abstract Wason selection task, people do not respond according to normativity since they have no special purpose mechanisms to derive such formal logic conclusion; that is not how people were designed to reason. They understand the drinking-age problem as a case of a Wason task with a standard social contract rule.
That is, the rules for detecting the drinking cheater match exactly the normativity rule. Because we have the domain-specific cheating detection mechanism, the modal answers of subjects are P and ~Q on that task. Furthermore, they argue that not all content effect results in modal P and ~Q responses because not all content have this structure. For a content to induce the normative response, the domain-specific mechanism that deals with such content must have a structure that matches the abstract logical path. So they do not claim that social contract content boosts reasoning performance, rather they claim participants were just following a social rule that matches normativity by accident. They ground their position with evidence that in fact the modal response is not P & ~Q when following a switched social contract rule. On these cases participants favor the social contract response ~P & Q, rather than the normative one.
With that interpretation, 
Cosmides (1989)
 claims to have explained all previous research on the Wason selection task and to be able to predict when content will make participants match normativity.
Evolutionary psychologists 
(Cosmides and Tooby, 1996)
 claim also to have explained the data on the Linda problem and on base-rate neglect. The heuristics and biases conclusions were that people did not inherit a mental calculus of probability, and that they used heuristics (such as representativeness for the Linda problem) which often result in biased responses. 
Cosmides and Tooby (1996)
 argue that there is not just one calculus of probability but various, depending on the chosen normativity, and whereas it was shown that subjects do not follow one of them, that does not imply they could not follow another. 
Cosmides and Tooby (1996)
 explain how there are disagreements in the mathematical community about how to understand probability. As 
Cosmides and Tooby (1996)
 explain, Bayesians (the normativity used evaluatively by the heuristics and biases tradition) believe probability refers to a subjective degree of confidence that can be used to estimate the chances of a single event occurring. On the other hand, frequentists believe probability always refers to a history of events, so a single event, that is an event without history for comparison, cannot be attributed a probability. 
Gigerenzer (1991)
 argued that the human intuitive mind represents probabilities in frequentist terms. He believes some of our innate mechanisms make us good intuitive statisticians of the frequentist school. The idea that we have mechanisms designed for dealing with frequencies is called the frequentist hypothesis. 
Cosmides and Tooby (1996)
 argue that our ancestors in the EEA did not have familiarity with data collection and information accumulation stored in a numerical fashion as we do today. Therefore, since we relied only on individual experience and at best other's opinions, the probability of a single event was impossible to be noticed. What was observable was not the percentage of chance of success in a hunt, but rather the amount of times the previous hunts were successful. Organisms could not evolve a cognitive mechanism which receives as input information that was not observable.
However, our ancestors could keep count of how many times one has been successful in a given endeavor considering all the attempts. One could use the information that 5 out of 20 hunts were successful, for instance, for the forthcoming hunt. So if this type of inductive reasoning was an adaptation advantage it is probable that it would have evolved in a frequency format. 
Cosmides and Tooby (1996)
 argue that considering the probability of the next hunt as '25%'
is not efficient because it needs an extra conversion which also would lose three benefiting properties of frequency format. First, '5 out of 20' says more than '25%' because it includes the number of events that the statistical information is based on, and therefore includes a degree of reliability. Second, the frequency format includes the possibility of increasing the data with the addition of new observations, '5 out of 21'. And third, the frequency format is more flexible, it can easily adapt changes such as the specific success of each hunt, or the time when a few of them were executed in comparison to other ones 13 .
As we have shown in the previous section, reconstructing the Linda problem or base-rate neglect problems in frequency formats facilitate responses that accord to normativity. Because of that, 
Cosmides and Tooby (1996)
 argue that we have special-purpose mechanisms for reasoning with frequencies and therefore do have a probability innate device, but fail in percentages and other formats because the input does not match the mechanism's requirements.


Pragmatics
Much of the effort of pragmatists 
(Cohen, 1981;
Adler, 1984)
 studying these reasoning and decision making tasks have been of arguing that normative standards by which these tasks are measured are incorrect, therefore other theorists conclude wrongly that humans are irrational.
While we agree that nothing in these studies show human irrationality or rationality, difference in task construals do not disqualify the evidence, they actually point to further evidence for descriptive theories as we will argue further in this chapter.
Despite the focus on normativism, there have also been some interesting explanations for specific tasks proposed by pragmatists 
(Adler, 1984;
Sperber et al., 1995)
. These are based on Gricean principles, mainly the maxim of relevance and the cooperation principles. We will first explain some of 
Sperber and Wilson's (1986)
 relevance theory, which is a version of Gricean pragmatics, and then we will show how these ideas propose interesting interpretations for the tasks. 
Sperber and Wilson (1986)
 argue that on their model the important point of communication is that the audience perceives evidence of speakers intention through inferences. Through these inferences one can extract contextual assumptions called implicatures. These implicatures are based on the principle of cooperation which establishes that the communicator has the interest of communicating his intentions in such fashion so that he uses the best possible words. Associated with this principle is the maxim of relevance, which will be soon discussed. These implicatures are not communicated through codes but by the fact that the speaker provides evidence for his intentions. For example, a son asks "Dad, can we play outside today?", to which the father responds "It's raining". The father's words do not directly answer the son's question, but by following cooperation and relevance, the son is able to infer the proposition 'we cannot play outside today'.
The communicator through stimuli tries to make manifest to his audience a set of assumptions. These assumptions become manifest depending on strengths. The strength is the confidence by which certain assumption is held, and depends on its processing history. For example, assumptions based on clear perceptions seem stronger than assumptions whose truth depends on the trust on the speaker. When an assumption is useful in a given situation it is strengthened, and when it is not it is weakened.
The strength of an assumption affects all three types of contextual effects mentioned by 
Sperber and Wilson (1986)
. The first is the derivations of new assumptions by means of contextual implications. Contextual implications are deductions that necessarily depend on new information present in some context and on older information. With only one of these elements no conclusion can be drawn, but with both of them, new information can be extracted. The second one is the strengthening of older assumptions, by means of new evidence that does so. The third is the elimination of an older assumption in favor of newer assumptions that contradict the previous. 
Sperber and Wilson (1986)
 define relevance by its relation with contextual effects. For an assumption to be relevant in a given context, it must have contextual effects there. The strength of other assumptions in such context must be somehow affected. The irrelevant assumption might contribute with new information, but one that does not contribute to the present context. It might try to strengthen an assumption which is already held with certainty, and it might be too weak to affect other assumptions. They can however be relevant if they are part of a relevant behavior, such as changing the subject. 
Sperber and Wilson (1986)
 believe the effort in processing also influences relevance. The effort increases with the increase in use of mental processes. The greater the effort to cause a contextual effect, the lesser the relevance will be. The authors posit a spectrum with various points between the amount of contextual effects and the amount of effort required. On one end is the maximum contextual effect and minimum effort, and on the other, minimum contextual effect and maximum effort. Therefore, 
Sperber and Wilson (1986, p.125
) define two conditions: 1) "an assumption is relevant in a context to the extent that its contextual effects in this context are large";
2) "an assumption is relevant in a context to the extent that the effort required to process it in this context is small". 
Sperber and Wilson (1986, p.158)
 believe both the speaker and the listener to presuppose optimal relevance in communication. They define this presumption: 1) "The set of assumptions I which the communicator intends to make manifest to the addressee is relevant enough to make it worth the addressee's while to process the ostensive stimulus." 2) "The ostensive stimulus is the most relevant one the communicator could have used to communicate I". So 
Sperber and Wilson's (1986)
 principle of relevance is the thesis that every act of ostensive communication informs the presumption of its own optimal relevance. 
Sperber et al. (1995)
 have shown how relevance theory explains previous results on the selection task. They show how modal answering follows relevance judgments. They believe the choices of P and Q in selection tasks, which are modal, are more relevant than the choices of P and ~Q. In tasks where P and ~Q are the modal response, the focus of relevance is on such answers.
They explain what happens on the traditional abstract selection task based on the effort and effect side of relevance. On the effort side, the normative response to the task requires subjects to consider negations, while the P and Q response do not. Difficulty in negations have been previously demonstrated 
(Horn, 1989;
Wason, 1959)
. So P and ~Q can be overlooked simply because it is tougher to process but also because its effort will lead to a conclusion of irrelevance.
On the effect side, general conditional statements make subjects expect that given P, Q will occur, so that they should search for instances of Q, in the absence of evidence pointing in other direction. They will search for the content that has been communicated to them through the rule and follow the cooperative principle. The reading of the rule leads to the search of P and Q cases. This is also in line with the 'matching effect' explanation. 
Sperber et al. (1995)
, based on relevance theory, have managed to predict which selection tasks will make subjects mainly respond P and Q and which will make subjects respond P and ~Q.
They have, therefore, developed a recipe for elaborating 'easy' or 'hard' selection tasks. To build an easy selection task one must: 1) select a pair of simple features P and Q such that P and ~Q can be represented with little effort, or less effort than P and Q; 2) Create a context where knowing P and ~Q cases can have greater contextual implications than knowing P and Q. 3) Present the rule in a pragmatically felicitous manner, which basically means making it accessible, realistic and interesting. 
Sperber et al. (1995)
, showed through a series of experiments on four types of selection tasks that they could control for answers of P/Q and P/~Q. Let us present at least one, the virginmothers problem. Such problem was contrasted with results in the traditional tasks in a withinsubject setting. The introductory text reads:
"Until recently, it was obvious that a woman who has children has had sex. With artificial insemination, it is now possible for a virgin to have children. The leader of the Hare Mantra (a very secret religious, Californian sect) has been accused of having had some of his sect's virgin girls artificially inseminated. His goal, it is claimed, is to create an elite of "Virgin-Mothers" alleged to play a particular role in his religion. The head of the Hare Mantra makes a joke out of these allegations. He claims that the women of his sect are, without exception, like any other women: if a woman has a child, she has had sex. Imagine that you are a journalist and that you are preparing an article on the Hare Mantra. You learn that a gynecological survey has been carried out among the Hare Mantra women. Some of them might be "Virgin Mothers". You go and visit the doctor who carried out the gynecological survey. Unfortunately, the doctor pleads professional secrecy and refuses to tell you what he discovered. You realize that, before you, on the doctor's desk, there are four individual information cards about Hare Mantra women examined in the gynecological survey. However, these four cards are partially concealed by other papers (as shown below). Of two cards, one can only see the top where you can read whether the woman has children or not. Of the other two cards, you can only see the bottom where you see whether the woman has had sex or not. You are determined to take advantage of a moment in which the doctor turns his back to uncover the papers and to learn more. Indicate (by circling them) those cards that you should uncover in order to find out whether what the leader of the Hare Mantra says ("if a woman has a child, she has had sex") is true, as far as these four women are concerned, indicate only those cards that it would be absolutely necessary to uncover." 
(SPERBER et al, 1995, p.63)
.
The choices of card were: Children -yes (P), Children -no (~P), Sex -yes (Q), Sex -no (~Q). Since the focus of relevance was on women did not have sex and who had children, the choices of P and ~Q were made by 78% of the subjects. While only 26% of the same subjects on the abstract version of the task chose P and ~Q.
By finding various facilitation effects based on relevance, 
Sperber et al. (1995)
 forces evolutionary psychologists to find Darwinian algorithms for each of these 'easy' selection task they can formulate. This scenario would make their case rather implausible. However, this does not completely exclude the possibility that there could be both relevance effects and Darwinian algorithms governing task performance 
(Cosmides and Tooby, 2000)
. 
Adler (1994)
 also tries to reinterpret tasks by means of pragmatics, basically by analyzing how experimenters cue for relevance, how subjects expect them to be informative and to engage in communicative cooperation. He focuses on the Linda problem and a base-rate problem. The baserate problem is the engineers and lawyers problem from 
Kahneman et al. (1982)
, which is much like the Harvard Medical School problem, but, like the Linda problem, is based on the description of a character and the chance that such character fits certain characteristics.
To understand these tasks, 
Adler (1994)
 proposes that when one engages in abstraction she loses sight of relevant properties that might be in a context. He argues that abstractions tend to be uncooperative, since there always will be a more pragmatic reading to a communication event than that of taking essential properties and ignoring others. He argues that in these tasks, to respond according to normativity, subjects must reject as irrelevant portions of the actual contribution that normally appear appropriate, so that abstraction sacrifices some of the informativeness of their contribution.
For instance, the description and the task of rating the probability of Linda having each property has to be ignored in order to give the normative response that it is less likely that Linda is a bank teller and a feminist. Because of that, 
Adler (1994)
 suggests that the actual task (respecting conjunction principles) is really hidden in the background, and subjects will follow what seems to be expected of them by means of relevance and cooperation, that is that they think of Linda as a feminist. 
Samuels et al. (2004)
 argue that "Linda is a bank teller who is not a feminist" has been controlled for, and hence there seem to be no linguistic factors of relevance involved. But Adler (1994) means to point out also that subjects are following what seems to be the exposed, rather than the hidden, interest of the experiments. If they were to abstract away from the exposed interest they would violate the cooperation principle.
The same type of explanation is given for the engineers and lawyers base-rate problem. To be cooperative subjects focus on the description of the character rather than dedicating their effort at calculating probabilities by means of base-rates. 
Adler (1994)
 argues that the description of the character is the experimenter's most salient contribution, while the base-rate data are part of the background information. However, there is evidence these normative difficulties are not only because of an attempt to read the experimenter's focus. Since on the Harvard Medical School problem, the information is given in a relevant and direct manner to the physicians. Also, Adler's distinction between abstraction and relevance does not explain why frequencies facilitate normative responding, as 
Samuels et al (2004)
 already noted, and they facilitate both in the conjunction problems and base-rate problems. If 
Adler's (1994)
 reading was the complete story, relevant information should also inhibit normative responding in frequency tests, but that is not the case.
Anyhow, 
Adler (1994)
 seems to have a noble goal in mind, which is to argue that domainindependent abstractions are not always the ideal against which all cognitive success should be measured by. That the relative costs and benefits between being guided by relevance or abstracting away need to be taken in consideration. In a way he is also arguing against making an evaluative use of normativism for reasoning tasks. On the other hand, 
Sperber and Wilson (1995)
 really make a distinctive contribution, from the pragmatics camp, to descriptive theories of reasoning.


The hidden emerging consensus
The initial accepted understanding of the field was that the heuristics and biases and the evolutionary psychology ideas were really at odds with one another. However, 
Samuels et al. (2002)
 point out that behind the collection of rhetorical excesses of both traditions, a hidden consensus based on their real core claims was emerging. They argue that once one eliminates such rhetorical excesses on both sides, the challenge of evolutionary psychology is really no challenge at all. Moreover, that the claims of evolutionary psychology could not make sense if not by endorsing the core claims of heuristics and biases tradition. An example of a rhetorical excess of the heuristics and biases tradition is claiming our species is probability-blind. Whereas an excess by the evolutionary psychologists would be claiming that we have 'elegant mechanisms' designed to solve our problems.
Possibly one of the main reasons for the apparent dispute has emerged from the different focus of theses research projects. While the heuristics and biases tradition has focused on finding experiments in which subjects do not accord to normativity and on explaining these 'failures', evolutionary psychology has been concerned in showing the reasons behind our 'success' in various tasks.
Therefore, perhaps because of methodology, the heuristics and biases tradition were showing cases where our mechanisms do fail to accord to normativity. Evolutionary psychology must agree that these mechanisms do fail, in order to propose cases where they do not fail. Both traditions agree that we have mental processes built by evolution to solve adaptation problems and that when facing the problems of today, these mechanisms might fail in various tasks but succeed in many others.
For instance, because of the evidence, heuristics and biases agree that frequency format or specific content will result in the success of participants because that is when these mechanisms are truly working. On the other hand, evolutionary psychology will also contend that in various cases people do not accord to normativity because the mechanisms are not designed to deal with such. It is important to notice that in both cases the total system will yield many mistakes although it will also result in correct answers.
The heuristics and biases tradition also does seem to contend that the mind is composed of a group of different specialized mechanisms. 
Kahneman et al. (1982, p.88)
 compare problem solving "with the operation of a flexible computer program that incorporates a variety of potentially useful subroutines". They also argue that "the actual reasoning process is schema-bound or contentbound so that different operations or inferential rules are available in different contexts", and that "consequently, human reasoning cannot be adequately described in terms of content-independent formal rules." 
(Kahneman and Tversky 1982, p.499
).
The same can be found in the other way around, evolutionary psychologists 
(Cosmides and Tooby, 2001, p. 162
) can also be found endorsing heuristics and biases claims: "A male robin red breast may not look particularly intelligent when overcoming obstacles to attack a tuft of red features; nor does a human male when he spends time looking for pornographic pictures rather than courting actual women […] These mechanisms lead to such odd outcomes because there are things in the world other than rival robins and living women that satisfy the input conditions for the monitoring devices employed by the computational systems that (respectively) regulate aggression in robins and courtship in humans". These apparent differences come because of making an evaluative use of normativism, not because of major differences in descriptive theories. Piattelli-Palmarini (1994) even admits a subscription to the modularity of mind for reasoning tasks, but argues (p.2) that: "This cognitive unconscious is a trait that 
[...]
 may have saved our ancient ancestors from wild beasts and famine, but even granted that such a simpleminded Darwinism should have actually been at work in their creation, for a very long time these illusions have been no more than a burden. Darwinism or not, we should all learn how to protect ourselves, individually and collectively, from the effects of our cognitive subconscious".
Perhaps not so surprisingly, of the three points 
Samuels et al. (2002)
 point out as the real disputes between these traditions, two are problems that stem from making an evaluative use of normativity.
The first is how to best apply probability in given tasks, which norm should be used in evaluating responses in specific probability tasks is not agreed on. However, if both agree that in some cases people fail to reason according to some norms and in others people do succeed, what difference does determining which is the best norm make for descriptive psychology? The answers to the problems stay the same regardless of the way you choose to evaluate correctness. Multiple normative standards can be used to understand what subjects were thinking.
The second problem noted in using normativism in an evaluative fashion is even further away from the realms of descriptive theories, it is a real dispute about which the correct interpretation of probability theory is, a Bayesian version or a frequentist version. While this is a real dispute, it is not for psychology to determine.
The third is a real descriptive theory incompatibility that points to others. Evolutionary psychology argues that their opponents propose limited explanations, they believe a better account of specific reasoning mechanisms need to be described, rather than just heuristics like representativeness or availability. In fact this seems like a good critique, but perhaps the main characteristic of heuristics and biases was in identifying the biases, not the reasons why.
However, we believe this leads to some further different incompatibilities that add to what 
Samuels et al. (2002)
 pointed out. For instance, it is not convincing that proponents of heuristics and biases would buy the story of social contract theory. While heuristics and biases and evolutionary psychology as general level frameworks do not differ too much, many differences might lie in specific theories for explaining specific phenomenon. However, these are differences that theorists in the same general framework tend to encounter and dispute over. It is a type of difference that Cosmides and Tooby could have with other evolutionary psychologists, not a core difference between traditions.
It is also important to note that even though Piattelli-Palmarini did claim these heuristics were designed by natural selection, he was a critic of adaptationism. Recently, he 
(Fodor and
 Piattelli-Palmarini, 2010) even attacked the idea of natural selection directly. However, this should not count as differences from the two reasoning traditions since these recent ideas are not taken very seriously and have very few supporters, they are not tied the heuristics and biases tradition.
This does not however, change the fact that as 
Samuels et al. (2002)
 pointed out, there is an emerging consensus on what these reasoning, judgment and decision making tasks tell us, namely that the mind is, in great part, sensitive to differences in type of input and that themes relevant to adaptation and social practice are favored over normativity. Mind mechanisms are efficient in dealing with what they are trained to work on, but might be responsible for unexpected answers in some tasks which are not in the form which they were designed to deal with. For these reasons, sometimes modal answers will accord to the expected normativity, while in others, other problem structures will dominate their processing choices. This consensus also includes the pragmatist collaborations we mentioned.
However, even granted that such emerging consensus is real, it seems some descriptive part is still missing, namely the cognitive mechanisms behind the answers that accord to normativity.
Although modal responses to the original Wason selection task, original Linda problem and original base-rate problems do not accord to normativity, what are the cognitive mechanisms that explain why at least some people do respond in accordance with normativity?
We believe that the next interpretation, given by dual process theories is a version of this emerging consensus, which synthesizes what each previous tradition best showed and also provides an explanation for the mechanisms behind normative responses.


Dual process theories
As we have previously argued, making a non-evaluative use of rationality can be an important part of interpretation of reasoning tasks because it offers the theorist a range of behaviors and reasoning steps that could be expected from the participant on a task. Evolutionary psychology focuses on ecological rationality psychologists and it is expected that individuals will answer accordingly. However, as is consensus in all camps, what is evolutionary valid might not be the best choice in the modern world (See 
Stanovich, 1999
Stanovich, , 2004
Tooby, 2001, Piattelli-Palmarini, 1994)
. Choosing fat food over non-fat food was a good strategy for the hunter life, but not for our modern life. Evolutionary psychologists do notice this but they do not develop theories to account for it. While normative rationality cannot be used to solve all our problems, for instance, since it would be too irrelevant for evolutionary situations, using normative rationality might be important when making a life-long economic decision for instance. Now if we expect both ecological rationality and normative rationality from the participants in a task, it is plausible that they also might have different task construals. However, as previously mentioned, difference in task construal is not only a normative matter, it tells us about mechanisms people are using in reasoning. 
Stanovich (1999)
 noticed that to have different task construals there might be different mechanisms which provide these construals. That is why we must take all modal answers and all consistent answers (by the same individual through various tasks) as evidence for reasoning processes, irrespectively of if any given task construal is the preferable according to the theorist's evaluative use of normativism.
The best way dual process theories can account for the evidence on reasoning is by considering all modal answers and consistent answers (by the same individual through various tasks) as answers that tell us about people's underlying cognitive mechanism. So, on the original Wason task, there seems to be a pragmatic choice (P & Q), and there is also a normative choice (P & ~Q). On the Linda problem, there is an ecological or pragmatic response (Linda as bank teller and feminist activist is more probable), and also a normative choice (Linda as a bank teller alone is more probable). On the Harvard Medical School problem we have a heuristic response (that ignores prior probability) and we have a normative response (that considers both probabilistic information). As 
Stanovich (1999)
 noticed, these different possibilities of task construal hint at the mechanisms that support them. In general, in these types of tasks studied, we seem to have either an intuitive response or a normative response. In the cases of facilitation, the task fails to show us the conflict between these two different types of responses. So in such cases, what happens is that evolutionary or pragmatic rationality and normative rationality coincide in the same answer; the answer is the same, irrespective of which one an individual follows.
One could need different concepts of rationality to explain modal answers or consistent answers (by the same individual through various tasks), that do not fit any of these rationality concepts. However, it seems this has not been the case in the history of these studies. It must also be acknowledged, at least for descriptive reasons, that we have no need to consider evolutionary or pragmatic rationality or normative rationality as the optimal choice for any given task. Why should it matter, for descriptive reasons, if people should not have to consider the probability of single events, as 
Cosmides and Tooby (1996)
 argue, if at least some can and do attempt to think in such a way?
Of course, the number of individuals that give normative responses in these types of task is low. That is the main reason why these individuals have been ignored, and not considered part of the evidence a theory should explain. However, through individual differences 
Stanovich and West (1998a
, 1998b
, 1998c
, 1998d
Frederick 2005 and
also Toplak et al., 2011)
 show that normative responses are given not only as a random choice but as a normative choice, because they are given by the same individuals (who also score high at analytic thinking) across various tasks. 
Stanovich and West (1998a)
 tested participants in various versions of the Wason selection task, in three studies. Some of these tasks were non-deontic (normally associated with nonnormative responding) and one of them was the drinking-age problem (a deontic problem in which modal answers are linked to normative responding). They found that individuals that respond normatively on one non-deontic task tend to do so across the other tasks. Also, individuals tested with high cognitive ability tend to respond normatively across tasks. Interestingly, the individuals that did not answer normatively in the drinking-age problem had worse results in cognitive ability testing. They had, also, similar cognitive ability results to participants that displayed normative answers on the drinking-age problem but did not on any other. Those who exhibited the matching effect on other tasks also had worse results in cognitive ability testing. 
Stanovich and West (1998b)
 replicated the original Linda problem, and 80.7% of their sample displayed the conjunction effect. However, the 29 subjects who responded normatively had also tested higher on cognitive ability. The correlation between cognitive ability and normatively responding had a large effect size. 
Stanovich and West (1998d)
 got mixed results on the importance of cognitive ability in base-rate. When studying base-rates applied in a selection task, cognitive ability did not predict normative answers (considering base-rate as relevant). However, individuals who did not include prior probabilities in two tasks (David problem and the Mark problem) which are similar to the original base-rate problems (the engineers and lawyers problem in 
Kahneman et al.,1982 and
 Harvard Medical School problem) had lower cognitive ability than individuals responding normatively. variance but a rare (but existent), justified response, which should serve as evidence for descriptive theories of reasoning. Thus, this use of cognitive ability is to determine that some people are answering normatively consistently (in contrast to a random choosing), not to determine that therefore the normative answer is more appropriate. Even more interesting was the correlation of .49 (that is, moderate to large) of cognitive reflection task with syllogistic and heuristics and biases tasks. Although the cognitive reflection task also correlates with cognitive ability and working memory, regression analysis showed that the cognitive reflection task was a unique predictor of normative responding. This is interesting for the Dual Process Theory (DPT) interpretation for the reasons will we argue for.
The cognitive reflection task was developed by 
Frederick (2005)
 as a task that would tap an intuitive answer that came quickly to mind, but one which is normatively awkward. While we 45 seem to have intuitive answers for some simple problems, there are others for which we have no intuition at all, such as the square root of 1897. To figure out the correct answer to the cognitive reflection task subjects would need to think a while longer. The task is very simple, it consists of three simple questions with relatively easy solutions, but to which our intuitive responses do not correspond. The introduction to the task (which would come along with other tasks) read:
"Below are several problems that vary in difficulty. Try to answer as many as you can": "(1) A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? _____ cents (2) If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets? _____ minutes (3) In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half of the lake? _____ days" 
(FREDERICK, 2005, p.26)
 The intuitive answers are 10 cents to the first problem, 100 widgets to the second and 24 days to the third. However, the careful solution of the problems easily shows that the answers are 5 cents, 5 minutes and 47 days. An amazing virtue of this task is that there can be no dispute over normativity and rationality, any researcher must agree that the task is posed as basic mathematics.
This lessens the possibility of divergent interpretations by reducing the rationality standards that we suspect the subject to be using.
It is possible to verify that there is an intuitive answer and a careful answer because of a few factors. An intuitive answer is one which comes to people without much effort and tends to be modal, and in fact 10 cents, 100 widgets and 24 days are highly modal answers. Even those who respond normatively report having first reached the intuitive answers, this is confirmed by verbal report, notes taken while solving the problem and introspection. People who responded 10, 100 and 24 also believed that the task was easier than those who gave the normative answer, the former estimated that more than 90% would get the correct answer, while the latter estimated that only 62% would (both were overestimates). Also, similar tasks that do not tap intuitive answers forces people to try and respond normatively, there is no effortless and modal awkward response in these cases.
These differences between an intuitive quick responding and normative slow responding have been shown from all these reasoning, judgment and decision making tasks throughout the years. Because of that, theorists have been gradually developing dual process theories, theories 46 which posit a difference in two types of processing: an intuitive implicit form of processing and a thought-out explicit form. However, these theories come in various forms 
(Chaiken, 1980;
Evans And Over, 1996;
Fodor, 1983;
Nisbett et al., 2001;
Kahneman, 2011;
Sloman, 1996;
Stanovich, 2004
). In the next chapter we will discuss if we can use a general DPT or if a specific one is always needed. Since we have not yet discussed this matter, for now we will just use a generic version. 
Evans (2008)
 reviewed several of these theories, even from different areas of research (like social cognition and evolution) to point out a common list of features that are usually ascribed to these two processing types.
According to this multi-theoretical cluster of attributes for each type of processing, the correlational features for Type 1 processes are: unconscious, implicit, automatic, low effort, rapid, high capacity, default, holistic, perceptual, evolutionary old, follows evolutionary rationality, shared with animals, nonverbal, modular, associative, domain-specific, contextualized, pragmatic, parallel, stereotypical, independent of general intelligence, independent of working memory. The correlational features for Type 2 processes are: conscious, explicit, controlled, high effort, slow, low capacity, inhibitory, analytic, reflective, evolutionarily recent, follows individual rationality, uniquely human, linked to language, fluid intelligence, rule based, domain general, abstract, logical, sequential, egalitarian, heritable, linked to general intelligence, limited by working memory capacity. 
Evans (2008)
 noted that positing all of these features as defining characteristics of these types of processing is troublesome, because these characteristics will not always stand. In the next chapter we will examine all of these theoretical details of dual process theories. Therefore, for now, let us consider a generic DPT with correlational features for each type, but with one fundamental difference: that Type 2 processes will aim at normative responding and Type 1 will aim at intuitive responses (be it evolutionary, heuristic or pragmatic).
This generic DPT can account for the hidden emerging consensus and for the individual differences literature. DPT accounts for the emerging consensus proposing that the mind is mostly composed of Type 1 processes which have helped humans respond to adaptive problems but that might find difficulties in the modern world. These rapid, contextually fine-tuned processes are efficient in dealing with their input, but might be prone to mistakes in tasks with different designs.
For these reasons, when Type 1 processing is well suited, modal answers will accord to what 47 theorists expect the normative answer to be. However when these Type 1 processes are not able to solve a given task, Type 2 processes can override and aim at a normative answer. It goes further than the hidden consensus in explaining the evidence, because it also succeeds in explaining the cognitive mechanisms that govern choices that individuals who respond with normative answers.
It is predicted that people with higher general intelligence, who are skilled at analytic intelligence, and who can use cognitive reflection will tend to override intuitive answers to aim at normative answer by using Type 2 processes. This does not mean that Type 1 processes will always be responsible for what the heuristics and biases tradition calls biases, Type 2 processes can result in non normative answers as well, but they will differ from Type 1 responses, in that they are not modal, intuitive, and are usually mistakes one would encounter when aiming at normative responding (such as making a calculation mistake in the widgets answer on the CRT).
We believe the CRT makes a pretty good case for DPT because in it, it seems clear that there are two types of possible responses, one intuitive Type 1 response and the other slow and reflective Type 2 response, the normative and intuitive answers are also easily agreed upon. The best part of this, for the DPT is that individuals who are able to give Type 2 responses in the CRT will tend to give what theorists believe to be Type 2 responses across several reasoning and heuristics and biases tasks. And individuals who respond with the intuitive Type 1 mechanisms to the CRT will tend to respond accordingly on the heuristics and biases task. 
Frederick (2005)
 and 
Toplak et al. (2011)
 argue that the CRT can therefore be seen as a task that measures the tendencies of individuals to override Type 1 answers and engage in Type 2 processing.
One could ask why different types of rationalities, such as pragmatic and evolutionary rationality, should not also be evidence of different types of processing themselves or how we one can know that they are not actually part of Type 2 processing. There are two ways to determine how a rationality aim will be fitting each type of processing. The first is by how they relate to correlational features of each Type of processing. So evolutionary, pragmatic and heuristics strategies all tend to have various Type 1 correlational characteristics, such as: being unconscious, intuitive, implicit, automatic, low effort, rapid, default, evolutionary old, modular, associative, domain-specific, contextualized, stereotypical, independent of general intelligence, independent of working memory. In that sense we can group those strategies as Type 1 strategies. The second way 48 is by investigating individual differences. One could suspect one of these three rationality strategies or other possible imaginable ones to be the focus of Type 2 processing if individuals who do well on the CRT tend to prefer them across various other tasks. This is a general framework that neatly suggests differences in cognitive mechanisms exposed by the evidence (usually described as puzzling) in reasoning, judgment and decision making literature. As we have seen in this chapter, this can simply be done without the need to appeal to the use of evaluative normativism. We are not arguing that dual process theorists have been free of an evaluative use of rationality all these years, only that their framework can provide the most complete account of these tasks without making use of evaluative normativism.
Our claim that DPT can be seen as the result of the hidden emerging consensus is also backed by the fact that main theorist from the heuristics and biases tradition has admitted to have something like that in mind before and have recently proposed a DPT 
(Kahneman, 2011)
. Further, Sperber, the main advocate of pragmatics involved in the discussions of reasoning has also subscribed to a similar view (see 
Mercier and Sperber, 2009)
, it is a view that is continuously growing.
We can also point out how the evidence has been suggesting DPT throughout the years.
Evans 
' (1996)
 computerized version of the selection task hints at DPT by showing how modal Type 1 responses are quick and intuitive, since subjects tend not to evaluate other cards, they spend time evaluating the cards they have quickly assured to be the relevant ones. If there are Type 2 processes involved in this type of answering, it is only after Type 1 responses have already determined relevant cards.
The Type 2 assumption for normative responding is supported by the empirically demonstrated connection between cognitive ability and the tendency to engage in analytic processing 
(Ackerman & Heggestad, 1997;
Cacioppo et al., 1996)
. 
Frederick (2005)
 also found that subjects that do well on the CRT, that therefore have tendencies to engage in Type 2 processes, tend to make more patient choices and to prefer receiving more money by waiting than less by receiving readily accessible money, when short periods are analyzed. They will also be more prone to engage in possible risk taking in economic decisions. little or no direct introspective access to higher order cognitive processes as a result that shows conflict between Type 1 processes and Type 2 theorizing about them. They also point out an association between using Type 2 processes for problem solving and tiring out the individual, which shows its correlation with need for working memory and concentration. Also, there is a correlation of normative responding with thinking aloud, using storing strategies such as offloading, and use of verbal memory. Now, let us use this generic DPT framework to account for the three major tasks in this chapter. On the Wason selection task individuals will tend to use Type 1 processes which will intuitively call for the pragmatic matching effect (answering P & Q). Some individuals however, will aim for a normative response that will take more effort and working memory use, by means of Type 2 processes (Answering P & ~Q). In the cases of content facilitation Type 1 and Type 2 processes will both end up in the same response, therefore there will be no way of distinguishing the two, that is why there will always be a very high modal answer on these tasks and no individual difference will account for variation responses.
On the Linda conjunction problem, by using Type 1 processing people will violate the normative conjunction rule by preferring a description that fits more with contextual knowledge (Linda is a bank teller and is active in the feminist movement). Individuals who tend to use Type 2 processes will prefer a normative response that does not violate the conjunction rule. Again, facilitations will make the conflict between these two type of process disappear. Stephen Jay 
Gould (1991, p.469
) had a revealing intuition on this: "I know the [conjunction] is least probable, yet a little homunculus in my head continues to jump up and down, shouting at me-'but she can't be a bank teller; read the description'."
On the Harvard Medical School problem quick and effortless Type 1 processes will make participants ignore base-rate, while Type 2 processes will require that they work out all the possibilities by using working memory and re-thinking first impressions, which will aim at normative responding. Again, facilitations in forms of frequencies will make the conflict between these two type of process disappear.


50
DPT summarizes the hidden emerging consensus because it can account for changes in specific explanations. For instance, what can be behind an intuitive answer can be evolutionary algorithms as proposed by the evolutionary psychologists, the use of heuristics, or relevance effects as suggested by 
Sperber and Wilson (1986)
, all of these fit with the correlational features of Type 1 processes. Whichever one of these specific theories tend to be right, they will also point to Type 1 processes and therefore to DPT, they can also all be right and apply to the explanation of different tasks. For instance, DPT will still hold if evolutionary psychologists are correct about evolutionary processes for dealing with frequencies and pragmatists are correct about a relevance factor for choosing the P & Q cards in the Wason selection task. It is important to note again that these other proposals do not account well for the cognitive mechanisms behind normative responding across tasks, so these disputes are only over exactly which intuitive Type 1 processing is behind modal answer in given tasks.
It is possible that evolutionary psychologists will not accord to the Type 2 explanations since it evokes a general-type process. However, 
Cosmides and Tooby (1994, p.94)
 do indicate that there might be a few of these general purpose processes if dependable on domain-specific ones:
"In short, although some mechanisms in the cognitive architecture may be domain-general, these could not have produced fit behavior under Pleistocene conditions (and therefore could not have been selected for) unless they were embedded in a constellation of specialized mechanisms that have domain specific procedures, or operate over domain-specific representations, or both." Perhaps, this could, again, be a difference of focus of research, rather than that of different cognitive theories. Perhaps difficulted by rhetoric excesses such as 
(Cosmides and Tooby 1994, p.90)
: "it is in principle impossible for a human psychology that contained nothing but domaingeneral mechanisms to have evolved, because such a system cannot consistently behave adaptively", since no one today would argue for a cognitive architecture with "nothing but domaingeneral mechanisms". 
Stanovich (2004)
 criticize evolutionary psychology mainly in three points: 1) that they do not account for how a hunter-gatherer brain can deal with scientific, social, technological advancements; 2) they do not explain the evidence for and the role of general intelligence in their 51 model; 3) they downplay individual differences, thinking styles and personality. Also evolutionary psychologists have not managed to dismiss the evidence for DPT in individual differences or give a plausible response for how normative responses in the original selection task or the first Linda problem are possible in their account. It is possible these two traditions still have core disagreements about the architecture of mind and how reasoning occurs. Although evolutionary psychologists might not agree, we see DPT as one that can accommodate results and evolutionary mechanisms proposed by evolutionary psychologists and go further on to explain how modern thinking is possible. Little of modern problems are linked in any direct sense to problems of the EEA, and yet we solve them efficiently, perhaps not all of us, but at least some of us, with the same human brain.
In fact, while trying to explain how relevance effects and their Darwinian algorithms might both exist in determining selection task responses, 
Cosmides and Tooby (2000)
 propose the principle of pre-emptive specificity, which states that human cognitive architecture will try to answer problems first with domain-specific mechanisms and if not successful more general ones will be called in. This seems to be in line with the emerging consensus towards DPT. As we have seen evolutionary psychology argues that general systems would not be naturally selected for. A dual process theorist might also agree which such claim, and propose, as 
Stanovich (2004)
 has, following 
Dennett (1991)
, that these mechanisms became possible by means of cultural evolution, by the emergence of memes or by the forces of the Baldwin Effect (see 
Baldwin, 1896;
Dennett, 1991
).


Concluding chapter remarks
We started this chapter endorsing a methodology for interpreting results in tasks of reasoning, judgment and decision making without making an evaluative use of normativism. We have seen how disputes about the boundaries of human rationality have been confused with disputes about the mechanisms of reasoning that humans use in these tasks. Such confusion hid an emerging consensus on the evidence of these tasks to descriptive theories of reasoning. This emerging consensus in short states that there are usually two types of possible responding to 52 reasoning, judgment and decision making tasks, that is in an intuitive manner (be it by relevance, heuristics or Darwinian algorithms) or in a normative (probabilistic, logic, or others) aimed manner.
We argued that this consensus points to DPT, since it is the only one that accounts for such distinction in types of responses, and for the cognitive mechanisms behind normative responding.
Studies in individual differences point out how normative responding is not a random variation, but is in fact a different type of response based on different cognitive mechanisms. We do not mean to imply (as some might) that general intelligence, analytic thinking, or reflective attributes show which individuals are answering correctly. Rather, to us it only points out that there is a pattern between these cognitive mechanisms and normative responses on tasks. This does not imply that a Type 1 response (which might follow relevance or Darwinian algorithms) to any of these tasks is wrong and the normative one is correct, so there is no need to make an evaluative use of normativism in a descriptive DPT. In fact, the opposite can also seem highly plausible. That is, perhaps people who rely more on intuitive relevance judgments, for instance, will be of higher success in social challenges for instance. Success depends on context and on whoever evaluates it.
It is more plausible to state that we have two basic types of reasoning processes both of which can be reliable (or malfunction) in various contexts.
It might be argued that the reason why dual process theories can be seen as accounting for such a hidden consensus is that they are too general and hence have little explanatory power. This is a challenge we will face in the next chapter, that is, one of applying rigor to dual process theories, to discover the best ways of defining and understanding each type of processing and to see how well different authors of this camp (such as 
Stanovich and Evans, 2013;
Kahneman, 2011;
Mercier and Sperber, 2009;
Nisbett et al., 2001)
 can keep coherence with one another and therefore can be understood as being in a collaborative enterprise.


53
CHAPTER TWO:
BASIC DUAL PROCESS THEORY 54 2 Basic Dual Process Theory
We have seen in the last chapter how evidence in reasoning tasks points to a difference in two forms of reasoning, one intuitive and fast and the other slow and careful. However, for such difference to be explained in a theory, much more rigor is needed. In the current state of the art in dual process theories, there are various proposed theories with much in common but also with some crosstalk. In this chapter, therefore, we must choose a coherent theory to continue our endeavors, be it a specific one or a general one, and we must lay out the reasons for such choice. In choosing, we will face a dilemma. If our theory is too specific it might narrow our following results too much.
However, if our theory is too general then it might be superficial. In order to make such decisions we must explore the following internal issues to dual process theories: (Q1) Can dual process theories that come from other evidence bases such as social cognition, learning, language and neuroscience be united with dual process theories of reasoning and decision making? (Q2) Do the clusters of features really divide perfectly into two groups? (Q3) If most features are only correlational, which are the defining ones? (Q4) Are defining features needed for a coherent theory? (Q5) Do these noticed distinctions point to two minds, systems or types of processing? (the reference problem) (Q6) Is a group of features older in evolution than the other? (Q7) Do these two processes share the same knowledge base and goal structure? (Q8) Do they operate in parallel and compete for control of behavior, or do they cooperate, with production of Type 1 default responses that are then assessed and sometimes overridden by Type 2 processes? (Q9) What processes determine if Type 2 processes are called in or not? (these questions can be followed in appendix I).
There are certainly other internal issues with the theory, but the ones mentioned above are central. If one were not to address a few of these issues in detail it would not even make sense to call this clustering of features a theory. Therefore, we start this chapter by reviewing and commenting on these major conflicts while also clarifying what each concept of these feature means. We then propose to form a Basic Dual Process Theory (Basic DPT), one which will be specific enough to be meaningful but general enough to gather some of the consensus. This Basic


55
DPT should be found consistently at the bottom of most dual process theories of reasoning and should allow for an easy access for further features to be implemented. This chapter will focus on four major groups of conflicts that subscribers of dual process theories face with one another. First (section 2.1), we will review a few theories that come from other domains, such as social cognition, learning, language and neuroscience, to see how they add to theories of reasoning and decision making (mostly Q1). Second (section 2.2, 2.3 and 2.4), we will talk about correlational features and defining features of each process (mostly Q2, Q3, Q4).
Third (section 2.5), we will see if the proposed distinctions apply to minds, types or systems (mostly Q5, Q6, Q7). Then, we will discuss differences in forms of conflict resolution between Type 1 and 2 responses (mostly Q8, Q9) (section 2.6). Finally, we sum up with a section (2.7) on Basic DPT.


Single or multiple domain?
One major conceptual matter is to determine if theories from various domains (such as social cognition, language and reasoning) can come together to form a coherent dual process theory of the mind (Q1). One strategy to find an answer is to compare work from diverse research fields in psychology. By the beginning of the 21rst century various similar dual process theories had emerged in different fields of psychology, such as learning and consciousness 
(Schiffrin and Schneider, 1977
, Reber, 1993
, Nisbet and Wilson, 1977
, Baars 1988
, language 
(Mercier and Sperber, 2009)
, social cognition 
(Chaiken, 1980, Chen and
Chaiken, 1999)
, reasoning and decision making 
(Evans and Over, 1996
, Stanovich, 1999
Kahneman, 2011)
 and neuroscience 
(Schneider and Chein, 2003;
Lieberman, 2003
, Goel, 2005
, Kahneman and Frederick, 2007
.
It would be compelling to think that all of these theories are actually referring to the same kinds of process in the mind. If that were the case, then perhaps there could be a general dual process theory that referred to distinctive processes of the various functions in the mind, an architectural theory of the mind. If the evidence from all these fields merged to a single theory, such theory would be much stronger than any of the others alone. However, 
Evans (2008)
 already tried to unite these different theories, but without much success, since, despite the similarities there are various 56 intrinsic disparities. These disparities are related to the questions we asked at the very beginning, that is, theories propose varying features of each process, varying conflict resolution models 14 and have varying ideas about what these processes refer to in the mind (dual types, systems, modes or minds). That is why we need to examine these issues in detail to see how a coherent, basic and fairly consensual theory could look like. We start now by reviewing some dual process theories of different domains to see how they can be relevant to our goals.
We start by characterizing what we see as two major groups of theories: General level theories and specific domain theories. The first group includes theories that attempt to explain general processes of the mind that can apply to multiple domains. In this group are theories that propose dual distinctions such as automatic/controlled 
(Schiffrin and Schneider, 1977)
, explicit/implicit 
(Reber, 1993)
 and conscious/unconscious 
(Baars, 1988)
 15 . Some of these theories are then used by specific domain theories such as 
Chaiken's (1980)
 theory of social cognition, 
Lieberman's (2003)
 neuroscientific approach, Mercier and Sperber's proposal in language and proposals in reasoning and decision making 
Overs, 1996, Evans and
Stanovich, 2013;
Sloman, 1996;
Stanovich, 1999
Stanovich, , 2004
Kahneman, 2011)
. Therefore, specific domain theories tend to be dependent on general theories, but not vice versa.
The distinctions explicit/implicit, conscious/unconscious and automatic/controlled seem to be given in most dual process theories so we will discuss both how they are presented in their general level theory of origin and how to distinguish these concepts which are usually very related.


Learning
The distinction between explicit and implicit processes has as one of its main relevant bases the study of learning and consciousness. 
Reber (1993)
 presents a distinction between implicit and explicit learning where the former is a type of learning that can be complex and occurs without the awareness of the subject, while the latter is a declarative form of learning, which means the subject can describe what he has learned and also the rules governing the knowledge he obtained. 
Reber (1993)
 mostly explains what he precisely means by implicit learning through the description of various experiments in which subjects learn patterns by being exposed to stimuli while being unaware of such patterns. For instance, 
Reber (1967)
 asked subjects to memorize what seemed to be random strings of letters (such as: PVPXVPS, TSSXXVPS, TSXS). These strings, however, were in fact formed by an artificial grammar, a hidden rule-governed structure. The subjects were told it was a memory task. These strings varied in length from three to eight letters and were presented four at a time. Subjects had to reproduce these memorized strings. In the beginning, subjects were making various mistakes, but after some time they were able to make only few mistakes in reproducing these strings. The control group was asked to do the same task, but strings presented to them were truly random, so for them no advance was made, the high number of reproduction errors of strings continued. Both groups believed their strings were random, so a form of implicit learning occurred in the testing group. 
Evans and Over (1996)
 have related this form of learning to Type 1 processes, and explicit learning to Type 2 processes. 
Reber (1993)
 shares with other dual process theorists the idea that these implicit processes are older in evolution and that declarative processes needed to be developed over these implicit ones at a later stage (Q6). He also shares with 
Stanovich (1999)
 the idea that individual differences can only be found in non-implicit processes, that is, implicit learning is species-specific and not specific to individuals. 
16
 The concept of implicitness is rather similar to that of the unconscious.
Other researchers have long held that we may be largely unaware of how we think, but aware only of the results of what we think. "It is the result of thinking, not the process of thinking, that appears spontaneously in consciousness" 
(Miller, 1962, p. 56)
. "The constructive processes [of encoding perceptual sensations] themselves never appear in consciousness, their products do" 
(Neisser, 1967, p. 301)
. 
Nisbett and Wilson (1977, p.33)
 argue that "people often cannot report accurately on the effects of particular stimuli on higher order inference-based responses". Also that " 
[...]
 people may not interrogate a memory of the cognitive processes that operated on the stimuli; instead, they may base their reports on implicit, a priori theories about the causal connection between stimulus and response".
It is possible that the concepts of 'implicitness' and 'unconsciousness' really just share the same reference in various uses, the implicit and explicit distinction being used by theorists afraid of using the 'dangerous' word consciousness. Furthermore, psychologists who do use such 'dangerous' word tend to define it the exact same way as those who speak of implicit and explicit processes. For instance, 
Baars (1988)
 defines consciousness (or access consciousness 17 ) as that to which we (or subjects) have access in a given moment, and unconscious that to which we do not.
Notice that this is the same definition of explicit processes. Unconsciousness or implicitness is postulated when the subject lacks awareness of some mental process. "Implicit learning is the acquisition of knowledge that takes place largely independently of conscious attempts to learn and largely in the absence of explicit knowledge about what was acquired" 
(Reber, 1993, pg 5)
. Where by explicit we believe he means reportable, and by reportable theorists of consciousness, such as 
Baars (1988)
, usually mean conscious.
Supposedly, explicit/implicit processes could be used to refer only to learning and memory functions, so they could be understood as conscious or unconscious learning, whereas consciousness could refer to any given mental process, be it language, vision, audition, decisions and hence refer to a global function. However, when specific domain theorists use the explicit/implicit distinction, such as 
Evans and Over (1996)
 and 
Lieberma"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]