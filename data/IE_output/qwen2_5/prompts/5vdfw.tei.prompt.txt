You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Agreement on what is morally "right" and "wrong" is essential to the functioning of society.
Until very recently humans have been deemed to be the only beings capable of moral decisions, although some animals have been, albeit inconsistently, seen as moral creatures (even if not as full moral agents) and the issue of animal morality is currently discussed anew (cf. 
Monsó et al., 2018)
. However, the ongoing rapid development of the capabilities of artificial intelligence (AI) might give rise to a new type of moral agent. Thus, for future societies of humans coexisting with various types of artificial and hybrid agents to function, questions need to be addressed regarding their respective tasks and responsibilities 
(Meyer et al., 2023)
. Given their current rapid development, these new agents will likely gain in autonomy and their decisions might (to varying degrees) be dissimilar from those that humans might take 
(Gesmann-Nuissl, 2018)
. Increasingly, artificial agents will also be involved in situations requiring moral judgments 
(Jentzsch et al., 2019)
. Although there is no complete agreement among all humans even on key moral issues and considerable individual and cultural differences exist in human moral decision-making 
(Friesdorf et al., 2015;
Graham et al., 2016)
, disagreements between humans and artificial agents on moral questions represent a new type of challenge. Since such disagreements also touch on the more fundamental issue of roles, rights, and responsibilities of humans and the emerging different artificial societal actors 
(Meyer et al., 2023)
, this challenge needs to be timely met. Thus, the question of how humans judge and respond to the decisions of artificial moral agents warrants further investigation, even iffor nowthis concerns decisions in fictional scenarios.
Moral decisions in humans have been investigated for decades using different approaches 
(review: Ellemers et al., 2019)
 with moral dilemmas being one well-established method. Moral dilemmas describe a brief (fictional) scenario with usually two mutually exclusive outcomes to choose (first-party perspective). Alternatively, the decisions of others in a dilemma situation might be presented to be judged from a third-party perspective 
(Christensen & Gomila, 2012)
. Importantly, the application of different moral principles suggests conflicting courses of action in these scenarios. Deontological principles are based on universal rules of what is right and wrong and are thus independent of a specific situation and its outcome. Conversely, utilitarian principles focus on the outcome of an action (or inaction) and ultimately aim to maximize benefit for as many people as possible even if it means harming one or a few individuals 
(Gawronski & Beer, 2017)
. Notably, these two principles are not necessarily in conflict with each other in every situation where moral decisions are required. However, moral dilemmas are specifically designed to pit these principles against each other. Furthermore, research into moral scenarios highlighted the importance of various design and content factors that affect responses 
(Christensen & Gomila, 2012)
. Among those factors are methodological aspects (e.g., word count, question format, expression style) and conceptual variables (e.g., personal force, intention, benefit recipient; 
Christensen et al., 2014;
Christensen & Gomila, 2012)
. It should also be noted that the validity of various scenarios, including the well-known trolley dilemma, has been questioned because some of these dilemmas are lacking in realism 
(Fried, 2012;
Gold et al., 2014;
Kahane, 2015)
. In response, new scenarios based on real-life events have been developed (e.g., 
Körner & Deutsch, 2022)
 and every-day dilemmas with lower stakes have been proposed (e.g., 
Singer et al., 2019)
, though these have been less studied to date.
Third-party perspective. Responses of third parties (i.e., 'observers') to moral decisions of others are of additional interest because third parties are important for maintaining social cooperation and interpersonal trust which depend on consistent reprimanding and punishment of moral transgressions 
(Boyd et al., 2003)
. Observers have been suggested to act as everyday judges and to object to perceived violations of moral or social norms even if they are not involved themselves 
(Weiner, 2006)
. They are of particular importance in this regard in larger and increasingly anonymous societies with frequent one-off interactions 
(Bendor & Swistak, 2001;
Boyd et al., 2003)
. Thus, although most studies on moral decision-making have investigated the first party perspective, there is a strong ongoing research interest in responses of observers to moral choices of others (e.g., 
Behnke et al., 2020)
.
Moral choices of artificial agents. While the majority of third-party studies investigated moral decisions by humans, research has started on responses to moral choices by artificial agents (e.g., 
Awad et al., 2018;
Bigman & Gray, 2018;
Malle et al., 2019;
Malle et al., 2015)
. The studies used similar approaches to the ones on human decision-makers and presented moral dilemmas to assess whether various decisions of (fictional) artificial moral agents were deemed appropriate. In addition, measures of blame and trust have been explored (cf. 
Malle et al., 2015)
.
Findings suggest that artificial agents are expected to act more utilitarian than human agents in adaptations of the trolley dilemma, i.e., to sacrifice one individual to save four 
(Malle et al., 2015
). Furthermore, human and artificial agents receive different amounts of blame for the same moral choices, although findings are inconsistent on whether artificial agents can be blamed at all, i.e., whether they can be morally responsible. While some studies reported that artificial agents were blamed (although to a different degree) by most participants 
(Malle et al., 2019;
Malle et al., 2015)
, others found that the majority did not or just to a small extent ascribe morality to robots and consequently did not blame them 
(Bretschneider et al., 2022;
. 
Bigman and Gray (2018)
, who investigated to what extend human decision makers are preferred to artificial agents in various (fictional) medical, legal, or military scenarios, concluded that there is a general preference for human agents. They described an 'aversion' to moral decision-making by artificial agents whose degree depended on the agents' perceived experience and expertise 
(Bigman & Gray, 2018)
. However, they also emphasized that they investigated life-or-death scenarios and that results might differ in dilemmas with less at stake.
In sum, there is growing interest into how artificial moral agents are evaluated by human observers accompanied by an ongoing debate about whether and to what extent such agents are actually capable of moral decisions and whether they should be allowed to make them 
(Malle, 2016;
Meyer et al., 2023;
Misselhorn, 2018)
.
Individual differences in evaluating moral choices. In addition to general preferences for human vs. artificial moral agents, individual differences in evaluating moral choices by artificial agents can be expected. Research focusing on human moral agents indicated gender differences with men being more likely to endorse utilitarian options (e.g., 
Armbruster et al., 2021;
Banerjee et al., 2010;
Bjorklund, 2003;
Capraro & Sippel, 2017;
Fumagalli et al., 2010)
 although some studies did not find differences between men and women (e.g., 
Brannon et al., 2019;
Seyedsayamdost, 2015)
. Furthermore, there are gender differences in attitudes towards robots and AI 
(Funk et al., 2020;
Sindermann et al., 2021)
 as well as towards technology in general (meta-analysis: 
Cai et al., 2017)
 with men reporting more positive attitudes. Thus, gender differences might also exist in judging decisions of artificial moral agents.
In addition to gender effects, general population differences in attitudes towards technology, robots, or AI are likely to affect responses towards artificial agents making moral decisions.
Previous studies show, for instance, that negative attitudes towards robots influence interactions with them 
(Babel et al., 2022;
Nomura et al., 2008)
 and evaluations of their behavior styles 
(Syrdal et al., 2009)
, while affinity for technology interaction as a more general measure of approaches to technology has been linked to self-reported usage of technical systems 
(Franke et al., 2019)
. Therefore, different outlooks on both broader as well as specific technological issues might predict responses to artificial moral agents.
Personality traits have also been found to be associated with moral judgement tendencies including, for instance, Need for Cognition (NFC) and trait psychopathy. NFC refers to the tendency to engage in and enjoy effortful cognitive activities 
(Cacioppo & Petty, 1982)
. Previous studies have linked NFC to utilitarian judgement tendencies (e.g., 
Conway & Gawronski, 2013)
 although findings are not consistent and associations with deontological preferences have also been found (e.g., 
Körner et al., 2020;
Park et al., 2016)
. Higher NFC levels had initially been suggested to lead to increased deliberation of costs and benefits and thus been associated with utilitarian decisions. However, recently NFC has also been proposed to result in increased reflections of moral norms to explain correlations with deontological decisions 
(Körner et al., 2020
). Furthermore, NFC predicts self-reported moral behavior, e.g., donating and supporting others in need or considering action consequences for others, suggesting that enjoyment of and engagement in effortful cognitions might represent a 'moral capacity' 
(Strobel et al., 2017)
.
Contrariwise, deviant moral behavior has been recognized as a core element of psychopathy as reflected in its early descriptions as 'moral derangement' 
(Rush, 1812)
 or 'moral insanity' 
(Prichard, 1835)
. Psychopathy comprises characteristics like antisocial behavior, lack of empathy, remorse or guilt, glibness, shallow affect and impulsivity 
(Hare & Neumann, 2009)
. Since psychopathy exists on a continuum, the clinical construct was adapted to the sub-clinical domain (cf. 
Hare, 1985)
. Findings of moral dilemma studies suggest that individuals with higher trait psychopathy are more inclined to make utilitarian choices (meta-analysis: 
Marshall et al., 2018)
, which results in less casualties in (fictional) sacrificial dilemmas. This conflicts with real-life behavior of persons with increased levels of psychopathy, who usually appear to be not particularly concerned with enhancing the 'greater good' as they are responsible for a disproportionate amount of physical, financial, social or emotional harm 
(Kiehl & Hoffman, 2011)
.
Further research has shown that a general lack of empathy and reduced compassion for others 
(Glenn et al., 2009;
Seara-Cardosa et al., 2013)
 together with a reduced dislike for performing harmful actions 
(Patil, 2015
) may contribute to these 'utilitarian' choices. Recent studies revealed that individuals scoring higher in trait psychopathy actually show reduced deontological and reduced utilitarian inclinations but increased action tendencies 
Körner et al., 2020)
. Trait psychopathy belongs to the group of 'dark' traits which have been originally suggested to comprise a Dark Triad together with narcissism and Machiavellianism 
(Paulhus & Williams, 2002)
. Later research proposed additional traits (e.g., spitefulness, egoism) as the basis of the dark core of personality 
(Moshagen et al., 2018)
. Given the links between these traits and moral attitudes in general, they might also impact reactions towards moral choices by artificial agents.
Religion has been identified as another important factor influencing moral decisions and attitudes 
(Cohen, 2015;
Graham et al., 2016)
 with deontological judgements being positively associated with religiosity 
(Szekely et al., 2015)
. Furthermore, religiosity has been linked to negative views on interactions with robots 
(Giger et al., 2017)
 and to more fearful attitudes towards them 
(Katz & Halpern, 2014)
, although different religions have been proposed to exert different effects in this regard 
(Halpern & Katz, 2012;
MacDorman et al., 2009;
Shaw-Garlock, 2009
).
Nevertheless, religiosity might shape attitudes towards artificial moral agents.
In this study, we aim to investigate potential differences in responses to moral choices by human vs. artificial agents. We evaluate ratings on whether these choices are considered (1) morally appropriate as well as (2) how much blame an agent deserved for their decision and 
3
to what degree the agent can be trusted. Furthermore, we investigate the effect of situation and person variables on response outcomes. Situation-related, we contrast responses to high-stakes with low-stakes dilemmas while person-wise we investigate effects of religiosity, NFC, Dark Triad traits, attitudes towards technology and robots as well as age and gender. We preregistered the following hypotheses (see https://osf.io/ukn28 and https://osf.io/e4jsb): (1a) Artificial moral agents are blamed less for acting in high as well as low-stakes dilemmas compared to human agents, and are thus rated more positively. (1b) Artificial moral agents are lamed more for in-action in both dilemma types compared to human agents. (2) Need for Cognition (NFC) is negatively associated with blaming artificial agents and positively with trusting them, particularly when those agents chose action over inaction. We also preregistered the following research questions: Compared to human agents, how much trust is attributed to artificial moral agents after action resp. inaction? How do attitudes towards robots influence the relationship between To what extent does Affinity for Technology Interaction (ATI) influence the relationship between agent type and the evaluation of the decision to act resp. not to act? Are there differences between evaluation of artificial moral agents and human actors in high-vs. low-stake dilemmas?


Materials and Methods
We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study 
(Simmons et al., 2012)
. All materials, data, and analyses of this study are available online (https://osf.io/mf2ax/ and https://osf.io/x8d4m/). Both studies were preregistered (https://osf.io/ukn28 and https://osf.io/e4jsb).


Samples
We conducted two online studies and originally recruited N1=500 and N2=500 participants using Prolific Academic (www.prolific.co; Palan & Schitter, 2018) who responded to decisions of human and artificial agents in high-stakes (study 1) and low-stakes (study 2) moral dilemma scenarios. Individuals who took part in study 1 could not participate in study 2. To ensure comprehension of the scenarios' text, participants had to be either German native speakers or had to speak the language at a proficient level. In study 1, n = 247 female (49.4%), n = 246 male (49.2%), and n = 7 diverse individuals (1.4%) participated. In study 2, n = 257 female (51.4%), n = 233 male (46.6%) and n = 10 diverse individuals (2.0%) took part. Since gender effects were investigated in all analyses, diverse subjects had to be excluded because of the small subsample size. Furthermore, two individuals indicated to be under 18 years of age and were excluded from all analyses as well. Thus, in the final first sample, N = 491 individuals remained (mean age = 30.53, SD = 11.06, range = 18-72 years) while the second sample consisted of N = 490 individuals (mean age = 31.46, SD = 10.97, range = 18-72 years).


Moral Dilemmas
In study 1, four high-stakes dilemmas detailing public health related scenarios were adapted from . They were rephrased in the third-party perspective with either a treating physician or a treating care robot as moral agent who decided to follow a suggested course of action or not (cf. 
Malle et al., 2015)
. Similarly, in study 2 four low-stakes dilemmas in the third-party perspective were employed. Content-wise, low stake dilemmas included mainly administrative decisions (e.g., on study grants or library bans) with either a human or an artificial moral agent who decided for or against a certain action. Three of the four lowstakes dilemmas were adapted from pre-existing scenarios. The library ban scenario is a modified version of the smart house dilemma 
(Liao et al., 2019)
 and the early parole scenario is based on a dilemma used by 
Bigman & Gray (2018)
. The obese patient dilemma was adapted from a conflict scenario posted on the Open Roboethics institute's website (Open Roboethics Institute, 2015), while the scholarship dilemma is inspired by real-life differences between funding bodies in how they take into account different characteristics of an applicant when deciding who gets funding and who does not. While in each high-stakes dilemma the lives of a single individual or multiple persons were on the line, none of the low-stakes scenarios contained a life-or-death situation. Thus, in contrast to the sacrificial high-stakes scenarios, low-stakes dilemmas represent every-day moral conflicts. The exact wording of all dilemmas can be found in the supplement and online (https://osf.io/mf2ax/ and https://osf.io/x8d4m/).
After reading the main text of a scenario, participants first indicated whether they judged it morally appropriate or not for the human or artificial agent to take the suggested action. Following, participants were presented with the decision made by the respective agent who either chose to act or not. Agreement (yes vs. no) between participants' own moral judgements and an agent's actual decision was later calculated for each scenario to which participants responded. Participants then rated how much blame the agent deserved for their choice on a visual analog scale ranging from "no blame at all" to "maximum blame"; see also supplement for a depiction of the scale) and to what degree the agent could be trusted. Trust was evaluated on an 8-point Likert scale ranging from 0 = "not at all" (trustworthy) to 7 = "very much". In both studies we used a 2 (agent: human vs. artificial) × 2 (decision: action vs. inaction) design. To obtain a balanced distribution in these four conditions across the four dilemmas, a Latin square was used in both studies resulting in 16 randomization groups in each study with differing condition combinations to which participants were randomly assigned.


Questionnaires
To assess Need for Cognition, the German version of the NFC Short Scale 
(Bless et al., 1994)
 was used. Participants responded to the 16 items of the questionnaire on a 7-point Likert scale with higher sum scores indicating higher Need for Cognition levels. Internal consistency of the NFC short scale was α = .89 (sample 1) and α = .87 (sample 2), respectively. Religiosity was assessed with three items each from the Duke University Religion Index (DUREL; 
Koenig & Büssing, 2010)
 and the Centrality of Religiosity Scale (CRS; 
Huber & Huber, 2012)
. In addition, participants responded to one item assessing self-perceived general religiosity (ranging from 1 -not religious to 10 -religious). Combining these z-standardized items resulted in a scale with an internal consistency of α = .90 and .91, respectively, in the two samples. Thus, a single indicator of religiosity was used with higher mean scores indicating higher trait levels.
The German Version of the Dirty Dozen 
(Küfner et al., 2015)
 was employed to measure the three traits of the Dark Triad: psychopathy, narcissism, and Machiavellianism (assessed with four items each). Participants responded on a 9-point Likert scale with higher means in the respective sub-scales indicating higher trait levels. In the two samples, Cronbach's alpha for the subscales were as follows: psychopathy α = .67 and .62, narcissism α = .74 and .82, and Machiavellianism α = .79 and .82. We used the Affinity for Technology Interaction (ATI) scale 
(Franke et al., 2019)
 to assess the tendency to actively engage in technology interaction. The 9item scale was originally developed in German and uses a 6-point Likert scale response format.
Internal consistency of the ATI scale in the two samples was α =.91 and .92, respectively. The German version of the Negative Attitudes towards Robots Scale (NARS; 
Nomura et al., 2008)
 was used to capture attitudes concerning interaction and communication with robots. The scale comprises a total of 14 items on three sub-scales assessing negative attitudes towards (1) situations and interactions with robots (six items; α = .75 and .74), (2) social influence of robots (five items; α = .72 and .70), and (3) emotions in interaction with robots (three items; α = .63 and .67). The NARS uses a 5-point Likert scale response format 
(Nomura et al., 2006)
 with higher mean scores indicating more pronounced negative attitudes towards robots.


Procedure
In both online studies, participants were first informed about the study aims and protocol as well as about data protection policies and were asked to indicate their consent via button press. Afterwards, a number of demographical variables (e.g., age, gender, education) were assessed.
Following, participants completed questionnaires to measure Need for Cognition (NFC), Religiosity, Dark Triad traits, as well as Affinity towards Technology in Interaction (ATI) and
Negative Attitudes towards Robots (NARS). Afterwards, participants were introduced to the moral dilemma experiment. They then completed the four scenario × condition combinations they had been randomly assigned to. As described above, for each scenario participants first indicated whether a suggested course of action was morally appropriate or not for a respective agent to take. Following, they were informed about the choice of the agent and rated blame for and trust in the agent. After completing the survey, participants were thanked. Participants of both samples received 5 € each for completing the study. The study design and protocol was approved by the ethics committee of Chemnitz University of Technology (#101499823).


Statistical Analysis
All analyses were performed with SPSS 29 (IBM Statistics). Using repeated measurements ANOVAs, we first analyzed effects on (a) participants' ratings on whether it was morally appropriate for the respective agent to take the suggested action. Agent type (human vs. artificial) was entered as within-subject factor, while dilemma type (low vs. high-stakes) and gender were entered as between-subject factors. In additional ANOVAs effects of dilemma type, agent type, agent's decision (action/inaction) and gender on (b) blame and (c) trust ratings were investigated. Following, regression analyses (enter method) were conducted to further assess the role of personality traits and attitudes as potential predictors of the amount of blame agents received and to what degree they were trusted. The following variables were entered as predictors for responses to human agents: dilemma type, agreement with agents' moral choices, gender, age, Need for Cognition (NFC), religiosity, and the Dark Triad traits narcissism, psychopathy and Machiavellianism. Agreement ratings were based on whether participants' own moral judgements in a given scenario were in line with the presented moral choice of the agent or not. In addition to the aforementioned predictors, four more variables were included to predict responses to artificial agents: affinity towards technology interaction (ATI) and the three NARS subscales negative attitudes towards (1) situations and interactions with robots, (2) social influence of robots, and (3) emotions in interaction with robots. Ancillary correlation analyses were conducted to investigate associations between the various personality traits and attitudes as well as between trust and blame whose results can be found in the supplement.


Results


ANOVA: Effects on moral appropriateness ratings of actions in dilemma situations
Participants rated for both high-stakes and low-stakes dilemmas whether a suggested course of action was appropriate or not for the respective agent. There was no difference between the averaged appropriateness ratings to the two dilemmas types (high-stakes: 49.03%; low-stakes: 50.05%; F1, 976 = 0.51, p = 0.477). Furthermore, there was no general effect of agent type (F1, 976 = 0.09, p = 0.767) with endorsement rates of suggested actions being 49.39% for human agents and 49.80% for artificial agents. There was also no interaction between dilemma type and type of agent (F1, 976 = 0.64, p = .422) and men and women did not differ in their overall endorsement of suggested actions (F1, 976 = 0.004, p = .948). There were also no interactions between gender and agent type (F1, 976 = 0.74, p = 0.389) or gender and dilemma type (F1, 976 < 0.001, p = .993). However, there was a three-way interaction between dilemma type, agent and gender (F1, 976 = 7.61, p = .006, η p 2 = 0.008; see 
Figure 1
). The effect is mainly due to highstakes dilemmas with follow-up analyses revealing a significant gender × agent interaction (F1, 489 = 7.01, p = .008, η p 2 = 0.014) which was not found in the low-stakes scenarios (F1, 487 = 1.69, p = .195). In high-stakes scenarios, women were somewhat less likely than men to endorse the actions of human agents (0.45 vs. 0.51). Contrariwise, more women than men endorsed the actions of artificial agents in high-stakes scenarios (0.53 vs. 0.47). 


ANOVA: Effects on blame of moral agents
Blame ratings differed by dilemma type (F1, 976 = 63.61, p < .001, η p 2 = 0.061) with increased blame in high-stakes scenarios. Overall blame ratings were also higher for human compared to artificial agents (F1, 976 = 46.24, p < .001, η p 2 = 0.045) and for actions compared to inactions (F1, 976 = 83.56, p < .001, η p 2 = 0.079). Furthermore, there were significant interactions between dilemma type and agent (F1, 976 = 7.65, p = .006, η p 2 = 0.008), dilemma type and decision type (action/inaction; F1, 976 = 17.01, p < .001, η p 2 = 0.017) and agent type and decision type (F1, 976 = 10.01, p = .001, η p 2 = 0.010; see 
Figure 2
). Follow-up analyses revealed that in both highstakes and low-stakes scenarios there was more overall blame for human agents (all p ≤ .003) and for actions compared to inactions (all p ≤ .001). However, there was an agent type × decision type interaction effect on blame in the high-stakes scenarios (F1, 489= 8.64, p = .003, η p 2 = 0.017) but not in the low-stakes dilemmas (p = .121). Overall, we could confirm one part of our hypothesis regarding differences in blaming human vs. artificial agents. Artificial agents were indeed blamed less for choosing to act, but they were also blamed less when deciding not to act. Contrary to our expectation, there were thus general higher blame ratings for human agents, not just in the "action" condition. There was no main effect of gender (F1, 976 = 0.028, p = .866), nor were there any interaction effects involving gender on blame ratings (all p ≥ .244). 


ANOVA: Effects on trust of moral agents
Overall, trust in the respective agents did not differ in high-compared to low-stakes scenarios (F1, 826 = 0.21, p = .644). However, there was a significant effect of agent type (F1, 826 = 6.35, p = .012, η p 2 = 0.008) with higher trust ratings for human agents as well as a decision type main effect (F1, 826 = 15.68, p < .001, η p 2 = 0.019) with higher trust ratings for agents who chose not to take action. There was also an interaction between dilemma type and decision type (F1, 826 = 7.79, p = .005, η p 2 = 0.009) and between agent and decision type (F1, 826 = 5.19, p = .023, η p 2 = 0.006; see 
Figure 3
). Follow-up analyses revealed an interaction between agent and decision type in high-stakes scenarios (F1, 489 = 4.07, p = .044, η p 2 = 0.008) that was not present in the low-stakes dilemmas (p = .193). Trust in human agents in high-stakes scenarios is only higher when agents do not take action while there is no difference between human and artificial agents when they do decide to act. Similar to blame ratings, gender did not affect trust ratings (F1, 826 = 0.004, p = .948) nor where there any significant interactions involving gender (all p ≥ .300). 


Regression analyses: Prediction of blame and trust in human agents
Regression analyses revealed the following predictors for blame of human agents: agreement with agents' moral choices (β = -.384, p < .001), dilemma type (β = -.238, p < .001), trait psychopathy (β = .077, p = .028) and religiosity (β = .060, p = .037). NFC, narcissism, Machiavellianism, gender, and age did not predict blame for human agents (all p ≥ .063; see 
Table 1
).
Trust in human agents was predicted by agreement with agents' choices (β = .258, p < .001) and trait psychopathy (β = -.095, p = .015). No other predictor reached significance (all p ≥ .132; see 
Table 2
).  


Regression analyses: Prediction of blame and trust in artificial agents
Blame of artificial agents was predicted by agreement with agents' choices (β = -.323, p < .001), dilemma type (β = -.106, p < .001), negative attitudes towards situations and interactions with robots (NARS subscale 1, β = .109, p = .005), narcissism (β = .096, p = .007), religiosity (β = .063, p = .040), and age (β = -.077, p = .014). Affinity for technology interaction, NARS subscales 2 and 3 as well as Machiavellianism, psychopathy, gender, and, contrary to our initial hypothesis, NFC did not predict blame for artificial agents (all p ≥ .070; see 
Table 3
). Trust in artificial agents was predicted by agreement with agents' choices (β = .218, p < .001) and negative attitudes towards emotions in interaction with robots (NARS subscale 3, β = -.085, p = .025). All other predictors did not reach significance, including, again contrary to our hypothesis, NFC (all p ≥ .146; see 
Table 4
).  


Discussion
Perceived appropriates of moral choices by human and artificial agents. We investigated responses to human and artificial agents in high-and low-stakes moral dilemmas, respectively.
Overall, there were no differences in perceived appropriateness of suggested actions in the two dilemma types and decision approval did not differ between human and artificial agents. Intriguingly, while there were also no overall gender differences in moral appropriateness ratings, an interaction occurred suggesting differences between men's and women's moral judgements in high-stakes scenarios depending on agent type. In the case of human agents, women endorsed the suggested course of action (i.e., instrumental harm to achieve a greater good) to a lesser degree than men. This result mirrors findings from previous moral dilemma research with human agents. Women have been reported to be less likely to choose 'utilitarian' options, i.e., to opt for actions that required harm to one or a few persons for the benefit of multiple individuals (e.g., 
Armbruster et al., 2021;
Banerjee et al., 2010;
Bjorklund, 2003;
Capraro & Sippel, 2017;
Fumagalli et al., 2010)
 although findings are not entirely consistent (e.g., 
Brannon et al., 2019;
Seyedsayamdost, 2015)
. Notably, a similar effect of gender was not present in the low-stakes dilemmas. Previous moral dilemma research has predominantly used scenarios that can be classified as high-stakes dilemmas similar to the ones used in study 1 where lives are on the line.
Contrariwise, in low-stake scenarios there are no life-or-death consequences associated with any of the two response options. Although choices in these scenarios may result in negative outcomes, their impact is considerably less severe which might explain the lack of difference between the ratings of men and women. Furthermore, in high-stakes scenarios with artificial agents the opposite pattern regarding gender was observed: women were more likely to endorse the suggested course of action compared to men. Again, no gender difference of this kind was found for the low-stakes scenarios.
Blaming and trusting moral agents. Unlike the assessment of moral appropriateness, blame and trust ratings differed between human and artificial agents. Overall, human agents received considerably more blame but were also more trusted. Blame requires attribution of moral responsibility which in turn depends on various legal and psychological capacities (e.g., the capacity to act, legal capacity, autonomy, liability, explainability, and moral agency) most of which are usually not ascribed to artificial agents 
(Meyer et al., 2023)
. Our finding of lower blame ratings for artificial agents (for making the same decisions as their human counterparts) indicate that they are held less responsible. Contrary to 
Malle et al. (2015)
, and to our own initial hypothesis, we did not find higher blame ratings of artificial agents in inaction conditions. Differences in blame ratings of human vs. artificial agents varied and were least pronounced in the low-stakes inaction condition but never veered in the opposite direction. Notably, response differences to human vs. artificial moral agents have been reported to be modulated by additional factors,
including an artificial agent's physical appearance 
(Laakasuo et al., 2021)
 or their perceived expertise and experience 
(Bigman & Gray, 2018)
. Furthermore, research into human moral agents highlighted the importance of dilemma content features for judging agents' decisions as morally right or wrong 
(Christensen et al., 2014;
Christensen & Gomila, 2012)
. While, for instance, 
Malle et al. (2015)
 used a scenario modelled after the trolley dilemma, we investigated moral decisions in medical and administrative contexts. Thus, the response pattern regarding blame and trust found in our study might also be partly due to dilemmas features and might differ in other moral scenarios. Furthermore, we did not provide participants with pictures of the robots or other clues regarding their features. Since the look of a robot has been recently shown to affect humans' responses to the robot's moral decisions 
(Laakasuo et al., 2021)
, the (different) way participants might have imagined them to look like, may have had an effect on their responses in this study as well.
Blame and trust were also affected by dilemma and decision type. In both high-and low-stakes dilemmas, actions resulted generally in higher blame and reduced trust ratings compared to inactions, which is in line with previous findings on omission bias. The latter refers to a typical preference of not taking action in moral dilemmas when both acting and doing nothing are expected to result in adverse outcomes, particularly when the consequences are perceived as similarly harmful (cf. 
Jamison et al., 2020)
. Furthermore, average blame ratings were higher in high-stakes compared to low-stakes scenarios, although there was no difference in trust ratings between the two dilemma types. Interaction effects indicate that these general tendencies are partly modified by other factors. For instance, in high-stakes dilemmas trust in human compared to artificial agents was only higher in the inaction condition, while there was no difference in trust when agents took action. Contrariwise, there was no interaction of this kind in low-stakes scenarios. Regression analysis additionally confirmed dilemma type as a predictor for blame ratings. However, the by far strongest and most consistent predictor for blame of and trust in both types of agents was agreement with agents' moral choices. Unsurprisingly, agents whose choices tended to be in line with those of the participants received on average less blame and were perceived as more trustworthy. If this finding proves to be robust, it would have significant implications for people's expectations of the behavior of artificial agents. Previously, 
Malle et al. (2015)
 reported that participants, who considered it impermissible to sacrifice one individual to save four persons, blamed human and artificial agents considerably more for choosing to act than for refraining to act. However, blame ratings of participants, who found it permissible to sacrifice the one person, did not significantly differ between agents who decided to act and those who refrained from doing so 
(Malle et al., 2015)
.
Individual differences in blaming and trusting moral agents. Religiosity emerged as a further predictor for blaming human and artificial agents alike with higher trait levels being associated with more blame. Generally, religion and religiosity have been argued to be key factors in moral attitudes and judgements 
(Cohen, 2015;
Graham et al., 2016)
. Previously, religiosity has been linked to deontological judgments and increased emotions while processing moral dilemmas 
(Szekely et al., 2015)
. Our findings of higher blame ratings (for human agents) are in line with this. However, religious values vary substantially within and between cultures contributing to considerable differences in morality 
(Graham et al., 2016)
. Furthermore, there are also associations between religion and religiosity, respectively, and attitudes towards robots and AI. 
Giger et al. (2017)
 reported an association between religiousness and negative attitudes towards interactions with robots. Religiosity has also been linked to a more fearful attitude towards robots 
(Katz & Halpern, 2014)
. These connections between religiosity and more negative attitudes towards robots or AI might be partly responsible for our finding of more blame for artificial agents by individuals with higher religiosity scores. This notion is supported by positive correlations between religiosity and the NARS subscales in our samples, in particular with subscale 2 (negative attitudes towards social influence of robots; Supplemental 
Table 1
). However, it should be noted that distinct religious belief systems are likely to differently affect acceptance of and attitudes towards robots and AI 
(Halpern & Katz, 2012;
MacDorman et al., 2009;
Shaw-Garlock, 2009)
. Recently, 
Ikari et al. (2023)
 investigated moral care for robots in US participants with predominantly Abrahamic beliefs and in Japanese participants with Shinto-Buddhist traditions. They found higher moral care for robots in Japan. Furthermore, more pronounced religious beliefs were linked to less moral care in the American but not the Japanese sample.
Additionally, lower scores in anthropocentrism and higher ones in animism were also linked to increased moral care 
(Ikari et al., 2023)
. Given the cultural background of our participants, the association of religiosity and increased blame of artificial agents is in line with similar findings in Western samples (e.g., 
Giger et al., 2017)
.
Of the Dark Triad traits, psychopathy in particular showed associations with blame and trust ratings. Psychopathy was associated with more blame of and less trust in human agents while narcissism was related to blame of artificial agents. Higher levels of trait psychopathy have been associated with harsher punishment of (human) moral agents in fictional moral dilemmas despite concomitantly being linked to reduced inappropriateness ratings of moral transgressions and increased understanding emotions towards the moral agent 
(Behnke et al., 2020)
. Psychopathy is also linked to increased vengefulness, and both psychopathy and narcissism have been found to predict reduced forgiveness 
(Giammarco & Vernon, 2014)
. Shared features of traits belonging to the Dark Triad 
(Paulhus & Williams, 2002)
 or the dark core of personality 
(Moshagen et al., 2018)
 are behavioral tendencies for self-promotion and maximizing one's own interests while ignoring, accepting, or maliciously causing detriment to others combined with beliefs that justify such behavior 
(Moshagen et al., 2018;
Paulhus & Williams, 2002)
. The key is the disregard of others linked to social malevolence, which together with reduced forgiveness might have manifested here in an inclination to ascribe greater blame and indicate less trust.
Contrary to initial expectations, NFC did not predict responses to moral agents, although it only just missed the significance level for blame (human agents: p = .101, artificial agents: p = .070).
NFC reflects the tendency to engage in and enjoy effortful cognitive activities 
(Cacioppo & Petty, 1982)
. Although previous findings are not entirely consistent, reasoning abilities and propensities (i.e., deliberate thinking styles like NFC) have been linked to increased preferences for utilitarian moral choices and optimization of overall welfare without being associated with reduced harm aversion (e.g., 
Patil et al., 2021)
. Cognitive abilities resp. motivation have be proposed to contribute to differences in processing moral problems with studies showing selective impairment of utilitarian judgements by cognitive load 
(Conway & Gawronski, 2013;
Timmons & Byrne, 2019)
. Based on findings that NFC is negatively associated with punitive responses, 
Sargent (2004)
 suggested that higher NFC levels might result in an increased ability and/or willingness to invest cognitive effort to reflect on specifics and constraints of dilemma settings and protagonists. Although associations of NFC with reduced blame did not reach significance in our study, they are, on a descriptive level, in line with reported links of NFC and reduced support for punishment 
(Sargent, 2004)
. Both are likely due to a more in-depth cognitive analysis of a dilemma situation and its protagonists.
Finally, negative attitudes towards situations and interactions with robots (NARS subscale 1) was a predictor of blaming artificial agents, while negative attitudes towards emotions in interaction with robots (NARS subscale 3) was a predictor of how much they were trusted. The other NARS subscales as well as Affinity for Technology Interaction (ATI) did not predict blame of resp. trust in artificial agents. NARS scores have been linked to actual behavior towards robots (e.g., time talking with them or touching them; 
Nomura et al., 2008)
 and evaluation of robot behavior styles 
(Syrdal et al., 2009)
. NARS scores were found to be negatively associated with complying with a robot's request in a VR setting 
(Babel et al., 2022)
, although in another reallife experiment, NARS scores did not correlate with complying with requests made by a geminoid robot 
(Aroyo et al., 2018)
. Thus, despite some inconsistent findings, self-reported attitudes towards robots appear to be linked to behavioral responses to robots, which is echoed in our findings on their associations with blame and trust. Contrary to NARS, Affinity for Technology Interaction (ATI) showed no association with responses to artificial agents, which might be due to the more general scope of the questionnaire which is not tailored to interactions with robots/AI but focuses on dealing with technology in comparatively broad terms.
Overall, our findings emphasize several general tendencies when responding to artificial compared to human agents. While humans were generally more blamed compared to artificial agents for the same decisions, they were nevertheless more trusted. Also, high-stakes scenarios resulted in higher blame ratings compared to low-stakes dilemmas. However, responses to agents' decisions also showed considerable individual variance and several person variables emerged as predictors of blame and trust. The most important and consistent ones were moral appropriateness ratings, i.e., whether there was agreement or not with an action that an agent was suggested to take. Further predictors included dilemma type and religiosity for blame of both human and artificial agents, and psychopathy for blaming and trusting specifically human agents. For artificial agents, negative attitudes towards robots were additional predictors for blame and trust, respectively. Still, the amount of explained variance (R 2 =.163-.225 for blame and .071-0.79 for trust; see also Tables 1-4) indicates that there are other predictors, particularly for trust, that were not part of the study and require further investigation.
Limitations. The study has several limitations. While we were able to recruit sufficiently large samples, the online format adds limits to data quality control. Furthermore, individuals who register on platforms like Prolific to partake in research studies are usually better educated.
Accordingly, in both samples about half of our participants reported to hold a university degree and more than additional 30% had graduated from high school ('Abitur/Matura'; see also Supplemental Material). Also, to ensure thorough comprehension of the dilemma texts, only German native speakers and individuals who spoke German at a sufficiently high level could participate. Furthermore, while the age range of our samples is rather large (18-72 years), more than 80% of participants were under 40 years old. Regarding the scenarios used, content variety was limited in particular for high-stakes dilemmas, all of which were set in a medical context.
As dilemma content features can affect responses 
(Christensen et al., 2014;
Christensen & Gomila, 2012)
, scenarios with different settings might lead to different results. Furthermore, we only investigated responses to 'generic' types of agents without further specifications or modifications of their characteristics. However, physical features 
(Laakasuo et al., 2021)
 and perceived experience and expertise of artificial agents 
(Bigman & Gray, 2018)
 might modulate responses to their moral decisions. Thus, there are several design factors limiting generalizabil-ity of our findings and further research is needed to investigate the interplay of agent and dilemma features.


Conclusion.
In sum, our study provides further insight into how moral decisions of artificial in contrast to human agents are evaluated and which variables might affect individual differences in those responses. While (dis)agreement with one's own moral choice preferences proofed to be the most important predictor for blaming and trusting other moral agents, several additional predictors emerged. Although studies investigating responses to artificial moral agents cannot sufficiently address the question of whether these agents are actually capable of making moral choices (cf. 
Meyer et al., 2023)
, findings nevertheless yield important information. Understanding human perception of and responses to artificial agents in situations with moral implications is essential for optimizing these interactions with due consideration of their psychological and legal limitations. With the presence of artificial agents increasing, ensuring smooth encounters with humans becomes ever more important for future societal functioning.
(a) NFC and blame judgement and (b) NFC and trust in artificial moral agents? How do Dark Triad personality traits (narcissism, psychopathy, Machiavellianism) influence blame judgement resp. trust in artificial moral agents after a decision for action/inaction in moral dilemmas?


Fig. 1 .
1
Mean percentage of agreement with suggested actions in high-stakes and low-stakes dilemmas with human and artificial agents (AI/R)


Fig. 2 .
2
Mean blame ratings (± SEM; range: 0-100) for actions resp. inactions of human and artificial agents in high-stakes and low-stakes dilemmas


Fig. 3 .
3
Mean trust ratings (± SEM; range: 0-7) of acting resp. not acting human and artificial agents in high-stakes and low-stakes dilemma


Table 1 :
1
Results of regression analysis on predicting blame of human agents
B
Std.-Error
β
p
95% CI lower bound
95% CI upper bound
(Constant)
80.138
5.954
<.001
68.454
91.823
Dilemma Type
-11.968
1.432
-.238
<.001
-14.777
-9.158
Agreement
-27.775
2.048
-.384
<.001
-31.794
-23.756
Gender
-.271
1.498
-.005
.856
-3.212
2.669
Age
-.127
.068
-.056
.063
-.260
.007
Religiosity
1.905
.910
.060
.037
.119
3.690
NFC
-1.361
.828
-.048
.101
-2.986
.264
Machiavellianism
-.172
.553
-.012
.756
-1.258
.913
Psychopathy
1.279
.581
.077
.028
.138
2.420
Narcissism
-.025
.510
-.002
.960
-1.027
0.976
R
2 = .225, F9,980 = 31.30, p < .001 Agreement = agreement with agent's moral choice, NFC = Need for Cognition


Table 2 :
2
Results of regression analysis on predicting trust in human agents
B
Std.-Error
β
p
95% CI lower bound
95% CI upper bound
(Constant)
3.498
.452
<.001
2.611
4.384
Dilemma Type
.100
.109
.029
.359
-.114
.315
Agreement
1.296
.156
.258
<.001
.989
1.603
Gender
.050
.114
.014
.663
-.175
.274
Age
.003
.005
.021
.531
-.007
.013
Religiosity
-.034
.069
-.016
.622
-.171
.102
NFC
.095
.063
.048
.132
-.029
.218
Machiavellianism
.004
.042
.004
.932
-.079
.086
Psychopathy
-.109
.044
-.095
.015
-.196
-.022
Narcissism
.011
.039
.011
.771
-.065
.087
R 2 = .079, F9,959 = 9.05, p < .001
Agreement = agreement with agent's moral choice, NFC = Need for Cognition


Table 3 :
3
Results of regression analysis on predicting blame of artificial agents
B
Std.-Error
β
p
95% CI lower bound
95% CI upper bound
(Constant)
60.541
8.692
<.001
43.483
77.600
Dilemma Type
-5.712
1.619
-.106
<.001
-8.888
-2.535
Agreement
-25.296
2.315
-.323
<.001
-29.838
-20.753
Gender
.319
1.810
.006
.860
-3.233
3.871
Age
-.189
.076
-.077
.014
-.339
-.039
Religiosity
2.145
1.042
.063
.040
.101
4.189
NFC
-1.826
1.008
-.059
.070
-3.804
.152
Machiavellianism
-.019
.623
-.001
.976
-1.240
1.203
Psychopathy
-.009
.654
.000
.990
-1.292
1.275
Narcissism
1.568
.576
.096
.007
.438
2.698
ATI
-.211
1.009
-.007
.834
-2.192
1.770
NARS 1
4.101
1.470
.109
.005
1.216
6.985
NARS 2
-.832
1.316
-.024
.528
-3.414
1.751
NARS 3
-1.408
1.162
-.043
.226
-3.689
.873
R 2 = .163, F13,979 = 14.47, p < .001
Agreement = agreement with agent's moral choice, NFC = Need for Cognition, ATI = Affinity for Technology Interaction,
NARS = Negative Attitudes toward (1) Situations and Interactions with Robots, (2) Social Influence of Robots, and (3) Emo-
tions in Interaction with Robots


Table 4 :
4
Results of regression analysis on predicting trust in artificial agents (R 2 = .071)
B
Std.-Error
β
p
95% CI lower bound
95% CI upper bound
(Constant)
5.423
.695
<.001
4.058
6.788
Dilemma Type
-.200
.129
-.050
.121
-.454
.053
Agreement
1.244
.183
.218
<.001
.884
1.604
Gender
.048
.144
.012
.741
-.236
.331
Age
.004
.006
.020
.556
-.009
.016
Religiosity
.122
.084
.048
.146
-.043
.286
NFC
-.050
.080
-.022
.532
-.206
.106
Machiavellianism
-.047
.051
-.040
.353
-.147
.052
Psychopathy
-.070
.052
-.054
.179
-.173
.032
Narcissism
-.002
.047
-.002
.966
-.093
.089
ATI
.005
.081
.002
.954
-.154
.164
NARS 1
-.143
.117
-.052
.222
-.373
.087
NARS 2
.008
.105
.003
.942
-.198
.213
NARS 3
-.208
.092
-.085
.025
-.390
-.027
R
2 = .071, F13,922 = 5.34, p < .001 Agreement = agreement with agent's moral choice, NFC = Need for Cognition, ATI = Affinity for Technology Interaction, NARS = Negative Attitudes toward (1) Situations and Interactions with Robots, (2) Social Influence of Robots, and (3) Emo- tions in Interaction with Robots








Funding
This work was funded by the Deutsche Forschungsgemeinschaft (DFG), Grant CRC 1410.


Conflict of Interest
The authors have no relevant financial or non-financial interests to disclose.
 










Androgenic Morality? Associations of Sex, Oral Contraceptive Use and Basal Testosterone Levels with Moral Decision Making




D
Armbruster






C
Kirschbaum






A
Strobel




10.1016/j.bbr.2021.113196








Behavioural Brain Research




408














Will People Morally Crack under the Authority of a Famous Wicked Robot?




A
M
Aroyo






T
Kyohei






T
Koyama






H
Takahashi






F
Rea






A
Sciutti






Y
Yoshikawa






H
Ishiguro






G
Sandini




10.1109/ROMAN.2018.8525744








27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)


Nanjing, China
















The Moral Machine Experiment




E
Awad






S
Dsouza






R
Kim






J
Schulz






J
Henrich






A
Shariff






J
F
Bonnefon






I
Rahwan




10.1038/s41586-018-0637-6








Nature




563


7729
















Step Aside! Vr-Based Evaluation of Adaptive Robot Conflict Resolution Strategies for Domestic Service Robots




F
Babel






A
Vogt






H
Kraus






J
Angerer






F
Seufert






T
Baumann






M




10.1007/s12369-021-00858-7








International Journal of Social Robotics




14
















Intuitive Moral Judgments Are Robust across Variation in Gender, Education, Politics and Religion: A Large-Scale Web-Based Study




K
Banerjee






B
Huebner






M
Hauser




10.1163/156853710x531186








Journal of Cognition and Culture




10


3-4
















When the Killing Has Been Done: Exploring Associations of Personality with Third-Party Judgment and Punishment of Homicides in Moral Dilemma Scenarios




A
Behnke






A
Strobel






D
Armbruster




10.1371/journal.pone.0235253








PLoS One




15


6














The Evolution of Norms




J
Bendor






P
Swistak




10.1086/321298








American Journal of Sociology




106


6
















People Are Averse to Machines Making Moral Decisions




Y
E
Bigman






K
Gray




10.1016/j.cognition.2018.08.003








Cognition




181
















Need for Cognition: Eine Skala Zur Erfassung Von Engagement Und Freude Bei Denkaufgaben




F
; H
Bjorklund






M
Wänke






G
Bohner






R
F
Fellhauer






N
Schwarz




10.1046/j.1467-9450.2003.00367.x








Differences in the Justification of Choices in Moral Dilemmas: Effects of Gender






44








Zeitschrift für Sozialpsychologie








The Evolution of Altruistic Punishment




R
Boyd






H
Gintis






S
Bowles






P
J
Richerson




10.1073/pnas.0630443100








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






100














Exogenous Testosterone Increases Sensitivity to Moral Norms in Moral Dilemma Judgements




S
M
Brannon






S
Carr






E
S
Jin






R
A
Josephs






B
Gawronski




10.1038/s41562-019-0641-3








Nature Human Behaviour




3


8
















Social Perception of Embodied Digital Technologies-a Closer Look at Bionics and Social Robotics




M
Bretschneider






S
Mandl






A
Strobel






F
Asbrock






B
Meyer




10.1007/s11612-022-00644-7








Gruppe. Interaktion. Organisation. Zeitschrift für Angewandte Organisationspsychologie (GIO)




53
















The Need for Cognition




J
T
Cacioppo






R
E
Petty




10.1037/0022-3514.42.1.116








Journal of Personality and Social Psychology




42


1
















Gender and Attitudes toward Technology Use: A Meta-Analysis




Z
Cai






X
Fan






J
Du




10.1016/j.compedu.2016.11.003








Computers & Education




105
















Gender Differences in Moral Judgment and the Evaluation of Gender-Specified Moral Agents




V
Capraro






J
Sippel




10.1007/s10339-017-0822-9








Cognitive Processing




18


4
















Moral Judgment Reloaded: A Moral Dilemma Validation Study




J
F
Christensen






A
Flexas






M
Calabrese






N
K
Gut






A
Gomila




10.3389/fpsyg.2014.00607








Frontiers in Psychology




5


607














Moral Dilemmas in Cognitive Neuroscience of Moral Decision-Making: A Principled Review




J
F
Christensen






A
Gomila




10.1016/j.neubiorev.2012.02.008








Neuroscience and Biobehavioral Reviews




36


4
















Religion's Profound Influences on Psychology: Morality, Intergroup Relations, Self-Construal, and Enculturation




A
B
Cohen




10.1177/0963721414553265








Current Directions in Psychological Science




24


1
















Deontological and Utilitarian Inclinations in Moral Decision Making: A Process Dissociation Approach




P
Conway






B
Gawronski




10.1037/a0031021








Journal of Personality and Social Psychology




104


2
















The Psychology of Morality: A Review and Analysis of Empirical Studies Published from 1940 through 2017




N
Ellemers






J
Van Der Toorn






Y
Paunov






T
Van Leeuwen




10.1177/1088868318811759








Personality and Social Psychology Review




23


4
















A Personal Resource for Technology Interaction: Development and Validation of the Affinity for




T
Franke






C
Attig






D
Wessel




10.1080/10447318.2018.1456150








Technology Interaction (Ati) Scale International Journal of Human-Computer Interaction




35


6
















What Does Matter? The Case for Killing the Trolley Problem (or Letting It Die). The Philosophical Quarterly




B
H
Fried




10.1111/j.1467-9213.2012.00061.x








62
















R
Friesdorf






P
Conway






B
Gawronski




10.1177/0146167215575731








Gender Differences in Responses to Moral Dilemmas: A Process Dissociation Analysis






41


















M
Fumagalli






R
Ferrucci






F
Mameli






S
Marceglia






S
Mrakic-Sposta






S
Zago






C
Lucchiari






D
Consonni






F
Nordio






G
Pravettoni






S
Cappa






A
Priori


















10.1007/s10339-009-0335-2








Gender-Related Differences in Moral Judgments




11














Science and Scientists Held in High Esteem across Global Publics




C
Funk






A
Tyson






B
Kennedy






C
Johnson








Pew Research Center




29














Consequences, Norms, and Generalized Inaction in Moral Dilemmas: The Cni Model of Moral Decision-Making




B
Gawronski






J
Armstrong






P
Conway






R
Friesdorf






M
Hutter




10.1037/pspa0000086








Journal of Personality and Social Psychology




113


3
















What Makes Moral Dilemma Judgments "Utilitarian" or "Deontological




B
Gawronski






J
S
Beer




10.1080/17470919.2016.1248787








Social Neuroscience




12


6
















Künstliche Intelligenz -Den Ersten Schritt Vor Dem Zweiten Tun!




D
Gesmann-Nuissl








Zeitschrift zum Innovations-und Technikrecht (InTeR)




3
















Vengeance and the Dark Triad: The Role of Empathy and Perspective Taking in Trait Forgivingness




E
A
Giammarco






P
A
Vernon




10.1016/j.paid.2014.02.010








Personality and Individual Differences




67
















Attitudes Towards Social Robots: The Role of Gender, Belief in Human Nature Uniqueness, Religiousness and Interest in Science Fiction




J.-C
Giger






D
Moura






N
Almeida






N
Piçarra








II International Congress on Interdisciplinarity in Social and Human Sciences


Faro, Portugal










Research Centre for Spatial and Organizational Dynamics, University of Algarve












Are All Types of Morality Compromised in Psychopathy




A
L
Glenn






R
Iyer






J
Graham






S
Koleva






J
Haidt




10.1521/pedi.2009.23.4.384








Journal of Personality Disorders




23


4
















The Outlandish, the Realistic, and the Real: Contextual Manipulation and Agent Role Effects in Trolley Problems




N
Gold






B
D
Pulford






A
M
Colman




10.3389/fpsyg.2014.00035








Frontiers in Psychology




5














Cultural Differences in Moral Judgment and Behavior, across and within Societies




J
Graham






P
Meindl






E
Beall






K
M
Johnson






L
Zhang




10.1016/j.copsyc.2015.09.007








Current Opinion in Psychology




8
















Unveiling Robotophobia and Cyber-Dystopianism: The Role of Gender, Technology and Religion on Attitudes Towards Robots




D
Halpern






J
E
Katz








7












ACM/IEEE International Conference on Human-Robot Interaction (HRI)












Comparison of Procedures for the Assessment of Psychopathy




R
D
Hare




10.1037/0022-006x.53.1.7








Journal of Consulting and Clinical Psychology




53


1
















Psychopathy: Assessment and Forensic Implications




R
D
Hare






C
S
Neumann




10.1177/070674370905401202








Canadian Journal of Psychiatry




54


12




















S
Huber






O
W
Huber




10.3390/rel3030710








The Centrality of Religiosity Scale (Crs). Religions




3


3
















Religion-Related Values Differently Influence Moral Attitude for Robots in the United States and Japan




S
Ikari






K
Sato






E
Burdett






H
Ishiguro






J
Jong






Y
Nakawake




10.1177/00220221231193369








Journal of Cross-Cultural Psychology




54


6-7
















Action-Inaction Asymmetries in Moral Scenarios: Replication of the Omission Bias Examining Morality and Blame with Extensions Linking to Causality, Intent, and Regret




J
Jamison






T
Yay






G
Feldman




10.1016/j.jesp.2020.103977








Journal of Experimental Social Psychology




89














The Moral Choice Machine: Semantics Derived Automatically from Language Corpora Contain Human-Like Moral Choices




S
Jentzsch






P
Schramowski






C
Rothkopf






K
Kersting




10.1145/3306618.3314267








Proceedings of the 2nd Aaai/Acm Conference on Ai


the 2nd Aaai/Acm Conference on Ai
















Sidetracked by Trolleys: Why Sacrificial Moral Dilemmas Tell Us Little (or Nothing) About Utilitarian Judgment




G
Kahane




10.1080/17470919.2015.1023400








Social Neuroscience




10


5
















Attitudes Towards Robots Suitability for Various Jobs as Affected Robot Appearance




J
E
Katz






D
Halpern




10.1080/0144929X.2013.783115








Behaviour & Information Technology




33


9
















The Criminal Psychopath: History, Neuroscience, Treatment, and Economics




K
A
Kiehl






M
B
Hoffman








Jurimetrics




51
















The Duke University Religion Index (Durel): A Five-Item Measure for Use in Epidemological Studies




H
G
Koenig






A
Büssing




10.3390/rel1010078








Religions




1


1
















Deontology and Utilitarianism in Real Life: A Set of Moral Dilemmas Based on Historic Events




A
Körner






R
Deutsch




10.1177/01461672221103058








Personality and Social Psychology Bulletin
















Using the Cni Model to Investigate Individual Differences in Moral Dilemma Judgments




A
Körner






R
Deutsch






B
Gawronski




10.1177/0146167220907203








Personality and Social Psychology Bulletin




46


9
















Das Dreckige Dutzend Und Die Niederträchtigen Neun. Kurzskalen Zur Erfassung Von Narzissmus, Machiavellismus Und Psychopathie




A
C P
Küfner






M
Dufner






M
D
Backc




10.1026/0012-1924/a000124








Diagnostica




61


2
















Moral Uncanny Valley: A Robot's Appearance Moderates How Its Decisions Are Judged




M
Laakasuo






J
Palomäki






N
Köbis




10.1007/s12369-020-00738-6








International Journal of Social Robotics




13
















Building Jiminy Cricket: An Architecture for Moral Agreements among Stakeholders AIES'19




B
Liao






M
Slavkovik






L
Van Der Torre




10.1145/3306618.3314257








Honolulu, HI, USA












Does Japan Really Have Robot Mania? Comparing Attitudes by Implicit and Explicit Measures




K
F
Macdorman






S
K
Vasudevan






C.-C
Ho




10.1007/s00146-008-0181-2








AI & Society


23














Integrating Robot Ethics and Machine Morality: The Study and Design of Moral Competence in Robots




B
F
Malle




10.1007/s10676-015-9367-8








Ethics and Information Technology




18
















Ai in the Sky: How People Morally Evaluate Human and Machine Decisions in a Lethal Strike Dilemma




B
F
Malle






S
T
Magar






M
Scheutz




M. I


















Aldinhas
Ferreira






J
Silva






G
Sequeira






Singh






M
O
Virk






Tokhi




10.1007/978-3-030-12524-0_11




Robotics and Well-Being


& E. E. Kadar




Springer International Publishing


95














Sacrifice One for the Good of Many?




B
F
Malle






M
Scheutz






T
Arnold






C
Voiklis






C
Cusimano




10.1145/2696454.2696458








People Apply Different Moral Norms to Human and Robot Agents 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)


Portland, Oregon, USA
















Embodied Digital Technologies: First Insights in the Social and Legal Perception of Robots and Users of Prostheses




S
Mandl






M
Bretschneider






S
Meyer






D
Gesmann-Nuissl






F
Asbrock






B
Meyer






A
Strobel




10.3389/frobt.2022.787970








Frontiers in Robotics and AI




9














Do Psychopathic Individuals Possess a Misaligned Moral Compass? A Meta-Analytic Examination of Psychopathy's Relations with Moral Judgment




J
Marshall






A
L
Watts






S
O
Lilienfeld




10.1037/per0000226








Personality Disorders: Theory, Research, and Treatment




9


1
















Responsibility in Hybrid Societies: Concepts and Terms




S
Meyer






S
Mandl






D
Gesmann-Nuissl






A
Strobel




10.1007/s43681-022-00184-2








AI and Ethics




3
















Artificial Morality. Concepts, Issues and Challenges




C
Misselhorn




10.1007/s12115-018-0229-y








Society




55
















Animal Morality: What It Means and Why It Matters




S
Monsó






J
Benz-Schwarzburg






A
Bremhorst




10.1007/s10892-018-9275-3








Journal of Ethics




22


3
















The Dark Core of Personality




M
Moshagen






B
E
Hilbig






I
Zettler




10.1037/rev0000111








Psychological Review




125


5
















Prediction of Human Behavior in Human--Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes toward Robots




T
Nomura






T
Kanda






T
Suzuki






K
Kato




10.1109/TRO.2007.914004








IEEE Transactions on Robotics




24


2
















Experimental Investigation into Influence of Negative Attitudes toward Robots on Human-Robot Interaction




T
Nomura






K
Takayuki






T
Suzuki




10.1007/s00146-005-0012-7








AI & Society


20














Results: Should a Carebot Bring an Alcoholic a Drink? Poll Says, It Depends on Who Owns the Robot Retrieved








Open Roboethics Institute
















Prolific.Ac -a Subject Pool for Online Experiments




S
Palan






C
Schitter




10.1016/j.jbef.2017.12.004








Journal of Behavioral and Experimental Finance




17
















At the Heart of Morality Lies Neuro-Visceral Integration: Lower Cardiac Vagal Tone Predicts Utilitarian Moral Judgment




G
Park






A
Kappes






Y
Rho






J
J
Van Bavel




10.1093/scan/nsw077








Social Cognitive and Affective Neuroscience




11


10
















Trait Psychopathy and Utilitarian Moral Judgement: The Mediating Role of Action Aversion




I
Patil




10.1080/20445911.2015.1004334








Journal of Cognitive Psychology




27


3
















Reasoning Supports Utilitarian Resolutions to Moral Dilemmas across Diverse Measures




I
Patil






M
M
Zucchelli






W
Kool






S
Campbell






F
Fornasier






M
Calo






G
Silani






M
Cikara






F
Cushman




10.1037/pspp0000281








Journal of Personality and Social Psychology




120


2
















The Dark Triad of Personality: Narcissism, Machiavellianism, and Psychopathy




D
L
Paulhus






K
M
Williams




10.1016/S0092-6566(02)00505-6








Journal of Research in Personality




36


6
















A Treatise on Insanity and Other Disorders Affecting the Mind




J
C
Prichard








Sherwood, Gilbert and Piper












Medical Inquiries and Observations Upon the Diseases of the Mind




B
Rush








Kimber & Richardson












Less Thought, More Punishment: Need for Cognition Predicts Support for Punitive Responses to Crime




M
J
Sargent




10.1177/0146167204264481








Personality and Social Psychology Bulletin




30


11
















Empathy, Morality and Psychopathic Traits in Women




A
Seara-Cardosa






H
Dolberg






C
Neumann






J
P
Roiser






E
Viding




10.1016/j.paid.2013.03.011








Personality and Individual Differences




55


3
















On Gender and Philosophical Intuition: Failure of Replication and Other Negative Results




H
Seyedsayamdost




10.1080/09515089.2014.893288








Philosophical Psychology




28


5
















Looking Forward to Sociable Robots




G
Shaw-Garlock




10.1007/s12369-009-0021-7








International Journal of Social Robotics




1
















A 21 Word Solution




J
Simmons






L
Nelson






U
Simonssohn




10.2139/ssrn.2160588


















Assessing the Attitude Towards Artificial Intelligence: Introduction of a Short Measure in German, Chinese, and English Language




C
Sindermann






P
Sha






M
Zhou






J
Wernicke






H
S
Schmitt






M
Li






R
Sariyska






M
Stavrou






B
Becker






C
Montag




10.1007/s13218-020-00689-0








35








KI -Künstliche Intelligenz








Decision-Making in Everyday Moral Conflict Situations: Development and Validation of a New Measure




N
Singer






L
Kreuzpointner






M
Sommer






S
Wust






B
M
Kudielka




10.1371/journal.pone.0214747








PLoS One




14


4














Need for Cognition as a Moral Capacity




A
Strobel






J
Grass






R
Pohling






A
Strobel




10.1016/j.paid.2017.05.023








Personality and Individual Differences




117


















D
S
Syrdal






K
Dautenhahn






K
L
Koay






M
L
Walters




The Negative Attitudes Towards Robots Scale and Reactions to Robot Behaviour in a Live Human-Robot Interaction Study. Adaptive and Emergent Behaviour and Complex Systems: Proceedings of the 23rd Convention of the Society for the Study of Artificial Intelligence and Simulation of Behaviour


Edinburgh, UK


















Religiosity Enhances Emotion and Deontological Choice in Moral Dilemmas




R
D
Szekely






A
Opre






A
C
Miu




10.1016/j.paid.2015.01.036








Personality and Individual Differences




79
















Moral Fatigue: The Effects of Cognitive Fatigue on Moral Reasoning




S
Timmons






R
M
Byrne




10.1177/1747021818772045








Quarterly Journal of Experimental Psychology




72


4
















Social Motivation, Justice and the Moral Emotions: An Attributional Approach




B
Weiner








Tayler & Francis













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]