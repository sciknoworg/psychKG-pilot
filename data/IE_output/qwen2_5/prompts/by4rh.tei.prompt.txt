You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



INTRODUCTION
People are no longer asking if robots ever will be part of human communities; they are wondering when robots will arrive, and in what roles and contexts. In fact, robots are beginning to appear as service personnel 
[18]
, soldiers 
[16]
, and astronauts 
[33]
, but also as caregivers 
[73]
, teachers 
[14]
, and partners [2, 
21,
49,
71]
. These roles would normally require a blend of risk awareness, social skills, and moral sensitivity. But do robots have these social-moral capacities? And should they have them?
The scientific community has responded in two ways to these questions 
[55]
. On the one hand, they have explored the ethics of designing such robots 
[23,
30,
43,
74,
78]
. Robot ethics has quickly become a burgeoning field, mentioned in 66,796 entries of the ACM Digital Library 
[3]
 as of this writing, 19, 449 since 2020. On the other hand, science and engineering have attempted to actually develop such robots, or moral machines more generally 
[5,
7,
25,
69,
86]
. Machines with social-moral capacities would advance the prospects of robots succeeding in human communities, but the challenges are enormous. The science of machine morality is itself quite young, and the technical demands on such capacities are considerable 
[86]
.
Some scholars warn against designing moral robots, or robots that have significant autonomy and responsibility in human affairs (e.g., 
[79,
85]
). Healthy skepticism is important here, and a conservative position would be viable if we could decide, as a collective, to keep robots out of human affairs-out of schools, hospitals, and private homes. But that position may no longer be available. Robots are already entering those affairs, and our best option is to make these robots as safe, beneficial, and socially and morally appropriate as possible.
Human-robot interaction researchers can take advantage of the slow pace at which moral robots are emerging. They can advance knowledge about people's expectations of moral robots and people's responses to early (yet still fictitious) versions of these robots, which some have called moral HRI 
[59]
. This knowledge must in turn guide the design of moral robots, turning research insights into the conditions under which robots' socially and morally significant actions prove acceptable to human communities.
To conduct this significant research into moral HRI, we need methodological tools to standardize and pool our knowledge. Such tools include validated stimuli and measures, effective procedures, and replicated patterns of results, in addition to standard good practices for empirical methods and open science 
[6,
26,
38,
67,
68]
. In this paper, we offer a set of tools to study one central capacity of future moral robots: to have norm competence and, in particular, to make acceptable decisions when moral norms are in conflict.


BACKGROUND AND GOALS 2.1 Robots with norm competence
In human communities, norms are a central force of regulating people's behavior and will be of critical importance to creating moral robots. Norms constrain individuals' self-interest in favor of the group's interest 
[12,
35]
, they increase the mutual predictability of behavior of both individuals and groups, and, as a result, they foster trust and group cohesion. For autonomous robots to act properly, be trusted and accepted by humans, these agents must be aware of, follow, and prioritize the norms of the community in which they operate-they must have norm competence 
[58]
 1
But even if we succeed in implementing such norm competence in robots, a substantial challenge will inevitably arise: Norms often conflict with one another. Sometimes an answer to a question will either be polite and dishonest or honest and impolite; sometimes being fair requires breaking a friend's expectations of loyalty. Recent literature has identified a number of potential norm conflicts for robots and other artificial agents-from self-driving cars to autonomous military drones, from home assistants to care robots 
[7,
42,
51,
59]
. And the more we see robots take part in human communities and take on significant social roles, the more they will face norm conflicts. Following norms is a minimal requirement for norm competence; being able to resolve norm conflicts-the right way-is going to be a benchmark of norm competence.


Resolving norm conflicts
The primary way for humans to resolve norm conflicts is by deciding to adhere to one norm-the more important one, while violating the other, less important one. This implies that any resolution to a norm conflict involves an inevitable norm violation. Thus, if robots are designed to resolve norm conflicts in a similar way, they will also have to violate some norms some of the time. Moreover, they will have to handle the likely human response to these violations-moral disapproval 
[59,
90]
 and potentially losses of trust 
[39,
40,
76]
.
To design robots that respond appropriately to these challenges we need to better understand how humans respond to robots in these situations, to the norm conflicts t he robots f ace, t he resolutions they choose, the inevitable ensuing norm violations, and the robot's attempts to mitigate negative evaluations and to repair human trust. But we still know relatively little about this range of responses. A few studies have assessed people's perceptions of robots facing norm conflicts 
[37,
47,
51,
59]
; some have examined evaluations of robot norm violations 
[77]
; and researchers have begun to investigate mitigation strategies such as explaining and apologizing 
[15,
27,
48,
81]
. But the many different stimuli, measures, and approaches make the results difficult to compare across studies and hardly generalizable across contexts. We need a systematic framework of scientific methods to discover and investigate human responses to robots' norm conflict resolutions, to the inevitable norm violations, and to robots' mitigation attempts. This is what we offer here.


Research needs and purpose
Interest in discussing scientific research methods, tools, and techniques among the HRI community has been steadily growing. For instance, several recent papers have described primers and best practices for conducting human-subjects studies 
[11,
36,
63,
65,
82]
. The HRI conference has held several workshops on test methods and standards for effective HRI over the past four years 
[61,
62]
. And discussions about the broader movement toward Open Science, originating in the replication crisis of psychology and related human behavior fields, has begun to reach the HRI community 
[10,
83]
.
But in addition to general methodological guides, the research community could benefit from concrete tools to conduct rigorous, replicable, and interpretable research in specific topic areas. Our purpose here is to contribute, first, to the study of robot norm conflict resolution by proposing a series of requirements, principles, and practices; and, second, to share with the community a set of validated testbeds and measures created from our evolving work. We thus hope to illuminate human responses to robot norm conflict resolution, advance the state of the science of moral HRI, and provide a foundation on which others in the community can build and collectively contribute towards moral HRI.


SYSTEMATIC METHODS FOR STUDYING ROBOT NORM CONFLICT RESOLUTION 3.1 Overview
As HRI researchers, we have little power over preventing premature computational design of moral robots. However, we can present insights into human responses to such robots that will select for better design. Our goal must be to accelerate systematic scientific research that illuminates how norm conflicts can be acceptably solved and effectively mitigated. This research could improve robots that are already deployed and guide deployment of future normcompetent robots.
For these methods to be systematic, and hence informative across a range of robot types, time scales, and operational contexts, several principal requirements should be met.
(1) Generalizability: The methods must be usable, hence replicable, by a large number of researchers, for a large number of robots, and for a large number of norm conflict settings and mitigation tools. (2) Control over norm conflict: The methods must ensure that the norm conflicts in the chosen settings are quantifiable, amenable to experimental manipulation, and assessed for theoretically relevant features (such as norms contributing to the conflict and degree of conflict). (3) Diversity of human responses: The methods must be able to assess a range of human responses to robots' norm conflict resolution as well as its mitigation attempts, such as moral evaluation, trust, and future reliance, among others. (4) Capture change in human responses: Methods must employ principled means to capture change in human responses (e.g., before and after the conflict resolution, before and after a mitigation attempt).
In the remaining sections, we discuss each of these principles as they are used in an integrated framework of methods as depicted in 
Figure 1
 and provide recommendations for meeting each of them. We also share validated tools-a stimuli set of moral dilemmas representing norm conflict across a variety of domains-as well as standardized measures of human responses (e.g., moral judgment and trust). We thus hope to contribute to best practices for systematically advancing the state of the science on robot norm conflict resolution and the mitigation of its negative effects for human-robot interaction.
The proposed framework could be expanded to other cases of norm violations, but we focus on those following norm conflict resolution because they constitute a fundamental challenge: Even a perfectly norm-abiding robot will face such conflicts and must trade off one norm against another. And such norm trade-offs will elicit moral judgments and challenges to human-robot trust. That makes the study of human perceptions of robot norm conflicts so important.


Principle 1: Generalizability
The prime stimuli for studying norm conflict resolutions are moral dilemmas. They can be defined as situations in which "a difficult choice has to be made between two courses of action, either of which entails transgressing a moral principle" 
[52]
 (see also 
[50,
54,
66]
. A conflict resolution is then simply defined as making a choice between the contrasting actions. The literature on moral dilemmas has focused on principles such as "deontology" and "utilitarianism" 
[28,
29,
31]
; but people's wildly different responses to dilemmas that allegedly all contrast abstract deontology with utilitarianism 
[20,
31]
 show that the right level of analysis is often not abstract principles but norms specific to contexts (see also 
[32,
45,
72]
).
The context specificity of dilemmas is just as important for artificial agents as for human decision makers. For example, an autonomous robot pilot may need to decide whether to launch a strike aimed at suicide attackers while running the risk of harming civilians. The specifics matter: how many attackers are there, how certain are we about their intentions, how many civilians are involved, how great is the risk to them, how precise is the strike, and much more.
Aside from an emphasis on abstract principles, previous research on moral dilemmas has often focused on life-and-death scenarios, where the two choices in the dilemma involve direct trade-offs between a number of lives saved and a number of lives sacrificed 
[7,
75]
. Although such life-and-death scenarios can happen for robots as well, they are not representative of operational contexts in which robots will confront real-world norm conflicts. For example, in space exploration, robots may make difficult choices between risks to material and mission focus; physical therapy robots may make choices between discomfort to a patient and effectiveness of an exercise; and training robots may have to choose between supporting a trainee's self-confidence and giving direct, accurate performance feedback, especially if that feedback concerns the safety of the performance.
In addition to domain and role variations for agents in dilemmas, other features of norm conflicts may be experimentally manipulated to demonstrate generalizability or to examine certain psychological dimensions of interest. Such features may include the urgency with which the decision must be made, the kinds of norms in conflict (e.g., formal vs. informal norms; conflicting prohibitions vs. conflicting prescriptions), or the strength of the norms in a given community. The point here is not that accumulation of experimental variations of dilemmas per se is desirable but rather that the stimuli chosen for this research endeavor must be flexible enough to accommodate a wide range of variation.
The first principal requirement of our framework is, thus, to offer a wide (and open) range of moral dilemmas, where domains and roles differ and where the norms differ that are in conflict. This approach permits testing for generality vs. context specificity of human responses to the robot's choices and mitigation attempts. People may accept a robot explaining its choice in some domains but demand an apology in others; and they may disapprove of a decision but maintain trust in robots occupying some roles but not others 
[1]
.
Aside from the desired generalizability across domains and roles, we should also highlight the goals of generalizability across researchers and across robots. First, to help generalize across researchers, we have designed, standardized, and validated a range of moral dilemmas that we share here with the HRI community, hoping to jump-start replicable and shareable research. Currently we offer dilemmas in four domains of HRI: military, space exploration, medical, and community services. The dilemmas are available at the following link: https://tinyurl.com/HRIDilemmas and we encourage our research colleagues to add dilemmas from other domains. In Section 4, we describe how these dilemmas were generated, how we validated their status as difficult norm conflicts, and what experimental manipulation to dilemmas can be employed in future experiments. Such an effort exists for moral dilemmas with human agents 
[20]
 but until now, none for dilemmas that are meaningful for both robot and human agents.
Second, to help generalize across robots we have devised the dilemmas in text form, referring to the robot merely by its role name (e.g., robot medical assistant). This is a conservative initial approach, whose experimental realism can be increased over timeby producing orally narrated versions, adding experimentally varied pictures of robots, creating video recordings or AR/VR experiences with a range of different robots, and eventually by designing live experiments. Live experiments are of course a gold standard in the HRI research community, because hypothetical encounters with robots do not necessarily generalize to real encounters with robots. However, in the case of studying robots that do not yet exist-robots with genuine norm competence-every encounter with such a robot (via text, VR, or in the laboratory) must be considered hypothetical. We should also acknowledge that there are serious challenges to conducting HRI studies with human subjects in contexts and tasks that could present psychological or physical risk, which is often precisely what is being traded off in a moral dilemma.
Moreover, studies that use just one particular robot unique to a particular company or research lab (a common situation in current HRI research) pose serious challenges to generalizability. So it might be safer to first examine the phenomena with robots described generically or displayed with experimentally controlled appearance features (e.g., different skins in VR, different features of human-likeness 
[70]
). Such studies can be feasibly replicated across researchers and are compatible with recruiting statistically powerful, large participant samples (e.g., relying on online platforms). These studies, however, can then be used to select the most impactful experimental conditions and test them again in live human-robot interactions.


Principle 2: Control over norm conflict
As noted, the value of moral dilemmas as HRI testbeds lies in their ability to evoke norm conflicts. However, for a moral dilemma to serve this function it must measurably and reliably create a norm conflict 
[50]
. This means (i) the dilemma's choice options are governed by norms (e.g., a prohibition against insulting a person, a prescription to be polite); (ii) whichever choice resolves the dilemma must violate a norm; and (iii) the norms are of comparable strength, pulling similarly toward one or the other choice (a hallmark of true dilemmas). All three conditions are important. If norm governance (i) is missing, we have cases of merely difficult decisions, which involve conflicting goals (a bigger house vs. fewer debts) rather than conflicting norms. If a norm violation (ii) is missing, people will not make any moral evaluations (such as wrongness or blame judgments) but at best assess rationality, intelligence, and the like. Finally, if the strength of norm conflict (iii) is low and one norm strongly dominates the other, we no longer have a norm conflict, thus no longer a dilemma.
Thus, after creating a range of moral dilemmas spanning a variety of real-world HRI contexts, the second principal requirement of our framework is to measure and standardize the degree of norm conflict inherently represented by each of the decisions (i.e., resolutions to the dilemmas). In our stimulus development (see Section 4) we aimed to create dilemmas with only two choices that present norms of approximately equal strength. Ensuring a compelling norm conflict also requires constraining other options for resolving the dilemma aside from the two normative choices, such as waiting, or passing the decision to another agent. (The latter is especially important in the case of robot agents because current robots are unlikely to make autonomous moral decisions.) Finally, the actual strength of norm conflict can be assessed by asking people to estimate the intuitive normative demand of each course of action (before they suggest choosing one) or, better yet, the rate of endorsing one choice over another (and perhaps the time taken for the endorsement; see 
[20]
). Endorsement rates that approach equalitya nearly equal number of people recommend each choice-provide the best evidence for a strong dilemma.


Principle 3: Measuring a diversity of human responses
When robots make socially or morally significant decisions, we can expect a variety of responses from people. When measuring such human responses, we must always ask two critical questions: What construct are we trying to understand? And what measures are available to optimally capture the given construct? We begin with these two questions because constructs are the foundation for translating theory into experimental design 
[82]
. For our studies of robot norm conflict, resolution, and mitigation, we focus on the constructs of moral judgment (e.g., blame), trust (both performance trust and moral trust) 
[60]
, as well as perceptions of robots' capacities 2 . The first two constructs respond to the norm violations inherent in norm conflict resolution and will change when the robot attempts a mitigation. The third construct responds to the robot's ability to make a difficult choice in the first place and to offer a mitigation strategy.
All three constructs are, in fact, multi-faceted or even divide up into more specific constructs. Moral judgments are not one unitary response 
[22,
80]
 but consist of at least four kinds: evaluations (good vs. bad), norm judgments (e.g., permissible, forbidden), wrongness judgments, and blame judgments 
[57]
. Likewise, many measures of trust in robots exist, but most have limited the construct to an attitude of confidence in the robot's reliability and competence. Recent advances in theory suggest that trust in robots is multidimensional, diverging into at least a performance dimension and a relational-moral dimension 
[8,
53,
60]
, and moral trust may be particularly sensitive to a robot's mitigation strategies after normconflict resolutions. Finally, robot capacities are widely seen as multi-dimensional 
[9,
56,
87,
90]
.
For measuring these constructs, it is advisable to create catalogs of measures for each construct (and each facet), compare and validate them, and then standardize the reporting of their use across studies. Standardization does not require the HRI community as a whole to settle on any one, or a handful of common measures. Rather, researchers should detail the verbatim instructions given to participants, display of items, scoring and scaling, as well as their placement in the timeline of a study's experimental procedure. Using validated measures, when available, is preferable, but development of new measures and improvement of existing measures is a continued process that would be hampered by a requirement of fixed, standard measures and metrics. We include in our supplementary materials a range of measures we have used, or are intending to use, in our research on robot norm conflict, resolution, and mitigation.


Principle 4: Capturing change in human responses
In study designs, the repeated-measures principle is promoted as a means to reduce sample size demands and control for individual differences. But a deeper reason for using repeated measures is to actually capture psychological change. Dynamic human-robot interaction fundamentally involves change-across time, contexts, or types of robots. Change is particularly important for the study of norm conflict, resolution, and mitigation, because people often update their perceptions, judgments, or trust as they learn more about the robot and especially about its response to a norm violation. Further, an often overlooked reason for measuring the construct twice is that a single-measure experiment does not, strictly speaking, allow us to interpret whether a given intervention decreased, increased, or maintained a measured psychological state.
To illustrate, consider a random-assignment experiment in which a robot is shown performing a norm-violating action in the experimental group and a neutral action in the control group. People report, as expected, lower levels of trust in the experimental group than in the control group, and we may interpret this result as showing that the norm-violating action decreased people's trust. This is plausible, but is also only one possible interpretation (see 
Figure 2)
.
People could have responded with indifference toward the norm violation (no change in trust) and found the neutral action trustworthy in the context. Or people may have had low trust to begin with, were impressed by the robot's decisive action in both conditions, and increased their trust both after the norm violation (which may have seemed intriguing; cf. 
[84]
 and even more so in the control group. Different researchers' intuitions or theoretical commitments may favor different interpretations. In our example, a norm violation more plausibly decreases trust. But in other cases, such plausibility may be absent. If some novel experimental intervention shows the above pattern of results, we have no a priori reason to favor one or the other interpretations. And we often assume, without evidence, that a control condition is "inert"-that it does not do anything. But especially in HRI studies, that is often incorrect; simply encountering a robot triggers psychological reactions. To address all these issues, one should carefully pretest the experimental and control conditions in advance; but the most decisive approach would be to employ repeated measures-in our examples, trust before and trust after the intervention.
Repeated measures come with challenges, but also with remedies for those challenges. First, people may have a difficult time answering certain questions before they know enough about the setting or object of judgment-e.g., a baseline trust assessment before interacting with a robot may simply not be meaningful. If so, then the baseline measurement could be placed after introductory information or after an initial encounter. Second, answering the same questions twice might sometimes elicit participants' inferences about what the researchers' hypotheses are. In that case, researchers might embed the questions in a variety of other measures or split the measure into two halves (e.g., 6 items each) and counterbalance the before/after presentation of the two halves. Third, presenting a measure in advance of the experimental intervention might change the interpretation of the intervention. If that is a concern, then researchers could administer the initial measure in half of the sample and omit it in the other half. Comparing the effect of the interventions across the two halves would clarify whether such a change of interpretation occurred, and if not, then the pattern of the half that received both measures guides interpretation of the overall results.
In sum, the fourth proposed principle is to include updates to measures as much as possible, for as many of the measured constructs of interest as possible, while keeping in mind pragmatic constraints to this approach (e.g., avoid overloading participants or telegraphing hypotheses).


VALIDATING DILEMMAS FOR GENUINE NORM CONFLICT
In sections 3.2 and 3.3, we discussed the need to offer a wide and open range of moral dilemmas for the study of human responses to robot norm conflict resolution. In the following s ections, we describe the procedures we employed to generate a range of moral dilemmas across a variety of domains for human-robot interaction (Principle 1) that represent genuine norm conflicts (Principle 2). For other researchers to use, we share these validated dilemmas (including all of their iterations) in the Supplementary Materials which can be found at the following URL: https://tinyurl.com/HRIDilemmas. Our aim is for these to serve as a standardized set of tools and testbeds. We also invite researchers to create additional moral dilemmas, using the principles in our framework as a guide, for studying a range of research questions relevant to the field.


Generating candidate dilemmas
We began by identifying common application domains for assistive, social, and military robots in the near future, relying on road maps 
[19]
 and other documents describing areas of need for robots 
[89]
. We especially attended to the health domain, where significant challenges have been forecast and where well-known norm conflicts and ethical dilemmas exist 
[4]
. From these procedures, we identified medical, military, space exploration, and community service as initial domains of interest (though future expansions into other domains will of course be valuable). We explicitly omitted the transportation domain (e.g., self-driving cars) where the research literature has already provided a plethora of dilemmas (e.g., 
[7]
). After identifying application domains, we drafted several narratives about a robot encountering a moral dilemma and used the following criteria to guide editing and refining:
• Strive for plausibility of (near-future) robot involvement and decision making in the dilemma. • Constrain the length of the dilemma to ensure readability for a wide range of human participants in both online and laboratory studies. • Constrain the dilemma so that no technical knowledge is needed for participants to understand the robot's decision making or to provide a judgment. • Ensure that the dilemma describes two norms in opposition to one another and phrase the decision choices in the dilemmas as representing these competing norms. • Constrain the details of the dilemma to eliminate opt-out choices-alternatives that the robot could pursue to resolve the dilemma aside from the norm-conflicting choices (e.g., asking for outside help).
Through pilot testing on smaller samples we refined the narratives to optimally meet these criteria (see 
Supplementary Materials)
. We thus generated eight candidate dilemmas across military, medical, space, and community domains, representing Principle 1 in our systematic methods framework. Further, we drafted equivalent human versions of the dilemmas to serve as comparisons.


Experimental procedures
The next step was to conduct an experimental human-subjects study to evaluate the strength of the competing norms in each dilemma, heeding Principle 2 in our methods framework. We examined the proportion of participants who endorsed each of the choices to resolve the dilemma, reasoning that scenarios in which participants endorsed each choice roughly equally would represent genuinely norm-conflicting dilemmas.
4.2.1 Participants. We aimed to recruit approximately 100 participants per dilemma across both human and robot versions for a planned total sample of = 1, 600, using the crowdsourcing platform Prolific (www.prolific.co). These cell sizes per dilemma enabled greater than 90% statistical power (at two-sided ℎ = .05) to detect significant deviations from an equal split of at least 33:67 (because we wanted to identify such weaker norm conflicts) and 80% power to detect differences of 20 percentage points or more between human and robot endorsement rates. A total of 1, 573 participants entered the study (self-reported in a free response text box: 740 Females, 739 Males, 55 Unreported, 19 Non-binary, 7 Gender Queer, 3 Transgender, 4 Agender, 2 Fluid, 1 Gender Non-conforming, 1 Third Gender, 1 Two Spirit, 1 They) with ages ranging from 18 to 79 years ( = 35.9, = 12.6). To screen out invalid participants we included two "bot checks" before participants provided informed consent. One item required the correct answer to the multiple-choice question, "What day of the week is one day before Wednesday?", the other required following the instruction to "Please refrain from writing in the [free response] box below. " If participants failed either of these checks, they did not enter the main study. In addition, we used prolific IDs to identify participants who had completed the study more than once and retained only the first instance of their responses. Results reported below are based on a final sample of = 1, 541 participants with 92 to 100 participants per dilemma and agent.


Design and procedure.
For each of the eight dilemmas, one version featured a robot agent and a parallel version featured a human agent. In a fully between-subjects design, these 16 dilemmas were randomly assigned to participants. An example dilemma (Hunger Strike) is given below; all dilemmas are included in the supplementary materials.
The male inmates at a federal maximum-security prison have gone on a hunger strike to protest their living conditions. They claim overcrowding and chronic mistreatment by prison guards.
After weeks of forgoing food, one of the convicts, Lee, is so nutrientdeprived that he may die without immediate nutritional intake. A robot medic, who works the night shift on the medical station, knows that sedating Lee and feeding him through a nasogastric tube would extend Lee's life.
Days ago, however, Lee put in writing that he was firmly committed to his hunger strike and would refuse the nasogastric tube. But he is too weak now to communicate whether, so close to dying, he would rather be saved by the medical procedure.
No supervisor can be reached at this hour, so the robot medic must decide whether to sedate and feed Lee or do nothing and assume he wants to die.
What should the robot do? (Feed Lee, Not feed Lee)
All study materials were presented in Qualtrics. After passing bot checks and giving informed consent, participants read their assigned moral dilemma narrative. The text was revealed piecemeal across pages, and text from the prior page persisted until the end. This approach increases readability and prevents participants from merely glossing over the text. The critical dependent variable was presented at the end: "What should the [agent] do?" Participants selected one of the two courses of action. Finally, participants were asked to report their age and gender, and to provide feedback about our procedures by recording their responses to the prompt, "Please leave us any honest feedback. "
All study procedures were approved by the authors' institutional review boards, and participants were compensated approximately $0.80 ($12/hour) in return for their participation. The study took an average of four minutes to complete.


Results and discussion
For each dilemma, and each agent (human, robot) version, we computed the proportion of participants who selected each of the two norm-conflicting options to resolve the dilemma, along with 99% confidence intervals around these population parameter estimate for the endorsement of each norm in the dilemma. Next we used the JASP software package 
[41]
 to perform Bayesian binomial analyses, establishing whether, for any given dilemma, there is evidence specifically for the "null hypothesis" of no difference in the rate of endorsement between the two courses of action-which indicates a strong norm conflict. A frequentist approach does not provide evidence for a null hypothesis.
This analysis returns "Bayes Factors, " which are odds ratios representing how probable the given data are under one hypothesis over another, which is interpreted as the relative evidence for or against a given hypothesis. A 01 = 10, for example, would suggest that the given data are ten times more likely to have been observed under the null hypothesis, 0 , than under the alternative hypothesis, 1 . Bayes Factors above 3.0 are interpreted as evidence in favor of a hypothesis 
[24]
. Bayes Factors between 0.33 and 3.0 are typically viewed as inconclusive or "anecdotal. " Bayes Factors below 0.333 are generally viewed as indicating at least moderate evidence against a hypothesis 
[24]
. 
Figure 3
 depicts the proportion of participants who endorsed the described norm in each of the moral dilemmas across human and robot agent types, and the 99% confidence interval around each proportion. Four dilemmas had near equal rates of endorsement for the norm-conflicting choices across robot and human agents. And for each, the binomial analysis had Bayes Factors > 3, suggesting evidence in favor of the null hypothesis that participants endorsed each of the norms of the dilemma in roughly equal proportions. These four (Hospital contamination, Drone Operations, Hunger Strike, and Blood Transfusion) represent genuine norm conflicts both for human and robot agents and can serve as a standard set for future research.
Two dilemmas (Mars Mission, Do Not Resuscitate) had endorsement proportions that were too extreme to be considered strong norm conflicts (i.e., Bayes Factors < 0.33, suggesting evidence against the null hypothesis). However, rephrasing some of their content and retesting them may render them suitable in the future.
The robot version of the remaining two dilemmas (Lung Transplant and Food Bank) returned endorsement ratios of 3 : 2, and Bayes Factors suggested weak evidence in favor of the null hypothesis. Here, too, rephrasing the content may strengthen the relevant norm conflict, but additional revisions would be necessary to bring the human results closer to the robot results.
Alternatively, some researchers may want to select the asymmetric dilemmas to create particularly demanding tests of robot mitigation strategies. If a robot chooses a course of action that many people reject, the robot would have to offer a compelling moral communication to mitigate people's disapproval or loss of trust.


GENERAL DISCUSSION AND FUTURE WORK
The purpose of this paper was to meet the need and calls within our community for systematic and replicable methods to advance moral HRI. Rather than simply demand that our community meet these requirements, we offer foundational tools for them to do so, rooted in principles for conducting studies that can guide both theory and practice. The examples we have used to illustrate this framework have focused on studying human responses to the significant challenges surrounding robot resolutions to norm conflicts. We recognize that understanding norm conflict resolutions alone will not be sufficient to create moral HRI. However, the proposed principles should be useful beyond just the study of norm conflict, by striking a balance between specific recommendations and general recommendations applicable to other topics within moral HRI. For instance, it will be important to also understand human responses to robots that refuse to engage in norm violating commands 
[13]
, that attempt to address or rebuke human norm violation 
[91]
, or that engage in moral communications to encourage norm compliance (see 
[46,
88]
), to name a few. One or more of the principles in the present framework should apply to conducting experimental studies of human responses to these topics. The principles also apply to other topics related to robots that will be fielded in the near term. For instance, the same procedures can be used to study human responses to deceptive robots, or robots that have been designed to use "dark patterns" of interaction 
[34,
64]
. These patterns refer to purposefully deceptive or manipulative techniques to coax people into long term engagement (e.g., to advance the economic motivations of the companies that make them). Emulating the proposed framework can serve as a starting point for the rigorous study of these topics: Beginning with a range of deceptive scenarios, ensuring and controlling the amount of deception in each, and capturing a range of human responses to that deception are all valuable guidelines and can further advance moral HRI.
Another goal of this work was to share with the community validated stimuli-in the form of moral dilemmas-that represent a wide range of robot roles and domains. We know that people are sensitive to the specific norms that define the context of individual moral dilemmas and resolve these dilemmas based on this context 
[30,
40,
58]
. Thus, to understand human responses to how robots resolve moral dilemmas, those dilemmas need to both clearly depict norms in conflict and also represent enough contexts to be able to make broad statements about human responses on the whole. To meet this need we created a range of moral dilemmas with competing norms, and have quantified and controlled the norm conflict in each of these dilemmas. We have also provided the first moral dilemmas that represent equivalent conflict meaningful for both human and robot agents. Further, we invite other researchers 


Dilemma
Focal Action 
Figure 3
: Proportion of participants who endorsed the described focal action in each moral dilemma across robot (above) and human (below) agents, and 99% confidence intervals around each proportion.
to not only use these dilemmas and modify them as needed for their research questions, but also to add dilemmas that represent other domains, agent types, or even presentation formats (e.g., mixed or virtual reality) we have not yet covered. Finally, although our dilemmas currently exist as narratives, an additional merit of validating the norm conflict within them is that they can now aid researchers in selecting the most impactful experimental conditions to test in live, virtual, or other in-person human-robot interaction studies. Interestingly, in the medical domain, robots are already deployed 
[44]
, have been identified as a means to address significant future challenges 
[19]
, and will face some of the wide-ranging medical ethics dilemmas 
[4]
. Thus, the likelihood of robots encountering moral dilemmas similar to the ones we have presented here could be high.


LIMITATIONS
Although we hope that the proposed framework will advance the science of creating norm competent robots, there are limitations to this framework. One is that, as a framework, it does not provide direct insights into the phenomena of interest; it is primarily a guide and organizing tool. Hence, it is a promise at this time, and its success will not be seen until a large number of research studies has accumulated generalizable knowledge that fosters the design of future robots. Another limitation is that the presented moral dilemmas have been validated for their norm conflict strength, but we have not yet assessed them for other features: the types of norms involved (e.g., protecting autonomy, being supportive, saving lives), the urgency of the conflict, and the like. A final limitation is that the proposed principles are derived from experimentation and measurement goals commonly used in quantitative research methods, focused on causal mechanisms and relationships between constructs. The principles do not reflect the goals of qualitative research methods common to other subdisciplines in our community (e.g., user interviews, ethnographic methods). We do not intend to imply that we have described the only way to advance the state of knowledge and practice in our field. As an interdisciplinary endeavor, HRI needs room for the contributions from many different scientific perspectives and standards. However, for researchers who aim to uncover the causes of phenomena and to document generalizable patterns of thought and behavior, specific and systematic experimental methods are needed to accomplish those goals.
Figure 1 :
1
Four principles for the study of norm conflict resolution, applied to an experimental procedure.


Figure 2 :
2
Multiple interpretations of a finding in which an experimental group (E, left) shows lower average trust ratings than a control group (C, right). Under interpretation I, E shows decreased trust; under II, E shows maintained trust, and under III, E shows increased trust, but not as much as C.


Hospital Contamination Drone Operations Hunger Strike Blood Transfusion Lung Transplant Food Bank Mars Mission DNRProportion Choice to act (vs. Inaction) Agent Robot Human Endorsement Rate Hospital Contamination Drone Operations Hunger Strike Blood Transfusion Lung Transplant Food Bank Mars Mission DNRProportion Agent Robot Human Drone Operations Hunger Strike Blood Transfusion Lung Transplant Food Bank Mars Mission DNRProportion Choice to act (vs. Inaction) Agent Robot Human Hospital Contamination Drone Operations Hunger Strike Blood Transfusion Lung Transplant Food Bank Mars Mission DNRProportion Choice to act (vs. Inaction) Agent Robot Human
Divert gas
Divert gas
Divert gas Divert gas
Launch strike
Launch strike
Launch strike Launch strike
Feed Lee
Feed Lee
Feed Lee Feed Lee
orm transfusion
Perform transfusion
Perform transfusion Perform transfusion
Follow score
Follow score
Follow score Follow score
an to take food
Allow man to take food Allow man to take food Allow man to take food
Return to base
Return to base
Return to base Return to base
uscitate patient
Resuscitate patient
Resuscitate patient Resuscitate patient
40% 50% 60% 70% 80%
40% 50% 60% 70% 80% 40% 50% 60% 70% 80% 40% 50% 60% 70% 80%


We draw no categorical distinction between "moral" and "social-conventional" norms. Whether there is such a distinction or not has been debated; for our purposes, the key point is that communities demand of their members to obey a variety of norms, and they are likely to demand the same of robots.


Other constructs are plausibly important as well, such as emotional reactions to the robot (e.g., sympathy, aggression) or a willingness to help the robot improve its norm competence
[17]
 








ACKNOWLEDGMENT
This works is supported by the Air Force Office of Scientific Research award number FA9550-21-1-0359. The views expressed in this paper are those of the authors and do not reflect those of the U.S. Air Force, Department of Defense, or U.S. Government.












11/26/2022




What is Gatebox? -Gatebox




















ACM. 2022. ACM Digital Library












Who says you're dead? Medical & ethical dilemmas for the curious & concerned




Jacob
M
Appel








Algonquin Books, Chapel Hill, North Carolina












Moral decision making in autonomous systems: Enforcement, moral emotions, dignity, trust, and deception




Ronald
C
Arkin






Patrick
Ulam






Alan
R
Wagner




10.1109/JPROC.2011.2173265








Proc. IEEE


IEEE






100
















B
Jens






Mark
Asendorpf






Filip
De
Conner






Jan
De
Fruyt






Houwer






J
A
Jaap






Klaus
Denissen






Susann
Fiedler






Fiedler






C
David






Reinhold
Funder






Brian
A
Kliegl






Nosek




Recommendations for increasing replicability in psychology
















Jean-François Bonnefon, and Iyad Rahwan




Edmond
Awad






Sohan
Dsouza






Richard
Kim






Jonathan
Schulz






Joseph
Henrich






Azim
Shariff








Nature




563










The moral machine experiment








Forms and frames: mind, morality, and trust in robots across prototypical interactions




Jaime
Banks






Kevin
Koban






Philippe
Chauveau




10.30658/hmc.2.4








Human-Machine Communication




2


1














Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots




Christoph
Bartneck






Dana
Kulić






Elizabeth
Croft






Susana
Zoghbi




10.1007/s12369-008-0001-3








International Journal of Social Robotics




1


1
















From characterising three years of HRI to methodology and reporting recommendations




Paul
Baxter






James
Kennedy






Emmanuel
Senft






Séverin
Lemaignan






Tony
Belpaeme




10.1109/HRI.2016.7451777








11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)


















Advice to new human-robot interaction researchers




Tony
Belpaeme








Human-Robot Interaction




Springer
















The grammar of society: The nature and dynamics of social norms




Cristina
Bicchieri








Cambridge University Press


New York, NY












Why and how robots should say




Gordon
Briggs






Tom
Williams






Ryan
Blake
Jackson






Matthias
Scheutz




10.1007/s12369-021-00780-y








International Journal of Social Robotics




14


2
















2020. Teachers, the robots are coming




K
Bushweller








7






But that's not a bad thing








Adriel Chua, Ee Jing Loh, and James Law. 2021. The effect of socialcognitive recovery strategies on likability, capability and trust in social robots




David
Cameron






Emily
C
Stevienna De Saille






Jonathan
M
Collins






Hugo
Aitken






Cheung








Computers in human behavior




114


106561














Chinese PLA Deploys Machine Gun Wielding Robots Near Indian Border; Will Robotic Warriors Change The Battles Of Future? Eurasian Times


Amit Chaudhery. 2022
















Instruct or evaluate: how people choose to teach norms to social robots




Vivienne
Bihe






Chi
Bertram F Malle








2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE
















The Rise of Service Robots in the Hospitality Industry: Some Actionable Insights




Sungwoo
Choi






Wan








Boston Hospitality Review


















Henrik
Christensen






Nancy
Amato






Holly
Yanco






Maja
Mataric






Howie
Choset






Ann
Drobnis






Ken
Goldberg






Jessy
Grizzle






Gregory
Hager






John
Hollerbach








A roadmap for us robotics-from internet to robotics 2020 edition






8














Moral judgment reloaded: A moral dilemma validation study




Julia
F
Christensen






Albert
Flexas






Margareta
Calabrese






Nadine
K
Gut






Antoni
Gomila




10.3389/fpsyg.2014.00607








Frontiers in Psychology




5














Sex Doll Sales Surge In Quarantine, But It's Not Just About Loneliness




Franki
Cookney














Accessed on 11/26/2022








Crime and punishment: Distinguishing the roles of causal and intentional analyses in moral judgment




Fiery
Cushman




10.1016/j.cognition.2008.03.006








Cognition




108
















The rise of the robots and the crisis of moral patiency




John
Danaher








ai & Society




34
















Using Bayes to get the most out of non-significant results




Zoltan
Dienes








Frontiers in psychology




5


781














Robots: ethical by design




Dodig
Gordana






Baran
Crnkovic






Çürüklü








Ethics and Information Technology




14
















Business not as usual




Eric
Eich




















Do you still trust me? humanrobot trust repair strategies




Connor
Esterwood






Robert








2021 30th IEEE International Conference on Robot & Human Interactive Communication


RO-MAN




IEEE
















The problem of abortion and the doctrine of double effect




Philippa
Foot








Oxford Review




5
















Consequences, norms, and generalized inaction in moral dilemmas: The CNI model of moral decision-making




Bertram
Gawronski






Joel
Armstrong






Paul
Conway






Rebecca
Friesdorf






Mandy
Hütter




10.1037/pspa0000086








Journal of Personality and Social Psychology




113
















Building moral robots: Ethical pitfalls and challenges




John-Stewart
Gordon








Science and engineering ethics




26
















An fMRI investigation of emotional engagement in moral judgment




Joshua
D
Greene






R
Brian
Sommerville






Leigh
E
Nystrom






John
M
Darley






Jonathan
D
Cohen




10.1126/science.1062872








Science




293


5537














A moral trade-off system produces intuitive judgments that are rational and coherent and strike a balance between conflicting moral values




Ricardo
Andrés Guzmán






María
Teresa
Barbato






Daniel
Sznycer






Leda
Cosmides




10.1073/pnas.2214005119








Publisher: Proceedings of the National Academy of Sciences






119


2214005119












A review of NASA human-robot interaction in space




Kimberly
Hambuchen






Jessica
Marquez






Terrence
Fong








Current Robotics Reports




2


3
















The dark side of human-robot interaction: ethical considerations and community guidelines for the field of HRI




S
Kerstin






Michael
Misha
Haring






Paul
Novitzky






Ewart J De
Robinette






Alan
Visser






Tom
Wagner






Williams








14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE
















Social norms. Russell Sage Foundation


Michael Hechter and Karl-Dieter Opp






New York, NY












A primer for conducting experiments in human-robot interaction




Guy
Hoffman






Xuan
Zhao








ACM Transactions on Human-Robot Interaction (THRI)




10
















Should moral decisions be different for human and artificial cognitive agents




Evgeniya
Hristova






Maurice
Grinberg








Proceedings of the 38th Annual Conference of the Cognitive Science Society


the 38th Annual Conference of the Cognitive Science Society
Austin, TX


















Sharing and reporting the results of clinical trials




L
Kathy






Francis S
Hudson






Collins








Jama




313
















Robot: Asker of questions and changer of norms




Ryan
Blake






Jackson






Tom
Williams








Proceedings of ICRES
















Language-capable robots may inadvertently weaken human moral norms




Ryan
Blake






Jackson






Tom
Williams








14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE




















Jasp Team








Jasp










Version 0.8. 0.0. software








The social dilemma of autonomous vehicles




Bonnefon
Jean-François






Shariff
Azim






Rahwan
Iyad








Science




352
















The morality of autonomous robots




M
Aaron






Sidney
Johnson






Axinn








Journal of Military Ethics




12
















Hospital Robots Are Helping Combat a Wave of Nurse Burnout | WIRED




Khari
Johnson




















On the wrong track: Process and content in moral psychology




Guy
Kahane




10.1111/mila.12001








Mind & Language




27






Blackwell












Robots as moral advisors: The effects of deontological, virtue, and confucian role ethics on encouraging honest behavior




Boyoung
Kim






Ruchen
Wen






Qin
Zhu






Tom
Williams






Elizabeth
Phillips








Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction


















Blaming the reluctant robot: parallel blame judgments for robots in moral dilemmas across US and Japan




Takanori
Komatsu






F
Bertram






Matthias
Malle






Scheutz








Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction


the 2021 ACM/IEEE International Conference on Human-Robot Interaction


















Trust repair in human-agent teams: the effectiveness of explanations and expressing regret




Esther S Kox






H
José






Kerstholt






F
Tom






Peter W De
Hueting






Vries








35








Autonomous agents and multi-agent systems








Human-dog relationships as a working framework for exploring humanrobot attachment: a multidisciplinary review




Frank
Krueger






C
Kelsey






Gopikrishna
Mitchell






Jeffrey S
Deshpande






Katz








Animal Cognition




24
















Moral dilemmas




Øyvind
Kvalnes




10.1007/978-3-030-15191-1_2








Moral Reasoning at Work: Rethinking Ethics in Organizations, Øyvind Kvalnes


Cham




Springer International Publishing
















Moral psychology of nursing robots: Exploring the role of robots in dilemmas of patient autonomy




Michael
Laakasuo






Jussi
Palomäki






Anton
Kunnari






Sanna
Rauhala






Marianna
Drosinou






Juho
Halonen






Noora
Lehtonen






Mika
Koverola






Marko
Repo






Jukka
Sundvall






Aku
Visala






Kathryn
B
Francis




10.1002/ejsp.2890








European Journal of Social Psychology
















Oxford Languages






n. d.












Oxford
Languages






Google
















Trust: Recent concepts and evaluations in human-robot interaction




Theresa
Law






Matthias
Scheutz




10.1016/B978-0-12-819472-0.00002-2


B978- 0-12-819472-0.00002-2








Trust in Human-Robot Interaction


Chang S. Nam and Joseph B. Lyons




Academic Press
















Varieties of moral issue and dilemma: a framework for the analysis of case material in business ethics education




Patrick
Maclagan




10.1023/B:BUSI.0000004364.63317.73








Journal of Business Ethics




48
















Integrating robot ethics and machine morality: the study and design of moral competence in robots




F
Bertram






Malle








Ethics and Information Technology




18
















How many dimensions of mind perception really are there?




F
Bertram






Malle








Proceedings of the 41st Annual Meeting of the Cognitive Science Society


E. K. Goel, C. M. Seifert, and C. Freksa


the 41st Annual Meeting of the Cognitive Science Society
Montreal, Canada


















Moral judgments




F
Bertram






Malle




10.1146/annurev-psych-072220-104358








Annual Review of Psychology




72
















Requirements for an artificial agent with norm competence




F
Bertram






Paul
Malle






Matthias
Bello






Scheutz








Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society


the 2019 AAAI/ACM Conference on AI, Ethics, and Society


















Sacrifice one for the good of many? People apply different moral norms to human and robot agents




Matthias
Bertram F Malle






Thomas
Scheutz






John
Arnold






Corey
Voiklis






Cusimano








10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE
















A multidimensional conception and measure of human-robot trust




F
Bertram






Daniel
Malle






Ullman




10.1016/B978-0-12-819472-0.00001-0


B978- 0-12-819472-0.00001-0








Trust in Human-Robot Interaction


Chang S. Nam and Joseph B. Lyons




Academic Press
















Novel and Emerging Test Methods and Metrics for Effective HRI




A
Jeremy






Shelly
Marvel






Megan
Bagchi






Murat
Zimmerman






Brian
Aksu






Xiang
Antonishek






Yue
Li






Ross
Wang






Terry
Mead






Heni
Fong






Ben Amor








Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction


















Test methods and metrics for effective HRI in collaborative human-robot teams




A
Jeremy






Shelly
Marvel






Megan
Bagchi






Murat
Zimmerman






Brian
Aksu






Yue
Antonishek






Ross
Wang






Terry
Mead






Heni
Fong






Ben Amor








14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)






















IEEE
















Introduction to the Special Issue on Test Methods for Human-Robot Teaming Performance Evaluations




A
Jeremy






Shelly
Marvel






Megan
Bagchi






Murat
Zimmerman






Brian
Aksu






Yue
Antonishek






Ross
Wang






Terry
Mead






Heni
Fong






Ben Amor








ACM Transactions on Human-Robot Interaction (THRI)




11
















Dark patterns at scale: Findings from a crawl of 11K shopping websites




Arunesh
Mathur






Gunes
Acar






J
Michael






Eli
Friedman






Jonathan
Lucherini






Marshini
Mayer






Arvind
Chetty






Narayanan








Proceedings of the ACM on Human-Computer Interaction




3






CSCW












A Framework to Explore Proximate Human-Robot Coordination




Sachiko
Matsumoto






Auriel
Washburn






Riek








ACM Transactions on Human-Robot Interaction
















Moral dilemmas




Terrance
Mcconnell










The Stanford Encyclopedia of Philosophy


N. Zalta and Uri Nodelman










Metaphysics Research Lab, Stanford University












Journals unite for reproducibility




Marcia
Mcnutt




10.1126/science.aaa1724








Science




346






American Association for the Advancement of Science












Promoting transparency in social science research




Edward
Miguel






Colin
Camerer






Katherine
Casey






Joshua
Cohen






Kevin
M
Esterling






Alan
Gerber






Rachel
Glennerster






P
Don






Macartan
Green






Guido
Humphreys






Imbens








Science




343
















Is it possible to program artificial emotions? A basis for Behaviours with moral connotation?




Luís
Moniz Pereira






António Barata
Lopes








Machine Ethics




Springer
















What is human-like?: Decomposing robots' human-like appearance using the anthropomorphic robot (abot) database




Elizabeth
Phillips






Xuan
Zhao






Daniel
Ullman






Bertram F
Malle








13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE
















Perceptions of Infidelity with Sex Robots




Nina J Rothstein






H
Dalton






Ewart J De
Connolly






Elizabeth
Visser






Phillips








Proceedings of the 2021


the 2021
















ACM/IEEE International Conference on Human-Robot Interaction
















Morally irrelevant factors: What's left of the dual processmodel of moral cognition?




Hanno
Sauer




10.1080/09515089.2011.631997








Philosophical Psychology




25
















Robots rise to meet the challenge of caring for old people




Neil
Savage








Nature




601
















Moral robots




Matthias
Scheutz






Bertram F Malle








The Routledge handbook of neuroethics. Routledge


















May machines take lives to save lives? Human perceptions of autonomous robots (with the capacity to kill)




Matthias
Scheutz






Bertram
F
Malle




J. Gaillot, Derek Macintosh, and J. D. Ohlin






Oxford University Press




Oxford, UK






In Lethal autonomous weapons: Re-examining the law & ethics of robotic warfare








I Don't Believe You": Investigating the Effects of Robot Trust Violation and Repair




Sarah
Strohkorb Sebo






Priyanka
Krishnamurthi






Brian
Scassellati








14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)




IEEE
















Attributions of morality and mind to artificial intelligence after real-world moral violations




B
Daniel






Alyssa
Shank






Desanti




10.1016/j.chb.2018.05.014








Computers in Human Behavior




86
















Can robots be responsible moral agents? And why should we care?




Amanda
Sharkey








Connection Science




29
















Can we program or train robots to be good?




Amanda
Sharkey




10.1007/s10676-017-9425-5








Ethics and Information Technology




22


000581950700002




Springer












The disunity of morality




Walter
Sinnott-Armstrong




10.1093/acprof:oso/9780199357666.003.0015




Moral Brains, S. Matthew Liao






Oxford University Press














Taxonomy of trust-relevant failures and mitigation strategies




Suzanne
Tolmeijer






Astrid
Weiss






Marc
Hanheide






Felix
Lindner






M
Thomas






Clare
Powers






Myrthe L
Dixon






Tielman








Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction


the 2020 ACM/IEEE International Conference on Human-Robot Interaction


















The power of theory




Paula
Gregory Trafton






Sangeet
Raymond






Khemlani








ACM Transactions on Human-Robot Interaction (THRI)




10
















Challenges and opportunities for replication science in hri: A case study in human-robot trust




Daniel
Ullman






Salomi
Aladia






Bertram F
Malle








Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction


the 2021 ACM/IEEE International Conference on Human-Robot Interaction


















Smart human, smarter robot: How cheating affects perceptions of social agency




Daniel
Ullman






Lolanda
Leite






Jonathan
Phillips






Julia
Kim-Cohen






Brian
Scassellati










Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






36














Critiquing the reasons for making artificial moral agents




Aimee
Van Wynsberghe






Scott
Robbins




10.1007/s11948-018-0030-8








Science and Engineering Ethics




25


















Wendell
Wallach






Colin
Allen




Moral machines: Teaching robots right from wrong




Oxford University Press














Rethinking people's conceptions of mental life




Kara
Weisman






Carol
S
Dweck






Ellen
M
Markman




10.1073/pnas.1704347114








Proceedings of the National Academy of Sciences of the United States of America




114
















Comparing strategies for robot communication of role-grounded moral norms




Ruchen
Wen






Boyoung
Kim






Elizabeth
Phillips






Qin
Zhu






Tom
Williams








Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction


















Combating COVID-19-The role of robotics in managing public health and infectious diseases




Guang-Zhong
Yang






Bradley
J
Nelson






Robin
R
Murphy






Howie
Choset






Henrik
Christensen






Steven
H
Collins






Paolo
Dario






Ken
Goldberg






Koji
Ikuta






Neil
Jacobstein












eabb5589 pages








Perceived agency of a social norm violating robot




Shannon
Yasuda






Devon
Doheny






Nicole
Salomons






Sarah
Strohkorb
Sebo






Brian
Scassellati










Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society
















Blame-laden moral rebukes and the morally competent robot: A Confucian ethical perspective




Qin
Zhu






Tom
Williams






Blake
Jackson






Ruchen
Wen








Science and Engineering Ethics




26

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]