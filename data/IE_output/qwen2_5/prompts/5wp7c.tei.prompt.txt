You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



ROC research and applies these principles to establish a set of guidelines for memory ROCs. Theoretical demonstrations show that creating ROCs based on likelihood or diagnosticity ratios (DRs) provides a general method for tasks that are not amendable to traditional methods. The demonstrations also establish an alternative interpretation for area under the curve (AUC) for an ROC that connects the observed operating points with straight lines, reinforcing the value of this measure for eyewitness research and revealing other potential applications. Although DR-based ROCs are theoretically justified, parameter recovery simulations show that creating lineup ROCs with observed sample DRs leads to biased estimation of AUC. We propose a new ROC method to correct for this bias, whereby the observed data are fit with a decision model, and ROCs are constructed based on the DRs from the model. Finally, we show how a DR-based ROC can be used to generate a full theoretical ROC for lineup tasks, and note that these theoretical curves reveal a previously unappreciated property of lineup decision making: changing response conservativeness changes the evidence distributions that determine lineup responses.


A theory-based approach for constructing recognition Receiver Operating Characteristics (ROCs) in complex tasks, with an application to full lineup ROCs
Recognition tasks are a mainstay of both basic and applied memory research. In a simple recognition task, the participant is asked to indicate whether or not a stimulus (e.g., a word) was previously seen in a specified context (e.g., a study list) by responding "old" or "new", respectively.
Researchers often analyze results using a Receiver Operating Characteristic (ROC), a curve that relates the proportion of correct "old" responses (hits) to the proportion of erroneous "old" responses (false alarms) as responding varies from conservative (reluctant to respond "old") to liberal (reluctant to respond "new"; e.g., 
Banks, 1970;
Egan, 1958;
Lockhart & Murdock, 1970)
. If participants are asked to indicate their level of confidence in each response, then a set of ROC points can be created by systematically changing the confidence levels that are included in the calculation of hit and false alarm rates, from most conservative (only the highest confidence "old" responses) to most liberal (everything except the highest confidence "new" responses).
Numerous models of the memory and decision processes that support old-new recognition tasks have been developed, including process models that specify storage and retrieval mechanisms (e.g., 
Hintzman, 1984)
, decision models that specify distributions of retrieval states and the rules for mapping them to overt responses (e.g., 
Tanner & Swets, 1954;
Lockhart & Murdock, 1970)
, and evidenceaccumulation models that specify how retrieval unfolds over time (e.g., 
Ratcliff, 1978)
. These models are often able to closely match empirical accuracy, confidence, and response time (RT) distributions, and they serve as invaluable analytical tools for investigating the various memory and decision mechanisms that underlie empirical phenomena (e.g., 
Ratcliff, Thapar, & McKoon, 2004;
Verde & Rotello, 2003)
.
Researchers frequently rely on decision models to interpret ROC data; for example, the area under a model-based ROC provides a measure of memory acuity that is stable across changes in response thresholds 
(Macmillan & Creelman, 2005)
. This measure, commonly referred to as area under the curve (AUC), represents the ability to discriminate old and new items. Researchers have also devised a number of non-parametric approaches for ROC analyses, such as calculating the area under a piecewise-linear ROC that connects observed ROC points with straight lines 
(Macmillan & Creelman, 2005)
.
ROC methods are extremely well established in some domains, such as old-new recognition tasks with a confidence scale (e.g., 
Wixted, 2007)
. However, researchers are still developing a set of guidelines for more advanced applications, such as ROCs generated from response time (RT) distributions 
(Thomas & Myers, 1972;
Weidemann & Kahana, 2016)
 and ROCs from eyewitness lineup tasks 
(Mickes, Flowe, & Wixted, 2012;
Smith, Yang, & Wells, 2020)
. There is currently a lively debate about the role of ROC methods in eyewitness research 
(Lampinen, Smith, & Wells, 2019;
Smith et al., 2020;
Wells, Smalarz, & Smith, 2015;
Wixted, Mickes,Wetmore, Gronlund, & Neuschatz, 2017)
. Thus, to ground the paper in a specific application, we focus on this domain, although the same basic principles can be applied across a variety of complex decision tasks.


Lineup Tasks
In a typical lineup task, the witness (or participant) views a perpetrator, either as a photo or in a video of a simulated crime 
(Wells & Olson, 2003)
. Later, the witness is presented with a lineup consisting of a suspect, who may or may not be innocent, and a number of fillers, who are known to be innocent.
The witness is asked to either identify the perpetrator, or, if they believe the perpetrator is not present, to reject the lineup. The lineup members may either be shown at the same time as a simultaneous lineup or one at a time as a sequential lineup. The possible witness responses are a suspect identification (ID), a filler ID, or a rejection. Each response can be made at different levels of confidence. An alternative to a lineup procedure is a showup, for which the suspect is presented without fillers.
Eyewitness researchers have been exploring lineup data with ROCs for roughly a decade, following a seminal publication by 
Mickes et al. (2012)
. The most common approach to measure lineup performance is to evaluate partial ROCs. Partial ROCs only consider a subset of the potential responses, for example, a partial ROC might include suspect IDs at each level of the confidence scale, but omit filler IDs and rejections. 
Smith et al. (2020)
 recently advocated the use of full ROCs that use all of the witness responses, including filler IDs and rejections at each confidence level. This approach introduces a number of conundrums that go beyond established methods for recognition ROC research, and as such, it presents an excellent opportunity to gain a deeper understanding of ROC principles.
One challenge associated with full lineup ROCs is how to order the ROC points. ROC points are properly ordered from most conservative to most liberal. The proper order is obvious in a standard oldnew task. For example, calculating hit and false alarm rates using only high-confidence "old" responses is more conservative than using both high-and medium-confidence "old" responses, which, in turn, is more conservative than using high-, medium-, and low-confidence "old" responses. Full lineup ROCs include multiple types of responses (suspect IDs, filler IDs, and rejections) at a range of confidence levels, making the relative conservativeness of different options potentially ambiguous. For example, it is unclear whether low-confidence filler IDs or low-confidence rejections should be considered a more conservative response. 
Smith et al. (2020)
 suggested that full ROC points should be ordered by diagnosticity ratio (DR), that is, the relative likelihood of observing a given response for lineups with guilty and innocent suspects 
(Clark, 2012)
. When a certain type of response (e.g., a high-confidence suspect ID) has a high DR, this indicates that witnesses are much more likely to make the response when the suspect is guilty than when the suspect is innocent. Thus, responses with higher DRs provide stronger evidence of suspect guilt than responses with lower DRs, suggesting that DRs might provide a reliable guide to for ordering ROC points. 
Yang and Smith (2020)
 proposed a variety of strategies for obtaining DRs, such as simply using the observed DRs of the current sample, using a fixed order, or using results from previous studies.
Although these methods have been applied to empirical data sets, they have not undergone a rigorous theoretical evaluation to determine whether ordering ROC points by DRs is consistent with the basic principles of ROC construction and whether particular strategies for defining DRs support accurate AUC estimates.
A second challenge associated with full lineup ROCs is how to create a continuous curve. For standard old-new tasks, a number of models are available to fit continuous ROCs to a set of observed points, and signal detection theory (SDT) models are the most popular option 
(Wixted, 2020)
.
Researchers almost never use piecewise-linear ROCs for old-new tasks, as the AUC is influenced by a variety of response biases that are independent of memory discriminability 
(Macmillan & Creelman, 2005
). To our knowledge, no one has proposed a method for fitting model-based curves to a full ROC, so existing studies have relied on piecewise-linear curves. Thus, researchers need to understand the differences between model-generated and piecewise ROCs to correctly interpret full ROCs, and new techniques are needed to define model-generated full ROCs.


Overview
This paper has two goals. The first goal is to expound the theoretical principles of ROC construction and interpretation that are relevant for current lineup ROC research. The second goal is to apply these principles to generate a set of guidelines for creating and interpreting memory ROCs in general and lineup ROCs specifically.
We first address a number of open questions surrounding the use of full lineup ROCs, using a simple example to illuminate basic principles. Specifically, we define the relationship between the ROC and diagnosticity ratios (DRs), explain the implications of ordering an ROC based on DRs, and explore differences in interpretation for model-generated and piecewise-linear ROCs. Demonstrations show that creating an ROC based on DR is theoretically sound and ensures that the ROC represents performance under an optimal decision rule. Moreover, we show that this practice offers a general method of ROC creation that can be used for both model-based and piecewise-linear ROCs, even for complex tasks that are not amenable to ROCs based on memory match strength. We also propose a new framework for interpreting piecewise-linear ROCs that clarifies their role in eyewitness memory research and reveals other ways that they could be used by memory researchers.
We next consider full-lineup ROCs through the lens of the principles established in the first section. We discuss potential problems with using sample diagnosticity ratios to order ROC points, and we review a signal detection model of lineup performance 
(Clark, 2003;
Cohen, Starns, Rotello, & Cataldo, 2020;
Wixted, Vul, Mickes, & Wilson, 2018)
 that can be used to help understand these problems and evaluate strategies for solving them. Using model predictions to define population-level diagnosticity ratios, we test whether the order of diagnosticity ratios is likely to be consistent across different studies.
To simulate variation in performance from one study to the next, we change parameters representing three aspects of performance: the collective ability of witnesses to discriminate guilty and innocent suspects, the level of conservativeness that witnesses adopt in making identification decisions, and the policy for mapping between memory retrieval outcomes and reported confidence levels. We find that the order of diagnosticity ratios is highly sensitive to these factors, and we use the model to explain why changing performance parameters can change the relative diagnosticity of different responses.
We also report parameter recovery simulations to explore different methods for estimating AUC for full lineup ROCs. We compare ROCs constructed from sample DRs 
(Smith et al., 2020)
 to a novel, model-based method. Specifically, we fit a lineup signal detection model to each data set and used the DRs in the model to order the ROC points. Results show that using sample DRs leads to biased estimation of AUC values, especially for small sample sizes. In contrast, the model-based method supports accurate estimation. Thus, our results show that decision models will play a central role in creating and interpreting full lineup ROCs; indeed, we contend that models are as important for this research program as they have been for ROC research using a standard old-new recognition task (e.g., 
Ratcliff, Sheu, & Gronlund, 1992;
Wixted, 2007)
.
We then explain how a continuous, model-generated ROC can be defined for full lineup ROCs using a lineup signal-detection model. To our knowledge, this will constitute the first method for generating a theoretical full lineup ROC. This method can be generalized to other complex memory tasks.
We compare theoretical full ROCs across different levels of response conservativeness to illuminate what these curves represent and how they should be interpreted. A key lesson in analyzing these theoretical functions is that changing the response criterion in a lineup task doesn't just move a response cutoff on a set of fixed distributions, as in old-new recognition, but actually changes the evidence distributions that determine the lineup responses.
We then compare the full ROC to a recently proposed measure of lineup performance based on information theory, Expected Information Gain 
(Starns, Cohen, and Rotello, 2021)
. We end with a set of recommendations for ROC creation and interpretation.


Basic Concepts in ROC Construction and Interpretation
This section uses the relatively simple example of old/new discrimination of targets (i.e., old stimuli) and lures (i.e., new stimuli) in a standard recognition task to establish principles that guide the 
Figure 1
. A) ROC points and model-generated continuous curve for a hypothetical old-new recognition experiment with four confidence levels; B) Distributions of responses used to create the ROC points in Panel A. construction and evaluation of ROCs. We focus on the relationship between diagnosticity ratios and the ROC, the logic behind ordering ROC points by diagnosticity ratio, and the interpretation of AUC for both model-generated and piecewise-linear ROCs. For simplicity, we use an equal-variance signal detection model to define model-generated ROCs. 
Figure 1A
 shows ROC points constructed from a hypothetical recognition data set with four confidence levels, where Confidence Level 1 is "Definitely New" (e.g., definitely not studied) and


Using Confidence to Define ROC Points
Confidence Level 4 is "Definitely Old" (e.g., definitely studied). The distributions of responses used to create these ROC points are shown in 
Figure 1B
. The first (leftmost) ROC point in 
Figure 1A
 only considers Confidence Level 4; thus, the hit rate is .50 (i.e., .50 of targets received a rating of 4) and the false-alarm rate is .25 (i.e., .25 of the lures received a rating of 4). The second point includes both Confidence Levels 3 and 4, resulting in a hit rate of .75 (.5 + .25) and a false-alarm rate of .5 (.25 + .25).
The third point includes Confidence Levels 1, 2, and 3, resulting in a hit rate of .9 (.5 + .25 + .15) and a false-alarm rate of .75 
(.25 + .25 + .25)
. In a standard old/new recognition experiment, including all four confidence levels results in both hit and false alarm rates of 1, so that point is often omitted from the plot.


Model-Based ROCs
A model is often used to define a continuous ROC for a set of ROC points, and signal detection theory is an extremely popular approach for this task 
(Wixted, 2020)
. A signal detection model is shown in 
Figure 2A
. The x-axis represents the memory match strength, i.e., the extent to which a currently considered item matches memory for the study list. Because these match strengths are assumed to be noisy, they are represented by Gaussian distributions. The distributions of memory match strengths for targets and lures in 
Figure 2A
 are displayed with solid and dashed lines, respectively. Match strengths tend to be higher for targets than lures, represented in the model as a higher mean for the former distribution.
A response is determined by sampling a memory match strength value from the appropriate distribution and comparing that value to a set of response criteria. For four confidence levels, there are three response criteria, e.g., the blue, green, and red lines in 
Figure 2A
. A value below the left (blue) criterion results in a response at Confidence Level 1, a value between the left (blue) and middle 
(green)
 criteria results in a response at Confidence Level 2, and so on. Shifting the criteria to the left or right 
Figure 2
. A) Equal-variance signal-detection model of old-new recognition with four confidence regions defined by three criteria; B) Diagnosticy ratio for each memory match strength for the model in Panel A; C) Log diagnosticy ratio for each memory match strength for the model in Panel A; D) ROC generated from the model in Panel A. See text for details. results in responding that is more liberal (i.e., old responses are relatively common) or more conservative (i.e., old responses are relatively rare), respectively.
In this particular version of the model, the target and lure distributions have equal variances. This equal-variance signal detection model is the basis for d', a common performance measure representing the ability to discriminate between old and new items 
(Macmillan & Creelman, 2005)
. To account for data from standard recognition tasks, signal detection models typically need to assume that memory strength values are more variable for targets than lures (e.g., 
Ratcliff et al., 1992)
. The equal-variance model, however, is often adequate for eyewitness studies (e.g., 
Cohen et al., 2020)
. This difference might come about because eyewitness studies often use a different witness for every identification trial; thus, individual differences across trials inflate the variability of both the target (guilty suspect) and lure (innocent suspect) distributions, attenuating the variability ratio.
A model-generated ROC ( 
Figure 2D
) can be formed by sweeping a theoretical response criterion across the memory match strength distributions from the far right to the far left of the continuum, i.e., from very conservative to very liberal 
(Macmillan & Creelman, 2005)
. For any given response criterion, the hit and false alarm (FA) rates are given by the area under the target and lure distributions greater than the criterion, respectively. ROC points near the "beginning" of the curve (FA rate=0, hit rate=0) represent a response policy in which the memory match strength must be extremely high to justify an "old" response, whereas points near the "end" of the curve (FA rate=1, hit rate=1) represent a response policy in which even a weak memory match strength justifies an old response. The three points on the ROC in 
Figure 2D
 are associated with the three response criteria in 
Figure 2A
, where the higher criteria are associated with lower ROC points. 
Figure 1A
 demonstrates how a model-generated ROC can be used to summarize a set of empirical ROC points. The displayed curve was generated from the equal-variance signal-detection model with the mean of the target distribution and the location of the response criteria optimized to best fit the empirically generated ROC points.


The Diagnosticity Ratio (DR)
A DR (or likelihood ratio; 
Figure 2B
) is the relative likelihood that a target versus a lure would produce a given match strength; that is, it is the ratio of the target and lure distribution densities at that strength. Because DRs are compressed below 1 ( 
Figure 2B
), it is often helpful to use the log diagnosticity ratio ( 
Figure 2C
; 
Clark, 2012)
, which is symmetric around 0.
The horizontal lines in 
Figure 2B
 and C show the DRs and log DRs for the three criteria values in 
Figure 2A
. Because the middle (green) criterion value in 
Figure 2A
 lies at the intersection of the distributions, it marks an "indifference point," where the memory match strength is equally likely to be produced by a target or lure. The green dashed lines in Panels B and C show that this criterion value corresponds to a DR of 1 and a log DR of 0. The right (red) criterion is placed at a higher memory match strength that provides moderate evidence in favor of an old decision. The likelihood of experiencing this match strength is a little over 3 times higher for targets than lures, corresponding to a DR just over 3 (Panel B) and a log DR just over 1 (Panel C). The left (blue) criterion is placed at a lower memory match strength value that provides moderate evidence in favor of a new response. The likelihood of experiencing this memory match strength is a little over 3 times higher for lures than targets, corresponding to a DR just under 1/3 and a log DR just under -1.
As discussed in detail in a later section, the ROC in 
Figure 2D
 could also be generated by sweeping a criterion for the DR or log DR from the top to the bottom of 
Figure 2B
 or C. At each DR criterion, the hit (false-alarm) rate is the probability that a target (lure) would meet or exceed the DR criterion. For this example, this strategy is equivalent to creating a curve by sweeping a criterion over the match strength continuum, because there is a monotonic relationship between strength and DR (see 
Figure 2B
 or C). Some memory researchers have proposed that people base recognition decisions on likelihood ratios instead of "raw" strength values (e.g., 
Glanzer, Hilford, & Maloney, 2009;
Osth, Dennis, & Heathcote, 2017)
. Imagining that the response criterion varies along a DR continuum is consistent with these accounts, but more importantly for our purposes, it also provides a method for constructing theoretical ROCs when the decision variable does not lie on a single continuum.
There is an interesting and useful correspondence between the diagnosticity ratio and the modelgenerated ROC. In particular, the diagnosticity ratio at any given memory match strength is the derivative of the model-generated ROC at the corresponding ROC point. In other words, the diagnosticity ratiothat is, the extent to which a given strength value is more typical of the target than the lure distributiondetermines the relative rate at which the ROC is "moving vertically" (changing hit rate) relative to "moving horizontally" (changing false-alarm rate) when the criterion is swept past that strength value. This relationship is illustrated in 
Figure 2D
 by the tangent lines that intersect the ROC points associated with the three response criteria. For example, the right (red) criterion is in a region more densely populated by targets than lures, so hit rates change at approximately three times the rate of false alarm rates, corresponding to a slope of approximately 3 at the leftmost ROC point. The middle (green) criterion is at a strength value that is equally typical of targets and lures, so hit and false alarm rates change at the same rate, corresponding to a slope of 1 at the middle ROC point. The left (blue) criterion is in a region more densely populated by lures than targets, so false alarm rates change at approximately three times the rate of hit rates, corresponding to a slope of approximately 1/3 at the rightmost ROC point. Thus, DRs and ROCs are inherently related.


Ordering ROC Points by Diagnosticity and AUC
As previously noted, area under the curve (AUC) is a common measure of memory acuity 
(Macmillan & Creelman, 2005)
, where higher values indicate better performance. More precisely, AUC is the percent correct that could be achieved in a forced-choice task, where the decision maker is asked to choose the target from target-lure pairs 
(Macmillan & Creelman, 2005)
. To link the ROC to forced-choice responding, note that, when faced with a forced-choice trial, the decision maker would reasonably choose whichever item would enter the ROC first. That is, if Item 1 would be considered a target (i.e., called "old") at a more conservative criterion than Item 2, then Item 1 provides stronger evidence of being a target and would be the natural choice in a head-to-head comparison. If decisions are based on match strength, then this means that the decision maker chooses the item with the higher match strength on each forced-choice trial. If decisions are based on diagnosticity, then the decision maker chooses the item with the higher DR. As an example, the AUC for the model-generated ROC in 
Figure 1A
 is .68, which indicates that a decision maker with access to the underlying memory match strength distributions assumed by the model could make correct forced-choice decisions on 68% of trials. Another way to think about this result is that there is a 68% chance that a randomly selected target produces stronger evidence for being a target than a randomly selected lure. 1
To achieve a high AUC, the ROC needs to reach high hit rates while the false alarm rate is still low; that is, the curve needs to quickly move up before it moves over to the right. Indeed, the highest possible AUC value (i.e., 1) requires that the hit rate goes to 1 while the false-alarm rate is still 0.
Achieving this requires that there is no overlap in the evidence distributions, that is, every target produces stronger evidence than the strongest lure (see 
Chen, Starns, & Rotello, 2015
, for some examples of empirical ROCs with a hit rate of 1 across all false-alarm rates).
Together with the previously noted link between DR and ROC slope, this property of ROCs provides a strong theoretical justification for creating an ROC by varying a DR criterion. Using a DR criterion means that more conservative responding -i.e., a response policy that produces a lower falsealarm rate -is achieved by requiring a higher DR, corresponding to a higher ROC slope. Thus, points farther to the left of the ROC will have a higher slope than points farther to the right, meaning that the hit rate will increase as quickly as possible at the "beginning" of the ROC when false-alarm rates are low. This ordering therefore achieves the maximum AUC possible for a given set of evidence distributions; in other words, it represents the optimal performance that can be achieved with access to the evidence distributions.


Strength vs. Diagnosticity Ratio Criteria
In the example that we have worked with so far, the same ROC could be generated by sweeping a criterion across either the strength continuum or the DR continuum, making it difficult to appreciate the difference between the two. To reinforce the unique properties of using a DR criterion, we now consider a simple example that decouples strength and DR and clarifies the difference between an optimal and suboptimal decision maker. The example will consider a categorization task as a way to starkly differentiate ROCs created with different decision rules. Specifically, we envision an experiment in which participants are challenged to learn that stimuli near the middle of a continuum tend to belong to a target category whereas stimuli far from the middle tend to belong to a lure category, with the continuum defined by a single dimension, such as line length. 
Figure 3A
 shows potential distributions of perceived line length for 
Figure 3
. A) Distributions of target and lure category members as a function of perceived line length in a hypothetical categorization experiment; B) ROC generated from the distributions in Panel A using a decision rule based on perceived line length; C) Log DR as a function of perceived line length for the distributions in Panel A; D) Regions defined by the log DR criteria from Panel C; E) ROC generated from the distributions in Panel D using a decision rule based on the log DR in Panel C. See text for details. stimuli in the target and lure categories. Target lengths are less variable than lure lengths, and thus target stimuli are more likely to have perceived lengths near the middle of the range.
Because participants are initially unaware of the best strategy for distinguishing the categories, they will likely apply misguided decision rules in the early phases of the experiment. For example, a participant might incorrectly assume that lines in the target category tend to be longer than lines in the lure category. Thus, particularly long lines will get a "target" response even when responding is conservative. If this participant were asked to respond to a forced-choice trial, the participant would pick whichever line was longer.
An ROC for this participant can be created by sweeping a criterion across the line length continuum from long (conservative) to short (liberal) as illustrated by the two criteria in 
Figure 3A
. 
Figure 3B
 shows the resulting ROC. The red (green) X shows the ROC point associated with the red (green) criterion in Panel A. For conservative responding, more of the lure distribution lies above the criterion than the target distribution, which is why the ROC dips below the chance line on the left side of 
Figure 3B
. For liberal responding, more of the target distribution lies above the criterion than the lure distribution, which is why the ROC crosses above the chance line on the right side of 
Figure 3B
. Overall, AUC is .5, a clear indication that this is a sub-optimal decision maker who is using a misguided decision rule. In a forced-choice task, the decision maker would be correct only half of the time because there is only a 50% chance that a line from the target category will be longer than a line from the lure category. 
Figure 3C
 shows the relationship between perceived line length and the log DR, revealing the expected pattern that lines near the middle of the continuum are more diagnostic of the target category (log DR above zero) and lines near the extremes of the continuum are more diagnostic of the lure category (log DR below 0). A participant who has seen enough trials to learn this category structure might respond based on the DR of the target and lure line length distributions. For this decision rule, conservative responding means that the decision maker will make a "target category" response only for lines that are very close to the center of the continuum, whereas liberal responding means that the decision maker will make a "target category" response even for lines that are far from the center of the continuum. Specifically, 
Figure 3D
 illustrates the regions on the line length continuum that exceed the different log DR criteria shown in Panel C. Whereas the more conservative criterion (red) ascribes a region near the peak of the target distribution between the two red lines, the more liberal criterion (green) ascribes a wider region between the two green lines. For a given DR criterion, the proportion of the target and lure distributions between the two corresponding lines on the length continuum give the hit and falsealarm rates, respectively. 
Figure 3E
 shows the ROC generated by varying a DR criterion from the top to the bottom of the DR continuum in Panel C. AUC is now .7, which is the forced-choice accuracy that can be achieved by a decision maker who always chooses the item with a higher DR. Using a decision rule based on DR means that the decision maker always makes the response that is more likely to be correct, leading to optimal performance.
Returning to recognition memory, memory match strength and DR will naturally be closely related, so one might conclude that distinguishing strength-based and DR-based ROCs is trivial. We concede this point for standard old-new recognition, but understanding the differences between different methods for constructing ROCs is important for advanced applications. Full lineup ROCs are a great example that we discuss in detail below, but creating ROCs with a DR criterion could play an important role in other areas as recognition researchers explore tasks that combine multiple types of responses (e.g., 
Meyer-Grant & Klauer, 2021;
Starns, Dube, & Frelinger, 2018)
 and consider ROCs based on alternative variables, such as response time (e.g., 
Weidemann & Kahana, 2016)
.
In summary, this section showed that creating an ROC by varying a DR criterion is the appropriate method if one wants AUC to represent the optimal discriminability between targets and lures supported by the evidence distributions, i.e., the forced-choice percent correct that can be achieved by an optimal decision-maker with access to the evidence distributions. This section also showed that ROCs are not uniquely determined by evidence distributions; instead, they are determined by combining the evidence distributions with a decision rule.


Interpreting the Piecewise-linear ROC
This section addresses the relationship between model-based and piecewise-linear curves. Smith et al. 
2020
defined the full-lineup ROC by connecting ROC points with straight lines, and this practice has been used more generally in an attempt to obtain a model-free estimate of the ability to discriminate targets and lures 
(Macmillan & Creelman, 2005)
. 
Macmillan and Creelman (2005)
 used the label A g to refer to the area under the piecewise-linear ROC, and they argued that A g is a poor measure of discriminability because it is affected by response biases. The area under the piecewise-linear ROC is also a component of A' 
(Pollack & Norman, 1964
), another popular model-free measure of discriminability that has also been criticized for being sensitive to response bias 
(Verde, Macmillan, Rotello, 2006)
. In this section, we re-consider the interpretation of the piecewise-linear AUC, with the goal of determining whether or not it is a useful measure.
Consider the piecewise-linear ROC shown in 
Figure 4
. The ROC points are the same as those in 
Figure 1
, but they are now connected by straight lines rather than a model-based curve. As before, this piecewise-linear ROC represents performance in a hypothetical recognition experiment with a confidence scale from 1 ("Definitely New") to 4 ("Definitely Old").
We previously showed that the slope of a model-generated ROC is directly linked to the DR at a particular strength value. This close relationship between diagnosticity and slope also holds for a piecewise-linear ROC. For example, ratings of 4 were 2 times more likely for targets than lures (.50/.25; 
Figure 1B
), so the slope leading from (0,0) to the first ROC point is 2. Ratings of 3 were equally likely for targets and lures (.25/.25), so the slope between the first and second ROC points is 1. Ratings of 2 were 0.60 times more likely for targets than lures (.15/.25), so the slope between the second and third ROC points is 0.60. Finally, ratings of 1 were 0.40 times more likely for targets than lures (.10/.25), so the slope leading to (1,1) is .40. More generally, the slope of the line leading to each new ROC point is determined by the DR of the latest response included in the calculation of the hit and false-alarm rates.
Any response that is more common for targets than lures will produce a slope above 1, and any response that is more common for lures than targets will produce a slope below 1.
If the slope of each line segment corresponds to a DR, then A g must relate to the overall diagnosticity of a response distribution. Indeed, A g can be interpreted as the forced-choice percent correct that can be achieved when the discrete distributions of recognition responses are used as a source of evidence. Consider a hypothetical scenario in which two people attempt to discriminate targets and lures with access to different sources of information. Observer M (for "memory") participates in a standard oldnew recognition task and is asked to respond to individual items on a confidence scale of 1-4 based on their memory for the study list. Observer R (for "response") is shown the same targets and lures, this time in target-lure pairs, and is asked to choose the target. Importantly, Observer R did not see the study list and thus cannot respond based on memory; instead, Observer R must base their decisions on the trial-bytrial responses provided by Observer M. For Observer R, the optimal decision process is to select the item associated with the response that is more diagnostic of targets, and in most recognition experiments, that would simply be the item with the rating that is closer to the "definitely old" end of the confidence scale.
When the two items in a test pair have the same response from Observed M, Observer R must guess, and thus has probability .50 of selecting the target. Now, we show that the probability with which Observer R selects the target is equal to the area under the piecewise-linear ROC, or Ag. As shown in 
Figure 4
, the area under the piecewise-linear ROC can be segmented into a series of rectangles and triangles. Each of these areas corresponds to the joint probability of Observer R observing a particular target-lure comparison and making the correct response.
Taken together, the sum of these areas provides the overall probability that Observer R makes a correct response.
For example, consider the case in which both the target and lure were rated 4 by Observer M. The probability of observing such a trial and guessing correctly is represented by the area of Triangle T 4 in 
Figure 4
. The base and height of the triangle correspond to the probability that a lure (.25) and a target (.50) have a rating of 4, respectively. Multiplying these values gives the probability of observing a trial on which both target and lure had ratings of 4, .125 (.25×.50). Dividing this probability in half then gives the joint probability of observing such a trial and correctly guessing which item is the target, .0625 (.5×.25×.50), which is also the area of triangle T 4 . Similarly, the probabilities of observing trials on which the target and lure are both rated 3, 2, and 1 and guessing correctly are represented by the areas of Triangles T 3 (.0313=.5×.25×.25), T 2 (.0188=.5×.15×.25), and T 1 (.0125=.5×.10×.25), respectively.
Observer R can do better than guess if the target is rated higher than the lure. For example, consider a trial on which the target is rated 4 and the lure is rated 3, represented by Rectangle R 3 . The base of R 3 is the probability that a lure has a rating of 3 (.25) and the height is the probability that a target has a rating of 4 (.50). Multiplying these probabilities -or, equivalently, taking the area of the rectanglegives the probability of a trial with a lure rating of 3 and a target rating of 4 (.125=.25×.50). All such trials result in a correct response when Observer R is responding optimally. Similarly, the probability of a trial on which the lure is rated 2 and the target is rated higher than 2 (i.e., 3 or 4) is given by the area of Rectangle R 2 (.1875=.25×(.25+.50)), and the probability of a trial on which the lure is rated 1 and the target is rated higher than 1 (e.g., 2, 3, or 4) is given by the area of Rectangle R 1 (.225=.25×(.15+.25+.50)).
In summary, each shape in 
Figure 4
 represents a different way that Observer R could correctly select the target on a forced-choice trial, and the area of each shape is the corresponding probability.
Adding the areas across all shapes gives the forced-choice proportion correct for Observer R; that is, the optimal performance that can be achieved with access to the discrete response distributions that were used to define the ROC points. Thus, AUC has the same fundamental interpretation for model-based and piecewise-linear ROCs -that is, it is an index of the degree to which the evidence distributions can be used to discriminate targets and lures. In both cases, constructing the ROC based on DRs ensures that AUC represents the discriminability that can be achieved with an optimal decision rule. The only difference is that AUC for a model-based ROC is the optimal performance that can be achieved with access to the underlying evidence distributions assumed by the model and AUC for a piecewise-linear ROC is the optimal performance that can be achieved with access to the discrete response distributions.


The Effect of Decision Strategy on Piecewise-Linear ROCs
This section presents examples to further clarify the difference between model-generated and piecewise-linear ROCs using the concepts established in the previous section. As noted by others, AUC for a piecewise-linear ROC (A g ) can change across decision strategies, i.e., how the underlying evidence distributions are mapped onto the discrete response distributions 
(Macmillan & Creelman, 2005)
. The examples in this section reinforce the point that variability across decision strategies makes A g an unreliable measure of the optimal performance that can be achieved with access to the underlying evidence distributions assumed by a model, but the examples also show that A g remains a reliable measure of the optimal performance that can be achieved with access to the discrete response distributions. 
Figure 5A
 shows the model-based and piecewise-linear ROC, and associated ROC points, generated from the equal-variance signal detection model in 
Figure 5B
. Whereas the AUC for the modelbased ROC is .86, the AUC for the piecewise-linear ROC (A g ) is .84. The AUC decreases because information is lost when the continuous evidence states are mapped onto a discrete response distribution.
For example, consider the task of determining which of a target-lure pair, both rated 4, is the target. With access to the discrete ratings alone, the only strategy is to guess -it is impossible to systematically discriminate targets and lures from the ratings alone. If the memory match strength for each item in the target-lure pair is known, however, above-chance performance is possible because the memory match strength of the target will typically be higher than that of the lure. This distinction is true for all other ratings; that is, memory match strength values support above-chance responding, even when target and lure receive the same confidence rating. In short, basing forced-choice decisions on discrete confidence ratings, rather than continuous memory match strength values, impairs performance.
Changing decision strategies, i.e., changing the locations of the decision criteria, can exacerbate this information loss. The ROCs in 
Figure 5C
, and associated generating model in 
Figure 5D
, illustrate a case in which the decision criteria are tightly grouped. In this situation, high-confidence responses are 
Figure 5
. A,C,E) Model-based (solid) and piecewise-linear (dashed) ROCs for the SDT models in Panels B, D, F, respectively; B, D, F) SDT models and response criteria used to generate the ROCs in Panels A, B, C, respectively. very common and low-confidence responses are very rare. This decision strategy exaggerates the AUC difference between the model-generated ROC, .86, and the piecewise-linear ROC, .78. Responding with high confidence on the vast majority of trials detracts from the optimal performance that can be achieved with access to the underlying evidence distributions, as it functionally eliminates the ability to differentiate accurate high-confidence responses from more error prone low-confidence responses. In practical terms, overuse of high-confidence ratings increases the proportion of trials for which the target and the lure have the same response -either a high-confidence old response or a high-confidence new response -necessitating a guess.
The ROCs in 
Figure 5E
, and associated generating model in 
Figure 5F
, illustrate that an overall response bias can also exacerbate the information loss involved when mapping evidence states to discrete responses. This decision strategy involves a conservative response bias; that is, old responses are relatively rare. Compared to 
Figure 5A
, the AUC difference between the smooth ROC, .86, and piecewise-linear ROC, .79, is again magnified. Because the identification threshold is high, highconfidence rejections are common for both targets and lures, so observing the response has little value for distinguishing the two. When attempting to discriminate targets and lures based on the discrete response distributions, one would encounter many instances in which both the target and lure are associated with a high-confidence rejection, necessitating a guess.
In summary, previous authors were correct to note that Ag is an unreliable measure of the optimal performance that can be achieved with access to the underlying evidence distributions assumed by a model 
(Macmillan & Creelman, 2005)
. It is important to note, however, that A g is a reliable measure of the optimal performance that can be achieved with access to the discrete distribution of responses. Even though A g is affected by decision strategies, the latter point holds, exactly because discrete response distributions are influenced by decision strategies, and so the mapping from continuous evidence to discrete response must be taken into account. Thus, A g can be a useful measure for scenarios in which the goal is to characterize the discriminability between targets and lures supported by discrete response distributions. Below, we discuss a research program in which this goal is critical, i.e., lineup research, but A g might also prove useful in other contexts. For example, metamemory researchers could use the difference in AUC for model-generated and piecewise-linear ROCs to measure the information loss associated with translating retrieval states to a confidence scale, thereby assessing a participant's ability to appropriately report the different levels of uncertainty that they experience across trials.


Lineup ROCs
With basic principles established for the standard old/new recognition task, we turn our attention to lineup ROCs. Recall that the witness is asked to indicate which, if any, of the lineup members is the perpetrator of a previously witnessed crime 
(Wells & Olson, 2003)
. Witnesses can identify a suspect, identify a filler, or reject the lineup; each at various levels of confidence. 
Macmillan & Creelman (2005)
 refer to such tasks as simultaneous detection and identification tasks. In addition to their longstanding prevalence in eyewitness research, this type of task is beginning to play a role in distinguishing models of recognition memory (e.g., Meyer-Grant & Klauer, 2021). Importantly, researchers have not yet mapped out a theoretically grounded set of guidelines for how ROCs from these tasks should be created and interpreted. This section demonstrates how application of the principles outlined in the previous section, in combination with a mathematical model of lineup decision making, can address both of these outstanding issues. As such, this section establishes a new theoretical framework for creating and interpreting full lineup ROCs.


The Partial ROC
Eyewitness researchers have been exploring lineup data with ROCs for roughly a decade 
(Mickes et al., 2012;
Wixted et al., 2017)
. The most common approach is to evaluate partial ROCs that only consider a subset of the potential responses. For example, 
Figure 6A
 shows the partial ROC for the hypothetical 6-member lineup data summarized in 
Figure 6C
, which uses a 3-level confidence scale (high, medium, low). The leftmost ROC point includes only high-confidence suspects IDs (sus. high), and the rates of identifying innocent and guilty suspects with high-confidence are provided on the x-axis (.032; false ID) and y-axis (.554; accurate ID), respectively. The next point also includes medium-confidence suspect IDs (sus. med.), which increases both false IDs (to .066=.032+.034) and accurate IDs (to .673=.554+.119). The final point adds low-confidence suspect IDs (sus. low), meaning that it includes suspect IDs at all confidence levels, which further increases false IDs (to .108=.032+.034+.042) and
accurate IDs (to .737=.554+.119+.064).
Note that filler IDs and rejections are ignored in the construction of the partial ROC.
Furthermore, unlike the old/new recognition ROCs discussed previously, partial ROCs do not typically approach (1, 1), which is a natural consequence of the restriction to suspect IDs. Furthermore, in a properly constructed lineup, it is highly unlikely that an innocent suspect will be selected at a rate significantly different from the fillers. Thus, a reasonable maximum false ID rate for a partial ROC would 
Figure 6
. A) Partial ROC for the data in Panel C; B) Full ROC for the data in Panel C with points ordered by observed diagnosticity ratio; C) Hypothetical data from a lineup study with 3 levels of confidence. be the reciprocal of the number of lineup members (e.g., with 5 fillers and 1 suspect we have a maximum false ID rate of 1/number of lineup members=1/6=.167).
For a given set of lineup data, a continuous partial ROC can be generated from a nonparametric fitting technique or defined by a decision model tailored to the lineup task. Alternatively, a piecewiselinear partial ROC can be defined by connecting the ROC points with straight lines, which is the strategy depicted in 
Figure 6A
.


The Full ROC
A more recent approach constructs a full ROC that uses all of the response combinations, including filler IDs and rejections 
(Smith et al., 2020)
. As mentioned previously, it is not immediately obvious how the additional ROC points associated with filler IDs and rejections should be ordered. The order of inclusion is important, because different orders will result in different values of performance measures, such as AUC.
One possibility, proposed by 
Smith et al. (2020)
, suggests that points should be ordered based on the observed diagnosticity ratios (DRs), with responses that are more diagnostic of guilt added before responses that are less diagnostic of guilt. Recall that the DR for a given response is determined by taking the ratio of response probabilities for guilty and innocent suspects (e.g., the DR for high-confidence suspect responses in 
Figure 6C
 is 17.3125=.554/.032). DRs are provided in the third column of 
Figure 6C
, and the final column of 
Figure 6C
 shows the order of ROC points prescribed by these DRs. 
Figure 6B
 shows the resulting full ROC. The first three points are identical to the points in the partial ROC in Panel A, and they again represent suspect IDs across the three confidence levels, which have the three highest diagnosticity ratios.
To date, no study has attempted to fit a model-based curve to the full ROC points. This omission is understandable given that it is unclear what theoretical tools would be used to do so. Instead, these studies adopt the practice of constructing piecewise-linear ROCs, as in 
Figure 6B
, and evaluating AUC as a performance measure.


Theoretical Foundations of the Full ROC
The full ROC approach has the advantage of using all witness responses, but researchers have not yet linked full ROC performance measures to a broader theory for ROC interpretation. Our analyses in the Basic Concepts section suggest that full ROC methods can be placed on a solid theoretical foundation.
Consider the controversial practice of connecting ROC points with straight lines. The Basic Concepts section established that area under the piecewise-linear ROC represents the optimal performance that can be achieved with access to the discrete distribution of responses, which we illustrated with the hypothetical "Observer M" and "Observer R" example. Although, at the time, this scenario may have seemed odd, it is analogous to the challenge of using eyewitness identifications as evidence of guilt or innocence. Witnesses do not actually make the consequential decisions in an investigation, like whether to issue a search warrant, whether to allow a case go to trial, or whether to convict a defendant 
(Smith et al., 2020;
Wells, Yang, & Smalarz, 2015)
. These decisions are made by other people (e.g., judges and jurors) who only have access to the witness response as a source of information. Thus, the discriminability between guilty and innocent suspects supported by the distributions of witness responses is a central concept in eyewitness memory research. High AUC for the piecewise-linear ROC means that witness responses effectively discriminate guilty and innocent suspects. Specifically, high AUC means that a witness response from a randomly selected lineup with a guilty suspect would consistently produce stronger evidence of guilt than a witness response from randomly selected lineup with an innocent suspect. Thus, AUC for a piecewise-linear ROC is a meaningful measure in eyewitness research.
The Basic Concepts section also provided a theoretical justification for ordering ROC points by DR. This practice produces the highest possible AUC, i.e., it represents the maximum discriminability of guilty and innocent suspects that can be achieved for a given set of underlying evidence distributions.
Specifically, the AUC value for a DR-ordered ROC is the forced-choice discriminability of an optimal decision maker; that is, a decision maker that always selects the item that is more likely to be the target.


Ordering Full ROC Points in Practice
Although ordering by DR is theoretically justified, it can be tricky in practice because the population-level DRs -i.e., the values after eliminating noise produced by sampling variability -are unknown. If the DRs observed for a particular sample are used as a proxy, then AUC estimates might be misleading. For example, 
Wilson and Colloff (2020)
 showed that ordering ROC points by observed diagnosticity ratios results in AUC values above chance performance (.5), even when participants must respond based on pure guessing. In this case, all population-level diagnosticity ratios are 1; that is, all responses are equally likely to be made for targets and lures. Sampling noise, however, will cause some sample diagnosticity ratios to be above 1 and some to be below 1. Recall that the slope of a line segment in a piecewise-linear ROC is equivalent to the DR associated with the latest response added to the ROC.
Thus, sampling variability ensures that some ROC line segments will have slopes above 1 and some below 1. If ROC points are ordered by observed DRs, then all the segments that happen to have slopes above 1 are moved to the left and all the segments with slopes below 1 are moved to the right, guaranteeing that AUC will be above .5. Indeed, this method ensures that the AUC will be the highest value possible for the sample response distributions, which could be much higher than the AUC that would be obtained by ordering the points based on DRs in the population response distributions. In other words, the sample AUC value will tend to overestimate the discriminability that can be achieved when one considers cases outside of the current sample. 
Yang and Smith (2020)
 also noted that using the observed diagnosticity ratios would artificially inflate AUC values, and they considered some potential solutions, such as deciding on a fixed order that is used across all studies or determining order based on the observed diagnosticity ratios of previous studies.
Given these difficulties, it is important to consider different strategies for ordering the full ROC points to determine which, if any, supports an accurate estimation of performance measures at the population level. Decision models can play an invaluable role in this process. In the following section, we review a signal-detection model that has the advantages of being relatively simple while consistently achieving a close match to observed lineup performance 
(Cohen et al., 2020;
Duncan, 2006)
. We then use this model to explore different methods for creating full ROCs for a lineup identification task.


A Model of Lineup Performance
In this section we present a model of lineup performance that will serve as the foundation for ordering points in a full ROC for a lineup task. Specifically, we explore a model that combines the core assumptions of signal detection theory (SDT; 
Tanner & Swets, 1954
) with a simple decision rule for responding in a lineup task 
(Clark, 2003;
Duncan, 2006;
Macmillan & Creelman, 2005)
. By selecting this model, we do not mean to imply that it is superior to alternative models, rather, we selected it as a model that generates plausible performance data based on a straightforward decision process. 
Figure 7
. A) Signal-detection model for the showup and lineup tasks with 3 confidence levels; B-D) Joint probability densities that the max member sampled from Panel A will have a given memory match strength and the lineup response will be a rejection, filler ID, and suspect ID, respectively, for guilty (solid line) and innocent (dashed lines) suspects.


A Model for the Showup
We first consider the simple signal-detection model discussed previously in the context of the old/new recognition task 
(Macmillan & Creelman, 2005)
 as outlined in 
Figure 2A
. A version of this model is provided in 
Figure 7A
. This model is appropriate for the showup task, in which the witness is asked either identify or reject a single suspect. In relation to the old/new recognition task, a guilty suspect acts as a target, an innocent suspect acts as a lure, an identification is an old response, and a rejection is a new response.
The dashed and solid distributions of 
Figure 7A
 represent the distributions of memory match strength, i.e., the strength of match between the suspect and witness's memory of the perpetrator, for guilty and innocent suspects, respectively. Guilty suspects tend to have a higher memory match strength, however, there is variability across identification attempts. For example, sometimes the witness will have poor memory for the perpetrator, producing a low memory match strength for a guilty suspect. Sometimes an innocent suspect will look similar to the perpetrator, producing a high memory match strength.
As in the old/new recognition task, showup responses are determined by comparing the suspect memory match strength to a set of criteria. The solid vertical line in 
Figure 7A
 is the identification criterion, which separates low memory match strengths that lead to a rejection from high memory match strengths that lead to an identification. The dashed vertical lines are additional criteria used to determine identification and rejection confidence. In this example, there are a total of five criteria, which demarcate six response regions, i.e., three confidence levels each for identifications and rejections. A memory match strength in the leftmost region will lead to a high-confidence rejection, followed by medium-and lowconfidence rejections, and then low-, medium-, and high-confidence identifications.


A Model for Lineups
Extending this model to a lineup task requires additional assumptions about how witnesses navigate the extra layer of uncertainty introduced by the fact that they don't know which lineup member is the suspect. Researchers have tested a number of assumptions for how witnesses approach this task 
(Duncan, 2006;
Wixted et al., 2018)
. Here, we will focus on a simple decision strategy that is psychologically plausible, computationally tractable, and consistently successful in providing a close match to lineup data 
(Clark, 2003;
Duncan, 2006;
Macmillan and Creelman, 2005, p. 255-259)
. This Max-SDT model assumes that witnesses first determine which lineup member provides the highest memory match strength (the "max member") and then use this max memory strength as the decision variable in a process analogous to the showup model described above. That is, witnesses determine a response by comparing the memory match strength of the max member to a set of response criteria as in 
Figure 7A
. The model further assumes that fillers have the same match-strength distribution as innocent suspects.
The two-stage decision process results in mixture distributions of the decision variable (max memory strength), where the mixture is over the lineup outcome (suspect ID, filler ID, or rejection).
Panels B-D in 
Figure 7
 collectively show the evidence distributions for lineups with guilty suspects (solid lines) and lineups with innocent suspects (dashed lines), with each of the three panels associated with one of the three components of the mixture distributions. These distributions were generated with the same memory parameters as in 
Figure 7A
, for a 6-member lineup (see Appendix B for details). 
Figure 7B
 shows the partial distributions associated with lineup rejections, that is, the joint probability density that the max member will have a particular memory match strength and the lineup response will be a rejection. Of course, these partial distributions include only lineups for which the strength of the max member was below the identification criterion, or else the response would not have been a rejection. Integrating these partial distributions below the identification criterion gives the total probability of a rejection. Integrating these partial distributions between two confidence criteria gives the probability of a rejection at a given confidence level. Note that these partial distributions are not separated based on whether the maximum memory strength was produced by the suspect or a filler. In standard practice, witnesses are not asked to select one of the lineup members if they reject the lineup, so model predictions for lineup rejections are derived by marginalizing over suspect and filler "wins." In other words, these distributions include all lineups that produce a max strength below the identification criterion regardless of whether the max value was produced by the suspect or a filler. 
Figure 7C
 shows the partial distributions associated with filler IDs, that is, the joint probability density that the max member will have a particular match strength and the lineup response will be a filler ID. These distributions necessarily only include lineups for which a filler produces the maximum strength value, with this max value falling above the identification criterion. Integrating these partial distributions above the identification criterion provides the probability of a filler ID. Integrating between two confidence criteria gives the probability of making a filler ID at a given confidence level.
Finally, 
Figure 7D
 shows the partial distributions associated with suspect IDs, that is, the joint probability density that the max lineup member will have a particular match strength value and the lineup outcome will be a suspect ID. These distributions only include lineups for which the suspect produces the highest match strength, with this match value falling above the identification criterion. Integrating above the identification criterion provides the total probability of a suspect ID. Integrating between adjacent confidence criteria gives the probability of making a suspect ID at a given confidence level.
Again, the full evidence distributions for guilty-suspect and innocent-suspect lineups are defined by all three of the partial distributions shown in 
Figure 7B
-D. The area of all three curves together integrates to 1 for both innocent and guilty suspect lineups; that is, the three together represent all possible lineup outcomes. The area of any one component gives the probability of the associated lineup outcome across all confidence levels.


A Comparison of Strategies for Ordering ROC Points
In this section, we use the Max-SDT model to explore the feasibility of different strategies for ordering ROC points. As discussed previously,  suggested ordering points on the full ROC based on observed diagnosticity ratios or based on a fixed order determined by analyzing past studies. We used the Max-SDT model to explore the feasibility of these strategies along with a newly proposed strategy: Fitting a model to the lineup responses and ordering the points based on the DRs in the model predictions. To assess the possibility of using a fixed order, we explore the extent to which point order varies across different performance profiles, i.e., differences in memory discriminability, response conservatism, and the mapping of memory strength match values to confidence levels. To assess the practice of using observed DRs or DRs from a model fit to the empirical data, we perform recovery simulations in which we generate sample data sets from a model with a known true AUC and test the ability to accurately estimate this AUC value using the different ordering strategies.


A Fixed-Order Strategy
Choosing a fixed order based on past studies would be feasible only if the order of DRs remains stable across different studies that might have different performance profiles in terms of memory or decision processes. If not, this practice will often prescribe an incorrect order for the ROC points, and as a consequence, AUC will fail to capture the optimal discriminability between guilty and innocent suspects that can be achieved with access to the discrete response distributions. We used the Max-SDT model to assess order consistency across studies. In particular, we repeatedly sampled model parameters across a range of values that might be observed in eyewitness experiments, and we evaluated the ranking of the diagnosticity ratios associated with each set of parameters. The main outcome of interest was whether these ranks remain consistent or vary widely across different performance patterns.
Consider the Max-SDT model applied to a 6-member simultaneous lineup with three confidence levels for both identification and rejection responses. We calculated the model predictions for each of 5000 parameter sets, where each parameter set defined the mean of the target distribution (representing memory discriminability), the position of the ID criterion (representing response conservativeness), and the position of the confidence criteria (representing the interpretation of the confidence scale). The parameters were sampled from uniform distributions with ranges that are likely to characterize the population-level response distributions for an individual eyewitness experiment (see the caption to 
Figure   8
 for details). For each parameter set, we determined the predicted response proportions for guilty-and innocent-suspect lineups and ordered the ROC points by the resulting diagnosticity ratios. Note that this simulation explored variability in the proper order of ROC points. That is, performance was defined by predictions at the population-level, not the observed proportions from a sample, so sampling variability played no role in the ordering of ROC points. 
Figure 8
 shows the distribution of diagnosticity ratio ranks for each of the nine possible lineup responses. Some responses were always assigned the same rank. For example, high-confidence suspect IDs were always the first ROC point, i.e., this response always had the highest diagnosticity ratio. Highconfidence rejections were always the last ROC point, i.e., these responses always had the lowest diagnosticity ratio. The position of some of the other responses, however, was quite variable across different parameter sets. For example, high-confidence filler IDs ranged from the 2 nd to 6 th ROC point, and low-confidence rejections ranged from the 4 th to the 7 th ROC point.
We defined ranking consistency as the probability that two parameter sets would prescribe the same order of ROC points. Different parameter sets produced the same order on only 15% of comparisons. Moreover, comparisons often revealed multiple ranking differences, with a median of 4 
Figure 8
. Distribution of diagnosticity ratio ranks for each of the nine possible lineup responses simulated from the Max-SDT model. Parameter ranges: Mean of the target distribution (µ) = (.5, 2.5); Position of the ID criterion = (µ/2 -1, µ/2 + 1); Distance between adjacent confidence criteria = (.1, .4).
differences between parameter sets. Thus, the order of ROC points prescribed by diagnosticity ratios varies considerably across different population-level performance profiles, that is, the prescribed order is likely to vary widely across different studies.
The simulations above assumed 3 confidence levels, but many eyewitness experiments have used scales with a larger set of confidence responses (e.g., 
Palmer, Brewer, Weber, & Nagesh, 2013)
.
Unfortunately, the order variability across parameter sets only increases when additional confidence levels are added. With 11 confidence levels (e.g., confidence levels 0-100 in intervals of 10; Palmer et al., 2013), different parameter sets produced the same order in only 7% of comparisons, and the median number of differences between orders was 8.
These results demonstrate that the proper order of ROC points -the order prescribed by the population-level performance statistics, as opposed to an individual noisy sample -varies substantially across different performance profiles and decision strategies in the Max-SDT model. As this model is usually able to closely match empirical data from lineup experiments (e.g., 
Cohen et al., 2020)
, it is likely that true population-level diagnosticity ratios are also quite variable across studies. As such, the practice 
Figure 9
. Joint probability densities that the max member sampled from the Max-SDT model will have a given memory match strength and the lineup response will be a filler ID or suspect ID for guilty (solid line) and innocent (dashed lines) suspects. The model parameters were selected such that a filler ID had a higher DR than a suspect ID for some confidence levels. of choosing an order for a planned study based on the results of past studies would often produce misleading full ROCs, as any difference in performance profiles between the studies could result in a different order of ROC points. Such differences may include the level of memory discriminability, the level of conservativeness adopted by the participants, and the policy for mapping memory match values to confidence levels. If the number of confidence levels between the planned study and the comparison study is different, then the last factor is especially problematic. If the planned study involves any new conditions not tested in previous studies, then it is likely that the prescribed order of ROC points would change in the new condition.


The Basis of Order Variability
To understand why ROC order varies across performance profiles, we explored a few example cases. First, it may be surprising that a filler ID can have a higher diagnosticity ratio (stronger evidence of guilt) than a suspect ID. 
Figure 9
 shows evidence distributions for a parameter set with this ordering. As before (see 
Figure 7)
, these are distributions of the joint probability density that a lineup will produce a specific response (either a filler ID or a suspect ID) and will have a given maximum memory match strength.
Across all strength levels, filler IDs are less likely in guilty-suspect than innocent-suspect lineups -that is, DRS are below 1 -but the likelihood ratio converges to 1 as the strength of the filler increases.
This convergence occurs because very strong fillers are likely to exceed the match strength of both innocent and guilty suspects. In other words, having a filler with a very high strength value is relatively rare for both guilty-and innocent-suspect lineups, but if this situation does occur, the lineup outcome is likely to be a filler ID in both cases.
The convergence to 1 means that high-confidence filler IDs can have a diagnosticity ratio that is very close to 1 if the criterion for high-confidence IDs is relatively high 
(Figure 9
). Filler IDs associated with diagnosticity ratios close to 1 can "switch" order with suspect IDs associated with diagnosticity ratios below 1. For example, in 
Figure 9
, the overall identification criterion is relatively liberal (i.e., witnesses are willing to make an ID even at relatively low match strengths), and suspect IDs with maximum strength values just above the criterion are actually more common for innocent-suspect lineups than guilty-suspect lineups. Indeed, in this case, low-confidence suspect IDs have a lower diagnosticity ratio (stronger evidence for innocence) than high-confidence filler IDs, so the high-confidence filler IDs are ranked higher than low-confidence suspect IDs. Note that reducing the high-confidence criterion would also decrease the diagnosticity ratio associated with high-confidence filler IDs, potentially moving this response to a later position in the ROC relative to low-confidence suspect IDs, so the position of the confidence criteria is a factor in determining the order of the diagnosticity ratios.
Next, we consider how rejections can have a higher diagnosticity ratio (stronger evidence of guilt) than a filler ID. 
Figure 10
 shows distributions from a parameter set that produces this pattern. Here, the identification criterion is set at a high memory match strength value, representing conservative responding. As a result, identified fillers necessarily have a strong match to the memory of the perpetrator; thus, filler IDs at all confidence levels have diagnosticity ratios just below 1 (also see 
Figure   9
). Also, maximum memory match strengths just below the ID criterion are more common in guilty- 
Figure 10
. Joint probability densities that the max member sampled from the Max-SDT model will have a given memory match strength and the lineup response will be a rejection or filler ID for guilty (solid line) and innocent (dashed lines) suspects. The model parameters were selected such that a rejection can have a higher DR than a filler ID for some confidence levels. suspect than innocent-suspect lineups, that is, low-confidence rejections are associated with a diagnosticity ratio above 1. Thus, low-confidence rejections are ranked higher than filler IDs at any confidence level.
In summary, the ordering of ROC points depends on both the position of the identification criterion relative to the strength distributions for a particular level of witness memory and the positions of the confidence criteria. These examples suggest that using a canonical order for ROC points, or an order derived from other studies, will often produce misleading results. They also suggest that the proper order of ROC points can be different for different conditions within the same study.


Ordering Effects on AUC
Given the potential difficulties with using a fixed order for ROC points, this section considers three alternative ordering strategies for generating the full ROC, proper-ordered, data-ordered, and modelordered ROC points. For each, the position of the ROC points was determined based on the response proportions from a set of simulated, sample data. The proper ordering strategy is an idealistic scenario in which the "true" order prescribed by the population-level diagnosticity ratios is somehow known, but the population-level response distributions are not known, and so must be estimated with a sample. Although impossible in practice, using the proper order provides a useful point of comparison in determining whether and how incorrect ordering of points can distort AUC estimates. As discussed previously, the data-ordered ROC is determined from the diagnosticity ratios observed in a data sample. Finally, the model-ordered ROC is determined from predicted diagnosticity ratios obtained by fitting the Max-SDT model to a data sample.
We calculated the ROC points associated with each of the three ordering strategies for each simulated data set, and then we defined AUC by connecting the ROC points with straight lines and calculating the area under this piecewise-linear curve. Thus, our simulations evaluate the ability to measure the discriminability between guilty and innocent suspects supported by the discrete response distributions, not the underlying memory distributions assumed by the model. These sample AUC estimates were compared to the "ground-truth," population-level AUC computed from an ROC in which the points were ordered based on the proper, population-level diagnosticity ratios and positioned based on the population-level response distributions. To emphasize a subtle but important distinction, we again remind readers that the proper-order AUC values come from ROC points positioned based on the sample response distributions; thus, they can differ from the ground truth values even though both are ordered in the same way.
We assumed that all lineups had 6 members and that witnesses could respond with 11 confidence levels, as in studies that use a confidence scale from 0-100 in steps of 10 (e.g., 
Palmer et al., 2013)
. The simulations involved the following steps, which were repeated 1000 times for each sample size that was evaluated. As in 
Smith et al. (2020)
, a piecewise-linear ROC was used throughout.


Sample generating parameters.
A set of Max-SDT parameters were sampled from uniform distributions (see 
Figure 8
 caption for details).


Generate the population-level response distribution. Using the set of parameters from
Step 1, Max-SDT model predictions were generated for the probability of each of the 33 possible lineup responses (11 confidence levels each for suspect IDs, filler IDs, and rejections) for both guilty-suspect and innocent-suspect lineups. These two multinomial distributions are designated as the population-level response distributions.
3. The ground-truth AUC. The ground-truth or population-level AUC was calculated using the proper ROC point order and population-level response distribution.
3.1. The proper order for the ROC points was determined by ranking the diagnosticity ratios computed from the population-level response distributions (Step 2).
3.2. The ground-truth ROC was constructed using the population-level response distributions (Step 2) in the proper ROC point order (Step 3.1).
3.3. The ground-truth AUC was calculated using the ground-truth ROC (Step 3.2).


Simulate the process of collecting experimental data.
A set of sample response frequencies were "collected" as draws from the population-level multinomial distributions (Step 2). N G guilty-suspect and N I innocent-suspect trials were simulated, where N G =N I . Results are reported in terms of the total sample size, N=N G +N I , which had values of N=200, 500, 1000, and 2000.


5.
Calculate the proper-order AUC. The proper-order AUC was calculated using the proper ROC point order, but with points positioned based on the response-distributions from sampled data.
6. Calculate the data-ordered AUC. The data-ordered AUC was calculated using the sampled response distributions with points ordered based on sample data. 6.1. The order of ROC points was determined by ranking the DRs computed from the sampled data response distributions (Step 4). 6.2. The data-ordered ROC was constructed using the sampled-data response distributions (Step 4) in the data-defined ROC point order (Step 6.1).
6.3. The data-ordered AUC was calculated from the data-ordered ROC (Step 6.2).


7.
The model-ordered AUC. The model-ordered AUC was calculated using the sampled-data response frequencies and ordering the ROC points based on a model fit to the sampled data. 7.1. The Max-SDT model was fit to the sample response frequencies (Step 4) by minimizing the G 2 statistic.
7.2. The model-defined order of ROC points was determined by ranking the diagnosticity ratios computed from the predictions of the Max-SDT model (Step 7.1).


7
.3. The model-ordered ROC was constructed using the sampled-data response distributions 
(Step 4)
 in the model-ordered ROC point order (Step 7.2).


7
.4. The model-ordered AUC was calculated from the model-ordered ROC (Step 7.3). 
Figure 11
 shows the proper-order, data-ordered, and model-ordered AUC estimation results (Columns 1-3, respectively) using 200, 500, 1000, or 2000 observations per data set (Rows 1-4, respectively). The x-axis values are the deviation between the AUC estimate and the ground-truth AUC value across all simulations, thus zero represents accurate estimation.
If the sample ROC points are ordered according to the population DRs (Column 1), then the sample AUC estimates provide unbiased estimation of the ground-truth AUC value, i.e., the deviations are roughly symmetrical around zero. Estimation error decreases with larger sample size. This result demonstrates that accurate estimation of the full lineup AUC is achievable in theory, if the ROC points are properly ordered. 
Figure 11
. Proper-order, data-ordered, and model-ordered AUC estimation results (Columns 1-3) using 200, 500, 1000, or 2000 observations per data set 
(Rows 1-4)
. See text for details.
If, instead, sample ROC points are ordered based on the observed diagnosticity ratios 
(Figure 11
, Column 2), sample estimates consistently overestimate the true population AUC, especially with small sample sizes. The resulting estimation error can be severe.
Fortunately, estimation can be dramatically improved by fitting an appropriate model to the sample data and ordering the ROC points based on the model predictions 
(Figure 11, Column 3)
. Indeed, this procedure produces estimations that are essentially as good as using the proper order 
(Figure 11
, Column 1), even with a relatively small number of observations. This key result demonstrates one important way in which decision models can be extremely useful in interpreting full ROC data from complex tasks, mirroring their central role in traditional old/new tasks (e.g., 
Lockhart & Murdock, 1970;
Wixted, 2007
Wixted, , 2020
.
Note that these results are not necessarily presented as evidence in favor of the Max-SDT model, and we acknowledge that alternative models might provide a better description of lineup data 
(Wixted et al., 2018)
. The critical result is that applying a model that is appropriate for the processes that generated the lineup responses essentially eliminates distortions in AUC estimates produced by mis-ordering the ROC points.
Overall, our results demonstrate that modeling can play a critical role in the interpretation of full ROCs. Strategies using a fixed order of ROC points, an order from previous studies, or an order based on the empirical DRs from the current sample do not provide a robust basis for estimating AUC. Fortunately, ordering points based on decision models fit to sample data supports highly accurate estimation.


Model-based Full Lineup ROCs
Calculation of AUC in the previous section was based on piecewise-linear ROCs, which, as discussed previously, represents the discriminability between guilty and innocent suspects that can be achieved with access to the discrete distribution of responses. For standard old/new tasks, evaluating AUC with a continuous ROC generated from a model dramatically improves the ability to correctly measure the discriminability of the underlying evidence distributions 
(Macmillan & Creelman, 2005)
. In this section, we consider whether using a continuous full-lineup ROC, generated from the Max-SDT model, provides a meaningful measure of performance.


Generating a Continuous Full-lineup ROC
Generating an ROC from the Max-SDT model is complicated by the fact that the strength distributions are mixture distributions, where the mixing variable is the lineup outcome. That is, the distributions of maximum memory match strength are different for suspect IDs, filler IDs, and rejections, e.g., see 
Figure 7
. As a result, DR is not monotonically related to maximum memory strength, so creating an ROC by sweeping a criterion across the memory strength continuum fails to represent the optimal potential for discriminating guilty and innocent suspects based on the mixture distributions. For example, consider the distributions in 
Figure 6
. A suspect ID with a memory match strength of 3 has a much higher diagnosticity ratio (the ratio of the density of the dashed to the solid line in Panel D) than a filler ID with a memory strength of 3 (the ratio of the density of the dashed to the solid line in Panel C). Despite these IDs being made with the same memory match strength, the DR varies across the mixture distributions.
In the Basic Concepts section, we showed how an ROC can be formed by sweeping a criterion across DR, rather than raw memory strength values (e.g., 
Figure 3)
. The basic idea is that, for a given DR criterion, the hit and false alarm rates for an ROC point are determined by the proportion of the guiltysuspect and innocent-suspect distributions that exceed that DR, respectively. An example is provided below. Analogous to the idea that the model-generated curve from a basic signal-detection model represents the optimal performance that can be achieved with access to the underlying memory strength distributions, the model-based full lineup ROC represents the optimal performance that can be achieved with access to the mixture distributions representing the joint outcomes of maximum memory strength and lineup response (suspect ID, filler ID, or rejection). This paper includes an accompanying R Shiny App located at <https://tuttlem.shinyapps.io/fullroc_finalapplication> that allows users to explore the process of generating a full ROC from the Max-SDT model using the DR criterion method. An example output is shown in 
Figure 12
. The left side of the screen provides the model and experimental design parameters used in the example, e.g., the lineup size, the mean of the guilty suspect distribution, etc. Here we focus on the five figure panels to the right. Analogous to Panel D of 
Figure 7
, the upper-left panel shows the memory match strength distributions for suspect IDs; that is, this panel provides the joint density that the maximum strength is a particular value and lineup outcome is a suspect ID. The vertical, black, dashed line is the identification criterion. Because the lineup is rejected if the max memory strength value is below the identification criterion, the distribution lies above the criterion. The solid and dashed distributions are for guilty and innocent suspects, respectively. Analogous to Panel C of 
Figure 7
, the middle-left panel shows the same distributions for filler IDs. Analogous to Panel B of 
Figure 7
, the bottom-left panel shows the distributions for rejections. The rejection distributions naturally lie below the identification criterion. 
Figure 12
. A screenshot from the Shiny app located at <https://tuttlem.shinyapps.io/fullroc_finalapplication >. The first column of plots show the distributions of max-member memory match strength for each of the three lineup outcomes (Suspect ID, Filler ID, Rejection). The top right plot shows the log diagnosticity ratio at a given max memory strength for suspect IDs (solid), filler IDs, (dotted line) and rejections (dashed line). The lower right plot shows the ROC.
The upper-right panel of 
Figure 12
 shows the log DR in favor of guilt associated with a given memory match strength. Because this log DR can vary depending on the lineup outcome, there are three different lines: One for suspect IDs (solid), one for filler IDs (dotted), and one for rejections (dashed). The vertical line is the memory match strength associated with the identification criterion. As previously, the suspect ID and filler ID lines lie to the right of the identification criterion and the rejection line lies to the left of the identification criterion.
The lower-right panel of 
Figure 12
 shows the ROC for this model. As usual, the x-and y-axes are response rates for innocent and guilty suspects, respectively. The ROC is generated as follows. Select a given log DR criterion. For a concrete example, consider the criterion associated with the yellow horizontal line in the upper-right panel. The yellow lines shown in the left distribution panels mark the max memory match strengths that produce this log DR criterion. This log DR is just below 0, indicating a DR that is slightly below one, that is, slightly in favor of an innocent suspect over a guilty suspect. Thus, at the location of the yellow lines in the left panels, the red (innocent suspect) lines are slightly higher than the blue (guilty suspect) lines. For suspect IDs, this location is just above the ID criterion, and all max strength values to the right produce a log DR that is higher than the criterion (i.e., more in favor of a guilty suspect). For filler IDs, this location is at a high match strength. DRs approach a value of 1 (log DR = 0) above the yellow line, so all of the max strength values to the right also exceed the target log DR criterion. For rejections, there is no point of the strength continuum that matches the criterion DR.
Lineups with maximum strength values just below the ID criterion are a relatively strong indication of innocence, and the evidence for innocence grows as the maximum memory strength decreases. In other words, the log DR in favor of guilt starts below the criterion log DR value near the ID criterion and dips farther below the log DR criterion as one moves to the left. Thus, none of the match strength values in the rejection distributions meet the log DR criterion.
The hit and false-alarm rates for the ROC are the proportion of the distributions that exceed the log DR criterion for lineups with guilty and innocent suspects, respectively. These values correspond the total area to the right of the yellow lines across the suspect ID, filler ID, and rejection distributions. When in the lower-right plot. The full set of ROC points is generated by sweeping the log diagnosticity ratio in the upper-right panel from top to bottom.


Evaluating the Continuous Full ROC
With the continuous full ROC defined, we now consider its properties. In short, in contrast to a standard old/new task, the model-generated full ROC cannot be interpreted as a pure measure of memory discriminability. 
Figure 13
 illustrates this point by plotting the full ROCs for three different identification criteria that range from more liberal to more conservative. The liberal criterion is the value criterion from 
Figure 12
. Importantly, the underlying memory strength distributions were held constant (to the values used in 
Figure 12
). The vertical lines in 
Figure 13
 are the innocent suspect ID rates associated with each criteria. Not surprisingly, increasing the identification criterion to a more conservative value decreases the false ID rate. Critically, however, changing the identification criterion also has a huge effect on the area under the full ROC. 
Figure 13
. Full ROCs generated with fixed memory strength distributions and varying identification criteria. The vertical lines are the innocent suspect rates associated with each criteria.
Why does changing the response criterion so dramatically impact AUC for the continuous full ROC? The key to understanding this pattern is realizing that the continuous full ROC represents the optimal performance that can be achieved with access to the mixture distributions of maximum memory strength across the different lineup outcomes (e.g., 
Figure 7B-D)
. The full ROC does not represent the optimal performance that can be achieved with knowledge of the actual distributions of memory strength for guilty and innocent suspects (e.g., 
Figure 7A
).
Because witnesses don't know which lineup member is the suspect, lineup responses are not uniquely determined by the suspect strength. Instead, the memory strengths of all lineup members are filtered through a decision process. For the Max-SDT model, this means determining the member with the highest memory strength and responding based on this maximum value. Thus, the distributions that underlie responding are the mixture distributions of maximum memory strength. These distributions change across different ID criteria, because changing the ID criterion changes the proportion of lineups that result in a suspect ID, a filler ID, or a rejection. For example, using a higher identification criterion increases the proportion of lineups that result in a rejection. Unfortunately, the distributions associated with rejections do a poor job of distinguishing lineups with guilty and innocent suspects, because these distributions conflate lineups in which the suspect versus one of the fillers produced the max memory strength. Thus, a higher ID criterion impairs the ability to discriminate guilty and innocent suspects. 
Mickes et al. (2012)
 recommended that researchers assess memory discriminability based on the partial AUC up to the false-alarm rate of the most conservative condition being compared. In 
Figure 13
, the partial ROCs, i.e., the ROCs generated from only suspect IDs, are embedded in the full ROCs. In particular, note that the ROCs are identical until they reach the false-alarm rate for the most conservative condition. In this example, using the partial ROC would support the correct conclusion about the distributions of match strength for guilty and innocent suspects; namely, that the liberal, moderate, and conservative conditions all have the same underlying memory distributions.
In summary, area under the full ROC is affected by decision processes even when it is defined by continuous evidence states in a decision model. Thus, when the goal is to measure the discriminability of witness memory independent of decision processes, researchers should rely on partial AUCs 
(Mickes et al., 2012)
. Another possible strategy is to fit a decision model to the lineup data, like the Max-SDT model, and base conclusions on relevant model parameters, e.g., the distance between the distributions of strength values for innocent and guilty suspects. When the goal is to measure the optimal performance that can be achieved with knowledge of the witness response distributions, researchers should use a piecewise-linear full ROC with points ordered by model predictions. The continuous full ROC from the Max-SDT model represents the optimal performance that can be achieved using the mixture distributions of max memory strength across different lineup outcomes.


Full ROCs and Expected Information Gain (EIG)
Recently, 
Starns, Cohen, and Rotello (2021)
 proposed a method for assessing identification procedures in terms of the Expected Information Gain (EIG) about guilt or innocence achieved by observing a witness response 2 . EIG incorporates all lineup responses, not just suspect IDs, and is thus a natural point of comparison for measures based on the full ROC. In this section, we will briefly compare and contrast these two measures. A complete comparison is left to future work.
EIG is the average amount of information about guilt or innocence that is gained by performing an identification attempt 
(Starns et al., 2021)
. To achieve high EIG, witness responses need to consistently provide strong evidence of guilt for guilty suspects and strong evidence of innocence for innocent suspects. EIG is based on principles from information theory 
(Shannon, 1948)
 and provides a complete measure of evidentiary value, in that it factors in all aspects of identification performance, including suspect IDs, filler IDs, and rejections at all confidence levels. That is, EIG incorporates the potential for all possible correct and incorrect responses.
EIG is closely related to AUC for the piecewise-linear full ROC. Full-lineup AUC indicates the extent to which the distributions of witness responses discriminate guilty and innocent suspects, and this discriminability has a strong influence on EIG. That is, witness responses cannot provide useful information about guilt or innocence (high EIG) if they do not effectively discriminate guilty and innocent suspects (high AUC). EIG, however, also incorporates factors that do not impact full-lineup AUC; namely, the prior probability of guilt and the level of calibration in probabilistic judgments of guilt or innocence. High calibration means that the judged level of certainty of guilt or innocence closely matches the true risk of making an incorrect classification 
(Lichtenstein & Fischhoff, 1980)
. These factors are important for assessing the usefulness of witness responses in real investigations, and some research questions can only be answered when these factors are considered. We elaborate on these points in the following paragraphs.
Prior probability of guilt has a clear impact on EIG 
(Starns et al., 2021)
, but has no effect on fulllineup AUC. ROCs are based on conditional response probabilities for targets and lures, and, therefore, are not influenced by the prior probability of encountering a target or a lure. For example, a false ID rate of .30 can occur whether the proportion of lineups with innocent suspects is .10 or .90. For some research questions, it is useful to have a measure that is unaffected by prior probability of guilt, and indeed, Starns et al. recommended that EIG should typically be applied with an assumed prior probability of .5 because this practice supports conclusions that are valid across a wide range of actual prior probabilities.
However, prior probability is intrinsic to answering certain research questions 
(Starns et al., 2021)
, and is an important factor to consider in applied research 
(Wells, Yang, & Smalarz, 2015)
. When prior probability is a critical factor, EIG has the advantage of combining prior probability with all other aspects of performance in a theoretically principled manner.
Achieving high EIG requires that judgments of guilt or innocence are associated with an appropriate level of certainty 
(Starns et al., 2021)
; for example, the level of certainty in innocence should match the actual risk of misclassifying a guilty suspect as innocent, and vice versa. This factor does not affect full lineup AUC, which only depends on how the responses are ranked in terms of conservativeness. For example, if high-confidence suspect IDs are considered more conservative than medium-confidence suspect IDs, then the former will enter the hit and false alarm rate calculations before the latter, and the ROC will be the same regardless of the absolute certainty of guilt adopted for either response.
Judgment calibration is a critical factor for real investigations, where decision makers have to judge guilt or innocence and assess whether the level of certainty justifies risking various misclassification costs. Imagine that a low-confidence suspect ID supports 60% certainty that a suspect is guilty; that is, 60% of suspects identified with low-confidence are guilty and 40% are innocent. If people in the criminal justice system interpret this evidence properly, then the lineup response will be a useful guide to effective legal decision making. That is, this level of certainty might justify actions like issuing a search warrant or expanding the investigation of the suspect. If people dramatically misinterpret the evidence from the low-confidence suspect ID, however, the results could be tragic. For example, if they interpreted the lineup response as evidence for innocence, they could miss the opportunity to garner further evidence of guilt and a criminal could avoid justice. If they interpreted the lineup response as supporting 99% certainty of guilt, then a potentially innocent person could be incarcerated and the guilty party could remain free.
EIG naturally quantifies the effect of miscalibration and shows that this factor plays an important role in determining the success of evidence gathering procedures 
(Starns et al., 2021)
. Thus, only EIG will support a comprehensive assessment of the use of identification evidence in real investigations. EIG will also be a critical tool for laboratory studies that investigate calibration; for example, studies evaluating quantitative techniques for estimating the posterior probability of guilt from data sets that mix guilty and innocent suspects in unknown proportions 
(Cohen et al., 2020;
Wixted, Mickes, Dunn, Clark, & Wells, 2016
).
In summary, area under the full ROC and EIG are different measures that factor in different aspects of identification performance. Only EIG incorporates the role of prior probability of guilt and judgment calibration, so conclusions can differ between the two measures. Although prior probability and judgment calibration are crucial factors to consider when making a comprehensive assessment of the usefulness of identification evidence, there are many research questions that can be effectively answered without considering these factors. Thus, area under the full ROC can be a useful tool for measuring evidentiary value for many investigations of eyewitness memory. Nevertheless, if researchers limit themselves to questions that can be addressed with ROCs, then they will ignore some important aspects of decision making.


Summary and Recommendations
We have explored various aspects of ROC creation and interpretation. Here, we make a series of recommendations for researchers and summarize the findings that justify them.
If the goal is to have AUC represent the optimal performance that can be achieved with knowledge of a given set of evidence distributions, we recommend creating ROCs based on diagnosticity ratios (DRs). In the Basic Concepts section, we showed that when ROCs are created based on DRs (or likelihood ratios), AUC is the forced-choice percent correct that could be achieved by an optimal decision maker with access to the evidence distributions. Stated otherwise, AUC is the probability that the targetto-lure DR will be higher for a randomly selected target than a randomly selected lure. Under certain circumstances, the resulting ROC will be equivalent to the ROC produced by a different method for constructing ROCs, such as using "raw" strength values. When this is the case, it demonstrates that the decision rule used to construct the ROC achieves optimal performance and bears a monotonic relationship with the DR (e.g., 
Figure 2)
. Critically, the DR strategy can be applied even when there is no clear method for creating a strength-based ROC; thus, it provides an important tool as memory researchers begin to investigate more complex recognition tasks 
(Meyer-Grant & Klauer, 2021;
Smith et al., 2020;
Starns et al., 2018)
.
If the goal is to measure the ability to discriminate targets and lures with knowledge of discrete response distributions, we recommend using piecewise-linear ROCs. Normally, memory researchers using ROCs have the goal of measuring the discriminability supported by the underlying evidence distributions assumed by some model or theory. For example, the distributions in a signal detection model represent the information retrieved from memory, and researchers are often interested in measuring the extent to which the information in these distributions can be used to distinguish targets and lures.
Piecewise-linear ROCs do not effectively meet this goal, because they are influenced by changes in decision strategies 
(Macmillan and Creelman, 2005)
. However, piecewise-linear ROCs do accurately represent the discriminability supported by discrete response distributions; that is, the extent to which the response distributions can be used to distinguish targets and lures. Although this measure has not played a meaningful role in basic recognition research, it should prove useful in some research areas. For example, metacognition researchers could benefit from having a way to quantify the information loss associated with mapping an underlying evidence variable onto a confidence rating scale. Alternatively, eyewitness researchers may be interested in the ability to discriminate guilty and innocent suspects when witness responses are used as a source of evidence, as this is a problem faced by many people in the criminal justice system 
(Smith et al., 2020)
.
Many factors impact the discriminability of the response distributions, including witness memory, the position of the ID criterion, and the position of the confidence criteria. For some research questions, it is important to account for all of these factors. For example, if one procedure promotes better witness memory and another procedure promotes a more optimal setting for the ID criterion, researchers need a way to determine which procedure is superior when both of these differences are taken into account.
To order ROC points in a full lineup ROC, we recommend that researchers use DRs defined by the predictions of a decision model fit to the data. Researchers interested in full lineup ROCs have used piecewise-linear ROCs 
(Smith et al., 2020)
, so no decision model was required to define the continuous function connecting the ROC points 
(Macmillan and Creelman, 2005)
. Nevertheless, we showed that decision models are needed to put the ROC points in an appropriate order. Using a plausible model of lineup decision making, we showed that the order of DRs across responses can change substantially based on the collective ability of witnesses to remember the perpetrator, the overall level of conservativeness adopted in making ID decisions, and the policy for mapping match strength values onto confidence levels.
Thus, researchers cannot use a fixed order of ROC points across all studies, nor can they assume that different conditions in the same study will have a shared order. Unfortunately, the order of the observed DRs is also quite variable across individual samples from the same population, at least for sample sizes that are typical for lineup studies. Thus, researchers cannot simply order ROC points based on the observed DRs for a given sample. Indeed, we reported recovery simulations that found a substantial bias to overestimate AUC when observed DRs are used.
Applying an appropriate decision model offers an elegant solution to the problem of ordering ROC points. Fitting the model provides a way to adjust the order of ROC points based on performance differences from study to study, but the model also imposes constraints on the order of the DRs and thereby attenuates order variability due to sampling noise. In recovery situations, we explored the effectiveness of fitting a decision model that is appropriate for the data. For each simulated data set, we fit the Max-SDT model to the observed data and ordered ROC points based on the predictions of the model with the best-fitting parameter values. This process supported unbiased estimation of AUC; indeed, it resulted in estimates that were nearly as accurate as using the proper order of ROC points for all samples.
Note that we are not advocating use of the Max-SDT model specifically -any model that provides a good description of lineup performance could serve in this role, and researchers should use the best model available. For example, if a data set is much better fit by the Ensemble-SDT model 
(Wixted et al., 2018)
, then points should be ordered based on this model's best fit to the empirical data.
If the goal is to measure the discriminability supported by the underlying evidence distributions that determine lineup responses, we recommend the use of theoretical full ROC functions. Lineup responses are based on the probability of observing each possible lineup outcome (suspect ID, filler ID, rejection) as well as the probability of using a particular confidence level for a given outcome. For the Max-SDT model, the decision variable determining these outcomes is the maximum memory strength across lineup members. We showed that one can create a DR-based ROC from the mixture distributions of maximum memory strength across lineup outcomes ( 
Figure 7B-D)
, and the resulting function represents how effectively these mixture distributions distinguish guilty and innocent suspects. Whereas AUC for the piecewise-linear ROC is affected by both the position of the ID criterion and the positions of the confidence criteria ( 
Figure 5
), AUC for the theoretical full ROC function is not affected by changes in the confidence criteria. Thus, this measure can be useful to researchers when they want to "cancel out" the effect of confidence criteria placement; for example, if they want to compare procedures that used confidence scales with different labels or a different number of levels.
Although theory-based full ROCs can be used to eliminate differences in confidence scale use as an influence on performance measures, they do not offer a pure measure of memory. The mixture distributions of maximum memory strength are affected by changes in the ID criterion, as moving the ID criterion changes the relative proportion of identifications and rejections ( 
Figure 13
). Identifications are generally more valuable for distinguishing guilty and innocent suspects, because they reveal whether or not the suspect was the lineup member with the highest match strength, whereas rejections hide this information. Lineup researchers have focused on developing techniques for measuring memory independently of all decision processes (e.g., 
Mickes et al., 2012)
, but they also need to acknowledge that changing the ID criterion has effects that are fundamentally different than changing the criterion in a standard old/new recognition task. Changing the ID criterion does not simply move a response cutoff on a set of evidence distributions, it changes the distributions. Here, we are specifically talking about the distributions that actually determine the lineup outcomes, represented by the distributions of maximum strength for the Max-SDT model ( 
Figure 7B-D)
. Although measuring memory ability is often an important goal, it is also important to have performance measures that acknowledge this fundamental property of the lineup task.
If the goal is to measure witness memory based on lineup responses, we recommend fitting a decision model and either evaluating the memory parameters of the model directly or calculating AUC for the model-based partial ROC . In eyewitness memory research, finding procedures that promote high witness memory is both theoretically and practically meaningful.
Effective witness memory means that witnesses tend to experience much higher match strength values for guilty suspects than for innocent suspects, which can be defined as the distance between the innocent and guilty suspect distributions (e.g., 
Figure 7A
). Measuring the dispersion of these distributions is complicated by the fact that witnesses do not know which lineup member is the suspect, so their lineup responses are not uniquely determined by the suspect strength. One way to assess witness memory is to fit a decision model of the lineup task, such as the Max-SDT model, and evaluate the distributions of suspect memory strength that provide the best fit to the data 
(Cohen et al., 2021)
.
Alternatively, many eyewitness studies have measured memory discriminability with AUC for a partial ROC (e.g., 
Mickes et al., 2012)
. Our results suggest that partial ROCs are a better way to measure memory than full ROCs, even when full ROCs are defined by a continuous, model-based curve. If underlying memory strength distributions are held constant and the identification criterion is varied, AUC for the full ROC changes dramatically, whereas the partial ROCs for suspect IDs do not 
(Figure 13
).
Thus, full ROCs are not appropriate for measuring witness memory independently of other factors. Note that, under these conditions, we do not recommend using a piecewise-linear curve for the partial ROC, because AUC will be affected by decision processes such as the policy for mapping memory strength values onto confidence ratings.
Although partial ROCs are useful in general for measuring witness memory, it is important to remember that partial ROCs are not based directly on the distributions of memory match strength for guilty and innocent suspects. Instead, they are defined by the distributions of maximum memory strength for lineups resulting in a suspect ID, such as the ones shown in 
Figure 7D
. These suspect ID distributions are robust to changes in the ID criterion and the position of the confidence criteria, which is what makes them a useful proxy for measuring witness memory.
If the goal is to assess all factors that impact the information value of lineup responses, then researchers should use EIG instead of ROCs. ROCs ignore two factors that impact the information value of lineup responses: prior probability of guilt and judgment calibration. Thus, EIG is required for some research questions, and for a complete assessment of the value of identification evidence in real investigations.
Finally, we recommend that researchers carefully consider the relationship between their measure and their research question. Current eyewitness researchers have a variety of sophisticated analysis tools at their disposal, setting the stage for a wealth of new discoveries (e.g., 
Clark, 2012)
. These tools range from Expected Information Gain (EIG), which incorporates every factor relevant to decreasing uncertainty about guilt or innocence, to partial ROCs, which isolate individual components of processing, such as witness memory. EIG measures which identification procedure produces better evidence of guilt or innocence. AUC for the piecewise-linear ROC measures the relative performance of lineup procedures, assuming the effect of prior probability of guilt and judgment calibration is not of interest. AUC for the continuous full ROC measures the relative performance of lineup procedures, assuming the effect of differences in confidence scale can also be disregarded. Decision models and AUC for the partial ROC can measure changes in memory without being influenced by the position of the ID criterion and the confidence criteria, so they reveal whether differences between procedures go beyond decision making processes. Given how cleanly these tools build upon each other, they can be deployed in a staged fashion to gain a rich understanding of differences between identification procedures.


Conclusion
ROCs have been an invaluable tool for advancing knowledge of memory and decision processes in general 
(Wixted, 2020)
 and eyewitness identification processes specifically 
(Wixted et al., 2017)
. The current results clarify the relationship between alternative ROC methodologies and various research goals, set the stage for further theoretical advances, and provide guidelines for the use of ROCs in both lab-based and practical settings.       
Figure 6
. A) Partial ROC for the data in Panel C; B) Full ROC for the data in Panel C with points ordered by observed diagnosticity ratio; C) Hypothetical data from a lineup study with 3 levels of confidence. 
Figure 7
. A) Signal-detection model for the showup and lineup tasks with 3 confidence levels; B-D) Joint probability densities that the max member sampled from Panel A will have a given memory match strength and the lineup response will be a rejection, filler ID, and suspect ID, respectively, for guilty (solid line) and innocent (dashed lines) suspects. Position of the ID criterion = (µ/2 -1, µ/2 + 1); Distance between adjacent confidence criteria = (.1, .4). 
Figure 9
. Joint probability densities that the max member sampled from the Max-SDT model will have a given memory match strength and the lineup response will be a filler ID or suspect ID for guilty (solid line) and innocent (dashed lines) suspects. The model parameters were selected such that a filler ID had a higher DR than a suspect ID. 
Figure 10
. Joint probability densities that the max member sampled from the Max-SDT model will have a given memory match strength and the lineup response will be a rejection or filler ID for guilty (solid line) and innocent (dashed lines) suspects. The model parameters were selected such that a rejection can have a higher DR than a filler ID. 
Figure 11
. Proper-order, data-ordered, and model-ordered AUC estimation results (Columns 1-3) using 200, 500, 1000, or 2000 observations per data set 
(Rows 1-4)
. See text for details.        transformation of s; specifically, the proportion of the lure distribution greater than s or 1-F(s, 0). The s value that corresponds to a particular ROC x value can be found by (1 − ), where z is the inverse of the cumulative distribution for a standard Normal. The y-axis value at each point of the ROC is the proportion of the target distribution above s; in other words, the y-axis value gives the conditional probability of a correct forced-choice response for a trial with a lure with memory match strength s corresponding to a particular ROC x value. Similar to Equation A3, the overall forced-choice accuracy can be found by calculating the joint probability density of getting a lure that has a certain ROC x value and making a correct forced-choice response, and then integrating this joint probability density across all possible x values to get the overall proportion correct. Because the transformation that maps a given lure strength s to an ROC x value involves the cumulative distribution function of the lure distribution, performing this transformation for the entire distribution of lure strengths produces a distribution of x values that is uniform over the range 0 to 1. Thus, the distribution of x values has a probability density of 1 at all points between 0 and 1, and the joint probability density of a given x value and a correct forced-choice trial is equal to the conditional probability of a correct forced choice trial for that x value (i.e., the ROC y value).
As such, overall percent correct can be found by integrating the ROC y values across all possible x values,
( . ) = ∫ 1 − ( (1 − ), ) 1 0 ,
(A4)
in other words, taking the area under the ROC.
The distributions in 
Figure 7D
 are the joint probability density that the suspect has a particular match strength above the identification criterion and that the suspect strength is the highest out of all lineup members. This is found by taking the probability density for a certain match strength s on the suspect distribution (the solid or dashed distribution in Panel A for guilty and innocent suspects, respectively) and multiplying by the probability that all of the fillers are below this strength, which can be found by taking the proportion of the filler distribution (the dashed distribution in Panel A) that falls below s and raising it to the power of the number of fillers.
The distributions in 
Figure 7C
 are the joint probability density that one of the fillers would have a particular match strength above the identification criterion and that this filler would have the highest match strength of all lineup members. For a single filler, the probability density that it has strength value s and is the highest match is found by taking the probability density at s on the filler distribution (the dashed distribution in Panel A) and multiplying by the probability that all other lineup members have strength values below s. The latter value is the proportion of the filler distribution below s raised to the power of the number of fillers minus 1 (i.e., all the other fillers) multiplied by the proportion of the suspect distribution below s (the solid or dashed distribution in Panel A for guilty and innocent suspects, respectively). To get the probability density that any filler has strength value s and wins the lineup, one must multiply the probability for one specific filler by the total number of fillers.
The distributions in 
Figure 7B
 are the probability density that the highest match strength for a lineup would have strength value s below the identification criterion. For innocent-suspect lineups, the match strength values are N draws from the innocent-suspect/filler distribution (the dashed distribution in 
Figure 7A
), where N is the lineup size. The probability density that a particular lineup member would have strength value s and would also have the highest match strength is the probability density at s multiplied by the probability that all other lineup members have match strengths below s, i.e., the proportion of the strength distribution below s raised to the power (N -1). Multiplying this value by N
gives the overall probability that the maximum match value is s. For guilty-suspect lineups, the calculations depend on whether the highest match strength is the suspect or one of the fillers. The joint probability density of the suspect having the highest match at strength value s and the joint probability density of a filler having the highest match at strength value s are found with the same method described in the first and second paragraphs of this Appendix. The only difference is that the method is applied to s values below instead of above the identification criterion. The probability density that any lineup member has the highest match at strength value s is found by adding the joint probabilities for the suspect-win and filler-win cases.
Figure 4 .
4
Piecewise-linear ROC and associated regions for the ROC points ofFigure 1.


Figure Captions
Figure Captions


Figure 1 .
1
A) ROC points and model-generated continuous curve for a hypothetical old-new recognition experiment with four confidence levels; B) Distributions of responses used to create the ROC points in Panel A.


Figure 2 .
2
A) Equal-variance signal-detection model of old-new recognition with four confidence regions defined by three criteria; B) Diagnosticy ratio for each memory match strength for the model in Panel A; C) Log diagnosticy ratio for each memory match strength for the model in Panel A; D) ROC generated from the model in Panel A. See text for details.


Figure 3 .
3
A) Distributions of target and lure category members as a function of perceived line length in a hypothetical categorization experiment; B) ROC generated from the distributions in Panel A using a decision rule based on perceived line length; C) Log DR as a function of perceived line length for the distributions in Panel A; D) Regions defined by the log DR from Panel C; E) ROC generated from the distributions in Panel D using a decision rule based on the log DR in Panel C. See text for detail.


Figure 4 .
4
Piecewise-linear ROC and associated regions for the ROC points of Figure 1.


Figure 5 .
5
A,C,E) Model-based (solid) and piecewise-linear (dashed) ROCs for the SDT models in Panels B, D, F, respectively; B, D, F) SDT models and response criteria used to generate the ROCs in Panels A, B, C, respectively.


Figure 8 .
8
Distribution of diagnosticity ratio ranks for each of the nine possible lineup responses simulated from the Max-SDT model. Parameter ranges: Mean of the target distribution (µ) = (.5, 2.5);


Figure 12 .
12
A screenshot from the Shiny app located at <https://tuttlem.shinyapps.io/fullroc_finalapplication >. The first column of plots show the distributions of max-member memory match strength for each of the three lineup outcomes (Suspect ID, Filler ID, Rejection). The top right plot shows the log diagnosticity ratio at a given max memory strength for suspect IDs (solid), filler IDs, (dotted line) and rejections (dashed line). The lower right plot shows the ROC.


Figure 13 .
13
Full ROCs generated with fixed memory strength distributions and varying identification criteria. The vertical lines are the innocent suspect rates associated with each criteria.


Figure 1
1
Figure 1


Figure 2


Figure 5


Figure 7


Figure 10


Figure 12


Because match strength and DR are monotonically related in this model, basing forced-choice on either of these measures would lead to equal performance, but, as shown below, that won't always be the case. For interested readers, Appendix A provides a detailed explanation for the correspondence between AUC and forced-choice accuracy.


a distribution does not have a yellow line (like rejections in this case), then it does not contribute to the hit and false-alarm rates. The resulting point for this particular log DR criterion is shown as the yellow circle


EIG can also be called the mutual information between witness responses and suspect guilt or innocence
(Shannon, 1948)
.








Appendix A: Forced-Choice Proportion Correct and AUC
Consider a forced-choice trial in which the lure has memory match strength s, corresponding to diagnosticity ratio r. For an optimal decision maker, the probability of responding correctly on such a trial is determined by the proportion of the target distribution that produces a diagnosticity ratio greater than r, or, put another way, the probability that the target would produce stronger evidence of being a target than the lure. For the model in 
Figure 2A
, the diagnosticity ratio monotonically increases with memory match strength ( 
Figure 2B
), so the proportion of targets with a diagnosticity ratio greater than r is equivalent to the proportion of the target distribution exceeding strength s. This quantity is given by
where s is the lure memory match strength, p(correct | s) is the probability of a correct response given s, µ
is the mean of the target distribution, and F(s, µ) is the cumulative distribution of a Normal distribution with mean µ and standard deviation 1.
Equation A1 provides the probability correct for a given lure memory strength s. The next step is to calculate the probability correct across all possible s. First, note that the joint probability density that a forced-choice trial has a lure of memory match strength s and a target memory match strength greater than s is given by
where f is the probability density function for a standard Normal distribution (i.e., the lure memory match strength distribution). Then, integrate over all possible s,
The result is the total probability that a target is stronger than its paired lure across all possible forcedchoice trials, or, in other words, the overall probability correct, p(correct), for an optimal decision maker.
As discussed previously, Equation A3 also provides another way to conceptualize AUC.
It might not be immediately obvious why Equation A3 is equivalent to AUC. Equations A1-A2
assess forced-choice accuracy as a function of lure memory match strength, s. In an ROC, the x-axis is a Appendix B: Deriving the Max-SDT Prediction Distributions
 










Signal detection theory and human memory




W
P
Banks




10.1037/h0029531








Psychological Bulletin




74
















A violation of the conditional independence assumption in the two-high-threshold model of recognition memory




T
Chen






J
J
Starns






C
M
Rotello




10.1037/xlm0000077








Journal of Experimental Psychology: Learning, Memory, and Cognition




41
















A memory and decision model for eyewitness identification




S
E
Clark




10.1002/acp.891






Applied Cognitive Psychology




17
















Costs and benefits of eyewitness identification reform: Psychological science and public policy




S
E
Clark




10.1177/1745691612439584






Perspectives on Psychological Science




7
















Estimating the proportion of guilty suspects and posterior probability of guilt in lineups using signal-detection models




A
L
Cohen






J
J
Starns






C
M
Rotello






A
M
Cataldo




10.1186/s41235-020-00219-4






Cognitive Research: Principles and Implications




5


21














sdtlu: An R package for the signal detection analysis of eyewitness lineup data




A
L
Cohen






J
J
Starns






C
M
Rotello




10.3758/s13428-020-01402-7


32700238






Behav Res Methods




53
















A signal detection model of compound decision tasks. (Tech Note DRDC TR 2006-256). Toronto: Defence Research and Development Canada




M
Duncan


















Recognition memory and the operating characteristic




J
P
Egan












USAF Operational Applications Laboratory Technical Note












Likelihood ratio decisions in memory: Three implied regularities




G
Glanzer






A
Hilford






L
T
Maloney








Psychonomic Bulletin and Review




16
















MINERVA 2: A simulation model of human memory




D
L
Hintzman




10.3758/BF03202365








Behavior Research Methods, Instruments & Computers




16
















Four utilities in eyewitness identification practice: Dissociations between receiver operating characteristic (ROC) analysis and expected utility Analysis




J
M
Lampinen






A
M
Smith






G
L
Wells




10.1037/lhb0000309






Law and Human Behavior




23
















Training for calibration




S
Lichtenstein






B
Fischhoff




10.1016/0030-5073(80)90052-5






Organizational Behavior and Human Performance




26
















Memory and the theory of signal detection




R
S
Lockhart






B
B
Murdock




10.1037/h0029536








Psychological Bulletin




74


















N
A
Macmillan






C
D
Creelman




Detection Theory: A User's Guide


Mahwah, NJ




Lawrence Erlbaum Associates








2nd ed.








Monotonicity of rank order probabilities in signal detection models of simultaneous detection and identification




C
G
Meyer-Grant






K
C
Klauer




10.1016/j.jmp.2021.102615






Journal of Mathematical Psychology




105


102615














Receiver operating characteristic analysis of eyewitness memory: Comparing the diagnostic accuracy of simultaneous and sequential lineups




L
Mickes






H
D
Flowe






J
T
Wixted








Journal of Experimental Psychology: Applied




18
















Likelihood ratio evidence-accumulation models of recognition memory




A
F
Osth






S
Dennis






A
Heathcote








Cognitive Psychology




92
















The Confidence-Accuracy Relationship for Eyewitness Identification Decisions: Effects of Exposure Duration, Retention Interval, and Divided Attention




M
A
Palmer






N
Brewer






N
Weber






A
Nagesh








Journal of Experimental Psychology: Applied




19
















A non-parametric analysis of recognition experiments




I
Pollack






D
A
Norman








Psychonomic Science




1
















A theory of me"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]