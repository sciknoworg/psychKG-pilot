You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Acknowledgments
I consider myself exceptionally fortunate to have been mentored by world-class scientists, surrounded by the sharpest colleagues, and supported by loving family and friends throughout my academic journey. It is difficult to express how much these people mean to me, both professionally and personally.
This work was completed under the supervision of my research advisor, Alex "Sandy" Pentland. I couldn't have asked for a better thesis adviser and mentor than Sandy. He never forced an agenda on my research, but instead he provided me with guidance and a nurturing environment in which I was able to develop my research interests and pursue my own path. His personal assistance and friendly nature has always made me feel at ease with him and I could always look to him for support.
Sandy will continue to be a role model to me as an academic entrepreneur who engages with a wide range of different types of people and technical areas. 


3-1
An illustration of the overall experimental design.In study 1, the feedback level is fixed (i.e., full feedback) and network plasticity is manipulated (i.e., static network versus dynamic network). In study 2, plasticity is fixed (i.e., always dynamic network) and feedback is manipulated (i.e., no feedback, self feedback, and full feedback). . . . .


3-2
Illustration of the experimental conditions in study 1. Panel (A) depicts the Solo condition (i.e., no social information) where participants make independent estimates. This condition corresponds to the baseline wisdom of the crowd context. Panel (B) describes the Static network condition (i.e., social learning) where participants engage in a stage of interactive social learning, where they are exposed to the estimates of a fixed set of peers in real time. Panel (C) describes the Dynamic network (i.e., selective social learning) condition that adds the possibility for participants to choose who to follow and be influ- The reduction is notably larger and more significant in periods where networks had adapted to the information environment (i.e., rounds 
[6,
10]
 and 
[16,
20]
). 


3-8
Mean-variance trade-off. Mean and standard deviation of absolute errors incurred by top-k estimates during the adapted periods (rounds ∈ 
[6,
10]
 ∪ 
[16,
20]
). Top-12 estimates correspond to the full-group mean, and top-1 to the group's best individual. Within each condition, top-k trade-off curves first gain in both objectives, then trade off lower average error for higher variability, and finally regress in both objectives as → 1. Across conditions, for any ∈ 
[1,
12]
, groups in the dynamic condition outperformed groups in the static and solo conditions. Moreover, the full-group mean of dynamic networks averaged 28% lower error and 48% less variability than the best individual playing solo (dynamic top-12 vs. solo top-1 ; < 10 −2 ); and the best individual in the dynamic condition averaged 32% lower error and 38% less variability than her analogue in solo (dynamic top-1 vs.  A-5 Participants who obtained a higher score on the two hard tasks in the phase one experiment (i.e., "high skill") outperformed participants who obtained a lower score on those two hard tasks (i.e., "low skill") on each single task instance. Error bars represent 95% confidence intervals. . 


Relation between team's average skill level and team performance.
Data is combined across teams in all six blocks, and for all five tasks.
Models relate performance measures (standardized within each task)
with the team's average skill level. All models included random effects for teams as intercept to account for dependence across tasks (i.e., random effects are clustered on each team, using team id as the identifier).
Increasing a team's average skill significantly increases the team's score in solving CSOPs, but has no effect on duration or efficiency. . . . . . 


Relation between team's average


Chapter 1 Introduction
A substantial body of work has shown that a group of individuals can often achieve higher levels of intelligence than their members working alone. For example, the classical concept of the "wisdom of crowds" articulates how-in a startlingly wide range of settings-the aggregate (e.g., average) estimate of a group is better than the estimate of the best-performing individual. Examples include financial markets, which provide a mechanism for revealing investors' private information in order to arrive at a global estimate of value, and democracy, which aggregates differences of opinion to reach a collective decision on who should lead us.
The use of these (e.g., teams, markets, polls, and votes) and related modern mechanisms is on the rise, finding applications in areas as diverse as problem-solving 
[136,
108]
 technological and economic forecasting 
[140,
215]
, crowdsourcing 
[100,
33,
194]
, product rating 
[192,
144]
, public policy design 
[137,
176]
, and mapping natural disasters 
[133,
70]
-just to mention standouts. At the same time, there are many instances of collective failure-from market crashes to the spread of false and harmful rumors.
Such collective decision systems are central to the way society organizes and allocates resources; hence, providing a sound understanding of and useful design guidelines for improving the performance of collective decision systems is of paramount importance.
Although recent availability of massive digital traces on human behavior and the ubiquity of computational approaches have both extended and changed classical social science inquiry 
[174]
 bringing the era of computational social science 
[117]
 and the emergence of network science 
[22,
209]
 (see Section 1.1). These advances have allowed scientists to generate a tremendous number of studies on collective intelligence, but they have been much less successful at reconciling some of the many inconsistencies and contradictions amongst them. For instance, studies have shown that the same attribute of interest (e.g., social interaction via communication networks, cognitive style diversity of team members, etc.) can either promote the "wisdom of the crowd" or, conversely, lead to the "madness of the mob." In general, for the same social context being studied and for the same global feature of interest, different theories have disagreed on which attributes are most relevant, and empirical studies offered an overwhelming lack of consistent evidence (see Section 1.2 on the incoherency problem).
However, I argue that many of the studies on this topic only consider, explicitly or implicitly, static and stable environments, offering at best a partial view of human collective decision making (see Section 1.3 on the need for an environment-dependent framework ).
In this dissertation, I address the question of the determinants of collective intelligence using an illustrative example (see Section 1.3.1) and a series of human experiments and supporting simulations (see Chapters 2 and 3). The results show that what is optimal always depends on the environment, and that groups provided with appropriate learning mechanisms can adapt to biased and non-stationary information environments, significantly improving both individual and collective judgments.
The findings presented in this thesis can help reconcile some previously conflicting claims from the collective intelligence literature and motivate a future research program to more systematically identify stable principles of collective performance (see Chapter 4).


Premise: A Unifying Theory
Many scientists continuously aspire to discover universal principles that are valid across many different systems. While the domain of physical systems has offered examples of such widely applicable "laws," social phenomena have tended to be less fruitful in terms of generating such generalizations. This desire to build models of social phenomena that are as predictive as those in physics, as well as the pursuit of unifying principles and operationally meaningful theorems in the social sciences, has has been termed "physics envy" in the social sciences 
[123,
125,
47]
. While physicists can explain most of all observable physical phenomena using Newton's three laws of motion, social scientists (probably) wish they had 99 laws that explain 3% of human behavior. It is not only that social science has one theory for one thing and another theory for another thing 
[94]
, but rather that it has many theories for the very same thing 
[210]
.
The unfavorable comparison of social sciences to the natural sciences (and physics in particular) has a long 
[141]
 and quite unproductive history (e.g., see Watts argument against it in 
[210]
). However, is it possible that this state of affairs has changed with the study of computational social science and complex networks emerging into prominence?
The Era of Computational Social Science. Recent widespread adoption of electronic and pervasive technologies, the development of e-government, and open data movements have enabled the study of human behavior at an unprecedented level and helped uncover seemingly universal patterns underlying human activity.
Lazer, Pentland et al. 
[117]
 formally introduced computational social science (CSS)
as a new field of research that studies individuals and groups in order to understand populations, organizations, and societies using big data 1 , i.e. phone call records 
[4,
5,
11,
10]
, GPS traces 
[104]
, credit card transactions 
[184,
52]
, web page visits 
[59,
7]
, emails 
[105,
16]
, and data from social media 
[152,
6,
12,
8]
. Driven by the ubiquitous availability of data and inexpensive data storage capabilities, the concept of big data has permeated the public discourse and led to surprising insights across the sciences and humanities. Such understanding can answer epistemological questions on human behavior in a data-driven manner, and provide prescriptive guidelines for persuading people to undertake certain actions in real-world social scenarios. In particular, this availability of data over the past fifteen years has shed light on the important role networks play in human society.
The Emergence of Complex Networks. At least for half a century now, there has been a surge in data availability and the ability to represent different systems (e.g., physical, biological, social, and technological) as a collection of nodes (or entities) connected with each other according to specific link topologies. Examples range from the tiny intracellular system, which consists of different molecules signaling each other via chemical reactions, that determines our biological existence, to the enormous cosmic web composed of discrete galaxies held together by gravity that determines the fundamental structure of our universe. We also see networks between these two scales, from food webs that represent the who-eats-whom (or interdependence) between species in ecology to social actors-be they individuals, organizations, or nations-exchanging ideas and favors in a social system. Indeed, recent research efforts have revealed a number of distinctive structural properties that many networks seem to share across many domains. Such properties include the "small world" effect 
[211,
208]
, the right-skewed degree distribution 
[21]
, clustering 
[149]
, and community structures 
[80]
. Considering the ubiquity of networks and their structural properties, much effort has been made to understand the relationship between network structures and a system's function. This is a topic that is of utmost rele- "Should social science be more solution-oriented?" highlighted that for any topic of which he has undertaken a great amount of studying-be it cooperation mechanism, organizational performance, collective action, network dynamics, systemic risk-one would likely encounter the problem of irreconcilable results 
[210]
. In this dissertation, I argue that the topic of collective intelligence is no exception.
For instance, studies that focused on the patterns of social interactions on collective intelligence found that social influence can promote the "wisdom of the crowd" 
[26]
 and, conversely, lead to the "madness of the mob" 
[127]
. Inefficient communication structures simultaneously enhance 
[116,
58]
 and hinder 
[136,
82,
26]
 collective performance. Weak bridging ties are advantageous for innovation 
[87,
165,
168]
, as well as the opposite-strong cohesive ties are more advantageous 
[202,
203,
164,
204]
. Other studies have found that network structures can affect (i.e., promote or hinder) cooperation 
[162,
74,
38]
, while others report no relationship between network structure and cooperation levels 
[193]
. Homogeneity of tie strengths have been found to be beneficial for coordination and also can derail it 
[38,
157,
158]
.
The issue of incoherency exists in many modeling and empirical settings, not just in those that focus on social network phenomena mentioned in the previous paragraph. For instance, when it comes to the composition of the group (i.e., the attributes of the constituents), some studies have identified the individual ability of the group members as an important predictor of collective performance 
[187,
60,
27]
, while others report no or at best a weak relationship 
[217,
166,
154]
. Skill diversity (i.e., variance in group members' ability) has been shown to both enhance 
[98]
 and handicap collective performance 
[18,
67,
60]
. Similar inconsistencies arise for cognitive style diversity 
[118,
150,
3]
, social perceptiveness 
[217,
69,
72,
120]
, and even the relative performance of teams versus individuals 
[48,
190,
219]
.
In general, for the same social system being studied and for the same global  
[40,
185,
191,
41,
37,
169,
172]
.
Therefore, I hypothesize that in order to systematically reconcile these contradictory results, we need to understand the relative importance of the determinants of collective dynamics under an environment-dependent framework. If the actors are the subjects, then the environment is the object (i.e., the stimulus). Therefore, the environment itself is system-dependent (e.g., may vary from one social system to another). In the case of problem-solving and collective intelligence, the environment may be characteristics of the task (e.g., complexity, type, information distribution across agents, etc.); in a product diffusion setting the environment may be characterized by the thing being diffused (e.g., product characteristics); and in cooperation or coordination games the environment may be characterized by the incentives (the payoff matrix, the rate of interaction, mutation, etc). 


Illustrative Example
To illustrate how paying little attention to the environmental conditions can lead to inconsistent conclusions, I will use a structural stability approach common to the study of ecological communities 
[169,
40]
. 
+1 = ( − )
In this model, the strength of beliefs in a community about some topic (e.g., the existence of supernatural agents) is represented by the -dimensional vector ,
where , corresponds to the strength/level of belief of individual at time . The temporal evolution of beliefs (e.g., how individuals update their beliefs in the next time step, +1 ) is a function of the beliefs at any given point , the vector of intrinsic attributes of individuals (i.e., confirmation bias, which is the rate at which individuals increase/decrease their belief independently about the topic), and the interaction matrix that captures the structure of social influence (i.e., the attribute of interest). Note that the confirmation bias rates are inherently linked to environmental conditions-in other words, the events that the individuals encounter in their environment. If we take our measure of collective outcome to be the persistence of the belief in the community (i.e., there are no non-believers at equilibrium), then this
implies * = −1 ( − 1) > 0.
We can see that this condition will be satisfied as long as the vector of confirmation bias rates falls inside a feasibility domain constrained by the interaction matrix 
[173,
40]
. Formally, this domain is defined by:
( ) = { = * ,1 1 + ... + * , > 0},
where is the ℎ column of the interaction matrix . Now, for simplicity (and to be able to depict the system graphically), let us assume that we have two types of In 
Figure 1
-1, it is easy to see how three different investigations can reach different conclusions about the role of social network structure in the persistence of beliefs in a community. For instance, if the first investigator sets the confirmation bias values to 1 then the conclusion that will be reached is that social structure 1 is superior to 2 when it comes to the persistence of beliefs. On the other hand, another investigator that sets the confirmation bias to 2 will reach exactly the opposite conclusion. A final investigator choosing 3 (or implicitly assuming no confirmation bias i.e., = [0, 0]) will find no relationship between network structure and the persistence of beliefs.
In this example, one might be tempted to come to the general conclusion that network structure 1 is superior to 2 when it comes to persistence of beliefs because of the relative sizes of the feasibility domains of these two networks 4 -i.e.,
( ( 1 )) > ( ( 2 ))
. This can be interpreted as follows: if the en- vironment values were uniformly sampled from the parameter space, then it is more likely to achieve the desired collective behavior under network structure 1 than 2 .
However, this conclusion has no conceptual support, as the environmental conditions we care about are usually characterized by a distribution (e.g., set of environmental conditions over a period of time), rather than any particular point in the parameter space-that is, in the field, it is virtually impossible to measure the environment exactly. Therefore, what we care about is the overlap between the environmental conditions in a given setting/time and the feasibility domains defined by social network structures. In 
Figure 1
-2, we can see that under different environmental conditions, what network structure is "best" can vary. Therefore, independent of the environment, there is no conceptual support of either a positive, negative, or no association between network structure and function, even when we can fully characterize the feasibility domains.


Conceptual Reflections and Dissertation Organization
Overall, this simple conceptual analysis demonstrates that the association of a given attribute of interest (i.e., network structure, in this case) with global outcome depends on the environment. Therefore, without an environment-dependent framework from which to draw hypotheses and tune our intuitions, it is difficult to distinguish results that are unusual and interesting from results that are unusual and probably irrelevant (i.e., wrong or not generalizable).
Additionally, in some areas where there is a premium on slick studies with surprising results, 'surprising' should occur with reference to the particular region in the parameter space, and not in absolute terms. For instance, if a counterintuitive result can only emerge under very specific environmental conditions that are narrow and not
representative of the conditions we care about or only occupy a small region in the parameter space that we rarely encounter, then how much does this result matter?
In the rest of this dissertation, I will continue to illustrate the importance of the environment-dependent approach by conducting human experiments and simulations where we explicitly manipulate the environment (i.e., task characteristics in Chapter 2 and information distribution in Chapter 3). In order to conduct those studies, we built an experimentation platform that allows for conducting behavioral experiments of a scale, duration, and realism that far exceed what is possible in brick-and-mortar facilities and blur the line between lab and field experiments (see Chapter 4).
One of the main contributions of those studies is having a framework where the environment in which groups are situated is explicitly defined and manipulable (e.g.,
to simulate the non-stationarity of the environment). For instance, by manipulating the environment (and keeping everything else fixed), we can show how some of the seemingly contradictory results in the literature can be obtained as a function of the environmental conditions (i.e., whether nominal teams are better than real teams as in Chapter 2; or whether efficient network structures are more advantageous as in Chapter 3). Therefore, this allows us to reevaluate the importance of some of these attributes from the point of view of the environment (i.e., the conditions under which those attributes are of relative importance).
In particular, In the first study (i.e., Chapter 2), we focused on how different individual level attributes and group compositions (e.g., skill, cognitive style diversity, social perceptiveness) can affect collective performance, and examined whether those effects are robust to environments (i.e., tasks) of variable complexity. In this study, we asked two main research questions: 1) Do groups perform worse than comparable individuals on simple tasks but better on complex tasks?; 2) Do the effects of group composition on group performance vary with task complexity?
In the second study (i.e., Chapter 3), we focused on the role of dynamic communication structures on promoting collective intelligence. In recent years, both theoretical and experimental work has been limited mainly to frameworks where agents are placed in static social structures in stable environments. Yet, it is increasingly recognized that most natural and social systems are best described as "dynamic" networks, with links existing only intermittently in response to environmental variations (i.e., the different environments the group can be situated in). In Chapter 3, we shift the focus on the role of communication networks from the purely structural aspects of the topology to the role environmental changes play in determining the dynamical processes defined on it. This would mean changing the usually ill-defined question "Which network structure is best to promote collective intelligence?" to "What mechanisms should we provide the social system to enhance its ability to adapt in a changing environment?" In the context of collective intelligence and group problem-solving, this dissertation overcomes some of the common limitations of prior studies by considering dynamical social influence networks where individuals can actively choose and dynamically rewire their social connections in non-stationary environments, which narrows the gap between stylized experiments and real-world social contexts.
In order to operationalize the "environment" in our lab settings, we built an experimentation platform (Empirica.ly). The platform forces the investigator to explicitly define the space of the environment in which the group of participants is situated, and therefore, the exploration of the interactions between the environments and the attributes of interest becomes more systematic (as opposed to having isolated and non-comparable studies). It is necessary to acknowledge that this remains a simplistic view of real social system environments. In real-world social systems (e.g., health, education, inequality, cultural norms, economic policies) environments are high-dimensional and interact in much more complicated ways in order to produce particular individual and group outcomes 
[210]
.


On the Shoulders of Giants
Samuel Taylor Coleridge, in The Friend (1828), wrote:
The dwarf sees farther than the giant, when he has the giant's shoulder to mount on.
Indeed, academic advancements rarely happen in a vacuum, but transpire as we build on ideas and tools from others: a path-dependent wisdom of crowds. The work of this dissertation is inspired by and built upon a few common patterns that have emerged in several different research programs. Here, in addition to the structural stability approach used for the illustrative example earlier 5 , I will briefly mention severalnon-exhaustive 6 -strands of academic research that lend support to the environmentdependent framework presented in this thesis.


Representative Design
There is little technical basis for telling whether a given experiment is an ecological normal, located in the midst of a crowd of natural instances, or whether it is more like a bearded lady at the fringes of reality, or perhaps like a mere homunculus of the laboratory out in the blank. 
[34]
 Egon Brunswik developed an innovative methodological framework called representative design, where he wondered why the logic we demand for generalization (i.e., sampling theory) over the subject side 7 is ignored when we consider the object side (i.e., conditions, stimulus, input, or environment)? In particular, he highlighted that one may only generalize the results of observations and experiments to those environmental conditions (or objects) that have been sampled in the experiment-in the same way that scientists apply this principle to the subjects (i.e., the participants) 
[61]
. That is, to study the agent × environment relations, the environmental conditions should be sampled from the agent's natural environment in order to be representative of the population of environments to which it has adapted and to which the experimenter could generalize. Therefore, Brunswik called for an explicit theory of the environment in experimental psychology 
[85]
. Similar to the discussion (in Section 1.3.1) on feasibility regions, Brunswik argued that experimenters should avoid oversampling highly improbable conditions (or conditions that do not exist in the population), because even if the results from those conditions are interesting, are they really relevant?
5 I have already demonstrated how tools borrowed from the study of ecological communities 
[40,
173,
169]
 are valuable for viewing communities through the lens of their environmental variations. 
6
 Interested readers should also refer to the No Free Lunch Theorem 
[102,
95,
216]
 and the Contingency Theory of Organizations 
[63]
. 
7
 In actuality, as Henrich 
[93]
 pointed out, most participants in psychological experiments are WEIRD; also see Chapter 4.
Since Brunswik's time, the idea that human behavior is shaped by the environment structure has been generally accepted (e.g., ecological rationality, evolutionary psychology/game theory, ecological psychology) and the concerns with the limited generalizability of research findings have been expressed periodically. Nonetheless, there were two mainstream criticisms of the representative design approach that I want to highlight here. First, there are concerns regarding the associated difficulty of implementing representative designs 
[50]
. How can one possibly sample situations?
However, I would argue that the difficulty of sampling situations 8 can be overcome with modern technologies, such as the Web, to effectively reproduce and explore environments (see Chapter 4 on high-throughput social science using virtual labs).
The second objection argues that, even if we could define and sample the environment, there is no need to do so. After all, the goal of the social scientist is not to generalize the results from the experiment to situations 'outside' the experiment, but to test hypotheses and advance particular theories. This criticism is brought on by the strong emphasis on ensuring internal validity for the sake of replicability, at the expense of external validity. In other words, this objection presupposes that the purpose of social science experiments is not to solve practical problems in the real world.


Solution-Oriented Social Science
Duncan Watts has argued in a recent article that social science should be more "solution-oriented" in order to reconcile the competing claims in the literature (i.e., the incoherency problem in Section 1.2). That is, the research community needs to place more emphasis on solving practical problems-the sort with direct engineering analogues 
[210]
-rather than the advancing of particular theories. For instance, in the article Watts suggests asking questions like:
• "How do I maximize the impact of my advertising spending?"
• "How do I increase productivity in my organization?"
• "How do I increase pro-social behaviour in my community?" I want to argue that Watts's perspective is akin to the environment-dependent framework proposed in this dissertation. In all of these questions, the locust of activity (e.g., "in my organization") is limiting the generality of the answer to the objective (e.g., "how to increase productivity"). In other words, the answer to the first part of the sentence is dependent on the conditions specified in the second part. Hence, the answer will be relative, not absolute, which-I will argue-will lead to reliable and coherent results, not falsely conceived as universally valid.


Adaptive Market Hypothesis
Andrew Lo 
[123,
124]
 applies the principles of biological evolution (i.e., competition, adaptation, and natural selection) to financial markets. In particular, the approach focuses on explaining how emergent market attributes (e.g., prices) are related to the interaction of distinct groups of market participants within a specific environmental conditions (e.g., regulations, number of competitors, magnitude of profit opportunities).
In particular, the Adaptive Market Hypothesis asserts that market behavior adapts to a given financial environment, and an efficient market (the dominant theory of markets) is merely the steady-state limit of a market in a static financial environment; an idealized market is unlikely to ever exist in practice.
The Adaptive Market Hypothesis is specifically studying the individual-level investor (i.e., economic agent) as well as the larger market (i.e., macroeconomy). However, I think the adaptiveness to environmental conditions approach applies to other collective social phenomena, more generally, and for the same reasons (i.e., evolutionary processes working in a non-static environment). In this work, we see how similar ideas can expand beyond the domain of financial markets.


Ecological Rationality
Ecological rationality 
[198,
183,
81]
-proposed by the German psychologist Gerd Gigerenzer of the Max Planck Institute for Human Development-in contrast to rational choice theory, maintains that the rationality of a particular decision depends on the context of circumstances in which it takes place. Therefore, what is considered rational under the rational choice account that focuses on agent characteristics (e.g., preference consistency) might not be considered rational under the 'ecological rationality' account, which also considers the structure of the environment.
This approach to decision-making is inspired by earlier work by Herbert A. Simon on heuristics and bounded rationality 
[181]
. In particular, he explored how heuristics (a decision strategy that partially ignores available information) in appropriate context can achieve higher intelligence than other more complex approaches. The ecological rationality focuses on individual-level decision making, while in this dissertation we investigate the emergent phenomenon of collective behavior.


Chapter 2 Varying Environmental Complexity
Recent work on teams has emphasized the counterintuitive claim that the absolute skill level of team members matters less to collective performance than other factors such as skill diversity, cognitive style, and social perceptiveness. Through a novel two-phase experiment (phase one = 1200, phase two = 828; pre-registered 1 ) in which individual on-task skill, cognitive style, and social perceptiveness were measured ex-ante and then systematically varied in team composition, we show that the effect of skill on team score is larger than all other factors across environments (i.e., tasks) of widely varying complexity. More importantly for practical applications, skill predicts twice as much out-of-sample variance as all other factors combined. We also show that while teams outperform comparable individuals on average, when compared with the best member from a same-sized group of individuals, teams score worse but compensate with faster completion time and higher efficiency when the task environment is complex. Our results help to clarify inconsistencies in the existing literature on the relationships between team construction and performance; they highlight the value of online experiments capable of supporting large sample sizes and complex, multifactorial designs; and they motivate a future research program to identify stable principles of collective performance (see Chapter 4).


Environments of Widely Varying Complexity
As organizations have moved inexorably to more team-based structures, the problem of improving team performance through judicious selection of team members has preoccupied management scientists and managers alike 
[115,
2,
101,
197,
145]
. Previous research has found a variety of intriguing results regarding the impact of skill diversity 
[98]
, cognitive style diversity 
[68]
, and social perceptiveness 
[110,
217]
  is little consensus on whether teams always outperform independent individuals (i.e., the relative performance of teams versus individuals) 
[48,
219,
190]
.
Reading this literature, a hypothetical manager wishing to construct a team for some task (or environment) would have difficulty deciding whether for a particular task would team be less/more effective than their members, which of potentially many individual-level attributes to measure, how to optimally combine individuals with those attributes, and how that combination might depend on the difficulty of the task at hand. Moreover, because the effects of different combinations of attributes are typically expressed in terms of regression coefficients, not their ability to predict the outcome of interest, it is unclear how much control over performance the manager could expect to exert in practice 
[122,
96,
220]
. Here we address these limitations by using a novel two-phase experiment to answer two main questions: 1) Under what conditions, if any, do teams perform better than individuals? 2) Which of the four widely studied attributes of teams-average skill level, skill diversity, cognitive style diversity, and social perceptiveness-individually and collectively dominate team performance (effect size and predictive power) and does it vary with task complexity?
Our experimental design exhibits five important features that address limitations with previous studies and speak directly to the hypothetical manager's problem outlined above:
1. By varying the difficulty of the task (i.e., the environmental conditions) over a wide range (from "easy" to "super hard") without changing the nature of the 


Experimental Setup
Specifically, participants were asked to solve a "room assignment" problem in which they had to assign "students" to "rooms" where each student had a specified utility for each room. Participants' objective was to maximize total student utility while also respecting constraints (e.g., "Students A and B may not share a room or an adjacent room"). Task difficulty (or the "environment complexity," therefore, could be varied systematically by changing the number of students ( ), the number of rooms ( ), and the number of constraints ( ). After completing five such tasks, each participant also completed a standard "Reading the Mind in the Eyes" (RME) test 
[25]
, which is commonly used as a measure of social perceptiveness (see   perceptiveness level, and cognitive style. Then, in phase two (see Section 2.2.3), we deployed a block randomization scheme to randomly assigning participants into one of six blocks based on their phase one measurement results. Within each block, we randomized whether the participants will work as individuals or in teams (i.e., teams of three randomly selected participants) to solve another set of room assignment problems. Teams were also provided with a chat box, enabling them to communicate freely with each other and scores were now assigned to teams not individuals. See Finally, we used the ex-ante measurements from phase one to construct the inde-pendent variables (i.e., whether the participants were assigned to individual or team condition as well as different influencing factors of team performance) and used the actual performance in phase two as the dependent variables, which together allowed us to examine the performance effects of being in a team versus an individual and the effect of different team compositions (see Section 2.2.4).
H H H ML block L L L H L L L H MH block H L H H H L H H LL block L L L L L L L L LH block H L H L H L H L
The experiment was developed using Empirica (https://empirica.ly/), an opensource "virtual lab" framework and a platform for running multiplayer interactive experiments and games in the browser 
[156]
.
The source code for the Room Assignment Tasks can be found at here, and the source code for the Reading the Mind in the Eye Test can be found at here .


Room Assignment Problem
In our experiments, we asked participants to solve room assignment problems, first individually and then within a team. A room assignment problem is a type of Constraint Satisfaction and Optimization Problem (CSOP, that is, an optimization problem on top of a constraint satisfaction problem) 
[79,
200]
. We chose this task for three reasons. First, CSOPs are an abstraction of many resource allocation and optimization problems; thus, they capture important features of real-world team problem solving exercises without requiring participants to have specialized skills. Second, the payoff function for CSOPs can be described as a "rugged landscape" characterized by many locally optimal but globally suboptimal solutions. Correspondingly, CSOPs are amenable to potentially many solution strategies and styles, where no single strategy is universally superior. Third, the complexity of CSOPs can be systematically varied by adjusting a few key parameters; in our case, by changing the number of students , the number of rooms , and the number of constraints .
In our operationalization of this problem, participant(s) were tasked with assigning each of "students" to one of "rooms," while also respecting constraints on their choices (e.g., students A and B must be neighbors, must not share a room, etc.). In each room assignment problem, a "utility table" was presented, providing participant(s) with the information on students' ratings (between 0 and 100) to each of the rooms indicating how satisfied they would be if being assigned to the room.
The participant(s) was then asked to find a room assignment plan that maximized satisfaction across all students without violating any constraints.
To incentivize the search for an optimal solution (i.e., the optimal room assignment plan) we provided participant(s) with additional bonuses based on how good their submitted solutions for the problem were. In particular, we defined the "score" of a room assignment plan as the following:
= The sum of students' ratings of their assigned rooms − 100 × the number of violated constraints
By submitting a complete plan (that is, each student got assigned to one room) with a positive score in a room assignment problem, participant(s) could earn a "performance-based bonus" using a 500 points:$1 USD conversion rate to exchange scores into payments. Moreover, if the submitted plan was indeed the optimal one, an additional $0.5 USD "optimal assignment bonus" would be given 2 . We determined these values for the payments by conducting a series of pilot studies and observing how participant behavior responded to different payment schemes. For screenshots of the task, see Appendix A.1 and A.3.


Reading the Mind in the Eyes (RME)
Each participant also completed the revised version of the "Reading the Mind in the 


Participants Recruitment
All participants were recruited on Amazon Mechanical Turk (MTurk 3 ), which is an online labor market with a large and diverse pool of people ready to promptly perform tasks for pay (called human intelligence tasks, or HITs) 
[9]
. We recruited our participants by posting a HIT for the experiment, entitled "Play games and get up to $17 in total pay," a neutral title that was accurate without disclosing the purpose of the ex- 


Design of Phase One Experiment
In phase one of the experiment, participants were asked to complete a sequence of 36 "Reading the Mind in the Eyes" (RME) test questions as well as a sequence of 5 room assignment tasks. More specifically, CSOP and RME were implemented as two distinct web apps, each of which appeared as a separate link in the MTurk iframe.
The order of the links was randomized for each participant but they could choose to click on them in whatever order they wished. For the RME questions, participants were shown in each question a pair of eyes and were asked to select one of the four words that best describe the emotions shown by the eyes (See Appendix A.2 for an illustration of the test).
For the room assignment task part, we first introduced participants to the problem and each completed one practice task (as per our pre-registration, it is not included in the analysis), in which = 8 students need to be assigned to = 5 rooms while respecting = 4 constraints. Each participant was then given a sequence of five room assignment tasks, to be completed independently, where the maximum amount of time a participant could spend on a task was 5 minutes. 
Table 2
.1 summarizes the main properties of the five task instances used in phase one. As shown in the table, we intentionally included 3 easy task instances and 2 hard task instances in the sequence. We did not randomize the order of the task instances in phase one to minimize the noise in the measurement of individual skill due to random ordering effects. We included more easy task instances than hard task instances in phase one to minimize potential self-selection in phase two of our experiments (i.e.
where only participants who did well in phase one would return for phase two 4 ), which turned out to be very effective (see Appendix A.5 for more details).
When working on a room assignment task, a participant was presented with a graphical interface where each student was represented as a person icon and each room was shown as a box (see Appendix A.1 for examples of the interface). The participant could then drag the icons of students and drop them to different boxes to adjust the room assignment plans. Assistive information such as the score of the current room assignment plan, the list of violated constraints, and the amount of time left in the task was also displayed and updated on the interface while the participant changed the solution. At any time during the allotted 5-minute period for a task, the participant could push a button to submit her solution and move on to the next task (or to the end of the room assignment task sequence), or the participant would be automatically redirected to the next task when the 5-minute timer was up. After the participant solved all five room assignment tasks in phase one, she was asked to complete an exit survey, in which we asked her to self-report the following information:
• Age
• Gender
• Highest Education Received -High School -US Bachelor's Degree -Master's or higher -Other
• Were the instructions clear?
• Was the pay fair?
• Was the time limit per task reasonable?
• Did you encounter any problems with the user interface?
• If you had assigned all students to rooms and had no conflicts, which of the following would you be most likely to do?
-Submit your solution and move on the next task -Put off resolving the conflict(s) until all students had been assigned?
-Stop assigning students to rooms until conflict(s) had been resolved?
-Continue assigning students as long as no more than one conflict were present?
• When assigning a student to a room, did you focus more on -Which room had the highest score?
-Which room(s) would avoid generating conflicts?
• Any other feedback?
At the end of phase one, we obtained a number of measurements for each participant:
• Skill : defined as the sum of the participant's score on the two hard room assignment tasks. We only use participant's scores on the hard tasks as hard tasks are more discriminative and scores on hard tasks have higher variability, but we note that a participant's score on the two hard tasks highly correlate with the participant's score on each of the five room assignment tasks (see Appendix A.4
for validity check).
• Social perceptiveness level: defined as the number of RME questions the participant correctly answered.
• Cognitive style: operationalized in four different ways:
1. speed (fast vs. slow), which is decided by whether the total amount of time the participant spent on solving the hard instances of phase one room assignment tasks is below or above the median;
2. problem-solving style (pragmatic vs. tenacious), which is decided by the participant's self-reported answer for the exit-survey question "If you had assigned all students to rooms and had no conflicts, which of the following would you be most likely to do?": pragmatic (i.e., the participant chose "submit your solution and move on the next task" or "try to increase your score by moving students around as long as you did not generate any new conflicts") or tenacious (i.e., the participant chose "try to increase your score by moving students around even if it meant generating new conflicts");
3. constraint violation tolerance (low vs. high), which is decided by the participant's self-reported answer for the exit-survey question "If you had assigned some (but not all) students to rooms and had encountered one or more conflicts, what would you do?": low (i.e., the participant chose "stop assigning students to rooms until conflict(s) had been resolved") or high (i.e., the participant chose "put off resolving the conflict(s) until all students had been assigned" or "continue assigning students as long as no more than one conflict were present"); and 4. problem-solving focus (optimizer vs. satisficer), which is decided by the participant's self-reported answer for the exit-survey question "When assigning a student to a room, what did you focus more on?": optimizer (i.e., the participant chose "which room had the highest score") or satisficer (i.e., the participant chose "which room(s) would avoid generating conflicts").
Although our measurements of each participant's skill and social perceptiveness level are continuous, to facilitate the block randomization scheme that we would adopt in phase two of our experiment, we further used a median split to categorize each participant into the high or low class on both measurements. For example, a participant whose skill was above the median skill while social perceptiveness was below the median level would be categorized as "high skill, low social perceptiveness."
We note that in our analysis we use the original (continuous) scores for individuals that we obtained from phase one (where a team's score is the average of the team members' scores), not the block labels, to differentiate high-skilled/low-skilled (or high social perceptiveness/low social perceptiveness) teams. See Section 2.2.4 for more details.


Design of Phase Two Experiment
As per our pre-registration, we included the first 1200 participants who completed our phase one experiment into the second phase of our experiment. Among these 1200 participants, there were 313 "high skill, high social perceptiveness" (HH) individuals, 284 "high skill, low social perceptiveness" (HL) individuals, 249 "low skill, high social perceptiveness" (LH), and 354 "low skill, low social perceptiveness" (LL) individuals.
During a pilot study we conducted prior to our main experiment, we deployed implies that a large sample size would be needed to detect any statistically significant performance effect of team composition.
To address this problem, we adopted a block randomization scheme in phase two of our main experiment. Specifically, prior to the start of phase two, we created six qualifications on Amazon Mechanical Turk, with each qualification corresponded to a "block." Participants of one particular block could only find and work on the HIT corresponding to their block, but not the other five HITs. 
Table 2
.2 provides a summary of these six blocks.
For each individual of a particular type (e.g. "high skill, low social perceptiveness" or HL), with 50% probability we assigned her to the block in which all individuals were of the same type (e.g., the "HL" block), and with 50% probability we assigned her to the block in which all individuals had the same social perceptiveness label as her, but may have different skill labels (e.g., the "ML" block, meaning "mixed skill levels, low social perceptiveness"). Within each block, we further randomly assigned participants either to the individual condition (31% of the time) or to the team condition (69% of the time). The individual condition was identical to phase one except that the five room assignment tasks were different (and generally more difficult) and that the maximum time allotted per task was ten rather than five minutes. 
Table 2.3
 summarizes the main properties of the 5 task instances we used in our phase two experiment (the task sequence used in the individual condition is the same as that used in the team condition). In the team condition, participants worked in teams of three members from the same block.
The main effect of the block randomization scheme was to oversample statistically less frequent combinations (e.g., all team members had high skills or high social perceptiveness), which helped us to increase the statistical power of our experiments (a secondary benefit was that it allowed us to match the distributions of participant types in phases one and two; see Appendix A.5). To illustrate, the frequency of HH individuals in the population is when one team member was moving a student icon, that particular student icon was "locked" and other team members could not move it until it was released. We provided a chatbox on the task interface, enabling team members to communicate freely with each other during the tasks. We also presented an event log on the task interface to help team members make sense of all movements that had been made within the current task. At any time during a task, each team member could indicate whether she was satisfied with the current solution using a toggle button. Once all three members of a team indicated they were satisfied with the solution, the team would move on to the next task (or to the end of the experiment). If the team had never unanimously suggested they were satisfied with the solution, the team would automatically be redirected to the next task when the 10-minute timer was up.
At the end of phase two of the experiment, while participants in the individual condition were asked to were asked to complete an exit survey that is identical to the one in phase one, participants in team condition were asked the following:
• How would you describe your strategy in the game?
• Do you feel the pay was fair?
• How satisfied are you with the outcome of the game?
-Extremely satisfied (1) -Extremely dissatisfied 
7
• Do you think your team worked well together?
-Strongly agree (1) -Strongly disagree (7)
• How valuable do you think your perspective was to the end results?
-Extremely valuable (1) -Extremely invaluable (7)
• How comfortable were you in sharing your perspective with the team through the chat?
-Extremely comfortable (1) -Extremely uncomfortable (7)
• Feedback, including problems you encountered.


Details of Analysis
In this work, we are interested in comparing the effect of being in a team (i.e., teams vs individuals) as well as examining the several factors (e.g., skill level, skill diversity, social perceptiveness level, cognitive style diversity, etc.) that determine the team performance. In the case of the first question (i.e., team vs individual) or independent variable is a binary indicator that specifies whether the observation in phase two is generated by an individual or a team. For the second question (i.e., different team compositions), we defined a number of measures as our independent variables to capture various possible influencing factors of team performance:
• • Normalized score: the score a team obtained in a room assignment task divided by the maximum score of that task, i.e., normalized score = score on Task max score for task
• Duration: the amount of time a team spent on solving a room assignment task
• Efficiency (not pre-registered): Acts as a useful summary of the two other metrics, i.e., efficiency = normalized score on task duration on task Task Difficulty B A C 
Figure 2
-2: The five task difficulty levels in phase two were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Increasing the task difficulty (i.e., environment complexity) reduces the normalized score and increases the time it takes participants to submit an assignment. Data is combined across both individual and team conditions across all 6 blocks. Error bars indicate 95% confidence intervals. The effective normalized score of a feasible solution is 80% and the minimum time required for a solution to be submitted is one minute, hence the starting points of the Y axes.


Results
Performance as a Function of the Environment 
Complexity   Fig 2-2
 shows how overall performance varied as a function of task complexity, where we use two independent definitions of performance. First, we define performance as normalized score (i.e., score on Task max score for task ) thereby allowing us to compare performance across tasks of different complexity which may have widely varying maximum possible scores. Second, we also define performance as duration (i.e., time elapsed from the start of a task until a solution is submitted) 
5
 . In addition, 
Fig 2-
2 also shows our third metric, efficiency, which acts as a useful summary of the two other metrics (i.e., efficiency = normalized score on task duration on task ).


Fig 2-2 shows that higher complexity led both individuals and teams (see also
Appendix A.6) to score a lower fraction of the maximum possible score (2A) and work for longer (2B). Efficiency, therefore also decreased with task complexity (2C).
Although the direction of these results is unsurprising, the large and roughly linear dependency of two separate performance measures on complexity validates our design, in which overall complexity is manipulated by varying one or more task/environment parameters ( , , ).
Moreover, the ability to vary human-experienced complexity by such substantial margins (on average, individuals and teams spent roughly three times as much work time on "super hard" as "easy" tasks, but obtained normalized scores that were roughly ten percentage points lower) allows us to test for interaction effects between optimal team composition and task complexity where theories of collective performance have been largely silent, i.e., to what extent does the optimal composition depend on the characteristics of the task being performed? Alternatively, one can view varying complexity as a robustness check on findings obtained for any single task 
[23]
. In other words, systematically varying task/environment complexity is informative with respect to our main research questions regardless of whether optimal team composition depends on it.
Groups are Superior; Only when the Environment is Complex 
Fig 2-3
 compares overall team performance with that of "comparable individuals," which we define in two ways: first, a randomly drawn individual from the same block;
and second, by constructing a "nominal team," drawing three individuals randomly and without replacement from the same block, and then choosing the individual with the highest score from phase one. Nominal teams, therefore, simulate a situation in which teams simply nominate their best performer to do all the work while the others contribute nothing. For all levels of task complexity, 
Fig 2-3A
 shows that teams score higher than randomly selected individuals but lower than nominal teams, consistent with longstanding findings that nominal teams outperform real teams under various circumstances 
[197]
.
Interestingly, however, 
Fig 2-3B
 shows that teams complete the most complex tasks-but not simpler ones-faster than either random individuals or nominal teams, suggesting that for tasks with many components (students and rooms) and many constraints the benefits of distributing work to a team outweigh the process losses (e.g., motivation loss, coordination cost) associated with groups 
[109]
. Finally, 
Fig 2-
3C shows that for complex tasks the gains in speed exceed the deficits in score, resulting in a striking interaction between task complexity and configuration with respect to efficiency: for easy tasks teams are considerably less efficient than either random individuals or nominal teams, yet they are considerably more efficient than either for the most complex tasks. This result is reminiscent of group decision making among social insects where a study have found that colonies outperform individuals when the discrimination task is difficult but not when it is easy 
[177]
. 
Fig 2-4
 shows the absolute and relative effects of all pre-registered independent variables on collective performance, which is quantified as score 
(Fig 2-4A)
, duration of completion 
(Fig 2-4B)
, and efficiency 
(Fig 2-4C
) respectively (and all three metrics are standardized within each task complexity level as per our pre-registration). Across      
Fig 2-4A
 shows that average skill had the largest effect on teams' scores, and was both positive and highly significant (Table 2.4). In addition, the effect of skill is consistently and significantly larger than that of social perceptiveness (Wald chi-square test; 2 = 6.35, = 0.012; see Appendix A.7 for additional "relative importance" analysis), which was also positive and significant (Table 2.5). In contrast, skill diversity (i.e., variance in team members' ability) has consistently and significantly negative effects on the score (see 
Table 2
.6) while no measure of cognitive style diversity has any consistent and significant effect (see Table2.7 and Appendix A.7).


Skill Accounts for 4 Times as Much as Everything Else
Compared with team score, the effects of skill, social perceptiveness, and diversity on duration 
(Fig 2-4B)
 and efficiency 
(Fig 2-4C)
 are small and not significant at the < 0.05 level.
Effect sizes are important for testing theories, but in practice, it is also important to consider predictive accuracy 
[96,
210,
220]
. To illustrate, recall our hypothetical manager who wishes to compose a team for some task, and who has prior information about the skill, cognitive style, and social perceptiveness of prospective team members. In essence, the manager's task is to predict which combination of traits will yield the best collective performance. More specifically, the manager cares about two 
Table 2
.5: Relation between team's average social perceptiveness and team performance. Data is combined across teams in all six blocks, and for all five tasks. Models relate performance measures (standardized within each task) with the team's average skill level. All models included random effects for teams as intercept to account for dependence across tasks (i.e., random effects are clustered on each team, using team id as the identifier). Increasing a team's average skill significantly increases the team's score in solving CSOPs, but has no effect on duration or efficiency.  
Table 2
.6: Relation between team's skill diversity and team performance. Data is combined across teams in all six blocks, and for all five tasks. Models relate performance measures (standardized within each task) with the team's average skill level. All models included random effects for teams as intercept to account for dependence across tasks (i.e., random effects are clustered on each team, using team id as the identifier). Increasing a team's average skill significantly increases the team's score in solving CSOPs, but has no effect on duration or efficiency.  
Table 2
.7: Relation between team's cognitive style diversity and team performance. Data is combined across teams in all six blocks, and for all five tasks. Models relate performance measures (standardized within each task) with the team's average skill level. All models included random effects for teams as intercept to account for dependence across tasks (i.e., random effects are clustered on each team, using team id as the identifier). Increasing a team's average skill significantly increases the team's score in solving CSOPs, but has no effect on duration or efficiency. to what extent it can be "engineered" at all. The answer to the second question indicates which of the observed variables to prioritize, and how much, when selecting team members. The latter is particularly important when there is a cost associated with the measurement of the relevant variables.


Score Duration


Score Duration Efficiency


Score Duration Efficiency
Addressing the first question, 
Fig. 2-5A
 shows the out-of-sample 2 for a simple linear regression model where the dependent variable is the total normalized score (i.e. summed over all tasks), and all observed independent variables are included first independently (i.e., separate, univariate regressions; green symbols) and then cumulatively (purple symbols) in order of increasing independent explanatory power (i.e., the 2 of the corresponding univariate regression). Overall, the 2 was approximately 0.24, meaning that the model "explained" about 24% of the observed variance in held- duration is a much harder task: almost no variance can be explained either by skill or by any combination of measured attributes (see Appendix A.8).


Discussion and Chapter Reflections
These results provide mixed support for previous studies and also help to clarify some inconsistencies between them. First, our results help to reconcile conflicting prior findings regarding the effectiveness of teams vs. individuals: whereas we find that teams clearly outperform comparable individuals selected at random, consistent with 
[219]
, we also find that teams score worse than the best individual selected from a nominal team of the same size, consistent with 
[109,
197]
. Interestingly, even as teams underperform nominal groups in terms of score, for the most complex tasks-but not for simpler tasks-they attain higher efficiency by completing their work faster.
Second, our finding that the effects of average individual skill and social perceptiveness are positive and highly significant is consistent both with the aforementioned meta-analytical studies that favored ability 
[60,
187,
27]
, and also with the more recent experiments that emphasized social perceptiveness work 
[217,
120,
69,
110]
. However, our ability to compare effect sizes and predictive performance across multiple effects resolves the apparent inconsistency between the two sets of results: skill dominates social perceptiveness by an order of magnitude.
Third, our findings of that skill diversity is negatively associated with team performance is consistent with 
[18]
 but directly contradicts 
[98]
. Even if the latter claim is interpreted as implicating cognitive diversity more generally rather than skill per se, we find no evidence that any of several measures of skill or cognitive style diversity is positively associated with performance. Naturally, teams can be diverse with respect to attributes other than skill and cognitive style (e.g., demographics, political ideology, worldview etc.) and diversity can affect outcomes other than performance on task (e.g., satisfaction, legitimacy, social equity, etc.); thus our results should not be construed as finding any effect of diversity writ large. Nevertheless, they do reinforce recent research 
[53,
66]
 which also concludes that unambiguously positive effects of diversity are more difficult to detect in carefully controlled empirical studies than what would be expected from theory 
[98]
.
Indeed, team composition and team performance are multifarious constructs each of which can be operationalized in many ways; moreover, the relationship between the two may be contingent on numerous other mediating variables related to the nature of the task 
[186,
138]
 and the environment 
[123,
35]
. Finally, the literature on team performance comprises a mixture of simulation, observational, and experimental studies; thus it is hardly surprising that it exhibits inconsistencies. In this paper we have introduced an approach to studying team performance that leverages a unique combination of (a) class of tasks with variable environmental complexity (i.e., the complexity parameters, , , and ) to increase the robustness of our results and allows us to test for interaction effects; (b) two-phase design which allows us to measure individual on-task skill and cognitive style as well as social perceptiveness prior to team assignment; (c) large sample size and block randomization to increase power;
and (d) a pre-registered analysis plan to constrain researcher degrees of freedom 
[180]
.
In conclusion, our results show that on-task skill of team members far outweighs other factors, such as skill diversity, cognitive style diversity, and social perceptiveness, that have been emphasized in recent years, accounting for roughly three-quarters of explained variance. Although this result is robust to task complexity, which we varied widely, a major limitation is that we only studied one type of task. We, therefore, hope that future work will apply a similar approach to qualitatively different tasks as well as varying other parameters of interest (e.g., team size, communication patterns, division of labor, leadership, etc.). Naturally a research program that explores many parameters while still running large-N samples is logistically challenging;
however, we propose that "virtual lab" experiments of the sort that we have described here, in combination with emerging "open science" practices such as pre-registration, open data and code, replication, and "many-labs" style collaborations 
[112]
, offer a promising route forward. Finally, our emphasis on predictive accuracy seeks to move studies of team performance away from tests of theoretical conjectures (e.g., "does X correlate with performance?") and toward tests of practical significance (e.g., "how much observed variance can be explained an in terms of what?"). 


Adaptive Systems and Environmental Conditions
Adaptive systems, both natural and artificial, rely on feedback, empirical learning, and reorganization 
[213,
201]
. Such systems are widespread, and can often be viewed as networks of interacting entities that dynamically evolve over time. Cell reproduction, for example, relies on protein networks to combine sensory inputs into gene expression choices adapted to environmental conditions 
[71]
. Neurons in the brain dynamically rewire in response to environmental cues to enable human learning 
[83]
.
Eusocial insects modify their interaction structures in the face of environmental hazards as a strategy for collective resilience 
[191]
. Human social network plasticity and feedback have been shown to promote human cooperation 
[161,
77]
, and culture transmission networks over generations enabled human groups to develop technologies above any individual's capabilities 
[89,
147]
. In the artificial realm, prominent machine learning algorithms rely on similar logic, where dynamically updated networks guided by feedback integrate input signals into useful output 
[30,
146]
. Across the board, the combination of environmental feedback (e.g., survival, payoff, reputation etc) and network dynamics represent a widespread strategy for collective adaptability in the face of environmental changes; providing groups with an effective and easy-toimplement mechanism of response to external and internal disturbance 
[97,
119,
191]
.
In our view, the information processing capabilities of interacting human groups are no exception. People's behavior, opinion formation, and decision-making are deeply rooted in cumulative bodies of social information 
[20]
, accessed through social networks formed by choices of whom we friend 
[11,
206]
, follow 
[195]
, call 
[65,
155]
, imitate 
[222,
179]
, trust 
[46,
205]
, and cooperate with 
[207,
161,
75]
. Moreover, peer choices are frequently revised, most often based on notions of environmental cues and feedback such as: success and reliability, or proxies such as reputation, popularity/prestige, and socio-demographics 
[142,
107,
214,
90,
77]
.


Conventional Wisdom on the Wisdom of Crowds
It is widely noted, however, that social influence strongly correlates individuals' judgment in estimation tasks 
[144,
127,
26,
82]
, compromising the first of two assumptions underlying common statistical accounts of 'wisdom-of-crowds' phenomena 
[194]
:
namely, that (i) individual estimate are uncorrelated, or negatively correlated, and (ii) individuals are correct in mean expectation 
[78,
82]
.
In recent years, numerous studies have offered conflicting findings, showing that social interaction can either significantly benefit group and individual estimates 
[142,
18,
26,
148]
, or, conversely, lead them astray by inducing social bias, herding, and group-think 
[144,
82,
127]
. There are some notable efforts that focused on providing a partial resolution to the conflict between the 'wisdom' and 'madness' of interactive crowds and found that these divergent effects are moderated by whether well-informed individuals are placed in prominent positions in the network structure 
[82,
26]
, how self-confident they are 
[18,
114,
129,
106]
, ability to identifying experts 
[142,
36]
, dispersion of skills 
[142,
16,
130,
29]
 and quality of information 
[103]
, diversity of judgments among group members 
[49,
29]
, and social learning strategies being deployed 
[24,
199]
 as well as the complexity/difficulty of the task being performed 
[199,
130]
. In other words, what is advantageous for the group depends on the environment in which the group is situated in. Because people often do not know their environment (or the environment is non-stationary) it is advantageous to find easy-to-implement mechanism that perform well across shifting environments.


The Role of the Environment, Again
Notably, both theoretical and experimental work on collective intelligence (including the reconciliation effort mentioned above) has been predominantly limited to frameworks where the communication network structure is exogenous, where agents are randomly placed in static social structures -dyads 
[18,
114]
, fully-connected groups 
[142,
127,
217,
148]
, or networks 
[82,
26]
.
Unlike what is explicitly or implicitly assumed in most existing work, the social networks we live in are not random or imposed by external forces, but emerge shaped by endogenous social processes and gradual evolution within a particular environmental conditions. The present study builds on the observation that agent characteristics, such as skill and information access, are not randomly located in network structure.
Intuitively, groups can benefit from awarding centrality to and amplifying the influence of well-informed individuals. Therefore, the distribution of agents is often the outcome of social heuristics that form and break ties influenced by social and environmental cues 
[107,
214,
32,
90]
, and therefore, the emergent structure cannot be decoupled from the environment.
Here, we hypothesize that dynamic social influence networks guided by feedback may be central to human collective intelligence, acting as core mechanisms by which crowds, which may not initially be wise, evolve into wisdom, adapting to biased and potentially non-stationary information environments.


Experiment: Guess the Correlation Game


Experimental Design
To test these hypotheses, we developed two web-based experiments (i.e., 1 and 2)
that allow us to identify the role of dynamic networks and feedback in fostering an adaptive 'wisdom of crowds.' In both studies, Participants ( 1 = 719; 2 = 480)
from Amazon Mechanical Turk engaged in a sequence of 20 estimation tasks. Each task consisted of estimating the correlation of a scatter plot, and monetary prizes were awarded relative to performance. Participants were randomly allocated to groups of 12, and each group was randomized to one of three treatment conditions in 1 or four treatment conditions in 2. In study 1, the feedback level is fixed (i.e., full feedback) and network plasticity is manipulated (i.e., static network versus dynamic network). In study 2, plasticity is fixed (i.e., always dynamic network) and feedback is manipulated (i.e., no feedback, self feedback, and full feedback). 
Fig. 3-1
 
Figure 3
-1: An illustration of the overall experimental design.In study 1, the feedback level is fixed (i.e., full feedback) and network plasticity is manipulated (i.e., static network versus dynamic network). In study 2, plasticity is fixed (i.e., always dynamic network) and feedback is manipulated (i.e., no feedback, self feedback, and full feedback).


Study 1: Manipulates network plasticity; full feedback
In 1, each group was randomized to one of three treatment conditions:
• solo condition: each participant solved the sequence of tasks in isolation (i.e., no social information). This condition corresponds to the traditional 'wisdom of the crowds' context 
[78,
194,
160]
. See 
Figure 3
-2A.
• static condition: participants were randomly placed in static communication networks. That means, participants will engage in a stage of active social learning, where they are exposed to their ego-network's estimates in real time. See 
Figure 3
-2B. This context is analogous to that studied by work at the intersection of the 'wisdom of crowds' and social learning, such as 
[127,
129,
82,
55,
136]
.
• dynamic condition: participants at each round were allowed to select up to three peers to to follow (i.e., get the ability to communicate with) in subsequent rounds. See 
Figure 3
-2C. This condition is novel to the work of this dissertation. A B C 
Figure 3
-2: Illustration of the experimental conditions in study 1. Panel (A) depicts the Solo condition (i.e., no social information) where participants make independent estimates. This condition corresponds to the baseline wisdom of the crowd context. Panel (B) describes the Static network condition (i.e., social learning) where participants engage in a stage of interactive social learning, where they are exposed to the estimates of a fixed set of peers in real time. Panel (C) describes the Dynamic network (i.e., selective social learning) condition that adds the possibility for participants to choose who to follow and be influenced by in the next round.
Note that in the social learning stage (i.e., in the static and dynamic conditions; see 
Figure 3
-2) participants observe in real-time the estimates of the other participants that they are connected to and can update their estimates multiple times before they submit their final estimate. It is up to the participant to decide how to update their guess to accommodate the information and experiences, the opinions and judgments, the stubbornness and confidence, of the other players.
After submitting a final guess, participants in all conditions were given performance feedback. That included how much they earned, what was the correct correlation, what was their guess.


Study 2: Manipulates feedback; dynamic network
In 2, each group was randomized to one of four treatment conditions:
• solo condition, where each individual solved the sequence of tasks in isolation.
• no feedback condition, in which participants were not shown performance feedback.
• self feedback condition, in which participants were shown their own performance feedback.
• full feedback condition, in which participants were shown scores of all participants (including their own)
Participants in all conditions (except solo, our baseline) were allowed to revise which peers to follow in subsequent rounds (i.e., similar to the 'dynamic network' condition in study 1). To further assist with reproducibility of our study, we pre-registered our 2 main research questions and analysis plan (AsPredicted.org #16474), and made all data and code available at OSF.io.


Estimation Task: Guess the correlation game
Participants were prompted to estimate the correlation from a scatter plot and were awarded a monetary prize based on the accuracy of their final estimate. We call this task, 'Guess the Correlation Game' 
[151]
.
This estimation task is designed to expose the mechanisms that allow intelligent systems to adapt to changes in their information environment. We can influence the performance level of participants by implementing three difficulty levels (i.e., varying the number of points, and adding outliers or non-linearities): easy, medium, and hard. see 
Figure 3
-3.
At every round, all plots seen by participants shared an identical true correlation, but difficulty levels could differ among them 
[143]
. The allowed the simulation of a shock to the distribution of information among participants. Specifically, each  from such online labor markets and deemed online experiments to be as reliable as that obtained via traditional methods 
[17,
45,
99,
28,
9]
. Accordingly, we posted each of our experimental sessions as an external HIT (a URL of our web application, is displayed in a frame in the Worker's web browser).
All participants were recruited on MTurk by posting a HIT for the experiment, entitled "Guess the correlation and win up to $10", a neutral title that was accurate without disclosing the purpose of the experiment. The study (Approval#: 1509172301) 


Experimental Results


Individual and Collective Outcomes
We first compared individual-and group-level errors across conditions. Evolutionary reasoning suggests that people's propensity to imitate follows from its direct benefits to the individual, but it may, nonetheless, induce benefits to the population as a whole 
[32]
. Our first result is that networked collectives across studies significantly outperformed equally sized groups of independent participants, which is consistent with prior work on search 
[136,
57]
 as well as estimation tasks 
[130]
. 
Fig. 3-5
 shows the individual and group error rates-using the arithmetic mean as group estimate-normalized with respect to baseline errors in the solo condition. Overall, we find that participants in dynamic networks with feedback achieved the lowest error rates. The performance edge was larger in periods where networks had adapted to their information environment (rounds 
[6,
10]
 ∪ 
[16,
20]
, the 'adapted periods').
In particular, in 1 dynamic networks averaged 33% lower individual error ( < 10 −5 ), and 34% lower group error, compared to participants in static networks ( < 10 −4 ). In the adapted periods, dynamic networks reduced error by 47% ( < 10 −10 ) compared to groups that lacked plasticity (i.e., connected by static networks).
In 2, participants with full feedback averaged 47% lower individual error ( < 10 −10 ), and 54% lower group error, compared to participants in the no-feedback condition ( < 10 −4 ). Additionally, participants with full feedback averaged 42% lower individual error ( < 10 −4 ), and 42% lower group error, compared to participants in the self-feedback condition ( < 10 −3 ). Overall, the differences between the selffeedback and no-feedback conditions are not significant. However, in the adapted  The reduction is notably larger and more significant in periods where networks had adapted to the information environment (i.e., rounds 
[6,
10]
 and 
[16,
20]
). Errors are normalized with respect to average errors in the solo condition within each study. Error bars indicate 95% confidence intervals.
periods, participants in the self-feedback condition achieved 60% lower group error than the no-feedback condition ( < 10 −3 ).
Hence, these results from both studies support our primary hypothesis that adaptiveness through feedback and network plasticity can benefit both individual and collective judgment.  
Figure 3
-6: Mechanisms promoting collective intelligence in dynamic networks. Panel (A) shows that the network becomes more centralized with time (Freeman global centralization-i.e., how far the network is from a star network). Panel (B) depicts the relation between performance (i.e., average error) and popularity (i.e., number of followers). Panel (c) shows the relationship between accuracy of initial estimate and confidence (i.e., resistance to social influence). Error bars indicate 95% confidence intervals.
anthropology, which indicate that people naturally engage in selective social learning 
[32,
214,
89]
-i.e., the use of social cues related to peer competence and reliability to choose whom we pay attention to and learn from selectively. Figs. 3-6A and 3-6B
show that participants in dynamic networks consistently used peers' past performance information as success cues to guide their peer choices. As rounds elapsed, performance information accrued, and social networks evolved from fully distributed into networks that amplified the influence of well-informed individuals. Upon receiving an information shock, the networks slightly decentralized, entering a transient exploration stage before finding a configuration adapted to the new distribution of information among participants (see 
Fig. 3-7
).  A centralization mechanism alone could suggest that group members may merely follow and imitate the best individual among them, hence bounding collective performance by that of the group's top performer. However, research on the two-headsbetter-than-one effect indicates that, in the simpler case of dyads, even the best individual can benefit from social interaction 
[18,
114]
; and that the critical mechanism enabling this effect is a positive relationship between individuals' accuracy and their confidence. 
Fig. 3-7C
 shows that participants in dynamic networks had, overall, a positive correlation between the accuracy of their initial estimates and their selfconfidence (measured in terms of resistance to social influence). Participants were likely to rely on private judgments whenever these were accurate and likely to rely on social information otherwise. 
Fig. 3
-7C also shows that, as rounds elapsed, participants used task feedback to calibrate their accuracy-confidence relation gradually, and were able to re-adapt gradually upon the shock. Consistent with prior literature 
[26,
129]
, a positive correlation of confidence and accuracy was found in all networked conditions (i.e., including static networks in study 1), explaining their favorable performance compared to unconnected groups in both studies.


Mean-Variance Trade-off
The  The shape of top-k curves reveals that, as we remove low-performing individuals (from = 12 to = 1), estimates initially improve in both mean and standard deviation. Then, as we further curate the crowd beyond = 6, top-k estimates trade off between decreasing mean error and increasing variability, and finally regress in both objectives as → 1. Comparison across conditions shows that, for any ∈ 
[1,
12]
, dynamic influence networks improved estimation errors in terms of both mean and standard deviation. In particular, 
Fig. 3-8
 shows that the full-group average in dynamic networks got 28% lower error and 48% less variability than the best individual in the solo groups (dynamic top-12 vs. solo top-1 ; < 10 −2 ). Moreover, even the best individual derived substantial benefits from social interaction, averaging 32% lower error and 38% less variability when forming and revising social connections rather than working in isolation (dynamic top-1 vs. solo top-1 ; < 10 −2 ). Top-12 estimates correspond to the full-group mean, and top-1 to the group's best individual. Within each condition, top-k trade-off curves first gain in both objectives, then trade off lower average error for higher variability, and finally regress in both objectives as → 1. Across conditions, for any ∈ 
[1,
12]
, groups in the dynamic condition outperformed groups in the static and solo conditions. Moreover, the fullgroup mean of dynamic networks averaged 28% lower error and 48% less variability than the best individual playing solo (dynamic top-12 vs. solo top-1 ; < 10 −2 ); and the best individual in the dynamic condition averaged 32% lower error and 38% less variability than her analogue in solo (dynamic top-1 vs. solo top-1 ; < 10 −2 ). Bars indicate 95% confidence intervals.


Numerical Model and Simulations
We implemented numerical simulations to further assess the extent to which the interaction between the quality of performance feedback and the adaptability of dynamic networks. Therefore, we focus on two conditions: 1) traditional wisdom of crowds (i.e., independent actors with individual feedback); 2) adaptive wisdom of crowds (i.e., dynamic networks and full feedback). In order to follow the properties of our framework (see 
Figure 3-1)
, we need to operationalize models of its human components, i.e., the social learning and network rewiring heuristics. We model the former as a DeGroot process 
[54]
, and propose a performance-based preferential detachment and attachment model for the latter. where each agent updates her belief by taking weighted averages of her own belief (i.e., private signal) and the beliefs of neighboring agents. DeGroot averaging as social learning heuristic has been well studied empirically and theoretically 
[55,
82]
, and shown to robustly describe real-world belief updating better than more optimal rational Bayesian models 
[43]
. In particular, we model post-social learning beliefs as the result of a two-stage DeGroot process on private signals, given by


Model Specifications
( ) = (︀ ( ) )︀ 2 ( ) (3.1)
Individual performance is evaluated based on the errors of post-social influence estimates. Individual cumulative error is defined by:
( ) = 1 + 1 ∑︁ ∈[0, ] ⃒ ⃒ ⃒ ( − ) − ( − ) ⃒ ⃒ ⃒,
where controls the number of retrospective periods that performance information is averaged across.
Agents assess performance of other agents relative to the performance of the best agent in the group. We define relative error of agent as initial signals. Probability of Attachment. High-performing agents are more likely to be followed. Analogous to generalized preferential attachment 
[21]
, probability that agent attaches to is inversely proportional to 's error, and given by
( ) = ⃒ ⃒ ( ) − 1 ∑︁ ( ) ⃒ ⃒ (3.2) ( ) = ⃒ ⃒ ( ) − 1 ∑︁ ( ) ⃒ ⃒
(
( ) = (︃ 1 − ( ) − ∑︀ ( ) )︃ 2 (3.5)
where is a normalization constant. 


Simulation Results
Using the above-described model, we performed Monte Carlo simulations of a group of twenty agents who participate in a sequence of estimation tasks, where agents can follow and be influenced by a maximum number of five peers ( = 5). We intentionally chose parameter values that differ from our experiments in order to examine the robustness of our findings under different parameter values. Indeed the results of these simulations corroborate our main experimental results that relates adaptability in non-stationary information environments to plasticity and feedback.  Lastly, we explore through simulation the interaction between network learning rates-a network's sensitivity to changes in agents' performance, parameterized by -and the arrival rate of environmental shocks 3-12. Networks with faster learning rates could adapt to environments with frequent information shocks. Conversely, networks with slower learning rates could leverage longer learning periods, eventually achieving lower error rates in environments with infrequent shocks. This short-term versus long-term accuracy trade-off implies that optimal network learning rates depend on the pace at which the information environment changes, analogous to notions of optimal learning rates in natural systems and artificial intelligence algorithms 
[30,
113]
. 


Chapter Summary and Reflections


Chapter 4
The Virtual Lab: High-throughput


Social Science
Behavioral labs have long played an important role in all social science disciplines.
The "Behavioral Lab" approach is a useful tool that offers a great degree of control and allows for the identification of causal effects. However, many social phenomena of interest to researchers and policy makers alike involve large populations interacting in complex non-stationary environments over extended periods of time. By contrast, behavioral experiments have historically been restricted to small samples of WEIRD 1 subjects interacting in highly simplified environments over very short (i.e., up to 1 hour) intervals. Consequently, the results of even well designed and run lab experiments suffer from severe external validity problems.
Another related problem, which is even more relevant to the framework of this dissertation, is that social theories are rarely precise enough to estimate exact parameter values from empirical data; thus, a robust test of even a single theoretical claim may require many experiments, each corresponding to a different set of parameters.
Unfortunately, the costs and logistics involved in running lab experiments typically restrict researchers to exploring a tiny fraction of the relevant parameter combinations, thereby leading to fragile and inconsistent findings that in turn lead to the problem of incoherency discussed in Chapter 1 (in particular, Section 1.2).
We propose to address both sets of problems by dramatically scaling up and speeding up the current state of the art in virtual lab technology. Specifically, we intend to ease three main bottlenecks to existing research capabilities:
• Size. Alleviate scaling and replication difficulties by recruiting and maintaining a large and diverse pool of subjects (see Section 4.2.1). Achieving these goals will require virtual lab software (e.g., Empirica.ly 
[156]
)
that is optimized for reducing the overhead associated with building and running experiments (see Section 4.1), and in return, it will allow research teams to coordinate research designs by recruiting a community of researchers to collaborate on a single research program (e.g., optimal team composition, increasing cooperation in realworld scenarios, influence maximization on networks) to explore the parameter space of social theories and maximize cumulative knowledge (see Section 4.2).


The Interactive Environment
The idea of web-based "virtual labs" to create experiments with rich interactive environments and crowdsourced labor from the internet has started to gain popularity.
Although the number of synchronous online experiments is on the rise 
[175,
132,
133]
, most of them make use of customized implementations, leaving a large number of open methodological challenges yet to be solved 
[19]
. So far, researchers trying to pursue this approach get side-tracked mostly by the tedious logistics of randomiza- while non-commercial (open-source or free) products include jsPsych 
[51]
 and Psy-Toolkit 
[189,
188]
. However, these are designed to support questionnaires and singleparticipant reaction-time experiments (i.e., no group interactions or multiplayer types of games). Experimental software like Breadboard 
[139]
, Z-tree 
[73]
, and oTree 
[44]
 do support group interactions, but they are originally designed for sequential interactions (not continuous) and for an insufficient number of highly constrained settings (e.g., stylized economic games or studies of social networks). Finally, nodeGame 
[19]
 3
and TurkServer 
[131]
 are flexible and support real-time group interactions; however, they require relatively substantial programming expertise.
Therefore, we decided to build our own platform, Empirica 


Towards Expanding the "Lab Experiment" Design Space
As we have discussed in the beginning of this chapter, the "virtual lab" approach is intended to lift the historical barriers along three major conceptual dimensions: (i) size, (ii) time, and (iii) complexity (see 
Figure 4
-1).


Size: In Complex Systems, Large is Different
Morris Zelditch in 1969 argued in his paper "Can you really study an army in the laboratory?" that it is neither possible nor necessary to study large human organizations in the lab 
[221]
. Zelditch asserted that it is sufficient to test social theories in smallgroup experiments and then use theory to generalize the results to the larger group.
Today, we know that this perspective is inadequate. In complex systems, the collective behavior is not merely the sum of the individual components. We cannot understand how an ant colony operates by studying individual ants. In the same way we cannot understand social systems-such as markets, organizations, institutions-by studying individuals or small groups in the lab setting. Groups behave differently at different scales 
[212,
196]
. For example, Elinor Ostrom 4 found empirically a non-linear relationship between community size and its ability to protect the commons 
[159]
. Also,  experimental work has shown some interesting relationships between group size and collective performance such as generating complex cultural artifacts 
[56]
 5 , mapping disasters 
[133]
, and disrupting science and technology 
[218]
. This explains why the municipal is different from the national and why the United States is not just ×1, 000
Singapores.
Therefore, when it comes to social theories, the size of a group is an important parameter that can change the qualitative behavior of interacting individuals. It seems that we do need to study the army in the lab, after all.
Unlike the physical lab, which is constrained by the behavioral lab space at a university, the virtual lab-in theory-has no limit to the number of participants.
In particular, shifting the meaning of a "large group" from a couple of dozens to hundreds of participants has been enabled by the availability of a large and cheap labor market for research. The ability to crowdsource participants has had a large impact on human subjects research, from the computational sciences to the behavioral sciences. For instance, crowdsourcing labor from Amazon Mechanical Turk (MTurk)
is an increasingly popular tool for conducting behavioral studies. There have been efforts to systematically replicate classic results from the social sciences 
[17,
99,
28]
, with outcomes that in many cases appear to be as reliable as data obtained via traditional methods.
The use of crowdworkers has had a profound influence on the nature and pace of data collection and has opened new avenues to cost-effective replication and extension of familiar research paradigms. This shift has had special impact in areas such as studies of cooperation and conflict, person perception, intergroup attitudes and stereotypes, and group behavior, where cumbersome interactive multi-participant experiments can be conducted much more easily via online platforms 
[88,
9]
.
The advances in scale offered by online labor markets for crowdsourcing participants, though significant, are not without limitations. In particular, there are at least
three challenges yet to be solved 
[134]
: 1) recruiting simultaneous participants (i.e., availability at the same time to study interaction between participants); 2) participants' uniqueness (i.e., avoiding learning affects 
[163,
42]
, where prior exposure to a task affects subsequent experimental results on similar tasks); and 3) large sample size (i.e., at any given time there are only around 2K active high-effort workers on
MTurk 
[62]
).


Timescale: Social Interactions Evolve Over "Time"
Social interactions between individuals evolve over time (as we have discussed in
Chapter 3), and the nature of time-be it simultaneous or sequential; real-time or offline; continuous or discrete; fast or slow; one-shot or repeated-can fundamentally change group-level outcomes 
[182,
170,
31]
.
A notable example is the dynamics of cooperation (i.e., paying a personal cost for a shared benefit). In Prisoner's Dilemma, where defection is the prevailing action in one-shot interactions, cooperation can be sustained when interactions are repeated and participants can remember previous actions of their peers 
[153]
. Moreover, cooperation can be further sustained in long-run experiments (i.e., lasting for 20 consecutive weekdays 
[132]
) and real-time interactions (i.e., going from discrete-time to continuous-time increases cooperation from 40% to 90% 
[76]
).
Virtual lab experiments grant great flexibility to experimenters to study human interactions at different time intervals, from one-shot real-time (i.e., seconds) to very long sequential (weeks and months) interactions-thereby allowing for more "immersive" environments.
Despite the opportunities virtual labs can bring to studying human behavior, behavioral research online has so far remained largely limited to offline decisionmaking tasks 
6
 or one-shot interactions with simultaneous decisions. This is partly because of the lack of an established software that is widely used for conducting synchronous online experiments, which leaves a large number of open methodological challenges yet to be solved by the experiment designer. • The researcher asks the "framework" what experiment to run, specifying what parameters to hold fixed (e.g., task characteristics) and which are allowed to vary (e.g., team compositions).


Complexity: The Parameter Space of Social Theories
• The platform suggests experiment parameters to be evaluated (e.g., scale=50
individuals, means of communication=language, structure='teams', skill diver-sity= high, etc.).
• The researcher implements the experiment using the provided parameters.
• The results are fed back to the framework, which uses them to update its parameters and proceeds to the next iteration of experimentation.
• Code and results (that are compatible and comparable) are made publicly available and published in peer-reviewed journals.


Reflections and Conclusions
In conclusion, group attributes (e.g., network structure, team composition, individuallevel attributes) and collective performance are multifarious constructs each of which can be operationalized in many ways. Moreover, the relationship between the two may be contingent on numerous other mediating variables related to the nature of the environment. Therefore, without an environment-dependent framework from which to draw hypotheses and tune our intuitions, it is difficult to distinguish results that are unusual and interesting from results that are unusual and probably irrelevant.
Although the idea that the environment shapes human behavior has been generally accepted, there are two mainstream criticisms of this approach. First, there are concerns regarding the associated difficulty of manipulating and measuring environment (i.e., how can one possibly sample situations?). In this dissertation, we showed that focusing on the formal properties of the environment (i.e., defining the universe of possible environments) and the use of modern virtual lab technologies can effectively overcome this limitation. The second objection argues that, even if we could define and sample the environment, there is no need to do so. After all, the goal of the social scientist is not to generalize the results from the experiment to 'outside' situations but to test hypotheses and advance particular theories. However, recent movements in social science have argued that social science should be more "solution-oriented" to reconcile the competing claims in the literature. That is, the research community needs to place more emphasis on solving practical problems-the sort with direct engineering analogs 
[210]
-rather than the advancing of particular theories. In this dissertation, we have followed a "solution-oriented" approach by advancing our fundamental understanding of collective intelligence in the course of solving applied problems.
In future work, we hope to apply the same approach to qualitatively different availability, open code, and "many-labs" style collaborations, offer a promising route forward. In order to operationalize the "environment," we built an experimentation platform (Empirica.ly). The platform forces the investigator to explicitly define the space of the environment in which the group of participants is situated, and therefore, the exploration of the interactions between the environments and the attributes of interest becomes more systematic (as opposed to having isolated and non-comparable studies). We hope that our emerging ability to conduct virtual lab experiments-of a scale, duration, and realism that far exceed what is possible in brick-and-mortar facilities-will blur the traditional boundary between "lab" and "field" experiments and revolutionize our understanding of human behavior, not only for the design of social science experiments but for rebuilding society as a whole.    In our experiment, we defined an individual participant's skill score as the sum of her scores on the two hard tasks in phase one experiment, and we further labeled the participant as "high" or "low" on skill by examining whether her skill score was larger or smaller than the median score obtained among all participants. To illustrate the validity of this measurement of skill level, 
Figure A
-5 contrasts the normalized scores (i.e., actual score obtained in a task instance / the maximum possible score for that task instance) obtained by "high skill" participants with those obtained by "low skill" participants, on each of the six tasks in phase one, including one practice task (hard) and five actual tasks (3 easy and 2 hard). Clearly, on all task instances, participants that are determined as "high skill" outperformed those participants that are determined as "low skill." In other words, participants' scores on the two hard task instances are highly correlated with their scores on any single task instance, regardless of whether it is easy or hard, which suggests that it is valid to use participants' scores on the two hard tasks to measure skill levels.


A.2 Reading the mind in the eye
Figure A-5: Participants who obtained a higher score on the two hard tasks in the phase one experiment (i.e., "high skill") outperformed participants who obtained a lower score on those two hard tasks (i.e., "low skill") on each single task instance. Error bars represent 95% confidence intervals.


A.5 Comparing participants in phase one and two
One natural concern regarding the two-phase experimental design is whether different participants' experience in the phase one experiment will lead to a varying tendency to participating in the phase two experiment, implying potential self-selection that may result in biased experimental results. To examine whether self-selection bias would be a substantial concern, we first conducted a pilot study, in which 42 participants (these participants were not allowed to participate in the actual study) were recruited from Amazon Mechanical Turk to complete the first version of our two-phase experiment.
In this pilot study, we asked each participant to complete a sequence of 5 room assignment tasks of varying difficulty levels as well as 36 RME questions in phase one. Two hours later, we invited all participants who had completed phase one to join the second-phase experiment, in which they would be randomly grouped together into teams of three members and they were asked to solve another sequence of 5 room assignment tasks together with their teammates. the pilot study, with respect to their skill levels (i.e., the cumulative score a worker got in the 5 room assignment tasks of the phase one experiment; top row) and their social perceptiveness levels (i.e., the number of RME questions a worker answered correctly in the phase one experiment; bottom row). Visually, it is clear that during the pilot study, participants who decided to take the phase two experiment had both higher skill levels and higher social perceptiveness levels, compared to the entire pool of participants who had completed the phase one experiment. In other words, the experimental design and procedure that we adopted during our pilot study indeed led to a degree of self-selection bias. To decrease the level of self-selection bias, we made three changes during our main experiment. First, we altered the mix of tasks that we included in the phase one experiment to 3 "easy" tasks and 2 "hard" tasks. We hypothesized that with a higher fraction of easy tasks in phase one, participants would have a higher perceived self-efficiency in the room assignment tasks, and thus more Figure A-6: Comparing the distributions of phase one participants and phase two participants with respect to their skill (i.e., scores obtained in room assignment tasks) and social perceptiveness levels (i.e., scores obtained in RME tests). Left: comparison results for the pilot study; Right: comparison results for the main experiment. Gaussian kernels are used for kernel density estimation. likely to come back during phase two to complete more such tasks. Second, we adopted a block randomization scheme rather than a simple randomization scheme during our real experiment. Each block corresponded to a particular mixture of participants with high/mixed/low skill and high/low social perceptiveness (see Section 2.2.3 for more details), and we set the target number of workers to recruit at the block level.
Doing so allowed us to effectively oversample the subgroups of participants who were potentially underrepresented in phase two, compared to the pool of participants in phase one (e.g., participants who had a lower skill and social perceptiveness levels) 1 .
Finally, we extended the gap between the two phases of our experiment from two hours to six days, conjecturing that a longer gap would refresh participants' memory and potentially lead more of them to find it enjoyable to take similar types of tasks again in our phase two experiment. 
Figure A-
6 (right panel) shows the distribution comparisons between participants who completed phase one and phase two of the real experiment. Here, we find there is no clear difference between the two groups of participants in terms of either their skill or their social perceptiveness. In other words, with the three changes that we made, we managed to minimize the self-selection biases between the two phases in our real experiment.  
Figure A-7
: Varying the room assignment task difficulty vs normalized score. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty reduces the normalized score for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals.  
Figure A-8
: Varying the room assignment task difficulty vs duration. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty increases the time it takes participants to submit an assignment for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals.  
Figure A-9
: Varying the room assignment task difficulty vs efficiency. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty reduces the efficiency for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals. 
Table A
.1: The relation between the team's cognitive style diversity (in terms of whether all team members are fast/slow problem solvers or both types exist in the team) and team performance. Data is combined across teams in all six blocks, and for all five tasks. Models relate performance measures (standardized within each task) with the team's cognitive style diversity. All models include random effects for teams as well as the team's skill level category as an intercept to account for dependence across tasks. Increasing a team's cognitive style diversity has no effect on the team's score, but reduces duration  
Table A
.3: The relation between the team's cognitive style diversity (in terms of whether all team members are pragmatic/tenacious or both types exist in the team) and team performance. Data is combined across teams in all six blocks, and for all five tasks. Models relate performance measures (standardized within each task) with the team's cognitive style diversity. All models include random effects for teams as well as the team's skill level category as an intercept to account for dependence across tasks. Increasing a team's cognitive style diversity has no effect on the team's score and duration.    (i.e., average error across all rounds) for the three experimental conditions. We find that participants in groups connected by dynamic influence networks (Dynamic condition) achieved 38% reduction in average error compared to participants in unconnected groups (Solo condition), and 12% reduction in average error compared to participants in groups connected by static influence networks (Static condition). Panel (B) compares the average performance of individuals across conditions. Two-sample t-tests show a significant difference between the average individual error of participants in the Solo and Static conditions ( < 0.0001), as well as between participants in the Static and Dynamic conditions ( < 0.001). Panel (C) compares the standard deviation of participant's individual performance across conditions, and shows that individual performance in groups connected by dynamic influence networks was, not only better on average, but also substantially more equal on its distribution among group members. shows the error in the adapted period (i.e., periods 
[6]
[7]
[8]
[9]
[10]
 and 
[16]
[17]
[18]
[19]
[20]
). The error for the initial guess in both panels is the same across conditions, however, the dynamic network condition incurs much lower errors in the adapted periods (as in Panel B).   


A.7 Group composition; Supporting tables


A B
I
want to thank Duncan Watts, a mentor and a committee member who challenged me to keep up with his fine taste in research questions and to maintain a high degree of scientific rigor. Duncan inspires me with his very sharp process of thought and ability to approach just about any problem. Most importantly, he provided me with a standard of excellence in research, a standard which I hope this dissertation and all of my subsequent research lives up to. It is an honor to work with one of your heroes, and I happen to be just such a lucky person. I would also like to thank Serguei Saavedra, another mentor and committee member. Serguei's 1.873 Fundamentals of Network and Community Ecology class taught me how to question my most fundamental assumptions and helped in forming much of the balance and direction of this dissertation. Serguei's unique approach to research and teaching set an example that I will try to follow. I have had many other mentors over the years as well, all of whom are special to me and inspired me not only by their research, but also by their professional and personal lives. In particular, I am thankful to Iyad Rahwan, Matthew Salganik, Christopher Bail, David G. Rand, Anas Alfaris, Sinan Aral, Marta Gonzalez, John R. Williams, List of Figures 1-1 Linking network structures and collective outcome. The green regions represent the feasibility domains (parameter space or values of confirmation bias rates compatible with the persistence of the belief in the community) of two network structures in a belief dynamics model. . . 1-2 Although the left domain (the efficient network) is larger than the right domain (inefficient network), what matters is the overlap between the feasibility domain and the characterization of the environment. . . . . 2-1 Schematic illustration of the experiment design. . . . . . . . . . . . . 2-2 The five task difficulty levels in phase two were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Increasing the task difficulty (i.e., environment complexity) reduces the normalized score and increases the time it takes participants to submit an assignment. Data is combined across both individual and team conditions across all 6 blocks. Error bars indicate 95% confidence intervals. The effective normalized score of a feasible solution is 80% and the minimum time required for a solution to be submitted is one minute, hence the starting points of the Y axes. . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-3 Comparing performance across individuals, real teams, and nominal teams. Individual, real team, or nominal team data is combined across all 6 blocks and standardized within each task complexity level. Error bars indicate 95% confidence intervals. . . . . . . . . . . . . . . . . . 2-4 Team composition and team performance. The effects of cognitive style diversity shown in the figure are for participants' cognitive styles in solving the room assignment task as defined by "optimizer" vs. "satisfier." Error bars indicate 95% confidence intervals. See Appendix A.7 for additional analyses on the effects of skill/cognitive style diversity. 2-5 Using linear regression (70% training, and 30% testing; randomized and repeated 5 times) to predict team's normalized score with team's skill level, skill diversity, social perceptiveness, cognitive style diversity, and the number of female team members. (A) Compares predictive performance for covariates regressed independently (i.e. in separate models; green symbols), and in a single model where covariates are added in order of increasing independent predictive performance (purple symbols). (B) Predictive performance for a single regression model where covariates are added in order of decreasing independent predictive performance. Error bars indicate 95%confidence intervals.. . . . . . . . .


enced by in the next round. . . . . . . . . . . . . . . . . . . . . . . . 3-3 Guess the Correlation Game. An illustrative examples of the scatter plots used in the experiment is shown in Panel (A). Task difficulty, therefore, could be varied systematically at the individual level by varying the number of points, linearity, and the existence of outliers. For any given round, all participants saw plots that shared an identical true correlation, but difficulty levels could differ among them as shown in Panel (B). Participants were not informed about the difficulty level they or other participants were facing. . . . . . . . . . . . . . . . . . 3-4 Shock to the Information Environment. We provide a change in the environment after round 10 by changing the difficulty levels for the participants for the remainder of the experiment and thereby we simulate non-stationary distributions of information among participants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-5 Individual and collective outcomes. Groups connected by dynamic influence networks and provided with feedback incur substantially lower individual errors as shown in Panels (A) & (B); and collective errors in Panels (C) & (D).


Errors are normalized with respect to average errors in the solo condition within each study. Error bars indicate 95% confidence intervals. . . . . . . . . . . . . . . . . . . . . 3-6 Mechanisms promoting collective intelligence in dynamic networks. Panel (A) shows that the network becomes more centralized with time (Freeman global centralization-i.e., how far the network is from a star network). Panel (B) depicts the relation between performance (i.e., average error) and popularity (i.e., number of followers). Panel (c) shows the relationship between accuracy of initial estimate and confidence (i.e., resistance to social influence). Error bars indicate 95% confidence intervals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-7 An example of the network evolution in the experiment. The circle color represents performance. The size of each circle represents the number of followers (i.e., popularity). The dashed orange line is the distribution of estimates prior to social influence, the blue solid line is the distribution of post-social influence estimates, while the dashed vertical line is the true correlation. . . . . . . . . . . . . . . . . . . .


solo top-1 ; < 10 −2 ). Bars indicate 95% confidence intervals. . . . . . . . . . . 3-9 Traditional accounts of 'wisdom of crowds' phenomena assume unbiased and statistically independent signals among agents. In our model, we assume arbitrary (potentially biased) initial signals. . . . . . . . . 3-10 Evolution of collective error: wisdom of the crowd (WC) and wisdom of the dynamic network (WDN). Panel A) stationary distribution of information among agents. Panel B) non-stationary information environment, shocks to the information distribution introduced at = {100, 200} 3-11 As the nose level increases in the provided feedback, the collective performance degrades until it converges to the performance of the independent crowd. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .


3- 12 97 A- 1
12971
Panel (A): Learning rates associated to different 's, where colored bands show 95% confidence intervals. Panel (B): Effects of and on collective error, where shades of orange indicate time-averaged collective error. Panel (C): Effects of and on collective error, normalized per type of information environment ( column). . . . . . . . . . . . 4-1 The Virtual Lab Framework. The figure illustrates the three conceptual dimensions for virtual lab experiments. . . . . . . . . . . . . . . An illustration of the "room assignment" task used in phase one of the experiment. In this case, there are = 6 students that need to be assigned to = 4 rooms, while satisfying = 2 constraints. . . . . . A-2 An illustration of a more difficult "room assignment" task. In this case, there are = 18 students that need to be assigned to = 8 rooms, while satisfying = 18 constraints. . . . . . . . . . . . . . . . . . . . A-3 An illustration of phase two "room assignment" task that was done by a group of three individuals in phase two. . . . . . . . . . . . . . . . . A-4 An illustration of the "Reading the Mind in the Eye" test used in phase one of the experiment. The participant is shown a pair of eyes and asked to choose the emotion that best describes what the individual in the picture is feeling or thinking of. . . . . . . . . . . . . . . . . . . .


A- 6
6
Comparing the distributions of phase one participants and phase two participants with respect to their skill (i.e., scores obtained in room assignment tasks) and social perceptiveness levels (i.e., scores obtained in RME tests). Left: comparison results for the pilot study; Right: comparison results for the main experiment. Gaussian kernels are used for kernel density estimation. . . . . . . . . . . . . . . . . . . . . . . A-7 Varying the room assignment task difficulty vs normalized score. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty reduces the normalized score for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals. A-8 Varying the room assignment task difficulty vs duration. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty increases the time it takes participants to submit an assignment for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals. . . . . . . . . . . . . . . . . . . . . . . . . . A-9 Varying the room assignment task difficulty vs efficiency. The five task difficulty levels were characterized by the different number of students to be assigned, the number of dorm rooms available, and the number of constraints. Data is analyzed separately for individuals and teams from each of the six blocks. Increasing the task difficulty reduces the efficiency for both individuals and teams of all skill levels and social perceptiveness. Error bars indicate 95% confidence intervals. . . . . . A-10 Out of sample predictions on the team's cumulative score. Predict the team's normalized score with the team's skill level, skill diversity, social perceptiveness, cognitive style diversity, and the number of female team members. Three models (i.e., linear regression, elasticNet, and random forests) are used. Models are first learned on 70% of the teams and then tested on the rest 30% of the teams. This procedure is then repeated 5 times. Error bars indicate 95% confidence intervals. In all models, the majority of the explained variance in team's normalized score can be attributed to the team's skill level. . . . . . . . . . . . . . . . . . . A-11 Out of sample predictions on team's duration on tasks. Predict the team's duration on tasks with the team's skill level, skill diversity, social perceptiveness, cognitive style diversity, and the number of female team members. Three models (i.e., linear regression, elasticNet, and random forests) are used. Models are first learned on 70% of the teams and then tested on the rest 30% of the teams. This procedure is then repeated 5 times. Error bars indicate 95% confidence intervals. The set of independent variables can hardly be used to explain the variance in team's duration on tasks. . . . . . . . . . . . . . . . . . . . . . . . B-1 Participants in all conditions make independent guesses about the correlation of two variables independently. . . . . . . . . . . . . . . . . . B-2 Participants in the network condition engage in a an active social learning phase, where they are exposed to their ego-network's estimates in real time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B-3 After each task round, participants in the feedback conditions see the appropriate level of feedback for the conditions. This figure illustrates the dynamic network condition with full feedback (i.e., as opposed to no-feedback or only self-feedback). In all of our experiments, the maximum number of outgoing connections is three. . . . . . . . . . . B-4 Dynamic social influence benefits the performance of individuals in the crowd. (A) Kernel Density Estimate (KDE) of participants' individual performance (i.e., average error across all rounds) for the three experimental conditions. We find that participants in groups connected by dynamic influence networks (Dynamic condition) achieved 38% reduction in average error compared to participants in unconnected groups (Solo condition), and 12% reduction in average error compared to participants in groups connected by static influence networks (Static condition). Panel (B) compares the average performance of individuals across conditions. Two-sample t-tests show a significant difference between the average individual error of participants in the Solo and Static conditions ( < 0.0001), as well as between participants in the Static and Dynamic conditions ( < 0.001). Panel (C) compares the standard deviation of participant's individual performance across conditions, and shows that individual performance in groups connected by dynamic influence networks was, not only better on average, but also substantially more equal on its distribution among group members. . B-5 Panel (A) shows individual errors in the full game and Panel (B) shows the error in the adapted period (i.e., periods [6-10] and [16-20]). The error for the initial guess in both panels is the same across conditions, however, the dynamic network condition incurs much lower errors in the adapted periods (as in Panel B). . . . . . . . . . . . . . . . . . . B-6 Panel (A) shows the errors before the interactive estimation phase (i.e, pre-social learning). Panel (B) shows the errors after the participants revised their estimates in the static and dynamic network conditions (i.e., post-social learning). . . . . . . . . . . . . . . . . . . . . . . . . B-7 The distribution of pre-social learning and post-social learning for the three conditions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B-8 Dynamic social influence effect in individual rounds: Adaptive with time and reduces individual error. All error rates are post-social learning errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 List of Tables 2.1 Main properties of the 5 room assignment tasks used in phase one of our experiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Summary of the six blocks that we used in phase two of our experiments. 2.3 Main properties of the 5 room assignment tasks used in phase two of our experiment. The order of tasks was randomized in the experiment.


individuals in this community (i.e., individuals can have one of two possible rates of confirmation bias 3 ). Then we can view the system from the lens of its parameter space in Figure 1-1. The axes of Figure 1-1 represent the 2-dimensional parameter space of confirmation bias rates. The points 1 , 2 , and 3 are three choices of confirmation bias parameter values. The colored regions correspond to the set of confirmation bias rates compatible with positive beliefs about the topic in the community (the necessary condition for the persistence of the belief). The size and shape of this region depend upon network structure (structures 1 and 2 ). In mathematical ecology, these regions are usually called the feasibility domain of a community [126].


Figure 1 - 1 :
11
Linking network structures and collective outcome. The green regions represent the feasibility domains (parameter space or values of confirmation bias rates compatible with the persistence of the belief in the community) of two network structures in a belief dynamics model.


Figure 1 - 2 :
12
Although the left domain (the efficient network) is larger than the right domain (inefficient network), what matters is the overlap between the feasibility domain and the characterization of the environment.


Figure 2 - 1 :
21
Schematic illustration of the experiment design.


Figure 2 -
2
1 for an illustration of the experiment design


periment. The study was reviewed by the Microsoft Research Ethics Advisory Board and approved by the Microsoft Research Institutional Review Board (Approval#: 0000019). All participants provided explicit consent to participate in this study and MSR IRB approved the consent procedure. All data collected in the experiment could be associated only with the participant's Amazon Worker ID on MTurk, not with any personally-identifiable information. All participants remained anonymous for the entire study. In each phase of the experiment, participants first read instructions and could start the experiment only after they had correctly answered a set of questions testing their comprehension of the instructions (see Appendix A.1 screenshots and examples).


a
simple randomization scheme and had individuals of different levels of skills and social perceptiveness to form teams of three members at random in phase two. The majority of the teams formed in this way contained a mixture of high/low skill (or high/low social perceptiveness) individuals. As a result, the variance of a team's skill or social perceptiveness level (defined as the average skill or social perceptiveness level of members in that team) across different teams was limited. Practically, this


1 2 × 1 2 = 1 4 1 4 3 = 1 164 .
113164
hence under simple random assignment the expected frequency of all HH teams would be Of the 1,200 participants who were qualified for phase two, 828 participants entered the experiment and 237 of them placed in the individual condition (the data for 3 of them was incomplete; hence the effective number of individuals is 234) and 591 placed in the team condition. Of the 197 teams formed, the data for 1 team was incomplete, hence the effective number of teams is 196. In the absence of block randomization, therefore, we would expect to have 196/64 = 3 All-HH teams. With block randomization, we guaranteed at least 22 All-HH teams (because of random assignment in the MH block it is possible that one or more additional All-HH teams would result). """

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]