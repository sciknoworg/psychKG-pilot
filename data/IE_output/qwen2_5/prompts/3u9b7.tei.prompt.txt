You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Deliberately making miskates 3


Statement of Relevance
We all must experience failure. However, sub-optimal performance following failure is not an intrinsic consequence of receiving negative feedback but rather due to the conflict of expecting success. We changed performance expectations by asking individuals to deliberately sabotage their behaviour by performing as badly (rather than as well) as possible. Our data show that behaviour is more consistent following success in the context of win maximization and more consistent following failure in the context of loss maximization. We may reduce the impact of failure by expecting it more. The normalization of failure may ultimately improve the consistency and effectiveness of behaviour within education and industry, as well as lead us to reevaluate the extent to which our understanding of human and animal behaviour sciences has been limited by our implicit focus on win maximization.


Introduction
Is failure an inherently negative state? Individuals are less likely to learn from their own failures relative to their successes 
(Eskreis-Winkler & Fishbach, 2019)
, failures have greater subjective value relative to successes of the same objective magnitude 
(Thaler, Tversky, Kahneman & Schwartz, 1997)
, and, failure generates reliably poorer future performance 
(Dyson, Wilbiks, Sandhu, Papanicolaou & Lintag, 2016;
Eben, Chen, Vermeylen, Billieux & Verbruggen, 2020)
. Sub-optimality following failure is also represented by Dual System Theory 
(Kahneman, 2011;
Sloman, 1996)
 in the impulsive and inflexible processing produced by System 1, and, Operant
Conditioning 
(Thorndike, 1911;
Skinner, 1969)
 in the predictable change in behaviour following failure (lose-shift). By these accounts, we would do well to avoid failure 
(Metcalfe, 2016)
.
However, a creative reimagining of failure is provided by the prediction error literature (e.g., 
De Loof, Ergo, Naert, Janssens, Talmsa, Van Opstal & Verguts, 2018;
Niv & Schoenbaum, 2008)
. Here, emphasis is not on the absolute nature of performance outcome but rather the relationship between expected and observed outcome. In the case of unsigned prediction error (UPE), actual outcomes are simply the same or different to expected outcomes (see also 
Carver, 2006)
. Thus, failure may have a detrimental impact not because the feedback is negative per se (ie, lose) but rather because we expected to win. By this argument, we should also see success generating disruption because the organism expects to fail.
We test the idea that success and failure are arbitrary states by measuring behavioural consistency when participants intentionally play a game both as well as possible (win maximization) and as badly as possible (loss maximization). During win maximization, positive outcomes (win) allow for the match between expected and observed states. Conversely, negative outcomes (lose) allow for the match between expected and observed states during loss maximization. If experiencing failure is inherently detrimental, then performance should be universally less consistent following losses relative to wins. If instead behavioural disruption is determined by the mismatch between expected and observed states, then performance should be less consistent for lose trials within the context of win maximization but also less consistent for win trials within the context of lose maximization.
We further challenge the immutability of the fundamental reinforcement learning principles associated with wins and losses (win-stay / lose-shift) by manipulating the strategy required for opponent exploitation (see 
Table 1
). For a fixed percentage of trials, opponents would either reliably repeat or alternate items. In the case of win maximization for the repetition opponent, goal-consistent performance was guaranteed via the expression of traditional win-stay and lose-shift mechanisms:
participants repeat their previous response following win but change their previous response following loss. However, win maximization for the alternation opponent could only be achieved by expressing the opposing rules: win-shift and lose-stay.
These assignments were reversed when the goal was loss maximization: win-shift and lose-stay was required to lose against the repetition opponent, and, win-stay and loseshift was required to reliably lose against the alternation opponent. If there is hardwired precedence for win-stay and lose-shift, then performance should be more consistent when strategy aligns with these principles (specifically, win maximization + repetition opponent, and, loss maximization + alternation opponent). However, if these outcome-action associations are arbitrary, consistency in expressing win-shift should be similar to win-stay, as should consistency in expressing lose-stay relative to lose-shift.


Methods
Data from convenience samples of 71 (Mage = 19.77, SD = 3.12, 33 female), 46 (Mage = 19.2, SD = 1.67, 23 female) and 84 (Mage = 19.47, SD = 3.25, 40 female)
participants were analyzed from the student population at the University of Alberta for Experiments 1, 2 and 3, respectively. These sample sizes exceed the 30-40 participants previously analysed for zero-sum games in our lab (e.g., 
Dyson et al., 2016;
Dyson, Musgrave, Rowe & Sandhur, 2020)
. All student completed the study for course credit and the on-line protocol was approved by the University of Alberta  
Table 1
). Our factor pertaining to cumulative score yielded no notable effects, so we collapse across score manipulation and focus exclusively on exploitable opponency. In Experiment 3, both win maximization (cf., Experiment 1) and loss maximization (cf., Experiment 2) goals were completed in a within-participants design across 6 counterbalanced conditions, without the cumulative score manipulation.
The 90 trials per condition were subdivided into 9 groups of 10 trials each, with groups randomized within conditions. For the unexploitable opponent, each group consisted of 5 Heads and 5 Tails response which were further randomized within each group (e.g., TTHTHHHTHT). For the exploitable via repetition opponent, 3 blocks were identical to the unexploitable opponent, 3 blocks consisted of 10 presentations of Heads in a row (e.g., HHHHHHHHHH), and, 3 blocks consisted of 10 presentations of Tails in a row (e.g., TTTTTTTTTT). For the exploitable via alternation opponent, 3 blocks were once again identical to the unexploitable opponent, 3 blocks consisted of 10 coin alternations beginning with a Head (e.g., HTHTHTHTHT), and, 3 blocks consisted of 10 coin alternations beginning with a Tail (e.g., THTHTHTHTH).
Consequently, all opponent types played an equal percentage (50%) of Heads and Tails within each condition.
At each trial, participants would press one of the two buttons corresponding to Heads 
[K]
 or Tails 
[L]
 prompted by a fixation cross. Both participant and opponent selections were shown on the left and right side of screen, respectively, for 1000 ms.
Selections were removed during a 500 ms pause, followed by either "WIN (+1)" or 'LOSE (-1)" in green and red font, respectively, for 1000 ms. Scores were updated and the fixation cross returned. Before win maximization conditions participants were instructed to "Just try to do as well as you can!" and before loss maximization conditions "Just try to do as BADLY as you can! Try to get the most negative score."  A double dissociation is observed between Experiments 1 and 2, wherein the degree of behavioural consistency is significantly greater following a winning rather than losing outcome when the goal is win maximization (Experiment 1) but significantly smaller when the goal is loss maximization (Experiment 2). In both Experiments 1 and 2, this is irrespective of whether the action is consistent or inconsistent with the fundamental reinforcement learning principles of win-stay and lose-shift.


Figure 2.
This double dissociation replicates in the within-participants design of Experiment 3. Both win-stay and win-shift are more consistent in the context of win maximization but both lose-stay and lose-shift are more consistent in the context of loss maximization.


Results
During win maximization against an opponent exploitable via repetition (Experiment 1; see left side of 
Figure 1
), both the degree of win-stay (r = .708, p < .001) and lose-shift (r = .423, p < .001) positively correlated with win rate. Stay actions following wins had a significantly stronger correlation than shift actions following losses (z = 2.347, p = .019; two-tailed z test 
[Diedenhofen & Musch, 2015;
Dunn & Clarke, 1969]
 losses had a significantly stronger correlation than actions following wins (z = 4.057, p < .001). Experiment 2 reveal a double dissociation with Experiment 1 in that when the goal of the task was loss maximization, the deployment of either stay or shift behaviour was now more consistent following losses relative to wins.
This same double dissociation replicated in Experiment 3 using a withinparticipants design. In the context of win maximization against an opponent exploitable via repetition, the ability to enact win-stay (r = .804, p < .001) behaviour was more consistent than the ability to enact lose-shift (r = .608, p < .001) behaviour (z = 2.744, p = .006; first column in 
Figure 2
). In the context of win maximization against opponent exploitable via alternation, the negative correlation between win rate and win-stay (r = -.877, p < .001) behaviour was more consistent than lose-shift (r = -.369, p = .001) behaviour (z = -6.047, p < .001; second column in 
Figure 2
). In the context of loss maximization against opponent exploitable via repetition, the ability to enact lose-shift (r = .879, p < .001) behaviour was more consistent than the ability to enact win-stay (r = .791, p < .001) behaviour (z = -2.250, p = .024; third column in 
Figure 2
). In the context of win maximization against opponent exploitable via alternation, the negative correlation between win rate and lose-shift (r = -.835, p < .001) was more consistent than win-stay (r = -.509, p < .001) behaviour (z = 4.060, p < .001; fourth column in 
Figure 2
). As in Experiments 1 and 2, behaviour is more consistent both following wins in the context of win maximization and following losses in the context of loss maximization.
Furthermore, we observed no evidence that win-stay / lose-shift were privileged forms of outcome-action association. For example, in the context of win maximization, participants were significantly less consistent in their ability to deploy win-stay behaviour (during repetition) than win-shift behaviour (during alternation) to intentionally increase win rate (r = .708 vs [abs] r = .874; z = -2.935, p = .003; two top left panels of 
Figure 1
). In the context of loss maximization, participants were no less consistent in their ability to deploy lose-stay behaviour (during repetition) than 1.086, p = .278; two bottom right panels of 
Figure 1
). Therefore, this increased behavioural consistency following wins when the goal-state is success and following losses when the goal-state is failure is independent of whether the individual is required to repeat (stay) or change (shift) their response to maintain maximization.


Discussion
There are two major implications of this work. First, our data show that failure is an arbitrary state. The disruption generated by failure is not the result of its absolute negative valence but rather of the error generated between the expected outcome of winning and the observed outcome of losing. Our observations that behaviour is similarly disrupted by losing in the context of pursuing wins, and, winning in the context of pursuing losses gives further credence to the framework of unsigned (UPE) rather than signed (SPE) prediction error. For UPE, the relationship between expected and observed outcomes are adjudicated on whether they are simply same or different.
In the case of SPE, the valence of outcome remains important such that in the case of expecting to win, an actual win would be as-expected but a loss worse-than-expected.
Similarly, in the case of expecting to lose, an actual loss would be as-expected but a win would be better-than-expected. In the current data, there is no indication that worse-than-expected outcomes impact behaviour any differently than better-thanexpected outcomes, and hence no indication of SPE. Rather, experiencing losses when the goal is to win, and, experiencing wins when the goal is lose, both create prediction error and disrupt behavioural consistency to the same degree.
Second, our data show a remarkable ease with which we can switch out of putatively fundamental operant conditional outcome-action associations represented by win-stay and lose-shift. Our fluid adaptation to counter reinforcement-learning strategies is consistent with data from nectarivorous birds and other organisms who also flexibility adapt to environments that have high depletion rates 
(Stagner, Michler, Rayburn-Reeves, Laude & Zentall, 2013;
Sulikowski & Burke, 2012)
. However, humans may be unique in our ability to sabotage our own performance in pursuit of the novel goal of loss maximization demonstrated here. Expansion of these findings to other binary and non-binary decision-making spaces is warranted to assess the generalizability of these findings.
The historical emphasis on win-stay and lose-shift mechanisms, and the presumed disruption of performance as a result of failure, are due to the often unspoken goal of win maximization within empirical sciences. We have shown that inconsistent performance generated by the experience of losing is simply the result of a mismatch between an expected goal state and the current observed state. The same mismatch and the same consequences of that mismatch are produced when the intention is to lose but instead the organism wins. In other words, there is nothing inherently damaging about the cognitive impact of losing, as long as one has the goal Deliberately making miskates 15 of losing in mind. Relishing loss as in the principle of 'fun failure' 
(McGonigal, 2011;
Nguyen, 2020)
 or demonstrating cognitive proficiency via the act of intentional selfsabotage, are all steps towards normalizing the experience of negative outcomes. The normalization of failure may ultimately improve the consistency and effectiveness of behaviour within education and industry, as well as lead us to re-evaluate the extent to which our understanding of human and animal behaviour sciences has been limited by our implicit focus on win maximization.
Figure 1. A double dissociation is observed between Experiments 1 and 2, wherein the degree of behavioural consistency is significantly greater following a winning rather than losing outcome when the goal is win maximization (Experiment 1) but significantly smaller when the goal is loss maximization (Experiment 2). In both Experiments 1 and 2, this is irrespective of whether the action is consistent or inconsistent with the fundamental reinforcement learning principles of win-stay and lose-shift.


). With the same goal of win maximization against an opponent exploitable via alternation, both the degree of win-stay (r = -.874, p < .001) and loseshift (r = -.446, p < .001) negatively correlated with win rate (consistent with the requirement to win-shift and lose-stay). However, actions following wins once again had a significantly stronger correlation than actions following losses (z = -5.019, p < .001). The data are clear that the requirement to deploy either stay or shift behaviour as a function of opponent was more consistent following wins relative to losses, when the goal of the task is win maximization.During loss maximization against an opponent exploitable via repetition (Experiment 2; see right side ofFigure 1), both win-stay (r = .585, p < .001) and loseshift (r = .895, p < .001) behaviour were again positively correlated with win rate. In contrast to Experiment 1, actions following losses had a significantly stronger correlation than actions following wins (z = -3.895, p < .001). Against an opponent exploitable via alternation, both the degree of win-stay (r = -.356, p = .015) and loseshift (r = -.859, p < .001) were negatively correlated with win rate. Actions following


lose-shift behaviour (during alternation) to intentionally decrease win rate ([abs] r = .895 vs. r = .859; z = 1.283, p = .199; two bottom right panels of Figure 1). In Experiment 3, win-stay behaviour (during repetition) was numerically less consistent than win-shift behaviour (during alternation) during win maximization (r = .804 vs [abs] r = .877; z = -1.601, p = .109; two top left panels of Figure 2). Lose-shift behaviour (during alternation) was numerically less consistent than lose-stay behaviour (during repetition) during loss maximization ([abs] r = .879 vs. r = .835; z =


In Experiment 2, the goal was loss maximization. The 6 counterbalanced conditions consisted of the presence or absence of a cumulative score, crossed with three different kinds of opponency (unexploitable, exploitable via repetition, exploitable via alternation; see
Research Ethics Committee (Pro00102699 and Pro00112365). Paradigms were controlled by Presentation 20.2 (build 07.25.18) and delivered remotely after participants downloaded Presentation Package Player. 12 (Experiment 1), 12 (Experiment 2), and 30 (Experiment 3) additional participants were excluded from analyses as a result of completing conditions multiple times and / or choosing only one response in at least one condition. For Experiments 1-3, participants played 540 rounds of Matching Pennies consisting of 6 opponents 90 times each. Participants won the trial if coin sides mismatched and lost the trial if coin sides matched. In Experiment 1, the goal was win maximization.








Funding: The studies were funded by an NSERC Discovery Grant (RGPIN-2019-04954), Alberta Gambling Research Grants, and start-up monies provided by the University of Alberta (RES0042096).
Competing interests: Authors declare no competing interests.
Data and materials availability: All data is available in the supplementary materials.
 










Approach, avoidance, and the self-regulation of affect and action




C
S
Carver








Motivation and Emotion




30
















Signed reward prediction errors drive declarative learning




E
De Loof






K
Ergo






L
Naert






C
Janssens






D
Talmsa






F
Van Opstal






T
Verguts








PLoS One




13


189212














cocor: A Comprehensive Solution for the Statistical Comparison of Correlations




B
Diedenhofen






J
Musch








PLoS One




10


121945














Correlation coefficients measured on the same individuals




O
J
Dunn






V
Clarke








Journal of the American Statistical Association




64
















A direct and conceptual replication of post-loss speeding when gambling




C
Eben






Z
Chen






L
Vermeylen






J
Billieux






F
Verbruggen








Royal Society Open Science




7














Not learning from failure-the greatest failure of all




L
Eskreis-Winkler






A
Fishbach








Psychological Science




30
















Behavioural and neural interactions between objective and subjective performance in a Matching Pennies game




B
J
Dyson






C
Musgrave






C
Rowe






R
Sandhur








International Journal of Psychophysiology




147




















B
Dyson






J
Wilbiks






J
M P
Sandhu






R
Papanicolaou






G
Lintag






J


















Negative outcomes evoke cyclic irrational decisions in Rock, Paper






Scissors. Scientific Reports




6


20479












Thinking, Fast and Slow (Farrar, Straus and Giroux




D
Kahneman








New York












Reality is broken: Why games make us better and how they can change the world




J
Mcgonigal








Penguin Press












Learning from errors




J
Metcalfe








Annual Review of Psychology




68














Games: Agency as art




C
T
Nguyen








Oxford University Press












Dialogues on prediction errors




Y
Niv






G
Schoenbaum








Trends in Cognitive Sc\ience




12
















Contingencies of reinforcement




B
F
Skinner








Appleton-Century-Crofts


New York












The Empirical Case for Two Systems of Reasoning




S
A
Sloman








Psychological Bulletin




119
















Midsession reversal learning: Why do pigeons anticipate and perseverate? Learning and Behaviour




J
P
Stagner






D
M
Michler






R
M
Rayburn-Reeves






J
R
Laude






T
R
Zentall








41














Shifting in nectarivorous birds: Selective inhibition of the learned win-stay responses




D
Sulikowski






D
Burke








Animal Behaviour




83
















The effect of myopia and loss aversion on risk taking: An experimental test




R
H
Thaler






A
Tversky






D
Kahneman






A
Schwartz








Quarterly Journal of Economics




112




















E
L
Thorndike








Animal Intelligence






Macmillan Company













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]