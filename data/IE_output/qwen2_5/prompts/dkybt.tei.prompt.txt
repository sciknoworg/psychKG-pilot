You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Both discrete and continuous response scales are used in many tasks that measure and/or assess components of psychological functioning, ranging over assessments of personality, physical and mental health, beliefs, opinions, confidence, and performance related to constructs such as intelligence, executive control, and working memory. 
Likert (1932)
 scales have a long history of extensive use in such areas, but there are many known limitations of such discrete measures 
(Paulhus, 1991)
. The addition of a continuous measure like response time or confidence, or additional responses like best-worst rankings, provide richer insights into the cognitive processes underlying responses and allow a researcher to make inferences that are not possible with simple binary choice data 
(Hawkins et al., 2014;
Louviere et al., 2015;
. Of course, the degree to which we can determine whether the true underlying constructs we seek to measure are discrete or continuous is a major challenge 
(Luce, 1997)
, and there are cases like item response theory where we would like to use a discrete scale because the different levels of a continuum cannot be fully distinguished 
(Hambleton & Swaminathan, 2013)
. Across all of these contexts, a particular challenge for models of psychological and cognitive processes is to provide a unified account of how continuous underlying psychological constructs (e.g., confidence, strength of belief, commitment) or percepts 
(pitch, timbre, length, color)
 are mapped onto continuous and discrete response scales 
(Townsend, 2008;
Luce, 1997)
. To address this challenge, we develop a modeling approach for, and empirical evidence about, tasks where participants choose among discrete options and provide values on a continuum of responses for those same options.
The key to developing a unified account is understanding how people map internal representations onto external responses, controlling for the systematic biases or distortions that occur during the mapping process 
(Zotov et al., 2010)
. Cognitive models have been developed for several types of graded response measures, capturing the responses and response times associated with confidence and probability estimates 
(Smith & Van Zandt, 2000;
Pleskac & Busemeyer, 2010;
Ratcliff & Starns, 2009
, 2013
Busemeyer, Kvam, & Pleskac, 2019)
, preference ratings 
(Bhatia & Pleskac, 2019;
, and opt-out (deferred or "don't know") responses 
(Bhatia & Mullett, 2016;
Reynolds, Garton, et al., 2020)
. Incorporating response times is important to understanding how these types of responses are generated and to accurately interpreting them, as an overwhelming body of empirical evidence shows that the time at which a response is made has a nontrivial interaction with the type or magnitude of the response and how diagnostic it is 
(Baranski & Petrusic, 1998;
Donkin, Brown, Heathcote, & Marley, 2009a;
Yu et al., 2015;
Luce et al., 1982)
. Thus, a complete account of behavior on mapping tasks must predict the joint distributions of responses and the time it takes to make them.
In the past, models have mainly focused on discretevalued (choice) responses, but continuous-valued responses can theoretically confer more information about internal representations. Recent developments of models for tasks involving continuous responses 
(Kvam, 2019a;
Ratcliff, 2018;
Smith, 2016)
 have enhanced our understanding of the cognitive processes underlying these tasks, and have begun to be applied successfully in domains such as orientation estimation 
(Kvam, 2019b;
Ratcliff, 2018)
, color identification 
(Ratcliff, 2018;
Smith et al., 2020)
, numeracy 
(Ratcliff & McKoon, 2020)
, and pricing 
(Kvam & Busemeyer, 2020)
. Critically, relations between continuous and discrete response measures have not been thoroughly explored. In this paper, we examine the relationship between different response scales, and evaluate how (if) the response scale influences the representation of decision evidence and decision strategies.
Although the model we develop has the potential to be applied to almost any type of response and response time data, we focus here on using it to tie together continuous and discrete response formats for two fundamental types of paradigms that require participants to take a perceived stimulus and map it onto a response, as in traditional absolute *Please direct correspondence to pkvam@ufl.edu, 945 Center Dr, P.O. Box 112250, Gainesville, FL, 32607, USA. Part of the material contained in this paper was presented at the 2018 Midwest Cognitive Science and 2017 Society for Mathematical Psychology conferences, and the data set from Study 2 was part of the first author's PhD dissertation. Study materials and code for this paper can be found on the Open Science Framework at osf.io/6d29q. This study was not preregistered. Acknowledgements: This research has been supported by Natural Science and Engineering Research Council Discovery Grant 8124-98 to the University of Victoria for A.A.J.M., a National Science Foundation Graduate Research Fellowship and international research opportunity grant DGE-1424871 to P.D.K., and an ARC Professorial Fellowship to A.H. (DP110100234).
The second author, Tony (A.A.J.) Marley, passed away shortly before this paper was submitted. We are extremely grateful for his guidance, his insights on the anchor-based approach to modeling that were fundamental to the creation of MAAT, his comments on this work, and to him personally as a colleague and collaborator. judgment tasks. Such paradigms have been modeled by extensions of choice RT models 
(Nosofsky, 1997)
 and used to address questions about information processing capacity 
(Miller, 1956)
, the relationship between the number of stimuli (and/or responses) and accuracy 
(Luce et al., 1982)
, and the relationship between response times and the response chosen 
(Kent & Lamberts, 2005;
Lacouture & Marley, 1991
, 1995
. Absolute judgments, therefore, provide an ideal test-bed for a general model of mapping tasks, revealing fundamental phenomena that will also impact more complex tasks and measures. A natural question is whether mappings onto continuous or discrete scales involve fundamentally different cognitive processes. 
Busemeyer et al. (1997)
 reviewed learning differences in discrete and continuous response tasks, including a number of diverging patterns for categorical versus continuous function learning, and developed a computational model of the disparate systems associated with discrete and continuous tasks. However, this model addressed the "front-end" differences in learning between the tasks, and not the "back-end" response processes involved in the two paradigms. This leaves unresolved the connection between continuous and discrete response scales, which we address here.


Extrapolating to the continuum
We might expect that a continuum of responses -as used in function learning tasks, for instance 
(Koh & Meyer, 1991)
 -would arise as the limiting case as the number of categories in a discrete-category task grows very large and fine-grained. This is arguably the most coherent relationship between discrete and continuous response paradigms (see 
Kvam, 2019a;
Schurgin et al., 2020)
. In this case, we could simply examine how the parameters of our chosen model shift as the number of response options increases, and extrapolate to the asymptotic case where the number of response options approaches infinity (or at least, hits the number of pixels on the response scale or the limits of spatial discrimination for human decision makers). However, the continuous case as an asymptotic limit can be somewhat problematic for models initially formulated for discrete responses. Using 
Hick's (logarithmic)
 law as an example 
(Hick, 1952)
, the lack of an asymptote as the number of responses increases forces the bizarre prediction that response times will become infinitely long in the continuous case. Naturally, this prediction does not pan out in empirical response time data, as violations of Hick's law have been observed when there are (substantially) more than eight alternatives 
(Longstreth, 1988;
Seibel, 1963)
.
Recent cognitive models are more promising in terms of identifying candidate processes that may differ for discrete versus continuous response scales. For instance, 
Usher et al. (2002)
 suggested that shifts in mean response times as a function of the number of response alternatives could be primarily attributed to changes in the threshold for making a de-cision. Due to the space of response options becoming more "crowded" and baseline or guessing accuracy decreasing as more alternatives are added 
(see Van Maanen et al., 2012;
Schurgin et al., 2020)
, models that predict responses as a function of racing accumulators must increase the amount of evidence gathered before making a decision in order to maintain a desired level of accuracy and speed 
(Hawkins et al., 2012a)
 or optimize time on the task 
(Hawkins et al., 2012b)
. In this case, we might expect a model of continuous response measures to simply implement a different threshold that sets the trade-off between speed and accuracy. To preview the results of our experiments, threshold shifts wind up being a plausible explanation for differences in performance between tasks with continuous versus discrete responses, making it straightforward to jointly model the two types of paradigms.
To test whether differences in performance among tasks with different numbers of responses can be attributed to threshold shifts, we develop a model that uses a common underlying evidence accumulation process to generate either discrete or continuous responses. In the first study, we examine how this approach can account for changes in accuracy and response time across an evenly-spaced span of stimuli ("bow" effects) and across different numbers of response options ("set-size" effects) with unidimensional (line length) stimuli. Our model utilizes a pair of "anchors", one at each end of the range in which the stimuli lie, which correspond to exemplars of very short or very long stimuli based on the stimuli a participant had seen 
(Marley & Cook, 1984
, 1986
Petrov & Anderson, 2005;
Lacouture & Marley, 2004;
. Note that a participant would learn in the practice trials exactly how long stimuli in the longest / shortest categories would be, so they were well-calibrated to this range. The location of the stimulus relative to these anchors drives an evidence accumulation process. As desired or elicited, either discrete or continuous responses are triggered by a stopping rule based on the total amount of evidence that has been gathered, with the chosen response determined by the balance of evidence with respect to each anchor.
In the second study, the stimuli vary in hue and the data are modeled with three anchors, one each for red, green and blue stimuli. Previous studies have shown that choosing the location of the brightest (target) stimulus is slower and less accurate when one of the distractor stimuli is a "near competitor" (i.e., similar in brightness to the target) 
(Teodorescu & Usher, 2013)
. We find an analogous near-competitor effect for hue, which follows from our model because target options with near competitors (i.e., similar response options) require more support to be chosen than do target options that are distinct from other options. This result violates a core assumption of several continuous models; namely, that stopping rules (i.e., the threshold amount of evidence required to trigger a response) should be consistent across options. Instead, we suggest that the similarity between response op-tions should be a fundamental consideration in selecting a stopping rule for each option in a discrete or continuous set of alternatives.


New predictions
The key component allowing our Multiple Anchored Accumulation Theory (MAAT) to accommodate different numbers of discrete responses is its ability to divide the stimulus representation into separate categories but map this representation onto a continuum. In some ways, this is similar to general recognition theory, where a continuous feature space is divided into categories corresponding to regions bounded by hyperplanes 
(Ashby, 2000;
Smith, 2019)
. The distinguishing feature of MAAT is how support for different categories is gathered and weighed against one another -the support for each choice is the relevant component of a vector defining the corresponding option. The location of the evidence vector (and hence its component along each of the option vectors) changes dynamically over time as a person considers the stimulus. This allows for decision rules that include the relative degree of support for each choice option based on the similarity relations between them (e.g., it is easier to select between options that are distinct, as opposed to options that are very similar).
MAAT makes several predictions that are quite different to those of other approaches to modeling continuous and discrete responses 
(Ratcliff, 2018;
Smith, 2016)
. The first is that the distribution of continuous responses should become more skewed as the target stimulus gets closer to upper or lower anchors (see also 
Kvam & Turner, 2021
, for a discussion of how this can be described in terms of representational similarity). As a result, responses near the ends of a scale should be faster and more accurate. That is, there will be a "bow" effect in performance as a function of stimulus magnitude 
(Luce et al., 1982)
.
Second, the distribution of responses should be wider when they are further away from an anchor point. This is related to the bow effects, and naturally arises when participants have a point of reference against which they can compare and contrast a presented stimulus. Responses near a point of reference tend to be closely grouped and relatively precise 
(Hollands & Dyre, 2000)
, while those in between points of references tend to be relatively uncertain. In our model, this results from how the rate of evidence accumulation is determined by comparing the stimulus to the available anchors (memory traces or exemplars). In supplementary material we show formally that the entropy of the accumulation process in MAAT increases as the distance between a stimulus and the anchors becomes greater.
Third, MAAT predicts that it is more difficult to select a particular response when it has similar competitors compared to when its competitors are very dissimilar. In this case, participants may set a greater threshold for response options with similar competitors in an attempt to improve accuracy, although at the cost of longer response times 
(Usher et al., 2002)
. Importantly, prominent theories of continuous responding, including the circular diffusion model 
(Smith, 2016
(Smith, , 2019
Smith et al., 2020)
 and spatially continuous diffusion model 
(Ratcliff, 2018;
Ratcliff & McKoon, 2020)
 have only a single threshold or an invariant function (e.g., in the color experiments of Ratcliff, 2018, there are higher thresholds for non-primary / non-secondary colors) specifying the thresholds for all response options. Consequently, a set of unequally spaced response options, say A, B, and C, with C being further away (i.e., A-B--C) must have the same thresholds for each option. In contrast to these other theories, MAAT is able to explain not only reduced accuracy for responses with near competitors, but also the ability of participants to maintain or even increase accuracy by selectively slowing such responses.
In the following sections, we apply MAAT to data from both discrete and continuous mapping tasks. In the first experiment we use line-length stimuli, demonstrating the model's ability to provide a simple and tractable account of perceptual judgments on a unidimensional scale with anchors at either end. The model of this first task is fit only to accuracy data (i.e, was a response within or outside the correct range?) and the distribution of continuous responses is used to provide as an "out-of-sample" (cross-validation) test. In the second experiment, we use a hue-based task requiring responses on a color circle. We instantiate a simple theory of vision to account for how people perceive the stimulus, with anchors for red, green and blue, but otherwise employ the same core principles as used for the first task. To show the generality of the model and fitting approach, we fit the model to the precise distribution of responses (multinomial in the discrete condition, and continuous in the continuous condition). Together, the two experiments and accompanying models elucidate how and why behavior changes with different response-set sizes.


Model Overview
Although our proposal uses a novel accumulation structure that allows it to model behavior on both discrete and continuous tasks, it invokes well-developed cognitive mechanisms from previous models of absolute identification, including dynamic models like the selective attention, mapping, and ballistic accumulation (SAMBA) model  and the relative judgment model 
(Stewart et al., 2005)
. It then extends these mechanisms to account for data involving continuous responses. The main hurdle to overcome with respect to previous models it that they associate a unique accumulator (and corresponding accumulation rate and threshold) with each possible response, which is difficult to translate to scenarios where the number of responses is very large. To yield an approach that simultane-ously handles both continuous and discrete responses, we instead base our new model on the geometric 
(Kvam, 2019a)
 and multiple threshold race 
(Reynolds, Garton, et al., 2020;
 frameworks for modeling dynamic choice.


Figure 1
Diagram of discrete (left) and continuous (right) response models. Evidence accumulates over time (red arrow) from its starting point (gray box) according to the drift rates for each anchor, in this case v S for short responses and v L for long responses. A decision is made when sufficient support for one of the responses is gathered, corresponding to the location and time at which the accumulation process crosses the quarter-circular boundary. This particular model diagram corresponds to the one used in Experiment 1, whereas a full circle is used in Experiment 2.
In unidimensional versions of these frameworks, evidence accumulation is represented in terms of two numbers: a balance of evidence among the options (How much does the evidence favor a response towards one end of the scale versus the other end?) and a total amount of evidence (How strong are these beliefs / representations?). The information that a decision maker accumulates changes on both dimensions over time, leading to different response options having different strengths over time, and increasing the overall amount of information that has been considered. An option is chosen when it has enough support, that is, when the match between a person's evidence state and the description of the choice alternatives align well and there is sufficient information to make a decision.
This type of evidence representation can be formally depicted in two dimensions. If "no information" corresponds to an evidence state at [0, 0], then the distance from the origin describes how much information has been collected, and the direction of the evidence state relative to the origin describes the option that is most favored at that moment. For example, in the first task that we study, the position of the state in the x-direction corresponds to the strength of evidence for responses at one end of the scale (i.e., a"short" responses) and the position of the state in the y-direction corresponds to the strength of evidence for responses at the opposing end of the scale (i.e., "long" responses) as shown in 
Figure 1
.
The response selected is determined by the ratio of these two dimensions (i.e., the angle), while response time is determined by the (squared) sum of the dimensions (i.e., the distance from the origin). In other words, a person enters a response when they have gathered enough information, but the response that they make is determined by the balance of evidence between support for short and long responses, reflecting Vickers's 
(Vickers & Packer, 1982;
Vickers, 2001)
 seminal ideas relating to confidence judgements.
This framework can be contrasted with the standard assumption of many diffusion-based models, which track only the balance of evidence -information favoring one relative to another option. Theories producing graded estimates based only on the balance of evidence between two options can fail to produce magnitude effects 
(Teodorescu et al., 2016;
Miletić et al., 2021)
 where increasing the magnitude of both stimuli / choice options (e.g., making both more coherent or easy to see) while maintaining the balance between them speeds up response times (but see 
Ratcliff et al., 2018
, for an approach to this issue that makes variability in the balance of evidence proportional to its mean). As is true racing accumulator models in general 
(Heathcote & Matzke, in press
), having more than one dimension to the evidence accumulation process allows MAAT to capture magnitude effect as well as typical difference effects, where adjusting the balance of evidence by manipulating the ratio of support for two options mainly affects the responses that are given rather than response times 
(Vickers, 2001;
.


General model specification
We propose an "opponent processes" account of evidence accumulation during decision making, where choices are driven by competing sources of information relative to anchors 
(Marley & Cook, 1984)
. In the first experiment, short and long anchors are represented as direction vectors at 0 and 90 degrees respectively. In the second experiment, red / green / blue are represented by directions vectors at 0 / 120 / 240 degrees, respectively. Responses that are in between these anchors are represented by intermediate angles. For example, a medium line length in the first experiment might be at 45 degrees, and a yellow stimulus in the second experiment might be located at 60 degrees.
The state of the information that a decision maker has at a particular point in time is described by a point, which can be defined by an x and y coordinate in two dimensions for the experiments and models presented in this paper (although in principle they can be higher-dimensional; 
Kvam & Turner, 2021)
. The degree of support for a particular response is described by the match between the decision maker's state and a vector v defining the response. Formally, it is the component of the state vector s along a vector v describing the choice option, comp v (s). For example, suppose a decision maker is choosing among different orientations in an orientation-detection task, and has two options: choice option A is in direction v A = [1, 0] (0 degrees) and choice option B is in direction
v B = [ √ .7, √ .3].
If the accumulated evidence corresponds to a state s = [.5, .2], then the degree of support for option A is equal to comp v A (s) = .5, while the degree of support for option B is equal to comp v B (s) ≈ 0.53. Therefore, the evidence state slightly favors option B over option A.
The state s changes over time according to how well the stimulus matches each of the anchors and the overall rate at which the decision maker gathers information. Its dynamics are determined by the degree of activation relative to each of the anchors, with such activation bounded between 0 and 1. The activation values vary randomly from trial to trial according to a beta distribution, Beta(cz, c(1 − z)), where z is the match between the stimulus and the anchor, and c > 0 is the precision of the representation. A value of z = 1 indicates a perfect match between the stimulus and the anchor and z = 0 indicates a perfect mismatch. The different ways in which the match is calculated for line length stimuli and for hue stimuli are detailed below. Precision describes how consistently participants can discriminate between stimuli: larger values of c correspond to representations that are more precise (i.e., vary less from trial to trial) and hence support better discrimination. We call c a "precision coefficient" to reflect the idea that lower information variability is required to form more precise representations 
(Hick, 1952)
.
The model dynamics unfold as different anchors pull the state in different directions. The overall movement of the state is determined by the balance of activation across the anchors, taking into account the conflict in direction. Formally, the dynamics of the state are determined by a vector δ all that is the weighted sum over anchors of unit length vectors d i defining each anchor's direction. The weight for each anchor vector is determined by its degree of activation, given by the beta distribution:
δ all = δ ∑ i d i • Beta(cz i , c(1 − z i ))
(1)
The parameter δ > 0 scales the rate of evidence processing per unit time, and can be viewed as a measure of overall information processing or "channel" capacity, with larger values corresponding to a wider channel and thus faster information accumulation 
(Townsend & Wenger, 2004;
Eidels et al., 2010)
 and shorter response times.


Figure 2
Outline of the structure of the two-dimensional representation of the model (left) and its corresponding racingaccumulator representation (right). In the top left panel the black arrow labeled s is the stimulus vector and the coloured arrows are vectors representing different possible choice options. The top panel on the right shows the increase in support for each choice in corresponding accumulators (indicated by having the same color). Note that support for options more closely aligned with s increases more quickly. The effect of manipulating drift (δ, white = low vs. black = high) is shown in the middle panels and the effect of manipulating the precision coefficient (c, white = less precise, black = more precise) is shown in the bottom panels. Drift affects all racing "accumulators" equally, while precision affects the average disparity between the best "accumulator" and its competitors across trials.
The evidence state changes from its initial position at time 0, s(0), to its position at time t, as a function of δ all . For simplicity, we assume a deterministic accumulation process, as in the Linear Ballistic Accumulator model (LBA Brown & Heathcote, 2008) 1 .
s(t) = s(0) + t • δ all
(2)
Evidence accumulation stops when one of the response options exceeds a threshold level of support, as determined by the match between the state and the direction describing that response. Formally, evidence accumulation halts and re-sponse (option) j is chosen at time t if it is the first option that satisfies the condition comp v j (s(t)) > θ. Thus, response times are determined by the shortest time t at which this condition is met, and choice is governed by which option j meets it first. The value of θ controls how strict the stopping rule is: as in the LBA lower values of θ result in faster response times but lower accuracy, while higher values result in slower response times but greater accuracy.
The models that we use for our two studies follow this general specification, although there are differences in certain details of the decision process, such as the anchors and the factors that determine the threshold(s) θ. Thus the general model structure can be adapted to the details of specific task paradigms according to the demands that they place on participants.
An outline of the type of model that we used for the first task is shown in 
Figure 2
. The two anchors -long and short (corresponding to A and D in the figure) -drift the evidence accumulation process until one of the responses along the continuum between them accumulates sufficient support to be selected. For a discrete set of responses, this model can also be instantiated as a competition between multiple correlated accumulators 
(Kvam, 2019a;
. This representation is shown in 
Figure 2
 on the right. Readers familiar with competing accumulators can conceptualise drift as describing the average rate of evidence accumulation for all options, while precision is more similar to drift variability or the difference between the best and next-best accumulator(s) 
(Brown et al., 2009)
. However, we should caution these readers that as the number of options grows increasingly large, it becomes more difficult to instantiate the model as a competing-accumulator process and so we must defer to the model representation on the left side of 


Line length identification
The first study tested the connection between discrete and continuous responses in an experiment motivated by classical absolute judgment tasks 
(Miller, 1953)
. In absolute judgment, a participant assigns each of a finite number of unidimensional stimuli to a different experimenter-defined response; the responses usually have the same ordering as the stimuli. In our study, the stimuli are line lengths defined by the distance between two points on the screen. Participants were tasked with assigning the line length shown to either a) one of a finite number of discrete categories or b) a position on a continuous (radial) scale. As shown in 
Figure 3
, in order to make the motor requirements of discrete and continuous responding as similar as possible, the response in each condition was to move a cursor to the selected response location on a semi-circle. More details on the task are provided in the Methods section below; first, we introduce the structure of the model used to explain behavior on this task.


Line-length Model
The model for the line length study is a two-dimensional evidence accumulation process that starts at randomly chosen coordinates in the first quadrant s(0) = [s S , s L ], which define the initial degree of support relative to the short and long anchors, respectively, with s S > 0 and s L > 0. As a person samples information about the stimulus, this state moves in the x-direction toward 'short' responses at a continuous rate d S , and moves in the y-direction toward 'long' responses at a continuous rate d L . Accumulation terminates when evidence for any response between the shortest and longest stimulus exceeds θ. A diagram of the model is shown in 
Figure 1
. For simplicity of computation, this response-selection process can be approximated as a circular boundary specified as x 2 + y 2 = θ 2 (i.e., accumulation stops when it reaches radius θ from the origin). This results in miniscule differences relative to separate boundaries for each response option.
We assumed the position on the arc that a stimulus maps onto is linearly related to its length, although in general this mapping can be nonlinear (as in utility representations or nonlinear similarity between stimuli, see 
Kvam & Busemeyer, 2020;
Kvam & Turner, 2021)
. Note that participants are not being asked to map each stimulus length onto an arc of equal length. Rather, they are mapping each stimulus length to a position on the semicircle from 180 degrees to 0 degrees with the position determined by the stimulus set.
The angle coordinate at the time when the state hits the response boundary determines the response. However, for the purposes of modeling, where starting points and drift rates are drawn with respect to Cartesian coordinates, we lay out the behavior of the model in terms of (x, y) coordinates. The Cartesian coordinates can then be mapped back into polar coordinates to compute continuous response distributions given the hitting point [x resp , y resp ] equates to angle φ = tan −1 (y resp /x resp ). For the first model, the value of x resp and y resp will necessarily be positive, as the accumulation process starts in the first quadrant and accumulates in the positive x and y directions (i.e., no negative drift rates are allowed). For convenience in plotting fits of the theory, a response is calculated by first mapping its value onto [0, 1] by taking φ 01 = 2φ π , which is then transformed into a bounded line-length response R on a length scale [R min , R max ] as:
R = φ 01 * (R max − R min ) + R min
(3)
In the present study, the minimum of the scale is R min = 50 pixels and the maximum of the scale is R max = 500 pixels For the discrete-response condition, the values for R are sorted into categories with boundaries C = {C 1,2 ,C 2,3 , ...,C n−1,n }, where n is the number of categories. We assume unbiased discrete responses by placing the category boundaries evenly from R min to R max at C m,m+1 = m n (0 < m < n) (see the left panel of 
Figure 1)
.
For both the discrete and continuous cases response time is the sum of the time it takes to travel from the starting point (s S , s L ) to the point where the process hits the boundary θ, and a non-decision time τ, representing the sum of the times to encode the stimulus and produce a motor response.
We now present the detailed assumptions regarding the models starting points, drift rates, and thresholds.


Starting points
On each trial, the activation relative to each anchor begins with some activation drawn from independent uniform random variables, S S ∼ Uni f (0, s) for the short anchor and S L ∼ Uni f (0, s) for the long anchor. Start-point variability, s, is a free parameter governing the degree of anchor activation before any evidence is collected, indexing activation either due to response biases or leftover activation from previous decisions 
(Heathcote et al., 2019)
. This is a fairly typical distribution of starting points utilized in both accumulator 
Busemeyer, Gluth, et al., 2019)
 and diffusion 
(Ratcliff et al., 2016)
 models.
Independent random starting points are a simplifying assumption that could be modified. Starting points could be arranged proportional to all of the possible responses as opposed to the anchors (creating a circular start point distribution in the continuous condition, for example). Starting points could also be used to specify systematic response biases, or be specified to reflect sequential effects, such as a dependence on the previous responses, as in models of absolute identification . For example, a bias favoring the previously-chosen category on the immediately following trial might be implemented by fixing the ratio of short / long start points to that of the previously-chosen category. This would result in an assimilation effect like that found in absolute judgment paradigms where response times and accuracy are improved when subsequent trials feature stimuli from the same category 
(Ward & Lockhead, 1970)
. However, it is also possible that these sequential responses are the result of fluctuations in selective attention 
(Matthews & Stewart, 2009
) -in which case they might be better incorporated into drift rates and/or thresholds 
(Donkin, Brown, Heathcote, & Marley, 2009b;
Donkin et al., 2015;
Treisman & Williams, 1984)
. The model we present here is theoretically neutral with respect to which mechanism is responsible for sequential effects, as any of these three mechanisms (start point, drift, or threshold) could potentially change across tri-als. For the sake of simplicity and computational tractability, and because these effects are not particularly pronounced in the data from the task described below, we do not include mechanisms to model sequential effects here. However, for those readers interested in these effects, we include a section in the supplementary material that gives an overview of the sequential effects in our data.


Drift rates
The main index of the strength of a particular alternative in dynamic models is the drift rate. A typical model of discrete-responses / decisions might assign separate drift rates to each of the response options, and determine some covariance structure across the response options (the approach taken by 
Ratcliff, 2018)
. However, this becomes burdensome for the continuous case. A simpler way to represent the accumulation process is to reduce the large number of accumulators to a smaller number of dimensions and specify a drift rate for each dimension 
(Kvam & Turner, 2021)
. Each dimension can then be thought of as being represented by an accumulator. In the present case, there are only two dimensions -defined by the upper and lower anchors R max and R min -and so we need only two drift rates.
To specify the drift rates for a stimulus relative to the lower and upper anchors (i.e., for the x and y directions), we build on the double-anchor model from the absolute judgment literature, which gives the strength of activation relative to each of the anchors based on the length of a given stimulus 
(Marley & Cook, 1984
, 1986
. In this theory, the strength of activation relative to an anchor for a given stimulus is based on the distance between the stimulus, L, and the anchor relative to the range between the two anchors, R = R max − R min . In our model this activation is equivalent to the match, z, in Equation 1: for the long anchor z L = (L − R min )/R, and for the short anchor z S = (R max − L)/R. Formally, the drift rates for short accumulator (δ S ) and long the accumulator (δ L ) are independent samples from Beta distributions that are both scaled by the channel capacity, δ, and with an overall level of trial-to-trial variability in rates determined by c:
v L = δ • Beta c • z L , c • (1 − z L ) + b (4) v S = δ • Beta c • z S + b, c • (1 − z S )
(5)
The b parameter controls a bias set before the trial commences to encode the stimulus as either short or long. It increases the drift rate for one response (e.g., short) over the other (e.g., long), reflecting a tendency to encode the properties of a perceptual stimulus according to the participant's bias. Formally, the way it is included in the model results in adjusting the relative activation of the short and long anchors, reflecting participants' "stimulus bias" 
(White & Poldrack, 2014)
 to respond toward one end of the scale or the other. Larger values of b increase the drift rate of the short anchor relative to the long anchor, and so increase responses at the short end of the scale. Smaller values of b, conversely, increase the drift rate of the long anchor relative to the short anchor and results in more and faster responses at the long end of the scale. The former is a bias that was displayed by our participants, as we report below. This is only one of several possible ways to instantiate a stimulus bias. For example, it might be alternatively achieved by shifting the values of stimulus length that goes into calculating z. We believe that these different implementations of bias would be difficult to tell apart in the present data, but might be discriminated by a manipulation of the stimulus range. We leave exploration of this issue to future research.


Response Time and Response Selection
The final ingredient is a rule that terminates the race between the accumulators associated with each anchor and selects a response. There have been a number of proposals for how response boundaries can be used to map a small number of accumulators onto a greater number of responses, which we examine in supplementary materials. Here we focus on the circular boundary model shown in 
Figure 1
, which corresponds to the asymptotic limit of both the multiple threshold race 
Reynolds, Garton, et al., 2020)
 and the geometric framework for modeling decision making 
(Kvam, 2019a;
Kvam & Turner, 2021)
. A single boundary allows discrete and continuous cases to use a commensurate stopping rule, as with the circular diffusion model 
(Smith & Corbett, 2019;
Smith, 2019)
. However, this assumption must be abandoned when we move to stimuli that are unequally spaced, as in our second experiment, because the similarity between options is a critical determining factor in how much evidence is needed to make a decision.
With all of these pieces specified, we can put together the full model, a process that halts whenever there is sufficient evidence for any alternative at distance θ from the originthat is, when the length of the state (i.e., the norm of the state vector s) exceeds θ: ||s|| ≥ θ.
This results in an extremely simple stopping rule, as all of the tractability of the circular boundary and linear accumulators translates into analytic equations for stopping locations and stopping times. For a state that moves around the (x,y) plane, we simply have to compute the location s f inal at which it hits the circular boundary in order to find the predicted response. The accumulation time -that is, the amount of time it takes to transit from the starting point to the hitting point -is given by taking the distance D
= ||s f inal − s initial || (where s initial = [s S , s L ]
is the initial state) and dividing by the accumulation rate. This gives the decision component of a response time, with overall response time, RT , given by:
RT = D v 2 S + v 2 L + τ. (6)


Predictions
The model we have developed follows in the tradition of models of absolute identification 
(Marley & Cook, 1984;
Luce et al., 1982;
Marley & Cook, 1986)
, expanding their purview and predictions while accounting for fundamental phenomena. Among the most important effects in absolute judgment are "bow" effects, where discrete responses to stimuli near the ends of a scale tend to enjoy both faster and more accurate responses than middle categories 
(Luce et al., 1982)
. Responding also typically becomes slower and less accurate as the number of responses increases, although at the ends of the stimulus range this effect is more evident in response time. These "set-size" effects are typically thought to reflect a limit on performance in terms of the amount of information transmitted 
(Miller, 1956)
. The lengthening response times and shrinking accuracy toward the center of the scale have previously been attributed to perceptual and memory processes 
(Guest et al., 2018)
 or to perceptual processes alone 
(Lamberts, 2000)
 -we test these proposals by leaving the stimulus on-screen in our experiments, to focus on perceptual and response processes. Although in typical absolute identification paradigms the number of stimuli and responses are the same, 
Lacouture et al. (1998)
 found that almost all of the set-size effect, and much of the bow effect, are due to the number of responses rather than number of stimuli when the two factors are manipulated separately.
We now examine MAAT's predictions for bow and setsize effects on accuracy and response time for both discrete and continuous responses. We also examine predictions for response deviations, which provide an inherently graded metric that tracks the difference between where a participant should and does respond. Response deviation is, therefore, a measure that is richer than simple correct/incorrect -it tells us the error direction, and in the case of multiple responses, which incorrect response is given.
In order to understand MAAT's predictions for response time it is important to understand that although the distance to the boundary is the same for every response, the overall
rate of accumulation, v 2 S + v 2 L ,
is not. This is because the effective threshold for one accumulator depends on the value of the other accumulator. For example, say the long accumulator has no evidence at time t = 1 then the short accumulator has to have a value of θ to trigger a response at the short end of the scale at that time. In contrast, if the long accumulator has a value of θ/ √ 2 at t = 1 then the short accumulator need only have the same value at that time to trigger a response in the middle of the scale. For the short response δ = θ whereas for the middle response δ = √ 2θ, so that the overall rate required to trigger a response at the same time is higher for responses in the middle of the scale than responses at the ends of the scale. Given rates for each accumulator are sampled independently, this means the model predicts a "bow" effect, with the average time to respond being faster at the edges of the response scale than in the middle.
In addition to an inverted U (i.e., downward) bow effect in response times, the model also predicts an upward bow effect in accuracy, where responses in the the middle of the scale are less consistently correct than responses at the ends of the scale. This occurs because the trial-to-trial variance of the drift rates is greatest when stimuli are toward the middle of the scale. The variance of a beta random variable,
Beta(a, b), is (a • b)/ (a + b) 2 • (a + b + 1) . In our case, a = c • z and b = c • (1 − z) and so a + b = c(z + 1 − z) = c.
Hence, the only part of the Beta variance that changes with the stimulus,
L, is a • b = c 2 • (L − R min ) • (R max − L)/R 2 ,
which has a maximum of (c/2) 2 when L is in the middle of the scale. As a result, we can expect the lowest accuracy in the middle of the scale, and expect it to monotonically increase as the stimulus moves toward either the long or short end of the scale. For both accuracy and response time the bow effects are symmetric when there is no response bias. However, bias (e.g., positive values of MAAT's b parameter cause a bias towards short responses) will cause both faster and more accurate responding for the favored end, and hence an asymmetric bow.
Like almost any reasonable model, MAAT predicts that accuracy decreases as set size (N) increases. This prediction arises because the base rate of accuracy decreases as 1/N. However, MAAT does not predict the large effects of set size on response time that are found in absolute identification experiments. This implies that either response threshold must increase or accumulation rates decrease as set size increases. Given 
Lacouture et al.'s (1998)
 finding that response rather than stimulus set size is the main driver of set size effects, an increase in the threshold appears most likely. If this is the case, MAAT predicts that there will be no differences in response time as a function of stimulus set size in the continuous condition.
MAAT also predicts an interaction that is typically observed between set size and bow effects in absolute identification: namely, faster responding for extreme categories as set size decreases. This pattern is not captured by Lacouture & 
Marley's (1995)
 mapping model of absolute identification but is by  SAMBA model. The corresponding interaction in accuracy -less errors for extreme response categories -is also observed, although it is often smaller than the response time interaction 
(Stewart et al., 2005)
, and may even be largely absent 
(Lacouture et al., 1998)
. Both the accuracy and response time interactions can be modulated by response bias effects, reducing set size effects at the favored end of the scale.
Response deviations cannot be examined in traditional absolute identification tasks as different responses are assigned to different buttons rather than different positions on a continuum. In MAAT, the magnitude of a response is a linear function of the accumulation angle, φ (Equation 3). This angle equals the inverse-tangent transformation of the ratio of the Beta distributed long and short accumulator rates, v L /v S . The transform places bounds at 0 o and 90 o , causing the distribution of φ to be positively skewed when centred around small angles, symmetric when centred around 45 o , and negatively skewed when centred around larger angles, at least when the Beta distribution variance is moderate. In both experiments the latter condition held, as distributions of response magnitudes had a single peak around the true value. Hence, MAAT predicts that response magnitude distributions should be skewed towards the centre of the scale for small and large stimuli, and symmetrically distributed for middle stimuli.
We used mean absolute deviations in response magnitudes as a robust way to to summarize bow and set-size effects on variability 2 . MAAT predicts bow effects on mean absolute deviations in the same upward direction as for response time (i.e., greater mean deviations towards the centre of the response scale) for the same reason it predicts skew effectsbecause of the bounded response range with anchors at either end. It does not, however, predict set-size effects unless precision (c) increases with set size, in which case variability will increase with set size. Such set-size effects in c may be plausible under an information-theoretic view 
(Hick, 1952)
 where precision decreases because limited information must be shared among more representations.
Response deviations also allow us to examine the degree to which any set-size effects on accuracy in the continuous condition are a result of dichotomizing responses as correct vs. incorrect. Van Maanen et al. 
2012
suggested that reduced accuracy is due to "crowding" caused by the increased similarity that necessarily occurs when set size is increased while holding the stimulus and response ranges constant, as was the case in our experiments. If mean absolute deviations are unaffected by set size, then any effects on accuracy in the continuous condition are solely due to crowding. However, if mean absolute deviations increase with set size, then the corresponding increase in overlap between adjacent response distributions will also play a role.


Study 1: Line Length
We initially report the results of our first experiment in a descriptive manner, and then evaluate the ability of MAAT to fit the data. Our emphasis is on developing a relatively simple model that describes the main features of the data rather than a more complex model that captures smaller details. For example, better fits could be obtained in the discrete condition by uneven spacing of category boundaries, and that may be appropriate in some circumstance. However, assuming equal spacing here makes comparisons of discrete and continuous responding more transparent.


Methods
A total of 6 participants from the University of Tasmania and 31 participants from the University of Newcastle took part in the experiment. These participants were paid AU$10 for participating, plus an additional $5 if they accumulated sufficient points in the experiment. The points bonus was designed to be easy to achieve if the participants were putting serious effort into the experiment (i.e., not simply guessing), and so we included all participants who achieved this payoff in the analyses. A total of 6 participants were dropped from analyses for failing to achieve this criterion, resulting in 31 remaining participants whose data were analyzed. We describe below how they earned points on the task.
Each participant completed 120 trials of training and 504 trials of the main task, consisting of 12 blocks of 10 practice trials and 42 experimental trials (i.e., 52 total trials per block). These 12 blocks were divided evenly among each of the 6 conditions 2 (continuous vs. discrete) × 3 (3/6/9 stimuli) so that each participant saw each condition exactly twice. In the discrete-response conditions the number of responses was the same as the number of stimuli. In both conditions stimuli were evenly spaced along the response scale (e.g., at 125, 275 and 425 pixels for 3 stimulus types). The range of the stimuli was 50-500 pixel, which was held constant across all conditions of the study. These blocks were randomly shuffled for each participant. This level of practice (results for which were not further analyzed) was employed to enable participants to adjust to the substantially different response requirements among conditions.
This study was not preregistered. Study materials including data, analysis code, and additional figures can be found on the Open Science Framework at osf.io/6d29q 
(Kvam & Heathcote, 2022)
.
Task A diagram of the task participants were asked to perform is shown in 
Figure 3
. The length of the stimulus was given by two horizontally aligned dots to avoid differences in overall screen brightness from full line segments that vary in length, as this would result in two-dimensional stimuli that vary in both length and brightness. Across trials, each stimulus category could result in one of three stimuli: a stimulus that was centered on the screen (so that its left and right points were equidistant from the center), a stimulus that was shifted slightly to the right, or a stimulus that was shifted slightly to the left. This was done so that participants could not rely on just one of the two points comprising the stimulus -if it were always centered, then participants could judge the distance of a single point from the center rather than judging the distance between the two points.


Figure 3
Diagram of practice trials (top) and experimental trials (bottom) for discrete (left) and continuous (right) scales. Blue dots indicate the stimulus, orange dots indicate the response corresponding to the mouse position in practice trials. For experimental trials (bottom), response times were recorded when the cursor moved outside the dotted circle and response location was determined by where the cursor crossed the scale semicircle. Mouse trajectories are shown in grey. For practice trials (top), responses were entered by clicking on the scale semicircle rather than simply crossing it in order to allow participants to match the response to the stimulus.
In the practice trials, participants could mouse over different response locations -numbers in the discrete condition or anywhere along the scale in the continuous condition -in order to match their response to the stimulus on the screen. This is shown in the top panels of 
Figure 3
: the blue dots represent the stimulus and the orange dots represent the response corresponding to the location of the mouse. Participants would confirm their selection in practice trials by clicking the mouse on the chosen response location. The ten practice trials preceding every full block gave them the opportunity to understand the scale they were about to use -whether that was continuous, or discrete with 3, 6, or 9 options.
In the experimental trials, the participant's task was to match the length of the stimulus with the location on the scale that they had learned was associated with the given stimulus. In the discrete condition, this was done by assigning it to one of the numbered categories (bottom left of 
Figure 3
), similar to traditional absolute judgment tasks with the exception of the responses being locations on the screen rather than physical buttons. In the continuous condition it was accomplished by assigning the stimulus to a position on the scale that corresponded linearly to the length of the stimulus (bottom right panel of 
Figure 3
).
Participants received 10 points per trial for a correct response, made by moving their mouse across the semicircular response scale within 1 o of the middle of the appropriate category (marked by a numeral) in the discrete condition, or within 1 o of the location corresponding to the stimulus length in the continuous condition. In all conditions, they lost a point for every degree away from the correct location, meaning that they would receive points as long as they deviated by less than 10 o . This range was chosen to match the width of the categories in the most difficult condition (9 categories), where each category corresponded to a 20 o arc on the scale. This ensured that the motor challenge of the task was constant across conditions, although it was still somewhat easier in the discrete condition because there was a number at the middle of the category that participants could use as a target.
Response times were recorded as soon as the mouse cursor moved outside the semi-circle (10 pixels away from the central fixation point), so that the time it took to reach the edge of the scale was not counted in the response times. This was done to minimize the impact of nonlinear trajectories that often accompany responses on the radial scale we used (see, e.g., the trajectories in 
Kvam & Busemeyer, 2020)
. While these trajectories can certainly be informative for understanding the evolution of the decision state over time (see 
Lepora & Pezzulo, 2015;
Friedman et al., 2013;
Dotan et al., 2018;
Koop & Johnson, 2011)
, we focus here on accounting for the final responses themselves and the associated response times. Forcing participants to make ballistic movements, and cutting out the time it took them to reach the scale, provided better control over response times and reduced the covariance between response location and response time. Responses were recorded as the angle of the cursor relative to the center when it crossed the response scale. To encourage participants to move the mouse directly from the center to the location of their desired response, they received an error message anytime their cursor spent more than 300ms between the starting circle and the semicircular boundary, and did not receive any points for these trials. Any trials on which this error message was triggered, as well as any on which participants responded too quickly (< 0.25 seconds) or too slowly (> 5 sseconds), were removed prior to data analysis ( 4.01% of responses).


Descriptive Results
For both discrete and continuous responses, we scored accuracy in terms of the proportion of responses that fell within equal-width adjoining ranges around the stimulus values (e.g., , and 350-500 pixels for three stimuli). We also analyzed response times and response devia-tions in both discrete and continuous conditions. Response times are the time to move the mouse from the center to the edge of the semi-circle. Response deviations are the raw difference between the response a participant gave and the response they were supposed to give, whether that was the center of a stimulus category (discrete condition) or the actual position of the stimulus length on the response scale (continuous condition).
As expected, there were bow effects for discrete responses, which were more accurate and faster toward the edges of the scale, and the same was true for the continuous conditions, as shown on the left and right of 
Figure 4
. The discrete condition also produced the expected set-size effects, with overall accuracy and speed being less in conditions with more responses. However, for set-size 6 and 9 there is also a strong bias favoring the short end of the scale and and associated asymmetry in the bows. As a result there was virtually no set-size effect in either speed or accuracy for the shortest response and an exaggerated set-size effect in both measures for longer responses. For the set-size 3 condition, in contrast, the response-time bow is quite symmetric and the accuracy bow, if anything, favors longer responses.
There was a similar pattern in accuracy for the continuous condition, except that it was a little higher for the shortest condition, the bows slightly deeper, and there was a small bias favoring shorter responses for the smallest set size. Participants were slightly less accurate in the continuous than the discrete condition, particularly in the middle stimulus categories, which is likely due to the presence of number labels on the scale in the discrete condition that were not present along the continuous scale, as shown in 
Figure 3
. Mean response time in the continuous condition exhibited a bowed pattern that was biased toward short responses, with the asymmetry being present even for the smallest set size. However, in contrast to the discrete condition, there was no overall set-size effect on response time.
Asymmetric and short-biased bow effects also appeared in mean absolute response deviations, with the exception of the set-size 3 discrete condition where deviations were unusually high. Importantly, there was no set-size effect in the continuous condition for either discrete or continuous responses, as illustrated by the overlapping lines in the bottom panels of 
Figure 4
. This does not support the idea that the ability to represent stimuli precisely is subject to capacity constraints, and hence that a model in which precision (c) does not change with set size is likely to fit this data well. It also suggests that the accuracy effects in the continuous condition are due to crowding. In combination with the lack of set-size effects on response time, this suggests that the same response threshold was used for all continuous conditions. In contrast, the response time effects in the discrete condition suggest that participants used larger response thresholds for larger set sizes.


Figure 4
Mean accuracy (top row, proportion correct), response times (middle row, seconds), and response deviations (bottom row, pixels) across conditions of the task. Error bars indicate ±1 unit of standard error with lines passing through the means. The x-axis indicates the length of the lines, divided into 3 (darkest / solid lines), 6 (medium / dashed lines), and 9 (lightest / dotted lines) line length conditions


Modeling Results
The model had 9 parameters per participant (see 
Table 1
). In line with our aim to produce a simple unified model and the empirical results just reviewed, only threshold parameters differed between discrete and continuous conditions. Only one value of non-decision time, τ, was estimated on the assumption that stimulus encoding and motor production was the same for all conditions. Similarly, the same value of start-point variability, s, was estimated for short and long accumulators, so this parameter played no role in explaining response bias. In light of the lack of effect of set size on mean absolute deviations in the continuous condition, capacity, c, was assumed to be unaffected by set size. Two further parameters completed the specification of drift rates, δ, which controls their overall magnitude, and b, which controls stimulus bias. The remaining four parameters were all thresholds, one for each set size in the discrete condition (θ 3 , θ 6 , and θ 9 ) and a single threshold for all of the continuous conditions.
A participant had 84 trials in each condition, distributed uniformly across stimulus lengths. However, this does not result in a large number of trials in a category, especially when there were 9 response categories and participants tended not to respond as often in categories 3, 4, 6, and 7 as in the center and edge categories. To address associated issues with measurement error, we used hierarchical Bayesian estimation to fit the model, allowing the number of participants to compensate in part for fewer responses in each category within each participant by sharing parameter-relevant information across participants. Each parameter, with the exception of bias, was restricted to the positive reals, and we use relatively uninformative and independent hyperpriors on the group-level parameter estimates. All parameters were constrained by a group-level distribution. Specifically, for the drift, capacity, threshold, and non-decision time parameters, the group-level prior was a very wide Gamma distribution, Gamma(.001, .001) (in a shape, rate parameterization). The start point variability parameter was set as a proportion of the θ 3 parameter (typically the lowest of the four thresholds) to avoid start points going above the threshold, using a group-level truncated normal distribution with hyperpriors µ Uni f orm(0, .99) and σ Gamma(.001, .001) (again in shape, rate parameterization). We fit the model to accuracy and response time data in both discrete and continuous conditions. Response deviation data was used in a cross-validation exercise to test whether the model makes good predictions for the continuous conditions. Model fit was quantified using a multinomial likelihood for responses (number of responses in each category) and the probability density of the associated response times.
Hierarchical Bayesian estimation was carried out in JAGS 
(Plummer, 2003)
 with a MATLAB interface, matjags 
(Steyvers, 2011)
. Each of the parameters shown in 
Table  1
 was estimated for each individual simultaneously, constraining the individual-level estimates with a group-level prior as described above. In total, this resulted in estimates for 9 parameters × 31 participants, plus 18 (9 group-level distributions× 2 hyperpriors) group-level parameters to fit N = 15,575 responses and associated response times. The JAGS code for fitting this model (and a version that can be used to fit individual participants) is on the Open Science Framework at osf.io/6d29q.
The model fit the general patterns in the average accuracy and response time data, as shown by the lines in 
Figure 5
. It was able to accommodate the bow effects in accuracy and response times even through only thresholds differed as a function of number of responses. There was clear overestimation of accuracy in the second category of the set-size 3 continuous condition. Our best explanation for this result, and the reason that accuracy differed so much in this case between


Figure 5
Data (Xs) and model predictions (lines, colored and dashed as in 
Figure 4
) for the mean response times, accuracy, and mean response deviations across the conditions of the experiment. Bars indicate the 95% Highest Density Intervals for the predicted mean based on simulations generated from the model, where the model produced one predicted response for every response in the data.
discrete and continuous conditions, is that there was no number in the continuous condition that allowed participants to match exactly the location of the middle of the scale. This may have resulted in poor accuracy in the continuous condition, where participants appear not have known that there were only three stimulus categories that could appear, and thus made responses further from the middle of the scale. Because there was only one threshold for the continuous condition, the model predicted entirely overlapping mean response times for these conditions (middle right panel), a prediction that was largely reflected in the data as shown in 
Figure 5
.
The slowing captured by the model in response time with set size in the discrete condition (middle left panel) is due entirely to the difference in thresholds (θ 3/6/9 ). For accuracy and response times in both discrete and continuous condi- tions there is a clear asymmetry in the bow effects, where accuracy is higher and response times are faster at the short end of the scale. The asymmetry is well described by the addition of the single b parameter to the model, but it is not clear whether this will be a necessary ingredient to account for behavior in other mapping tasks. One possibility is that it is related to using a mouse to respond, as lateral biases in responses can sometimes be eliminated when a joystick is used (see 
Busemeyer, Kvam, & Pleskac, 2019)
.
A finer grained test of thresholds mediating set-size effects is provided by considering the fit to the entire distribution of response times, as thresholds have different effects on distribution shape than the other parameters such as drift rates or non-decision time 
(Ratcliff & Smith, 2004)
. 
Figure 6
 shows the fits for the discrete condition (fits of the continuous condition are provided in supplementary materials, and are of similar quality). Consistent with a threshold effect, set-size conditions differ mainly in the leading edge and variability. Consistent with differences in drift rate, both variability and skew were increased toward the middle of the response scale. 
Table 1
 shows the group-level mean parameter estimates, illustrating the central tendency of each one across individuals. Note that the δ, c, and b parameters combine to determine the drift rates for the short (v S ) and long (v L ) accumulators. The 95% credible intervals that accompany each parameter show that they were precisely estimated, consistent with the model's simple structure. The model is also about as simple as it can be without serious misfit: when we tried to simplify the model further by removing start-point vari-ability, accuracy in the set-size 3 condition was drastically over estimated. The discrete-condition threshold parameters showed an increase in magnitude with set size, and consistent with empirical results the difference between set sizes 3 and 6 was greater than that between 6 and 9. The overall increase suggests that participants responded more cautiously as set size increased in order to ameliorate the associated decrease in baseline accuracy. In agreement with this idea, the threshold for the continuous condition was very similar to that for the middle set size in the discrete condition. Consistent with the greater motor demands of using a mouse and thus greater motor preparation time 
(Fitts, 1954)
, the non-decision time estimate was greater than typically found in paradigms with a button-press response.


Out of sample predictions
The model was fit to response time and accuracy (i.e., whether each response was within 10 pixels of the correct value) data. However, in the continuous condition a response is not simply correct or incorrect, it also has a magnitude, distributions which are shown in 
Figure 7
 as histograms. Although the model was not fit to this data, we performed a type of cross-validation test to determine how well it could predict it. To do so we generated a single simulated trial for each trial completed by each individual based on the model's maximum a posteriori parameters, estimated by passing a kernel density estimator over the individual-level parameter samples and linearly interpolating the maximum height for each participant. This allowed us to equitably represent the predicted data and the relative influence of each participant (and their corresponding model parameters) in the simulated data, with the resulting predicted distributions shown as densities in 
Figure 7
. 
Figure 7
 shows that the observed distributions of response magnitudes had the pattern of skewness predicted by MAAT: for each stimulus below (resp., above) the middle category, the response distribution exhibits exhibits right (resp. left) skew. Furthermore, the skew is more extreme as the stimulus gets closer to the edges of the response scale. This skew is well accounted-for by the model, which produced even the strong skew that was characteristic of stimuli in the minimum (1) and maximum (3/6/9) categories. It can do so because of the beta distributions used to characterize the activation of the anchors. A strong activation for one anchor produces a beta-distributed drift rate that is concentrated very close to 0 or 1, which is then scaled by the drift parameter δ. As a result, there are mainly large values, with occasional low values, for the drift of the strongly favored anchor and so the drift rate distribution itself exhibits skew when stimuli are close to either end of the scale.


Figure 7
Distribution of responses magnitudes in the continuous condition (histograms) and model prediction based on fits to accuracy and response time data (lines). Light grey vertical lines mark category boundaries. The histograms show aggregate data (collapsed across participants) from each condition, while the model predictions are a weighted average of the probability densities generated from simulated data for each participant. To generate the model predictions, we created an artificial data set matching the exact composition of the real sample. For example, if there were 50 trials from Participant 1 in the real data, we simulated 50 trials from Participant 1's maximum a posteriori parameters. Once simulated data was generated for every participant, we passed a kernel density estimator over it to approximate the density of responses (lines).


Discussion
The key take-away from the modeling of the line-length experiment is that the same fundamental mechanisms can support both discrete and continuous responding. We were able to produce all of the most important phenomena in accuracy, response time, and response magnitude distributions (deviations) in both cases by varying only the response thresholds. In the discrete condition, slowing with increased set size was accounted for by increasing thresholds. Thresholds are typically thought of as being set in a strategic but slow manner, which is consistent with set-size differences in the discrete condition being evident to participants from the display (see 
Figure 3)
. The 10+ practice trials with each new display would easily allow participants to make the threshold adjustment. It appears that participants set higher thresholds in an attempt to ameliorate the decreased accuracy associated with larger set sizes. A separate threshold accounted for the continuous condition, which again seems reasonable given that it also had a distinctive display with corresponding practice trials. Although it is possible that participants might notice that different stimuli were used in different blocks of the continuous condition and adjust their thresholds, this does not seem to have been the case as quite good fits were obtained with the same threshold for all stimulus set sizes.
Our modeling results also indicated that the decrease in accuracy with increasing set size was due to increasing similarity between adjacent response options 
(Van Maanen et al., 2012)
. Increasing the number of response options while keeping the same span of stimuli constant as we did in the present experiment naturally means that adjacent stimuli are closer, and hence more similar, to each other. Therefore, the associated decrease in accuracy, which at least in the continuous conditions was not modulated by threshold differences according to our model, can be attributed to a "crowding" effect. However, set size might also fundamentally alter how the response alternatives are represented, as suggested by information theoretic accounts of multi-alternative choice 
(Hick, 1952)
. MAAT could allow for this possibility by letting the precision coefficient, c, which controls the precision with which response alternatives are represented, vary with set size. However, this was not necessary to obtain good fits, and is inconsistent with empirical findings about the precision of response magnitudes, which did not differ with set size. Rather, there seems to be a single representation of a stimulus that is common across different response conditions. However, the present design was limited in that the number of response options and similarity are confounded. In the next study we varied similarity of response options within each set size.


Hue identification
Responses on the hue wheel are a prevalent methodology in visual working memory tasks 
(Zhang & Luck, 2009
, 2011
 and have driven the development of continuous-response models 
(Smith, 2016;
Ratcliff, 2018;
Smith et al., 2020)
. It is particularly notable in these tasks that participants' responses tend group near 'cardinal' hues -red, green, yel-low, blue, magenta, and cyan -even when the stimuli are uniformly distributed across the hue wheel. As such, our model ought to be able to account for the concentration of responses around these locations in the continuous response task while simultaneously describing accuracy and response times in both discrete and continuous conditions.
We obtained the grouping effect empirically, and used it to deepen our theory of the connection between discrete and continuous responses, using a hue identification task. In this task, an array of differently colored dots was shown on the screen and participants were asked to assess which color is most common. Saturation was set to 1.0 and the value was set to 0.8 in HSV color space, so this discrimination was based only on hue. In the discrete-response condition, a fixed set of response options was available in each block of trials, and so participants had to determine which of these possibilities was most consistent with the stimulus. In the continuous condition, they had to make a response on the hue wheel. 
Figure 12
 illustrates the identification displays for continuous and discrete conditions, which were run within subjects. Each participant worked with a wide variety of hue combinations in order to provide a rich and highly constraining data set. In order to model each person's behavior individually there were few participants who all had high-quality data (i.e., each performed a large number of trials 
Smith & Little, 2018)
. As in Experiment 1, before blocks of trials in each condition participants were given ample practice in order to adjust to each new response configuration. Details of stimulus and display construction are given in the methods section below. Here we provide an overview in order to set the stage our modeling choices, which are described in the next subsection.
The entire hue wheel was used for responding in the continuous condition. The stimulus to be judged consisted of a cloud of dots centred on the middle of the hue wheel. Each display had dots of 16 different hues. Half of the 16 were the same on all trials within a block, and half differed from trial to trial. On each trial, a dominant hue was chosen from the set of 8 constant hues. The dominant hue occurred more often than the remaining hues, which occurred equally often. The participant's task was to identify the dominant hue. Each block in the discrete condition had either N = 2, 3, 5 or 8 responses displayed as disconnected arcs. Stimuli were constructed in the same way as for the continuous condition, except the dominant hue was randomly chosen from the response hues. Response hues were randomly chosen from the constant set of 8 at the start of the block and were the same for all trials within a block.
For smaller set sizes, Hick's Law often provides a good account of the increase in mean response time with set size, at least when set size is not confounded with similarity. For example, van Ravenzwaaij et al. (2019) modeled data from Van Maanen et al.'s (2012) experiment where participants identified 3, 5, 7, or 9 movement directions that were equally spaced (i.e., the range of directions increased with set size). The data followed Hick's Law, a pattern that was fit by van Ravenzwaaij et al.'s (2019) ALBA (Advantage Linear Ballistic Accumulator) model even when the same threshold was used for each response and set size. However, differences in thresholds with set size were required to provide a finegrained account of set-size effects on accuracy. 
Usher et al.'s (2002)
 LCA (Leaky Competitive Accumulator) model also predicts Hicks Law in the broad, but also best fits the finegrained pattern of data when thresholds are allowed to vary with set size.
In our design the average similarity among responses increases with set size because the range of possible response hues remains fixed. If responding slows with similarity, as is commonly observed with other manipulations that increase difficulty, response time should increase more quickly than predicted by Hick's Law. This deviation from Hick's Law might be accommodated in models such as the ALBA and LCA by an appropriate adjustment of thresholds with similarity. However, it is more difficult to see how these racing accumulator models would deal with the continuous case if they continue to follow Hick's Law as the number of accumulators is increased because in the limit of large set sizes this predicts response time will increase without bound, which is clearly unreasonable.
On the other hand, contemporary models built for continuous responses often assume a single threshold that is the same for all responses 
(Smith, 2016;
Smith et al., 2020;
Ratcliff, 2018)
. Such single threshold models may have trouble dealing with the unequally spaced responses, and hence variations in the similarity among responses, that occur in the discrete condition of our design. For example, in our set-size 3 condition, suppose there are two similar response options (e.g., pink and orange) and one dissimilar response option (e.g., cyan, see lower panel of 
Figure 8)
. A cyan stimulus is distinct from the other response alternatives, suggesting that participants may exercise less caution (i.e., use a lower threshold) for responding with the cyan option and enjoy greater accuracy when a cyan stimulus is presented. Conversely, they would experience greater difficulty when the stimulus is pink or orange and use a higher threshold in an attempt to compensate.
In other paradigms where difficulty varies among responses, participants sometimes adjust their decision thresholds, trading speed for accuracy in an attempt to compensate for the differences in difficulty. In our unified MAAT account of continuous and discrete tasks, we propose that thresholds are adjusted in two ways to provide a detailed account of set size and similarity effects in the hue task. In the discrete task, we propose a set of separate thresholds for each option, where the average threshold changes with set size and individual thresholds change based on their similarity to other


Figure 8
Diagram of the model used for the color task. Stimuli are quantified in terms of drift rates for red (v R ), green (v G ), and blue (v B ) anchors. These drift rate components are then combined into an overall drift rate vector δ all that characterizes the evidence accumulation process.


options.
In the continuous condition, in contrast, thresholds do not change systematically with these factors, but can vary across trials, representing fluctuations in control, attention, perceived difficulty, or as a response to recent errors 
(Frank et al., 2005;
Logan et al., 2014)
 We describe the mechanism by which this threshold change occurs, as well as the other details of the model, in the next section.


Hue Model
The color task model uses a trichromatic representation where responses are based on the amounts of red, green, and blue in the stimulus. This method of representing stimuli is based on the three (red, green, and blue) cone receptors in human color vision 
(Boynton, 1979;
Schnapf et al., 1987)
.
These three colors serve as the anchors in the MAAT model of Study 2, reflecting the idea that participants should have well-established exemplars for the colors red, green, and blue. The relative hue values, specified as R = red, G = green. and B = blue with each quantified on a 0 (hue not present) to 1 (hue at maximum) scale, drive three evidence accumulation process that anchor responses around these three hues (the large arrows in 
Figure 8)
.
As in the line length model where the long and short accumulators started with some activation, we assume that each of these new anchors (R,G,B) starts with a random degree of activation. The activation of each anchor is drawn as an independent uniform random variable on [0, 1]. The maximum value of this uniform distribution is fixed at 1 to set the scale of the model as in typical evidence accumulation models 
(Ratcliff et al., 2016;
Busemeyer, Gluth, et al., 2019;
. Without such a scaling constraint, drift, threshold, and start points cannot be separately identified (i.e., different values can the same distribution of responses and response times, see 
Donkin, Brown, & Heathcote, 2009)
. This variability is shown as the shaded region in 
Figure 8
.


Drift rates
The rates for the three red, green, and blue anchors are specified using the corresponding relative hue (i.e., R, G, and B) values. Each of the drift rates is set according to a normal distribution:
3 v R = δ • Normal(R, R(1 − R)/c)) v G = δ • Normal(G, G(1 − G)/c) v B = δ • Normal(B, B(1 − B)/c) (7)
The drift rate variance is set according to the uncertainty of a binomial random variable on [0, 1], which mimics the way in which variance changes in a Beta distribution (see Predictions section) by having the greatest variance when R, G, and B is close to 0.5 and the lowest variance when R, G, or B is close to 0 or 1. As with the line length model, drift rates are scaled based on the overall information sampling rate δ. The free parameter c corresponds to the precision coefficient for the system, in this case scaling the variance in R, G, and B accumulators as shown in the Normal distributions in Equation 7. The hue match values are analogous to short and long match for line length, being on the unit interval. They are also constrained to sum to 1 in this case because we used a hue wheel. In a hue wheel saturation and value in hue-saturation-value color space are constant, so a balance between colours is maintained with a constant sum of R + G + B (see 
Figure 8)
. As a result, the amount of red in the options, for example, was a direct inverse of the amount of green plus blue. This is a simplified account of color perception; in the future more nuanced theories of color vision could be introduced (including but not limited to tetrachromacy in humans and other animals-see 
Jameson et al., 2020)
.
The three drift rate scalars are combined to form the overall drift vector by multiplying each one by the direction in which the corresponding color is located. Red responses are in direction d R = [0, 1] (i.e., at 12 o'clock). Green responses are 120 degrees clockwise from red (i.e., at 4 o'clock) in direction
d G = [ √ 3
2 , − 1 2 ], and blue responses are 120 degrees counter-clockwise from red (i.e, at 8 o'clock) in direction
d B = [ − √ 3 2 , − 1 2 ].
The overall drift vector δ all , where v i , i=R,G,B, is given by Equation 7, is:
δ all = v R d R + v G d G + v B d B
(8)
The combined drift rate δ all describes the overall effect of the stimulus as a two-dimensional vector. The direction in which it points indicates the response that the stimulus favors, while its magnitude represents the rate of accumulation toward that response. 
Figure 9
 illustrates drift vectors for different hue stimuli and capacities.
Drift rates are skewed toward the red, green, and blue, just as responses were skewed toward the upper and lower anchors in Study 1. This produces peaks in responding around red, green and blue. Secondary peaks also occur around cyan, magenta and yellow (CMY, as shown in 
Figure 9
) because these are local maxima of the drift vectors relative to the circular threshold: areas where the value of R+G+B = 1 (a linear constant) is larger relative to R 2 + G 2 + B 2 = θ (a quadratic constant), producing a shorter accumulation-tothreshold time for RGB and CMY stimuli.
These properties line up with previous work on continuous colors selections by both 
Ratcliff et al. (2018)
 and 
Smith et al. (2020)
 showing that responses tended to be concentrated on these six values. Our anchored-dimension representation provides a firm theoretical basis for why this should occur. Thus, there is no need for Ratcliff et al.'s sine-shaped thresholds or Smith et al.'s extra vector components added to drifts.
Smaller precision coefficients result in less ability to discriminate fine differences between hues, and generate responses that are more heavily biased toward the cardinal hues. As precision increases, this bias decreases and responses shift toward the true hues in the stimulus (bottom panels of 
Figure 9
.


Figure 9
Simulated model predictions for drift vectors (left) distributions of evidence for different hues (middle), and the resulting response distributions (right) for three different levels of precision c (increasing from top to middle to bottom). Note that responses are skewed toward primary and secondary colors as a result of the hexagonal shape of the drift vectors created from different color hues.
Our focus is on the ability of the model to account for the consequences of manipulations of the number and similarity of the response options. Allowing the anchor-based drifts to shoulder the burden of accounting for distributions of color responses permits us to shift focus toward the response sets instead. However, as elaborated below, model fit was improved by fine-tuning this representation by integrating the effects of differences in subjective similarity as assessed through a multidimensional scaling task performed by each participant. Similar approaches could be used to integrate more elaborate theories of color vision in order to provide a more complete account of perceptual grouping in these tasks.


Thresholds
As for the first study, thresholds play a key role in determining differences in behavior among conditions with different numbers of discrete alternatives. We slightly simplified the approach used in the line-length study by assuming that the thresholds increase linearly with set size (N) at a rate θ N from a baseline θ 0 . This increase compensates, at least in part, for the decrease in accuracy that naturally occurs as the number of responses-alternatives increases.
Simplicity also motivated our approach to the effect of similarity among responses on thresholds. Although the second study partially unconfounds the effects of set size and similarity by using unequally spaced stimuli, thresholds also play a key role in explaining similarity effects. When all responses have equal thresholds, MAAT necessarily predicts that response crowding reduces accuracy due to capture by nearby responses. However, as we show below, responses to options in a choice set that are dissimilar to (spaced further from) other alternatives are also faster as well as being more accurate than responses to options in the same set that have similar competitors. In order to capture the effect on speed in a simple manner, we assume that participants adjust their threshold for each option in the discrete condition as a linear function of its similarity to other options in the choice set. As in Experiment 1 this adjustment is plausible because the similarity among discrete responses was evident from the display and participants were afforded practice trials to make the adjustment.
Formally, we assume that the threshold θ j required to select response j corresponds to a base value, θ 0 , plus the adjustment for the number of response options θ N , and the adjustment for the degree of similarity to other options in the choice set (formalized as the evidence it confers to all of the other responses):
θ j = θ 0 + θ N × N + κ ∑ i j d i • d j (9) , Here κ D (0 ≤ κ D ≤ 1)
is a weighting factor determining how responsive a participant's decision rule is to similarities between response alternatives. The similarities are quantified by the sum of the dot products between the directions for each response option. Hence, thresholds increase with similarity, slowing RT and mitigating the associated decrease in accuracy.
To illustrate threshold setting, suppose there are three responses that align with the directions of the anchors (i.e.,
d R = [0, 1], d G = [ √ 3
2 , − 1 2 ], and
d B = [ − √ 3 2 , − 1 2 ])
. In this case the sum's of the dot products are the same (-1) for every response, and so the thresholds are the same in every case. If instead, as is shown in 
Figure 8
, two of the responses are more similar to each other (i.e., pink,
d P = (− 1 2 , √ 3
2 ), and orange,
d O = ( 1 2 , √
3
2 )) than to the third (i.e., cyan, d C = (0, −1)) response, then the similar responses have smaller dot products (both -0.366) than the dissimilar response (-1.73). Hence, the threshold for cyan is smaller than the thresholds for pink and orange, as shown in 
Figure 8
.
The value of κ D modulates the strength of the relative component, and so determines the degree to which the threshold adjustment optimizes reward rate (i.e., minimizes response time for a given level of accuracy, see 
Kvam, 2019a;
Tajima et al., 2019;
Bogacz et al., 2006
Bogacz et al., , 2010
. In many reward-rate paradigms it is found that threshold adjustment is too small to produce reward rate maximization, and we also found that the values of κ D that participants use are too small to be optimal. As a result, both accuracy and response time vary across manipulations of the stimuli and number of response options.
While the threshold in the continuous condition did not change systematically with the response options or set size as in the discrete condition, as the response options were always the same, we did allow it to vary around its mean estimate according to a normal distribution. This was done to compensate for the fact that the same continuous condition was repeated many times across many trials, blocks, and even sessions of the experiment. It is natural to expect that it would change based on fluctuations in cognitive control and attention 
(Logan et al., 2014)
 as well as trial-to-trial shifts in thresholds following errors 
(Navarro-Cebrian et al., 2016;
Frank et al., 2005)
 and potentially even differences in perceived similarity of each color hue to its competitors. For these reasons, we include a parameter κ C that describes threshold variability across trials in the continuous condition. The threshold in the continuous condition was therefore drawn from a normal distribution N(θ C , κ C ) on each trial.
It is possible, although extremely rare based on the parameter estimates, for a start point to exceed the threshold for one or more of the choice options. When this happened, the choice option with the highest activation (greatest start point) was predicted as the option to be chosen and the response time for that trial was fixed at the value of non-decision time τ.


Study 2: Hue Discrimination
Participants each took part in six study sessions. In the first session they rated the similarity on a scale of 0 to 100 of all pairwise combinations of 30 equally-spaced hues. In the remaining 5 sessions they completed the decision task. The similarity ratings were used for two purposes. First, they were used to test for the dissimilarity advantage in response times in the decision task. Second, they were used to calculate subjective versions of the d vectors for each participant, which replaced the objective d values in Equation 9.


Methods
Six Michigan State University graduate students (4 female, 2 male, age range 22-30 years), completed the 6 sessions. Each participant completed approximately 500-1000 practice trials of the decision task, 1400-2300 experimental trials of the decision task, and 435 trials of the similarity rating task. Stimuli were generated and presented in MATLAB using Psychtoolbox 3 
(Brainard, 1997;
Kleiner et al., 2007)
. Analyses used the machine learning and circular statistics MATLAB toolboxes 
(Berens, 2009)
. All responses were recorded from the mouse.
Each session took approximately 1 hour to complete. Participants were paid $10 per session for participating, and informed of their average accuracy at the end of each session. After completing informed consent, they were placed in a dark, windowless office and completed the similarity rating task in the first session. On later dates, they completed 5 sessions of the decision task in the same setting.


Figure 10
Demonstration of the effects of manipulating each parameter of the model (rows) on distributions of responses on a continuum (left panels) as well as response times (right panels). Accuracy for discrete conditions is inset as pie charts in the left panels.
The similarity rating task was self-paced, so that participants could take as long as they wanted to make exact similarity judgments and take breaks as they needed. They were encouraged to take a constant amount of time on each trial and to make sure that their judgments were internally consistent (e.g., a rating of 30 should indicate that a pair of colors is more similar than a rating of 25).
During the first session of the decision task, the experimenter demonstrated how to perform the task in both discrete and continuous conditions, emphasizing that mouse movements should be consistent and ballistic -i.e., that partici-pants should not move the mouse until they were ready to respond, at which point they were to move the pointer directly to the response they wished to make. In addition to their initial briefing and demonstration, participants completed an extra 30 practice trials at the outset of the first session that covered all numbers of alternatives they might see during the task. In the first session, they then completed 10 or 15 blocks (dependent on time) of the decision task, including practice trials. In subsequent sessions, they completed 15 or 20 blocks of the decision task, including practice trials.
Once all 6 sessions were completed, participants were de-briefed on the purpose of the study and the results of their performance, if desired.
Similarity rating task 
Figure 11
 shows an example similarity rating display. Participants simply had to compare the two colors on the screen and assign a value from 0 (opposite) to 100 (identical) indicating how similar to one another they thought the colors were.


Figure 11
Layout of the similarity rating task.
The colors used for the rating task were 30 hues equally spaced along the color wheel (see the large circle on the right of 
Figure 15
). Participants were presented with each possible combination of 2 non-identical colors exactly once, giving a rating for each pairwise comparison. These pairwise ratings were used to populate the upper diagonal of a similarity matrix, which was used to generate a multidimensional scaling (MDS) solution that arranged the colors in 2 dimensions. This created an MDS arrangement for each participant that allowed us to personalize the model predictions: even for the same set of parameters, the model would make different predictions for participants with different MDS solutions. This MDS step is enables psychological similarity -as opposed to merely the distance in physical / stimulus spaceto be used to predict behavior. 
Schurgin et al. (2020)
 showed in working memory tasks fixed-capacity models, where stimulus representations change with set size, are required to account for performance. However, when the subjective similarity between choice options is taken into account, performance can be explained by a single unitary signal detection framework with stimulus representations unaffected by load. Therefore, the version of MAAT that we we fit here uses the subjective / psychologically scaled similarity between response options to determined how thresholds are set, while allowing the actual stimulus itself (through the drift rates) to remain invariant to the relationships between stimuli. The exact procedure for setting these thresholds is described in the modeling section.


Decision task
The decision task used in Study 2 is shown in 
Figure 12
. Participants viewed displays of 78 dots scattered around a disc whose diameter subtended approximately 10 degrees of visual angle. The dots varied in hue such that no two colors had a hue that was within 0.04 units of one another (with hue ranging from 0.0 to 1.0, wrapping around such that 0.0 = 1.0). In each display there was a single dominant dot hue for exactly 18 dots. In addition to the dominant dot color, there were 15 other hues present in the display, with 4 dots of each. The participants' task was to identify which color was the dominant color in the display, match it to the alternatives shown surrounding the dot display 
(Figure 12, top right)
, and respond by moving their mouse to the corresponding hue in the display of alternatives.
Of the 16 hues that would appear on each block of trials, 8 were fixed across a block and 8 were drawn randomly from trial to trial. The fixed hues depended on the available response colors -each of the possible response colors had to appear in the set of dots on every trial. The remaining 8 nonfixed hues present in the dot display were drawn randomly on every trial subject to the restriction that no pair of hues in the display be closer than .04 hue units apart. The nonfixed hues were never the target color, so they served strictly as distractors or noise in the stimulus. In total, this yielded the 78 (18 target + 7 × 4 non-target fixed + 8 × 4 non-target random hues) dots in the display.
Each block consisted of 10 practice trials and 30 trials of the decision task. The set of response options was held constant across all 40 trials. A random alternative out of those available was chosen as the dominant color on each trial. The alternatives available to a participant were placed around the edges of a circle at approximately 20 visual degrees from the center, as shown in 
Figure 12
. Each response alternative took up a 14-degree arc along the edge of this circle in the discrete condition (top / left panels). In the continuous condition (bottom right panel), a hue circle was shown where every degree of the circle was a different hue, approximating a continuous gradient of hues. Across trials, this method of displaying the response alternatives ensured that all such alternatives were equidistant from the center of the screen (where the mouse began the trial) and equal in size. Therefore, motor difficulty was matched across conditions, so differences in accuracy and response time were not attributable to motor demands (i.e., Fitt's law 
Fitts, 1954;
MacKenzie & Buxton, 1992)
.
One issue that arose in Study 1 is stimulus bias, which could result from either a bias toward responding on the "short" side of the scale or result from most participants being right-handed (and thus their responses following a curved trajectory when responding on the left side of the scale; see 
Kvam & Busemeyer, 2020)
. To avoid a similar effect in the data from Study 2, we randomly flipped and/or rotated the response scale between sessions. To do so, a random orientation from 0 to 360 degrees was draw, and a binomial random variable (0 or 1) was drawn to determine whether the scale would also be mirrored at the beginning of each session. Participants were oriented to the alignment of the


Figure 12
The decision task in Study 2. Depending on the condition, these were comprised of 2, 3, 5, 8, or a continuous span of alternatives. Participants responded by moving their mouse across the arc corresponding to their desired response. scale with at least 20 practice trials at the beginning of each session, in addition to the 10 practice trials preceding each block of the task. This ensured that response location was not confounded with the hue of participants' responses.
At the beginning of a trial, a participant saw the available response options but not the stimulus. They began the trial by clicking within a small white circle in the middle of the screen, at which point their mouse cursor was centered and the stimulus appeared. Once a trial had started, a response was entered by moving the mouse across the edge of the circle or arcs on which the alternatives were shown. As soon as the mouse cursor crossed this boundary, their response time was recorded and their response was graded as correct or incorrect. In order to match the accuracy criterion across all conditions, responses were considered correct if they were within 7 degrees of the center of the location of the true dominant dot color. In the discrete case, this meant that responses were correct if they crossed the arc colored in the true dominant dot hue. Participants were informed of this grading criterion prior to beginning the study.
Within a session, the number of response alternatives in a block was evenly split between 2, 3, 5, 8, and a continuum of alternatives. Participant saw 2 blocks of each type per session, with the block order randomly shuffled. At the end of each decision trial, participants received feedback on whether or not their choice was correct in the form of 100 (correct) or 0 (incorrect) points for that trial. Similarly to Study 1, ballistic mouse movement was encouraged by penalizing participants for straying between the dot display and available alternatives. This penalty was 1 point per every 20 ms above 300ms from when the cursor started to move to when it indicated a response. For example, a trial where a participant spent 360 ms moving the cursor from its initial position in the center to its final position at the response location incurred a penalty of 3 points.


Practice trials
In order to ensure that response times were affected as little as possible by practice effects, the physical locations of alternatives, and the time it took to make a ballistic movement to the edge of the circle, there were a minimum of 10 practice trials before every block of decision trials. Each practice trial was similar to the decision trials, except that a single large, colored dot was shown rather than a noisy multicolored dot display. Instead of picking the dominant hue out of the display, participants simply had to match the hue shown in the center of the screen to the alternatives available by moving their mouse through the arc for the corresponding hue. As in the decision task, accuracy and response time were recorded.
The alternatives shown during the practice trials were the same as those in the succeeding decision trials. One of the goals of the practice was to make sure participants knew exactly where each of the alternatives was before they began the decision task. Therefore, for each choice option present in the display of alternatives (2, 3, 5, or 8 for the discrete conditions), a participant saw at least two instances of the corresponding color appear in the center for them to match (meaning there were 16 practice trials in the 8-alternative condition). In the continuous condition, they saw 10 or more random hues appear in sequence in the center. Anytime a participant made an incorrect assignment during the practice trials, an additional practice trial was added.
After each practice trial, the participant received immediate feedback on their accuracy, including the hue they chose, the location of their response on the screen (in terms of degrees around the circle), the correct hue, the correct response's location on the screen, and how far away in degrees their response was from the center of the correct response.


Model estimation
The model was fit using a standard Metropolis-Hastings algorithm for Markov chain Monte Carlo sampling. For the starting point of each chain, we used a point that was randomly jittered (multivariate normal with standard deviation of . 
1, .1, .05, .1, .05, .05, .05, and
 .05 for parameters c, δ, θ 0 , θ C , κ D , τ, θ N , and κ C , respectively) around the maximum likelihood estimate, which was obtained from a Nelder-Mead simplex algorithm (fminsearch in MATLAB; 
Lagarias et al., 1998)
. This used 5 chains, each of whose length was 1000 samples, with 300 burn-in samples. Each step in the chain was drawn from a multivariate normal distribution with standard deviations of σ = 
[.1, .1, .05, .1, .05, .05, .05, .05]
.
We included two modifications to the standard Metropolis-Hastings MCMC algorithm. First, the likelihood of the data given each set of model parameters was re-computed after every three rejected steps. This was necessary to ensure that the sampler would not get "stuck" at locations where the simulation-based likelihood was unusually high 
(Holmes, 2015)
, which can occur when the simulated data that is generated at a particular combination of model parameters happens to line up unusually well with the real data. Second, we included a migration step on every 10 samples, where the value of σ was multiplied by 5 on every 10 th step in the chain. This helps the sampler avoid getting stuck at local minima and explore more of the parameter space than including only smaller steps 
(Turner et al., 2013)
.
Chains were visually inspected for convergence, and are shown in the supplementary materials. Ther statistics were computed for each participant, where values close to 1 indicated good convergence between chains 
(Gelman & Rubin, 1992;
Roy, 2020;
Robert et al., 2010)
. These values were 1.004, 1.01, 1.02, 1.002, 1.002, and 1.01 for participants 1-6, respectively, indicating good mixing across chains.
This study was not preregistered. Study materials including data, analysis code, and additional figures can be found on the Open Science Framework at osf.io/6d29q.


Results
We first report two descriptive analyses. The first tests the form of the set-size effect on response time (i.e., Hick's Law) and accuracy, and the second compares response time in discrete and continuous conditions. Specifically, the second tests whether response time increases for responses that are more similar to other responses. These results are key to evaluation of the model, as they focus on two phenomena that are not predicted by other approaches to modeling continuous-outcome responses. We then report the results of model fitting. 
Figure 13
 (data shown as lines) illustrates that for all participants mean response times increased with set size, and mean accuracy decreased. In most cases response time does not follow Hick's Law, which is predicted by MAAT due to the entanglement between effects of similarity and set size. In fact, overall, a hierarchical Bayesian model predicting mean response time as a function of the number of alternatives showed better fit with a linear link between number of options and RT than one which predicted response times as the log 2 of the number of alternatives, DIC(linear) = 22176 < DIC(log 2 ) = 24290.


Descriptive Analysis
More problematic for Hick's Law, or indeed a linear increase, is the pattern of response times that appears in the continuous condition. For half of the participants, mean response times in this condition are contained within the range of those produced by discrete numbers of alternatives (usually between 5 to 8). Whether the relationship between the number of response options and mean response times is linear or log-linear, any monotonically increasing effect would predict much longer response times in the continuous than in any of the discrete-alternative conditions simply because there are many more options available.
We next evaluated the effect of similarity between a target and distractors and the time it took to respond to the target on each trial. Similarity was measured in terms of the rated similarity between the target hue and its nearest distractor hue, linearly interpolated for hues in between the hues used in the similarity rating task described above. Response times on each trial were rank transformed relative to the block of trials to remove the skew. Response times were nested within conditions and conditions within participants in a hierarchical Bayesian model assuming Gaussian error, allowing us to estimate the within-condition relationship between targetdistractor similarity and response time as a random effect. The average slope across each of these relationships between target-distractor distance and response time was -.18 (95% HDI = [−.24, −.12]), indicating that the closer an alternative was to other competitors, the longer participants took to respond to it. 4 4 The objective distance between the target and its competitors additionally affected responses, but only accounted for approximately 0.8% more of the variance in response time above and beyond subjective similarity (R 2 = .101 with only subjective similarity, R 2 = .109 with objective similarity included) and 0.6% of the variance in accuracy above and beyond subjective similarity (R 2 = .048 with only subjective similarity, R 2 = .054 with objective


Figure 13
Relationship between the number of alternatives and accuracy (top panels) and and number of alternatives and response time (bottom panels) for each of the participants in the study, including Participants 1 (top left, orange), 2 (top middle, green), 3 (top right, turquoise), 4 (bottom left, blue), 5 (bottom middle, purple), and 6 (bottom right, red). Empirical results for each participant are shown as the lines, and best fit predictions from the model for each participant are shown as Xs in the corresponding color. Bars correspond to the model predictions derived from the 95% most likely parameter estimates (HDIs).
We also examined the distribution of responses in the continuous condition and their skew, as MAAT predicts skewed distributions of responses for target hues adjacent to one of the anchors (i.e., primary or secondary colors on the hue wheel). As described above, the response location was the exact spot that the mouse crossed the edge of the response circle. MAAT also predicts that these anchor-based responses should themselves be more frequent. The result is shown in 
Figure 14
. Skewness was quantified by the third central moment of all responses divided by the cube of its standard deviation, for each group. The groups of responses were created by dividing responses into 100 categories according to the target hue on that trials (all responses are divided into target hues between n and n + .01, for n = 0, .01, .02, ..., .99). Similar results are obtained with robust (median or mode based) skewness measures. In the observed data, responses tended to group near the primary and secondary colors (top panels). This resulted from a tendency for responses to be heavily skewed toward these responses, reflected in a marked shift in skew from negative to positive when the target shifted from below the anchor to above the anchor (where anchors are indicated by dotted vertical black lines in 
Figure 14)
.
We tested this more formally using simple linear correlations of the distance between the target and the nearest an-chor [target-anchor distance] and the degree of skew of the responses when a particular hue was the target. This allowed us to evaluate whether the closeness to an anchor affected the skew of distributions. It did, as indicated by a negative correlation between target-anchor distance and response skewness: M = −.24, 95% HDI = 
[−.38, −.11]
. Responses were more frequent near the anchors, as indicated by a negative correlation between the frequency of responses in each bin (out of the 100) and the distance between the target hue and the nearest of the six anchors: M = −.23, 95% HDI = [−.37, −.10]. As in Study 1, the results of Study 2 support the prediction of the model, shown in 
Figure 9
, that responses will be skewed toward the anchors, resulting in more frequent responses at these values.


Model Analysis
The empirical phenomena strongly suggest that an approach like MAAT, where responses are skewed toward anchors and where thresholds are set separately for each option distance added) in regression models, implemented using default priors in JASP 
(JASP Team, 2022;
Consonni et al., 2018)
. This suggests that the motor difficulty introduced by having alternatives close together did matter somewhat, but its effect was dwarfed by the effect of subjective similarity on performance.


Figure 14
Distribution of responses for each participant (top six panels; bars are data, lines are kernel-smoothed data superimposed with corresponding hues) across all continuouscondition trials of the experiment and the skewness of these distributions based on the target hue (bottom panel). Dotted vertical lines correspond to anchors (primary / secondary hues) in all plots. in discrete choice, will out-perform any approach that fails to include these elements. Beyond this, it is still important to evaluate the absolute fit of the model to ensure that it captures not only the qualitative effects but the quantitative patterns in the data.
In contrast to Study 1, where the the models were fit based on accuracy (i.e., whether or not a response fell within 7 degrees of the true stimulus hue), we fit the models to the complete distribution of responses in Study 2. This was done for two reasons. First, there are many potential applications of MAAT where there is no "correct" response -such as judgments of price, confidence, preference, or Likert-style ratings. Second, a key element of the response data is their skew. While predicting the skew as an out-of-sample exercise in Study 1 shows that MAAT can capture the skew in principle, fitting the model only to accuracy throws away valuable information that can be used to inform the model fits, especially in the continuous conditions. Therefore, we fit the data from Study 2 using a multinomial likelihood for the discrete conditions and a continuous probability density approximation likelihood in the continuous condition.
The distribution of responses and response times was computed by evaluating the intersection of a ray, projecting from the starting point to the thresholds in either the discrete or continuous condition. For the discrete condition, this can be simplified by computing the path of an accumulator for each option. This is obtained by taking the component of the starting point along each option comp v j (s) (intercept of each accumulator) and the component of the drift along each option comp v j (δ all ) (drift of each accumulator), and comparing their values to the threshold for each option θ from Equation 9.
The continuous condition is somewhat more complex to derive distributions of responses. For this condition, we must solve an equation relating the linear path of evidence accumulation to the circular response boundary. As with the line length model, the full solution is presented in the supplementary materials.
In both conditions, response time is given by the distance between the start point and the threshold (θ) divided by the rate of accumulation toward the corresponding response option (δ all • v i ), plus a fixed non-decision time (τ). The likelihood of the data for a particular trial was obtained by generating 500 simulated trials from the proposed set of parameters (at each step in the MCMC chain) for every real trial and calculating the predicted joint distribution of responses and response times by passing a kernel density estimator over the response-RT data to perform probability density approximation 
(Turner & Sederberg, 2014;
Holmes, 2015;
Lin et al., 2019)
.


Incorporating subjective similarity
To incorporate the ratings from the similarity task 
(Figure 11)
, we re-mapped the locations of the different color hues on the circle using a circular multidimensional scaling (MDS) procedure 
(Cox & Cox, 1991;
Kvam & Turner, 2021)
. The results for each participant are shown in 
Figure 15
. The participant-level solutions in this figure show the best MDS solution, so that the change in perceived similarity between any two adjacent dots is the same. As we might expect, colors near the center of the green, blue, and red portions of the color wheel were grouped closer together, indicating that they were perceived to be more similar than colors that were in between the anchors. These subjective similarities were used to inform the estimates of thresholds in the decision tasks: if participants had two hues that appeared very similar (e.g., two green hues), then their thresholds would be higher than if they had two hues that appeared to them very different (e.g., a yellow and orange hue) even if the objective distance between those hues in HSV color space was the same. Formally, the location of each hue in 
Figure 15
 Locations of colors on the standard hue color wheel (right) compared to the multidimensional scaling solutions for the locations of these colors based on each participant's subjective similarity ratings (left) 
Figure 15
 for each participant was substituted for the values of r i and r j in Equation 9.
To evaluate whether the addition of subjective similarity ratings helped account for behavior on the task, we fit the model with and without this similarity transformation included. The addition of subjective similarity ratings based on the MDS solutions did not add any additional parameters, so the models can be compared based on their raw log likelihoods. For all but one participant (Participant #2), the log likelihood from the model fit improved substantially with the inclusion of the subjective similarity ratings (all log likelihood differences > 1, 000). Participant #2 did not appear to be sensitive even to objective similarity between options in the choice set, as indicated by the estimates of κ D , so insensitivity to subjective similarity is not too surprising. The Bayes factors for other participants suggest that they were responsive to the perceived similarity between the options in their choice set when they set their thresholds above and beyond the distances between these colors on the raw hue color wheel. The fact that thresholds are responsive to subjective similarity will not be surprising to vision scientists, as it is well-documented that discriminability is not uniform across the hue color wheel 
(Wyszecki & Stiles, 1982;
Ohta & Robertson, 2006)
. However, it provides further evidence of the importance of subjective similarity to the modeling of both discrete and continuous response tasks 
(Schurgin et al., 2020)
. In light of these findings, the results discussed in the next section are from the model that used subjective similarity.


Model Results and Discussion
Maximum a posteriori (MAP) parameter estimates along with the 95% HDIs are presented in 
Table 2
. The mean accuracy and response times predicted by the model for each participant are shown as Xs in 
Figures 13 and observed
 vs. predicted response (location at which the mouse crossed the response circle) and response time quantiles for both discrete (red) and continuous conditions (yellow) are shown in 
Figure  16
.
Model posterior predictions were generated by simulating 100 trials of artificial data for every real data point from the MAP estimates for each participant, and the 95% HDIs were generated based on the resulting mean accuracy / RT estimates from these 95% most likely parameter values. Detailed results for one example participant are shown in 
Figure  17
, with similar plots for the other participants provided in supplementary materials.
The model succeeds in providing a relatively good fit to response times, both in terms of the mean response times shown in the bottom panels of Figures 13, quantiles of the response and RT distributions shown in 
Figure 16
, and in terms of full RT distributions aggregated over participants separately for discrete and continuous conditions in the bottom panel of 
Figure 17
. The one area where improvement could potentially be made is in the tails of the response time distributions, as shown by the model consistently underestimating the 90 th percentile of response times 
(Figure 16
). These response times are quite long, on the order of 2-10 seconds, which is outside the typical range that perceptual evidence accumulation models typically predict. These may also include trials where participants' attention lapsed or where they had a particularly challenging pair of stimuli. We did not include drift rate variability in the model, striving to go as far as we could with threshold adjustments alone, but adding this variability (signifying fluctuations in attention or capacity from trial to trial) may also help the model account for the long tail of response time distributions.
A key factor in the model's success is allowing thresholds to change based on the perceived similarities among a partic-ipant's response options. This is indicated by κ D estimates in 
Table 2
, which are much greater than zero for all but one participant. We tested the importance of this component by evaluating the likelihood of a nested model where similarity does not affect thresholds (i.e., κ D = 0). To do so, we calculated the Savage-Dickey Bayes factor 
(Wagenmakers et al., 2010)
, evaluating the height of the prior at κ D = 0 against the height of the posterior at the same point. Here, we present the log Bayes factors: positive Values indicate support for threshold changes across sets of options, while negative values indicate that the data provide support against participants shifting their thresholds. The prior for the values of κ D were uniform on [0,1]. For participants 1-6 respectively, the log Bayes factors in favor of similarity-based adjustments to thresholds are -0.83 (inconclusive), -1.59 (weakly favoring no adjustment), >1000 (strongly favoring adjustment), 390.8 (strongly favoring adjustment), >1000 (strongly favoring adjustment), and 5.92 (strongly favoring adjustment). Although mixed across participants, this generally supports the inclusion of threshold changes based on similarity in the discrete condition.
Likewise, we can examine the degree of support for threshold variability in the continuous condition by computing the Savage-Dickey Bayes factor on κ C . Since the value of κ C can be any positive value, we used an exponential distribution for the prior, Pr(κ C ) ∼ Exp(5). This resulted in a height of the prior of 0.2 at κ C = 0. As before, we report the log Bayes factor for each participant, where positive values support threshold variability and negative values support no threshold variability. The results strongly supported threshold variability in the continuous condition for all but one participant, with participants 1-6 respectively having log Bayes factors of -3.43 (strongly disfavoring variability), 120.40 (strongly favoring variability), 422.14 (strongly favoring variability), >1000 (strongly favoring variability), >1000 (strongly favoring variability), and 37.90 (strongly favoring variability). 
Table 2
 also shows that the effect of set size on thresholds (θ N ) varied across participants, suggesting substantial individual differences in the way participants modulated their thresholds as a function of set size. Three participants even had 95% HDIs on θ N that included zero, indicating that it is credible that they did not change their thresholds based on the number of options on the screen. Much of the variability across set size can be accommodated through the changes in similarity and κ D (because the response space gets more "crowded" Van Maanen et al., 2012), meaning that this parameter only indexes the changes in thresholds with set-size over and above this effect.
The model also provides a relatively good fit to mean accuracy ( 
Figure 13
) and distributions of responses ( 
Figures  16 and 17, top panels)
. It does, however, have a tendency to slightly over-estimate the accuracy in the 2-and 3-alternative conditions, perhaps not capturing motor variability that could have led participants to accidentally miss the arc as participants could be incorrect either by selecting a different option or by missing the arcs altogether.
Although the observed responses lined up with those predicted from the model, there were occasional exceptions, particularly in the discrete choice case (e.g., top panel 
Figure  17
, light gray dots) where the model predicted a correct response but participants chose a response alternative in their choice set that was far away from the target choice alternative. However, note that the clusters of points near the top left and bottom right do not represent large misfit, but occur because the response scale wraps around near the extremes.
In summary, the MAAT model provides a parsimonious account that captures the main trends in the distributions of responses and response times across five conditions and a variety of similarity relations with only seven parameters. Notably, the changes in behavior across the number and similarity of response options were handled purely by changes to the thresholds in the model, so that the fundamental perceptions and representations of the stimuli are unaffected by manipulations of response options.


General Discussion
The results of these experiments add to a growing body of work on continuous response tasks, which has discovered skewed responses on bounded scales 
(Kvam & Busemeyer, 2020)
, used changes in two-dimensional drift to predict response distributions 
(Smith et al., 2020;
Ratcliff, 2018)
, and proposed that the mapping from stimulus to response is the main mechanism distinguishing between continuous and discrete response tasks 
(Smith, 2016)
.
We evaluated four hypotheses that should hold if our anchor-based modeling framework is valid: (1) bow effects in unidimensional judgments (Study 1); (2) response skew towards anchors (Studies 1 & 2); (3) slower and less accurate responses with an increasing number of response options (Studies 1 & 2); and (4) higher threshold setting for responses with more similar competitors (Study 2). In each case, the empirical phenomena supported the hypotheses and the MAAT model was able to account for the qualitative patterns in the data.
By connecting the discrete and continuous responding we were able to disentangle the perceptual processes related to representing stimuli from the response processes related to generating a decision. MAAT makes strong and plausible selective influence assumptions: dynamic (drift) components of these tasks are entirely ascribed to stimulus-driven factors -the length or color of the stimulus -while decision thresholds account for response-related factors, such as whether responding is discrete or continuous and in the discrete case the number and similarity of responses options. Approaches that have focused purely on the discrete case have taken a different approach. 
Usher et al. (2002)
 propose that an in-


Figure 16
Quantile-quantile (Q-Q) plots of the observed (x-axis) versus prediction of the model (y-axis) for the 10 th , 30 th , 50 th , 70 th , and 90 th quantiles. These are included for both response / hue (top panels) and response times (bottom panels), for both discrete (black / o) and continuous (gray / +) conditions, and for each participant (columns 1-6, respectively). Points along the dotted diagonal line indicate perfect fit of a particular quantile.
crease of drift rates due to lateral inhibition plays a large part in explaining set-size effec"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]