You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Going to university is a major life transition. As such, it is important to make the right decision about where to study and what course to undertake. However, within most countries there is likely to be a variety of possible institutions where a student could study. Moreover, within each institution there will be numerous courses that can be taken. This mass of choice can make deciding where to study and what to study difficult for students. Therefore, it is important to have metrics that are available to help students make this important decision.
Within different countries, there are league tables that are produced which rank the universities overall, as well as at the subject level (e.g., top university for chemistry, veterinary science, psychology). For example, in the US students can consult the US News League 
Table.
 In the Netherlands, there is the 'Keuzegids'. Similarly, in the UK there are a variety of league tables, such as the Guardian University Guide, Complete University Guide and the Times League 
Table.
 Universities market themselves based on these rankings. As such, it is unsurprising that rankings influence a prospective student's choice of university 
(Gibbons, Neumayer, & Perkins, 2015)
. Given this, it is important to ensure that university rankings in league tables are reliable. In this paper, we first outline the issues with using rankings from university league tables. Moreover, we apply a novel Robust Ranking Aggregation Algorithmic (RRA) method 
(Kolde, Laur, Adler, & Vilo, 2012)
, which is predominantly used in genomics, to test the reliability of university rankings for two well-used league tables.


Issues with University League Tables
There is substantial variation in the data used to create the league tables. For example, the US News rankings combine data from 17 different factors to create its ranking of UNIVERSITY RANKINGS 4 institutions in the US (https://www.usnews.com/education/best-colleges/articles/how-usnews-calculated-the-rankings). This includes data on graduation rates, faculty salaries, peer assessment and student-staff ratio. In contrast, in the UK the Complete University Guide ranks universities using data on entry standards (i.e., average grades needed to study at the university), student satisfaction, research quality and graduate prospects (https://www.thecompleteuniversityguide.co.uk/sector/insights/university-and-subjectleague-tables-methodology). Even within countries, there is variation in the data used to construct league tables. The Complete University Guide and the Guardian University Guide are both UK-based university ranking systems, but the data used in the Guardian guide is substantially different to the Complete University Guide (https://www.theguardian.com/education/article/2024/sep/07/methodology-behind-the-2025-guardian-university-guide). The Guardian rankings are based on entry tariff, satisfaction with teaching (from National Student Survey), satisfaction with feedback (from National Student Survey), student-staff ratio, money spent on each student, added value (i.e., how their degree result relates to their entry qualifications), the number of graduates who have a graduate-level job within 15 months and continuation (i.e., percentage of first year students who continue to second year). Overall league tables combine both objective data (e.g., student-staff ratio, grades needed to enter the course) and subjective data (e.g., student satisfaction, peer-review of the institution) to rank the institutions within a country.
As such, despite there being some similarities, league tables rely on different data both within and between countries.
The use of different data across league tables creates a problem with this ranking system. Indeed, a university's ranking is likely to vary depending on which league table is consulted 
(Bowden, 2000)
. Aforementioned, institutions often use university rankings in their marketing materials. The incorporation of these inconsistent rankings into such marketing materials is likely to cause further confusion. For example, universities may cherry pick the ranking system that makes them look best, resulting in two universities both UNIVERSITY RANKINGS 5 marketing themselves as the top course in their region for a specific course 
(Wakeford, 2015)
.
Therefore, these inconsistencies are likely to create problems for prospective students, as they may be unsure as to which guide to trust.
Problems with the underlying data may also be problematic for university rankings. As mentioned above, in the UK both the Complete University Guide and the Guardian University Guide rankings are partly based on data from the NSS. This is an annual survey completed by students in their final year who are studying at UK institutions. The Complete University Guide uses the satisfaction with the quality of teaching students received in its league table. In contrast, the Guardian uses the satisfaction with the quality of teaching alongside the satisfaction with feedback subscales from the NSS in its league tables. However, numerous researchers have highlighted problems with NSS data. Indeed, NSS scores vary based on factors beyond the quality of the course, including the region in the UK where the university is based, the subject that is studied (i.e., media studies versus clinical dentistry) and whether the course is being studied full-time or part-time 
(Bell & Brooks, 2018)
. As a result, it is difficult to use NSS data to rank the quality of different institutions and courses.
Students from different universities may also rate the same item using different criteria 
(Bennett & Kane, 2014)
. Moreover, researchers have questioned the data sorting processes applied to NSS data 
(Pollet, Bilalić, & Shepherd, 2024)
. Additionally, the structure of the NSS subscales has been questioned 
(Pollet & Shepherd, 2022)
, with the researchers suggesting that there may be fewer subscales than suggested by the NSS. This is particularly problematic for the Guardian League Table which used different NSS subscales. Therefore, the use of this subjective data in university rankings may be problematic 
(Marginson, 2014)
.
Although the NSS is specific to the UK, there are other subjective ranking metrics that have received criticism. One such metric is the use of peer review. This involves experts judging the academic quality of different universities. Researchers have argued that such ratings are instable and may reflect methodological issues (e.g., 
Bookstein, Seidler, Fieder, & UNIVERSITY RANKINGS 6 Winckler, 2010)
. Although peer-review is weighted heavily on some league tables, some experts have questioned the validity of this task and chosen to boycott this 
(Enserink, 2007)
.
Related to this, not all metrics contribute equally to the university rankings. Instead, some metrics are weighted more heavily than others. For example, in the Complete University Guide student satisfaction is weighted more heavily than entry requirements. In contrast, in the US news rankings there is variability between the different rankings, with peer assessment being highly weighted and retention rates and student-staff ratio receiving much lower weights. Researchers have argued that the arbitrary assignment of weights to the different metrics is problematic for such ranking league tables 
(Marginson, 2014)
. Indeed, weighing metrics in university league tables based on less arbitrary values is likely to produce very different results 
(Turner, 2005)
. Additionally the metrics in university league tables are unlikely to form a single construct, leading to questions about the use of weightings to create single scores 
(Johnes, 2018)
. As such, although the weights are applied consistently for each university, numerous issues have been identified with this approach.


Reliability of University Rankings
The issues above have been well-discussed in previous research [e.g., 
(Enserink, 2007;
Marginson, 2014;
Wakeford, 2015)
. However, there are other issues that need to be considered with regards to a university's overall ranking. Indeed, students are taught at the subject-level rather than the university level. As such, it is important for overall assessments of universities to be based on subject-level performance. This is problematic as the subjects taught at different universities vary. Given that NSS ratings vary based on the subject 
(Bell & Brooks, 2018)
, simply combining overall scores across the university, without taking the subjects taught at an institution into account may be problematic. Instead, it is important to base the judgements on how well each institution performs in the subjects that are taught. However, this poses the additional challenge that not all subject ranking lists will be of even length. For example, a guide might rank 100 institutions for accounting degrees, but only 30 UNIVERSITY RANKINGS 7 for Aeronautical and Aerospace engineering. This makes it hard to compare if certain institutions are consistently ranked top across lists. Being top ranked when there are only 30 courses ranked should be interpreted with caution compared to when 100 courses are ranked.
Therefore, a robust method needs to be applied if we want to compare universities across courses.
Here, we apply a robust ranking method from genomics which allows for aggregation across lists of ranks. In the domain of genomics, different studies will produce lists of candidate genes associated with a trait -but the length of these lists might vary. For example, Study X might have examined the top 10, Study Y the top 30 and Study Z the top 50, et cetera. A challenge is to synthesise across these lists varying in length to determine if there are candidate genes consistently present across all lists. A similar issue applies with university courses. Indeed, each institution may teach a subset of subjects. These then need to be combined to create the overall evaluation of the institution. Despite the parallels between the issues in genomic research, where researchers want to synthesise across lists, and the university league table issue, to our knowledge this approach has not been used to synthesise subject rankings across UK universities.
This project is largely descriptive and we are interested in how a robust ranking method compares to the institutional rankings produced by well-used university guides. In our analyses, we first analyse all institutions and then focus on The Russell group. They are 24 universities who identify as world-class research intensive universities. However, a 2023 article in the Guardian, made the case that some of these universities might be leading but others are not. This was supported by data from the Guardian University Guide 2024. Part of the evidence cited was that the leading institution was a non-RG member (University of St. Andrews). Therefore, based on the above rationale, we address the following research questions:
1. Are universities similarly ranked by their respective guide (institutional ranking) and UNIVERSITY RANKINGS 8 the robust ranking method we used? 2. Across courses are there universities that are consistently ranked highest? We expect that these will be predominantly Russell Group universities.
3. Are there universities that are consistently improving based on the robust ranking method? We use a small window (year to year, rank difference in 2024-2023) and a larger window (rank difference 2024-2019). We calculate rank differences for all courses that we are able to match, as groupings might have changed. This is then compared against the well-used university guides.


Method


Data sources
We selected two popular guides based on public data availability. For both sources, the structure of the metric and methodology of the rankings are broadly constant since their inception. However, it should be noted that (substantial) revisions do happen. That being said, the guides themselves routinely offer year-to-year comparison. For our analysis, we do not combine the two guides or directly compare them as they vary not only too widely in their methodology, but they also vary in how they group subjects, the number of subjects and the number of institutions they include. This hampers any comparison across guides.
The guides themselves provide detailed information on how their metrics are calculated at both the institutional level and subject level. These typically involve measures derived from the National Student Satisfaction survey, Staff Student Ratio, and we refer the reader to the respective guides. For our purpose, the details on how these metrics are constructed does not matter gravely, rather we are interested how a robust ranking would match up to one produced by a provider of league tables. Studies has the most institutions ranked (n = 121). The smallest list was for Celtic studies where only five institutions were ranked. The Complete University Guide institution list allows for ties in rankings (for 2024: 14 ties; none in top 60). However, in their subject ranking list, only one subject showed ties (Drama, Dance, and Cinematics), and this was limited to one tie only.


Ethics and data availability
The project was approved by the local ethics committee at the institution of the corresponding author. As described above, the data are publicly available from the Guardian website and the complete university guide website.


UNIVERSITY RANKINGS 10


Analytical strategy
Our analyses were conducted in R 4.4.2 (R Development Core 
Team, 2008)
. For research question 1, we apply the Robust Ranking Aggregation algorithmic (RRA) method developed by 
Kolde et al. (2012)
, widely used in bioinformatics (over 1,000 citations in Scholar Google). Very simply put, in this algorithmic approach a null model is generated with random ranking. No ties are allowed. This is a list with random shuffles of all items on the list, and it is thus non-informative. Next, we can evaluate the probability of being ranked at a certain position compared to the reference distribution with all these non-informative lists. A p value can thus be derived for each item, and an aggregated list can be generated via ordering these p values. The full details for the method are described by 
Kolde et al. (2012)
. As algorithmic approaches take a random starting point, we ran the analyses in duplicate with a different random seed -in all cases we obtained the same results with a different starting seed. There was perfect correspondence in rankings (Spearman ρ = 1), so we only report the first analysis. We refer to the ranking produced as the 'RRA ranking', and we generate this for all institutions in the included subject lists as well as restricted to just the institutions on the institutions list but when it comes to comparing institutional rankings based on guides and our method, we only rely on the latter. In our analyses, all the subjects are weighed equally but due to list length, larger subject rankings will inherently carry more weight, i.e. being ranked 1st when 80 universities are ranked is more informative compared to when just five institutions are ranked. In this way, the analyses thus account for the relative popularity of varying subjects.
Our results are mostly descriptive statistics, ordinal correlations (Spearman ρ, 
Siegel & Castellan, 1988)
 and the p values associated with the ranking. We apply an error correction to account for multiple testing as every item gets ranked 
(Benjamini & Yekutieli, 2001
). This method is conservative, as compared to other methods (e.g., 
Benjamini & Hochberg, 1995)
.
It makes no assumptions regarding the dependency of p values. An alternative is the UNIVERSITY RANKINGS 11 Bonferroni method, however, there are suggestions that this method is overly restrictive (e.g., 
Moran, 2003;
Nakagawa, 2004;
Perneger, 1998)
. A significance level of 5% is used throughout, though we will also report p < .1. For judging the correspondence or stability between rankings, based on ordinal correlations, we interpret thresholds of .667 as acceptable and .8 as good (see 
Krippendorff, 2004
, pp. 239-ff for similar guidelines). Though the RRA method does not allow for ties in its ranking, note that Spearman ρ does allow for ties when calculating correspondence. For examining whether there is significant change in the ranks between 2023 and 2024 within institution lists we use a one sample rank test we use a one sample Wilcoxon signed rank test against a median of 0. Our analysis plan and full analysis document is available at https://osf.io/ubnwy/. This document also contains some further analyses and full tables not reported here.


Results


Guardian data
2024. 
Table 1
 shows the rankings for all those universities ranked by the RRA method, as well as their Guardian 2024 institution ranking. The top 13 institutions are ranked significantly better than chance (p = .05), regardless of whether the 
Benjamini and Yekutieli (2001)
 or Bonferroni method is used to adjust for multiple testing. Approximately half of the Russell Group universities were not included in the list and the list also included some non-Russell Group members (e.g., St Andrews, Swansea).
The correspondence between the two ranking lists was acceptable, both when including any institution ranked in the subject list (Spearman ρ = .667) and when restricting to universities which are listed in the GUG institution list (Spearman ρ = .673). The absolute median rank shift between the two methods is 18 ranks (n = 122 institutions ranked, 
Figure   1A
). Some institutions did not move (min = 0), but at its extreme they can move with over 100 ranks (max = 103). Thus, even though there is acceptable correspondence, some institutions can exhibit very large shifts up or down the list. 
Table 2 shows the institutions   UNIVERSITY RANKINGS   12
 with the largest absolute shifts in ranks. Most notably the largest shifts are for some specialist providers: the University of the Arts London (RRA rank = 118, GUG = 15) and SOAS (School of Oriental and African Studies, University of London, RRA rank = 118, GUG 2024 = 15). These are two specialist providers, not offering all subjects. Excluding these two specialist providers improves the correspondence somewhat but not greatly so (Spearman ρ = .712). That being said other universities with broader curricula also showed a large discrepancy. For example, both Chichester and Aberystwyth shift 69 places downwards on the RRA rankings. It also is not a given that specialist providers inherently will have poor rankings. For example, some specialist providers, e.g., the London School of Economics (RRA Rank = 17), still float toward the top, albeit in a less pronounced way than under the Guardian Institution rankings (GUG 2024 = 4).   We can use the Russell Group Universities to illustrate some shift in rankings. 
Figure   X
 shows the largest changes in ranking between the two methods.  We examined whether there were institutions that dropped or climbed rankings more so than expected under chance with the RRA method. Just three universities out of 121 ranked in total, i.e. Leeds, Birmingham, Hull, experienced a significant drop based on the RRA method. The conclusion is the same regardless of the choice for the 
Benjamini and Yekutieli (2001)
 
(p Benjamini&Yekutieli < .00001, p Benjamini&Yekutieli = .0030, p Benjamini&Yekutieli = .0146) or Bonferroni adjustment (p Bonferroni < .00001, p Bonferroni = .0011, p Bonferroni = .0081).
There were no significant climbers based on the RRA method when applying an adjustment based on 
Benjamini and Yekutieli (2001)
  There was little correspondence between the ranking for ranks dropped between 2023
and 2024 based on the Guardian Institution list and the RRA method (Spearman ρ = .187).
Similarly there was hardly any correspondence between the ranks gained between 2023 and 2024 based on the Guardian Institution list and the RRA method (Spearman ρ = .259). We examined whether there were institutions that dropped or climbed rankings more so than expected under chance with the RRA method. Seven universities out of 118 ranked in total experienced a significant drop based on the RRA method 
(Table 3)
 and p Bonferroni = .097).


2019
There was little correspondence between the ranking for ranks dropped between 2019
and 2024 based on the Guardian Institution list and the RRA method (Spearman ρ = .284).
Similarly there was hardly any correspondence between the ranks gained between 2019 and 2024 based on the Guardian Institution list and the RRA method (Spearman ρ = .248).


Table 3
The providers with the largest drops in rank between 2019 and 2024 vs. p Bonferroni = 1) and Queen Mary University London (QMUL, both methods: p = 1).
The absolute median rank shift between the two methods is 11 ranks (n = 130 institutions ranked). Some institutions did not move (min = 0), but at its extreme they can move with over 80 ranks (max = 87; 
Figure 1B
). Thus, even though there is acceptable correspondence, some institutions can exhibit very large shifts up or down the list. 
Table 5
 shows the ten institutions with the largest absolute shifts in ranks. These tend to be specialist providers but also some non-specialists (e.g., University of Buckingham).  We can again use the Russell Group Universities to illustrate some shift in rankings.   The vast majority of subject rankings were stable (Spearman ρ ≥ .667, for 68 out of 74 subjects), with most showing strong stability (Spearman ρ ≥ .8, for 54 out of 74 subjects).


UNIVERSITY RANKINGS 20
Three universities out of 129 ranked in total, i.e. Coventry, Central Lancashire, University of Manchester, experienced a significant drop based on the RRA method. The conclusion is the same regardless of the choice for the 
Benjamini and Yekutieli (2001)
 (p Benjamini&Yekutieli = .007, p Benjamini&Yekutieli = .032, p Benjamini&Yekutieli = .032) or Bonferroni adjustment (p Bonferroni = .001, p Bonferroni = .012, p Bonferroni = .017). Ulster University also experienced a drop, but not significantly so at p = .05 (p Benjamini&Yekutieli = .087 and p Bonferroni = .064). There were three universities, experienced a significant climb based on the RRA method.
There was hardly any correspondence between the ranking for ranks dropped between 2023 and 2024 based on the CUG Institution list and the RRA method (Spearman ρ = .054).


UNIVERSITY RANKINGS 24
There was little correspondence between the ranking for ranks gained between 2023 and 2024 based on the CUG Institution list and the RRA method (Spearman ρ = .258). 


2019


UNIVERSITY RANKINGS 25
When we group the data by subject, for 3,185 items ranked we were able to calculate a rank difference between 2019 and 2024. There was strong correspondence between 2019 and 2024 ranks (Spearman ρ = .886). Median movement across all items in ranks was six ranks (min = 0, max = 83). There was no correspondence between the ranking for ranks dropped between 2019 and 2024 based on the CUG Institution list and the RRA method (Spearman ρ = -.003). There was little correspondence between the ranking for ranks gained between 2019 and 2024 based on the CUG Institution list and the RRA method (Spearman ρ = .219).


Discussion
Our paper represents the first effort to systematically analyse subject rankings with a method developed for comparing ranks from bioinformatics. A key benefit is that it provides a relatively assumption free method for comparing across lists of various lengths. Using a method primarily for synthesising gene lists in genomics, we tested the extent to which UNIVERSITY RANKINGS 26 ranking based on a robust ranking aggregation algorithmic (RRA) method corresponded to the rankings provided by two well-used university guides: the Guardian University Guide and the Complete University Guide. Overall, we found some correspondence between the ranking based on the RRA method and both the rankings in the Guardian University Guide and the Complete University Guide. However, the RRA method was more closely associated with the rankings in the Complete University Guide than the Guardian University Guide.
Indeed, for the 2024 data, the median rank shift was 18 ranks for the Guardian University
Guide and 11 ranks for the Complete University Guide. There were also some extreme cases with large shifts in rank (i.e., 103 for the Guardian University Guide and 87 ranks for the Complete University Guide). This discrepancy between the RRA method and the university guides was also found when focusing specifically on Russell Group universities. Therefore, we argue that the way in which university ranking data is calculated has implications for a university's position in league tables.
For the university guides, there was strong correspondence between the 2023-2024 rank data. For the Complete University Guide, there was also strong correspondence between the 2019-2024 rank data. However, for the Guardian League 
Table the correspondence between
 the 2019-2024 rank data was only moderate. Interestingly, there was hardly any alignment between movement at subject level via our method and institutional rank changes as captured by the university guides. This was regardless of whether the comparison was made between 2023-2024 or 2019-2024. As such, this questions whether changes in ranks are meaningful or whether they are dependent on the methodology used to rank the universities.
A university's rank within a league table has important implications. For example, students may use these to choose which university to attend 
(Gibbons et al., 2015)
. This has important consequences for both the student and the higher education institution, who often use these in their marketing materials (for example, University of Bath, University of Bolton, City University London, Loughborough University, University of Portsmouth). Therefore, UNIVERSITY RANKINGS 27 these ranks need to be reliable. In this research, we demonstrate that there is variation between each university guide's ranking and the ranking determined by our RRA method. This is not the first study to show variations in university rankings based on different analytical methods (e.g., 
Turner, 2005)
. However, in contrast to this previous research, our analysis was based on the original ranking data from the original university guides -we did not adjust the weighting of the different metrics. Given this, greater correspondence between ranks could be expected. As such, the discrepancies between the RRA method and the university guides is problematic. As a result, we argue that it is important to keep this reliability in mind when making decisions based on league table rankings.
This issue is especially difficult given the lack of correspondence in increases and deceases in ranks between the RRA method and the university guides. Universities aim to improve their ranks in league tables and undertake different strategies to achieve this. Any changes in ranks from one year to the next may be attributed by university staff to strategies that have been put in place to improve the university. However, if the implementation of such strategies has been effective, we would expect to see similar changes in the rankings based on the RRA method and the university guides. The lack of correspondence suggests changes may be more likely to be driven by random fluctuations. By focusing specifically on the university guides, institutions may misattribute changes to these strategies rather than random fluctuation. As such, resources may be allocated to ineffective strategies for improving the institution (e.g., https://blogs.lse.ac.uk/impactofsocialsciences/2021/03/22/the-absurdity-of-universityrankings/.


Strengths and Limitations
There are numerous strengths of this research. To our knowledge, this is the first time that the RRA method commonly used in genomics has been applied to institution ranking league tables. The ability of this algorithm to aggregate data across lists of different lengths UNIVERSITY RANKINGS 28 makes this well suited to being applied to university league table data. Moreover, we contrasted this RRA method with two separate well-used university ranking league tables and at different points in time. The fact that the findings were fairly consistent throughout the years demonstrates their robustness.
Despite this, it is important to discuss the limitations of this research. We focussed on one particular method for generating ranks. However, there are other algorithmic approaches one could take (e.g., 
Feng, Deng, Wang, Song, & Liang, 2023;
Li, Wang, & Xiao, 2019)
. Some researchers have suggested that rather than comparing universities as a whole, it may be more effective to use data envelopment analysis to categorise similar universities into groups based on their objectives as this would account for the diverse nature of higher education 
(Johnes, 2018;
Turner, 2005)
. We are not claiming the method we used is the fastest or best performing for this problem. In fact, depending on the analytical problem, there might be others. As such, more work is needed on optimal rank aggregation. Instead, we used this research to highlight that discrepancies may occur depending on how institutions are ranked.
The ranking method likely puts specialist universities at a disadvantage. Indeed, we found a large shift in ranks, in both the Guardian and Complete University Guide, for specialist providers (e.g., University of Arts London, Norwich University of the Arts). There were some non-specialist providers with large shifts as well (e.g., Buckingham, Chichester, Aberystwyth). It is difficult to circumvent this issue as it would be rather difficult to say which institutions are specialist providers or where to draw the line. For example, an institution may not offer Medicine, but may offer Nursing, which a Russell Group university might not offer. Alternatively, the analysis could focus specifically on the Russell Group.
However, there are still some specialist institutions there (e.g., London School of Economics).
It could also be argued that specialist institutions are not an issue when they can boost their rankings in larger subjects.
It is also worth noting that both university ranking guides were from within the UK.


UNIVERSITY RANKINGS 29
The aim of this research was to illustrate the implementation of the RRA method for university ranking data. Although both guides were from the UK, there is no reason why this approach could not be applied in a similar manner to other university ranking data. Our data analysis code is freely available to others, enabling them to apply this RRA method to other university ranking data. In this research, we have demonstrated the application of this approach. We would strongly encourage others to extend this research by applying this approach to other university ranking data.


Further directions
We focused on rankings specifically but more broadly, we believe there is future work to be done to more closely investigate so-called pooling fallacies (also known under various guises, such as Simpson's paradox 
(Kievit, Frankenhuis, Waldorp, & Borsboom, 2013;
Simpson, 1951;
Wagner, 1982)
 or the ecological fallacy 
(Piantadosi, Byar, & Green, 1988;
Robinson, 1950)
, review in 
Tu, Gunnell, and Gilthorpe (2008)
). As an example, institution A could outperform institution B in an overall institution list but when looking at the subject list institution B outperforms A on a majority of subjects -this can occur even after weighing for subject size or number of subjects compared. These phenomena have been well-documented in other disciplines (e.g., Anthropology: 
Pollet, Tybur, Frankenhuis, & Rickard, 2014;
Biology: Allison, 2002;
Scheiner et al., 2000;
Criminology: Demers & Rossmo, 2015;
Epidemiology: Persoskie & Leyva, 2015;
Reintjes, de Boer, van Pelt, & Mintjes-de Groot, 2000;
Psychology: Hintzman, 1980;
Kievit et al., 2013
) but we do not believe these are fully considered when it comes to institutional metrics. 
Soh (2012)
 examined this form of paradox for QS rankings when it comes to lower level indicators versus overall ranking, but to our knowledge, in the context of ranking subjects. We would welcome more work examining these reversal paradoxes.
Another further direction is to synthesise global university rankings (e.g., QS, Times
Higher Education, US news global rankings), though a challenge is that each ranking UNIVERSITY RANKINGS 30 approach has its own methodology and therefore possibly cannot be compared. This is known as an issue of measurement equivalency (review in 
Jilke, Meuleman, & Van de Walle, 2015
). Then again, a similar approach is used in political science, where across multiple polls results are aggregated even if they vary considerably in design 
(Blumenthal, 2014)
. A useful next step would be to synthesise global rankings and evaluate if similar institutions float to the top.


Conclusion
In conclusion, this research demonstrated the application of a RRA method from genomics to university league table rankings. We found that the RRA methods corresponded with rankings from two well-used league tables, but that it was more closely aligned to the Complete university Guide than the Guardian University Guide. However, we did find some discrepancies between the ranks our RRA method and the rankings in both university guides.
These discrepancies may lead to questions about the reliability of rankings from these well-used league tables. This is further collaborated by the lack of correspondence between the increases and decreases in the university rankings from the well-used league tables and that of the RRA method. Given the importance of university rankings on student decisions regarding where to study, as well as university policy, it is important to identify robust ranking methods.
Guardian
University Guide data (2019, 2023, 2024). There are 122 institutions in the 2024 ranking list of institutions, but 159 unique institutions appear in the list of 66 subjects (each denoted with an 'S code'). For 2023, the respective numbers are 121 UNIVERSITY RANKINGS 9 for institution list and 158 for the subject list. For 2019, the respective numbers are for institution list and 158 for the subject list. For 2024, Business and Management (code S240) has the most institutions ranked (n = 119). In contrast, Veterinary Science (code S030) had the fewest institutions ranked with just ten institutions being ranked. In total, across all 66 subjects, 3,738 ranks were produced in 2024. The Guardian institution list allows for ties in rankings (for 2024: 15 ties, none in top 20) as does the subject ranking list, but these are relatively rare at subject level (for Median = 2.5 ties across 66 subjects). The supplementary materials contain further descriptives for the years 2023 and 2019. Complete University Guide (2019, 2023, 2024). There are 130 institutions ranked in the institution list for 2024, but 151 unique institutions appear in the list of 74 subjects. For 2023, the respective numbers are 130 for the institution list and 73 for the subject list. For 2019, the respective numbers are 131 for the institution list and 70 for the subject list. For 2024, similar to the Guardian University Guide 2024, Business and Management


Figure 1 .
1
Histograms with absolute rank difference based on the RRA method and university rankings. A = Guardian University Guide Rankings, B = Complete University Guide UNIVERSITY RANKINGS 13


Figure 2 .
2
Plot with ranks of Russell Group Universities based on two ranking methods. We highlighted the largest increase and largest drop. n = 122 institutions ranked in total. UNIVERSITY RANKINGS 16 2024 ranks (Spearman ρ = .863). Median movement across all items in ranks was six ranks (min = 0, max = 66). All subjects showed significant changes in ranks between 2023 and 2024 (all Wilcoxon signed rank tests of median movement against 0, p < .05). 245 out of 3,556 items did not move rank (6.89%). The largest individual climber was the University of East London in Business and Management studies (S240) from rank 100 to 34. The largest individual drop was by Keele university in Computer Science & Information Systems (S220) from rank 20 in 2023 to rank 85 in 2024. Veterinary Sciences (S030) had the lowest median movement (Median = .5 rank) and Animation and Game design (S225) had the greatest median movement (Median = 11 ranks). The greatest correspondence between 2023 and 2024 ranks was found in Medicine (S010, Spearman ρ = .954, n = 36), the poorest correspondence was for Anatomy (S040, Spearman ρ = .505, n = 21). The vast majority of subject rankings were stable (Spearman ρ ≥ .667, for 61 out of 66 subjects), with most showing strong stability (Spearman ρ ≥ .8, for 41 out of 66 subjects).


-2024 difference. Four institutions (University for the Creative Arts, Abertay, Arts University Bournemouth and Glyndwr) were not included in the 2019 institution rankings and thus excluded from further calculations. The correspondence between an institution's rank for 2019 and 2024 was moderate (Spearman ρ = .762). Six universities did not move ranks between 2019 and 2024 (5.08%). The median movement on the institution list was 14.5 ranks and this is significantly different from 0 (Wilcoxon signed rank test = 6328, p < .00001). The biggest climber was Ulster, climbing 65 ranks, from 93 to 28. The largest drop was 63 places, and this was Brunel moving from rank 58 to rank 121. 49 out of 66 subjects can be matched between 2019 and 2024. When we group the data by subject, for 2,651 items ranked we were able to calculate a rank difference between 2019 and 2024. There was weak correspondence between 2019 and 2024 ranks (Spearman ρ = .616). Median movement across all items in ranks was twelve ranks (min = 0, max = 97). All subjects showed significant changes in ranks between 2023 and 2024 (all Wilcoxon signed rank tests of median movement against 0, p < .05). 84 out of 2,651 items did not move rank (3.17%). The largest individual climber was the University of Wolverhampton in Business and Management studies (S240) from rank 122 to 28. The largest individual drop was by Falmouth university also in Business and Management studies (S240) from rank 14 to rank 111. Veterinary Sciences (S030) had the lowest median movement (Median = .5 rank) and Forensic Science (S120) had the greatest median movement (Median = 22.5 ranks). The greatest correspondence between 2019 and 2024 ranks was found in Theology and religious studies (S480, Spearman ρ = .811), the poorest correspondence was for Pharmacy (S090, Spearman ρ = .057). The vast majority of subject rankings were not stable (Spearman ρ ≥ .667, for 8 out of 49 subjects), with just one subject showing strong stability, the UNIVERSITY RANKINGS 18 aforementioned Theology and religious studies.


Figure 3
3
shows the largest changes in ranking between the two methods. The correspondence between the 24 Russell Group universities' rankings between the two methods is relatively poor (Spearman ρ = .394).2023-2024 difference.Both the 2023 and 2024 list contained 130 institutions.


Figure .
.
Plot with ranks of Russell Group Universities based on two ranking methods. CUG = Complete University Guide, LSE = London School of Economics and Political Science, University of London. We highlighted the largest increase and largest drop. n = 130 institutions ranked in total. UNIVERSITY RANKINGS 23 rank difference between 2023 and 2024. There was strong correspondence between 2023 and 2024 ranks (Spearman ρ = .921). Median movement across all items in ranks was five ranks (min = 0, max = 63). All but one subject, African and Middle Eastern Studies, showed significant changes in ranks between 2023 and 2024 (all Wilcoxon signed rank tests of median movement against 0, p < .05). 302 out of 3,556 items did not move rank (8.12%). The largest individual climber was the University of Winchester in Drama, Dance, and Cinematics from rank 96 to 33. Two institutions showed the largest drops in a subject: the University of Sunderland in Biological Sciences from rank 35 in 2023 to rank 94 in 2024 and Buckinghamshire New University in Sport science with a drop from rank 20 to rank 79. African and Middle Eastern Studies had the lowest median movement of all subjects (Median = 0 ranks, n = 9). Two subjects, English and Drama, Dance, and Cinematics, had the greatest median movement (Median = 11 ranks, respective n = 97 and n = 96). Perfect correspondence between 2023 and 2024 ranks was found in Celtic Studies (Spearman ρ = 1, n = 5), the poorest correspondence was for American Studies (Spearman ρ = .476, n = 8).


-2024 difference. Birkbeck (University of London) was included in the 2019 rankings but was not included in the 2023 or 2024 list. The correspondence between an institution's rank for 2019 and 2024 in the CUG list was strong (Spearman ρ = .884). Six universities did not move ranks between 2019 and 2024 (7.69%). The median movement on the institution list was ranks and this is significantly different from 0 (Wilcoxon signed rank test = 7750, p < .00001). Two institutions were the biggest climbers, each climbing 55 ranks. The University of the Arts London rose from 84 to 29. Leeds Beckett University rose from 121 to 66. The largest drop, 51 places, was observed for the University of Buckingham, moving from rank 76 in 2019 down to rank 127 in 2024. There were 70 subjects included in the 2019 list (74 in 2024). Not all the 70 subjects were included in the 2024 list (not included: Anatomy and Physiology, Aural and oral sciences), leading to 68 subjects which could be matched. All but one subject, Complementary Medicine, showed significant changes in ranks between 2023 and 2024 (all Wilcoxon signed rank tests of median movement against 0, p < .05). 202 out of 3,185 items did not move rank (6.34%). The largest individual climber was the University of Wales Trinity Saint David in Drama, Dance, and Cinematics from rank 96 to 25. The largest drop was observed for Goldsmiths (University of London) in Business and management studies with a drop from rank 22 to rank 105. Materials Technology had the lowest median movement of all subjects (Median = 0.5 ranks, n = 10). Education had the greatest median movement (Median = 13 ranks, n = 78). The highest correspondence between 2019 and 2024 ranks was found in Land and Property management (Spearman ρ = .929, n = 7), the poorest correspondence was for Occupational Therapy (Spearman ρ = .154, n = 28). The vast majority of subject rankings were stable (Spearman ρ ≥ .667, for 58 out of 70 subjects), with more than half showing strong stability (Spearman ρ ≥ .8, for 39 out of 70 subjects).


Table 1 Table
1
RRA ranking and p valuesThe ten largest rank discrepancies between two ranking methods
Provider
Guardian (2024) RRA rank
p value (Benjamini
p value
& Yekutieli, 2001)
(Bonferroni)
Cambridge
3
< 0.00001
< 0.00001
Oxford
2
2
< 0.00001
< 0.00001
St Andrews
1
3
< 0.00001
< 0.00001
Leeds
27
4
< 0.00001
< 0.00001
UCL
8
5
< 0.00001
< 0.00001
Manchester
24
6
< 0.00001
< 0.00001
Edinburgh
14
7
< 0.00001
< 0.00001
Bristol
17
8
< 0.00001
0.000014103
Glasgow
13
9
0.000075146
0.00012558
Durham
7
10
0.00056034
0.00104050
Southampton
20
11
0.00586511
0.01197999
Swansea
25
12
0.00704198
0.01569148
Loughborough
10
13
0.01232522
0.02975267
Bath
6
14
0.08012644
0.20830112
Exeter
18
15
0.08755044
0.24385817
Note:
RRA = Robust Rank Aggregation. Institutions reported until p < .1 based on
Benjamini & Yekutieli (2001) procedure. Max. rank is 122.
Note: RRA = Robust Rank Aggregation. Max. rank is 122.


Ten universities did not move ranks between 2023 and 2024 (8.26%). The median movement on the institution list was five ranks and this is significantly different from 0 (Wilcoxon signed rank test = 6216, p < .00001). The biggest climbers were Portsmouth and SOAS with each climbing 34 ranks, respectively from 67 to 33 and from 86 to 52. The largest drop, 42 places, was observed for Keele University moving from rank 32 in 2023 to rank 74 in 2024.
The correspondence between the 24 Russell Group universities' rankings between the two methods is relatively poor (Spearman ρ = .574). 2023-2024 difference. One institution, Birmingham Newman University, was not included in the 2023 institution rankings and thus excluded from further calculations. The correspondence between an institution's rank for 2023 and 2024 was strong (Spearman ρ = .929).When we group the data by subject, for 3,556 items ranked we were able to calculate a rank difference between 2023 and 2024. There was strong correspondence between 2023 and UNIVERSITY RANKINGS 15


at p = .05. The University of Brighton was a significant climber based on the RRA method following Bonferroni correction (p Bonferroni = .045). The second ranked climber, the University of Portsmouth, was no longer a significant
climber after Bonferroni correction (p Bonferroni = .069). These two universities also were climbers on the Guardian University Guide institution list (Brighton 2023: 92, Brighton 2024: 70; Portsmouth 2023: 67, Portsmouth 2024: 33).


Benjamini&Yekutieli > .1, respective: p Bonferroni = .075, p Bonferroni = .094
. Only one
institution, the University of Edinburgh, was a significant climber (p Benjamini&Yekutieli <
.00001 and p Bonferroni < .00001). Other climbers were Cardiff, Ulster and Glasgow but not
significantly so (all > p


Benjamini&Yekutieli = .017 vs. p Bonferroni = .083). Two Russell Group Universities are not included in the list of significant entries, York (p Benjamini&Yekutieli = .292
UNIVERSITY RANKINGS
19
to universities which are listed in the complete university guide institution list (Spearman ρ
= .825).
Table 4 shows the rankings for all those universities ranked by the RRA method, as
well as their Complete University Guide 2024 institution ranking. The top 27 institutions are
ranked significantly better than chance with the Benjamini and Yekutieli (2001) method at p
< .05. The Bonferroni method leads to 26 universities being ranked. The discrepancy is for
Swansea University (p
Provider
Guardian (2019) Guardian (2024) RRA rank drop
p value (Benjamini
p value
& Yekutieli, 2001)
(Bonferroni)
Leeds
10
27
1
< 0.00001
< 0.00001
Nottingham
17
59
2
< 0.00001
< 0.00001
Birmingham
19
37
3
0.000030123
0.000016885
De Montfort
71
117
4
0.0015274
0.0011415
Nottingham Trent
16
42
5
0.0110738
0.0103452
Newcastle
27
67
6
0.0367210
0.0471327
Coventry
13
46
7
0.0367210
0.0480270
Note:
RRA = Robust Rank Aggregation. Institutions reported until p < .1 based on Benjamini & Yekutieli (2001)
procedure. Max. rank is 118.
824) and when restricting
Complete University Guide data 2024. The correspondence between the two ranking lists was good, both when including any institution ranked in the subject list (Spearman ρ = .


Table 4
4
RRA ranking and p values
Provider
Complete
RRA rank
p value (Benjamini
p value
University Guide
& Yekutieli, 2001)
(Bonferroni)
(2024)
Cambridge
1
1
< 0.00001
< 0.00001
Oxford
2
2
< 0.00001
< 0.00001
Edinburgh
12
3
< 0.00001
< 0.00001
Manchester
19
4
< 0.00001
< 0.00001
UCL
< 0.00001
< 0.00001
Birmingham
14
6
< 0.00001
< 0.00001
Bristol
16
7
< 0.00001
< 0.00001
Leeds
22
8
< 0.00001
< 0.00001
Nottingham
28
9
< 0.00001
< 0.00001
Durham
8
10
< 0.00001
< 0.00001
Bath
5
11
< 0.00001
< 0.00001
Glasgow
26
12
< 0.00001
< 0.00001
Sheffield
20
13
< 0.00001
< 0.00001
Exeter
15
14
< 0.00001
< 0.00001
St Andrews
4
15
< 0.00001
< 0.00001
Cardiff
21
16
< 0.00001
< 0.00001
Southampton
17
17
0.000014878
0.00004642
Liverpool
24
18
0.000031640
0.00010453
Loughborough
7
19
0.000041162
0.00014354
Warwick
11
20
0.000052721
0.00019352
Newcastle
30
21
0.000094760
0.00036522
King's College London
24
22
0.00012359
0.00049901
Imperial College London
6
23
0.00090960
0.00383968
Lancaster
10
24
0.00170966
0.00753071
Strathclyde
31
25
0.00641571
0.02943747
Queen's University Belfast
27
26
0.00956588
0.04564717
Swansea University
40
27
0.01674178
0.08296237
LSE
3
28
0.05962561
0.30641261
Surrey
13
29
0.09993079
0.53187929
Note: RRA = Robust Rank Aggregation. Institutions reported until p < .1 based on Benjamini & Yekutieli (2001) procedure. Max. rank is 130.


Table 5
5
The ten largest rank discrepancies between two ranking methods
Provider
Complete University Guide (2024) RRA rank Absolute rank difference
University of the Arts London
29
116
87
Harper Adams University
33
111
78
Norwich University of the Arts
56
118
62
Falmouth University
61
120
59
St George's, University of London
78
125
47
Arts University Bournemouth
72
117
45
University of Buckingham
127
83
44
University of Suffolk
80
123
43
University of Bradford
105
64
41
St Mary's University, Twickenham
86
127
41
Note: RRA = Robust Rank Aggregation. Max. rank is 130.


However, Ravensbourne University London was included in the 2023 list but not in the 2024 list. Conversely, Leeds Arts University was included in 2024 but not 2023. As with the Guardian's institution list, the correspondence between an institution's rank for 2023 and 2024 in the CUG list was strong (Spearman ρ = .966). Ten universities did not move ranks between 2023 and 2024 (7.75%). The median movement on the institution list was four ranks and this is significantly different from 0 (Wilcoxon signed rank test = 7140, p < .00001). The biggest climber was Leeds Beckett University climbing 40 ranks, respectively from 106 to 66. The largest drop, 31 places, was observed for the University of Bradford, moving from rank 74 in 2023 down to rank 115 in 2024.
UNIVERSITY RANKINGS
22
CUG (2024)
RRA ranking
Rank (lower = better)
LSE
Nottingham
0
When we group the data by subject, for 3,718 items ranked we were able to calculate a


Benjamini&Yekutieli < .0001, p Benjamini&Yekutieli < .0001, p Benjamini&Yekutieli = .013) or Bonferroni adjustment (p Bonferroni < .0001, p Bonferroni < .0001, p Bonferroni = .007). Both the Robert Gordon University and the University of Dundee also experienced notable drops, but not significantly so at p = .05 (respectively: p Benjamini&Yekutieli = .081, p Benjamini&Yekutieli _= .081 and p Bonferroni = .063, p Bonferroni = .074).
Three universities out of 130 ranked in total, i.e. Coventry University, De Montfort
University, University of Nottingham, experienced a significant drop based on the RRA
method. This conclusion is the same regardless of whether it is based on the Benjamini and
Yekutieli (2001) correction (p Seven universities were significant climbers regardless of whether Benjamini and Yekutieli
(2001)'s method or the Bonferroni method is used (University of Edinburgh, Cardiff
University, University of Plymouth, University of Manchester, University of Liverpool, Ulster
University, University of Leeds; all p's < .05, see for more details).














Missing data series: Quantitative applications in the social sciences




P
D
Allison








Sage


Thousand Oaks. CA












What makes students satisfied? A discussion and analysis of the UK's national student survey




A
R
Bell






C
Brooks




10.1080/0309877X.2017.1349886








Journal of Further and Higher Education




42


8
















Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing




Y
Benjamini






Y
Hochberg








Journal of the Royal Statistical Society


















10.2307/2346101








Series B (Methodological)




57


1














The control of the false discovery rate in multiple testing under dependency




Y
Benjamini






D
Yekutieli








Annals of Statistics




















10.1214/aos/1013699998














Students' interpretations of the meanings of questionnaire items in the National Student Survey. Quality in Higher Education




R
Bennett






S
Kane








20
















10.1080/13538322.2014.924786














Polls, forecasts, and aggregators




M
Blumenthal








PS: Political Science &#




38
















10.1017/S1049096514000055








Politics




47


2














Too much noise in the Times Higher Education rankings




F
Bookstein






H
Seidler






M
Fieder






G
Winckler








Scientometrics




85


1


















10.1007/s11192-010-0189-5














Fantasy higher education: University and college league tables. Quality in Higher Education




R
Bowden




10.1080/13538320050001063








6














Simpson's Paradox in Canadian Police Clearance Rates




S
Demers






D
K
Rossmo








Canadian Journal of Criminology and Criminal Justice




57


3


















10.3138/cjccj.2014-E29














Who ranks the university rankers?




M
Enserink




10.1126/science.317.5841.1026








Science




317


5841
















An Experimental Study of Unsupervised Rank Aggregation Methods in World University Rankings. 1-8




S
Feng






Q
Deng






S
Wang






L
Song






C
Liang




10.1109/IEIR59294.2023.10391254


















Student satisfaction, league tables and university applications: Evidence from Britain




S
Gibbons






E
Neumayer






R
Perkins




10.1016/j.econedurev.2015.07.002








Economics of Education Review




48
















Simpson's paradox and the analysis of memory retrieval




D
L
Hintzman




10.1037/0033-295X.87.4.398








Psychological Review




87


4
















We need to compare, but how? Measurement equivalence in comparative public administration




S
Jilke






B
Meuleman






S
Van De Walle




10.1111/puar.12318








Public Administration Review




75


1
















University rankings: What do they really show?




J
Johnes




10.1007/s11192-018-2666-1








Scientometrics




115


1
















Simpson's paradox in psychological science: A practical guide




R
A
Kievit






W
E
Frankenhuis






L
J
Waldorp






D
Borsboom




10.3389/fpsyg.2013.00513








Frontiers in Psychology




4














Robust rank aggregation for gene list integration and meta-analysis




R
Kolde






S
Laur






P
Adler






J
Vilo








Bioinformatics




28


4


















10.1093/bioinformatics/btr709














Content analysis: An introduction to its methodology




K
Krippendorff












2nd ed.). Sage publications








A comparative study of rank aggregation methods for partial and top ranked lists in genomic applications




X
Li






X
Wang






G
Xiao




10.1093/bib/bbx101








Briefings in Bioinformatics




20


1
















University rankings and social science




S
Marginson




10.1111/ejed.12061








European Journal of Education




49


1
















Arguments for Rejecting the Sequential Bonferroni in Ecological UNIVERSITY RANKINGS 33




M
D
Moran




















10.1034/j.1600-0706.2003.12010.x








Studies. Oikos




100


2














A farewell to Bonferroni: The problems of low statistical power and publication bias




S
Nakagawa








Behavioral Ecology




15


6


















10.1093/beheco/arh107














What's wrong with Bonferroni adjustments




T
V
Perneger




10.1136/bmj.316.7139.1236








BMJ




7139
















Blacks smoke less (and more) than whites: Simpson's paradox in US smoking rates




A
Persoskie






B
Leyva




10.1353/hpu.2015.0085








Journal of Health Care for the Poor and Underserved




26


3
















The ecological fallacy




S
Piantadosi






D
P
Byar






S
B
Green




10.1093/oxfordjournals.aje.a114892








American Journal of Epidemiology




127


5
















Consequences of arbitrary binning the midpoint category in survey data: An illustration with student satisfaction in the National Student Survey




T
V
Pollet






M
Bilalić






L
Shepherd








Studies in Higher Education




49


11


















10.1080/03075079.2023.2284808














Subscales in the national student survey (NSS): Some considerations on their structure




T
V
Pollet






L
Shepherd




10.1080/0309877X.2022.2060069








Journal of Further and Higher Education




46


9
















What can cross-cultural correlations teach us about human nature? Human Nature




T
V
Pollet






J
M
Tybur






W
E
Frankenhuis






I
J
Rickard




10.1007/s12110-014-9206-3








25




Hawthorne, N.Y.












R : A language and environment for statistical computing






R Foundation for Statistical Computing


Vienna, Austria










R Development Core Team.








Simpson's paradox: An example from hospital epidemiology




R
Reintjes






A
De Boer






W
Van Pelt






J
Mintjes-De Groot








Epidemiology




11


1


















10.1097/00001648-200001000-00017














Ecological correlations and the behavior of individuals




W
S
Robinson








American UNIVERSITY RANKINGS




34
















10.2307/2087176








Sociological Review




15


3














Species richness, species-area curves and Simpson's paradox




S
M
Scheiner






S
B
Cox






M
Willig






G
G
Mittelbach






C
Osenberg






M
Kaspari




10.1152/ajpendo.00279.2010








Evolutionary Ecology Research




2


6
















Nonparametric statistics for the behavioral sciences




S
Siegel






N
J
Castellan








McGraw-hill


New York, NY






2nd ed.








The Interpretation of Interaction in Contingency Tables




E
H
Simpson




10.1038/168063d0








Journal of the Royal Statistical Society. Series B (Methodological)




13


2
















Simpson's Paradox and confounding factors in university rankings: A demonstration using QS 2011-12 data




K
C
Soh




10.1080/21568235.2012.743223








European Journal of Higher Education




2


4
















Simpson's Paradox, Lord's Paradox, and Suppression Effects are the same phenomenon-the reversal paradox




Y.-K
Tu






D
Gunnell






M
S
Gilthorpe




10.1186/1742-7622-5-2








Emerging Themes in Epidemiology




5














Benchmarking in universities: League tables revisited




D
Turner




10.1080/03054980500221975








Oxford Review of Education




31


3
















Simpson's paradox in real life




C
H
Wagner




10.1080/00031305








The American Statistician




36


1
















Medical schools should stop cherry picking league table data




R
Wakeford




10.1136/sbmj.h1359








BMJ




350















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]