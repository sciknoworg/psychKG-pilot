You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Substance use disorders (SUDs) are a major public health risk. In the United States, lethal drug overdose is the leading cause of accidental deaths 
(Jones et al., 2013;
Rudd et al., 2016)
. Risky behavior, and loss of career, relationships, and other sources of well-being in SUDs, is thought to derive in part from dysfunctional decision-making processes. Maladaptive decisionmaking has been associated with negative personal long-term outcomes in SUDs, including relapse 
(Passetti et al., 2008;
Verdejo-Garcia et al., 2018)
. While evidence-based treatments are available, relapse rates are high, and patients often discontinue treatment 
(Connery, 2015;
Hser et al., 2014)
. Consequently, it is crucial to better understand computational processes promoting maladaptive choices within SUDs to improve treatment retention/success and reduce relapse rates.
A growing behavioral literature suggests that SUDs show decision-making impairments associated with a number of factors, including a focus on short-term outcomes, poor choice flexibility, differential learning from rewards and punishments, and memory deficits. For example, opioid users are less likely to predict distal future events and more likely to continue selecting actions with short-term rewards but larger delayed punishments (implying impaired learning to avoid suboptimal choices; 
(Petry et al., 1998)
. When compared to healthy controls (HCs), opioid users also show reduced sensitivity to losses paired with greater responses to known risks 
(Ahn et al., 2014)
. Moreover, opioid users perform more poorly than HCs while learning to avoid punishment under high memory load 
(Myers et al., 2017)
 and higher ambiguity tolerance predicts prospective opioid use 
(Konova et al., 2019)
. Both opioid and stimulant (cocaine and/or amphetamine) users are less likely than HCs to stick to successful decision strategies and instead: (a) choose to switch responses even when a previous response has been rewarding 
(Myers et al., 2016)
; or (b) perseverate on responses independent of outcomes 
(Kanen et al., 2019)
. Similar difficulties in flexibly adjusting behavior following punishments have also been reported in other stimulant user samples 
((Ersche et al., 2016;
Ersche et al., 2011)
; but see 
(Kanen et al., 2019)
). In contrast, stimulant users appear to exhibit heightened sensitivity to monetary reward 
(Ahn et al., 2014)
.
Individuals with multiple SUDs also show neuroimaging evidence of abnormalities during risk-taking decision-making that appear consistent with behavioral evidence 
(Gowin et al., 2013)
. For example, a general blunting of neural responses in stimulant users has been observed across several brain regions (as well as consistent behavioral differences) in response to negative affective stimuli signaling threat and/or punishment 
(Hester et al., 2013;
Stewart et al., 2014)
. These blunted responses could correspond to the reduced reflection on future outcomes and reduced sensitivity to action consequences observed behaviorally, and self-report measures also offer consistent evidence of lower sensitivity to punishment in stimulant users (as well as marijuana users; see 
(Simons and Arens, 2007;
Simons et al., 2008)
). Jointly, extant behavioral, neural, and self-report evidence thus suggests a pattern of reduced future thinking and a reduced ability to learn from negative outcomes in SUDs.
While these studies exemplify progress in identifying potentially meaningful differences, understanding of several aspects of aberrant decision-making in SUDs remains incomplete. One area in which further investigation may be useful is how individuals with SUDs solve what is known as the explore/exploit dilemma, which has recently been highlighted as of potential importance in psychiatric disorders 
(Addicott et al., 2017;
Linson et al., 2020)
. This dilemma arises in cases where decisions must first be taken to gather information about the environment, before exploiting knowledge of the environment to maximize reward. If an individual "overexploits," they will fail to learn better behavioral strategies (especially in a changing environment)and subsequently develop strong habits for less adaptive choices. Overexploration instead reflects an inefficient use of past experience to inform subsequent decisionmaking, and thus a suboptimal preference for information-seeking behavior. One factor determining the efficiency in using past experience is the rate at which individuals learn (i.e., update beliefs) about their environment after making a new observation. A higher learning rate will facilitate learning from negative and positive outcomes and induce a faster switch to exploitative behavior in a stable environment. One additional distinction concerns different explorative strategies. Specifically, individuals can simply act more randomly ("random exploration") to sample the outcomes of different choices; or they can strategically seek out observations that are expected to provide the most useful information ("goal-directed exploration"; 
(Wilson et al., 2014)
). Importantly, goal-directed exploration implies futureoriented cognition, in that behaviors are strategically chosen to gather information benefiting future choices one will need to make.
There is a small, but emerging literature on explore/exploit dynamics in in addiction. For example, nicotine smokers make fewer exploratory choices and evidence a higher learning rate 
(Addicott et al., 2012)
; moreover, more ingrained smoking habits are associated with greater cognitive effort when making exploratory choices 
(Addicott et al., 2014)
. Stimulant users make choices based primarily on recent outcomes (e.g., which could follow from an overly high learning rate 
(Harle et al., 2015)
), while individuals with alcohol use disorders show fewer strategic exploratory decisions than HCs 
(Morris et al., 2016)
. This is consistent with current models of dopaminergic function suggesting that increases in dopamine promote (energetic) exploratory behavior, and that the chronic use of dopaminergic drugs reduces dopaminergic efficacy, therefore reducing exploration 
(Beeler et al., 2012)
. While informative, this body of work on addiction remains in its infancy, with only one or two studies for a given substance. Replication will be necessary, and it remains unclear whether effects are substance-specific or common across SUDs. Further, the distinction between directed and random exploration has not been thoroughly addressed. Reduced future thinking and learning from negative outcomes in SUDs both suggest reduced directed exploration, but this remains to be established.
Recent work in one area of computational neuroscience, active inference, has focused on how individuals make decisions to actively infer and learn about the structure of their environment, distinguishing different mechanisms that affect the explore/exploit trade-off 
(Schwartenbeck et al., 2019)
. These mechanisms include differences in random exploration, separate learning rates for wins and losses, and goal-directed exploration. Because this computational framework allows testing for differences in each of these separate mechanisms, we chose to employ this modeling approach to investigate potential differences in these computational mechanisms between HCs and SUDs when solving the explore/exploit dilemmawith the aim of better distinguishing mechanisms best accounting for sub-optimal exploratory behavior.
Participants from the Tulsa 1000 project, a prospective longitudinal cohort study of HCs and treatment-seeking individuals with substance, mood/anxiety, and eating disorders, completed a three-armed bandit task; SUDs (N = 147) and HCs (N = 54) from this sample were extracted for analysis based on the presence versus absence of one or more SUD diagnoses 
(alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids)
. Based on the evidence for poor learning and future thinking in SUDs, we expected that, relative to HCs, SUDs would solve this dilemma sub-optimally and obtain less reward. Model-based analyses could disambiguate whether suboptimal performance in SUDs was due to greater random or directed exploration, greater learning rate for wins, lower learning rate for losses, and/or less sensitivity to new informationwhich could inform novel interventions more specifically targeting active learning strategies in SUDs. Between-group analyses were performed with all SUDs and HCs as well as subgroups propensity-matched on age, sex, and a measure of pre-morbid intelligence quotient (IQ) (SUDs: n = 49; HCs: n = 51). Based on prior work, we predicted that SUDs would exhibit lower directed exploration and lower learning rate for losses than HCs.


Methods


Participants
Participants were identified from the first 500 participants of the Tulsa 1000 (T1000) 
(Victor et al., 2018)
, a prospective longitudinal cohort study recruiting subjects based on the dimensional NIMH Research Domain Criteria framework. The T1000 study included individuals 18-55 years old, screened on the basis of dimensional psychopathology scores: Patient Health Questionnaire (PHQ-9 
(Kroenke et al., 2001
)) ≥ 10, Overall Anxiety Severity and Impairment Scale (OASIS 
(Norman et al., 2006)
) ≥ 8, and/or Drug Abuse Screening Test 
(DAST-10 (Bohn et al., 1991)
) score > 3. HCs did not show elevated symptoms or psychiatric diagnoses. Participants were excluded if they: (a) tested positive for drugs of abuse via urine screen, (b) met criteria for psychotic, bipolar, or obsessive-compulsive disorders, or reported (c) history of moderate-to-severe traumatic brain injury, neurological disorders, or severe or unstable medical conditions, (d) active suicidal intent or plan, or (e) change in medication dose within 6 weeks. Full inclusion/exclusion criteria are described in 
(Victor et al., 2018)
. The study was approved by the Western Institutional Review Board. All participants provided written informed consent prior to completion of the study protocol, in accordance with the Declaration of Helsinki, and were compensated for participation. ClinicalTrials.gov identifier: #NCT02450240.
Participants were grouped based on DSM-IV or DSM-5 diagnosis using the Mini International Neuropsychiatric Inventory (MINI) 
(Sheehan et al., 1998)
. This analysis focuses on treatment-seeking individuals with SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids) with or without comorbid depression and anxiety disorders (N = 147), and HCs with no mental health diagnoses (N = 54). Most SUDs were currently enrolled in a residential facility or maintenance outpatient program after completion of more intensive treatments (mean days abstinent = 92; SD = 56).  18 (37%) Note: Stimulants = amphetamine, methamphetamine, and/or cocaine.


Procedure
Participants underwent an intensive assessment for demographic, clinical and psychiatric features. We focus on the clinical measures indicated above, as well as the Wide Range Achievement Test (WRAT), a common measure of premorbid IQ 
(Johnstone et al., 1996)
. The complete list of assessments and references supporting their validity and reliability are provided in 
(Victor et al., 2018)
.
We employed a commonly used three-armed bandit task to assess decision dynamics in the context of the explore/exploit dilemma 
(Zhang and Yu, 2013)
 consisting of 20 blocks of 16 trials. Within each block, participants were informed that they could choose one of three bandits (slot machines), and that each bandit had a different probability of reward that was stable throughout the block, but that the probabilities changed at the start of each block. They were not informed about the probabilities. Thus, with each block the participant started with no knowledge of these probabilities, and, to maximize reward, they decided how many times to observe the outcomes of selecting each bandit (explore) before settling on the bandit believed to have the highest reward probability. Reward rates were fixed for all bandits in each block, and were generated from a Beta (2,2) distribution prior to the start of data collection. Identical reward rates were used across participants, with pseudorandomized block order (see 
Figure 1
).


Computational Modeling
To model task behavior, we adopted a Markov decision process (MDP) model under the active inference framework; for more details about the structure and mathematics of this class of models, see 
(Friston et al., 2017a;
Friston et al., 2017c;
Parr and Friston, 2017)
. We selected this approach because these models can test for differences in learning rates, random exploration, and goal-directed exploration 
(Schwartenbeck et al., 2019)
 and are therefore useful to address how optimal decision processes can lead to suboptimal behavior in SUDs as a result of suboptimal model parameter settings 
(Schwartenbeck et al., 2015)
.
For full modeling details and example simulations, see Supplementary Materials. The model is outlined in 
Table 3
; important vectors, matrices, and equations are shown in 
Figure 1
 and described in the legend. As described there, the model was defined by the choices (states and state transitions) available at each time point in the task, the observable outcomes of those choices (wins/losses), the choice-dependent reward probabilities, and the value of each possible outcome. There are several free model parameters that influence behavior: action precision (α), reward sensitivity ( ), learning rate ( ), and insensitivity to information ( 0 ). We estimated 10 different nested models, illustrated in 
Table 4
, each with different choices of which model parameters were estimated. Based on our interest in goal-directed exploration, was always estimated. We then performed Bayesian model comparison (based on 
(Rigoux et al., 2014;
Stephan et al., 2009)
) to determine the best model. Variational Bayes (variational Laplace; 
(Friston et al., 2007)
) was used to estimate parameter values that maximized the likelihood of each participant's responses, as described in 
(Schwartenbeck and Friston, 2016)
. This task is designed to quantify how individuals switch between an "exploration" and "exploitation" strategy. Participants had to sample from 3 different choice options (lotteries) with unknown probabilities of winning/losing with the goal of maximizing reward. The optimal strategy is to start by "exploring" (trying all possible options) to gain information about the probability of winning for each lottery, and then begin "exploiting" after a few trials by repeatedly choosing the lottery with highest reward probability. Participants performed a total of 20 games with a known number of trials (16) per gamecorresponding to 16 tokens, which had to be assigned to one of the three lotteries of their choice (white panels on left, right and middle of the screen). After placing each token, they earned 1 point if the token turned green or zero points if the token turned red. Each token decision lasted about 2 sec. After the button press, the chosen lottery became highlighted for 250ms, after which the token turned green or red to reveal the decision outcome. Participants were instructed to find the most rewarding lottery and maximize the points earned in each game. Participants were paid an additional $5 or $10 based on task performance. (B) Graphical depiction of the computational (Markov decision process) model used to model the task. Here, arrows indicate dependencies between variables such that observations (o) depend on hidden states (s), where this relationship is specified by the A matrix, and those states depend on both previous states (as specified by the B matrix, or the initial states specified by the D vector) and the sequences of actions (policies  


A ( | )
A matrix encoding the relationship between states and observations (one matrix per observation category).
1. A reward probability matrix:
(o |s ℎ )
2. An identity matrix for observed choice (entailing that participants had no uncertainty about the choice they made):
(o ℎ |s ℎ )
a Dirichlet priors associated with the A matrix that specify beliefs about the mapping from states to observations. Learning corresponds to updating the concentration parameters for these priors after each observation, where the magnitude of the updates is controlled by a learning rate parameter (see 
Supplementary  Materials and Figure 1)
.
Each entry for learnable reward probabilities began with a uniform concentration parameter value of magnitude 0 , and was updated after each observed win or loss on the task. The learning rate and 0 (which can be understood as a measure of sensitivity to new information; see Supplementary Materials) were fit to participant behavior.


B ( +1 | , )
A set of matrices encoding the probability of transitioning from one state to another given the choice of policy (π).
Transition probabilities were deterministic mappings based on a participant's choices such that, for example, ( 1 | , 1 ) = 1, and 0 for all other transitions, and so forth for the other possible choices.


C ( )
One vector per observation category encoding the preference (reward value) of each possible observation within that category.
The value of observing a win was a model parameter reflecting reward sensitivity; the value of all other observations was set to 0. The value of was fit to participant behavior. Crucially, higher values have the effect of reducing goaldirected exploration, as the probability of each choice (based on expected free energy ) becomes more driven by reward than by informationseeking (see 
Supplementary  Materials and Figure 1)
.


D ( =1 )
A vector encoding prior probabilities over states.
This encoded a probability of 1 that the participant began in the start state. A vector encoding the probability of selecting each allowable policy (one entry per policy). The value of each policy is determined by its expected free energy ( ), which depends on a combination of expected reward and expected information gain. Actions at each time point are chosen based on sampling from the distribution over policies, = ( ); the determinacy of action selection is modulated by an inverse temperature or action precision parameter α (see 
Supplementary  Materials and Figure 1
).
This included 3 allowable policies, corresponding to the choice of transitioning to each of the three bandit choice states. The action precision parameter α was fit to participant behavior.


Statistical Analyses
All analyses were performed in R. We used multiple regression analyses with each model parameter as the outcome variable and included age, sex, premorbid IQ, and group (SUDs versus HCs) as predictor variables. 
Table 1
 demonstrates that SUDs exhibited lower premorbid IQ and higher depression/anxiety (PHQ/OASIS) symptoms than HCs. 
Table 2
 illustrates that over half of SUDs met criteria for lifetime major depressive disorder (MDD), with almost half meeting criteria for two or more lifetime MDD, anxiety and/or stress disorders. We took two steps to help address these potential confounds. First, we reran analyses after propensity-matching (resulting in 51 HC and 49 SUD that did not differ significantly on age, sex or premorbid IQ; see 
Table 1
). Second, we ran within-SUDs correlations between model parameters and each of the following to assess whether direction of these relationships could provide an alternative interpretation of our results: (a) PHQ; (b) OASIS; and (3) premorbid IQ.
We also ran a confirmatory parametric empirical Bayes (PEB) analysis 
(Friston, K.J. et al., 2016)
, using standard MATLAB routines (see software note), computing group posterior estimates that incorporate posterior variances of individual-level parameter estimates when assessing evidence for group-level models with and without the presence of group differences.
To assess relationships between model parameters and model-free metrics of task behavior, we calculated: (a) total number of wins and mean reaction times (RTs; trimmed to remove the upper and lower 10%); (b) number of stays vs. shifts in bandit selection after win and loss outcomes. Next, we ran correlations between model parameters and each of these modelfree metrics and performed two-sample t-tests to assess group differences. For strategy differences, we examined the first and second halves of the games separately (i.e., first 7 choices vs. final 8 choices) to assess periods wherein exploration vs. exploitation would be expected to dominate.


Results
Out of the 10 nested computational models we estimated 
(Table 4)
, the model including action precision, reward sensitivity, separate learning rates for wins and losses, and insensitivity to information was the best model (protected exceedance probability = 1). On average, this model accurately predicted true actions on 60% of trials (SD = 11%); SUDs = 59% (SD = 10%), HCs = 62% (SD = 12%). Average probability assigned to true actions by this model was .53 (SD = .1); SUDs = .52 (SD = .09), HCs = .56 (SD = .12). Note that chance accuracy = 1/3.  
Table 5
 presents group descriptive statistics for both samples, while 
Figure 2
 depicts significant group differences in computational model parameters for the entire sample. Within the entire sample, SUDs exhibited (a) lower action precision (t = 2.9, p = .004), (b) higher learning rate for wins (t = 2.1, p = .02), and (c) higher learning rate for losses (t = 2.4, p = .02) than HCs. Groups did not differ in reward sensitivity or insensitivity to information. With respect to other predictors, higher age was linked to (a) higher reward sensitivity (t = 3.2, p = .002), (b) lower learning rate (t = 2.8, p = .007), and (c) less sensitivity to new information (t = 2.7, p = .008). Higher IQ was also linked to lower learning rate for losses (t = 2.7, p = .007). 
Table 5
 indicates that group difference results for action precision and win/loss learning rates were also significant after propensity matching for age, sex, and premorbid IQ. Bayesian (PEB) analyses indicated that, in the full sample, the winning model provided positive evidence for the group difference in action precision (posterior probability = .81) and learning rate for losses (posterior probability = .88); the effect was stronger for the difference in learning rate (illustrated in 
Figure 2
). If age, sex, and IQ were included in the model, only the difference in learning rate for losses was retained, and the evidence for this group difference became stronger (posterior probability = 1). In the propensity-matched sample, the winning model retained the group difference in learning rate for losses, with strong evidence (posterior probability = .97). It also included the group difference in action precision (weak evidence; posterior probability = .45) and learning rate for wins (positive evidence; posterior probability = .75).  
Table 6
 lists group descriptive statistics in model-free behavioral measures for both samples. 
Figure 2
 indicates that SUDs achieved fewer wins than HCs, although groups did not differ in RTs or their use of win-stay/lose-shift strategies. However, during early trials, when exploratory behavior would be expected to dominate, SUDs in the propensity-matched sample made more lose/stay choices than HCs (t(91) = 2.23, p = .03, Cohen's d = 0.45). *When only examining early trials in each game (i.e., first 7 choices), this difference was significant at p = 0.03.
Across all participants, faster RTs were associated with greater reward sensitivity (r = -.37, p < .001), higher learning rate for wins (r = -.26, p < .001), lower learning rate for losses (r = .30, p < .001), and less sensitivity to information (r = -.22, p = .002). A greater number of wins was associated with greater action precision (r = .27, p < .001) and greater reward sensitivity (r = .48, p < .001). For relationships between model parameters and win stay/shift vs. lose stay/shift strategies, see 
Supplementary Figure S2
. As shown there: (a) higher action precision promoted greater numbers of stays in win trials; (b) higher learning rate for losses promoted shifts on loss trials (whereas learning rate for wins had the opposite influence); and (c) both higher reward sensitivity and lower sensitivity to information promoted stay behavior.
Within SUDs, we observed negative correlations between: (a) reward sensitivity and OASIS (r = -.19, p = .02); and (b) insensitivity to information and both OASIS (r = -.17, p = .04) and PHQ (r = -.18, p = .03). However, these results did not survive correction for multiple comparisons. No relationships were observed between other parameters and clinical measures in SUDs, although premorbid IQ showed significant associations with learning rate for wins (r = .22, p = .01) and losses (r = -.25, p = .004).


Discussion
We used a computational framework that dissociated between goal-directed information seeking and random exploration, alongside differences in learning rates, to examine whether differences in these parameters could shed light on which of several possible computational failure modes best accounts for poor decision-making in SUDs when solving the explore/exploit dilemma. While SUDs won less often than HCs, reward sensitivity and insensitivity to informationboth of which influence goal-directed explorationdid not differ between groups. In contrast, SUDs exhibited lower action precision, greater learning rate for rewards, and lower learning rate for losses than HCswith Bayesian analyses finding the strongest evidence for the group difference in learning rate for losses.
Our finding that lower action precision was associated with fewer wins, and a greater number of shifts to a new choice after a win, suggest a failure of SUDs to settle on a behavior strategy despite sufficient evidence. This appears consistent with previous work suggesting that substance users are less likely than HCs to stick to successful decision strategies 
(Kanen et al., 2019;
Myers et al., 2016)
. Future work will be necessary to better understand the possible bases of this difference (e.g., underconfidence, distractibility, reduced awareness, etc.). Our finding that, relative to HCs, individuals with SUDs show attenuated learning rate for losses also appears consistent with neural and self-report results suggesting diminished responses to negative stimuli in SUDs (i.e., under the assumption that learning about a stimulus is facilitated by stronger affective or salience-based responses to that stimulus; 
(Hester et al., 2013;
Simons and Arens, 2007;
Simons et al., 2008;
Stewart et al., 2014)
). This result, as well as the greater learning rate we observed for wins, also supports previous work in SUDs demonstrating a lower impact of large losses on future choices (opioid users; 
(Petry et al., 1998)
), reduced sensitivity to losses (opioid users; 
(Ahn et al., 2014)
), greater sensitivity to reward (stimulant users; 
(Ahn et al., 2014)
), and difficulty avoiding punishment (opioid users; 
(Myers et al., 2017)
).
Unlike our model-based results, model-free analyses of RTs and behavioral strategy revealed few significant group differences, which may be due to the fact that different computational strategies can lead to similar summary statistics. This highlights the potential utility of our computational approach in its ability to pick up on potentially important differences in the mechanisms whereby individuals with SUDs differ in decision-making from HCs. Aside from fewer wins, the only significant model-free finding was that SUDs showed a greater number of lose-stay choices in early trials. This is consistent with our finding that individuals with SUDs learn more slowly from losses -hindering the ability to "lock on to" the most optimal choice during exploration. Together, our findings could be taken to suggest that, in the face of uncertainty, individuals with SUDs persist in making poor choices (at least in part) because they do not appropriately update their beliefs when drug use leads to negative consequencesand that, even in the face of positive outcomes, they fail to reliably adopt the actions that produce them. While supportive of previous findings, this study was also distinct in using an active inference (active information-seeking) model to disambiguate different possible mechanisms that may be affected while solving the explore/exploit dilemma. In our computational model, lower reward sensitivity values and higher sensitivity to information both promote goal-directed exploration in different ways (i.e., sensitivity to information is more prominent in cases of high uncertainty), whereas low action precision promotes random exploration (which can reflect several factors, including simple computational noise; 
(Findling et al., 2019)
). SUDs and HCs did not differ on either reward sensitivity or information sensitivity, suggesting no difference in the use of goal-directed information seeking. Random exploration may therefore be of greater relevance.
If our results can be replicated, it may be useful to explore the potential utility of designing interventions focused on facilitating attention to, and learning from, negative outcomes. Learning rates are thought to be modulated by estimates of environmental volatilitysuch that learning rates should be lower in more stable environments to avoid learning from random outcomes 
(Lawson et al., 2017;
Mathys et al., 2014;
Sales et al., 2019;
Sutton and Barto, 1998)
. A lower learning rate for losses may indicate that SUDs believe losses are explained more by chance, as opposed to by a consistent relationship with their past behavior (i.e., unexpected losses are treated as noise instead of signal). As such, it would be helpful to test whether interventions focused on helping substance users more explicitly see poor outcomes as reliable consequences of their actions could help address this.
This study is not without limitations. While we chose a particular modeling frameworkmotivated by the natural distinction between different forms of exploration and learning afforded by the active inference approachother models could also be used to examine behavior. It is worth noting here that, if the goal-directed information-seeking component were removed from active inference, the resulting model would become a simple model-based reinforcement learning model. While we compared several nested models, we were also required to choose prior parameter values. However, correlations between parameters and RTs, as well as the model's accuracy in predicting behavior, both support its validity. Finally, our SUD group was heterogenousincluding various, often comorbid drugs of choice and combinations of lifetime emotional disorders. Through propensity-matching sub-groups and examining within-group depression/anxiety symptoms, we believe that this issue has been adequately addressed.
In summary, sub-optimal explore/exploit decisions in SUDs appear to be due to both inconsistent choices (especially in the face of positive outcomes) and sub-optimal learning rates for rewarding vs. non-rewarding outcomes. These results may help explain the difficulty in adjusting to more adaptive patterns of behavior in SUDs. Future work should examine ways of facilitating substance users' abilities to learn the relationships between poor choices and negative outcomes and perhaps ways of increasing consistency in healthy behaviors after reinforcing outcomes.
Software Note: All model simulations and parametric empirical Bayes analyses were implemented using standard routines (spm_MDP_VB_X.m, spm_dcm_peb.m, spm_dcm_peb_bmc.m) that are available as Matlab code in the latest version of SPM academic software: http://www.fil.ion.ucl.ac.uk/spm/. Matlab code specifying the generative model of the


Supplementary Materials


Computational modeling details
To model behavior on the three-armed bandit task, we adopted a Markov decision process (MDP) model under the active inference framework (see main text 
Figure 1)
; for more details about the structure and mathematics of this class of models, see 
(Friston et al., 2017a;
Friston et al., 2017c;
Parr and Friston, 2017)
. This approach requires creating a model with specific sets of observable stimuli (o; in our model, a "starting" observation, reward vs. no reward, and the choices made by a participant), beliefs about the states of the task (s; in our model, a "starting" state as well as the state of having chosen each bandit), and beliefs about the sequences of actions (policies; ) that can be chosen (in our model, this defined the option to choose any of the three bandits on each trial). The relationships between these variables at a time (t) are described by a set of matrices. The A matrix encodes the way task states are related to observations, ( | ); in our model, these define the probability of observing reward vs. no reward given the state of having chosen each bandit (as well as an identity matrix ensuring perfect knowledge of each action chosen):
( ) = ( | ) = [ 1 0 0 0 0 1 2 3 0 1 − 1 1 − 2 1 − 3 ]
Here, columns indicate (from left to right) the starting state and bandits 1, 2, and 3, where the rows (from top to bottom) indicate the starting observation, observing reward, or no reward (first matrix). There was also an A-matrix mapping each bandit state to the observation of that choice, which was set as an identity matrix (i.e., there was no uncertainty in the choice a participant made) and the observation that each choice has been made.
The B matrix encodes the probability that one task state will transition into another depending on selected policies ( +1 | , ); in our model, this defined the transition from the "starting state" to the state of having chosen each possible bandit under each respective policy. Here, the transition probabilities where simply a deterministic mapping based on subject's choices, such that, for example, P(s bandit 1 |s , π bandit 1 ) = 1 and 0 for all other transitions.
The value of an observation is encoded within a vector referred to as the C vector; in our model, a value of 0 was fixed for all observations except for "reward", the magnitude of which was estimated based on participant behavior as an index of reward sensitivity ( ):
( ) = [0 0]′
These preferences are defined as a participant's log-expectations over observations. These C values are passed through a softmax function and correspond to log probabilities. For example, if = 4, this would indicate the expectation that observing reward is exp(4) ≈ 55 times more likely than observing no reward, exp(0) = 1.
A vector D = [1 0 0 0]' specified a prior over initial states, such that the participant always started at an undecided state at the start of each trial.
In active inference, policies are assigned value, and subsequently selected, based on the statistical quantity of expected free energy (G). Policies with lower G have higher value and a higher probability of being selected, where G can be decomposed in the following manner:
π = ∑(o , ⋅ (ln o π,t − ln ) − s , ⋅ s π,t ) ∶= 1 2 ( ⨀(−1) − 0 ⨀(−1) )
Here, denotes the current concentration parameters of Dirichlet (Dir) priors over reward probabilities associated with the A matrix, and 0 corresponds to at t = 0, when all reward probabilities are uniform (see further information in the description of learning below). The ⨀ symbol here denotes element-wise power. This formulation shows that policies will have higher value if 1) they minimize the divergence between predicted and preferred outcomes -o , ⋅ (ln o π,t − ln )which can be thought of as maximizing reward probability; and if 2) they seek out states expected to provide the most informative observations about the reward probabilities -, ⋅ , -which can be thought of as goal-directed information-seeking. For a full derivation, see 
(Da Costa et al., 2020)
. Crucially, higher values have the effect of reducing goal-directed exploration, as G become more driven by reward value (C) than by the value of information. For a more detailed mathematical treatment, see 
(Friston, K. et al., 2016;
Friston et al., 2017a)
; for its implementation within our model, see the spm_MDP_VB_X MATLAB routine, freely available within the DEM (dynamic expectation maximization) toolbox of the most recent versions of SPM academic software (http://www.fil.ion.ucl.ac.uk/spm/). To illustrate the effect of the information value term in G, in 
Supplementary Figure S1
 we show example simulations comparing full model performance to a model where the information value term has been removed.
The final value distribution over policies after each observation is then:
= ( )
Where indicates a softmax operator that converts into a proper probability distribution. Finally, actions at each time point are chosen based on sampling from the resulting distribution, where the precision in action selection is modulated by an inverse temperature or action precision parameter α:
( | ) = ( ⋅ ln ( | ))
Learning within this model corresponds to updating the concentration parameters of Dirichlet (Dir) priors associated with the A matrix (a) that specify beliefs about the mapping from states to observations. At t = 0: Here ⊗ indicates the cross-product, and 0 is the prior value for concentration parameters, where higher values prior to learning (i.e., prior values for an initially flat distribution) encode (inverse) sensitivity to information, such that higher values will reduce information-seeking behavior aimed at increasing the accuracy of beliefs about state-outcome mappings. The variable is a learning rate that controls the magnitude of updates in after each observation. This rate can also differ for different observations. Based on our model, there are therefore several free parameters that could influence participant behavior: action precision (α), reward sensitivity ( ), learning rate ( ), and prior value for concentration parameters ( 0 ; henceforth referred to as insensitivity to information). To arbitrate between different model choices, we estimated 10 different nested modelseach with different choices in what model variables were included (or fixed at default values) and which to estimate. 
Table 3
 in the main text shows each model, as well as the default values used for each parameter if not estimated. Note that, based on our primary interest in goal-directed exploration vs. exploitation, was always estimated. We then performed Bayesian model comparison (based on 
(Rigoux et al., 2014;
Stephan et al., 2009)
) to determine the best model.
(
Example simulations under different parameter settings are shown in 
Supplementary Figure S1
. 
Figure S1.
 (A) Example model simulation of one game with and without the information value term included in policy valuation. Reward probabilities for bandits 1-3 in this game are 0.46, 0.49, and .64 (respectively). Darker shades indicate higher choice probabilities; blue circles indicate the action taken; red and green circles indicate losses and wins (respectively). While the agent on the left panel is driven by both reward maximization and uncertainty reduction, the agent on the right panel only cares about reward. This induces subtle differences in predictions for behavior that are visible, for example, at time step three. Here, after having observed one rewarding and one non-rewarding outcome in bandit three, the agent on the left now prefers to minimize uncertainty about the other two bandits, whereas the agent on the right equally prefers the three bandits because they all have a reward value of 0.5. Here, action precision (AP; lower values promoting random exploration) was set to a high value of 16 to highlight the effects of goal-directed exploration. Reward sensitivity (RS; lower values promote goal-directed exploration) was set to 4, learning rates were set to 0.5, and the prior concentration parameters in the observation model, governing sensitivity to information, were defined as 0.25. (B) Example model simulations under single changes from the above-stated parameter values. As can be seen here, reduced RS leads to over-exploration, while reduced sensitivity to information leads to behavior similar to the reward only model. Reduced AP leads to more stochastic behavior, and reduced learning rate leads to less confident choices in later trials. 
Figure S2
. Correlations between model parameters and decision strategies. AP = action precision, RS = reward sensitivity, LR = learning rate, SI = (inverse) sensitivity to information.


Supplemental Figures
Figure 1 .
1
(A) Illustration of the task interface (for each of three choices, green circle = win; red circle = loss).


; π) selected by the agent. Here, D = [1 0 0 0]', such that the participant always started in an undecided state at the beginning of each trial. The probability of selecting each policy in turn depends on the expected free energy (G) of each policy with respect to the prior preferences (C vector) of the participant. These preferences are defined as a participant's logexpectations over observations. These C values are passed through a softmax function and correspond to log probabilities. For example, if = 4, this would indicate the expectation that observing reward is ( ) ≈ times more likely than observing no reward, ( ) = . When actions are sampled from the posterior distribution over policies, randomness in chosen actions is controlled by an inverse temperature parameter ( ), as depicted in the equation shown in the top left. (C) Depicts the A matrix learned by the agent (encoding probability of reward given choice of bandit) and the C vector encoding the preference (of magnitude cr) for reward. Here, a0 indicates beliefs about reward probabilities at time t = 0, before observing the outcomes of any action. Dir(A) indicates a Dirichlet prior over the state-outcome mappings in A, such that Dirichlet concentration parameters can encode confidence in learned reward probabilities. (D) Learning involves accumulating concentration parameters (a) based on outcomes observed after each choice of action. Learning rate is controlled by as depicted in the displayed equation. Here ⊗ indicates the cross-product. (E) Policies are evaluated byG (lower G indicates a higher policy value), which can in this case be decomposed into two terms. The first term maximizes reward (as in a reinforcement learning model), by minimizing the divergence between predicted outcomes and rewarding outcomes. The second term maximizes information gain (goaldirected exploration) by assigning higher values to policies that are expected to produce the most informative observations (i.e., the greatest change in beliefs about reward probabilities; based on a novelty term, ∶= ( ⨀(− ) − ⨀(− ) ), where ⨀ denotes element-wise power). For more details regarding the associated mathematics, see supplemental materials as well as(Da Costa et al., 2020;
Friston et al., 2017b;
Friston et al., 2017c)
.


Figure 2 .
2
Left: Means and standard errors for significant group differences in model-based and model-free measures. HCs = healthy controls, SUDs = substance use disorders. Data displayed is based on the full sample of 54 HCs and 147 individuals with SUDs. Right: Results of parametric empirical Bayes (PEB) analyses, showing the posterior means and variances for group difference estimates in the full and propensity-matched samples. These Bayesian group comparisons largely confirmed the mean group difference effects found in frequentist analyses; they also indicated a particularly pronounced group difference in the learning rate for losses when taking the individual posterior variances of parameter estimates into account. The model with the most evidence only retained the difference in these parameters, which is why other parameters have 0 values. Action precision, Reward Sensitivity, and Information Sensitivity values are in log-space. Learning Rate values are in logit-space.


Table 1 lists group demographics and symptom severity, whereas Table 2 lists diagnosis frequency within SUDs. Table 1. Descriptive Statistics (Means and Standard Deviations) for Demographic and Clinical Measures by Group Full Sample HCs SUDs
DAST
0.11 (0.37)
7.54 (2.22) <0.001
PHQ
0.80 (1.28)
6.58 (5.70) <0.001
OASIS
1.35 (1.94)
5.84 (4.63) <0.001
WRAT
63.53 (4.93)
58.47 (5.85) <0.001
Regular nicotine smoker*
8 (15%)
54 (37%) <0.001
Propensity Matched
HCs
SUDs
p
N=
51
49
Age
32.35 (11.40) 32.25 (7.72) 0.96
Sex (Male)
0.45 (0.50)
0.55 (0.50)
0.32
DAST
0.12 (0.38)
7.57 (2.36) <0.001
PHQ
0.78 (1.30)
7.12 (5.19) <0.001
OASIS
1.31 (1.92)
6.98 (4.55) <0.001
WRAT
63.53 (4.93) 61.76 (5.06) 0.08
Regular nicotine smoker*
8 (16%)
18 (37%) <0.001
* defined as >3650 lifetime cigarettes. DAST = Drug Abuse Screening Test. PHQ = Patient
Health Questionnaire. OASIS = Overall Anxiety Severity and Impairment Scale. WRAT = Wide
Range Achievement Test.
p
N=
54
147
Age
32.27 (11.35) 34.05 (9.17)
0.26
Sex (Male)
0.44 (0.50)
0.49 (0.50)
0.57


Table 2 . Lifetime DSM-IV/DSM-5 psychiatric disorders within SUDs SUDs (n = 147) Propensity
2
-Matched
SUDs (n = 49)


Table 3 . Computational model description
3
Model
General Description
Model specification
element
One vector per category of possible
Possible observations for
observations. Each vector contains
reward:
entries corresponding to possible
1. Start
observable stimuli for that category
2. Reward
at time t.
3. No reward
Possible observations for choice:
1. Start
2. Bandit 1
3. Bandit 2
4. Bandit 3
A vector containing entries
Possible choice states:
corresponding to the probability of
1. Start
each possible state that could be
2. Bandit 1
occupied at time t.
3. Bandit 2
4. Bandit 3


Table 4 . Nested models
4
α
Parameter:
(action precision)
(reward sensitivity) (learning rate)
(insensitivity to information)
Default value if not estimated
4
(always estimated)
(removed from model) 0.25
Prior means during estimation* 4
4
0.5
0.25
Model 1
Y
Y
N
N
Model 2
Y
Y
Y
N
Model 3
Y
Y
Y
Y
Model 4
N
Y
Y
Y
Model 5
N
Y
Y
N
Model 6
N
Y
N
N
Model 7
N
Y
N
Y
**Winning model


Table 5 . Model Parameters by Group (Means and Standard Deviations)
5
Full Sample
HCs
SUDs
p*
Cohen's d
N=
54
147
Action Precision
2.59 (0.88) 2.18 (0.58) 0.004
0.43
Reward Sensitivity
4.43 (1.44) 4.26 (1.42) 0.85
Learning rate (Wins)
0.48 (0.12) 0.50 (0.13) 0.04
0.31
Learning rate (Losses)
0.42 (0.13) 0.38 (0.15) 0.02
0.36
Insensitivity to Information 0.76 (0.29) 0.81 (0.30) 0.27
Propensity Matched
HCs
SUDs
p
Cohen's d
N=
51
49
Action Precision
2.60 (0.9) 2.17 (0.59) 0.005
0.57
Reward Sensitivity
4.38 (1.45) 4.37 (1.56) 0.98
Learning rate (Wins)
0.47 (0.12) 0.53 (0.12) 0.02
0.46
Learning rate (Losses)
0.41 (0.13) 0.34 (0.16) 0.01
0.52
Insensitivity to Information 0.77 (0.28) 0.84 (0.30) 0.26
*within a linear model including Age, Sex, and WRAT scores


Table 6 . Model-Free Measures of Task Behavior by Group (Means and Standard Deviations)
6
Full Sample
HCs
SUDs
p
Cohen's d
N=
54
147
Wins
183.59 (12.08)
179.33 (13.01)
0.03
0.33
Reaction Time
0.61 (0.24)
0.58 (0.25)
0.38
Win/Stay
136.54 (32.46)
130.20 (36.44)
0.26
Win/Shift
35.41 (28.04)
37.48 (29.51)
0.70
Lose/Stay
42.17 (26.54)
45.89 (30.22)
0.43
Lose/Shift
85.89 (28.01)
86.43 (32.10)
0.91
Propensity Matched
HCs
SUDs
p
Cohen's d
N=
51
49
Wins
183.25 (12.34)
178.16 (13.60)
0.05
0.39
Reaction Time
0.62 (0.24)
0.55 (0.25)
0.18
Win/Stay
136.24 (32.75)
131.10 (38.92)
0.48
Win/Shift
35.47 (28.00)
35.63 (32.46)
0.98
Lose/Stay
41.43 (27.00)
52.45 (33.87)
0.08*
Lose/Shift
86.86 (28.47)
80.82 (34.30)
0.34








Acknowledgment:
The authors would like to thank Edda Bilek for her suggestions regarding the PEB analyses.
Funding: This work has been supported in part by The William K. Warren Foundation, the National Institute of Mental Health (K23MH112949 (SSK), K23MH108707 (RLA)), and the National Institute of General Medical Sciences Center Grant, P20GM121312.






Conflict of Interest:
None of the authors have any conflicts of interest to disclose.
 










Smoking withdrawal is associated with increases in brain activation during decision making and reward anticipation: a preliminary study




M
A
Addicott






D
A
Baranger






R
V
Kozink






M
J
Smoski






G
S
Dichter






F
J
Mcclernon








Psychopharmacology (Berl)




219


2
















Smoking automaticity and tolerance moderate brain activation during explore-exploit behavior




M
A
Addicott






J
M
Pearson






B
Froeliger






M
L
Platt






F
J
Mcclernon








Psychiatry Res




224


3
















A Primer on Foraging and the Explore/Exploit Trade-Off for Psychiatry Research




M
A
Addicott






J
M
Pearson






M
M
Sweitzer






D
L
Barack






M
L
Platt








Neuropsychopharmacology




42


10
















Decisionmaking in stimulant and opiate addicts in protracted abstinence: evidence from computational modeling with pure users




W
Y
Ahn






G
Vasilev






S
H
Lee






J
R
Busemeyer






J
K
Kruschke






A
Bechara






J
Vassileva








Front Psychol




5


849














Putting desire on a budget: dopamine and energy expenditure, reconciling reward and resources




J
A
Beeler






C
R
Frazier






X
Zhuang








Front Integr Neurosci




6


49














Validity of the Drug Abuse Screening Test (DAST-10) in inpatient substance abusers. Problems of drug dependence 119




M
Bohn






T
Babor






H
Kranzler




















Medication-assisted treatment of opioid use disorder: review of the evidence and future directions




H
S
Connery








Harv Rev Psychiatry




23


2


















L
Da Costa






T
Parr






N
Sajid






S
Veselic






V
Neacsu






K
Friston




2020. ACTIVE INFERENCE ON DISCRETE STATE-SPACES -A SYNTHESIS. arXiv










07203v07202 [q-bio.NC








Carrots and sticks fail to change behavior in cocaine addiction




K
D
Ersche






C
M
Gillan






P
S
Jones






G
B
Williams






L
H
Ward






M
Luijten






S
De Wit






B
J
Sahakian






E
T
Bullmore






T
W
Robbins








Science




352


6292
















Response perseveration in stimulant dependence is associated with striatal dysfunction and can be ameliorated by a D(2/3) receptor agonist




K
D
Ersche






J
P
Roiser






S
Abbott






K
J
Craig






U
Muller






J
Suckling






C
Ooi






S
S
Shabbir






L
Clark






B
J
Sahakian






N
A
Fineberg






E
V
Merlo-Pich






T
W
Robbins






E
T
Bullmore








Biol Psychiatry




70


8
















Computational noise in reward-guided learning drives behavioral variability in volatile environments




C
Findling






V
Skvortsova






R
Dromnelle






S
Palminteri






V
Wyart








Nat Neurosci




22


12
















Active inference and learning




K
Friston






T
Fitzgerald






F
Rigoli






P
Schwartenbeck






J
Doherty






G
Pezzulo








Neuroscience and biobehavioral reviews




68
















Active Inference: A Process Theory




K
Friston






T
Fitzgerald






F
Rigoli






P
Schwartenbeck






G
Pezzulo








Neural Computation




29
















Active Inference, Curiosity and Insight




K
Friston






M
Lin






C
Frith






G
Pezzulo






J
Hobson






S
Ondobaka








Neural Computation




29
















Variational free energy and the Laplace approximation




K
Friston






J
Mattout






N
Trujillo-Barreto






J
Ashburner






W
Penny








Neuroimage




34


1
















The graphical brain: Belief propagation and active inference




K
Friston






T
Parr






B
De Vries








Network Neuroscience




1
















Bayesian model reduction and empirical Bayes for group (DCM) studies




K
J
Friston






V
Litvak






A
Oswal






A
Razi






K
E
Stephan






B
C M
Van Wijk






G
Ziegler






P
Zeidman








NeuroImage




128
















Altered risk-related processing in substance users: imbalance of pain and gain




J
L
Gowin






S
Mackey






M
P
Paulus








Drug Alcohol Depend




132


1-2
















Altered Statistical Learning and Decision-Making in Methamphetamine Dependence: Evidence from a Two-Armed Bandit Task




K
M
Harle






S
Zhang






M
Schiff






S
Mackey






M
P
Paulus






A
J
Yu








Front Psychol




6














The influence of monetary punishment on cognitive control in abstinent cocaine-users




R
Hester






R
P
Bell






J
J
Foxe






H
Garavan








Drug Alcohol Depend




133


1
















Treatment retention among patients randomized to buprenorphine/naloxone compared to methadone in a multi-site trial




Y
I
Hser






A
J
Saxon






D
Huang






A
Hasson






C
Thomas






M
Hillhouse






P
Jacobs






C
Teruya






P
Mclaughlin






K
Wiest






A
Cohen






W
Ling








Addiction




109


1
















The comparability of the WRAT-R reading test and NAART as estimates of premorbid intelligence in neurologically impaired patients




B
Johnstone






C
D
Callahan






C
J
Kapila






D
E
Bouman








Arch Clin Neuropsychol




11


6
















Pharmaceutical overdose deaths, United States




C
M
Jones






K
A
Mack






L
J
Paulozzi








JAMA




309


7
















Computational modelling reveals contrasting effects on reinforcement learning and cognitive flexibility in stimulant use disorder and obsessive-compulsive disorder: remediating effects of dopaminergic D2/3 receptor agents




J
W
Kanen






K
D
Ersche






N
A
Fineberg






T
W
Robbins






R
N
Cardinal








Psychopharmacology (Berl)




236


8
















Computational Markers of Risky Decision-making for Identification of Temporal Windows of Vulnerability to Opioid Use in a Real-world Clinical Setting




A
B
Konova






S
Lopez-Guzman






A
Urmanche






S
Ross






K
Louie






J
Rotrosen






P
W
Glimcher








JAMA Psychiatry
















The PHQ-9: validity of a brief depression severity measure




K
Kroenke






R
L
Spitzer






J
B
Williams








J Gen Intern Med




16


9
















Adults with autism overestimate the volatility of the sensory environment




R
Lawson






C
Mathys






G
Rees








Nature Neuroscience




20
















Active inference, stressors, and psychological trauma: A neuroethological model of (mal)adaptive explore-exploit dynamics in ecological context




A
Linson






T
Parr






K
J
Friston








Behav Brain Res




380


112421














Uncertainty in perception and the Hierarchical Gaussian Filter




C
D
Mathys






E
I
Lomakina






J
Daunizeau






S
Iglesias






K
H
Brodersen






K
J
Friston






K
E
Stephan








Front Hum Neurosci




8


825














Biases in the Explore-Exploit Tradeoff in Addictions: The Role of Avoidance of Uncertainty




L
S
Morris






K
Baek






P
Kundu






N
A
Harrison






M
J
Frank






V
Voon








Neuropsychopharmacology




41


4
















Learning and generalization from reward and punishment in opioid addiction




C
E
Myers






J
Rego






P
Haber






K
Morley






K
D
Beck






L
Hogarth






A
A
Moustafa








Behav Brain Res




317
















Probabilistic reward-and punishment-based learning in opioid addiction: Experimental and computational data




C
E
Myers






J
Sheynin






T
Balsdon






A
Luzardo






K
D
Beck






L
Hogarth






P
Haber






A
A
Moustafa








Behav Brain Res




296
















Development and validation of an overall anxiety severity and impairment scale (OASIS)




S
B
Norman






S
Hami Cissell






A
J
Means-Christensen






M
B
Stein








Depression and Anxiety




23


4
















Working memory, attention, and salience in active inference




T
Parr






K
Friston








Scientific Reports




7














Neuropsychological predictors of clinical outcome in opiate addiction




F
Passetti






L
Clark






M
A
Mehta






E
Joyce






M
King








Drug Alcohol Depend




94


1-3
















Shortened time horizons and insensitivity to future consequences in heroin addicts




N
M
Petry






W
K
Bickel






M
Arnett








Addiction




93


5
















Bayesian model selection for group studiesrevisited




L
Rigoux






K
E
Stephan






K
J
Friston






J
Daunizeau








Neuroimage




84
















Increases in Drug and Opioid Overdose Deaths--United States




R
A
Rudd






N
Aleshire






J
E
Zibbell






R
M
Gladden








MMWR Morb Mortal Wkly Rep




64
















Locus Coeruleus tracking of prediction errors optimises cognitive flexibility: An Active Inference model




A
C
Sales






K
J
Friston






M
W
Jones






A
E
Pickering






R
J
Moran








PLoS Comput Biol




15


1


1006267














Optimal inference with suboptimal models: addiction and active Bayesian inference




P
Schwartenbeck






T
H
Fitzgerald






C
Mathys






R
Dolan






F
Wurst






M
Kronbichler






K
Friston








Med Hypotheses




84


2
















Computational Phenotyping in Psychiatry: A Worked Example




P
Schwartenbeck






K
Friston








3














Computational mechanisms of curiosity and goal-directed exploration




P
Schwartenbeck






J
Passecker






T
U
Hauser






T
H
Fitzgerald






M
Kronbichler






K
J
Friston












Elife 8








The Mini-International Neuropsychiatric Interview (M.I.N.I.): the development and validation of a structured diagnostic psychiatric interview for DSM-IV and ICD-10




D
V
Sheehan






Y
Lecrubier






K
H
Sheehan






P
Amorim






J
Janavs






E
Weiller






T
Hergueta






R
Baker






G
C
Dunbar








The Journal of Clinical Psychiatry




59


20
















Moderating effects of sensitivity to punishment and sensitivity to reward on associations between marijuana effect expectancies and use




J
S
Simons






A
M
Arens








Psychol Addict Behav




21


3
















Methamphetamine use in a rural college population: associations with marijuana use, sensitivity to punishment, and sensitivity to reward




J
S
Simons






R
D
Dvorak






B
D
Batien








Psychol Addict Behav




22


3
















Bayesian model selection for group studies




K
E
Stephan






W
D
Penny






J
Daunizeau






R
J
Moran






K
J
Friston








Neuroimage




46


4
















You are the danger: attenuated insula response in methamphetamine users during aversive interoceptive decision-making




J
L
Stewart






A
C
May






T
Poppa






P
W
Davenport






S
F
Tapert






M
P
Paulus








Drug Alcohol Depend




142
















Reinforcement Learning: An Introduction




R
Sutton






A
Barto


















Stages of dysfunctional decision-making in addiction




A
Verdejo-Garcia






T
T
Chong






J
C
Stout






M
Yucel






E
D
London








Pharmacol Biochem Behav




164
















Tulsa 1000: a naturalistic study protocol for multilevel assessment and outcome prediction in a large psychiatric sample




T
A
Victor






S
S
Khalsa






W
K
Simmons






J
S
Feinstein






J
Savitz






R
L
Aupperle






H
W
Yeh






J
Bodurka






M
P
Paulus








BMJ Open




8


1


16620














Humans use directed and random exploration to solve the explore-exploit dilemma




R
Wilson






A
Geana






J
White






E
Ludvig






J
Cohen








Journal of experimental psychology. General




143
















Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting




S
Zhang






A
J
Yu








Advances in neural information processing systems



















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]