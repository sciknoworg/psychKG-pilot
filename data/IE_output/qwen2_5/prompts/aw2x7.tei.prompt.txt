You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Traditionally, empirical investigations of decision-making under risk have mostly been carried out in behavioral setups limited to one-shot description-based choice problems 
1,
2
 : unique binary choices between mutually exclusive probabilistic options (lotteries), where relevant information (i.e., prospective outcomes and probabilities) is explicitly displayed, and considered known to the decision maker. This experimental setup matches the scope and limits of both normative and descriptive decision theories, which are generally silent about the effects of feedback and choice repetition 
[3]
[4]
[5]
 . Arguably, although both theoretically and empirically convenient, this one-shot description-based framework is not representative of the vast majority of decision situations that one faces every day. Most decision problems are recurrent, and, very often, one gets to know the outcome of one's choice (partial feedback) -and sometimes also the outcome of the forgone option (complete feedback) 
[6]
[7]
[8]
[9]
 . To address those shortcomings, repetitions and feedback have been gradually incorporated into the study of human decisionmaking under risk over the last couple of decades 
[10]
[11]
[12]
[13]
 . This experimental innovation revealed that, in contrast to the normative dictate, human choices and risk preferences elicited in repeated decisions under risk do appear to change depending on the presence versus absence of feedback.
A widespread and intuitive hypothesis concerning the effect of feedback on risk preferences proposes that outcome information modifies the decision-maker's subjective representation of probabilities. Indeed, from studies involving one-shot decisions, it appears clearly that individuals behave as if their subjective representation of probabilities is distorted (overweighting of rare events, underweighting of common events 
2,
14 )
. In the presence of feedback, the realized frequency of the outcomes received in the context of repeated decisions can be used to update (if not correct) the subjective beliefs concerning their probabilities, ultimately affecting one's preferences and choices 
15
 . We shall refer to this category of accounts as the learning hypothesis. Because the integration of feedback in future choices is supposed to correct originally distorted subjective probabilities, the learning hypothesis often assumes that the presence of feedback should correct representational biases and, as a consequence, promote optimal (i.e., expected value maximizing) choices 
7,
16,
17
 . Admittedly, though, these simple predictions from the learning hypothesis can be challenged, e.g., by the presence of learning biases 
18,
19
 , or when the probabilities of the outcomes are extreme and the options are not sufficiently sampled: two conditions that lead the experienced and the actual frequency of the outcomes to diverge 
20 . 4
 The notion that feedback should enhance optimal decision-making goes beyond academic circles and has been widely proposed as a strategy to debias individuals and improve their decisions in significant applied contexts, such as finance and healthcare 
[21]
[22]
[23]
 . Feedback is, in fact, a fundamental element of "boosting"-an approach distinct from "nudging"-which aims to create a more enduring positive impact on decision-making performance by incorporating a learning component 
[24]
[25]
[26]
 .
A relatively recent instantiation of the learning hypothesis is embedded in the BEAST model (Best Estimate And Sampling Tool), which becomes particularly influential after emerging victorious from a choice prediction competition, notably featuring choices between fully described lotteries followed by complete feedback 
27
 . The model, routed in the decisionby-sampling tradition 
28
 , postulates that as soon as feedback is available, the subjective estimations of decision variables (outcomes are probabilities) are based on samples drawn from the past experienced outcomes. The BEAST is currently considered the reference against which new (and old) models of decision-making under risk should be compared, as testified by numerous follow-up studies 
[29]
[30]
[31]
[32]
 .
An alternative category of accounts, which we shall refer to as the attitudinal hypothesis, conjectures that the mere presence of feedback changes the decision-maker's preferences, independently from any learning process. We identify in the literature two main candidate cognitive processes for the attitudinal hypothesis: epistemic curiosity and regret.
Epistemic curiosity pertains to the idea of gaining utility from the resolution of uncertainty 
[33]
[34]
[35]
[36]
[37]
 . Indeed, when feedback is available, some options acquire different informational values with respect to the resolution of uncertainty. For instance, when only the outcome of the chosen option is revealed (partial feedback) and the decision features a riskier (high variance) versus a safer (low variance) lottery, choosing the safer lottery resolves less uncertainty about the final state of the world. In other words, there is an extra informational incentive to choose the riskier option, if partial feedback is provided. This informational asymmetry explains how curiosity −or an uncertainty minimization drive − may shift choices in favor of the risky options if the participant anticipates that the decision will be followed by feedback 
38,
39
 .
Regret can also cause attitudinal effects of feedback when it is expressed as an anticipated emotion, during decision-making 
40
 . The rationale is that, when considering choosing a safe lottery over a risky one, one might forecast the regret elicited by observing a positive resolution from the best alternative outcome (the unchosen, riskier lottery) 
41
 , and therefore make more risk-seeking choices to avert this possibility. As opposed to epistemic curiosity, anticipated regret should notably emerge when the outcome of the unchosen option is also available (complete feedback). In support of the prominent role of counterfactual emotions in decisionmaking, regret has recently been invoked to explain certain aspects of choices elicited in paradigms that couple descriptions with feedback 
8,
9,
27,
42
 .
Critically, in contrast to the learning hypothesis, which supposes that the effect of the feedback should emerge gradually, the attitudinal hypothesis conceives that it can emerge even before any feedback is experienced -i.e. by anticipation. Because the two hypotheses differ in the temporal relation with respect to the time of choice and outcomes, fine-grained temporal dynamics can dissociate the two accounts: attitudinal effects precede choices while learning effects follow outcomes. In addition, the two hypotheses are also different in their relation to choice optimality. Learning mechanisms are generally supposed to correct representational biases and, accordingly, should increase expected value maximization. Attitudinal ones are more silent in that respect.
The goal of the present study is to evaluate the relative merit of these alternative hypotheses in seven newly conducted experiments (N=538) and a reanalysis of a previous dataset (N=446).
The new experiments have been designed to systematically investigate the role of feedback in decision-making under risk across different experimental manipulations that allow discriminating competing accounts. Our findings consistently challenge the learning hypothesis in favor of attitudinal mechanisms, while suggesting that both curiosity and regret play a role under different feedback information regimens and provide compelling empirical evidence against the otherwise very influential BEAST model.


6


Results


Experimental design
To address our research questions, we ran a series of seven incentivized experiments (six online -N=100 for each experiment before the application of strict exclusion criteria -and one in the laboratory -N=30; see Methods). The six online experiments were variants of the experimental paradigm that we will describe below.


Figure 1: Experimental design (A, B) & basic results for individual experiments (C, D). (A) Typical screens
of our behavioral tasks (including the beginning of a block and a typical trial). For each variant (with or without block instructions; partial or complete feedback information) we specify in which experiment has been implemented. (B) Decision problems in Experiments 1-6. Top panel: specification of the risky options. The Y and the X axes present in bold the decision variables (non-zero magnitude and probability, respectively) of the risky option -which always has the form (pts, prob; 0, 1-prob), while the numbers contained in the cell of the matrix (in gray italics) represent the Expected Value of the risky option. Bottom panel: specification of the sure/safe options.
In Exp1-4, we have a sure lottery and in Exp5-6 we have a 50/50 safe lottery. Magnitudes depend on three independent factors of the design (pts and prob of the risky option and whether the risky or the sure/safe option has a higher EV). (C) R-Rate (risky-choice rate) and O-rate (optimal-choice rate, namely Expected Value maximizing rates) as a function of the presence or absence of feedback (F and nF, respectively) across the seven experiments. Note: in Exp 7 the two options had equal EVs, so O-rates are not defined. (D) The effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) as a function of which option (risky or sure/safe) has higher EV across experiments 1-6 (as explained above, Exp 7 is not eligible for this analysis either). *p<0.05, ***p<0.001, ns = not significant, with respect to two-tailed Wilcoxon Signedrank test.
Our experimental design allowed us to overcome several methodological shortcomings identified in previous literature, which overall delivered an unclear picture concerning the directionality of the effect of feedback in decision-making under risk (Supplementary Materials/Literature Review for a quantitative survey of the literature). One novelty of our experimental design is using a factorial design with a within-subjects manipulation of postchoice feedback (present or absent); feedback was treated as a between-subjects factor in most of the previous studies, with only one exception 27 -our design improves over 
Erev et al., (2017)
 by equalizing the number of repetitions for the two conditions (their design featured five repetitions for no-feedback and twenty for feedback) and by counterbalancing the order of the conditions. Furthermore, unlike most of the previous experiments, we included the same number of trials 
(10)
 in both feedback conditions, to disentangle between the effect of mere repetition and the effect of feedback itself 
43
 . Trials featuring the same decision problem were clustered in blocks of 10 trials. Feedback and no-feedback blocks were randomly interspersed ( 
Figure 1A
).
We used a binary choice task featuring a sure and a risky option in each trial. The risky option had the form of (m, p; 0, 1-p), namely giving m points with probability p and zero points otherwise. In addition to feedback (present or absent), we also factorially manipulated the choice optimality, i.e., whether the risky or the sure option has a higher expected value (EV): in one condition, the risky option maximizes the EV; in the other condition, the sure option maximizes EV ( 
Figure 1B)
. This allowed us to orthogonalize risk preference and decision optimality -two features that have often been confounded in the literature. Finally, we manipulated within-subjects the probability of (positive) gain associated with the risky option (three levels, namely 0.1, 0.5, and 0.9), and the magnitude of the risky option (two levels, 40 and 60 points), thus leading to a decision space of 12 unique decision problems. Together with our feedback/no feedback manipulation and the repetitions (×10 per decision context), our final factorial design comprised 240 choices per subject, which is larger than that commonly found in the literature.


8


The role of feedback and instructions on risk preferences
The first experiment (Exp.1) featured partial feedback i.e., we revealed only the outcome associated with the chosen option. Participants were not informed about the presence or absence of the feedback before starting a given block. Dependent variables, i.e., the propensity to choose the risky option (R-rate) or the optimal -EV maximizing -option (O-rate) were analyzed using a generalized linear mixed effect (GLME) model with the task factors (presence of feedback and option optimality) as independent variables (see Methods for more details).
Our analyses of the R-rate identified a significant main effect of feedback (logistic regression βFEEDBACK1 > 0, P = e-07, 
Table 1
, choiceRisky, Exp1(1)), which was characterized by an increased propensity to choose the risky option when trial-by-trial feedback was present 
(Figure 2A; Figure 1C
, Exp1). Interestingly, this increase was of the same size and direction both when the risky option was better and when the sure option was better (with respect to EV) ( 
Figure 1D, Exp1)
, such that there was no detectable main effect of feedback on the optimal choice rate (logistic regression P = 0.78, 
Table 1
, choiceOptimal, Exp1(1); Figure 1C, Exp1; 
Figure 2B
). In other terms, there was a significant interaction between feedback and option optimality: feedback decreased the O-rate when the sure option was the EV-maximizing one but increased it otherwise (logistic regression βFEEDBACK1 < 0, P = e-10, βFEEDBACK1:RISKYBETTER1 + βFEEDBACK1 > 0, Pinteraction = e-56, 
Table 1
, choiceOptimal, Exp1(2)). Finally, we examined the trial-by-trial unfolding of the main effect of feedback on R-rates. The learning hypothesis predicts that the effect of feedback should be absent at the first trial, then gradually emerge and increase after the repeated experience with feedback. Our analyses revealed a slightly different pattern: while indeed no significant effect of feedback could be detected in the first trial R-rate in feedback and no-feedback conditions abruptly diverged in the second trial and the difference remained constant until the end of the block (P = .12 for Trial1, P < 0.01 for Trial2-10 Benjamini-Hochberg adjusted values of two-tailed WSRTs; 
Figure    2C
).
Overall, the results of this first experiment appeared, at the macroscopic level, in line with most of the existent literature generally showing an increase of R-rate in the presence of feedback.
At a finer grain level, because the effects develop after the first feedback, they seem overall consistent with a learning effect. The fact that the effect is abrupt rather than gradual could be understood as one-shot learning. However, because participants in Exp.1 started each block without knowing whether they would receive feedback or not, the first trial also implicitly but unambiguously informed them about the presence of feedback in the ongoing block (feedback or no-feedback), which may have altered their attitude toward risky options. Thus, although the separation of R-rates in the second trial can be a result of (one-shot) learning, it can also reflect the triggering of an attitudinal change. Of note, also against the learning hypothesis is the fact that the presence of feedback did not improve the optimal choice rate.
To disentangle these two possibilities, we ran a second experiment (Exp.2) in which, at the beginning of each block, participants received explicit instructions (hence block instructions) mentioning whether they would receive post-choice feedback in the upcoming block or not.   
Table 1
). Yet, the between-experiment manipulation of the block instruction produced a significant difference in the effect of feedback on first-trial R-rates (P = .0022; two-sample two-tailed t-test; inset of 
Figure 2F)
. Actually, the difference in R-rates between feedback and no-feedback blocks in Exp.2 arose from the very first trial -and remained significant for the rest of the block (P < 0.01 for Trial1-10 BH-adjusted values of two-tailed WSRTs; 
Figure 2F
). Thus, it seems that the mere anticipation of feedback information induced by the block instructions was enough to change risk preference before any feedback was actually experienced.
In summary, results from Exp.1 and Exp.2 clearly revealed that the presence of feedback about the outcome of the chosen lottery increased risk propensity but not choice optimality. Besides, while Exp.1's results only superficially supported the learning hypothesis, the results following the introduction of explicit block instructions in Exp.2 favor the attitudinal hypothesis. Exp.1 and Exp.2 featured a partial feedback regimen and since the result of the sure option is always known by definition and the result of the unchosen option is not disclosed, choosing the risky option provides the participant with more information about the current state of the world. Thus, this pattern of results is consistent with an attitudinal effect created by epistemic curiosity, where the demand for uncertainty resolution increases risk propensity because of the informational asymmetry between the risky and sure lotteries. 


Curiosity cannot be the only determinant of the effect of feedback on risk preference: a role for regret
If epistemic curiosity is the only driver of the observed effect, the informational asymmetry between the sure option (whose result can be inferred with certainty in absence of feedback) and the risky one (whose result cannot be inferred with certainty in absence of feedback) causes the increased risk-taking propensity in the presence of feedback. Thus, according to the epistemic curiosity account, the effect should vanish (or, at least, decrease) under a complete (or 'full') feedback regimen, i.e., when the forgone outcome of the unchosen option is additionally revealed. To test this hypothesis, we ran Exp.3 and Exp.4, which were analogous to Exp. 1 (without block instructions) and Exp.2 (with block instructions) except for the fact that they both featured complete feedback.
The complete-feedback experiments replicated the main effects observed in their partial feedback counterparts (Exp.1 and Exp.2). Most importantly, the presence of complete feedback still increased the R-rate and had no effect on the O-rate ( 
Figure 1C
, Exp.3 and Exp.4; 
Table    1
).
The pattern of results at the level of the trial-by-trial dynamic was also replicated. In Exp.3
(without block instructions), the divergence induced by the presence versus absence of feedback was detectable from the third trial and remains significant for the rest of the block (P = 0.10 for Trial1, P = 0.21 for Trial2, P < 0.01 for Trial3-10 BH-adjusted values of two-tailed WSRTs; 
Figure 3A
). In Exp.4 (the one with block instructions), contrary to the idea of epistemic curiosity being the sole determinant of the change in risk propensity between feedback and no feedback conditions, we found an effect from the first trial. The R-rates in the feedback blocks are significantly higher than the no-feedback ones starting from the first trial and throughout the block (P < 0.01 for Trial1-10 BH-adjusted values of two-tailed WSRTs; 
Figure 3Β
). As in Exp.1 versus Exp.2, the between-experiment manipulation of the block instructions produced a significant difference in the effect of feedback on first-trial R-rates in Exp.3 versus Exp.4 (P = .0025; two-sample two-tailed t-test; inset of 
Figure 3Β
).
While these results are once again consistent with a feedback-induced attitudinal change in risk preference (the effect arose before any feedback was received), they are not easily accommodated by the epistemic curiosity account, because, under the complete feedback regimen, there is no uncertainty resolution utility bonus attributable to choosing the risky option. An alternative psychological mechanism for this effect that is compatible with the complete feedback scenario is the anticipated regret. To understand why anticipated regret could represent a possible explanation for this effect, it should be first noted that in many economic decision-making settings, regret is generally thought to be dependent on a comparison between the obtained and the forgone outcome and that this comparison is made explicit only in the complete feedback condition, where regret is experienced whenever the forgone outcome is higher than the obtained one. We propose that at the decision stage, the option that gives the best outcome most of the time gets the regret "premium". This premium is further modulated by the ratio between the more frequent best outcome (by comparison to the alternative) and the range of the possible outcomes (i.e., the difference between the best and worst possible outcomes) in the decision problem. Hence, the regret premium of an option depends both on the probability that the option delivers an outcome greater than the alternative and on its magnitude relative to the outcome range.
Note that according to this definition, in our design, the options with the greater regret premium are those associated with a high (90%) probability of delivering the risky outcome because: 1) they deliver the best possible outcome compared to the alternative 9 times out of 10, and 2) since the risky outcome is the largest in a decision pair, its value relative to the range of available outcomes is high (r~=1). Conversely, in decision problems where the probability of delivering the risky option is low (10%), the option that delivers the best outcome most of the time is the safe one. However, its regret premium is drastically reduced because the value of the safe outcome is generally much smaller than the range of the available outcomes (<<1).
Thus, to assess the anticipated regret hypothesis, we evaluated the effect of feedback as a function of the probability of the best-risky outcome (10%, 50% and 90%) and of the type of feedback (partial and complete). This analysis revealed a clear interaction (linear regression on the differences of individual average R-rates between F and nF conditions βpRISKY:TypeOfFeedback > 0 P = 0.0011; 
Figure 3E
), which was driven by the effect of feedback increasing as a function of the probability of the best-risky outcome in the complete feedback experiments (logistic regression 
Table 1
, choiceRisky: PFEEDBACK1 = 0.86, βFEEDBACK1:P_RISKY1 > 0, P = e-04, βFEEDBACK1:P_RISKY2 > 0, P = e-11, Exp3(2); PFEEDBACK1 = 0.31 βFEEDBACK1:P_RISKY1 > 0, P = e-15, βFEEDBACK1:P_RISKY2 > 0, P = e-19, Exp4(2)) but being stable in the partial feedback experiments (βFEEDBACK1 > 0, P = e-04, PFEEDBACK1:P_RISKY1 = 0.14, βFEEDBACK1:P_RISKY2 > 0, P = 0.013 Exp1(2); βFEEDBACK1 > 0, P = e-11, PFEEDBACK1:P_RISKY1 = 0.24, PFEEDBACK1:P_RISKY2 = 0.065, Exp2(2)). Note, that since the regret "premium" is also proportional to the relative magnitude of the best outcome of the regret-minimizing option and given that the magnitude of the sure option is very small when we are in the low-probability case (because the sure option's magnitude is close to the Expected Value of the risky option (see 
Figure 1B)
 


Extending the results to moderate risk options and losses
Next, we attempted to further clarify the psychological mechanisms involved in this effect. The fact that, in all experiments, the sure option is systematically a certain prospect leaves open the possibility that the effect of feedback is idiosyncratic to this framing. Indeed, certainty effects are known to heavily weigh decisions and to create robust paradoxes 
44
 . In the next two experiments, we therefore assessed the robustness of our results to variation in outcome probabilities, specifically in contexts where the non-risky option is not certain. To do so, we designed Exp.5 and Exp. 6, where we substituted the sure option (which gives a specific amount with certainty) with a 50%-50% low variance lottery with EV equal to the one of the sure option ( 
Figure 1B)
. This new option remains relatively safe (given its low variance), yet now features an uncertain outcome. We shall refer to this option as the safe option, to differentiate it from both the sure and the risky. All other things considered (i.e., except for the sure options being substituted with the corresponding safe ones), Exp.5 and Exp.6 were respectively indistinguishable from Exp.2 (partial feedback) and Exp.4 (complete feedback) ( 
Figure 1A)
.
Consolidating our conclusions, all the main results identified in Exp.1-4 were replicated in this modified setup ( 
Figure S1)
. Notably, the presence of feedback increased risk-taking from the first trial significantly in the partial feedback Exp. 5 and numerically in the complete feedback  
& 4E)
. Finally, if anything when pooling all the experiments, contrary to the expectations of the learning hypothesis, we found a small negative impact (~1%) of the presence of feedback on the optimal choice rate ( 
Table S1)
.
Bayesian evidence suggest a strong to very strong (depending on the scale used -e.g. Jeffreys or Kass and Raftery 
46
 ) support in favor of the null hypothesis of O-rate not being increased by feedback against the alternative hypothesis that feedback increases O-rates (BF10 = 0.015 with uninformed Cauchy prior with scale 0.707; BF10 = 0.017 with informed Normal prior with mean and std calculated by the empirical data of Erev_2017).
Finally, in a last experiment performed in the lab, we also showed that our main findings hold when incentives are increased and monetary losses are introduced (see 
Figure 1C
, Exp7 and Supplementary Materials/Loss Experiment). 


Trial-by-trial analysis
A follow-up question is whether the abrupt emergence of the feedback effect on the second trial in the experiments without block instructions was dependent on the choice made on the first trial. We observe that the main effect of feedback is observed regardless of the previous choice type (sure/safe, versus risky), and additively to the main effect of the previous choice type on R-rates ( 
Figure 5A and 5C)
. Since the second-trial effect is present regardless of the choice made on the first trial, only the mere discovery of the presence of feedback (and consequent attitudinal change) could explain the effect, challenging the learning hypothesis.
The existence of feedback-induced attitudinal effects on risk preferences does not rule exclude that additional feedback-induced learning processes co-exist. However, feedback-induced learning processes may not be apparent when looking at the average risky choice rate, because their effect depends on the previous trial choice and outcome. To investigate possible learning effects in trial-by-trial dynamics, we analyzed the probability of repeating a risky choice as a function of the outcome received in the previous trial. The logic of this analysis is that virtually any instantiation of a learning process would induce a "positive recency" effect, meaning that the probability of repeating a risky choice should increase after receiving the best possible (nonzero) outcome, compared to receiving the worst possible (zero) outcome 
(Figure 5E, left)
. We tested this hypothesis by analyzing this behavioral variable (probability of repeating a risky choice, p(Rt|Rt-1)) in the feedback condition across all datasets. The results are in sharp contrast with the predictions of learning hypothesis ( 
Figure 5F)
. In fact, the probability of repeating a risky choice was lower after receiving positive feedback (logistic regressions βRISKY(t-1)>0 < 0, P = .012 for Exp5 and P < .001 for the all the rest, 
Table 1
, repeatRisky, Exp1-6(1); means p(Rt|Rt-1=0) ± std = 0.69±0.27, p(Rt|Rt-1>0) ± std = 0.62±0.26, Wilcoxon Signed Rank Test, two-tailed P = e-05). The analysis of trial-by-trial dynamics thus shows no support for any form of feedback-induced learning process, and rather strictly falsifies it. The observed behavioral pattern exhibits in fact negative recency, which is better understood as a manifestation of the gambler's fallacy (in the laboratory 
[47]
[48]
[49]
 and in an ecological (real-life) setting 
50
 ), according to which participants would move away from a recently rewarded risky choice because they (wrongly) assume that the subsequent likelihood of positive feedback will be lower ( 
Figure 5F
 and 
Figure 5G
). This gambler's fallacy interpretation is further confirmed by conditioning this analysis on the probability of the risky outcome (recall, in our task we featured three probability levels: 0.1, 0.5 and 0.9). This actually reveals that the effect is modulated by the underlying outcome probability and is maximal when outcomes are rare (p=0.1: subjects perceive the likelihood of receiving two positive outcomes in a row lower than reality) and absent when the outcomes are common (p=0.9) (logistic regressions βRISKY(t-1)>0 :P_RISKY2 > 0, P = .007 for Exp3 and P < .001 for all the rest, 
Table 1
, repeatRisky, Exp1-6(2); 
Figure 5G
).
Next, we wanted to check whether the abrupt effect of feedback between the first and the second trial in experiments without block instructions was influenced by the outcome received on the first trial. While confirming the presence of a negative recency effect, the results also show that the effect of feedback is detectable, regardless of the nature of the outcome received on the first trial ( 
Figure 5B and 5D)
. This result further supports the idea of an outcome-independent attitudinal change in risk preferences.
To sum up, not only do we demonstrate that the effect of feedback on risk preference precedes the reception of any feedback (and is, therefore, better understood as a change in attitude), but we also disprove any residual role for feedback-induced learning processes in the trial-by-trial dynamics by evidencing biased reactions to probabilistic and stochastic events akin to the gambler's fallacy 
[47]
[48]
[49]
[50]
 . 


Confirming our findings in a previous dataset
We started our investigation by noting some discrepancies in the literature concerning the directionality of the effect of feedback in decision-making under risk, which was generally understood as stemming from a learning process 
(Table S2 &
 
S3)
. Over 7 Experiments we found that the presence of feedback increases the propensity of taking risks, with no detectable consequence on the optimal choice rate. By manipulating block-wise instructions (present vs absence) we also found that the effects were mediated by a change of attitude of a different nature in the partial (consistent with curiosity) and complete (consistent with regret) feedback condition; trial-by-trial dynamics analysis further ruled out that outcome-based learning play a role in these processes.
We re-analyze a previously published dataset that stands out as containing the largest sample size among the studies analyzed (N=446) and as proposing an influential cognitive model of decision-making (see next paragraph) 
27,
29,
32
 , despite featuring some important limitations (it involved only complete feedback, the feedback and no-feedback conditions featured different number of trials and always appeared in the same order, there was no clear manipulation of the instructions). In order to replicate our analyses as comprehensively as possible, we restricted this re-analysis to the decision problems that feature identical or similar properties to ours, namely decisions opposing a sure to a risky option (to define risky choice rate), decisions involving options with different expected values (to define optimal choice rate) and decisions featuring non-extreme probabilities (excluding 1% or 99%) for the risky option. We also excluded trivial decision problems in which one option dominates the other (see  
Figure    6B
). Having in mind the fact that Erev et al. (2017) featured complete feedback we looked at the effect of feedback specifically for different probability levels (low: prob≤0.25, medium: 0.25<prob<.075, high: prob≥0.75) of the risky high-value outcome. As expected by the regret hypothesis, and consistent with our own findings, the effect of feedback monotonically scaled with the risky best-outcome probability ( 
Figure 6C
; low: -0.053±0.317; medium: 0.038±0.209; high: 0.132±0.329). Note, that the negative effect observed in the low condition is also consistent with the regret account we described above and it is a result of Erev's data set including decision problems with negative outcomes. Indeed, if we split the low-probability problems in those containing only non-negative outcomes (as in our design) and those containing negative outcomes, we see that in the former set the effect is absent (as in our data), while the latter set exhibits a negative effect. This could originate from the fact that in the decision problems including negative outcomes the magnitude of the safe option is bigger relative to a range that extends in the negative domain. Moreover, we looked at the trial-bytrial dynamics and found a negative recency pattern, consistent with a gambler's fallacy bias ( 
Figure 6D)
. Finally, while the manipulation of instructions was not present as such in , the different orders of presentation of decision problems allowed us to perform an analogous analysis, which further supports a first-trial/attitudinal effect (for the details look  


BEAST model and key behavioral results
Having confirmed that all the results that we could test in Erev et al. (2017) are replicated, we turned to test whether or not the model originally stemming from this dataset and later picked up by many other influential studies of human decision-making 
29,
31,
32,
51
 is able to capture the main behavioral patterns we observed.
The main idea of the Best Estimate and Sampling Tools (BEAST) model is that the attractiveness of one option is the sum of its Expected Value (assuming for simplicity that no ambiguity is involved, which is the case here) and an average Sampling Value which is generated by the use of four sampling tools (which correspond to four behavioral tendencies) 
27
 .
Three of the sampling tools are "biased" in the sense that they comprise mental draws from distributions that differ (in a specific, biased manner each) from the objective distributions of the prospects; they are not related to our discussion, so, we omit the details here. The unbiased tool consists of mental draws either from the objective described distribution of the prospects, in case of no feedback, or from the realized one, in case of feedback. Crucially, the reliance on the observed history of outcomes when feedback is present makes the BEAST model a learning model. The unbiased tool captures the tendency to minimize immediate regret -or, to put it another way, it captures the tendency to prefer the option that gives the best outcome most of the time. The reliance on the unbiased tool increases as the subject receives more feedback, thereby making the impact of regret stronger.
We fitted the participants' data of our experiments 1-6 to the BEAST model and then we ran simulations with the fitted parameters to compute predicted choice probabilities for each subject. The model predictions replicate the main effect of feedback in increasing R-rates, but fail to capture the effect induced by the manipulation of block instructions -the effect of feedback is absent in the first trial independently of the presence or absence of the block instructions ( 
Figure 7A & 7B)
. This is an expected result, since the BEAST attitudinal changes occurring before any feedback are experienced play no role. When it comes to the interaction of feedback with probability, model predictions capture very accurately the complete feedback case, but fail to capture the partial feedback case 
(Figure 7C, 7D & 7E)
. This is also an expected result, given that the regret component of the BEAST model does not depend on feedback regimen and is equally deployed in partial and feedback condition. Finally, in contrast to our robust behavioral finding, BEAST predicts a positive recency pattern resulting from its learning nature 
(Figure 7F & 7G)
.  Experiments with block instructions. In addition, the inset displays the effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) for the first trial for both Experiments without block instructions (no effect) and Experiments with block instructions (no effect); non-significant difference between the two conditions of the model predictions (P = 0.27 two-sample two-tailed t-test). (C) Risky choice rate as a function of the feedback condition and the probability level of the risky option for Experiments with partial feedback 
(1,
2,
5)
. (D) Same as (C) but for Experiments with complete feedback 
(3,
4,
6)
. (E) The panel displays the effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) as a function of probability levels for the partial and the complete feedback experiments. (F)
The rate of repeating a risky choice using the data of feedback blocks only, because the analysis cannot be performed in blocks without feedback. Repeat risky rate is plotted as a function of the outcome of the risky choice in the previous trial and of the probability level of the risky option. (G) The panel displays the difference of the rates of repeating a risky choice, between the case where the outcome of the risky choice in the previous trial was positive and the case where it was zero, across probability levels. In the inset of (B), (E), and (G), the black dots show the behavioral data.


Discussion
In the present study, we aimed to clarify the effect of feedback on description-based decisionmaking. This represents a pressing question in behavioral decision-making research as many (if not most) real-life decisions feature both description (in various forms) and feedback (we enjoy -or suffer -the consequences of our choices). Our investigations aimed to address two (related) research questions. First, the directionality of the effect on two main outcome measures, namely risk aversion and value maximization. Second, the cognitive mechanisms underpinning the effect. To address these questions, we devised a series of original experiments designed to overcome some frequent (if not systematic) limitations identified in previous studies.
Regarding the directionality of the effect of feedback on decision-making under risk, we found that the presence of feedback increased the propensity to choose the risky option. Because it falls in line with the majority of studies surveyed in our literature review, we argue that this result can be considered credible, robust and replicable 
27,
36,
[52]
[53]
[54]
 . In light of the strength of our empirical evidence (including several experiments, different levels of outcomes, probabilities, gains and losses and an overall extremely large sample size), we believe that the occasional reports of negative effects might be attributed to peculiarities of the design of the related studies -but further investigation would be needed to verify this.
Our experimental design also allowed us to confidently establish that the presence of feedback had no (beneficial or detrimental) effect on the optimal (i.e., EV-maximizing) choice rate: risky choice rate was increased by feedback regardless of whether or not the risky option was more advantageous. This question was somehow overlooked in the literature, as most of the surveyed studies actually did not allow for testing this effect, either because they featured risky option which was consistently associated with the highest expected value 
55,
56
 , or because they used decision problems where the risky and the sure/safe option had the exact same expected values 
15,
36,
52,
54,
57
 . The lack of clarity and investigation of the effect of feedback on choice optimality is surprising since feedback is often suggested as a possible way to correct decision-making biases and improve decision-making 
24,
25
 . Of note, once restricted to the subset of decision problems that are relevant to our investigation, a re-analysis confirmed our main results in a large dataset originally published by 
Erev et al. (2017)
.
In addition to disambiguating the directionality (and amplitude) of feedback-induced changes in risk preferences, our study also provides further insights into the cognitive mechanisms underlying these effects. In the literature, the (more or less explicit) standard assumption is that experiencing decision outcomes affect the subjective beliefs characterizing the probability of realization of those outcomes 
15,
54,
55,
58
 . In other terms, the effects of feedback are traditionally conceived as the result of a learning process due to the sampling experience. Although predominant, the learning hypothesis has rarely been empirically challenged and compared to plausible alternatives, one of which being that the presence of feedback changes the attitude of a subject to make a risky decision. This attitudinal change can naturally be induced by the anticipation of the (informational and emotional) state that results from receiving the feedback.
In order to evaluate the merit of this alternative hypothesis, we designed a new experimental manipulation that consisted of disclosing (or not) whether the upcoming block of trials would feature explicit feedback. This manipulation allowed us to reveal a simple but unambiguous behavioral signature of an attitudinal change: when subjects were informed about the presence of feedback, its effect was present in the first trial, in decisions that preceded the disclosure of the first feedback. Furthermore, when looking at the experiments without block instructions, we observed that the effect of feedback on risk preference was present from the second trial regardless of which choice had been made or which outcome had been received in the first trial, further confirming that the change in risk preference was due to a sudden change in attitude, rather than being based on feedback integration.
These results unambiguously falsify the learning hypothesis as a sole determinant of the effect of feedback on risky decision-making, given that the effect of feedback is present before any actual learning could occur.
Several cognitive processes or psychological motives can actually underpin this attitudinal effect toward feedback. Our results, when restricted to the experiments featuring partial feedback (i.e., only the outcome of the chosen option was presented) are consistent with a curiosity-driven attitudinal change 
59
 : since the outcome of the sure option is known with certainty before making a choice, choosing the risky option is the only way to resolve the uncertainty characterizing the outcome of the whole decision situation. These results are in line with the epistemic curiosity literature that shows that subjects attribute a positive utility and actively seek uncertainty resolution 
[33]
[34]
[35]
[36]
[37]
 . However, while providing a satisfactory interpretation of the effects on risky decisions in the partial feedback condition, curiosity-driven motives cannot account for the fact that these effects persist under completed feedback conditions (i.e., when the outcomes of both the chosen and the unchosen options were presented). To shed light on this incongruity, we analyzed the effect of feedback as a function of the probability level of the risky option. This analysis revealed that, while the effect of feedback seemed undistinguishable in the partial and complete feedback experiments at the aggregate level, clear differences emerged when splitting across probability levels. More specifically, the feedbackinduced increase in risk-taking was a monotonic function of the probability of the risky option, a pattern consistent with the idea that increased risk-taking in the complete feedback experiments is induced by the willingness to reduce the chance of experiencing regret 
60
 . The rationale underlying this interpretation can be broken down into two main steps. First, regret is typically thought to be elicited -in the context of value-based decision-making -by the comparison between the obtained and the forgone outcome, and is, therefore, more salient in complete feedback environments (where this comparison can actually be directly observed).
Second, the risky option minimizes the chance of experiencing regret, specifically when the probability associated with the best possible outcome is high. Of note, as far as the analyses could be replicated, the results of Erev et al. (2017), which only featured complete feedback, also indicated an interaction between the effect of feedback and the probability of the risky option, suggestive of a regret minimization.
Although explaining our current results requires assuming two different psychological processes operating in the two informational regimens, we note that regret and curiosity have been shown to interact in other experimental (and real-life) situations 
[61]
[62]
[63]
 . Further research will be needed to better characterize the relation (cooperation or competition) between these two motives. Our results add up to the behavioral literature, spanning from reinforcement learning to behavioral economics, showing that partial and complete feedback situations may elicit (radically) different cognitive processes 
9,
[64]
[65]
[66]
 .
Evidence for an attitudinal influence (as manifested by the first-trial effect) does not rule out the possibility that learning processes operate in parallel and influence choices. To test this hypothesis, we looked at feedback-induced trial-by-trial choice adjustments. We reasoned that virtually all instantiations of learning, be they rooted in Bayesian update or Reinforcement learning 
67
 , generate positive recency effects, i.e. predict that the probability of choosing a risky option should increase following a rewarded risky choice. Critically, this is also true for decision-by-sampling models, such as the BEAST, which supposes mental samples of the outcomes are drawn from the empirical (i.e., experienced) distribution. Accordingly, having experienced positive outcomes in the past should increase the likelihood of repeating the same 26 choice. In striking contrast with the prediction of the learning hypothesis, our analysis revealed a negative recency pattern: a positive feedback in the preceding trial reduced the chance of repeating a risky option. Our re-analysis of Erev et al. 2017 provided further support in favor of a negative recency (gambler's fallacy) effect. This prima facie puzzling behavioral effect can be understood as a manifestation of what is called the gambler's fallacy, i.e., the fact that human subjects tend to misrepresent the independence of probabilistic outcomes 
47,
49,
68
 . This interpretation is further confirmed by the fact that this effect interacted with the probability of the risky outcome, such that the negative recency effect was stronger when the probability of the risky outcome was lower: a situation where the gambler fallacy intuition suggests that two consecutive lucky strikes appear almost impossible. These results not only rule out the possibility that residual feedback-induced learning effects are at play but also suggest that an explicit description of the probabilistic process may create (biased) prior expectations that prevent learning processes.
Taken together, our observations significantly challenge any cognitive model relying on learning process as a mechanism of the effect of feedback on decision-making under risk, be it routed in reinforcement learning 
69
 , Bayesian 
70
 or decision-by-sampling theories 
28
 . Concretely, we found the influential BEAST model, which supposes that once feedback is available values are constructed by (mentally) sampling outcomes from the empirical distribution falls short of capturing critical features of behavioral results, such as the first trial effect and the negative recency effect. Also of note, while the BEAST model does capture the role of regret, it is not equipped to deal with the shape of effects highlighted in the partial feedback condition. Our results, therefore, suggest that the BEAST model to be continued used as a valid model of human decision-making should undergo major restructuring vis-à-vis several key structure features 
29,
32,
51
 . Our findings also tell a cautionary tale against validating computational cognitive models using choice prediction, data-driven competitions, which, focusing on quantitative metrics and aggregate data, may miss critical features of human behavior that can be highly diagnostic of the underlying cognitive mechanisms 
27,
71
 .
To conclude, our findings shed new lights on the behavioral effects of feedback in descriptionbased decisions under risk, and on their underlying psychological mechanisms beyond learning.
Because of the ubiquity of those situations, elucidating the effect of feedback in descriptionbased scenarios can improve our understanding of apparent decision anomalies relevant for many real-life situations and give us the opportunity to improve our policies. In particular, our results suggest that, contrary to what common sense could dictate, providing feedback cannot be considered as a panacea to correct decision-making biases, notably because the effects of feedback are at least partially mediated by attitudinal changes rather than purely driven by learning processes 
[24]
[25]
[26]
 . Our results add up to an increasing body of evidence highlighting complex interactions between description-and experience-based choices that are currently not well accounted for by standard models 
6,
72
 .


Methods


Participants
For the six online experiments, we recruited a total of 620 participants (4x100 for Exp1-4, 104 for Exp5, and 116 for Exp6 | 300 females, 300 males, 
20
 


Exclusion criteria
To ensure the high quality of the data of the online experiments, we applied the following exclusion criteria (which were explicitly pre-registered in the master thesis pre-planning document produced by the first author):
• participants with a missing trial or with a repetition of a trial in the test phase
• participants with two or more submissions of more than 100 out of 270 trials 1
• participants with less than 9 (out of 10) correct answers in the catch block (see below for more information on the catch block)
• excessively long completion time [two standard deviations more than the average]
• right or left option >95% of the time [low quality data] -note that the position of the options was counterbalanced, so this behavior is aberrant After applying the above exclusion criteria, we were left with a total of 508 participants for the main analyses (80, 95, 86, 85, 80, 82 for experiments 1-6 respectively). It is worth noting that all our main results replicate when we drastically lower our exclusion criteria (keeping participants with less than ten missing trials and with an accuracy in the catch block of at least 6/10), thereby including 609/620 participants in the analysis.
No participants were excluded from the analysis of the laboratory experiment. 


Experimental design
The online experiments started with participants giving their consent to participate, followed by detailed instructions on the behavioral task, the structure of the experiment, and the compensation. Afterward, participants went through training which was a mini version (four blocks of five trials each) of the actual experiment featuring similar yet not identical decision problems to the ones of the actual experiment.
Then, participants started the actual experiment which consisted of 24 blocks (12 decision problems with and without feedback, as described in the main text) and a catch block (see below) in the middle of the actual experiment. The 24 blocks were comprised of 10 trials each and were randomized within and across participants. The actual experiment was divided into three sessions between which the participants were given the opportunity to take a self-paced break.
Each block started with a screen providing block instructions about the presence or absence of feedback in the upcoming block (Exps 2, 4, 5, 6) or just prompting the participants to start the block by clicking on the "Start Block" button (Exps 1 & 3). This step was self-paced. Then, the two options (their magnitude and probability) were presented side-by-side, with a clickable white square below each option. The position of the options (left or right) was randomized.
Also, the relative vertical position of the magnitude and the probability (magnitude above and probability below or vice versa) was randomized across participants (but was constant within participants). Participants could make their choice at their own pace by clicking the white square below their preferred option. The outcome of the risky (or the safe) option was determined by an independent random draw -by definition, the outcome of the sure option was fixed. After the choice was made and the outcomes were determined, the outline of the selected square/option was highlighted and inside the white square, the outcome of the chosen option was revealed (showing the obtained points) or hidden (showing a question mark) for 1500ms.
In experiments where complete feedback was used, the forgone outcome was revealed as well (showing the points of the unchosen option) or hidden (showing a question mark) in a light gray font (in contrast to the standard black font for the obtained outcome). (note partial feedback means that the outcome only of the chosen option is revealed; complete feedback means that the outcome of both the chosen and the unchosen option is revealed). Then, the next trial started showing the same options (potentially in a different position). At the end of the block a screen marking the end of the block was presented for 1500ms.
In the middle of the actual experiment, a catch block featuring the trivial choice between a sure option (probability = 100%) giving 5 points and a sure option giving 30 points was presented.
The actual block was identical to all the rest blocks of the actual experiment. The related exclusion criterion (participants that scored below 9/10 correct answers in the catch block were excluded from the analysis) enabled us to ensure that participants understood and paid attention to the task.
At the end of the online experiments, participants were informed about the randomly selected trial and the associated monetary bonus, about their total compensation and they were redirected to the recruiting platform (Prolific) to formally complete their participation.
The laboratory experiment was conducted at the laboratory of Département des Etudes Cognitives at Ecole Normale Supérieure, Paris. Participants were told their rights and gave their consent. They completed a mathematical questionnaire assessing their ability to make multiplication. The experimenter explained the task using a visual support and read an example of a gamble that might be encountered by the decision maker to ensure that they understand the possible outcomes of the gamble. Participants were told to maximize their gains and minimize their losses.
On the computer, participants were presented with an instruction screen indicating the type of information that they would be facing in the block. Individuals read "réalisation révélée essai par essai -feedback provided after each trial" or "réalisation cachée -feedback not provided" and were informed that, at the end of the block, they would be told the number of points gained or lost at that block. After reading the instruction, participants were presented with two options side by side on the screen. Each was associated with a geometrical shape and two labels: one gave information on the probability of occurrence of the outcome and the other, on the magnitude of the outcome. The shapes and their color varied randomly throughout the experiment and across participants. Colors and the brightness of the screen were adjusted to prevent eye fatigue and for easy reading. Labels describing the magnitude and the probability of each lottery figure were above and below the shapes. For half of the participants, the probabilities are always above and the magnitude below. For the other half of the sample, the labels are reversed. The position of the risky and the sure lotteries was randomized within and across blocks. Participants could not anticipate their position and therefore have to stay attentive during the sequential choices. Choices were made using a mouse. After each one, an arrow indicated the localization of the choice, disappeared after 500ms to be replaced by a text at the place of the label of the magnitude of the chosen lottery. The text indicated either the number of points gained or lost (e.g : -32pts) or hid them (XX pts) -as in the online experiments, the outcome of the risky lottery was determined by an independent random draw. The disclosure of the points depended on the type of information provided at the beginning of the block. 1.5s after, another screen appeared with the second round of choice providing exactly the same gamble as before. At the end of the sequence of 10 choices, the accumulated points were shown on the screen. Most steps of the task were self-paced such that the participant could read and take as much time as needed for the instruction, the choice and the bonus screens. The experiment was divided in three sessions to avoid fatigue. Participants were given feedback on their total points accumulated at the end of each session. At the end of the experiment, their payoff was computed on the basis of their total points.


Decision Problems
By experimental design, the magnitude and the probability of the risky option were determined -and by definition the probability of the sure option was determined too. Given these, we computed the magnitude of the sure options, for Experiments 1-4, so that the EV difference of the risky and the sure option was at 5%. Namely, '()* : = (1 + (1 − 2 * riskyBetter)0.05) )@'AB )@'AB
In the end, we rounded the mag_sure towards the direction that agrees with the riskyBetter factor (if riskyBetter=1, we rounded mag_sure with the floor and if riskyBetter=0 with the ceiling function).
Safe options (Experiments 5 & 6) had also defined probabilities (50/50) and were set such that 'IJ* = '()* . Additionally, one of the two magnitudes was set equal to the EV_risky, so that both outcomes of the safe option were above (below) EV_risky when riskyBetter=0 (=1). So, the two outcomes for the safe option satisfied the following:
'IJ*K = )@'AB & 'IJ*L = 2 '()* − 'IJ*K
In the laboratory experiment (Experiment 7), the EV of the two options was set to be equal.
Hence, the magnitude of the sure option was determined given the magnitude and the probability of the risky one. Rounding to the closest integer was used in this experiment.


Statistical analysis
The main analyses included two dependent variables: risky choice (1 if one chooses the risky option and if one chooses the sure/safe one) and optimal choice (1 if one chooses the Expected Value-maximizing choice; 0 otherwise). We ran a regression analysis using a Generalized
Linear Mixed-Effects model (GLME). We used the canonical link function for response variables with the binomial distribution, namely the logit function. The method for estimating model parameters was maximum likelihood and the analysis was run with R. The predictors were feedback (categorical: absent/present), riskyBetter (categorical: no/yes), the probability of the risky option (categorical: low, medium, high for .1, .5, .9 respectively), the magnitude of the risky option (categorical: low and high for 40 and 60 respectively) and trial (continuous: from 1 to 10). In the basic model, we included the main effects. Specific pairwise interactions were also added when they were relevant. To account for individual differences, we incorporated a random effects structure with random intercepts and random slopes at the participant level. The main effects of all predictors were included as random slopes (the only exception being the predictor trial in the optimal choice case, which had a very small variance and led to singular fitting; therefore, it was excluded).
We also ran similar regressions for the dependent variable repeatRisky (1 if one chooses the risky option and 0 if one chooses the sure/safe one at time t, conditional on choosing the risky option at time t-1). The predictors were the probability of the risky option (specified as above)
and whether the previous (at time t-1) risky outcome was zero or positive (categorical: 0/1). As fixed effects, we included a basic model with the main effects only and a full model with the interaction too. Only the main effects of the predictors were used as random slopes. Note that the analysis was run only on feedback blocks.


Modeling
The Best Estimation And Sampling Tools (BEAST) model contains six free parameters -with one capturing attitudes towards ambiguity, so it is irrelevant to us. The key idea behind the BEAST model is that it assumes decisions to be determined by a combination of the best estimation of the expected value of an option and its subjective value determined by different sampling tools 
28
 . Four parameters in the model are dedicated to quantify the weight of different sampling tools (i.e., rules for drawing and comparing hypothetical outcomes) as well as parameters governing the probability of using one of these tools. In the original publication the authors estimated the best-fit parameters on a population level, allowing parameters of individual agents to be drawn from U(0, parambest-fit) or {1,…, parambest-fit) (depending on whether the parameter is continuous or discrete). Mean squared deviation and some additional constraints to capture various fundamental behavioral phenomena were used for estimating these population parameters. For all the details of the model and its optimization we refer to the original publication .
We used a different approach. We fixed a search space on the population level which was quite wider than the one resulting from the original optimization (the best fits for the parameters (σ, β, γ, θ, κ) was (7, 2.6, .5, 1, 3) in Erev, et al. (2017) while we used 
(10,
10,
1,
5,
6)
 as the search space). We computed the best-fit parameters on the participant level using maximum likelihood estimation (note that we fixed the individual parameters on the participant level and simulated each decision problem one hundred times to obtain the predicted R-rates for each subject). Then we ran simulations using the fitted parameters of each participant to obtain the predictions of the BEAST model and compare them with the behavioral data (but note that we also produced simulations using the originally proposed best-fitting parameters which lead to the very same conclusions).
We also submitted our data to a modeling analysis which involved a standard descriptive behavioral model -a variant of cumulative prospect theory 
74
 . Details about the model, the fitting procedures following standard guidelines 
75,
76
 , and the resulting statistical analyses can be found in the Supplementary Materials/Cognitive modeling (Prospect Theory).
Everything else was kept the same as in Exp.1.At the aggregate level Exp.2 replicated Exp.1 in all respects(Figure 1C and 1D; Figure 2D


and 2E;


Figure 2 :
2
behavioral results Experiments 1-2. (A) Risky choice rate (R-rate) as a function of the feedback condition in Exp.1. (B) Optimal choice rate (O-rate) as a function of feedback condition and whether or not the optimal response was the sure ('sureBetter') or the risky ('riskyBetter') option in Exp.1. (C) The colored lines represent the risky choice rate as a function of the feedback condition and the trial number within a block (Exp.1). (D) and (E) display the same variables as (A) and (B), but for Exp.2. (F) The panel displays the same variables as panel (C), but for Exp.2. In addition, the inset displays the effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) for the first trial for both Exp.1 (no effect) and Exp.2 (significant effect); significant difference between Exp.1 & Exp.2 (** P = 0.0022 two-sample two-tailed t-test). In (A), (B), (D) and (E), points indicate individual averages, violin plots indicate probability density functions, line segments connect the values of the participants in different conditions, orange lines have the direction of the means; black the opposite, boxes indicate 95% confidence interval and errors bars indicate s.e.m.. In (C) and (F), the central bold line is the mean of the individual averages in each condition and the shaded area above and below the mean is plus and minus, respectively, the s.e.m. of the individual averages. The solid grey line is drawn in trials displaying a significant difference (p < 0.01) between the two conditions.


), the regret premium we would expect in this case is negligible. And this is exactly what we observe in the behavioral data (Figure 3D & 3E, P = 10%).


Figure 3 :
3
behavioral results Experiments 3-4 and comparison to Experiments 1-2. (A) The colored lines represent the risky choice rate (R-rate) as a function of the feedback condition and the trial number within a block (Exp.3: no block-wise instructions). (B) The panel displays the same variables as panel (A) but for Exp.4 (blockwise instructions present). In addition, the inset displays the difference between the feedback and no feedback condition for the first trial for both Exp.3 (no effect) and Exp.4 (significant effect). (C) Risky choice rate as a function of the feedback condition and the probability level of the risky option in Experiments 1/2 (partial feedback). The numbers above the mean/error bars indicate the experiment being displayed. The shaded area of the violin plot indicates 95% confidence interval over the grouped data. (D) Same as (C) but for Experiments 3/4 (complete feedback). (E) The panel displays the effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) as a function of probability levels in the partial (Exps 1/2) and the complete (Exps 3/4) experiments. **p<0.01 two-sample t-test in the inset of (B). In (C) and (D) points indicate individual averages, violin plots indicate probability density functions, line segments connect the values of the participants in different conditions, orange lines have the direction of the means; black the opposite, boxes indicate 95% confidence interval and errors bars indicate s.e.m.. In (A) and (B), the central bold line is the mean of the individual averages in each condition and the shaded area above and below the mean is plus and minus, respectively, the s.e.m. of the individual averages. The solid grey line is drawn in trials displaying a significant difference (p < 0.01) between the two conditions.


Exp. 6 ,
6
(Exp.5: P < 0.01 for Trial1-10; Exp.6: P = 0.22 for Trial1, P < 0.01 for Trial2-10 BH-adjusted values of two-tailed WSRTs), and this attitudinal effect interacted with the probability of the risky option in the complete feedback condition (linear regression on the differences of individual average R-rates between F and nF conditions βpRISKY:TypeOfFeedback > 0 P = 0.014; logistic regression Table 1, choiceRisky: βFEEDBACK1 > 0, P = e-08, PFEEDBACK1:P_RISKY1 = 0.19, PFEEDBACK1:P_RISKY2 = 0.16 Exp5(2); PFEEDBACK1 = 0.86, βFEEDBACK1:P_RISKY1 > 0, P = e-09, βFEEDBACK1:P_RISKY2 > 0, P = e-21 Exp6(2)). This result illustrates that our key findings are not idiosyncratic to some design choices, and might therefore reflect a generalizable psychological effect. Leveraging this robustness, we completed our demonstration by a comprehensive assessment of our main claims, evaluated over our six experiments. This analysis confirmed that the attitudinal effect induced at the first trial was robustly elicited in the experiments featuring block instructions (Exps 2,4,5,6; Figure 4B) and vanished in the absence of the said instructions (Exps 1,3; Figure 4A). Consistent with different psychological mechanisms operating under partial or complete feedback regimens, the effect of feedback was identical across all levels of the probability of the risky option in the partial feedback experiments (Exps 1,2,4; Figure 4C), while significantly modulated by this factor in the complete feedback experiments (Figures 4D


Figure 4 :
4
behavioral results across experiments 1-6. (Α) The colored lines represent the risky choice rate (Rrate) as a function of the feedback condition and the trial number within a block (Experiments 1/3: no block-wise instructions). (B) The panel displays the same variables as panel (A) but for Experiment 2/4/5/6 (block-wise instructions present). In addition, the inset displays the effect of feedback on R-rates (namely, the difference of Rrates between Feedback and no-Feedback conditions) for the first trial both for experiments without instructions (no effect) and for the experiments with instructions (significant effect). (C) Risky choice rate as a function of the feedback condition and the probability level of the risky option in Experiments 1/2/5 (partial feedback). (D) Same as (C) but for Experiments 3/4/6 (complete feedback). (E) The panel displays the effect of feedback on R-rates (namely, the difference of R-rates between Feedback and no-Feedback conditions) as a function of the probability levels in the partial (Exps 1/2/5) and the complete experiments (Exps 3/4/6). ****p<0.0001. In (C) and (D) points indicate individual averages, violin plots indicate probability density functions, line segments connect the values of the participants in different conditions, orange lines have the direction of the means; black the opposite, boxes indicate 95% confidence interval and errors bars indicate s.e.m.. In (A) and (B), the central bold line is the mean of the individual averages in each condition and the shaded area above and below the mean is plus and minus, respectively, the s.e.m. of the individual averages. The solid grey line is drawn in trials displaying a significant difference (p < 0.01) between the two conditions.


Figure 5 :
5
feedback-induced trial-by-trial dynamics. (A) Risky choice rate (R-rate) in the second trial as a function of the feedback condition and the choice made in the first trial (sure/safe versus risky). (B) R-rate in the second trial as a function of the feedback condition and the outcome obtained in the first trial (zero versus positive). (C) and (D) result from (A) and (B) respectively, by collapsing the feedback dimension. (E) Theoretical predictions of repeating a risky choice at trial t as a function of the outcome of the risky option in trial t-1. In the bottom graph, two accounts are presented (the learning and the gambler's fallacy). In the top part, the color coding of the two conditions (risky choice being rewarded, with yellow, or not, with orange) is explained. (F) Behavioral results of the rate of repeating a risky choice using the data of experiments 1-6 and feedback blocks only, because the analysis cannot be performed in blocks without feedback. Repeat risky rate is plotted as a function of the outcome of the risky choice in the previous trial and of the probability level of the risky option. (G) The panel displays the difference of the rates of repeating a risky choice, between the case where the outcome of the risky choice in the previous trial was positive and the case where it was zero, across probability levels.


Supplementary Material/Erev et al. (2017) re-analysis for more details about the study and the decision problem selection). This re-analysis of Erev et al. (2017) data was consistent with our own results on the absence of positive effect of feedback on the optimal choice rate (without feedback 0.66±0.21; with feedback: 0.65±0.17; two-tailed WSRT P = .96, n = 446; Figure 6A), as well as on the increase in risk choice rate (without feedback: 0.38±0.22, with feedback 0.42±0.18; P = e-10;


Supplementary
Material/Erev et al. (2017) re-analysis & Figure S2). Overall, all the behavioral analyses that could be replicated in Erev et al. (2017) lead to similar results and conclusions as the ones performed on our own new data.


Figure 6 :
6
re-analysis of Erev et al. (2017). (A) Optimal choice rate as a function of the feedback condition. (B) Risky choice rate as a function of feedback condition and the probability level of the best outcome of the risky option. (C) The difference of risky choice rates between the feedback(F) and no feedback(nF) conditions across probability levels of the best outcome of the risky option. (D) Repeat risky choice rate as a function of the outcome of the risky choice in the previous trial and of the probability level of the best outcome of the risky option.


Consequently, the BEAST model is effective in capturing some of the behavioral patterns emerging in the set-up of Erev, et al. (2017) (a role for regret), while missing the negative recency effect also present in this dataset. Furthermore, we provide conclusive evidence that the model's predictions miss key behavioral findings, as soon as changes of the experimental set-up are implemented, such as the presence of explicit block-wise instructions (attitudinal change) and partial feedback (role for curiosity).


Figure 7 :
7
BEAST model predictions using fitted parameters of our behavioral data. (A) The colored lines represent the risky choice rate as a function of the feedback condition and the trial number within a block (Experiments without block instructions). (B) The panel displays the same variables as panel (A), but for


sex not available | aged 29.29 ± 9.24 years) from an online platform (www.prolific.com). The research was carried out following the principles and guidelines for experiments including human participants provided in the Declaration of Helsinki (1964, revised in 2013). The INSERM Ethical Review Committee/IRB00003888 approved the study on 13 November 2018, and participants were provided written informed consent before their inclusion. For the laboratory experiment, 30 healthy participants completed the experiment (18 females | aged 28 ± 7.22). Given Bellemare et al. 73 assert that 20 subjects are needed in a within-subject analysis to achieve a power of 80%, a sample size of 30 is expected to have enough power to detect any effect. The choice of model to analyze the dataset, the generalized mixed effect model, also requires a smaller sample size than traditional ANOVA. Participants were contacted via the "Relais d'information en sciences de la cognition" (RISC), part of the French "Centre national de la recherche scientifique". Participants enroll on the platform and as such, voluntarily accept to be contacted for scientific studies. Participants were recruited on the basis of their good understanding of French. They received an email in their mailing list containing a link to a questionnaire asking them for general information. After completion, they are contacted by the experimenter to agree on the day of the experiment. The experiment was approved by the local Ethics Committee of Ecole Normale Supérieure, Paris. Participants which, on average, lasted 45 minutes.


Incentives
For the online experiments, participants received a fixed compensation of £3 for about half an hour of engagement (average completion time in minutes: 28.47 ± 8.68). Additionally, we incentivized participants to reveal their true preferences by offering a monetary bonus determined by the outcome of a randomly selected trial of the testing phase (average bonus won in British pounds: 2.68 ± 2.21). For the laboratory experiment, participants received a show-up fee of 10€ for an average engagement of 45 minutes. To motivate the revelation of true preferences, an incentive system was settled on the basis of hypothetical gains or losses. Each decision gave a pay-off in points -a draw from the selected option payoff distribution. They were told that their goal was to maximize their number of points in the gain domain and to minimize the number of losses. The total of points would determine their final payoff. The conversion rate between experimental units (EU) and euros was set according to the maximum and minimum possible payoffs at 0.02€/EU. The payoff structure was determined such that no participant would incur a loss. On average, they received 15€. At the end of the experiment, participants received and signed the reception of their payments.


Table 1 : main results of the logistic regression analyses. Onlythe Supplementary Materials/Regression Tables S1-S3. For more details on the regressions, see the Statistical Analysis section of the Methods.
1
the predictors discussed in the main text are included here. For the detailed tables, containing all the predictors, see


The total number of trials, 270, includes training trials. So, we kept the complete submission of participants that had, for example, two submissions one with only a few trials and a complete one.














Comportement de l'Homme Rationnel devant le Risque: Critique des Postulats et Axiomes de l'Ecole Americaine




M
Allais






Le








Econometrica




21
















Prospect Theory: An Analysis of Decision under Risk




D
Kahneman






A
Tversky








Econometrica




47
















Regret in Decision Making under Uncertainty




D
E
Bell








Operations Research




30
















Regret Theory: An Alternative Theory of Rational Choice Under Uncertainty




G
Loomes






R
Sugden








The Economic Journal




92
















An Axiomatization of Cumulative Prospect Theory




P
Wakker






A
Tversky








Journal of Risk and Uncertainty




7
















The impact of experience on decisions based on pre-choice samples and the face-or-cue hypothesis




I
Erev






O
Yakobi






N
J S
Ashby






N
Chater








Theory Decis




92
















Description-experience Gaps: Assessments in Other Choice Paradigms




E
Fantino






A
Navarro








Journal of Behavioral Decision Making




25
















The influence of biased exposure to forgone outcomes




O
Plonsky






K
Teodorescu








Journal of Behavioral Decision Making




33
















Timing of descriptions shapes experience-based risky choice




L
Weiss-Cohen






E
Konstantinidis






N
Harvey








Journal of Behavioral Decision Making




34
















The description-experience gap: a challenge for the neuroeconomics of decision-making under uncertainty




B
Garcia






F
Cerrotti






S
Palminteri








Philosophical Transactions of the Royal Society B: Biological Sciences




376


20190665














The description-experience gap in risky choice




R
Hertwig






I
Erev








Trends in Cognitive Sciences




13
















A Description-Experience Framework of the Psychology of Risk




R
Hertwig






D
U
Wulff








Perspect Psychol Sci




17
















How experimental methods shaped views on human competence and rationality




T
Lejarraga






R
Hertwig








Psychological Bulletin




147
















The Probability Weighting Function




D
Prelec








Econometrica




66
















Noisy retrieval models of over-and undersensitivity to rare events




D
Marchiori






S
Di Guida






I
Erev








2














The role of experience in decisions from description




B
R
Newell






T
Rakow








Psychonomic Bulletin & Review




14
















Minding the Gap: On the Origins of Probability Weighting and the Description-Experience Gap




R
Oprea






F
M
Vieider














Context-dependent outcome encoding in human reinforcement learning




S
Palminteri






M
Lebreton








Current Opinion in Behavioral Sciences




41
















The computational roots of positivity and confirmation biases in reinforcement learning




S
Palminteri






M
Lebreton








Trends in Cognitive Sciences




26
















Mental Models and Learning: The Case of Base-Rate Neglect




I
Esponda






E
Vespa






S
Yuksel








American Economic Review




114
















Debiasing on a Roll: Changing Gambling Behavior Through Experiential Learning




M
Abel






S
A
Cole






B
Zia














SSRN Scholarly Paper at








The Bias Bias in Behavioral Economics




G
Gigerenzer








RBE




5
















Debiasing Decisions: Improved Decision Making With a Single Training Intervention




C
K
Morewedge








Policy Insights from the Behavioral and Brain Sciences




2


















B
Fischhoff






A
Debiasing ; Tversky






D
Kahneman






P
Slovic




10.1017/CBO9780511809477.032


Judgment under Uncertainty: Heuristics and Biases


Cambridge




Cambridge University Press
















Nudge Versus Boost: How Coherent are Policy and Theory? Minds & Machines




T
Grüne-Yanoff






R
Hertwig








26














Action Change Theory: A Reinforcement Learning Perspective on Behavior Change




I
Vlaev






P
Dolan








Review of General Psychology




19
















From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience




I
Erev






E
Ert






O
Plonsky






D
Cohen






O
Cohen








Psychol Rev




124
















Decision by sampling




N
Stewart






N
Chater






G
D A
Brown








Cognitive Psychology




53
















Using largescale experiments and machine learning to discover theories of human decision-making




J
C
Peterson






D
D
Bourgin






M
Agrawal






D
Reichman






T
L
Griffiths








Science




372


















O
Plonsky






I
Erev






T
Hazan






M
Tennenholtz






Forest




Predicting Human Behavior. Proceedings of the AAAI Conference on Artificial Intelligence






31












Predicting human decisions with behavioral theories and machine learning




O
Plonsky


















Modelling dataset bias in machine-learned theories of economic decisionmaking




T
Thomas








Nat Hum Behav




8
















The neuroeconomics of epistemic curiosity




A
Buyalskaya






C
F
Camerer








Current Opinion in Behavioral Sciences




35
















Valuation of knowledge and ignorance in mesolimbic reward circuitry




C
J
Charpentier






E
S
Bromberg-Martin






T
Sharot








Proceedings of the National Academy of Sciences




115
















People adaptively use information to improve their internal states and external outcomes




I
Cogliati Dezza






C
Maher






T
Sharot








Cognition




228


105224














The role of expecting feedback during decisionmaking under risk




F
Rigoli






C
Martinelli






S
S
Shergill








Neuroimage




202


116079














Preschoolers search longer when there is more information to be gained




A
Ruggeri






O
Stanciu






M
Pelz






A
Gopnik






E
Schulz








13411












Towards a neuroscience of active sampling and curiosity




J
Gottlieb






P.-Y
Oudeyer








Nat Rev Neurosci




19
















How people decide what they want to know




T
Sharot






C
R
Sunstein








Nat Hum Behav




4
















Consequences of regret aversion: Effects of expected feedback on risky decision making




M
Zeelenberg






J
Beattie






J
Van Der Plight






N
K
De Vries








Organizational Behavior and Human Decision Processes




65
















Regret and its avoidance: a neuroimaging study of choice behavior




G
Coricelli








Nat Neurosci




8
















On the impact of experience on probability weighting in decisions under risk




D
Cohen






O
Plonsky






I
Erev








7














Investigating the origin and consequences of endogenous default options in repeated economic choices




J
Couto






L
Maanen






M
Van & Lebreton








PLOS ONE




15


232385














Rational Choice and the Framing of Decisions




A
Tversky






D
Kahneman








The Journal of Business




59
















The Theory of Probability




S
H
Jeffreys








Oxford University Press


Oxford, New York
















R
E
Kass






A
E
Raftery








Bayes Factors. Journal of the American Statistical Association




90
















The hot hand fallacy and the gambler's fallacy: Two faces of subjective randomness?




P
Ayton






I
Fischer








Memory & Cognition




32
















The role of experience in the Gambler's Fallacy




G
Barron






S
Leider








Journal of Behavioral Decision Making




23
















The experience-description gap and the role of the inter decision interval




K
Teoderescu






M
Amir






I
Erev








Prog Brain Res




202
















Notes: The "Gambler's Fallacy" in Lottery Play




C
T
Clotfelter






P
J
Cook




10.1287/mnsc.39.12.1521






Management Science
















Cognitive model priors for predicting human decisions




D
D
Bourgin






J
C
Peterson






D
Reichman






S
J
Russell






T
L
Griffiths








Proceedings of the 36th International Conference on Machine Learning


the 36th International Conference on Machine Learning




PMLR
















Feedback Influences Discriminability and Attractiveness Components of Probability Weighting in Descriptive Choice Under Risk




S
Goyal






K
P
Miyapuram








Front. Psychol




10














Incorporating conflicting descriptions into decisions from experience




L
Weiss-Cohen






E
Konstantinidis






M
Speekenbrink






N
Harvey








Organizational Behavior and Human Decision Processes




135
















The Role of Personal Experience in Contributing to Different Patterns of Response to Rare Terrorist Attacks




E
Yechiam






G
Barron








Journal of Conflict Resolution -J CONFLICT RESOLUT




49














Feedback produces divergence from prospect theory in descriptive choice




R
K
Jessup






A
J
Bishara






J
R
Busemeyer








Psychol Sci




19
















Effects of feedback and complexity on repeated decisions from description




T
Lejarraga






C
Gonzalez








Organizational Behavior and Human Decision Processes




116
















Protecting the self from the negative consequences of risky decisions




R
A
Josephs






R
P
Larrick






C
M
Steele






R
E
Nisbett








J Pers Soc Psychol




62
















Experience and rationality under risk: re-examining the impact of sampling experience




I
Aydogan






Y
Gao








Exp Econ




23
















The psychology of curiosity: A review and reinterpretation




G
Loewenstein








Psychological Bulletin




116
















The experience of regret: What, when, and why




T
Gilovich






V
H
Medvec








Psychological Review




102
















Learning about unchosen alternatives: When does curiosity overcome regret avoidance?




D
F
Caldwell






J
M
Burger








Cognition and Emotion




23
















When and why do we want to know? How experienced regret promotes post-decision information search




Y
Shani






M
Zeelenberg








Journal of Behavioral Decision Making




20
















When curiosity killed regret: Avoiding or seeking the unknown in decision-making under uncertainty




E
Van Dijk






M
Zeelenberg








Journal of Experimental Social Psychology




43
















Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning




S
Bavard






A
Rustichini






S
Palminteri








Science Advances




7


340














Learning relative values in the striatum induces violations of normative decision making




T
A
Klein






M
Ullsperger






G
Jocham








Nat Commun




8














Signals in Human Striatum Are Appropriate for Policy Update Rather than Value Prediction




J
Li






N
D
Daw








J. Neurosci




31
















Learning and the Economics of Small Decisions (Chapter 10). in The Handbook of




I
Erev






E
Haruvy








Experimental Economics




2






Princeton University Press












The influence of biased exposure to forgone outcomes




O
Plonsky






K
Teodorescu








Journal of Behavioral Decision Making




33
















Reinforcement Learning: An Introduction




R
Sutton






A
Barto








The MIT Press












Bayesian Models of Perception and Action




W
J
Ma






K
P
Kording






D
Goldreich








The MIT Press












A choice prediction competition: Choices from experience and from description




I
Erev








Journal of Behavioral Decision Making




23
















Experiential values are underweighted in decisions involving symbolic options




B
Garcia






M
Lebreton






S
Bourgeois-Gironde






S
Palminteri








Nature Human Behaviour




7














Statistical Power of Within and Between-Subjects Designs in Economic Experiments




C
Bellemare






L
Bissonnette






S
Kröger














Advances in prospect theory: Cumulative representation of uncertainty




A
Tversky






D
Kahneman








J Risk Uncertainty




5
















The Importance of Falsification in Computational Cognitive Modeling




S
Palminteri






V
Wyart






E
Koechlin








Trends in Cognitive Sciences




21
















Ten simple rules for the computational modeling of behavioral data




R
C
Wilson






A
G
Collins








Elife




8


49547















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]