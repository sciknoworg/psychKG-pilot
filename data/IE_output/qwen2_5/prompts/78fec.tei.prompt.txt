You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Probabilistic forecasting guides dynamic decisions
Imagine that you are working on a final course project. One option is easier to get started with, but kind of incremental. The other option is more ambitious, but could take more time to see results. What would you choose if the project is due tomorrow versus in a month? And what if you have already made some progress towards one of them? Would you switch to the other if you feel stuck? We often have to make these decisions in life, from choosing which job to take on to which intern to hire. These decisions also have long-term ramifications-for example, switching jobs early in one's career can lead to a larger increase in wages compared to later in one's career 
(Topel and Ward, 1992)
. However, deciding which asset to invest in and when to divest from the current asset can be difficult; it involves anticipating how these assets will develop and adjusting decisions based on the various factors at play, such as how much time you have, how fast these assets grow, and how much progress has been made on the asset at hand. This type of dynamic decision making problem has been extensively studied in a number of fields, including reinforcement learning (RL), patch foraging, and study time allocation.
Although each field makes their own assumptions, they all share a key feature: the agent maximizes cumulative rewards. This is different from the asset selection and switching problems we describe, where the agent's goal is to maximize the reward from an asset at a specific timestep (e.g., submitting a final project at a deadline), and any progress made on the other assets (projects that were not submitted) does not count towards the reward. Below, we review related frameworks for dynamic decisions and their connections to the current problem of interest. See 
Table 1
 for a summary.


Related frameworks for dynamic decisions
One relevant framework is Markov Decision Processes (MDPs). By defining a transition function that captures environmental dynamics, MDPs have laid the foundation for numerous RL algorithms. This framework has been extensively linked to human decision making and planning 
(Niv, 2009;
Daw et al., 2011;
Dayan and Daw, 2008)
. At the core of many RL algorithms is the Bellman Equation, which operates recursively under two key assumptions: 1) the Markov property-state transitions depend only on the current state and action, not the past-and 2) time-homogeneity-the probability of transitioning between states under a given action remains invariant over time 
(Sutton and Barto, 2018)
. These assumptions become restrictive in dynamic decision problems where asset performance exhibits long-range temporal dependencies. To see this, suppose we have two assets with identical current performance, thus sharing the same state under a standard MDP formulation. However, one asset's performance may have already plateaued, while the other is still improving. The Markovian transition structure of time-homogeneous MDPs cannot distinguish these two cases, making it inadequate for forecasting future asset performance. While this problem could be addressed by encoding all past time points within a single state representation or defining a separate MDP for each asset, these approaches can quickly become computationally intractable and hard to generalize across assets due to the high dimensionality and massive redundancy involved 
(Sutton and Barto, 2018)
. While RL algorithms that relax time-homogeneity exist 
(Padakandla et al., 2020)
, they primarily focus on change point detection rather than modeling long-range temporal dependencies, and their applications to human behavior remain rare.
A second framework for dynamic decisions is patch foraging, which models how agents exploit resources within their current patch before deciding whether to move elsewhere. The normative marginal value theorem (MVT) prescribes leaving a patch when its reward rate falls below the average reward rate across all available patches 
(Charnov, 1976)
. A key assumption underlying MVT and its related heuristics is that resource availability in a patch decreases over time with continued consumption 
(Charnov, 1976;
Kilpatrick et al., 2021;
Krebs, 1973;
Green, 1984;
Gibb, 1962;
Lin and von Helversen, 2023;
Hall-McMaster et al., 2021)
, resources within the current patch are assumed to decline monotonically. However, many real-world dynamic decisions violate this assumption. In cases where continued investment leads to growth-such as skill development, research progress, or project outcomes-the chosen asset would improve over time and eventually outperform undeveloped alternatives. In these scenarios, foraging algorithms prescribe a myopic strategy of comparing current asset performance, neglecting the forecasting of future performance altogether 
(Constantino and Daw, 2015)
. This raises an important question:
under what conditions do foraging-based strategies break down in such settings, and do humans still adhere to them despite the potential drawbacks?
A third relevant domain is study time allocation, which examines how individuals distribute time across multiple learning tasks to maximize their summed final performance at some time horizon. This objective is computationally equivalent to maximizing cumulative performance gains over time, making it more akin to foraging than asset selection/switching (which focuses instead on maximizing the final performance of the single chosen asset). Empirical studies in study time allocation suggest that people prefer tasks that they are worse at and tasks that they can improve faster at 
(Son and Kornell, 2008;
Metcalfe and Kornell, 2005;
Ten et al., 2021;
Tekin, 2022)
. Some models incorporate improvement velocity indirectly through a foraging-like mechanism, where continued engagement is reinforced by rewards that decrease monotonically over time 
(Green, 1984;
Payne et al., 2007)
. Like foraging models, study time allocation models remain inherently myopic, as they do not explicitly forecast future asset performance. In addition, these models assume that the agent can switch back to an old task to build it further, whereas in asset selection and switching problems, it depends on the situation-a researcher may switch back to a previous project, while an employee who quits a position usually cannot return later.
A final framework is optimal stopping, which models decision-making in sequential choice tasks where agents must select the best available option upon its presentation, without the ability to return to previously encountered options 
(Taylor, 1968)
. The predominant models for explaining human decisions in these tasks are threshold models, where an option's value is compared against a position-dependent threshold, and selection occurs once the threshold is exceeded 
(Baumann et al., 2020;
Guan et al., 2014;
Lee, 2006)
. Although optimal stopping introduces a temporal component, its structure is fundamentally misaligned with asset selection/switching problems. Here, dynamism exists only in the presentation of options, not in their development over time. Once an option is chosen, its value remains fixed, and there is no subsequent notion of switching. Even if the entire sequence of presented options were treated as a fluctuating timeseries of utilities, the framework still lacks the key feature of decision-making among dynamically evolving alternatives. Given such incompatibility, we will not consider optimal stopping models during our investigation.


Problem scope of interest
We begin by outlining the defining characteristics of asset selection and switching problems. First, assets develop only when the agent invests in them and remain unchanged otherwise. Each asset could thus be described by a performance trajectory, specifying how its performance changes over time since the start of its investment. In general, assets tend to improve under sustained investment-some assets, such as course projects, exhibit monotonically increasing performance, while others, like jobs or relationships, may fluctuate. We focus on the increasing case, as it is the possibility of improvement that renders past algorithms (e.g., foraging) suboptimal and underscores the need for new frameworks.
Second, the agent can only invest in one asset at a time. This naturally incurs an opportunity cost, where time spent developing one asset cannot be used to develop another asset.
Third, the agent maximizes reward at a particular time horizon. This means that only the performance of the asset invested at this timepoint matters. For example, a student might switch between two final course projects, but can only submit one for evaluation. Similarly, jobs and relationships we leave do not count towards our reward anymore. In some cases, the time horizon is explicitly set, such as a final project deadline. In other cases, the time horizon can be seen as an approximation of when this reward might matter (e.g., an employee's rank at retirement).


Table 1
Comparison between asset selection/switching and other dynamic decision problems. expected final performance at the relevant time horizon and compare them to make an informed choice. Over time, they update their prior beliefs for the current asset based on observed development, form posterior final performance estimates, and decide whether to stay or switch. In three behavioral experiments ( = 380), we compare this model to alternative models inspired by heuristics in the fields of patch foraging and study time allocation. In the following sections, we will outline the rationale behind our model's assumptions and provide a detailed explanation of the model itself.


Dynamically evolving alternatives


Maximize


Modeling background and motivations
Our Bayesian framework, which integrates extrapolation and comparison, draws inspiration from prior research in human forecasting, function learning, and evidence accumulation. In this subsection, we will outline the key theoretical motivations and connections that inform this approach.


Human forecasting and function learning
Asset performance trajectories can be naturally framed as time series-one-dimensional functions that evolve over time. From this perspective, agents must infer patterns in an asset's past performance, represent them as functions, and extrapolate these trends to anticipate future outcomes. Here, we review the theoretical foundations supporting this view, drawing from research on human forecasting and function learning.
The human ability to forecast future outcomes has long intrigued researchers from both economic and psychological traditions 
(Jones, 1979;
De Bondt, 1993;
Harvey et al., 1994;
Andreassen, 1990;
Sanders and Ritzman, 1992)
. While the human forecasting of timeseries-often referred to as "judgmental forecasting"-demonstrates various biases 
(Goodwin and Wright, 1993)
, they frequently match the performance of quantitative forecasting models in real-world applications 
(Lawrence et al., 2006)
. Given the ecological relevance of predicting future outcomes, how humans achieve such forecasting accuracy is a problem with both scientific and practical importance.
A significant body of research suggests that humans rely on extrapolation when explicitly forecasting timeseries, although the precise cognitive mechanisms underlying this process remain debated 
(Andreassen, 1990;
Harvey et al., 1994)
. The tendency to extrapolate-using past data points to inform future predictions-likely stems from an inductive bias that assumes past trends will continue (De 
Bondt, 1993)
. Empirically, This reliance on past data is evident not only in the forecasting of artificially created functions 
(DeLosh et al., 1997;
Harvey et al., 1994)
 but also when augmented with empirical contexts such as stock market predictions and expert judgments 
(Hohle and Teigen, 2015;
Johnson and Tuckett, 2022)
. Even in cases where domain-specific knowledge discourages extrapolation, human forecasters often still exhibit this tendency 
(Johnson and Tuckett, 2022)
.
Early models of human forecasting commonly assumed linear extrapolation, positing that individuals expect a perceived trend to persist indefinitely 
(Andreassen, 1990;
Harvey et al., 1994
). However, subsequent empirical findings challenged this assumption. First, when faced with timeseries that exhibit stable linear trends, human forecasts do not strictly follow these trends but instead show damping, where predicted growth or decline is attenuated relative to a purely linear extrapolation 
(Lawrence et al., 2006;
Harvey and Reimers, 2013)
. Second, people do not rely on a fixed extrapolation rule, but adjust their forecasts based on the functional form of the underlying trend and the degree to which past data points inform future values 
(DeLosh et al., 1997;
Li et al., 2023)
. These observations indicate that human forecasting incorporates more than simple linear extrapolation.
A crucial factor in resolving these discrepancies is contextual information. In real-world forecasting, we rarely encounter timeseries in isolation; rather, we interpret them in light of their semantic context, our prior experience with similar timeseries, as well as external knowledge (e.g., knowing that an energy shortage could influence stock prices; 
Lawrence et al., 2006;
Harvey and Reimers, 2013)
. This integration of contextual information is particularly beneficial when the observed timeseries is noisy and offers little information for forecasting 
Ritzman, 1992, 1995)
. Moreover, incorporating context resolves key empirical puzzles regarding damping effects. Specifically, damping can be understood as a regression-to-the-mean effect: when a timeseries exhibits higher-than-average growth, people tend to damp their predictions downward, whereas for below-average growth, they may anti-damp-overestimating future changes 
(Harvey and Reimers, 2013)
. Similarly, in environments where past time points are highly informative, human forecasts reflect strong extrapolation; in contrast, when past values are weakly predictive, people shift toward a learned average trend across timeseries 
(Li et al., 2023)
.
Despite progress in identifying the role of context in human forecasting, questions remain on how contextual information is quantitatively integrated into decision-making. A promising approach builds on Bayesian inference, which offers a normative account of how contextual knowledge should be combined with past observations to refine future predictions 
(Harvey and Reimers, 2013)
. This perspective is supported by adjacent research on Bayesian function learning, which suggests that humans can indeed learn functions from sequentially presented data points, which mirrors the real-world presentation of timeseries 
(Lucas et al., 2015;
Schulz et al., 2017)
. It has also been shown that Bayesian function learning is used to support downstream decision tasks like multi-armed bandits 
(Schulz et al., 2018;
Wu et al., 2018;
Schulz et al., 2020)
. This is similar to the role of function learning in our framework, though without long-term forecasting. Lastly, recent studies highlight humans' capacity for meta-learning functions 
(Li et al., 2023)
, which involves representing and memorizing priors over functions. This meta-learning ability is consistent with the empirically observed damping and anti-damping tendencies in the forecasting literature, and further supports our framework's assumptions on prior knowledge.
While significant progress has been made in understanding forecasting and function learning, their connection to asset selection and switching remains underexplored. Most quantitative studies focus on functions in isolation, emphasizing estimation and prediction rather than how forecasts guide downstream decision-making 
(Harvey and Reimers, 2013;
Lucas et al., 2015;
Li et al., 2023)
. Some work has linked forecasting mechanisms to value estimation in reinforcement learning 
(Schulz et al., 2018;
Wu et al., 2018;
Schulz et al., 2020)
, but less attention has been given to their role in dynamic decisions with long-term consequences. As a result, it remains unclear how humans integrate forecasting into asset selection and switching, or whether such cognitive processes play a direct role in these decisions.


Bayesian evidence accumulation
After forecasting each asset's future performance, the agent must compare these projections in a way that informs asset selection and switching decisions. The decision to stay or switch can be framed as a two-alternative forced-choice (2AFC) problem under noisy evidence inputs, a paradigm extensively studied in cognitive science featuring deep connections with Bayesian inference 
(Bogacz et al., 2006;
Ratcliff and McKoon, 2008)
. Traditional evidence accumulation models often assume that decision-makers track some measure of relative advantage over time. For example, in the classical drift-diffusion model (DDM) for perceptual decisions, this advantage is represented as the log posterior ratio (i.e., the difference in log posterior probabilities) between two alternatives 
(Ratcliff and McKoon, 2008)
. Similar difference computations underpin more recent evidence accumulation models developed for value-based decision-making, which have been successful in capturing both empirical choice and decision time data 
(Pedersen et al., 2017;
Fontanesi et al., 2019;
van Ravenzwaaij et al., 2020)
. Our modeling approach follows this principle by assuming that agents track the difference in the posterior or prior mean final performance between options over time, using this as the basis for asset selection and switching decisions.
A limitation of the advantage-based approach is that it is specifically suited for 2AFC tasks, where a difference term is computable. This setup captures asset selection between a pair of assets, as well as switching decisions where the agent transitions into a randomly selected new asset. However, in scenarios where the agent must choose among multiple assets or has the flexibility to select a specific new asset to switch into (Hall-McMaster and Luyckx, 2019), the framework requires extensions to accommodate multi-alternative evidence accumulation.
Addressing this challenge connects to a broader class of models designed for multi-alternative evidence accumulation 
(Brown and Heathcote, 2008;
Tajima et al., 2019;
Kilpatrick et al., 2021)
.
Since the broader domain of asset selection and switching remains relatively understudied, we begin with the simplest 2AFC case as a foundation for understanding these decisions. Extending the framework to more complex task setups remains a valuable future direction.


Theoretical framework
For asset selection and switching problems, we propose a normative framework that extrapolates asset performance trajectories into the future and estimates asset final performance at the time horizon.
Consistent with typical Bayesian inference models, we assume that agents, either through evolution or experience, have formed prior distributions over asset performance trajectories, which are instantiated as parametric functions over time. This prior knowledge allows agents to compute the prior mean of each asset's performance at the designated time horizon, which reflects the utility of selecting each asset. These utilities can then be compared to solve asset selection problems. In asset switching problems, agents additionally observe the current asset's performance (data points along the trajectory) up to the current time, which allows them to evaluate the likelihood of various trajectory shapes. By integrating likelihoods with the prior trajectory distribution, agents can derive the posterior mean final performance associated with staying with the current asset (i.e., the utility of staying). This posterior mean quantity can then be compared with the prior mean final performance of switching to a new asset and developing it in the remaining time (i.e., the utility of switching), which informs stay-switch decisions.
In the following sections, we will quantitatively formulate the Bayesian model and other alternative models of interest.


The Bayesian final performance (BFP) model
The BFP model uses past performance data to make predictions about future performance.
Time is indexed by ≥ 0. The time horizon (i.e., the time point at which asset performance is evaluated) is denoted as > 0. For an asset , its expected performance trajectory is represented as ( ; θ ) ∈ R, where ≥ 0 denotes the relative time since the beginning of asset investment, and θ denote the trajectory's parameters. We consider ( ; θ ) as an intrinsic property of the asset, specifying how its performance improves under sustained investment. The expected performance trajectory may take any arbitrary functional form; our models used monotonically increasing logistic curves, which we define later. The agent aims to maximize the asset final performance at = through asset selection or stay-switch decisions.
We begin by presenting the model for asset switching, as asset selection is a special case with = 0, before any data have been observed. We assume that agents have already learned a prior over trajectory parameters, (θ ), and that the agent starts with investing in asset from time 0 to the current time > 0. The current observed performance for asset , denoted ( ) ∈ R, is assumed to follow a univariate Gaussian distribution centered at ( = ; θ ) with a standard deviation , capturing external noise that affects momentary asset performance (or observation noise):
( ) ∼ N ( ; θ ), 2 .
(1)
Because the agent is currently investing in asset , let r ( ) = ( (0), (1), . . . , ( )) represent all performance observations of asset up to and including . The agent updates its posterior distribution over the trajectory parameters according to Bayes' rule:
(θ |r ( )) ∝ (θ ) (r ( )|θ ) = (θ ) ′ =0 N ( ′ ); ( ′ ; θ ), 2 .
(2)
Using the updated posterior, the agent computes the posterior predictive expectation of the current asset's final performance ( ):
E[ ( )|r ( )] = ∫ ( ; θ ) (θ |r ( )) θ .
(3)
The agent also calculates the prior predictive expectation of the final performance E[ ( )] for a randomly sampled new asset if they were to switch now at . The final performance of the new asset reflects the time remaining to improve it from its baseline, which is ( − ):
E[ ( )] = ∫ ( − ; θ ) (θ ) θ . (4)
The equivalence of priors (θ ) and (θ ) depends on the task's generative process. The new asset's performance trajectory could be sampled from the same distribution as the current asset. It could also reflect a new asset type that only became available after = 0, featuring a fundamentally different trajectory distribution. The non-equivalent and equivalent scenarios are tested in Experiments 2 and 3, respectively. In either case, the agent computes the expected performance difference between staying and switching, which reflects the advantage of the former option over the latter:
Δ = E[ ( )|r ( )] − E[ ( )].
(5)
To produce a stay-switch decision for the current time , the agent feeds the expected performance difference into a logistic decision rule:
(stay) = 1 1 + exp(− (Δ − ))
(6)
where and are the slope and offset parameters of the logistic function, capturing decision noise and bias, respectively.
Asset selection is equivalent to computing Δ between two assets and with different prior distributions (θ ) and (θ ), in the absence of performance observations at = 0. All other aspects of the model remain the same.
The BFP model has three free parameters ( , , ) that can be fit to human data. We compare two versions of the model, one with as a free parameter and another with = 0. 7LPH 3HUIRUPDQFH
$ 7LPH 3HUIRUPDQFH % 0RGHOV %)3PRGHO %9PRGHO (+PRGHO 9+PRGHO


Figure 1
Overview of models. (A) Asset selection. The black solid vertical line denotes current time = 0, and the dotted vertical line denotes the task's time horizon ( = 6 used as an example). We assume that agents have some intuitive knowledge of the prior distribution of asset performance trajectories over time (blue and orange denote prior distributions for two example assets; semi-transparent lines and ribbons denote prior mean±SD). (B) Asset switching. Agents observe the current asset's performance up to the current time (using the orange asset and time = 2 as an example), and use this information to estimate the current asset's posterior mean final performance (semi-transparent orange lines and ribbons denote posterior mean±SD of the current asset). They also extrapolate how the other asset would develop based on their prior knowledge (semi-transparent blue lines and ribbons denote prior mean±SD of the other asset), suppose they were to switch at the current time. For both (A) and (B), the Bayesian final performance (BFP) model compares the prior/posterior mean final performance of different assets at the time horizon (green points), preferring the asset with higher estimated mean final performance. The Bayesian velocity (BV) model computes future velocity (the slopes of red lines) from the current performance of both assets to their prior/posterior mean final performance, preferring the asset with faster growth. The Exploitation heuristic (EH) model prefers the asset with higher current performance (purple points). For asset switching, we additionally consider a velocity heuristic (VH) model which compares the recent velocity of the current asset's performance (between the current time and the previous one) to its average velocity from = 0 to the current time (the slopes of brown lines; close to flat in this example), preferring to stay if the former is greater.


Alternative models
To assess whether the BFP model can explain human asset selection and switching better than preexisting models, we contrasted it with three alternative models ( 
Figure 1)
. We use the same policy (Eq. 5) for all the alternative models. As with the BFP models, we fit models with and without a bias term.
The Bayesian velocity (BV) model. The BV model infers the final performance of each option in a way identical to the BFP model. However, inspired by previous studies on training workers 
(Xiang et al., 2024)
, we assume instead that agents prefer to see faster performance improvement from the current time to the time horizon , even if such decisions may lead to low final performance. This is operationalized through a different definition of the advantage term Δ:
Δ BV = E [ ( )|r ( )] − E [ ( )|r ( )] − − E[ ( )] − E[ ( )] −
(7)
where E [ ( )|r ( )] is the posterior mean final performance of trajectory computed identically to the BFP model, and E [ ( )|r ] is its posterior mean current performance, informed by past observations of the current asset's trajectory. Similarly for the new asset , E ( ) and E ( ) are its prior mean final performance and prior mean current performance respectively. Intuitively, the BV model prefers to invest in assets that predict greater future growth before the time horizon.
The BV model contains the same free parameters as the BFP model: ( , , ).
The exploitation heuristic (EH) model. The EH model is inspired by MVT in foraging.
It is a myopic heuristic model that directly compares the currently observed asset performances, such that its decisions do not depend on the future time horizon :
Δ EH = ( ) − E ( ) ,
(8)
where E ( ) is the new asset's prior mean current performance. This advantage term is akin to MVT, which compares the richness of the current patch with the average patch in the environment to make stay-switch decisions.
Given that the EH model only uses raw observed performances and the prior distribution, there is no Bayesian updating that necessitates the parameter . Hence, the model only has 2 free parameters ( , ).
The velocity heuristic (VH) model. The VH model is inspired by study time allocation models, which emphasize the role of current performance improvement velocity in determining stay-switch decisions. Notably, it is the only model that does not compare different options to each other, but instead compares the current asset's recent improvement velocity with its average velocity in the past. Hence, the VH model is only possible for asset switching, where the notion of "past velocities" applies when > 0. As a myopic heuristic model, it also directly uses raw observations of asset performances to compute recent and average velocities, ignoring the time horizon :
Δ VH = ( ) − ( − ) − ( ) − (0)
(9)
where the first term is the most recent change in velocity over a step size of (which we assume to be 1), and the second term is the average change in observed performance since the beginning of the trajectory. This is akin to study time allocation models that increasingly prefer to switch out of the current asset when its performance plateaus. The VH model has 2 free parameters ( , ).


Summary of models
Overall, we have proposed four computational models-the Bayesian final performance (BFP) model (3 parameters), the Bayesian velocity (BV) model (3 parameters), the exploitation heuristic (EH) model (2 parameters), and the velocity heuristic (VH) model (2 parameters), each including the logistic offset parameter . In addition, we consider the counterpart of each model without the logistic offset parameter . In Experiments 2 and 3 (asset switching), we fit all eight models to behavioral data, visualize model predictions using fitted parameter values, and perform quantitative model comparison using 5-fold cross-validation. In Experiment 1 (asset selection), the VH model is unable to make predictions. Therefore, we fit the remaining six models and perform model comparison.


Experiment 1
Experiment 1 explores the asset selection problem, where participants chose between two assets at = 0. This mirrors real-world scenarios such as a student choosing a final project, an employer selecting a trainee to hire, or a recent graduate weighing different job offers. In the experiment, the two candidate assets differ in their prior distributions, manifesting as different characteristic shapes. One asset features a higher starting point but slower improvement over time, which we call the "start-high-grow-slow" asset. In contrast, the other asset features a low starting point but faster improvement, which we call the "start-low-grow-fast" asset. The two assets are visualized in 
Figure 2A
. This design elicits qualitatively different model predictions.
To make the experiment intuitive for participants, we asked them to imagine themselves as the leader of a scientific team preparing for an imminent alien attack. They must build a defense system powered by one of two energy types (the "assets" to choose between). Each energy type develops according to a different performance trajectory family ("start-high-grow-slow" or "start-low-grow-fast"), which they observed during training. The participants' goal was to choose the energy type they would use that would maximize the performance of the defense system at the time of attack.
We asked participants three hypothetical questions, each assuming a different attack time.
Critically, participants were not told these time points until after they had observed all the trajectories. This design rules out the possibility of using simple associations to respond.
The four models make different predictions. The BFP model predicts that the probability of choosing the "start-low-grow-fast" asset increases with the amount of time left to develop the system. In contrast, the BV model predicts that the highest probability of choosing the "start-low-grow-fast" asset happens at its peak velocity and decreases as the growth assymptotes (anticipating slower future growth). The EH model predicts the same probability of choosing the "start-low-grow-fast" asset across time points. The VH model could not make predictions for this experiment.


Materials and Methods


Participants
100 participants (46 women, 53 men, 1 non-binary) were recruited on Amazon Mechanical Turk (MTurk). We selected the sample size based on the lowest estimated effect size among dependent variables of interest from a pilot experiment ( = 29), to detect the effects with 80% power. Participants received a $1 base pay and a $0.5 bonus for making the correct judgment in each scenario. Participants were allowed to participate in the main experiment only after passing a comprehension check following the instructions. We did not exclude any participant or observation. Participants gave informed consent, and the experiment was approved by the Harvard Institutional Review Board and preregistered at https://aspredicted.org/3jv3-kgxr.pdf.


Stimuli
The performance of each asset ("energy type" in the story) was presented as the length of a horizontal bar. It starts as a dark green bar showing the starting performance at Month = 0, and with each additional month ( = 1 through = 10), a light green bar showing the most recent growth is appended to the right (see 
Figure 2B
 for an example). The total length of the bar signifies the system's current performance.
We modeled performance trajectories using logistic curves, drawing inspiration from prior research demonstrating that logistic growth characterizes many real-world asset trajectories, including human learning, socioeconomic development, population growth, and many more 
(Son and Sethi, 2006;
Murre, 2014;
Harvey and Reimers, 2013)
:
( ; θ ) = 1 + 1 1 + exp 2 • (3 − 3 ) ,
(10)
where θ = ( 0 , 1 , 2 , 3 ), and each parameter of the logistic curve is sampled from a generative distribution. For the start low-grow fast trajectory distribution, we have 0 = 0.05, 1 ∼ N (0.9, 0.03 2 ), 2 ∼ N (0.5, 0.03 2 ), 3 ∼ N (12, 0.03 2 ). For the start high-grow slow distribution, we have 0 = 0.5, 1 ∼ N (0.2, 0.03 2 ), 2 ∼ N (1, 0.03 2 ), 3 ∼ N (14, 0.03 2 ).
Participants observed three trajectories sampled from each asset family during training ( 
Figure   2A
). These trajectories were the same for every participant.
The performances of the two systems were displayed one on top of the other, the order of which was randomized.


Procedure
Participants first completed a training block, in which they observed the trajectories of three past defense systems powered by each energy type improve over ten months, from = 0 to = 10. Participants observed these trajectories sequentially in whatever order they preferred.
Each trajectory was shown only once.
In the test trials, participants answered three hypothetical questions. Each question asked them to choose which energy type they would use if the attack happened at the end of Month = 3, 5, or 8, presented in this order ( 
Figure 2C
). Participants responded to each question by making a button click. Importantly, participants could not observe the trajectory development again and had to respond by forecasting how the two systems will develop. They could only see the initial performance of both energy types (dark green bars) on the screen at Month = 0 when they needed to make the decision.


Statistical analysis
We conducted two-sided binomial tests. We report the proportion of participants choosing each energy type and the corresponding -values according to the binomial tests. We also report the 95% confidence intervals (CI) constructed using the Clopper-Pearson exact method 
(Clopper and Pearson, 1934)
.


Model fitting and comparison
We fit each of the eight candidate models to the binary choice data, assuming a discrete uniform prior over the three performance trajectories observed during training. Since each model had one to three free parameters, they are overparameterized to fit individual participant data, because each participant had only three data points. We hence fit each model to the choices generated by all participants, a shortcoming that would be addressed by Experiment 3. We fit model parameters using maximum-likelihood estimation (MLE), which connects the model's outputted logistic probabilities to participant choices. Parameter optimization was performed using the Limited-memory BFGS algorithm. We performed the above optimization starting from 10 sets of randomly initialized parameter values, reporting and visualizing results based on the fitted set of parameter values with the lowest loss value.
To compare the models, we performed 5-fold cross-validation and juxtaposed the fold-averaged test loss values of each model. Specifically, we randomly partitioned the behavioral data into five folds with equal numbers of data points. We fit each model to every possible combination of 4 training folds and tested it on the left-out fold (using 10 sets of randomly initialized parameter values). This generated 5 test loss values, each corresponding to one train-test dataset partition. These five values were then averaged as the model's test loss.


Results
Overall, more participants chose the "start-high-grow-slow" asset at = 3 (proportion = 0.67, two-sided binomial test < .001, 95% CI [0.57, 0.76]). In contrast, they prefer the "start-low-grow-fast" asset at both = 5 (proportion=0.64, = 0.007, 95% CI [0.54, 0.73]) and = 8 (proportion=0.77, < .001, 95% CI [0.68, 0.85]). Quantitatively, as the time horizon increases, the proportion of participants choosing the "start-low-grow-fast" asset increased. We fit the three models that could make predictions for asset selection-BFP, BV, and EH-to the behavioral data, with and without the logistic offset parameter . As shown in 
Figure 2D
 and E, the BFP model captures the behavioral pattern that the "start-low-grow-fast" asset is increasingly preferred as the time horizon increases. In contrast, the BV model predicted that the "start-low-grow-fast" asset was most likely to be chosen at the middle = 5 condition, whereas the EH model predicted the same choice probabilities across all three time horizons. Model comparison via 5-fold cross-validation supports the qualitative conclusions above: the BFP models with and without logistic offset had the lowest fold-averaged test loss across all candidate models, with including providing a slightly better fit ( 
Figure 2F
).


Discussion
In Experiment 1, we found that participants were sensitive to asset performance trajectories into the future as well as to differing time horizons. Their decisions were best described by the BFP model, compared to the alternative models that use myopic strategies common in the fields of foraging and study time allocation.


Experiment 2
In Experiment 2, we extended the problem to asset switching by asking participants to make decisions after developing an asset for a while. This mirrors real-life scenarios where a new job offer arises, requiring one to decide whether to leave their current job based on the future prospects of both. Staying with the current asset builds on the current progress, whereas switching to another asset builds from its starting performance. Therefore, participants need to mentally translate the trajectories in order to account for the progress already made. This additional difficulty makes it informative to study asset switching as an independent problem from asset selection, and to explore whether humans resort to heuristics instead.
For consistency, we used the same two assets as in Experiment 1. The only difference was that participants were forced to develop a randomly assigned starting asset up to = 2, at which point they decide whether to stay with the current asset or switch to another asset for different hypothetical scenarios 
(Figure 3
).
The BFP model chooses to stay with or switch to the "start-high-grow-slow" asset when there is little time remaining to develop the system, and stay with or switch to the "start-low-grow-fast" asset when there is more time. In contrast, all three alternative models prefer the same asset across time horizons.


Materials and Methods


Participants
200 participants (103 women, 93 men, 1 non-binary, 3 prefer not to say) were recruited on MTurk. 100 of them (55 women, 43 men, 1 non-binary, 1 prefer not to say) were assigned to start with the "start-low-grow-fast" asset, and the other 100 participants started with the "start-high-grow-slow" asset. We selected the sample size based on the lowest estimated effect size among dependent variables of interest, according to a pilot experiment ( = 50 per condition) to detect the effects with 80% power. Participants received a $1.5 base pay and a $0.5 bonus for making the correct judgment in each scenario. Participants were allowed to participate in the main experiment only after passing a comprehension check following the instructions. We did not exclude any participant or observation. Participants gave informed consent, and the experiment was approved by the Harvard Institutional Review Board and preregistered at https://aspredicted.org/f33z-s27j.pdf.


Stimuli
We used the same stimuli and trajectory distributions as in Experiment 1, except that participants observed two months of development on one randomly chosen energy type before they made decisions to stay or switch in three hypothetical scenarios ( 
Figure 3A-B)
. In the test trials, the asset they started was always displayed above the other asset ( 
Figure 3C
).


Procedure
Participants completed two training blocks. The first training block was the same as Experiment 1, where they observed the trajectories of three past defense systems powered by each energy type improve from = 0 to = 10. Participants observed each trajectory sequentially in whatever order they preferred. Each trajectory was shown only once. In the second training block, they first observed a defense system develop with one energy type for two months, and then observed how the performance changes with either continuing with the same energy type or switching to the other energy type, from = 2 to = 10. Participants observed each trajectory sequentially in an interleaved order, where an example of staying with the current energy type was followed by an example of switching to the other energy type, and vice versa. Each trajectory was shown only once.
In the test trials, participants answered three hypothetical questions. Each question asked them to whether to continue with the current energy type or switch to the other energy type if the attack happened at the end of Month = 3, 5, or 8, presented in this order ( 
Figure 3C
).
Participants responded to each question by making a button click. Importantly, participants could not observe the trajectory development again and had to respond by forecasting how the two systems will develop. They could only see the first two months of performance of the current energy type and the initial performance of the other energy type on the screen at Month = 2 when they needed to make the decision.


Statistical analysis
The statistical analysis was identical to Experiment 1.


Model fitting and comparison
The model fitting and model comparison procedures were identical to Experiment 1.


Results
Overall, among participants who were assigned the "start-low-grow-fast" asset to begin with, more participants chose to switch when = 3 (proportion = 0.73, < .001, 95% CI participants who were assigned the "start-high-grow-slow" asset to begin with, more participants chose to stay when = 3 (proportion = 0.83, < .001, 95% CI [0.74, 0.90]) and = 5 (proportion = 0.62, = .021, 95% CI [0.52, 0.72]), whereas more participants chose to switch when = 8 (proportion = 0.9, < .001, 95% CI [0.82, 0.95]). These qualitative behavioral patterns were only predicted by the BFP model, whether it was fit with or without the logistic offset parameter ( 
Figure 4A-B
). Model comparison via 5-fold cross-validation also shows that the BFP models provides the best fit to the data, with including providing a slightly better fit ( 
Figure 4C
).
Despite the qualitative match, humans quantitatively deviate from the BFP model. The BFP model predicts that the probability of staying should be near-identical between = 5 and = 8 when participants start with the "start-low-grow-fast" asset, and near-identical between = 3 and = 5 for the other task condition. This is because the advantage term Δ is near-identical for the two test scenarios under the same value, such that the downstream logistic decision rule could not map them to significantly different choice probabilities. However, among human participants, we consistently see higher uncertainty in the = 5 test scenario, where the choice probabilities are closer to the chance level of 0.5. The other models also fail to capture this phenomenon.


Discussion
In Experiment 2, we found that participants were capable of extrapolating asset performance into the future even when accounting for prior investment in one asset, and they could use that extrapolation and translation to make decisions that consider the relevant time horizons. Their decisions were best described by the BFP model, compared to the alternative models that use myopic strategies common in the fields of foraging and study time allocation.
It is also worth noting that this task required storing trajectory distributions in working memory and mentally translating the trajectories, which was a considerable challenge that might have swayed participants in the direction of using myopic heuristics instead. Even so, participants continued forecasting future performances and making decisions qualitatively consistent with the BFP model. It is also evident that humans did not misapply the BFP model under the premise of asset selection, which seemed tempting as it would also remove the burden of mental translations.
If such misapplication had occurred, participants would consistently prefer the "start-low-grow-fast" asset at = 5 across both conditions (as in Experiment 1). In contrast, our participants preferred different assets depending on which asset they were assigned to start with.
The unexplained uncertainty of human responses in the = 5 scenario highlights areas for   
(Glanzer and Cunitz, 1966)
, leading to more uncertain decisions at the middle = 5 compared to = 3 and = 8.
Another plausible forecasting heuristic is that participants are using the asset performance differences at = 2 and = 10 as pivot points (where memory is least noisy), and derive forecasts for in-between timepoints via linear interpolation. It hence becomes an interesting future direction to study how the time duration from the current time to time horizon affects human stay-switch strategies, and whether longer time durations elicit usage of increasingly myopic heuristics.


Experiment 3
Experiments 1 and 2 demonstrated that the BFP outperformed the alternative models in both asset selection and switching. However, one limitation was that the experiment designs were simplistic and contrived-for example, the asset trajectory families (θ) were highly distinguishable from each other and rather homogenous within each family, there were only two possible assets at a time, and we enforce a strict decision time = 2 upon participants in Experiment 2. In real-world situations, these decisions are often more complicated and less tractable. For example, switching jobs usually entails more uncertainty about what it would feel like in a new position, and there is usually not a fixed time point to make these decisions. The enforcement of decision times also prevents us from comparing the BFP model to other popular foraging heuristics, such as the fixed-time (FT) and fixed-number (FN) patch-leaving strategies 
(Lin and von Helversen, 2023;
McNair, 1982;
Gibb, 1962;
Krebs, 1973)
, which assume that individuals flexibly determine their switch times. In addition, given that each experiment only asked for three binary judgments, the models were fitted on all participants' data, which overlooks individual differences.
In Experiment 3, we address these limitations by sampling the current and new trajectories from the same distribution featuring large variance, and allowing participants to switch whenever they want. Each participant completed two conditions with different trajectory distributions-one with trajectories separating closer to the time horizon = 10 ("separate-late" condition), and the other with trajectories separating closer to the beginning = 0 ("separate-early" condition; 
Figure   5A
-B). In each condition, participants completed 50 runs each lasting from = 0 to = 10, where they must choose to stay or switch at every time point (but they could only switch at most once per run). This design more closely examines the processes underlying participants' decisions and provides more data points that enables model fitting to individual participants.
The BFP model makes the following predictions: 1) the mean switch time is earlier in the "separate-early" condition; 2) the mean asset performance at the switch point is higher at the "separate-early" condition; 3) switching is less likely in the "separate early condition"; 4)
comparing runs in which participants stayed until the end versus switched halfway through, the mean final performance of the starting asset should be higher in the former group of runs. In contrast, none of the alternative models makes all four of these predictions. We also compare participants and models in their probability of staying at every time point conditioned on them not switching so far, analyzed both separately and jointly across the two conditions. Finally, we perform 5-fold cross-validation to compare the four models (with and without logistic offset ).


Materials and Methods


Participants
80 participants (36 women, 43 men, 1 non-binary) were recruited on MTurk. We selected the sample size based on a pilot experiment ( = 29) to detect the effects of interest with 80%
power. Participants received a $6 base pay and a bonus up to $2 proportional to the mean final performance achieved across test runs. Participants were allowed to participate in the main experiment only after passing a comprehension check following the instructions. We did not exclude any participant or observation. Participants gave informed consent, and the experiment was approved by the Harvard Institutional Review Board and preregistered at https://aspredicted.org/sh73-d3z4.pdf.


Stimuli
The trajectory distributions differed across the two conditions:
( ; θ ) = 0 + 1 1 + exp 2 • (3 − 3 )
(11)
where 0 = 0, 1 ∼ N (0.55, 0.15 2 ), 2 = − 1 2 1 + 9 8 for both conditions. For the "separate-early" condition, 3 = 17 − 25 24 + 25 6 1 ; for the "separate-late" condition, 3 = 3 + 25 6 1 . In other words, the trajectory distributions (θ) used for the two conditions were horizontally translated versions of each other, where stochasticity was introduced by the single trajectory parameter θ = 1 ( 
Figure 5A
). Our experiment directly visualized the above ( ; θ ), leading to the simplification ( ) = ( ; θ ) in the true generative process. Unlike Experiments 1 and 2, the trajectories used in training and test time were randomly sampled for each participant in each run. The starting asset is always displayed above the other asset that could be switched into ( 
Figure 5C
).
Due to the greater number of stay-switch decisions participants must make, we changed the response format from clicking buttons to pressing keyboard keys-the "right" arrow to continue developing the current asset and the "down" arrow to switch to a new asset.


Procedure
Participants completed the "separate-late" and "separate-early" task conditions in randomized order. In each condition, participants first observed 50 assets improve from = 0 to = 10 simultaneously, where the asset performance trajectories were sampled from the condition-specific prior cond (θ). Due to experimental time constraints, the 50 trajectories were presented simultaneously on the screen, ordered from lowest to highest final performance ( 
Figure   5B
). Participants then completed a second training block consisting of five runs. These five training runs were identical to those in the test block, but included numeric feedback on the final performance achieved after the run ended at = 10.
In the subsequent test block, participants completed 50 runs for that condition. In each run, participants must first develop the randomly assigned starting asset for one month (till Month = 1)-so that there was an asset to stay with or switch out of-but they were allowed to make stay-switch decisions afterwards. If at any point participants decided to switch, the new asset's performance trajectory would be randomly sampled from the same condition-specific trajectory distribution cond (θ). Participants were only allowed to switch once per run, to prevent recursion in estimating the utility of switching in all our models. Given this restriction, we did not analyze participants' stay-switch decisions after they first switched in each run, because from that point on, they were not allowed to switch again, rather than choosing not to switch. Participants were told that their bonus was proportional to the sum of the final performances achieved across all 100 test runs across both conditions.


Statistical analyses
We conducted two-sided paired t-tests. In the main text, we report the t-statistics, their
corresponding -values, the Cohen's effect sizes, and 95% confidence intervals of the effect sizes. The BFP models' first two predictions were assessed based on all participants who switched at least once in both conditions ( = 63), whereas the fourth prediction was assessed based on all participants who switched at least once in either of the conditions ( = 79). Prediction 3 was assessed based on all participants ( = 80).


Model fitting and comparison
For each participant, we fitted all four models, with and without the logistic offset , to their binary stay-switch data, jointly across both conditions. The trajectory prior used was the true generative prior cond (θ = 1 ) for each condition, enumerated over a dense grid of 1 values for numeric integration because the logistic function families prevent analytic solutions for integrals over parameter values. Similar to Experiments 1 and 2, we fit model parameters using maximum-likelihood estimation (MLE), which connected the model's outputted logistic probabilities to participant choices. Due to the greater dataset sizes in Experiment 3, optimization was performed by fitting all model parameters using the PyBADS optimizer 
(Acerbi and Ma, 2017;
Singh and Acerbi, 2023)
. We performed the above optimization starting from 10 sets of randomly initialized parameter values, reporting and visualizing results based on the fitted set of parameter values with the lowest loss value.


Starting system
Month 4/10
Press "→" to continue building the current system. Press "↓" to switch to a new system. After reaching Month 10, you'll proceed to the next run.


New system
......


Separate-early condition:
......


Separate-late condition:
Month To compare candidate models, we again performed a 5-fold cross-validation and juxtaposed the fold-averaged test loss values associated with each model. In this process, we randomly partitioned each participant's data into five folds with equal numbers of data points. For each model fitted on each participant, we optimize it over every possible combination of 4 training folds and tested it on the left-out fold (also using 10 sets of randomly initialized parameter values),
generating 5 test loss values that each corresponded to one train-test dataset partition. These five values were then averaged as the model's test loss. In the main text, we visualized the fold-averaged test loss summed across all participants, to capture each model's performance in accounting for the behavior of all participants.


Results
Across both task conditions, the mean number of analyzable stay-switch responses provided by participants was 701 ± 13.9, excluding the = 0 and = 10 times of every run and all the time points within a run after switching because participants could not choose during these times. Among the 80 participants, 79 participants switched at least once throughout the experiment, and 63 participants switched at least once in both conditions. The number of data points per participant and the nonuniformity of their responses enabled the fitting of participant-specific models and our subsequent interpretation.
Consistent with the BFP model predictions, the mean switch time of participants was later in the "separate-early" condition ( 
62
 
Figure 6A
). The mean performance level at which participants switch was also higher for this condition ( (62) = 14.5, < .001, = 2.41, 95% CI [−3.01, −1.91]; 
Figure   6B
), and they were less likely to stay with their starting asset till = 10 ( (79) = 6.52, < .001, = 0.719, 95% CI [0.483, 0.981]; 
Figure 6C
). Most importantly, participant responses reflected considerations for the starting asset's final performance-which is also the basis for the BFP model's judgments. Among the runs they stayed until the end, the starting asset's final performance was higher than that among runs in which they switched ( (78) = 10.9, < .001, = −1.90, 95% CI [−2.40, −1.47]; 
Figure 6D
).
We compared the above behavioral patterns to the predictions generated from all four models with and without logistic offset fitted to the behavioral data. The models fitted with are visualized in 
Figure 6
; for models fitted without , see 
Figure 7
. Only the BFP model with logistic offset could quantitatively capture the behavioral patterns across conditions and stay versus switch runs.
In addition, we compared the data and model predictions in terms of the probability of staying at each time point, conditional on the agent not switching yet-for data pooled across conditions ( 
Figure 6E
) and for each condition ( 
Figure 6F-G
 


Figure 6
Experiment 3 results. The models visualized have the logistic offset as a free parameter. Error bars denote SEM across participants. (A-C) Data (black) and model-predicted (color) mean switch time (A), mean starting asset performance at switch (B), and probability of continuing with the starting asset till the end of the run (C), for the "separate-late" and "separate-early" task conditions. (D) The starting asset's true final performance (if the participant had continued till the end of the run), for runs where participants continued till the end versus switched midway. (E-G) Data (black) and model-predicted (color) probability of staying at each time point given that they have not switched so far in the run, visualized for both task conditions together (E), the "separate-late" condition (F), and "separate-early" condition (G). Models predictions with logistic offset are shown. switch over time in the "separate-late" condition, as well as the surge in switching probability around = 3 in the "separate-early" conditions when the trajectories had separated. In contrast, all alternative models with fitted logistic offsets failed to capture these behavioral trends.
Additionally, all models fitted without the logistic offset consistently predict chance levels of switching at early timesteps, failing to capture the human tendency to stay ( 
Figure 7E
-G).
Model comparison via 5-fold-cross-validation similarly favored the BFP model with logistic offset, which has the lowest test loss among all models ( 
Figure 8A
). This conclusion is not 


Figure 7
Experiment 3 behavioral results continued. The models visualized have the logistic offset fixed to 0 (no decision bias). The visualization schema is identical to 
Figure 6
.
the result of a few outliers, and holds for individual participants: post-hoc two-sided paired t-tests between the BFP model with logistic offset and the other models with or without are all significant at the = 0.0001 level ( 
Figure 8B-C)
. We also assessed whether a 5-fold cross-validation could recover the true generative model through model recovery, by performing the same model-fitting procedures over the predictive data generated by each participant-fitted model. The model recovery results demonstrate that our procedure could reliably recover the BFP, BV, EH, and VH models from each other, but is less powerful in recovering the models without 
(Figure 9
). Despite this, model recovery confirms that the BFP model only has the lowest test loss on data it generated (i.e., when it is the true generative model), but not others.


Discussion
In Experiment 3, we adopted a more naturalistic task setup for asset switching, where the starting and new assets were sampled from the same trajectory distribution and participants were The modeling results of Experiment 3 differ from Experiments 1 and 2 in meaningful ways. In Experiments 1 and 2, even though the BFP model with logistic offset was the winning model, its counterpart without also outperformed the alternative models. However, in Experiment 3, the logistic offset parameter became more crucial in capturing the behavioral data.
As revealed through the model visualizations ( 
Figures 6-7
) and model comparison 
(Figure 8
), models without predicted much more frequent switching at earlier times, which was reasonable because the performance trajectories had not separated (thus leading to chance levels of switching
+XPDQV 3HUIRUPDQFH 3DUWLFLSDQW 3DUWLFLSDQW 3DUWLFLSDQW 3DUWLFLSDQW 3DUWLFLSDQW %)3 3HUIRUPDQFH %)3B 3HUIRUPDQFH 7LPH


Figure 10
Data from example participants. Different columns correspond to different human participants. Row 1: The human data. The starting asset's performance for each task condition over time (colored lines) is visualized until the participant switches for that run (colored point). Row 2: The predictive data generated by the BFP model with logistic offset fitted to human data. These "fake" datasets generated for each fitted model are used for 
Figure 6
-7 visualizations and model recovery. Row 3: The predictive data generated by the BFP model without logistic offset fitted to human data.
could address this shortcoming and lead to a more realistic account of human asset switching.


General Discussion
In this paper, we present a systematic study of asset selection and switching problems.
This problem domain is characterized by assets that develop with investment (which tend to improve over time but not necessarily), can only be developed one at a time, and the time horizons at which only the current asset's performance is evaluated. This problem resembles a broad class of important real-life scenarios, from deciding which project to take on and which trainee to recruit, to whether and when to give up a project, quit a job, or leave a relationship. Navigating these problems requires probabilistically forecasting of how assets will develop while taking into consideration of the relevant time horizons. These features make the myopic solutions in the fields of foraging and study time allocation less likely to apply.
Here, we develop a normative Bayesian framework for asset selection and asset switching problems. The framework assumes that people extrapolate the performance trajectories of assets into the future and consider them in conjunction with time horizons, with its plausibility supported by previous work in function learning and evidence accumulation. Across three experiments, we manipulated whether there was prior investment, the trajectory shapes and time horizons, and the degree of freedom in making the decisions. We demonstrated that the Bayesian modeling framework captures participants' behavior better than alternative models inspired by foraging and study time allocation, as well as a Bayesian model based on future improvement velocity. These results suggest that people use probabilistic forecasting to guide decisions with long-term implications, as opposed to myopic heuristics alone.
The current study advances Bayesian function learning in humans to more complex domains. Previously, researchers have mostly studied human function learning either as an isolated cognitive capability 
(Lucas et al., 2015)
, or used in conjunction with decision-making to estimate the utility of unexplored options 
(Schulz et al., 2018;
Wu et al., 2018)
. Here we show that Bayesian function learning is applicable to time-dependent decision problems.
The framework we propose here can be extended to predict the trajectory of switching among assets if we allow switching multiple times, and potentially switching back to a previously invested asset. The possibility of switching back may be especially helpful when there is greater uncertainty with how assets develop, and when assets develop unexpectedly (e.g., a seemingly easy project becomes much harder than expected). However, it requires calculating utilities recursively, by anticipating not only the utilities of staying or switching now, but also the possibility of switching again later, which can quickly become computationally intractable. One solution is to combine RL and Bayesian function learning mechanisms to provide a more comprehensive account of how people make these decisions 
(Schulz et al., 2018;
Wu et al., 2018)
.
We made a number of simplifications in our experiments. First, we always included a training phase that allowed participants to develop priors over the performance trajectories.
However, in real life, the experiences become the new priors-humans must learn priors as they observe assets, and simultaneously make selection and switching decisions to maximize final performance. Hence, when an inexperienced agent with weak priors unluckily starts with a poorly performing asset, they may erroneously infer that the entire trajectory distribution is poor and be discouraged from switching, an explanation that rationalizes "sunk costs" usually thought to be irrational 
(McAfee et al., 2010)
. Second, our experiments made clear the relevant time horizons, which might not be available or certain in the real world. Inaccurate time horizon priors might explain human suboptimalities in asset selection and switching 
(McGuire and Kable, 2012)
.
Third, we only tested participants on growing trajectories that follow a functional pattern (in our experiments, a logistic function). In reality, asset performance can decrease at times and constantly fluctuate. An open question is how we can adapt the framework to anticipate and adapt to changes in the trajectory shape. Fourth, while we focus on the chosen asset's performance at a specific time horizon, some asset selection/switching problems may instead involve maximizing cumulative rewards over time (e.g., choosing jobs to optimize total savings by retirement). To better understand human decision making in these scenarios, it is important to examine the environmental conditions under which probabilistic forecasting provides an advantage over myopic heuristics, develop corresponding decision models, and assess whether such forecasting is integrated into human strategies.
Our study also raises several questions for future investigations. While making asset selection and switching decisions, people are simultaneously investing effort into their current asset, which may take up cognitive resources. How do we navigate both the low-level action and the meta-cognitive decision? For example, if the current job is very exhausting, it would be hard to notice or consider other jobs while maintaining the current job. In these situations, would people ignore available alternatives along with the possibility of switching, or resort to myopic heuristics more? Evidence from the study time allocation literature suggests that high-level metacognition is still at play when humans solve low-level tasks 
(Payne et al., 2007)
, and they have a basic understanding of typical learning trajectory shapes needed for Bayesian priors 
(Zhang et al., 2025)
. This suggests that people may still be able to consider asset selection and switching while engaged in lower-level tasks, but exactly how these low-level actions affect selection and switching decisions is an open question for future work.
Another question concerns asset selection and switching when more than two alternatives are available. As discussed in the Introduction, this includes scenarios where the agent must choose from three or more assets or has the flexibility to switch to a specific asset among many options. Expanding the framework to accommodate these cases may require not only new evidence accumulation models 
(Brown and Heathcote, 2008;
Tajima et al., 2019)
 but also a consideration of the cognitive load associated with forecasting and comparing multiple assets, as discussed above. Previous research has shown that the number of available choices influences switching behavior in the context of static-valued goods 
(Ratner and Herbst, 2005)
, and that humans may spontaneously simplify tasks by considering only a subset of available options under cognitive load 
(Liu and Gershman, 2025)
. Additionally, related work in foraging suggests that the flexibility to switch to particular alternatives may alter human decision strategies 
(Hall-McMaster and Luyckx, 2019)
. Given such neighboring evidence, future research should examine how the presence of multiple alternatives influences human asset selection and switching.
Lastly, the development of an asset is also closely related to how we invest in them. People usually have some control over the growth of an asset; for example, putting more effort into a research project can lead to more progress, and treating your partner with love and respect can foster a better relationship. Indeed, past work has shown that, when training collaborators, we need to decide not just whom to train but also how much to train them 
(Xiang et al., 2024)
. The right amount of investment also depends on the performance of the asset; while we showed participants the asset's performance through the length of a bar, this information often has to be inferred from observations 
(Xiang et al., 2023)
. The current work is limited in that participants did not have control over how the trajectory develops. Future experiments should examine how people make these asset selection and switching decisions when they have agency in investing in the assets, and conversely, how the option to switch out affects their investment in the current asset. 


CRediT authorship contribution statement
and results. (A)The performance trajectories were sampled from two asset families: one starts low and grows fast (blue) and the other starts high and grows slow (orange). We sampled three trajectories from each family. The black solid vertical line denotes the current time (Month = 0). The dotted vertical lines denote the time horizons of each scenario. (B) Participants saw the development of each sampled trajectory. In this example, "A" corresponds to the "start-low-grow-fast" asset and is displayed on top. Each asset's starting performance at Month = 0 is shown as a dark green segment. Over time, increments in performance are shown as light green segments appended to the right, whose lengths are proportional to the performance improvements. (C) The three test scenarios with time horizons = 3, 5, 8. Participants can only observe the starting performance of both assets at Month = 0 while making selection decisions. (D-E) Behavioral results and model predictions. Black error bars denote standard error of mean (SEM). The dotted black horizontal line at 0.5 denotes the probability associated with randomly selecting assets. The overlaid colored lines denote the prediction of models using fitted model parameters (D) without the logistic offset , or (E) with . (F) Cross validation fold-averaged test loss differences. The "0" suffix denotes models fitted without the logistic offset parameter .


[ 0 .
0
63, 0.81]), whereas more participants chose to stay when = 5 (proportion = 0.67, < .001, 95% CI [0.57, 0.76]) and = 8 (proportion = 0.83, < .001, 95% CI [0.74, 0.90]). Among Different rows correspond to different between-participant task conditions. (A) The three performance trajectories sampled for each trajectory distribution. Trajectory colors denote the different asset families. The black solid vertical line denotes the current timestep (Month = 0), and the dotted vertical lines denote the time horizons of different test scenarios. (B) Visualization of asset performance over time. Participants must develop their task condition's assigned starting asset up to Month 2, from which they can decide to continue or switch to the other asset, developing it in the remaining time before the time horizon. (C) The three test scenarios with = 3, 5, 8 as presented to participants. They can only see the starting performance of the new asset, as well as the current performance of their starting asset at the designed decision time of Month = 2.


Experiment 2 results. (A-B)Behavioral results and model predictions. Black error bars denote SEM. The dotted black horizontal line at 0.5 denotes the probability associated with randomly choosing assets. The overlaid colored lines denote the prediction of models using fitted model parameters (A) without the logistic offset , or (B) with . Top row corresponds to the condition where participants started with the "start-low-grow-fast" asset. Bottom row corresponds to the condition where participants started with the "start-high-grow-slow" asset. (C) Cross validation fold-averaged test loss differences. The "0" suffix denotes models fitted without the logistic offset parameter . future model development. One possible explanation arises from the experiment's sequential presentation format-asset performance is revealed to participants chronologically, a setup typical in real life. Consequently, participants may have retained better memory of the starting and final performance of assets under the well-known primacy and recency effects


Example performance trajectories sampled from the "separate-late" (blue) and "separate-early" (orange) conditions. (B) The display of performance trajectories during training. The plateauing region of trajectories correspond to black smudges at the beginning or end of the green bars, representing light green segments with near-zero width. (C) The display of a test run. At each time from = 1 to = 9 inclusive, participants must decide to either continue investing in their current defense system/asset, or switch to a new one and develop it in the remaining time before = 10. The starting and new asset trajectories are both sampled from the condition-specific trajectory distributions visualized in (A).


= 2.47, = .016, = −0.370, 95% CI [−0.689, −0.0687];


losses for cross-validation. (A) Cross validation fold-averaged test loss differences across models. The visualized losses are summed across the individual-participant fits. The error bars denote SEM of test losses over participants. The "0" suffix denotes models fitted without the logistic offset parameter . (B) Distribution of individual fold-averaged test losses across all candidate models. error bars within each violin plot denote the median and interquartile ranges. (C) Zoom-in plot for the 4 models with logistic offset fitted. Points represent individual participants. allowed to switch at any time. By fitting the models to each individual participant's data, weshowed that the BFP model with logistic offset captured participants' responses and outperformed the other models. Some key features of the behavioral data-such as the mean switch time and mean performance at switch point-could not be explained by the fixed-time (FT) and fixed-number (FN) foraging heuristics.


Shuze Liu :
Liu
Conceptualization, Methodology, Investigation, Software, Formal Analysis, Visualization, Data curation, Validation, Project administration, Writing -original draft. Yang Xiang: Conceptualization, Methodology, Software, Visualization, Writing -review & editing. Samuel J. Gershman: Conceptualization, Methodology, Supervision, Writing -review & editing, Funding acquisition.


). This analysis was based on the actual runs each participant completed. The BFP model with logistic offset captured the qualitative and quantitative patterns of the behavioral data, including the decreasing tendency to
$
%
&
'
0HDQVZLWFKWLPH
0HDQSHUIRUPDQFHDWVZLWFK
3URSRIUXQVFRQWLQXHGWLOOHQG
0HDQILQDOSHUIRIVWDUWLQJWUDM
6HSDUDWHODWH
6HSDUDWHHDUO\
6HSDUDWHODWH
6HSDUDWHHDUO\
6HSDUDWHODWH
6HSDUDWHHDUO\
6ZLWFKUXQV
6WD\UXQV
(
%RWKFRQGLWLRQV
)
6HSDUDWHODWHFRQGLWLRQ
*
6HSDUDWHHDUO\FRQGLWLRQ
SVWD\_KDVQHYHUVZLWFKHGHDUOLHU_
+XPDQV %)3PRGHO %9PRGHO (+PRGHO
SVWD\_KDVQHYHUVZLWFKHGHDUOLHU
SVWD\_KDVQHYHUVZLWFKHGHDUOLHU
9+PRGHO
7LPH
7LPH
7LPH








without decision bias). Such chance-level switching is also a potential reason why model recovery was poorer for the models without -they encouraged switching early on in a run, leaving very few data points to fit models over (see 
Figure 10
 bottom row).
In contrast to the models without , participants tended to stay when evidence was ambiguous before trajectories separate, leading to improved fits of models with . However, due to limitations of our experiment design that participants could only switch once per run, it is unclear whether the tendency to stay comes from a genuine stay-bias (similar to sunk costs), or whether participants were saving their only opportunity to switch to a later time point, when plausible trajectories would be separated more. To distinguish these hypotheses, we would need a task that allows participants to switch an unlimited number of times per run. On the modeling side, this complicates the picture because the utility of switching must now incorporate the recursive utilities at later times associated with switching and not switching then. Future modeling studies
 










Practical bayesian optimization for model fitting with bayesian adaptive direct search




L
Acerbi






W
J
Ma








Advances in neural information processing systems






30












Judgmental extrapolation and market overreaction: On the use and disuse of news




P
B
Andreassen








Journal of Behavioral Decision Making




3


3
















A linear threshold model for optimal stopping behavior




C
Baumann






H
Singmann






S
J
Gershman






B
Von Helversen








Proceedings of the National Academy of Sciences




117


23
















The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks




R
Bogacz






E
Brown






J
Moehlis






P
Holmes






J
D
Cohen








Psychological review




113


4


700














The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive psychology




57


3
















Optimal foraging, the marginal value theorem




E
L
Charnov








Theoretical population biology




9


2
















The use of confidence or fiducial limits illustrated in the case of the binomial




C
J
Clopper






E
S
Pearson








Biometrika




26


4
















Learning the opportunity cost of time in a patch-foraging task




S
M
Constantino






N
D
Daw








Cognitive, Affective, & Behavioral Neuroscience




15
















Model-based influences on humans' choices and striatal prediction errors




N
D
Daw






S
J
Gershman






B
Seymour






P
Dayan






R
J
Dolan








Neuron




69


6
















Decision theory, reinforcement learning, and the brain




P
Dayan






N
D
Daw
























Cognitive, Affective, & Behavioral Neuroscience




8


4














Betting on trends: Intuitive forecasts of financial risk and return




De
Bondt






W
P








International Journal of forecasting




9


3
















Extrapolation: the sine qua non for abstraction in function learning




E
L
Delosh






J
R
Busemeyer






M
A
Mcdaniel








Journal of Experimental Psychology: Learning, Memory, and Cognition




23


4


968














A reinforcement learning diffusion decision model for value-based decisions




L
Fontanesi






S
Gluth






M
S
Spektor






J
Rieskamp








Psychonomic bulletin & review




26


4
















L. tinbergen's hypothesis of the role of specific search images




J
A
Gibb








Ibis




104


1
















Two storage mechanisms in free recall




M
Glanzer






A
R
Cunitz








Journal of verbal learning and verbal behavior




5


4
















Improving judgmental time series forecasting: A review of the guidance provided by research




P
Goodwin






G
Wright








International Journal of Forecasting




9


2
















Stopping rules for optimal foragers




R
F
Green








The American Naturalist




123


1
















Threshold models of human decision making on optimal stopping problems in different environments




M
Guan






M
Lee






A
Silva








Proceedings of the annual meeting of the cognitive science society


the annual meeting of the cognitive science society






36












Control over patch encounters changes foraging behavior




S
Hall-Mcmaster






P
Dayan






N
W
Schuck








Iscience




9


24














Revisiting foraging approaches in neuroscience




S
Hall-Mcmaster






F
Luyckx
























Cognitive, Affective, & Behavioral Neuroscience




19














On the nature of expectations




N
Harvey






F
Bolger






A
Mcclelland








British Journal of Psychology




85


2
















Trend damping: Under-adjustment, experimental artifact, or adaptation to features of the natural environment




N
Harvey






S
Reimers








Journal of Experimental Psychology: Learning, Memory, and Cognition




39


2


589














Forecasting forecasts: The trend effect




S
M
Hohle






K
H
Teigen








Judgment and Decision making




10


5
















Narrative expectations in financial forecasting




S
G
Johnson






D
Tuckett








Journal of Behavioral Decision Making




35


1


2245














A generalized polynomial model for perception of exponential series




G
V
Jones








Perception & Psychophysics
















Uncertainty drives deviations in normative foraging decision strategies




Z
P
Kilpatrick






J
D
Davidson






A
Hady








Journal of The Royal Society Interface




18


180


20210337














Behavioral aspects of predation




J
R
Krebs








Perspectives in ethology




Springer
















Judgmental forecasting: A review of progress over the last 25 years




M
Lawrence






P
Goodwin






M
O'connor






D
Önkal








International Journal of forecasting




22


3
















A hierarchical bayesian model of human decision-making on an optimal stopping problem




M
D
Lee








Cognitive science




30


3
















Learning to learn functions




M
Y
Li






F
Callaway






W
D
Thompson






R
P
Adams






T
L
Griffiths








Cognitive science




47


4


13262














Never gonna give you up even when it is suboptimal




H.-Y
Lin






B
Von Helversen








Cognitive Science




47


7


13323














Action subsampling supports policy compression in large action spaces




S
Liu






S
Gershman








PsyArXiv












A rational model of function learning




C
G
Lucas






T
L
Griffiths






J
J
Williams






M
L
Kalish








Psychonomic bulletin & review




22


5
















Do sunk costs matter? Economic Inquiry




R
P
Mcafee






H
M
Mialon






S
H
Mialon








48














Decision makers calibrate behavioral persistence on the basis of time-interval experience




J
T
Mcguire






J
W
Kable








Cognition




124


2
















Optimal giving-up times and the marginal value theorem




J
N
Mcnair








The American Naturalist




119


4
















A region of proximal learning model of study time allocation




J
Metcalfe






N
Kornell








Journal of memory and language




52


4
















S-shaped learning curves




J
M
Murre








Psychonomic bulletin & review




21
















Reinforcement learning in the brain




Y
Niv








Journal of Mathematical Psychology




53


3
















Reinforcement learning algorithm for non-stationary environments




S
Padakandla






P
Kj






S
Bhatnagar








Applied Intelligence




50


11
















Discretionary task interleaving: heuristics for time allocation in cognitive foraging




S
J
Payne






G
B
Duggan






H
Neth








Journal of Experimental Psychology: General




136


3


370














The drift diffusion model as the choice rule in reinforcement learning




M
L
Pedersen






M
J
Frank






G
Biele








Psychonomic bulletin & review




24
















The diffusion decision model: theory and data for two-choice decision tasks




R
Ratcliff






G
Mckoon








Neural computation




20


4
















When good decisions have bad outcomes: The impact of affect on switching behavior




R
K
Ratner






K
C
Herbst








Organizational Behavior and Human Decision Processes




96


1
















The need for contextual and technical knowledge in judgmental forecasting




N
R
Sanders






L
P
Ritzman








Journal of Behavioral Decision Making




5


1
















Bringing judgment into combination forecasts




N
R
Sanders






L
P
Ritzman








Journal of Operations Management




13


4
















Finding structure in multi-armed bandits




E
Schulz






N
T
Franklin






S
J
Gershman








Cognitive Psychology




119


101261














Putting bandits into context: How function learning supports decision making




E
Schulz






E
Konstantinidis






M
Speekenbrink








Journal of experimental psychology: learning, memory, and cognition




44


6


927


















E
Schulz






J
B
Tenenbaum






D
Duvenaud






M
Speekenbrink






S
J
Gershman


















Compositional inductive biases in function learning






Cognitive Psychology




99














Pybads: Fast and robust black-box optimization in python




G
S
Singh






L
Acerbi




arXiv:2306.15576










arXiv preprint








Research on the allocation of study time: Key studies from 1890 to the present (and beyond). A handbook of memory and metamemory




L
K
Son






N
Kornell




















Metacognitive control and optimal learning




L
K
Son






R
Sethi








Cognitive Science




30


4
















Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








MIT press












Optimal policy for multi-alternative decisions




S
Tajima






J
Drugowitsch






N
Patel






A
Pouget








Nature neuroscience




22


9
















Optimal stopping in a markov process




H
M
Taylor








The Annals of Mathematical Statistics


















Can learners allocate their study time effectively? it is complicated




E
Tekin








Educational Psychology Review


















Humans monitor learning progress in curiosity-driven exploration




A
Ten






P
Kaushik






P.-Y
Oudeyer






J
Gottlieb








Nature communications




12


1


5972














Job mobility and the careers of young men




R
H
Topel






M
P
Ward








The Quarterly Journal of Economics




107


2
















Accumulating advantages: A new conceptualization of rapid multiple choice




D
Van Ravenzwaaij






S
D
Brown






A
Marley






A
Heathcote








Psychological review




127


2


186














Generalization guides human exploration in vast decision spaces




C
M
Wu






E
Schulz






M
Speekenbrink






J
D
Nelson






B
Meder








Nature human behaviour




2


12
















Collaborative decision making is grounded in representations of other people's competence and effort




Y
Xiang






N
Vélez






S
J
Gershman








Journal of Experimental Psychology: General




152


6


1565














Optimizing competence in the service of collaboration




Y
Xiang






N
Vélez






S
J
Gershman








Cognitive Psychology




150


101653














People accurately predict the shape but not the parameters of skill learning curves




X
Zhang






S
D
Mcdougle






J
A
Leonard








Cognition




258


106083















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]