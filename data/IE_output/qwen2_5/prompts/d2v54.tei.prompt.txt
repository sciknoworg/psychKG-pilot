You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Every day, we engage in a variety of complex behaviours. As we read, we move our eyes across the screen or page, performing complex integration of pre-existing and new knowledge and anticipating what will come next. In order to make important life decisions-buying a house, figuring out how to best engage unruly children-we sample and weigh various pieces of information and engage with our close family and friends to integrate their perspectives and often make joint decisions with them. In many cases, we share our space with a variety of urban adaptor species with their own interests at stake and decisions to make 
(Clucas, Marzluff, et al., 2011)
. When those interests diverge from ours, both humans and animals may find themselves in a race to out-wit the other, such as in the case of bin-opening by sulphur-crested cockatoos (Cacatua galerita) 
(Klump et al., 2021)
. Psychological and cognitive sciences might seem obvious approaches to understanding the mechanisms underlying complex behaviours like those mentioned above, which we will encounter again in the rest of the paper (see boxes).
Yet, these very scientific approaches are facing a confidence crisis in their ability to meaningfully explain individual and collective behaviours. For instance, many well-accepted cognitive and psychological findings cannot be replicated in new studies 
(Nosek et al., 2022)
, putting to question what we actually know about human and animal minds. The problem has been attributed to methodological or statistical shortcomings, but it is becoming increasingly clear that a crucial cause is the absence of robust and precise theories capable of guiding research programs 
(Devezer, 2024;
Guest & Martin, 2021;
Muthukrishna & Henrich, 2019;
Rich, Haan, Wareham, & Rooij, 2021;
Rooij & Blokpoel, 2020;
Smaldino, 2017;
Szollosi et al., 2019)
. In other words, it is not just a question of collecting more observations and "facts", no matter how robustly and replicably we measure them with conservative statistics. " 
[E]
ven if all uncertainty is removed from scientific inference problems, there are further principled barriers to deriving explanations [from observations], resulting from the computational complexity of the inference problems" 
(Rich et al., 2021)
, a challenge that becomes particularly acute when explaining complex behaviours. In order to build stronger and more precise theories that could guide our studies and interpretation of observation, it has been suggested that we need to build computational or mathematical models formalising our tentative theories 
(Borsboom, Maas, Dalege, Kievit, & Haig, 2021;
Fried, 2020;
Oberauer & Lewandowsky, 2019;
Rooij & Blokpoel, 2020)
. Such formalisation can be seen as a form of "extended cognition," allowing scientists to augment their cognitive capacities, enabling a more precise understanding of the implications of their theories, but also to open up to others the cognitive process of understanding and theorising 
(Lindsay, 2021, ch. 1;
. A formal model is easier to communicate precisely and allows other to manipulate it, allowing for testing assumptions and modifying its components as new scientific insights emerge 
(Burnston, Sheredos, Abrahamsen, & Bechtel, 2014;
M. Fusaroli, Mitchell, Rudolph, Rocca, & Fusaroli, 2025)
.
Mathematical psychology and computational modelling seem to be the right disciplines to provide the necessary models, tools, and competencies, given that they have been creating and developing formalised models of cognitive processes for at least 70 year 
(Batchelder, 2010)
. However, there is a gap between the behaviours traditionally modelled by these disciplines and the complex target behaviours introduced above. Mathematical psychology and computational modelling have understandably started from simple behavioural tasks that could be controlled and manipulated in the laboratory. Reading was constrained to single-word and then single-sentence reading 
(Rayner & Reichle, 2010
; e.g., 
Seidenberg, Farry-Thorn, & Zevin, 2022)
, memory to the learning of a list of random words (e.g., 
Howard & Kahana, 2002)
, social interactions to tapping or otherwise waving a finger in tune with each other (e.g., 
Konvalinka, Vuust, Roepstorff, & Frith, 2009;
Tognoli, Zhang, Fuchs, Beetle, & Kelso, 2020)
, decision making to picking between a limited set of pre-defined choices (e.g., 
Maanen et al., 2012;
Ratcliff, Smith, Brown, & McKoon, 2016)
. Such reduction to simpler behaviours has been invaluable to building modelling traditions, which gradually developed more and more precise models describing the cognitive mechanisms generating them. Yet, two main obstacles prevent moving from these tasks, techniques and models to complex behaviours.
The first obstacle is that researchers with domain and experimental expertise on those very complex behaviours do not generally have the modelling competence required; indeed, most psychology, linguistics, and anthropology programs do not include computational or formal modelling in their curricula. This results in mathematical models being opaque precisely for those with the most expertise and insight into the complex behaviours for which a modelling approach could indeed provide a meaningful increase in understanding. Furthermore, there is a gap between verbal descriptions of the mechanisms and their mathematical formulations, with the same shorthands being used for potentially very different underlying formalisations. For example, the term "social learning" has been used to describe both reinforcement learning and transmission models, which obfuscates the distinction between acquiring a novel behaviour from others and being influenced by the preferences of others 
(Chimento, Barrett, Kandler, & Aplin, 2022;
Galef Jr, 1995)
. These ambiguities make it even more challenging for domain experts to fully grasp the modelling issues at stake and can lead them to misapply or misinterpret computational models.
The second obstacle is that even when domain experts invest time in understanding and disambiguating existing computational models, they will likely find them unsatisfactory. Many models may be effective at explaining simple tasks in controlled lab settings, but they are unlikely to directly account for the complex dynamics of real-world behaviours. For instance, it might be hard to see how models of learning developed for two-alternative forced-choice laboratory experiments could be scaled up to apply to the arms race between humans and cockatoos where individuals need to adjust to ever-learning opponents and might learn from other individuals' solutions as well (see Box 4). Similarly, it is hard to see how models of single-sentence reading could be scaled up to the reading of a complex novel across days or weeks. 
Rosen (1977)
 highlights that early scientific breakthroughs, especially in physics, stemmed from the ability to transform apparently incomprehensible situations, like celestial body movements, into explanations based on very simple models with straightforward and unchanging laws. However, " 
[such]
 notion that all complex processes could be understood in terms of underlying simple universal laws is indeed an article of faith 
[. . . ]
" 
(Rosen, 1977)
. Historically, science has often opted to tackle simpler problems, sidestepping the challenges posed by complexity, with the justification that if the need arose, they would address more complex issues (P. W. 
Anderson, 1972;
Frank, Gleiser, & Thompson, 2024;
Kello & Van Orden, 2009)
. The situation is not different in cognitive and psychological science, where existing computational models often do not directly apply to the questions and phenomena investigated by domain experts and overwhelmingly focus on very simple non-naturalistic tasks (such as the two-alternative forced-choice tasks) or toy examples (see 
Navarro, 2019)
. We argue that the more traditional focus on targeting simplified behaviours should now be complemented with models directly addressing more complex behaviours. We also acknowledge that it is, in fact, very hard to break free from the classic modelling tasks: existing tutorials and textbooks on cognitive modelling present only simplified toy examples. Dennis Lindley is asserted to have said that "if you can't do simple problems, how can you do complicated ones?" 
(Navarro, 2019)
, presupposing that complicated problems are just an extension of simple ones. And yet, we find that it is often too hard to extend these toy examples to a realistic scenario (e.g., 
Rooij & Blokpoel, 2020)
. The depicted scenario (or sometimes the model) may be too underspecified (e.g., 
Smaldino, 2017
Smaldino, , 2020
Wilson & Collins, 2019)
, or the model too task-specific 
(Bontje & Zgonnikov, 2024;
e.g., Farrell & Lewandowsky, 2018;
Lee & Wagenmakers, 2014)
 to extend to a realistic scenario.
Indeed, as Navarro (2019) puts it, "if we only solve simple problems, we may never learn how to think about the complex ones": simple models assume modularity in the world, while complexity approaches have persuasively argued that the complexity of the entire system tends to affect its apparently simple components in unpredictable ways (P. W. 
Anderson, 1972;
Rosen, 1977)
. In other words, the behaviour of simple components in isolation will likely be unable to explain their behaviour in a more realistic complex context. Instead, the context has to become part of the modelling procedure from its very conceptual inception 
(Favela & Amon, 2023;
Van Orden & Stephen, 2012)
.
Summing up, the available computational models might be both too complicated for many practitioners to grasp and too simplistic to capture naturalistic behaviours of interest. How can we improve this situation? Undoubtedly, providing additional education in developing and formalising mechanistic theories and their computational implementation would help bridge the gap. We argue that the problem is not simply a lack of technical expertise but also a cultural one: There is a lack of communities developing cultural practices that tackle complex contexts and processes and support each other. Such communities can only emerge by bringing together the complementary perspectives of modellers and domain experts. However, we currently lack the practices and contexts to facilitate and support this dialogue.
To start tackling exactly this issue, three of the authors (BN, MvV, and RF) organised a workshop on the "Cognitive modelling of complex behaviours" with a decided focus on creating communities. The workshop was structured to facilitate interdisciplinary collaboration and foster a community-based approach to cognitive modelling. The organisers selected participants who were experts in specific modelling approaches and/or experts in specific complex behaviours. On the first day, we aimed to create common ground: some of the modelling experts presented their frameworks and applications; some of the content experts presented brief overviews of their topics and highlighted potential target complex behaviours. On the basis of this common ground, we developed pitches for possible projects modelling real-world complex behaviours. The participants spontaneously organised around some of these projects, while we ensured a balanced mix of modelling and content experts, as well as researchers at different career stages. The bulk of the workshop consisted of collaborative sessions on these projects, allowing participants to get hands-on experience and insights from different disciplinary perspectives.
The projects undertaken during the workshop covered a range of topics-partially anticipated in the opening of this manuscript: language comprehension through reading (Box 1), categorisation processes in the context of autism (Box 2), complex decisions when negotiating a benefits package (Box 3), and a behavioural arms race between cockatoos and humans (Box 4). In each of these cases, defining a modelling approach was not straightforward. The participants struggled to identify the right level of complexity to maintain and often re-negotiated the initial research questions and modelling approaches. Over the workshop, several key good practices (here conceptualised as "rules") emerged, which are discussed below.


Box 1: Language comprehension through reading
Humans read to understand the information contained within a given text. Many modelling approaches to human language comprehension rely on behavioural measures. Those often include eye fixations, recorded with eye-tracking devices during naturalistic reading, or key-presses collected in a self-paced reading task, where participants only see the next fragment of a text after pressing a key. Such measures have been instrumental in advancing our understanding of human sentence processing, among others, to reveal areas of processing difficulty and their relation to predictability in context 
(Hale, 2001;
Levy, 2008)
. However, these experimental methods do not directly reveal how humans comprehend sentences. This group aimed to create a modelling approach that prioritises comprehension. We settled on conceptualizing comprehension as contextualisation, i.e., integration of texts or pieces of texts, such as words, within the given context. This approach has the advantage of implementing an intuition of comprehension that was general and shared by all of us. At the same time, it lends itself well to integration with modelling in NLP, in which the meaning of words is represented based on the aggregation of linguistic contexts in which the word appears. The next step was to integrate this conceptualisation of comprehension with the study of processing, as investigated in psycholinguistics using the behavioural measures discussed above. We assumed that the link between comprehension and processing is due to time allocation, which allows readers to block time for contextualisation, i.e., integration of words or text fragments within the context. Large Language Models (LLMs: 
Vaswani et al., 2017
) such as BERT 
(Devlin, Chang, Lee, & Toutanova, 2018)
 learn contextualised representations through embeddingsvector representations of words (or tokens) with multiple layers. Recent research suggests that these models can offer insights into how semantic representations evolve in different contexts 
(Cassani, GÃ¼nther, Attanasio, Bianchi, & Marelli, 2023)
. Building on these developments, we operationalised contextualisation as the difference between layers in BERT. On top of this, we implemented a simple mechanistic model simulating both reading times and depth of comprehension. To a large extent, this approach to cognitive modelling was driven by theoretical and conceptual considerations rather than specific datasets or experiments. This was further complicated by the diversity of backgrounds within the group, which required reconciling different perspectives and methodologies. Yet, at this point, we already started observing some benefits of the approach. We constructed an intuitive understanding of what it means to comprehend and what it means to process text that all of us found intuitively appealing. The processing hypothesis, based on time allocation, provided an explanation for some fundamental aspects of reading. It accounted for why predictable words, as well as those less influenced by context (e.g., some function words), tend to be processed more quickly. Additionally, it captured how readers adjust their reading strategies depending on the task (e.g., detailed vs skim reading).


Box 2: Joint categorisation processes in the context of autism
This group's work focused on understanding participants behavior in the Alien Game 
(TylÃ©n, Fusaroli, Ã˜stergaard, Smith, & Arnoldi, 2023)
. The Alien Game is an experimental task in which individuals and dyads learn to associate different combinations of visual features to the dangers posed by a large set of diverse aliens. The task has been applied in various settings to explore categorisation and abstraction processes 
(Fjaellingsdal, Vesper, Fusaroli, & TylÃ©n, 2021;
TylÃ©n et al., 2023)
 and conversational dynamics 
(Bruna & Kello, 2024;
 in autistic and non-autistic populations 
(Corrigan et al., 2023)
. This task raises many questions from a modeling perspective: How do people learn to categorise novel stimuli (aliens) as dangerous or not? What are the relevant cognitive components that should be modeled? Given the interpersonal angle, how should we rethink the categorisation process when it happens in a dialogue between participants? How do participants share their information and conjectures, negotiate them and integrate them (or not)? Is there a difference in how autistic and nonautistic individuals process the alien stimuli, as well as produce, elicit, and use social information during the joint categorisation process? The group working on this project-including experts on the task, autism, and computational modelling-was initially focused on the interpersonal dynamics involved in the task. Perceptual processes were simplified away in favor of representing the aliens as pre-given vectors of features. The debates concerned how to model the dyad: as one collective agent or two interacting individual agents? In the latter case, how should linguistic exchanges between them be modeled? Yet as the debate progressed and difference in autistic and non-autistic categorisation processes examined, the group leaned more and more on modelling the autistic and non-autistic perceptual processes, identifying and paying attention to relevant visual features, instead of the simplified assumption from previous modelling work that all features are identified immediately and available at all times. Notably, as the conceptual debate on how to carve the research question evolved, so did the modeling approaches considered. A preliminary sketch of the interaction and use of social information was made in ACT-R. However, as the focus shifted on attentional processes and statistical learning of relevant features, Bayesian models of perceptual processes and learning were brought to the foreground 
(Van de Cruys et al., 2014;
Weber et al., 2023
). Yet, the discussion of the ACT-R cognitive architecture was not forgotten, as the group endeavored to combine rule-based categorisation models-easy to implement in ACT-R-with probabilistic updates of their formation and likelihood as more easily afforded by Bayesian models. How to pass information between the two model types and the order in which processes were completed was left as an open question.


Box 3: Complex decisions when negotiating a benefits package
Negotiation practitioners have long held that a key challenge in reaching an agreement is identifying issues that offer potential for mutual benefit. Many issues may be at stake during the hiring process for a new job, from the salary to the number of vacation days or the opportunity to work from home. On some of these issues, the applicant and employer may have conflicting interests (e.g., the applicant prefers a higher salary than the employer), whereas on others, their interests may be complementary (e.g., the applicant and employer may both prefer a remote position). A successful negotiation requires both parties to infer each other's interests as well as to agree on a distribution of the available issues. One of the members of this group, SC, had conducted an experiment in which the negotiators indicated their beliefs about each other's interests multiple times over the course of a simulated job negotiation 
(Dudzik, Columbus, Hrkalovic, Balliet, & Hung, 2021)
. Another member, NT, had previously developed a virtual negotiation agent in the cognitive architecture ACT-R 
(Stevens et al., 2018)
. The group members were modelling experts with expertise ranging from Bayesian models to cognitive architectures. However, many members also had direct experience of negotiations in academic environments both as applicants and selection committee members. The group made use of the available data and diverse modelling expertise to pursue multiple convergent modelling strategies. On the one hand, they used Bayesian hierarchical models to model the distribution of perceived interests across issues, time, and experimental conditions. This approach, which could be described as bottomup or exploratory, was informed by the research questions which had originally motivated the experiment, but also gave rise to new questions based on observed patterns in the data. One key feature of the experiment was a manipulation of the strength of conflict between the negotiators, which varied with the number of distributive issues 
(Majer, Schweinsberg, Zhang, & TrÃ¶tschel, 2022)
. At the same time, all negotiations involved issues allowing for mutually beneficial agreements. The exploratory Bayesian hierarchical models suggested that in negotiations involving stronger conflict, negotiators were more likely to miss opportunities for mutual gain. On the other hand, the group implemented a negotiation agent in the cognitive architecture ACT-R. This was based on an existing cognitive model of negotiation behaviour 
(Stevens et al., 2018)
, thus representing a more top-down modelling approach. To respond to actions by its opponent, the negotiation agent makes simultaneous inferences about the negotiation strategy of its opponent (cooperative or competitive) and about the opponent's interests (complementary or conflicting). Although the negotiation agent operates in a simpler setting than the experimental multi-issue negotiation, it reproduces the general pattern observed in the data: When negotiators first bargain over a distributive issue, this impedes their ability to identify subsequent opportunities for mutual benefit. This occurs because the negotiation agent, when its opponent rejects an offer, infers not only that their interests are in conflict, but also that the opponent is pursuing a competitive strategy. Consequently, the agent becomes less likely to infer complementary interests on subsequent issues. In this case, the top-down approach based on an existing cognitive model suggests a potential mechanism to explain the behavioural pattern revealed in the empirical data using a more bottom-up modelling approach.


Box 4: Behavioural arms race between cockatoos and humans
Residents of the Sydney metropolitan area have been complaining for years about bin-opening behaviour by urban-living sulphur-crested cockatoos, with evidence that the behaviour is socially transmitted between cockatoos 
(Klump et al., 2021)
. In an effort to prevent this behaviour, residents have applied a wide variety of protective measures to their bins. Neighbouring households adopting similar bin protections suggests that that the households are learning from each other 
(Klump, Major, Farine, Martin, & Aplin, 2022)
. This sets up a very interesting scenario to model the complex, potentially evolving behaviour between populations situated in space. Understanding the dynamics of such an arms race is valuable not only for the basic science of complex behaviour but also for understanding the practical efficacy of decisions at an individual and institutional level regarding human-wildlife conflict. For example, some local councils have responded by applying standard locks to all bins. However, this might push the behaviour into new neighbourhoods without protections. The interdisciplinary working group involved experts in animal behaviour, social learning and cultural evolution, cognitive science, and reinforcement learning. While cockatoo bin-opening was the inspiration for the model, the group aimed to create a model that was generalisable to other contexts. The team modelled two populations: attackers and defenders. The attackers were conceptualised as reinforcement learners who innovated bin-opening techniques, and who could socially learn when observing successful colleagues (although this component could be turned off to test its contribution). For the defenders, a subtly different reinforcement learning model was used. This model had an effort discounting penalty, reflecting the increased effort each additional defence level incurred. In addition, the static defenders were only connected to their neighbours, and thus socially learned from a more limited set. The interaction between the two populations reflected an innovation arms race. As attackers learned to open bins, defenders learned to defend them. This encouraged attackers to move to less-defended neigbourhoods, thereby spreading their bin-opening knowledge more widely. When the level of attacker success reduced in an area, its defenders slowly reduced their defence level due to effort discounting. The group used this framework to simulate the effects of different policy initiatives. For example, when a local council universally introduced bin locks (a harder defence to defeat), it drove attackers away from that neighbourhood. This caused the problem of bin-opening to spread more widely, and ultimately the remaining attackers learned to defeat even the locks. The behaviour that emerged between these populations was not always easy to predict, which demonstrated the benefit of simulations. Especially in this example, where empirical data collection is impractical and difficult, a modelling approach thus offered the opportunity to learn about a complex system and its potential responses to pertubations. The model served as a basis for a paper in preparation for a special issue of Philosophical Transactions B, for which a preprint has been published 
(Chimento, Dalmaijer, Klump, & Aplin, 2025)
.
Participants were encouraged to explicitly discuss within groups how common ground and common goals were being developed and their struggles; and at regular intervals we held common sessions were the different groups presented their progresses, approaches and difficulties. From these discussions we identified several key rules for tackling complex modelling:
1. Define the problem 2. Form and maintain a team 3. Select the modelling approach(es) 4. Implement the model(s) and make practical decisions 5. Evaluate the model(s)


Define the problem
When computationally modelling a complex behaviour, the initial indeterminacy of the problem is perhaps the most challenging aspect of the research project. Not only are there many possible modelling choices to make, but the complex behaviour to model and the specific data to use are also underdetermined. For instance, in the project described in Box 2-individual and joint categorisation in autistic and non-autistic individuals-what should be the focus? There are perceptual and learning processes involved in identifying which alien features are relevant, and this has to be stored in memory. The features need to be correctly associated to the possible danger posed by the aliens. A decision needs to be made individually. Information needs to be shared, elicited and combined. Any of these aspects could present relevant (and irrelevant) differences in the autistic and non-autistic populations. Indeed, figuring out what to model was difficult. An early focus on how to model the linguistic exchanges in the joint categorisation, was gradually replaced with a focus on the perceptual process preceding the decision. A similar challenge arose in the language comprehension project (see Box 1). Psycholinguistic research often addresses online processes in reading, such as eye movements and reading times, but rarely considers actual text comprehension. In contrast, educators and NLP researchers emphasise comprehension measured by test and benchmark outcomes, neglecting online processes. Defining the problem here requires clarifying the relationship between online processes, cognitive mechanisms, and comprehension across these approaches. Conceptually and operationally defining the focus of the model is a difficult and often unpredictable process.
An essential step in defining the problem is carefully examining what qualifies as data in the first place. Data are not simply a collection of raw observations but rather structured representations that mediate the relationship between theoretical models and empirical observations 
(Kellen, 2019;
Suppes, 1966)
. For example, in the cockatoo project (Box 4), participants determined that some data, such as bin-opening angles or specific communication mechanisms (e.g., vocalisations or body movements), could be safely abstracted away without compromising the model's insights into the behaviour. On the contrary, the autism project (Box 2) initially focused on communication, abstracted away from crucial differences between the autistic and non-autistic populations modelled. Ultimately, modelling requires deliberate decisions about which aspects of structured data are critical to understanding the behaviour in question and which can be safely simplified, ensuring that the balance between complexity and tractability aligns with the research goals.
There is an inherent circularity between defining the problem and determining what counts as data, driven by the initial indeterminacy of the problem. One approach to addressing this indeterminacy is to clarify the problem's significance. This might involve researchers explaining their current understanding in layperson terms to engage both interdisciplinary collaborators and non-academic stakeholders. For instance, the cockatoo project (Box 4) demonstrates how significance can operate at multiple levels: While it advances basic science understanding of behavioural arms races, it also addresses practical challenges in human-wildlife conflict management that matter to local communities and policymakers. Precise problem framing facilitates valuable feedback from diverse perspectives. This methodological diversity helps identify potential pitfalls early (e.g., identifying goals for which no clear benchmark can be defined within the scope of the project) and generates innovative approaches that might not emerge within a single discipline (see Rule 2).
When defining the problem and research questions, it is crucial to consider whether top-down, bottom-up, or a combination of both perspectives provides the most useful explanation of the problem at hand. The cockatoo project (Box 4) illustrates how mixing top-down and bottom-up approaches can be fruitful: top-down by conceptualising the problem at its core as a learning process applicable beyond this specific case, and bottom-up by incorporating detailed spatial behaviours of birds and households. At some points, the researchers emphasised the first, and at others, the second, to escape impasses and enrich the model's explanatory power. Open discussions helped the researchers in different groups employ whichever of the two strategies was less familiar to them, yielding new insights and a more nuanced understanding of the problem. Furthermore, it is often useful to consider at which of 
Marr's (1982)
 levels of analysis the problem should be framed. According to Marr, problems can be approached at different levels: focusing on what the system does and why (computational level), the mechanisms underlying the behaviour (algorithmic level), its physical implementation (implementation level), or a combination of these. The chosen analysis level has significant implications for both the types of questions one can address and, importantly, for the modelling paradigm, as different paradigms tend to operate more effectively at specific levels (Rule 3). Linking hypotheses across levels of analysis can help constrain and guide the modelling effort, fostering coherence and integration 
(Turner, Forstmann, Love, Palmeri, & Van Maanen, 2017;
Van Rijn, Borst, Taatgen, & Van Maanen, 2016
).


Form and maintain a team
Although it is not often discussed, scientific research is rarely a solitary endeavour. Scientific ideas and practices are only established in the context of a community and the structure of that community matters a great deal for how ideas emerge, spread, develop, and change 
(Castille, Cobb, Siegel, & Thomas, 2022;
Field, 2022;
O'Connor, 2023;
Peirce, 2017;
Zollman, 2010)
. Further, scientists routinely work in teams, especially when tackling complex phenomena and, as for the larger communities, the composition and dynamics of a research team play a crucial role in shaping the direction and success of the project. A team structure, its interdisciplinary nature, and its internal dynamics influence, on the one hand, the tools available, the theories considered, the interpretation of phenomena, and even the data deemed relevant for modelling; on the other hand, they influence the ability to effectively rely on this diversity, accepting, criticising and building on each other's work 
Hofstra et al., 2020;
Salas, Reyes, & McDaniel, 2018;
Wu, Wang, & Evans, 2019)
. To help coordinating effective groups, we now discuss several key aspects of teamwork that we have found useful across the different projects at the workshop, namely, diversity and common ground, as well as aspects of temporality, and enjoyment of the cooperative effort.
Why is diversity so important? Consider the scenario sketched in Box 1 aimed to model "reading comprehension." Just by bringing a heterogeneous group of researchers to work on the problem, very diverse aspects of comprehension and methodological approaches are brought to the table. For example, Natural Language Processing (NLP) researchers might focus from the start on benchmarks to evaluate comprehension (e.g., test answers) and whether there might be datasets available already; psycholinguistic researchers might instead emphasise the need to understand the process of language processing and push for considering reading times and eye movements, while educational researchers might be concerned with performance on standardised tests. In this instance, the team had to draw on knowledge from each of these domains to develop a more thorough notion of language understanding, integrating diverse perspectives to capture the full complexity of the behaviour and their related challenges and affordances.
This need for diversity, especially across disciplines, has been highlighted, for instance, by 
Gigerenzer (1998)
, who focuses on the issues of disciplinary isolation. In psychology, subfields such as cognitive, social, and developmental psychology have become increasingly siloed, each with its own journals, grant programs, and research communities, not to mention the lack of engagement with fields like anthropology, educational research, and mathematical psychology. This fragmentation allows researchers to focus narrowly within their own fields and deeply explore research venues without undue pressure to conform across fields 
(Devezer, Nardin, Baumgaertner, & Buzbas, 2019;
Zollman, 2010
). Yet, when we consider how to formalise a complex behaviour, focusing only on one disciplinary perspective can easily lead to forgetting all the complexities outside of that field. A cognitive psychology model of memory might do a fantastic job at unveiling the complexities of, for example, working memory 
(Baddeley, Hitch, & Allen, 2021
) and yet fail to account for everyday memory practices in the real world. Among other things, these practices rely on external storage of information, cultural practices and social interactions, which a more ethnographic perspective would have immediately brought up 
(Clark & Chalmers, 1998;
R. Fusaroli, Gangopadhyay, & TylÃ©n, 2014;
Hutchins, 1995;
Tollefsen, 2006)
. Thus, cross-disciplinary exchanges have the potential to create more robust, comprehensive models that can address the intricacies of complex behaviours.
With this in mind, how does one assemble a team with the potential to tackle the computational modelling of complex behaviours? Some of the groups at the workshop came together because they shared an excitement for a specific topic-e.g., the project presented in Box 3 involved several participants who had studied negotiations, whereas others had personal experience with negotiation as part of their job. Other groups were formed over a more extended time as an initial core deliberately sought out experts in complementary areas-e.g., the cockatoo project presented in Box 4 started with ethologists and was interested in including experts in reinforcement learning modelling. Diversity manifested not only in complementary expertise but also in a varied mix of motivations within the team. Some members might be driven by methodological development, others by paradigmatic shifts, and still others by a commitment to specific research questions.
Having established the importance of diversity, we now turn to the second key aspect of teamwork: building common ground. While diversity certainly enriches collaboration, it also introduces challenges that must be addressed to ensure effective teamwork. It is often easy to collaborate with somebody sharing the same background and level of abilities 
(Bahrami et al., 2010;
Mahmoodi et al., 2015)
. Different disciplines and backgrounds involve not only different technical terms or even different interpretations of the same words but also different scientific practices. An ethologist might deliberately dwell on the details and nuance of an animal's behaviour as a first step in identifying patterns, while a physicist might feel more at ease in starting with the simplest possible scenario before gradually adding details. In order to cultivate and productively engage these diversities, the team should be willing to step outside their comfort zones and invest in building a shared understanding 
(BjÃ¸rndahl, Fusaroli, Ã˜stergaard, & TylÃ©n, 2015;
R. Fusaroli et al., 2012;
Page, 2008)
. For instance, in the project dealing with joint categorisation in autism (Box 2) it was important to discuss not only the cognitive atypicalities of autism but also to ensure that everyone was at least somewhat engaged with the human aspect of the condition, as well as with the conceptual operations of the programming aspects of the computational modelling process. It was only through these discussions that the importance of perceptual and attentional processes emerged, for instance. While vertical expertise (depth in one's own domain) is important, so too is the readiness to learn about the expertise of others-whether that means understanding the basics of a different modelling technique or engaging with the social and identity issues inherent in autism research 
(Fletcher-Watson et al., 2019)
. Interdisciplinarity should be more than a superficial blend of fields; it should involve a genuine exchange of knowledge and perspectives. Engaging in open dialogue, adopting a shared language, but also explicitly addressing miscommunication and incomprehension, and actively managing diversity are crucial 
Healey, Mills, Eshghi, & Howes, 2018)
. Participants of a group should feel empowered to ask questions, not worrying whether they might be considered naive or uncomprehending. This creates a more cohesive and effective team, facilitates collaboration and ensures that the research remains grounded in a common purpose. Yet, for diversity to be effective, it needs also to emphasise differences and support complementary takes 
R. Fusaroli, RÄ…czaszek-Leonardi, & TylÃ©n, 2014)
. Participants should feel empowered to take somebody else's idea and, while acknowledging the original contribution, to develop it in a different direction. For example, in the group working on joint categorisation in autism (Box 2), two subgroups formed that developed the basic idea for the categorisation task into two completely different directions.
The interplay of diversity and common ground in team collaboration operates across multiple timescales that require careful consideration and planning. Research on team science has demonstrated that effective collaboration patterns often emerge through a combination of planned intensive periods and intermittent engagement 
(Hall et al., 2018)
.
Pre-workshop preparation, even if minimal, serves as a crucial foundation. While most participants may have limited availability for in-depth preparation, encouraging participants (or even top-down identifying select individuals) to explore specific domain phenomena or modelling approaches beforehand, producing a pitch of a project or a few relevant readings or data. These prompts helped generate intellectual curiosity and facilitated more productive discussions during the intensive period 
(Salas & Fiore, 2004
).
An intensive workshop period provides a unique opportunity to establish enthusiasm and project momentum by removing participants from routine responsibilities. However, even within such concentrated periods, effective collaboration requires a balanced rhythm between group discussions and individual work. This pattern allows team members to contribute according to their expertise, whether through model programming, data preparation, visualisations, or literature review. Effective teamwork does indeed involve a combination of collaborative effort and individual work on subtasks that align with each member's strengths, so that, for instance, not everybody is just waiting as a modeller debugs some funky code, or a selection of relevant reference papers is identified. Regular check-ins during these individual work periods, however, are needed to ensure that the project remains engaging for everyone, that all members feel capable of contributing, and that the team is progressing toward its goals 
(Hilton & Cooke, 2015
).
However, a week is rarely sufficient to complete a project. Sustaining project momentum and enthusiasm beyond intensive periods requires deliberate planning and structural support. A process with more intensive periods punctuated by breaks can be very productive as it allows for ideas to germinate and develop, but there needs to be an incentive to keep coming back to them. Research on distributed scientific teams suggests that establishing clear deliverables, such as grant applications or publications, combined with regular virtual meetings and presentation opportunities, helps maintain engagement 
(Cummings & Kiesler, 2007)
. While not every collaboration needs to result in a formal outcome, maintaining an active project overview and fostering enjoyment in the work process significantly influences long-term success. Enjoyment can be fostered in various ways, including the creation of an inclusive environment valuing both intellectual rigour and creativity, and the challenge involved in the project and its potential impact. This creates what has been termed "sustainable collaboration patterns" in team science research 
(BÃ¶rner et al., 2010)
.
In short, the team is an incredibly important part of successful modelling. Attention needs to be paid not only to the composition of the team but also to how the team works together.


Select a modelling approach
Selecting a modelling approach represents a critical decision step in modelling a complex behaviour. A modelling approach encompasses both the theoretical framework used to conceptualise the behaviour of interest and the analytical methods chosen to formalise and investigate it. This selection determines what aspects of behaviour can be readily explained, what mechanisms can be proposed, and what types of predictions can be made. The choice of approach thus requires careful consideration of both theoretical commitments and practical constraints.
The range of available approaches reflects the diversity of theoretical perspectives in cognitive and social sciences. These approaches can be broadly categorised based on their scope and theoretical commitments. Some models are designed to explain specific classes of behaviours. For instance, the Generalised Context Model focuses on the categorisation of novel exemplars as we encounter them 
(Nosofsky, 1984)
, while other models address specifically decision-making in economic games (e.g., 
Waade, Enevoldsen, Vermillet, Simonsen, & Fusaroli, 2022)
. These behaviour-specific models often make minimal commitments about how their proposed mechanisms fit into broader theories of cognition, instead focusing on providing detailed accounts of particular behavioural phenomena.
In contrast, several frameworks propose general principles for modelling cognitive behaviour. Cognitive architectures like ACT-R (J. R. 
Anderson et al., 2004)
 and SOAR 
(Laird, 2019)
 posit that cognition consists of manipulating symbolic representations through specified computational mechanisms. These architectures aim to provide end-to-end models of behaviour by combining multiple processing stages within a unified theoretical framework (J. R. 
Anderson et al., 2004)
. However, since these architectures contain clearly defined components, these components are sometimes used as computational models in isolation (e.g., 
Nicenboim & Vasishth, 2018)
. Other frameworks put more emphasis on the driving principles for specific cognitive processes. Some examples relevant to the workshop were reinforcement learning, deep learning frameworks, and Bayesian approaches to cognition. Reinforcement learning approaches conceptualise behaviour as driven by reward maximisation and punishment minimisation 
(Wagner & Rescorla, 1972)
, while sequential sampling models view decisions as arising from information accumulation processes over time 
(Ratcliff, 1978;
Weigard & Sripada, 2021)
. Connectionism views processing as patterns of activation across simple units interconnected in complex networks, with knowledge encoded in the strength of these connections 
(Thomas & McClelland, 2008)
. Bayesian approaches frame all cognition as a process of probabilistic inference (e.g., 
Oaksford & Chater, 2007)
. General claims notwithstanding, each of these general frameworks makes different theoretical commitments about the nature of cognition and naturally lends itself to modelling different aspects of behaviour. Certainly, a model of some given behaviour is often implementable within a certain paradigm (or a combination of them)-some of them might even be equivalent or generalise each other-but they will vary in the types of behaviour or mental mechanisms that are most naturally conceptualizable and implementable within them.
Another important dimension is whether the modelling approach is aimed at qualitative demonstrations or at models that can be fit to empirical data. Some approaches focus on demonstrating that a model can or cannot produce behaviour qualitatively similar to that observed empirically 
Smaldino, 2023)
. This demonstrationbased modelling can provide existence proofs for proposed mechanisms, generate novel predictions, or call for the reconceptualisation of a theory as the hypothesised mechanisms are too complex to be tractable or simpler alternatives with the same explanatory power are available 
(Van Arkel, Woensdregt, Dingemanse, & Blokpoel, 2020;
Woensdregt et al., 2021)
. Other approaches emphasise quantitative fitting of models to empirical data, using various statistical frameworks, such as maximum likelihood or a Bayesian framework, to estimate parameters and compare alternative models 
(Deffner, Fedorova, Andrews, & McElreath, 2024;
Farrell & Lewandowsky, 2018;
Lee & Wagenmakers, 2014)
. The choice between these approaches depends on the research goals and available data.
Selecting a modelling approach also requires considering the intended level of analysis. Following 
Marr's (1982)
 influential framework, approaches can target the computational level (what the system does and why), the algorithmic level (how it performs the computation), or the implementation level (how it is physically realised). Different frameworks naturally operate at different levels: for instance, Bayesian models of cognition often focus on the computational level, cognitive architectures typically span both computational and algorithmic levels, and connectionist models span from the algorithmic level to the implementation level. Notice that the mind potentially doing Bayesian inference is orthogonal to Bayesian methods as a statistical framework often used to model cognitive process-Bayesian inference or not- 
(Lee & Wagenmakers, 2014;
Nicenboim, Schad, & Vasishth, 2025)
.
The appropriate level of analysis should align with the research questions being addressed as each modelling approach in practice restricts the types of observations that can feasibly be explained, as well as the mechanisms that can be used to explain them. For example, in our workshop, there were no models on the implementation level. In the joint categorisation in autism project (Box 2), one subgroup decided to model the specifics of the decision-making process (computational level), whereas the other subgroup focused on the full negotiation process and incorporation of different sources of information, albeit with less detail (algorithmic level).


So how should we select a modelling approach?
Several key considerations should guide the selection of a modelling approach when studying complex behaviour. The composition and expertise of the research team often play a decisive role. The negotiations project (Box 3) illustrates this clearly: one team member had previously developed a virtual negotiation agent using ACT-R, while another had expertise in Bayesian modelling of belief updates during negotiations. While both approaches were explored, the pre-existing ACT-R model provided a more developed foundation that could be readily adapted to explore the new research questions. This exemplifies how existing implementations, even more than theoretical expertise, can significantly influence approach selection.
The relationship between empirical data and theoretical goals also shapes approach selection. The cockatoo project (Box 4) demonstrates an interesting trajectory in this regard. While the project began with rich empirical data about bin-opening behaviour, spatial spread, and protection measures, the team ultimately focused on theoretical simulations to understand what mechanisms were necessary to produce some key aspects of observed patterns of behaviour-namely, the spatial spread of novel behaviours. The team had to choose whether to zoom in on a particular cognitive operation, such as reinforcement learning 
(Botvinick & Weinstein, 2014)
, or a more comprehensive sequence of cognitive processes and stages (J. R. 
Anderson, 2014;
J. R. Anderson et al., 2004)
. The team's combined expertise in reinforcement learning, social transmission models, and agent-based modelling led them to develop one particular cognitive operation as a unified framework where both cockatoos and humans were modelled using the same reinforcement learning principles-a choice that balanced theoretical elegance with practical implementability and the opportunity to explore more in detail the mathematical behaviour of the model. The evolution of research questions during the modelling process can also necessitate alternating and possibly integrating approaches. The joint categorisation in autism project (Box 2) provides an instructive example. The initial focus on modelling communication processes in joint categorisation seemed to favour a cognitive architecture approach using ACT-R, which excels at modelling sequential decision processes: individuals combining their decisions into one collective decision. However, the team's discussion changed focus from the interpersonal aspects of the decision involved to the typical and autistic perceptual processes. This shift meant that team members with expertise in Bayesian models of perception and statistical learning became more central to the project. Integration of this latter approach to ACT-R has been relegated to future developments of the project. Given the constraints of time and cognitive resources, these affordances often lead the team to focus on what is most intuitive or straightforward to model within each framework, which can narrow the scope of the problem and guide the modelling process in specific directions. Similarly, the language comprehension project (Box 1) highlighted the challenge of integrating deep learning models like BERT, which capture semantic contextualisation, with cognitive models that account for time-dependent processing. These shifts often led teams to prioritise what was most feasible within each framework, guiding the modelling process in specific directions. However, bridging these approaches remains essential, as comprehension involves both the temporal dynamics of processing and the evolving structure of meaning.
The selection of modelling approaches for complex behaviours thus requires balancing multiple considerations: theoretical alignment with research questions, practical feasibility, data availability, team expertise, and the potential for integration across approaches. This selection process sets the stage for subsequent implementation and evaluation steps, but it should remain flexible as insights gained during the modelling process might suggest modifications or alternatives to the initial approach. The workshop projects demonstrate that successful modelling often emerges from an iterative process where the approach evolves in response to both theoretical insights and practical constraints.


Implement the model(s) and make practical decisions
Once a modelling approach is at least tentatively defined, it becomes crucial to start transforming theoretical ideas into equations or formulae that represent core components of the model and then into actual code. However, these translations are rarely straightforward. As our groups attempted to implement existing models and frameworks to analyse complex behaviours, the gaps between the model and actual behaviours became evident. Consequently, repeated adjustments were necessary to ensure that the implemented model accurately reflected the underlying theory. This process was not one way: as the theoretical considerations shaped the implementation and development of the models, the constraints imposed by implementation also reshaped theoretical assumptions.
For instance, in the joint categorisation in autism project (Box 2), implementing both perceptual processes, rule-based categorisation, and interpersonal information exchange would have required developing custom solutions bridging different modelling traditions, an issue which was not solved during the workshop. By developing implementations of the perceptual processes, the group was forced to bring to focus, for instance, issues of attention and working memory, which had not been part of the early discussion. This did not only develop the theoretical model of perceptual processes at play, but it also reshaped the conception of what interpersonal exchanges to the overall process: perhaps, by linguistically labelling features in the aliens, the linguistic exchange was focusing and simplifying attention processes 
(Clark, 2006;
R. Fusaroli, Gangopadhyay, et al., 2014)
.
This iterative development and refinement brings the critical challenge of distinguishing between core theoretical assumptions and auxiliary implementation decisions. Indeed, as 
Cooper and Guest (2014)
 emphasise, while models embody theories, they also include implementation assumptions that may not be part of the theory itself. For example, in the language comprehension project (Box 1), the core theoretical assumption about comprehension as contextualisation had to be complemented with specific decisions about how to represent word meanings and update them over time. These auxiliary assumptions, while necessary to make the model operational, should not be confused with the foundational theoretical principles. Failing to make this distinction can lead to a model burdened with unnecessary complexity, potentially obscuring the very phenomena it was designed to explain.
This distinction is particularly significant because implementing and testing a model computationally can be more challenging than it seems. It involves numerous assumptions that are not directly included in the guiding theory, even in mathematically well-specified models that produce distinct predictions 
(Robinson, Williams, Wixted, & Brady, 2022)
. These auxiliary assumptions arise from practical considerations-deciding which predictions are central to the theory and which are not, validating methods, and ensuring robustness against noise and imprecise data 
(Lakatos, 1978)
. The cockatoo project (Box 4) illustrates this well: while the core theoretical assumptions involved reinforcement learning, implementing the spatial dynamics required specific decisions about movement patterns and reward structures that, while necessary for the model to run, were not central to the theoretical claims being tested.
To muddy the waters, the distinction between core theoretical and auxiliary assumptions is often context-dependent and might shift over time. As debates evolve, new theoretical questions may emerge, and often requiring specific parameterisations of computational models to be testable. A key aspect of successful implementation is, therefore, the willingness to revise or discard assumptions and code that, while functional, no longer serve the model's goals. It is easy to become attached to "working" code, but effective modelling requires the discipline to let go of elements that do not contribute to the overall research purpose. For instance, in the negotiations project (Box 3), initial implementations focused on complex linguistic exchanges were eventually simplified to focus on the core dynamics of interest and belief updates. There was a certain resistance to doing so; much work had gone into the developed code, and there were worries about devaluating members' work and contributions. It was not an easy choice, although it led to a more workable project.
Through the implementation phase, data can also provide constraints to the theoretical model. The failure of a model to account for data-or the biased performance when compared with another model-can indicate that either a core principle is wrong or that an auxiliary assumption was inappropriate 
(Lakatos, 1978)
. For example, in the language comprehension project (Box 1), the team initially hypothesised that deeper layers in a transformer model would correspond to deeper semantic contextualisation. However, testing revealed that contextualisation related more to the probability of accessing contextual representations than to hierarchical depth, which led to a conceptual change in the direction of the project.
The previous paragraphs emphasise that theory and implementation are in a dynamical relation, not only with iterative reciprocal refinements but also with the occasional deep rethinking of the theory or code 
(Guest & Martin, 2021)
. We found that this process benefits particularly from deploying interdisciplinary teams of domain experts, modellers, and experimentalists who can contribute different perspectives on implementation decisions and whose expertise can become relevant at different stages of the process. For example, in the autism project (Box 2), the domain experts helped the modellers to remember that autism does not only have negative aspects but also components that may aid adaptive behaviour. Consequently, diverse collaborations enhance the robustness of the model, leading to a more accurate and insightful understanding of the phenomenon under study.
Finally, as the project evolves and gets rethought, it is imperative to carefully document implementation decisions and their rationales. This will ensure that the relationship between theoretical principles and their computational realisations remains clear and subject to revision as understanding develops during the workshop and also on the slower scale of the longer collaboration that can follow.


Evaluate the model(s)
A crucial criterion for evaluating a model is its relevance to the phenomenon of interest. Relevance is not always easy to quantify, yet it is essential to ensure that the model addresses meaningful questions rather than simply providing accurate answers to irrelevant ones 
(Levins, 1966)
. A model that prioritises accuracy in the wrong context risks being of little use for advancing understanding or addressing practical concerns. The model should directly address the behaviour or phenomenon being studied, capable of making meaningful distinctions within the research context. Even if the model does not perfectly capture every detail, it should still offer insights that are practically informative or can give ideas for follow-up research. For instance, the group modelling job negotiations (Box 3) focused on the conditions of the negotiation process while abstracting away the specifics of linguistic interactions, aligning their model with the core questions of interest.
Assessing the theoretical foundations of a model is a critical aspect of evaluating a model's value. A model with strong theoretical underpinnings should be preferred over one without them, as such foundations ensure that the model accurately reflects the theoretical constructs it aims to represent and proposes biologically or cognitively plausible mechanisms. This alignment between theory and model guarantees that the model is not just mathematically sound but also relevant to the real-world processes it seeks to explain. If the theoretical foundations of a model are sound, the model must perform its intended function: it should accurately represent the phenomena it was designed to model and not make major errors, that is, according to 
Shiffrin, Lee, Kim, and Wagenmakers (2008)
 to achieve a relatively good fit to the data or a basic level of descriptive adequacy.
Assessing the fit to the data (or its descriptive adequacy) means, in many cases, that predicted accuracy (or choice) and response times should be evaluated and compared to the empirical data, but other behavioural metrics are also possible: These range from a simple average difference between predicted and observed behaviour to a correlation between the two across a range of values. Additionally, Bayesian comparisons of the full distributions can be employed on various behaviour measures, particularly through posterior predictive checks. Although demonstrating a good fit between a model's predictions and observed data does not strongly confirm its validity, a significant mismatch can serve as strong evidence against it 
(Shiffrin et al., 2008)
. However, comparing models solely based on how well they match the data is insufficient for determining their relative performance. 
Roberts and Pashler (2000)
 emphasise that a good fit to the data is not enough for a model to be convincing. A key consideration is whether the model's predictions are meaningfully constrained: if a model can accommodate nearly any outcome, then a good fit to a particular dataset carries little explanatory power 
(Meehl, 1997;
Roberts & Pashler, 2000)
. At the same time, failing to match the data does not necessarily invalidate a model, as models are always simplifications of reality. There will inevitably be aspects of the data that a model cannot fully capture. This underscores the need for clearly defined success criteria that remain flexible enough to accommodate new insights. Avoiding rigid standards too early in the modelling process allows for creative exploration and the discovery of novel solutions. While expectations should be explicit, they must also evolve with the research, adapting to emerging findings and perspectives. The joint categorisation in autism project (Box 2) exemplifies this evolution: initial success criteria followed from the need to adequately understand the impact of communication, e.g., by comparing-in terms of fit to the data and out-of-sample errors-models with explicit communication components and models without. However, the interdisciplinary discussion among autism experts and computational modellers shifted the focus to modelling perceptual processes and reflecting lived experiences and previous findings in the formal structure of the model. Similarly, the negotiations project (Box 3) demonstrates how success criteria can expand to encompass multiple modelling approaches, from Bayesian hierarchical modelling of perceptions to a cognitive architecture implementation of different negotiating styles. This flexibility in success criteria, however, must be balanced with maintaining clear research direction and measurable progress benchmarks.
As 
Shiffrin et al. (2008)
 outline, besides a reasonable fit to the data, a good model should also meet four other key goals: (1) provide insight, (2) facilitate prediction and generalisation, (3) direct new empirical investigations, and (4) foster theoretical progress. The model should not only represent observed data but also generate meaningful predictions and inspire further exploration through new experiments or theoretical developments 
(Navarro, 2019)
. However, this makes sense only if historical data can also be accounted for; otherwise, there is a risk of overfitting to recent observations without demonstrating broader validity 
(Nicenboim et al., 2025, Chapter 12)
. For example, ideally, the model developed by the group working on joint categorisation in autism (Box 2) should generate predictions for that specific task and also account for key findings from past research. Moreover, its conclusions should extend beyond the immediate context, offering predictions for other social decision-making tasks as well.
Practical considerations also play a significant role in evaluating a complex model resulting from modelling complex behaviour. The model should be feasible to implement, meaning it should reliably converge, operate efficiently within the available computational resources, and be written in a language accessible to the research team. If the model is too complex to implement or understand, it loses much of its practical value. For example, in the case of modelling cockatoos opening bins (Box 4), when focusing the model on all individual movements involved in bin opening, this would detract the inference from the social learning that is the key explanandum. 
Lee et al. (2019)
 propose a number of techniques and practices aimed at making psychological modelling more transparent, trustworthy, and robust-principles that are equally relevant to modelling complex behaviour. In addition, 
Cooper and Guest (2014)
 suggest that building the same model in different programming languages can enhance robustness by highlighting discrepancies that might otherwise go unnoticed. Of course, this is a time-consuming and sometimes feasible approach. This approach was naturally adopted in the language comprehension project (Box 1), where team members had expertise in different programming languages. To ensure a thorough understanding of the formal model and increase robustness, the group implemented it twiceonce in R and once in Python-allowing them to cross-check for discrepancies and validate the implementation. Addyman and French (2012) highlight another major limitation of current modelling practices: models are often difficult to access, check, explore, reuse, or further develop. They argue that these challenges could be mitigated if modellers routinely made their models publicly available, ensuring that others can run and evaluate them-an essential aspect of open science. 
Cooper and Guest (2014)
 further emphasise that model specification deserves greater attention as an independent step, separate from implementation. A well-specified model should be clearly defined before coding begins, ideally using formal mathematical frameworks. This approach enhances transparency, facilitates replication, and helps prevent errors that may arise from ambiguities in implementation. While these practices require additional effort, they ultimately improve the clarity, robustness, and longevity of computational models.
A critical question is how specific the results of a model are to the specific set of parameters (albeit not always, see for example, Pitt 
& Myung, 2002)
. One established approach for evaluating a model's implementation is through parameter recovery studies. These studies assess whether the model can accurately recover the parameter values that generated simulated data 
(Cook, Gelman, & Rubin, 2006)
. In Bayesian models, this idea is extended through simulation-based calibration, which serves to validate the posterior distributions generated by a model 
(ModrÃ¡k et al., 2023;
Talts, Betancourt, Simpson, Vehtari, & Gelman, 2018)
. In cognitive architecture models, on the other hand, the specific parameters play less of a central role, and ideally, the model would work well for a large range of parameter values. Nevertheless, these techniques are invaluable for understanding the properties of a model and diagnosing issues like weak identifiability.
Another important aspect of evaluation is comparing a model against other established models or against variations of the same model with alternative theoretical principles. 
Nicenboim et al. (2025, Chapter 12)
 discuss how model comparison methods, such as Bayes factors and cross-validation, can determine which model generalises better within a given dataset. However, they caution that generalisation is only reliable within the range of observed data. Models validated on narrow datasets (often university students from Western populations) may not generalise to broader or more diverse populations 
(Hansen et al., 2023;
Henrich, Heine, & Norenzayan, 2010;
Rybner et al., 2022)
. While quantitative model comparison is useful, it cannot replace qualitative evaluation. 
Navarro (2019)
 points out that a model may provide a good statistical fit while contradicting substantive knowledge. It should be noted that model comparison aims to find the most "useful model" for characterising data, but no model comparison method guarantees selecting the model closest to the truth, even with enough data. A model closest to the true data-generating process may not produce the best predictions, and a model based on a flawed process might still generate strong predictions (for examples, see 
Navarro, 2019;
and Wang & Gelman, 2014
). Thus, model comparison requires a careful balance between statistical fit and theoretical relevance.
For some researchers, the simplicity or parsimony of a model may also be a key consideration in evaluating model quality. A model that captures the essential principles of the system it represents-through minimal assumptions-can be particularly appealing. In fact, across scientific domains, it has been argued that introducing additional complexity requires stronger justification (e.g., 
Myung & Pitt, 1997)
. In many cases, however, abandoning parsimony in favour of more complex models is essential for scientific progress 
(Dubova et al., 2024)
. Complex models, such as those used for 3D protein folding 
(Jumper et al., 2021)
 or text generation (e.g., 
LLMs: Vaswani et al., 2017)
, demonstrate that sometimes simplicity must be sacrificed to capture the richness of real-world phenomena. Ultimately, the model should contribute to a deeper understanding of the phenomenon under study. It should help clarify the underlying mechanisms and provide a framework for further investigation, whether by explicitly testing mechanisms or generating new hypotheses that advance the field. In evaluating a model, no single criterion is likely to be sufficient on its own, but a thoughtful combination of these considerations can guide the development of robust, insightful models that offer meaningful contributions to the understanding of complex phenomena.


Concluding thoughts
In summary, we have provided five points of attention when attempting to create computational models of complex behaviour: (1) define the problem, (2) form and maintain a team, (3) select the modelling approach(es), (4) implement the model(s), and (5) make practical decisions and evaluate the model(s). Although these were presented sequentially, in reality, they are more of a cycle, each influencing the other. Hopefully, this will provide the reader with some concrete starting points for modelling a complex behaviour of their interest. One of the most surprising findings of organising the meeting on this topic was the importance of community in the modelling effort. Organizing meetings to model complex behaviors is made easier by many institutions that provide spaces for interdisciplinary gatherings, such as the Lorentz Center in the Netherlands and the Harnack House in Germany. On the other hand, funding instruments and institutional incentives tend to predominantly facilitate mono-disciplinary research. This is particularly challenging because interdisciplinary research and, in particular, the modelling of complex behaviour is an example of Slow Science 
(Frith, 2020)
.
We think this work of creating computational models of complex behaviours is critically important to address the challenges we have today. Although we have solid models of perceptual decision making 
(Ratcliff, 1978;
Ratcliff et al., 2016)
 we fail to be able to model the decisions about whether to mask or not during a pandemic 
(Chan, Zhang, & Weman-Josefsson, 2021)
, which requires an integration of a much more complex web of evidence 
(Gerlee, JÃ¶ud, Spreco, & Timpka, 2022;
Woensdregt et al., 2024)
. Modelling complex problems such as climate change or pandemics is nevertheless critical because models allow for making very precise and quantitative predictions that can help to oversee the effects of different behavioural strategies. Moreover, modelling helps to clarify and streamline the problem and to identify critical assumptions. As such, computational modelling of behaviour has an important role to play in addressing the polycrisis.
 












Computational modeling in cognitive science: A manifesto for change




C
Addyman






R
M
French








Topics in Cognitive Science




4


3
















Rules of the mind




J
R
Anderson








Psychology Press












An integrated theory of the mind




J
R
Anderson






D
Bothell






M
D
Byrne






S
Douglass






C
Lebiere






Y
Qin








Psychological Review




111


4


1036














More is different: Broken symmetry and the nature of the hierarchical structure of science




P
W
Anderson








Science




177


4047
















A multicomponent model of working memory




A
D
Baddeley






G
Hitch






R
Allen








Working Memory: State of the Science


















Optimally interacting minds




B
Bahrami






K
Olsen






P
E
Latham






A
Roepstorff






G
Rees






C
D
Frith








Science




329


5995
















Mathematical psychology




W
H
Batchelder








Wiley Interdisciplinary Reviews: Cognitive Science




1


5
















Thinking together with material representations: Joint epistemic actions in creative problem solving




J
S
BjÃ¸rndahl






R
Fusaroli






S
Ã˜stergaard






K
TylÃ©n








Cognitive Semiotics




7


1
















Agreeing is not enough: The constructive role of miscommunication




J
S
BjÃ¸rndahl






R
Fusaroli






S
Ã˜stergaard






K
TylÃ©n




10.1075/is.16.3.07fus








Interaction Studies




16


3
















Theoretical modeling for cognitive science and psychology




M
Blokpoel






I
Van
Rooij


















How sure is the driver? Modelling drivers' confidence in left-turn gap acceptance decisions




F
Bontje






A
Zgonnikov




10.1007/s42113-024-00207-7








Computational Brain & Behavior




7




















K
BÃ¶rner






N
Contractor






H
J
Falk-Krzesinski






S
M
Fiore






K
L
Hall






J
Keyton














A multi-level systems perspective for the science of team science




B
Uzzi








Science Translational Medicine




2


49
















Theory construction methodology: A practical framework for building theories in psychology




D
Borsboom






H
L J
Maas






Van Der






J
Dalege






R
A
Kievit






B
D
Haig




10.1177/1745691620969647








Perspectives on Psychological Science




16


4
















Model-based hierarchical reinforcement learning and human action control




M
Botvinick






A
Weinstein








Philosophical Transactions of the Royal Society B




369














Scientists' use of diagrams in developing mechanistic explanations: A case study from chronobiology




P
Bruna






C
Kello






D
C
Burnston






B
Sheredos






A
Abrahamsen






W
Bechtel








Pragmatics & Cognition




22


2










Least effort and alignment in task-oriented communication








Meaning modulations and stability in large language models: An analysis of BERT embeddings for psycholinguistic research




G
Cassani






F
GÃ¼nther






G
Attanasio






F
Bianchi






M
Marelli


















Opening up: Tips for fostering belongingness in our scholarly communities while encouraging open science. The Industrial-Organizational Psychologist




C
M
Castille






H
R
Cobb






J
A
Siegel






C
L
Thomas








60














Why people failed to adhere to COVID-19 preventive behaviors? Perspectives from an integrated behavior change model




D
K C
Chan






C.-Q
Zhang






K
Weman-Josefsson




10.1017/ice.2020.245








Infection Control & Hospital Epidemiology




42


3
















Cultural diffusion dynamics depend on behavioural production rules




M
Chimento






B
J
Barrett






A
Kandler






L
M
Aplin








Proceedings of the Royal Society B




289


20221001














Temporal dynamics and policy implications in an innovation arms race




M
Chimento






E
Dalmaijer






B
C
Klump






L
M
Aplin




10.1101/2025.02.13.638049


















Material symbols




A
Clark








Philosophical Psychology




19


3
















The extended mind




A
Clark






D
Chalmers








Analysis




58


1
















Coupled relationships between humans and other organisms in urban areas




B
Clucas






J
M
Marzluff








Urban Ecology: Patterns, Processes, and Applications


















Validation of software for Bayesian models using posterior quantiles




S
R
Cook






A
Gelman






D
B
Rubin




10.1198/106186006X136976








Journal of Computational and Graphical Statistics




15


3
















Implementations are not specifications: Specification, replication and experimentation in computational cognitive modeling




R
P
Cooper






O
Guest




10.1016/j.cogsys.2013.05.001








Cognitive Systems Research




27
















Autistic teenagers lexically align at similar rates to typically developing peers




G
Corrigan






J
Carmona






R
Fusaroli






E
Weed






D
Fein






L
Naigles




15.30






Age (Years)




20
















Coordination costs and project outcomes in multiuniversity collaborations




J
N
Cummings






S
Kiesler








Research Policy




36


10
















Bridging theory and data: A computational workflow for cultural evolution




D
Deffner






N
Fedorova






J
Andrews






R
Mcelreath








Proceedings of the National Academy of Sciences




121


48


2322887121














There are no shortcuts to theory




B
Devezer








Behavioral and Brain Sciences




47


38














Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity




B
Devezer






L
G
Nardin






B
Baumgaertner






E
O
Buzbas








PLoS One




14


5


216125














BERT: Pre-training of deep bidirectional transformers for language understanding




J
Devlin






M.-W
Chang






K
Lee






K
Toutanova




10.48550/ARXIV.1810.04805


















Language-specific constraints on conversation: Evidence from danish and norwegian




C
Dideriksen






M
H
Christiansen






M
Dingemanse






M
HÃ¸jmark-Bertelsen






C
Johansson






K
TylÃ©n






R
Fusaroli








Cognitive Science




47


11


13387














Quantifying the interplay of conversational devices in building mutual understanding




C
Dideriksen






M
H
Christiansen






K
TylÃ©n






M
Dingemanse






R
Fusaroli








Journal of Experimental Psychology: General




152


3


864
















M
Dubova






S
Chandramouli






G
Gigerenzer






P
GrÃ¼nwald






W
Holmes






T
Lombrozo




10.31222/osf.io/bs5xe




Is Ockham's razor losing its edge? New perspectives on the principle of model parsimony
















Recognizing perceived interdependence in face-to-face negotiations through multimodal analysis of nonverbal behavior




B
Dudzik






S
Columbus






T
M
Hrkalovic






D
Balliet






H
Hung




















Computational modeling of cognition and behavior




S
Farrell






S
Lewandowsky








Cambridge University Press












Reframing cognitive science as a complexity science




L
H
Favela






M
J
Amon








Cognitive Science




47


4


13280














Charting the constellation of science reform




S
Field




10.33612/diss.229114775












University of Groningen). University of Groningen






PhD thesis








Diversity promotes abstraction and cognitive flexibility in collective problem solving




T
G
Fjaellingsdal






C
Vesper






R
Fusaroli






K
TylÃ©n


















Making the future together: Shaping autism research through meaningful participation




S
Fletcher-Watson






J
Adams






K
Brook






T
Charman






L
Crane






J
Cusack






.
.
Pellicano






E








Autism




23


4
















The blind spot: Why science cannot ignore human experience




A
Frank






M
Gleiser






E
Thompson








MIT Press












Lack of theory building and testing impedes progress in the factor and network literature




E
I
Fried




10.1080/1047840x.2020.1853461








Psychological Inquiry




31


4
















Fast lane to slow science




U
Frith








Trends in Cognitive Sciences




24


1
















Causal inference tools for pharmacovigilance: Using causal graphs to systematize biases, plan disproportionality analyses, and reduce the risk of spin. OSF Preprints




M
Fusaroli






J
Mitchell






A
Rudolph






E
Rocca






R
Fusaroli




















Coming to terms: Quantifying the benefits of linguistic coordination




R
Fusaroli






B
Bahrami






K
Olsen






A
Roepstorff






G
Rees






C
Frith






K
TylÃ©n








Psychological Science




23


8
















The dialogically extended mind: Language as skilful intersubjective engagement




R
Fusaroli






N
Gangopadhyay






K
TylÃ©n








Cognitive Systems Research




29
















Dialog as interpersonal synergy




R
Fusaroli






J
RÄ…czaszek-Leonardi






K
TylÃ©n








New Ideas in Psychology




32
















Why behaviour patterns that animals learn socially are locally adaptive




B
G
Galef
Jr








Animal Behaviour




49


5
















Computational models predicting the early development of the COVID-19 pandemic in Sweden: Systematic review, data synthesis, and secondary validation of accuracy




P
Gerlee






A
JÃ¶ud






A
Spreco






T
Timpka




10.1038/s41598-022-16159-6








Scientific Reports




1


12














Surrogates for theories




G
Gigerenzer




10.1177/0959354398082006








Theory & Psychology




8


2
















How computational modeling can force theory building in psychological science




O
Guest






A
E
Martin








Perspectives on Psychological Science




16


4
















A probabilistic Earley parser as a psycholinguistic model




J
Hale


















The science of team science: A review of the empirical evidence and research gaps on collaboration in science




K
L
Hall






A
L
Vogel






G
C
Huang






K
J
Serrano






E
L
Rice






S
P
Tsakraklides






S
M
Fiore




10.1037/amp0000319








American Psychologist




73


4
















Speech-and text-based classification of neuropsychiatric conditions in a multidiagnostic setting




L
Hansen






R
Rocca






A
Simonsen






L
Olsen






A
Parola






V
Bliksted








Nature Mental Health




1


12
















Running repairs: Coordinating meaning in dialogue




P
G
Healey






G
J
Mills






A
Eshghi






C
Howes








Topics in Cognitive Science




10


2
















The weirdest people in the world?




J
Henrich






S
J
Heine






A
Norenzayan




10.1017/S0140525X0999152X








Behavioral and Brain Sciences




33


2-3
















Enhancing the effectiveness of team science




M
L
Hilton






N
J
Cooke


















The diversity-innovation paradox in science




B
Hofstra






V
V
Kulkarni






S
M
Galvez






.-N
He






B
Jurafsky






D
Mcfarland






D
A








Proceedings of the National Academy of Sciences




117


17
















A distributed representation of temporal context




M
W
Howard






M
J
Kahana








Journal of Mathematical Psychology




46


3
















Cognition in the wild




E
Hutchins








MIT Press












Highly accurate protein structure prediction with AlphaFold




J
Jumper






R
Evans






A
Pritzel






T
Green






M
Figurnov






O
Ronneberger




10.1038/s41586-021-03819-2








Nature




596


7873
















A model hierarchy for psychological science




D
Kellen




10.1007/s42113-019-00037-y








Computational Brain & Behavior




2


3-4
















Soft-assembly of sensorimotor function




C
T
Kello






G
C
Van Orden








Psychology, and Life Sciences




13


1


57








Nonlinear Dynamics








Is bin-opening in cockatoos leading to an innovation arms race with humans?




B
C
Klump






R
E
Major






D
R
Farine






J
M
Martin






L
M
Aplin








Current Biology




32


17
















Innovation and geographic spread of a complex foraging culture in an urban parrot




B
C
Klump






J
M
Martin






S
Wild






J
K
HÃ¶rsch






R
E
Major






L
M
Aplin








Science




373


6553
















A coupled oscillator model of interactive tapping




I
Konvalinka






P
Vuust






A
Roepstorff






C
D
Frith








ESCOM 2009: 7th Triennial Conference of European Society for the Cognitive Sciences of Music
















The Soar cognitive architecture




J
E
Laird








MIT Press












Falsification and the methodology of scientific research programmes




I
Lakatos








The methodology of scientific research programmes: Philosophical papers


J. Worrall & G. Currie




Cambridge University Press




















M
D
Lee






A
H
Criss






B
Devezer






C
Donkin






A
Etz






F
P
Leite














Robust modeling in cognitive science




J
Vandekerckhove




10.1007/s42113-019-00029-y








Computational Brain & Behavior




2


3-4
















Bayesian cognitive modeling: A practical course




M
D
Lee






E.-J
Wagenmakers








Cambridge University Press












The strategy of model building in population biology




R
Levins










American Scientist




54


4
















Expectation-based syntactic comprehension




R
Levy








Cognition




106


3
















Models of the mind: How physics, engineering and mathematics have shaped our understanding of the brain




G
Lindsay








Bloomsbury Publishing












Similarity and number of alternatives in the random-dot motion paradigm




L
Maanen






Van






R
P
Grasman






B
U
Forstmann






M
C
Keuken






S
D
Brown






E.-J
Wagenmakers








Perception, & Psychophysics




74










Attention








Equality bias impairs collective decision-making across cultures




A
Mahmoodi






D
Bang






K
Olsen






Y
A
Zhao






Z
Shi






K
Broberg








Proceedings of the National Academy of Sciences




112


12
















Conflict strength: Measuring the tension between cooperative and competitive incentives in experimental negotiation tasks




J
M
Majer






M
Schweinsberg






H
Zhang






R
TrÃ¶tschel








Collabra: Psychology




8


1


35330














Vision




D
Marr








W.H. Freeman


San Francisco












The problem is epistemology, not statistics: Replace significance tests by confidence intervals and quantify accuracy of risky numerical predictions




P
E
Meehl




L. L.
















What if there were no significance tests?




S
A
Harlow






Mulaik




J. H. Steiger




Erlbaum


Mahwah, New Jersey












Simulation-based calibration checking for Bayesian computation: The choice of test quantities shapes sensitivity




M
ModrÃ¡k






A
H
Moon






S
Kim






P.-C
BÃ¼rkner






N
Huurre






K
FaltejskovÃ¡






.
.
Vehtari






A




10.1214/23-BA1404








Bayesian Analysis


















A problem in theory




M
Muthukrishna






J
Henrich








Nature Human Behaviour




3


3
















Applying occam's razor in modeling cognition: A bayesian approach




I
J
Myung






M
A
Pitt




10.3758/bf03210778








Psychonomic Bulletin &Amp












Review, 4 (1








Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection




D
J
Navarro




10.1007/s42113-018-0019-z








Computational Brain & Behavior




2


1
















An introduction to Bayesian data analysis for cognitive science




B
Nicenboim






D
Schad






S
Vasishth








Chapman; Hall/CRC












Models of retrieval in sentence comprehension: A computational evaluation using Bayesian hierarchical modeling




B
Nicenboim






S
Vasishth




10.1016/j.jml.2017.08.004








Journal of Memory and Language




99
















Replicability, robustness, and reproducibility in psychological science




B
A
Nosek






T
E
Hardwicke






H
Moshontz






A
Allard






K
S
Corker






A
Dreber








Annual Review of Psychology




73
















Choice, similarity, and the context theory of classification




R
M
Nosofsky








Journal of Experimental Psychology: Learning, Memory, and Cognition




10


1


104














Modelling scientific communities




C
O'connor








Cambridge University Press












Bayesian rationality: The probabilistic approach to human reasoning




M
Oaksford






N
Chater








Oxford University Press












Addressing the theory crisis in psychology




K
Oberauer






S
Lewandowsky




10.3758/s13423-019-01645-2








Psychonomic Bulletin & Review




26


5
















The difference: How the power of diversity creates better groups, firms, schools, and societies




S
Page








Princeton University Press












When a good fit can be bad




C
S
Peirce






Routledge






M
A
Pitt






I
J
Myung




10.1016/S1364-6613(02)01964-2








Chance, love, and logic






6








The fixation of belief








Diffusion decision model: Current issues and history




R
Ratcliff






R
Ratcliff






P
L
Smith






S
D
Brown






G
Mckoon








Trends in Cognitive Sciences




85


2










Psychological Review








Models of the reading process




K
Rayner






E
D
Reichle




10.1002/wcs.68








WIREs Cognitive Science




1


6
















How hard is cognitive science?




P
Rich






R
Haan






De






T
Wareham






I
Van
Rooij








Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






43












How persuasive is a good fit? A comment on theory testing




S
Roberts






H
Pashler




10.1037/0033-295X.107.2.358








Psychological Review




107


2
















Zooming in on what counts as core and auxiliary: A case study on recognition models of visual working-memory




M
M
Robinson






J
R
Williams






J
Wixted






T
F
Brady




10.31234/osf.io/7an3x


















Formalizing verbal theories: A tutorial by dialogue




I
Rooij






Van






M
Blokpoel








Social Psychology




51
















Complexity as a system property




R
Rosen








International Journal of General System




3


4
















Vocal markers of autism: Assessing the generalizability of machine learning models




A
Rybner






E
T
Jessen






M
D
Mortensen






S
N
Larsen






R
Grossman






N
Bilenberg








Autism Research




15


6
















Team cognition: Understanding the factors that drive process and performance




E
Salas






S
M
Fiore








American Psychological Association












The science of teamwork: Progress, reflections, and the road ahead




E
Salas






D
L
Reyes






S
H
Mcdaniel








American Psychologist




73


4


593














Models of word reading




M
S
Seidenberg






M
Farry-Thorn






J
D
Zevin




10.1002/9781119705116.ch2








John Wiley & Sons












A survey of model evaluation approaches with a tutorial on hierarchical Bayesian methods




R
Shiffrin






M
D
Lee






W
Kim






E.-J
Wagenmakers




10.1080/03640210802414826








Cognitive Science




32


8
















Models are stupid, and we need more of them




P
E
Smaldino








Computational social psychology












Routledge








How to translate a verbal theory into a formal model




P
E
Smaldino








Social Psychology




51


4


207














Modeling social behavior: Mathematical and agent-based models of social dynamics and cultural evolution




P
E
Smaldino








Princeton University Press












Using cognitive agents to train negotiation skills




C
A
Stevens






J
Daamen






E
Gaudrain






T
Renkema






J
D
Top






F
Cnossen






N
A
Taatgen








Frontiers in Psychology




9


154














Models of data




P
Suppes








Studies in logic and the foundations of mathematics




Elsevier




44














Is preregistration worthwhile?




A
Szollosi






D
Kellen






D
Navarro






R
Shiffrin






I
Rooij






Van






T
Van Zandt






C
Donkin




10.1016/j.tics.2019.11.009








Trends in Cognitive Sciences
















Validating Bayesian inference algorithms with simulation-based calibration




S
Talts






M
J
Betancourt






D
P
Simpson






A
Vehtari






A
Gelman




arXiv:1804.06788










arXiv Preprint








The cambridge handbook of computational psychology




M
S C
Thomas






J
L
Mcclelland




R. Sun






Cambridge University Press




New York






Connectionist models of cognition








Coordination dynamics: A foundation for understanding social behavior




E
Tognoli






M
Zhang






A
Fuchs






C
Beetle






J
A S
Kelso








Frontiers in Human Neuroscience




14


317














From extended mind to collective mind




D
P
Tollefsen








Cognitive Systems Research




7


2-3
















Approaches to analysis in model-based cognitive neuroscience




B
M
Turner






B
U
Forstmann






B
C
Love






T
J
Palmeri






L
Van Maanen




10.1016/j.jmp.2016.01.001








Journal of Mathematical Psychology




76


















K
TylÃ©n






R
Fusaroli






J
S
BjÃ¸rndahl






J
Raczaszek-Leonardi






S
Ã˜stergaard






F
Stjernfelt




Diagrammatic reasoning: Abstraction, interaction, and insight. Pragmatics & Cognition






22














The social route to abstraction: Interaction and diversity enhance performance and transfer in a rule-based categorization task




K
TylÃ©n






R
Fusaroli






S
M
Ã˜stergaard






P
Smith






J
Arnoldi








Cognitive Science




47


9


13338














A simple repair mechanism can alleviate computational demands of pragmatic reasoning: Simulations and complexity analysis




J
Van Arkel






M
S
Woensdregt






M
Dingemanse






M
Blokpoel








The 24th (Virtual) Conference on Computational Natural Language Learning




The Association for Computational Linguistics
















Precise minds in uncertain worlds: Predictive coding in autism




S
Van De Cruys






K
Evers






R
Van Der Hallen






L
Van Eylen






B
Boets






L
De-Wit






J
Wagemans








Psychological Review




121


4


649














Is cognitive science usefully cast as complexity science?




G
Van Orden






D
G
Stephen








Topics in Cognitive Science




4


1
















On the necessity of integrating multiple levels of abstraction in a single computational framework. Current Opinion in Behavioral Sciences




H
Van Rijn






J
P
Borst






N
Taatgen






L
Van Maanen




10.1016/j.cobeha.2016.07.007








11














Attention is all you need




A
Vaswani






N
Shazeer






N
Parmar






J
Uszkoreit






L
Jones






A
N
Gomez








Advances in Neural Information Processing Systems






30






Polosukhin, I








Introducing tomsup: Theory of mind simulations using Python




P
T
Waade






K
C
Enevoldsen






A
Vermillet






A
Simonsen






R
Fusaroli




10.3758/s13428-022-01827-2


















Inhibition in Pavlovian conditioning: Application of a theory. Inhibition and Learning




A
R
Wagner






R
A
Rescorla




















Difficulty of selecting among multilevel models using predictive accuracy




W
Wang






A
Gelman




10.4310/SII.2015.v8.n2.a3








Statistics and Its Interface




7


















L
A
Weber






P
T
Waade






N
Legrand






A
H
MÃ¸ller






K
E
Stephan






C
Mathys




arXiv:2305.10937


The generalized hierarchical gaussian filter










arXiv Preprint








Task-general efficiency of evidence accumulation as a computationally defined neurocognitive trait: Implications for clinical neuroscience




A
Weigard






C
Sripada








Biological Psychiatry Global Open Science




1


1
















Ten simple rules for the computational modeling of behavioral data. eLife




R
C
Wilson






A
G
Collins








8


49547












Lessons for theory from scientific domains where evidence is sparse or indirect




M
S
Woensdregt






R
Fusaroli






P
Rich






M
ModrÃ¡k






A
Kolokolova






C
Wright






A
S
Warlaumont








Computational Brain & Behavior




7


4
















Why is scaling up models of language evolution hard?




M
S
Woensdregt






M
Spike






R
Haan






De






T
Wareham






I
Rooij






Van






M
Blokpoel








Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






43












Large teams develop and small teams disrupt science and technology




L
Wu






D
Wang






J
A
Evans








Nature




566


7744
















The epistemic benefit of transient diversity




K
J
Zollman








Erkenntnis




72


1

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]