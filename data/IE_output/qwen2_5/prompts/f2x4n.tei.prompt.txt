You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Over the last decades, the number of tools based on artificial intelligence (AI) designed to assist human decisions has increased in many professional fields such as justice 
(Green & Chen, 2019;
Valdivia et al., 2022)
, personnel selection 
(Kupfer et al., 2023;
Lacroux & Martin-Lacroux, 2022
) and healthcare 
(Adlung et al., 2021;
Esteva et al., 2019;
Topol, 2019;
Yu et al., 2018)
. In the medical realm, the advent of AI-based decision support systems has been welcomed as a promise to minimize errors in human decision-making 
(Garcia-Vidal et al., 2019;
Hinton, 2018;
Loftus et al., 2020;
Shortliffe & Sepúlveda, 2018)
.
Such optimism is founded on the impressive precision that artificial intelligence has reached in some clinical tasks such as image-based diagnostics, predicting the outcomes of interventions, or recommending treatments 
(Esteva et al., 2019;
Gulshan et al., 2016;
Rajpurkar et al., 2022)
. Thus, AI tools can help health professionals in, for example, diagnosis and triage tasks 
(Lyell et al., 2021)
 by offering them specific and quite accurate data-driven recommendations 
(Hollister & Bonham, 2018;
Sutton et al., 2020)
. This cooperation between natural and artificial intelligence is hoped to augment clinicians' knowledge and capabilities, as well as compensate for some of the weaknesses of their human minds, such as cognitive biases 
(Berthet, 2022;
Blumenthal-Barby & Krieger, 2015;
Croskerry et al., 2023;
Saposnik et al., 2016)
 and vulnerability to fatigue 
(Linder et al., 2014;
Neprash & Barnett, 2019)
, and therefore reduce diagnostic errors, improving clinical decisions 
(Kahneman et al., 2021)
. But these AI tools are not free of errors and biases themselves.
Despite their promising benefits, the introduction of AI-based decision support systems in clinical contexts has also raised concerns about the peril of using biased AI to assist medical decisions 
(Cho, 2021;
Norori et al., 2021;
Parikh et al., 2019)
. People tend to perceive artificial intelligence algorithms as objective, secure 
(Sundar & Kim, 2019
) and impartial 
(Claudy et al., 2022)
, but AI algorithms are a product of human design, so they often inherit our mistakes and biases 
(Fry, 2018;
O'Neil, 2016)
.
Although AI-based tools have proven to be quite accurate, their performance is far from perfect and most of them still need to be validated in real-world settings 
(Topol, 2019)
.
Thus, while AI can help to overcome some of the limitations of human reasoning, other new problems may arise in the human-AI interaction 
(Rastogi et al., 2022)
. Biased AI systems could sometimes diminish rather than augment the correctness of clinical decisions in that collaborative AI-human decision-making 
(Howard et al., 2020;
Lai et al., 2021)
.
Bias in an algorithm is defined as a systematic error in its outputs or processes 
(Fletcher et al., 2021;
Mehrabi et al., 2021)
. One important potential source of bias in AI systems is biases or imbalances in the data used to train the algorithms 
(Danks & London, 2017;
Mehrabi et al., 2021;
Norori et al., 2021;
. AI algorithms identify patterns and generate predictions based on historical data (i.e. what happened in the past). High-quality data sets used to feed the algorithms are difficult to obtain 
(Adlung et al., 2021)
, especially when they pertain to clinical data 
(Wiens et al., 2019)
. Consequently, if a given data set contains imbalances or biases 
(Larrazabal et al., 2020)
, the AI systems trained with these data will learn and potentially reproduce those biases 
(Obermeyer & Mullainathan, 2019)
.
In the healthcare area, the data used to train the algorithms is usually a product of past human decisions, for example, medical decisions about what kind of patient requires a certain test or what kind of patient would benefit more from a given treatment. When the historical record of these choices shows an extended systematic error such as, for instance, a misdiagnosis of a certain pathology in young patients, the AI system trained with such historical data will simply inherit this error 
(Obermeyer & Lee, 2017)
. It is relevant to note the technical nature of AI bias, as it is, in essence, a mathematical or statistical artefact. However, AI bias can result in discrimination or prejudice towards a person or group of people (High-Level Expert Group on AI, 2019) since the patterns embedded in the historical data often reflect systematic social and economic inequities. When an artificial intelligence system is trained with data that does not represent the diversity of the reality and population groups to which an AI tool is to be applied 
(Larrazabal et al., 2020)
, the model will produce undesirable effects due to the difficulty in generalising its predictions to social groups or environments with characteristics underrepresented in that databases 
(Buolamwini & Gebru, 2018;
DeCamp & Lindvall, 2020)
. As a consequence, AI recommendations may exhibit systematic errors such as, for example, under-diagnosis in historically underservedpatient populations, such as patients of low socioeconomic status, female patients or Black and Hispanic patients 
(Seyyed-Kalantari et al., 2021)
.
In high-stakes areas such as healthcare, clinicians assume the responsibility and accountability for the decisions of the AI-human team (European Commission, 2021).
Thus, they are expected to supervise the outcomes of their artificial counterparts. Since there is a risk of bias (i.e., systematic errors) in the recommendations of AI-based decision support systems, health professionals should interpret AI advice as just an additional piece of evidence to help them in the decision-making process. Hence, they need to critically supervise and decide whether the AI advice is correct or useful for each decision 
(Suresh et al., 2020)
 Some evidence has pointed out that this effective oversight and control of AI by humans could be possible 
(Reverberi et al., 2022;
Solans et al., 2022;
Tschandl et al., 2020)
, while another corpus of research has documented an excessive trust towards AI recommendations, which calls into question the ability of humans to exercise a good supervision of algorithmic outcomes 
(Bogert et al., 2022;
Logg et al., 2019;
Rebitschek et al., 2021)
. The tendency to over-rely on artificial intelligence can lead to humans uncritically adhering to AI recommendations, even incorrect ones 
(Goddard et al., 2012
(Goddard et al., , 2014
Suresh et al., 2020)
.
Human over-reliance on AI advice seems to be modulated by the context and the characteristics of the task (e.g., subjective vs objective) in which AI and humans collaborate 
(Agudo & Matute, 2021;
Castelo et al., 2019;
Chong et al., 2023;
Himmelstein & Budescu, 2023;
Lee, 2018)
. In the clinical context, algorithmic advice is usually seen as trustworthy because artificial intelligence is perceived as accurate in objective and analytical tasks 
(Araujo et al., 2020;
Castelo et al., 2019;
Logg et al., 2019)
, and these are the ones that are common in the medical domain, such as diagnosis and image classification. Thus, there are reasons to believe that decision-making in a health context could be particularly vulnerable to human over-reliance on AI advice, and therefore humans could tend to accept the recommendations of AI algorithms even when they are noticeably biased or erroneous.
There is some evidence that incorrect AI recommendations can have a detrimental influence on clinicians ' decision-making 
(Dratsch et al., 2023;
Gaube et al., 2021;
Lyell et al., 2017
Lyell et al., , 2018
. As an example, a recent study showed that when prescribing antidepressants in different scenarios, incorrect AI recommendations led to a reduction in the accuracy of the clinician's decisions in comparison to a baseline and a correct advice condition 
(Jacobs et al., 2021)
. These results suggest that occasional mistakes in the AI recommendations can make people err, but our main concern refers to the presence of systematic biases in AI systems and to the potential of humans to perpetuate those biases. Thus, it is necessary to explore how humans react to systematic errors in AI advice, because AI bias may have a more profound impact on human behaviour than occasional and random errors. To our knowledge, very few investigations have addressed the effect of biased artificial intelligence systems on human behaviour 
(Solans et al., 2022)
, and even fewer studies have focused, specifically, on the impact of algorithmic bias on human decisions in a clinical context. The present research aims to answer two main questions. The first one is whether biased recommendations of an AI system can influence human behaviour, specifically decision-making, in a medical context. The recent study of Adam et al. 
(Adam et al., 2022)
 represented an interesting starting point to this problem since they directly explored the influence of biased recommendations of an algorithm on the human response to mental health emergencies. These authors reported that the biased recommendations strongly influenced the decisions of experts and non-experts on how to respond to a crisis, while the decisions of participants without the advice of the algorithm were unbiased. In addition, we believe that to fully explore the potential impact of AI biases on human behaviour, it is also necessary to answer a second question. This question is whether, after the interaction with a biased AI system, people would reproduce those biases in their own decisions when they move on to a context without their artificial partner. The reader should consider the following scenario: given the case of a group of people accustomed to performing a specific task with the suggestions of a biased AI-based decision support system, is there a risk that the system's biased recommendations will exert a training effect such that people will reproduce the AI bias when making decisions on their own? Could the system influence the behaviour of these people so that they inherit the AI bias, even in a context without AI recommendations?
The potential inheritance of AI bias and its propagation through human decisions is a phenomenon that remains unexplored. The current research aims to examine how biased AI recommendations can influence people's decision-making in a health-related task and to test whether the impact of the biased advice on human behaviour extends beyond the phase in which the AI recommendations are explicitly present. That is, we will explore whether human decision-makers can inherit the bias of an artificial intelligence system.


Overview of the experiments
In a series of three experiments, we empirically tested whether (a) people follow the biased recommendations offered by an AI system, even if this advice is noticeably erroneous sometimes (Experiment 1); (b) people who have performed a task assisted by the biased recommendations will reproduce the same type of errors than the system when they have to do the same task without assistance, showing an inherited bias (Experiment 2); and (c) performing a task first without assistance will prevent people from following the biased recommendations of an AI and, thus, from committing the same errors, when they later perform the same task assisted by a biased AI system.


Experiment 1
The first experiment tested the influence of explicitly biased recommendations made by a fictitious AI system on participants' behaviour using a classification task with a medical-themed story: a simulation of an image-based diagnosis.


Method


Ethics statement
The Ethical Review Board of the University of Deusto reviewed and approved the methodology reported in this article and the experiments were conducted according the approved guidelines. Informed consent was obtained from all participants. Due to ethical considerations, as well as to prevent the influence of prior knowledge and beliefs on experiments' results, the clinical context, the classification task, the artificial intelligence system and its recommendations, the tissue sample images, patients and syndromes used in these experiments, were all fictitious.


Participants
A group of 169 Psychology students took part in the experiment (73.6% female, 20.7% male, 2.4% non-binary, mean age = 18.4, SD = 0.79). Their participation was anonymous and we did not ask for any personal data other than age and gender.
Participants were randomly assigned to either one of two groups, assisted by AI (n = 85) and unassisted (n = 84). The post hoc sensitivity analysis showed that, with this sample size, we obtained a power of 0.80 to detect a size effect of d = 0.38 or higher in a test of difference between two independent means.


Materials and Procedure
In the three experiments we used the same experimental task: a classification task framed in a fictitious health context. With this procedure we tried to simulate a clinical decision-making process that was assisted by a biased AI system for some participants, while others performed the task without assistance.
The experimental task was constructed though Qualtrics, a platform that also facilitated the data collection. We wanted to simulate a clinical decision-making process with a medical diagnosis task that was simple to learn, but also challenging enough to require sustained attention. With this aim, we used the stimuli created by Blanco et al.
(2022) in their study related to causal illusion. Each stimulus consisted of a matrix of 50
x 50 pixels which represented a human tissue sample obtained from a given patient.
Each tissue sample contained 2500 cells of two colours (dark pink and light yellow) randomly distributed in the matrix space so that there were no two identical samples.
The proportion of dark and light cells in each tissue sample was variable, so we created a large set of different stimuli with varying levels of discrimination difficulty. The different proportions of dark and light cells for different stimuli were 80/20, 70/30, 60/40, 40/60, 30/70, and 20/80.
In the classification task, participants were instructed to observe a series of tissue samples, to decide, for each sample, whether it was affected or not by a fictitious disease called Lindsay Syndrome. Each tissue sample had cells of two colours, but one of them was presented in a greater proportion and volunteers were instructed to follow this criterion to identify the presence of the syndrome. A greater proportion of dark than light cells was described in the instructions as "Positive", that is, affected by the Lindsay Syndrome. If the tissue sample had a greater proportion of light than dark cells, then it should be classified as a "Negative" because it was not affected by the syndrome.
It is important to note that the assignment of dark and light colours to the positive/negative categories was randomly decided for each participant (in this section we will describe only one of the two possible assignments for simplicity, but the reader should bear in mind that half of the participants viewed the opposite one).
To ensure that all participants correctly understood the instructions the experiment begun with a practice phase in which participants categorised six tissue samples with different dark/light colours proportions. Each sample was presented twice, so that the practice phase consisted of two blocks of six stimuli, presented one per trial, that is 12 trials in total. In the first block, the six samples were presented in order of difficulty. In the second block, the same six samples were presented in random order. If participants did not get five correct classifications out of six trials in the second block, they had to repeat all the practice phase.
Once the volunteers finished the practice phase, they were randomly distributed into two groups, AI-assisted and unassisted. The design of the experiments is summarized in 
Table 1
. The task of the participants was to classify a series of 60 tissue samples following the criteria they had just learned in the previous phase. In the AI-assisted group, in each trial, the tissue sample and the AI recommendation were presented simultaneously. The recommendation had the form of an orange label with the text "POSITIVE +", or a blue label with the text "NEGATIVE -", placed above the tissue sample. In the unassisted group, only the tissue sample was presented in each trial. Both groups viewed the same stimuli, the only difference between them was the presence or absence of the advice of the fictitious AI. The sequence of trials included ten stimuli (i.e., tissue samples) of each of the dark/light cell proportions, that is, 80/20, 70/30, 60/40, 40/60, 30/70 and 20/80, resulting in a total of 60 stimuli. 
Figure 1
 shows some examples of trials in both the unassisted and the AI-assisted groups.


Figure 1
Screenshots showing examples of trials in the unassisted (top) and 
AI-assisted conditions (bottom)
.
The AI of our experiment was quite reliable because it correctly classified 50 tissue samples out of 60, which corresponded to approximately 80% correct classifications. However, the AI showed a bias that consisted on a systematic error in the recommendations on the 10 stimuli with the proportion of dark/light cells 40/60. In these samples, there was a contradiction between the evidence (i.e., number of dark/light cells in the tissue) and the recommendation given by the AI. For example, following the instructions, the 40/60 tissue samples had a greater number of light cells so they should be classified as negative, however, the recommendation given by the AI for the 40/60 samples was positive.
The order of the stimuli was randomly assigned to the sequence of 60 trials, with one exception: the stimulus 40/60, where the AI recommendation was erroneous, did not appear during the first 10 trials of the sequence of 60 trials. This manipulation tried to keep certain level of trust in the AI system before the problematic stimuli showed up.
We wanted the AI bias not to be so evident from the beginning of the task. The 40/60 stimulus did appear in the sequence of the next 50 trials, randomly intermixed with the stimulus of other proportions.
After the classification task was finished, we asked participants, as a manipulation check, whether the AI algorithm had been offered them recommendations during the task. Volunteers then answered, on a Likert scale from 1 (not at all) to 9
(completely), a series of questions about their own performance on the task, about their trust on the artificial intelligence used in this experiment, and about their trust in artificial intelligence in the area of health, in general.
The main dependent variable in our experiment was the number of misclassifications of the ten 40/60 tissue samples of the task. Although the stimulus 40/60 was more difficult to classify than others, its discrimination was clear, and it was possible to detect the AI mistakes easily so, we expected the unassisted group to classify them correctly. However, since the AI showed a rather high reliability, we expected volunteers in the group assisted by the biased AI to get used to performing the task following the AI recommendations and without careful examination of the tissue sample. As a result, the AI-assisted group would misclassify more often the 40/60 stimuli, where the recommendations were erroneous, than the unassisted group.


Data selection criteria
If participants failed to reach the threshold of five correct classifications out of six trials in the second attempt of the practice phase, their data were excluded from the analyses. In addition, data from participants who misclassified tissue samples on more than half of the trials of the first phase of the classification task (i.e., more than 30 trials out of 60), were excluded from the analyses, as they were, presumably, paying little attention.


Results and Discussion


Figure 2
Mean number of misclassifications of the 40/60 stimuli in the AI-assisted group and the unassisted group.
Note. Error bars represent 95% CI.
As expected, participants who performed the task assisted by a biased AI made more errors than unassisted participants (see 
Figure 2
). The mean number of misclassifications of the 40/60 trials was 2.21 (SD = 3.17) in group AI-assisted, and 0.69 (SD = 1.83) in the unassisted group. This difference was significant, t(167) = 3.81, p <.001, d = 0.586 . These results showed that the explicit recommendations of a biased On average, participants though they had performed the task fairly well and showed a moderate trust in AI in the area of health. In the AI-assisted group we found a positive correlation between participants' incorrect classifications of the 40/60 samples and how helpful they considered the AI of the experiment to be, r = .544, p <.001, the accuracy they attributed to the AI recommendations in the task, r = .563, p <.001, and their confidence in AI in the health domain in general, r = .470, p <.001.
These results evidenced human compliance with the AI recommendations during a decision-making process. A large part of participants from the AI-assisted group reproduced the AI bias in their own responses to the medical task.


Experiment 2
The purpose of Experiment 2 was (a) to replicate the observation of Experiment 1 that a biased AI can influence human decision-making, (b) to test whether such influence persists when the AI is no longer present, and (c) to test whether this influence generalizes to novel stimuli as well. Additionally, we now also measured and analysed changes in the participants' behaviour over the sequence of trials during the classification task.


Method


Participants
The final sample included 199 participants (65.3% female, 30.7% male, 4% nonbinary, mean age = 26.2, SD = 6.95). We initially recruited 200 participants through Prolific Academic, but data from one participant was excluded due to the data exclusion criteria described in Experiment 1. Participation in the experiment was offered only to those applicants in Prolific Academic's pool who speak English fluently and had not participated in previous experiments from our research team. Volunteers were randomly distributed between two groups, AI-assisted (n = 100) and unassisted (n = 99). A sensitivity analysis showed that, with this sample size, we obtained a power of 0.80 to detect a small-sized effect (f= 0.09) for a repeated measures ANOVA with withinbetween interaction.


Materials and Procedure
The procedure was similar to that of Experiment 1. The practice phase and the classification task with 60 trials were identical to those of the previous experiment. The only difference during the 60 trials phase was that in Experiment 2 the order of appearance of each stimulus was randomly assigned to a fixed position in the 60-trial sequence of the task. The creation of this fixed randomised sequence ensured that all participants viewed each stimulus in the same position through the task which facilitated the measurement of changes in the behaviour of both groups and its comparison, with particular attention to participants' responses to those trials where the AI recommendation was erroneous.
The main novelty in Experiment 2, with respect to Experiment 1, was the addition of a second phase of 25 trials in which both groups had to sort the tissue samples without assistance (see 
Table 1
). That is, participants in the unassisted group continued performing the task as in the previous phase, while AI-assisted participants switched to performing the task without assistance. An additional feature of this second phase was the introduction of five trials of novel and ambiguous stimuli with a dark/light cell ratio of 50/50, so that it was not possible to assign these stimuli to either one of the two categories, positive or negative, based on the instructions received or on specific recommendations of the AI during the previous phase. Thus, for this second phase, we expected that participants in the AI-assisted group would tend to classify both the 40/60 and the 50/50 samples in the same category as the AI biased recommendation suggested for the 40/60 stimuli in the previous phase. By contrast, we expected participants in the non-assisted group to classify correctly the 40/60 stimuli and to classify the 50/50 ambiguous stimuli randomly. We would interpret these results as an inheritance of the bias. Thereby, the purpose of this experiment is not only to reproduce but also to extend the results of Experiment 1, showing that biased AI recommendations may influence participants' behaviour even when the AI is no longer present and even in novel conditions. Also, in this phase, stimuli with the dark/light cell ratio 80/20 and 20/80 were not included in the classification task, because we considered them too easy and we were mainly interested in participants' classification of the 40/60 and 50/50 stimuli.
Thus, the second phase of the task consisted on five stimuli of each of the 70/30, 60/40, 50/50, 40/60, and 30/70 proportions, resulting in 25 trials. The order of appearance of each trial was randomly assigned to a fixed position in the 25-trial sequence of the task.
We measured an additional dependent variable in this phase: the number of times each participant classified the five 50/50 stimuli in the same direction as the bias of the AI recommendations. We named this variable as biased classifications.
At the end of the classification task, participants answered the same postexperimental questions as in Experiment 1. In the present Experiment 2, we added two questions to ask volunteers directly whether they based their answers on the recommendations and whether they detected any errors in the AI recommendations.
Experiment 2 was preregistered at https://aspredicted.org/CJG_ZLN


Results and Discussion
As expected, the AI-assisted group made more errors in the classification of the 40/60 stimuli in both phases of the task, than the unassisted group. Importantly, even in Phase 2, that both groups performed without the support of AI recommendations, the AI-assisted group committed significantly more misclassifications than the unassisted group (see 
Figure 3)
. These impressions were confirmed by a mixed ANOVA 3 (Block)
x 2 (Group) that showed a main effect of Group, F (1,197) = 41.3, p <. 001, η 2 p = .173, a main effect of Block, F (2,394) = 25.3, p <. 001, η 2 p = .114, and a Group x Block, interaction F (2,394) = 17.4, p <. 001, η 2 p = .081.


Figure 3
Mean number of misclassifications of the 40/60 stimuli in the AI-assisted group and the unassisted group along the task.
Note. The 40/60 trials of the task were divided into three blocks: the ten 40/60-stimuli trials of the first phase of the task were divided into two blocks of five trials each, resulting in Block 1 and 2, and the five 40/60-stimuli trials of the third phase of the task formed Block 3. Error bars represent 95% CI.
Within group post-hoc comparisons, with Tukey correction, showed an increase in the 40/60 misclassifications in the AI-assisted group between Block 1 and Block 2, t(197) = -4.657, pt < .001, as well as a decrease between Block 2 and Block 3, in which this group no longer had the AI recommendations, t(197) = 8.87, pt < .001. In this last phase, although none of the two groups were assisted by the biased AI, there was still a significant difference between them t(197) = 3.66, pt = .004, the AI-assisted group did not reduce their errors enough to reach the performance of the non-assisted group.
One of the main novelties of the second phase of Experiment 2 was the addition of five tissue samples with a 50/50 dark/light cell proportion. Since neither of the groups received instructions or AI recommendations on how to classify these stimuli, we expected the AI-assisted group to classify the ambiguous novel 50/50 stimuli in the same direction as the AI bias from the previous phase. An independent samples t-test showed that the AI-assisted group made more biased classifications of the 50/50 samples, M = 2.91, SD = 2.13, than the unassisted group, M = 2.15, SD = 2.02, with a small but significant difference between the groups, t(197) = 2.58, p = .011, d = 0.366. This suggest that the inherited bias is not constrained to the stimuli where the AI made errors, but can also generalize to novel stimuli that had not been seen before and had not received any previous AI recommendation.
Regarding the post-experimental questions, 58% of participants from the AIassisted group detected errors in the IA advice. Moreover, similar to Experiment 1, participants who made more errors in the classification task also found the AI of our experiment more helpful, r = .638, p < .001, perceived it to be more accurate, r = .612, p < .001, and trusted more, in general, in the usefulness of AI in the health context, r = .492, p < .001.


Experiment 3
Experiment 3 tried to replicate the bias inheritance effect observed in Experiment 2 for the IA-assisted group in the 40/60 and 50/50 stimuli during the nonassisted phase. In addition, we sought to extend the results of the previous experiments by analysing the effect of the order in which the AI-assisted phase takes place. We hypothesised that performing the classification task without assistance first, could have a protective effect against the biased recommendations when participants switched to performing the task assisted by the misleading AI.


Method


Participants
A group of 197 participants (49.2% male, 47.7 % female, 3% non-binary, mean age = 27.1, SD = 8.28) from Prolific Academic took part in the experiment (initially we recruited 200 participants but data from three of them were excluded following the data exclusion criteria described in Experiment 1). Participation in the experiment was offered only to those applicants in Prolific Academic's pool who speak English fluently and had not taken part in previous studies carried out by our research team. Volunteers were randomly assigned to groups AI-assisted → unassisted (n = 98) and unassisted → AI-assisted (n = 99). A sensitivity analysis showed that, with this sample size, we obtained a power of 0.80 to detect a small-sized effect (f = 0.08) for a repeated measures ANOVA with within-between interaction.


Materials and Procedure
The main difference on the design of Experiment 3 with the previous experiments is that all participants went through both experimental conditions, that is assisted by the biased AI recommendations and unassisted, but in different order (see 
Table 1
). Thus, the procedure of Experiment 3 was similar to that of the previous experiments, but with some modifications to adapt it to the design of the present experiment. Specifically, a change in the number of trials in Phase 1 and Phase 2, and some slight adjustments in the task instructions were necessary.
In Experiment 3, each of the two phases of the task had 40 trials. 
Table 2 depicts
 the number of trials of each type of stimuli for the AI-assisted and the unassisted phases of the task. The ten 40/60 stimuli appeared intermixed with stimuli of other proportions as in the previous experiments. The order of trials within each phase was random and identical for both groups, but in neither case did the 40/60 stimulus appear among the first five trials of the sequence. The unassisted phase of the task was characterised by the inclusion of five ambiguous stimuli (see 
Table 2
).


Table 2
Number of trials for each type of tissue sample of different dark/light cells proportions from the AI-assisted and unassisted phases of the task. Given that all participants were assisted by the AI in one phase of the task, but were unassisted in another phase, the task instructions and screens explicitly informed participants when the AI system was connected and gave them recommendations, and when the AI was off, so that they had to perform the task without assistance. In Experiment 3 two images were included to emphasise this information.
The post-experimental questions in Experiment 3 were the same as in Experiment 2, but we made a modification to the question about whether the AI had been on and specified that we were referring to whether it had been connected at any point throughout the task. We thought that this specification was necessary because the group AI-assisted → unassisted might have interpreted that this question referred only to the last part of the task. Experiment 3 was preregistered in https://aspredicted.org/8JS_FMS
Results and Discussion


Figure 4
Mean of misclassifications of the 40/60 stimuli in the AI-assisted→ unassisted group and the unassisted→ AI-assisted group along the task.
Note. The ten 40/60 trials of Phase 1 were divided into two blocks, Block 1 and 2, of five trials each. The same procedure was followed with the ten 40/60 trials of Phase 2, that were also divided in two blocks of five trials, Block 3 and 4. In total, we created four blocks of five 40/60 trials each. Error bars represent 95%
The group that completed the first phase (i.e., Blocks 1 and 2) assisted by the biased AI made more errors in 40/60 trials than group unassisted. Interestingly, when there was a change in task conditions between Blocks 2 and 3, we observed an increase in 40/60 misclassifications for the unassisted → AI-assisted group, but did not observe an opposite trajectory for the AI-assisted → unassisted group. When this group transited from the AI-assisted to the unassisted phase, its mean number of errors in 40/60 trials was not reduced. During the second phase (i.e., Blocks 3 and 4) both groups committed a similar number of errors, although in this second phase the unassisted → AI-assisted group received biased recommendations from the AI and the AI-assisted → unassisted group did not. Thus, the group of participants who completed Blocks 1 and 2, assisted by the AI recommendations exhibited the same errors as the AI system when, during the next phase, in Blocks 3 and 4, they had to perform the task without guidance (see 
Figure   4
).
These impressions were confirmed by a mixed ANOVA 3 (Block) x 2 (Group) that showed a main effect of Group, F (1,195) = 7.59, p = .006, ⴄ 2 p = .037, a main effect of Block F (1,585) = 26.0, p < .001, ⴄ 2 p = .118, and a Group x Block interaction, F
(1,585) = 43.8, p < .001, ⴄ 2 p = .183. For the unassisted → AI-assisted group, post-hoc comparisons (Tukey correction) confirmed an increase in 40/60 errors between Block 2
and Block 3, t(195) = 8.75, p < .001, when this group switched to performing the task with the AI assistance. Conversely, for the AI-assisted → unassisted group an increase in 40/60 errors was detected between Block 1 and Block 2, t(195) = -4.05, p = .002, during the AI-assisted phase of the task, but no differences were observed between the other blocks. So, there was not a significant decrease in the number of 40/60 misclassifications for this group when they switched from Block 2 to Block 3, that is, to the phase without AI recommendations, t(195) = 2.86, p = .085. This suggests that the responses of the AI-assisted → unassisted group reproduced the systematic errors of the AI recommendations during the unassisted phase, a result that supports the inheritance bias effect.
Similarly, the mean number of biased classifications of the 50/50 ambiguous stimuli during the unassisted phase in the AI-assisted → unassisted group was 3.72 (SD = 2.02) and 2.65 (SD = 1.88) in the unassisted → AI-assisted group. This difference was significant, t(195) = -4.28, p < .001, d = -.609. Both results replicated those observed in Experiment 2, and add support to the human inheritance of AI bias effect.
It is important to note that in this experiment 80.7% of participants detected mistakes in the AI recommendations. Although participants found that the AI was not entirely accurate, many of them still relied on the AI recommendations to perform the task. In contrast to previous experiments, in this experiment we did not observe a positive correlation between the number of 40/60 misclassifications and how much participants considered AI to be helpful, r = .051, p = .479, accurate, r = .055, p = .442, and reliable in the health domain, r = .083, p = .246. Thus, in this experiment, participants' responses seemed to be less influenced by their prior trust in AI.


Discussion
Our results show that biased recommendations made by AI systems can adversely impact human decisions in professional fields such as healthcare. Moreover, they also show that such biased recommendations can influence human behaviour in the long term. Humans reproduce the same biases displayed by the AI, even time after the end of their collaboration with the biased system, and in response to novel stimuli.
Although this inheritance bias effect could have serious implications, to the best of our knowledge it had not yet been explored in any empirical research.
Intending to empirically test the potential effect of the human inheritance of the AI bias, we conducted three experiments. As we have hypothesized, the AI-assisted participants made more errors than the unassisted participants in the classification task, both during the AI-aided and non-aided phases, a result consistently observed over the three experiments. These results are robust: we obtained the same effect in a laboratory experiment conducted under controlled conditions with a sample of University students (Experiment 1) and in two online experiments with an international sample recruited via Prolific Academic (Experiment 2 and Experiment 3).
Importantly, Experiments 2 and 3 evidence that participants from the AI-assisted group still misclassified, specifically, the 40/60 stimulus in the non-aided phase of the task, meaning that their mistakes when performing the task by themselves mimicked the bias previously shown by the AI during the first phase. Moreover, we also observed a tendency in the AI-assisted group to categorize the new 50/50 stimuli during the unassisted phase in the same direction as the AI classified the 40/60 stimuli during the previous phase. We interpret that participants used the AI recommendations as a cue to categorise ambiguous stimuli, which could not be classified according to the classification criteria specified in the instructions.
In sum, we observed an inheritance effect of the AI biased recommendations from the first phase of the task on participants' responses during the second phase, where they were no longer supported by the AI. We believe that this is the most interesting and novel result of our present work because it shows an influence of AI bias on human decisions that extends beyond the stage and stimuli in which the AI recommendations are explicitly present. Thus, AI biases could have the potential to propagate through humanity. To our knowledge, this inheritance of the bias effect has not been previously revealed in any empirical research.
The presence of AI erroneous recommendations for the 40/60 samples resulted in the AI-assisted group misclassifying significantly more 40/60-samples than the group unassisted. In Experiments 2 and 3, we also analysed changes in participants' behaviour over the sequence of trials, and observed an increase in the number of 40/60 misclassifications throughout the AI-assisted phase. This result suggests that participants' monitoring of the information decreased and the tendency to rely on the AI recommendations to make their judgments increased as their experience with the AI increased. This could be due to fatigue but also to habituation and increased trust in the AI recommendations.
The "Negative" or "Positive" recommendation labels, in blue and orange colours, were probably more salient to participants' attention than the tissue samples that they had to classify, which were, in comparison, more complex. Thus, the tissue samples required effortful observation and processing to make a correct judgement about the presence (i.e., positive) or absence (i.e., negative) of the syndrome.
Recommendation labels provided by the AI could have interfered with the volunteers' assessment of tissue sample objective information 
(Howard et al., 2020)
. Volunteers may also have been reluctant to engage in a deep assessment of the reliability of each AI recommendation 
(Buçinca et al., 2021)
 and, instead, they probably developed general rules about whether or not to follow AI suggestions, what diminished their ability to detect and correct the biased advice 
(Lai & Tan, 2019)
.
During the unassisted phase of the task, we detected a reduction in the errors made by the AI-assisted group, but they still misclassified the 40/60 stimuli more than the unassisted group. This effect could have two possible interpretations: (a) a difficulty for participants in the AI-assisted group to regain control over the task because they had to change to slower and more conscious processing 
(Kahneman, 2003)
 when they no longer had AI support 
(Moulton et al., 2007)
 or (b) a training effect of the recommendations made by the AI on our participants' responses so that they learnt from the AI bias.
Regarding our first interpretation, if participants trusted AI recommendations to the detriment of analytic or effortful processing 
(Buçinca et al., 2021;
Rastogi et al., 2022)
 during Phase 1, then when they moved to Phase 2, and they needed to take complete control of the task, they would have needed some time to adjust to the new task demands and redirect attentional resources. It seems that, the transition from automated to controlled performance was slow, and participants were still heavily influenced by AI bias. The errors derived from this slow transition had no serious consequences in our experiments, but there are professional areas, such as healthcare,
where the consequences of decisions taken under the influence of an inherited bias could be fatal.
Concerning the second interpretation stated above, in Experiment 3, we did not observe a decrease in the errors made by the AI-assisted group in the transition from the AI-assisted to the unassisted phase. Maybe AI recommendations modelled participants' behaviours so that they learnt a new classification criterion based on the biased AI output. This could explain why the mean number of 40/60 misclassifications remained high in the context without the AI for the participants who had previously been supported by the biased AI in Experiment 3. If errors in the unassisted phase of the task were only due to difficulty in regaining control over the information processing, these errors should have been progressively reduced throughout successive trials of the task.
The results of our three experiments support a human over-reliance on the recommendations of AI systems 
(Adam et al., 2022;
Jacobs et al., 2021)
, and add the new finding of the inherited bias effect. Human trust in automation influences the tendency to over-accept algorithmic outcomes 
(Goddard et al., 2012
(Goddard et al., , 2014
 even when they are noticeably wrong. This means that humans are not only willing to rely on AI because they are "cognitive misers", that take mental shortcuts when making decisions, but also because they perceive artificial intelligence to be trustworthy 
(Araujo et al., 2020;
Kool & Botvinick, 2018)
. It has been suggested that trust could induce compliance with AI advice due to an authority effect 
(Baudel et al., 2020)
. In Experiments 1 and 2, we detected that participants who perceived the AI of the experiment as more helpful and accurate, and trusted more in the usefulness of artificial intelligence in healthcare in general, were those who followed the AI recommendations more often, and committed more errors in the classifications task as revealed by the positive and significative correlations observed between the mean number of 40/60 misclassifications in the task and the participants' answers to the post-experimental questions.
As a limitation to our present work, it could be argued that our experimental task was a simulation of clinical decision-making with a fictitious diagnostic process and a fictitious artificial intelligence system. Although our experiments simplify a potential real-world setting, we believe that our controlled experimental task can help to analyse which basic psychological processes mediate on human-AI collaboration. We created a classification task with a low level of uncertainty, in which participants were provided with a clear classification criterion to perform the task, in which the error in the recommendations was systematic, controlled, and noticeable by the control participants, and in which prior expertise had no influence. These characteristics make our experimental task a fruitful method to study human reliance on AI algorithms and the potential inheritance of their biases.
AI influenced participants' behaviour and increased the number of errors in a health framed task.


Table 1
1
Design summary of the three experiments
Experiment
Group
Classification Task
Phase 1
Phase 2
Experiment 1
Assisted
AI-assisted
-
Unassisted
Unassisted
-
Experiment 2
Assisted
AI-assisted
Unassisted
Unassisted
Unassisted
Unassisted
Experiment 3
Assisted-Unassisted
AI-assisted
Unassisted
Unassisted-Assisted
Unassisted
AI-assisted


















H
Adam






A
Balagopalan






E
Alsentzer






F
Christia






M
Ghassemi


















Mitigating the impact of biased artificial intelligence in emergency decisionmaking


10.1038/s43856-022-00214-4








Communications Medicine




2


1












Machine learning in clinical decision making




L
Adlung






Y
Cohen






U
Mor






E
Elinav




10.1016/j.medj.2021.04.006








2














The influence of algorithms on political and dating decisions




U
Agudo






H
Matute








PLoS ONE




16


















10.1371/journal.pone.0249454














In AI we trust? Perceptions about automated decision-making by artificial intelligence




T
Araujo






N
Helberger






S
Kruikemeier






C
H
De Vreese




10.1007/s00146-019-00931-w








AI & SOCIETY




35


3




















T
Baudel






M
Verbockhaven






G
Roy






V
Cousergue






R
Laarach


















Addressing Cognitive Biases in Augmented Business Decision Systems












The Impact of Cognitive Biases on Professionals' Decision-Making: A Review of Four Occupational Areas




V
Berthet








Frontiers in Psychology




12
















10.3389/fpsyg.2021.802439














Cognitive biases and heuristics in medical decision making: A critical review using a systematic search strategy




J
S
Blumenthal-Barby






H
Krieger








Medical Decision Making




35


4
















Human preferences toward algorithmic advice in a word association task




E
Bogert






N
Lauharatanahirun






A
Schecter








Scientific Reports




12


1


















10.1038/s41598-022-18638-2














To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decisionmaking




Z
Buçinca






M
B
Malaya






K
Z
Gajos








Proceedings of the ACM on Human-Computer Interaction




CSCW1


5
















10.1145/3449287














Gender shades: Intersectional accuracy disparities in commercial gender classification




J
Buolamwini






T
Gebru








Proceeding of Maniche Learning Research


eeding of Maniche Learning Research






81














Task-Dependent Algorithm Aversion




N
Castelo






M
W
Bos






D
R
Lehmann








Journal of Marketing Research




56


5


















10.1177/0022243719851788














Rising to the challenge of bias in health care AI




M
K
Cho




10.1038/s41591-021-01577-2








Nature Medicine




27


12
















The Evolution and Impact of Human Confidence in Artificial Intelligence and in Themselves on AI-Assisted Decision-Making in Design




L
Chong






A
Raina






K
Goucher-Lambert






K
Kotovsky






J
Cagan




10.1115/1.4055123








Journal of Mechanical Design




145


3








Transactions of the ASME








Artificial Intelligence Can't Be Charmed: The Effects of Impartiality on Laypeople's Algorithmic Preferences




M
C
Claudy






K
Aquino






M
Graso








Frontiers in Psychology




13


















10.3389/fpsyg.2022.898027














The challenge of cognitive science for medical diagnosis




P
Croskerry






S
G
Campbell






D
A
Petrie




10.1186/s41235-022-00460-z








Cognitive Research: Principles and Implications






8


13












Algorithmic Bias in Autonomous Systems




D
Danks






A
J
London




10.24963/ijcai.2017/654








Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence


the Twenty-Sixth International Joint Conference on Artificial Intelligence






0














Latent bias and the implementation of artificial intelligence in medicine




M
Decamp






C
Lindvall




10.1093/jamia/ocaa094








Journal of the American Medical Informatics Association




27


12




















T
Dratsch






X
Chen






M
Rezazade Mehrizi






R
Kloeckner






A
Mähringer-Kunz






M
Püsken






B
Baeßler






S
Sauer






D
Maintz






D
Pinto Dos Santos


















Automation Bias in Mammography: The Impact of Artificial Intelligence BI-RADS Suggestions on Reader Performance


10.1148/radiol.222176














A guide to deep learning in healthcare




A
Esteva






A
Robicquet






B
Ramsundar






V
Kuleshov






M
Depristo






K
Chou






C
Cui






G
Corrado






S
Thrun






J
Dean




10.1038/s41591-018-0316-z








Nature Medicine




25


1
















Proposal for a Regulation laying down harmonised rules on artificial intelligence (Articifical intelligence act) and amending certain union legislative acts. COM (2021) 206 final




European Commision




















Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health




R
R
Fletcher






A
Nakeshimana






O
Olubeko








Frontiers in Artificial Intelligence




3


















10.3389/frai.2020.561802














Hello World: Being Human in the Age of Algorithms




H
Fry








W. W. Norton & Company












Artificial intelligence to support clinical decision-making processes




C
Garcia-Vidal






G
Sanjuan






P
Puerta-Alcalde






E
Moreno-García






A
Soriano






















Ebiomedicine




10.1016/j.ebiom.2019.07.019






46














Do as AI say: susceptibility in deployment of clinical decision-aids




S
Gaube






H
Suresh






M
Raue






A
Merritt






S
J
Berkowitz






E
Lermer






J
F
Coughlin






J
V
Guttag






E
Colak






M
Ghassemi








Npj Digital Medicine




1


4
















10.1038/s41746-021-00385-9














Automation bias: A systematic review of frequency, effect mediators, and mitigators




K
Goddard






A
Roudsari






J
C
Wyatt




10.1136/amiajnl-2011-000089








Journal of the American Medical Informatics Association




19


1
















Automation bias: Empirical results assessing influencing factors




K
Goddard






A
Roudsari






J
C
Wyatt




10.1016/j.ijmedinf.2014.01.001








International Journal of Medical Informatics




83


5
















The principles and limits of algorithm-in-the-loop decision making




B
Green






Y
Chen




10.1145/3359152








Proceedings of the ACM on Human-Computer Interaction




3






CSCW












Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs




V
Gulshan






L
Peng






M
Coram






M
C
Stumpe






D
Wu






A
Narayanaswamy






S
Venugopalan






K
Widner






T
Madams






J
Cuadros






R
Kim






R
Raman






P
C
Nelson






J
L
Mega






D
R
Webster




10.1001/jama.2016.17216








JAMA -Journal of the American Medical Association




316


22
















Preference for human or algorithmic forecasting advice does not predict if and how it is used




M
Himmelstein






D
V
Budescu




10.1002/bdm.2285








Journal of Behavioral Decision Making




36


1
















Deep learning-a technology with the potential to transform health care




G
Hinton








JAMA -Journal of the American Medical Association




320


11


















10.1001/jama.2018.11100














Should Electronic Health Record-Derived Social and Behavioral Data Be Used in Precision Medicine Research?




B
Hollister






V
L
Bonham




10.1001/amajethics.2018.873








AMA Journal of Ethics




20


9
















Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making




J
J
Howard






L
R
Rabbitt






Y
B
Sirotin
























PLoS ONE




15


















10.1371/journal.pone.0237855














How machine-learning recommendations influence clinician treatment selections: the example of the antidepressant selection




M
Jacobs






M
F
Pradier






T
H
Mccoy






R
H
Perlis






F
Doshi-Velez






K
Z
Gajos




10.1038/s41398-021-01224-x








Translational Psychiatry




1


11














A Perspective on Judgment and Choice: Mapping Bounded Rationality




D
Kahneman




10.1037/0003-066X.58.9.697








American Psychologist




58


9
















Noise: A Flaw in Humam Judgment




D
Kahneman






O
Sibony






C
R
Sunstein








Suparyanto dan Rosad




William Collins




5












Mental labour




W
Kool






M
Botvinick




10.1038/s41562-018-0401-9








Nature Human Behaviour




2


12
















Check the box! How to deal with automation bias in AI-based personnel selection




C
Kupfer






R
Prassl






J
Fleiß






C
Malin






S
Thalmann






B
Kubicek








Frontiers in Psychology




14


















10.3389/fpsyg.2023.1118723














Should I Trust the Artificial Intelligence to Recruit? Recruiters' Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening




A
Lacroux






C
Martin-Lacroux




10.3389/fpsyg.2022.895997








Frontiers in Psychology




13














On human predictions with explanations and predictions of machine learning models: A case study on deception detection




V
Lai






C
Tan




10.1145/3287560.3287590








FAT* 2019 -Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency


















Human-AI Collaboration in Healthcare : A Review and Research Agenda




Y
Lai






A
Kankanhalli






D
C
Ong








Proceedings of the 54th Hawaii International Conference on System Sciences


the 54th Hawaii International Conference on System Sciences


















Gender imbalance in medical imaging datasets produces biased classifiers for computeraided diagnosis




A
J
Larrazabal






N
Nieto






V
Peterson






D
H
Milone






E
Ferrante




10.1073/pnas.1919012117








Proceedings of the National Academy of Sciences




117


23
















Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management




M
K
Lee




10.1177/2053951718756684








Big Data & Society




5


1


205395171875668














Time of day and the decision to prescribe antibiotics




J
A
Linder






J
N
Doctor






M
W
Friedberg






H
Reyes Nieva






C
Birks






D
Meeker






C
R
Fox








JAMA Internal Medicine




174


12


















10.1001/jamainternmed.2014.5225














Artificial Intelligence and Surgical Decision-making




T
J
Loftus






P
J
Tighe






A
C
Filiberto






P
A
Efron






S
C
Brakenridge






A
M
Mohr






P
Rashidi






G
R
Upchurch






A
Bihorac








JAMA Surgery




155


2


















10.1001/jamasurg.2019.4917














Algorithm appreciation: People prefer algorithmic to human judgment




J
M
Logg






J
A
Minson






D
A
Moore








Organizational Behavior and Human Decision Processes




















10.1016/j.obhdp.2018.12.005














How machine learning is embedded to support clinician decision making: An analysis of FDA-approved medical devices




D
Lyell






E
Coiera






J
Chen






P
Shah






F
Magrabi








BMJ Health and Care Informatics




1


28
















10.1136/bmjhci-2020-100301














The Effect of Cognitive Load and Task Complexity on Automation Bias in Electronic Prescribing




D
Lyell






F
Magrabi






E
Coiera




10.1177/0018720818781224








Human Factors




60


7
















Automation bias in electronic prescribing




D
Lyell






F
Magrabi






M
Z
Raban






L
G
Pont






M
T
Baysari






R
O
Day






E
Coiera




10.1186/s12911-017-0425-5








BMC Medical Informatics and Decision Making




17


1
















A Survey on Bias and Fairness in Machine Learning




N
Mehrabi






F
Morstatter






N
Saxena






K
Lerman






A
Galstyan








ACM Computing Surveys




6


54
















10.1145/3457607














Slowing down when you should: A new model of expert judgment




C
A E
Moulton






G
Regehr






M
Mylopoulos






H
M
Macrae




10.1097/ACM.0b013e3181405a76








Academic Medicine




10
















Association of Primary Care Clinic Appointment Time With Opioid Prescribing




H
T
Neprash






M
L
Barnett




10.1001/jamanetworkopen.2019.10373








JAMA Network Open




2


8














Addressing bias in big data and AI for health care: A call for open science




N
Norori






Q
Hu






F
M
Aellen






F
D
Faraci






A
Tzovara








Patterns




2


10
















10.1016/j.patter.2021.100347














Weapons of Math Desctruction




C
O'neil








Crown Publishers












Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People




Z
Obermeyer






S
Mullainathan






















10.1145/3287560.3287593














Addressing Bias in Artificial Intelligence in Health Care




R
B
Parikh






S
Teeple






A
S
Navathe




10.1001/jama.2019.18058








JAMA




322


24














AI in health and medicine




P
Rajpurkar






E
Chen






O
Banerjee






E
J
Topol




10.1038/s41591-021-01614-0








Nature Medicine




28


1
















Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making




C
Rastogi






Y
Zhang






D
Wei






K
R
Varshney






A
Dhurandhar






R
Tomsett




10.1145/3512930








Proceedings of the ACM on Human-Computer Interaction




6


CSCW1
















People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors




F
G
Rebitschek






G
Gigerenzer






G
G
Wagner




10.1038/s41598-021-99802-y








Scientific Reports




11


1
















Experimental evidence of effective human-AI collaboration in medical decision-making




C
Reverberi






T
Rigon






A
Solari






C
Hassan






P
Cherubini






G
Antonelli






H
Awadie






S
Bernhofer






S
Carballal






M
Dinis-Ribeiro






A
Fernández-Clotett






G
F
Esparrach






I
Gralnek






Y
Higasa






T
Hirabayashi






T
Hirai






M
Iwatate






M
Kawano






M
Mader






A
Cherubini




10.1038/s41598-022-18751-2








Scientific Reports




12


1


14952














Cognitive biases associated with medical decisions: a systematic review




G
Saposnik






D
Redelmeier






C
C
Ruff






P
N
Tobler




10.1186/s12911-016-0377-1








BMC Medical Informatics and Decision Making




16


1
















Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations




L
Seyyed-Kalantari






H
Zhang






M
B A
Mcdermott






I
Y
Chen






M
Ghassemi




10.1038/s41591-021-01595-0








Nature Medicine




27


12
















Clinical Decision Support in the Era of Artificial Intelligence




E
H
Shortliffe






M
J
Sepúlveda




10.1001/jama.2018.17163








JAMA -Journal of the American Medical Association




320


21
















Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias




D
Solans






A
Beretta






M
Portela






C
Castillo






A
Monreale








Proceedings of ACM Conference (Conference'17)


ACM Conference (Conference'17)




Association for Computing Machinery




1












Machine heuristic: When we trust computers more than humans with our personal information




S
S
Sundar






J
Kim




10.1145/3290605.3300768








Conference on Human Factors in Computing Systems -Proceedings, 1-9
















A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. Equity and Access in Algorithms




H
Suresh






J
Guttag








Mechanisms, and Optimization




1


1


















10.1145/3465416.3483305














Misplaced Trust: Measuring the Interference of Machine Learning in Human Decision-Making




H
Suresh






N
Lao






I
Liccardi








WebSci 2020 -Proceedings of the 12th ACM Conference on Web Science




















10.1145/3394231.3397922


















R
T
Sutton






D
Pincock






D
C
Baumgart






D
C
Sadowski






R
N
Fedorak














An overview of clinical decision support systems: benefits, risks, and strategies for success




K
I
Kroeker




10.1038/s41746-020-0221-y








Npj Digital Medicine




3


1














High-performance medicine: the convergence of human and artificial intelligence




E
J
Topol








Nature Medicine




25


1


















10.1038/s41591-018-0300-7














Human-computer collaboration for skin cancer recognition




P
Tschandl






C
Rinner






Z
Apalla






G
Argenziano






N
Codella






A
Halpern






M
Janda






A
Lallas






C
Longo






J
Malvehy






J
Paoli






S
Puig






C
Rosendahl






H
P
Soyer






I
Zalaudek






H
Kittler








Nature Medicine




26


8


















10.1038/s41591-020-0942-0














Judging the algorithm: A case study on the risk assessment tool for gender-based violence




A
Valdivia






C
Hyde-Vaamonde






J
García-Marcos












implemented in the Basque country








Do no harm: a roadmap for responsible machine learning for health care




J
Wiens






S
Saria






M
Sendak






M
Ghassemi






V
X
Liu






F
Doshi-Velez






K
Jung






K
Heller






D
Kale






M
Saeed






P
N
Ossorio






S
Thadaney-Israni






A
Goldenberg




10.1038/s41591-019-0548-6








Nature Medicine




25


9
















Artificial intelligence in healthcare




K
H
Yu






A
L
Beam






I
S
Kohane




10.1038/s41551-018-0305-z








Nature Biomedical Engineering




2


10
















Framing the challenges of artificial intelligence in medicine




K
H
Yu






I
S
Kohane




10.1136/bmjqs-2018-008551








BMJ Quality and Safety




28


3

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]