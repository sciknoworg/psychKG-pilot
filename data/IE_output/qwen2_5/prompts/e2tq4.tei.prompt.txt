You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Keywords: belief bias; individual differences; diffusion model; cognitive abilities; reasoning A diffusion-model analysis of belief bias: Different cognitive mechanisms explain how algorithmic and reflective thinking contribute to conflict resolution in reasoning Prior beliefs about the world shape our perception of new information. Often, these prior beliefs allow us to make fast decisions based on previous experiences and world knowledge without having to resort to effortful processing, enabling adaptive behavior in a wide range of standard situations. However, prior beliefs may bias our reasoning, resulting in suboptimal decision making in certain situations. Nearly a century's worth of research on the phenomenon of belief bias has shown that people are more likely to ignore the logical validity of a conclusion if it is in conflict with their prior beliefs 
(Evans et al., 1983
). However, not everyone is equally likely to be afflicted by task-irrelevant prior beliefs: Individuals with high cognitive abilities and a disposition to engage in critical thinking are less prone to fall for belief-based reasoning than others 
(De Neys, 2006;
Stanovich et al., 2016)
.
The traditional view is that correct responses to problems with a conflict between ruleand belief-based reasoning rely on slow, effortful processing, as opposed to fast and autonomous processing 
(Evans, 2008)
. More formally, dual-process model accounts of belief bias distinguish between undemanding, belief-based Type 1 processes and cognitively demanding, rule-based Type 2 processes. It is proposed that Type 1 processing endorses/opposes a conclusion if it is consistent/inconsistent with prior beliefs regardless of its logical validity. If this default response conflicts with a logic-based response, and if that conflict is detected, Type 2 processes may be used to inhibit and replace the initial response.
Individual differences in working memory capacity and the disposition to engage in critical thinking have been shown to predict performance in reasoning tasks with a conflict between rule-and belief-based processing, which has been interpreted as evidence for the successful engagement of deliberate Type 2 processing 
(De Neys, 2006;
Stanovich et al., 2016)
.
Cognitive abilities in general and working memory capacity in particular have been suggested to facilitate the inhibition of premature responses resulting from Type 1 processing 
(Stanovich, 2009
(Stanovich, , 2010
. Moreover, they have been suggested to be helpful in creating a decoupled representation of the problem, which can then be used to generate an alternative, rulebased response 
(Stanovich, 2009
(Stanovich, , 2010
. In addition, the disposition and ability to engage in critical thinking has been suggested to facilitate the initiation of decoupled reasoning and the successful override of Type 1 responses 
(Stanovich & West, 2000;
Toplak et al., 2011)
. According to Stanovich's theory of rational thinking 
(Stanovich, 2009
(Stanovich, , 2010
, cognitive abilities (the algorithmic mind) and critical thinking dispositions (the reflective mind) contribute together to the successful override of incorrect responses generated by belief-based Type 1 reasoning (the autonomous mind).


Challenges to dual-process theories of reasoning
Although there is substantial evidence supporting the key assumptions of dual-process theories (for a review see 
Ball et al., 2018)
, several recent findings have challenged the assumption that intuitive Type 1 processes generate an initial belief-based response that may later be inhibited and overridden by deliberate Type 2 processes.
First, belief-based reasoning may not be as fast and autonomous as commonly presumed. Indeed, shorter problem presentation times do not increase belief bias 
(Evans et al., 2009)
, and granting individuals more time may even increase their propensity to draw beliefbased conclusions 
(Newman et al., 2017)
. Moreover, when asked to evaluate the believability of a conclusion instead of its validity, participants performed worse when the two sources of information were in conflict 
(Handley et al., 2011)
, which cannot be explained by sequential dual-process theories, as relatively slow deliberate processes should not interfere with fast and autonomous belief-based reasoning. These results question the notion of exclusively fast and autonomous Type 1 processes. In addition, performing a concurrent demanding task has been shown to not only impair performance when participants had to evaluate the logical validity of conclusions, but also when they had to evaluate their believability 
(Howarth et al., 2016)
, suggesting that both judgments rely on working memory resources.
Furthermore, recent findings challenge the assumption that logic-based reasoning is slow and requires effortful processing. Individuals sometimes seem to detect the conflict between logic and believability even when defaulting to the belief-based response, suggesting that people may have logical intuitions 
(De Neys, 2012
. Moreover, individuals have been shown to be able to evaluate the validity of inferences containing abstract non-words even under challenging time constraints 
(Newman et al., 2017)
.
These findings are hard to reconcile with core ideas of traditional dual-process models.
Instead, they suggest that rule-and belief-related problem features are processed in parallel and that reasoning problems that elicit a conflict between rule-and belief-based thinking may also elicit more than one Type 1 response 
(De Neys, 2012
Handley & Trippas, 2015;
Pennycook et al., 2015)
. Problem features such as feature complexity or fluency may then determine whether participants give a rule-or belief-based default response 
(Handley & Trippas, 2015;
Pennycook et al., 2015;
Trippas et al., 2017)
. Nevertheless, a divergence between ruleand belief-based processing will always cause mutual interference, resulting in lower accuracies and higher reaction times 
(Trippas et al., 2017)
.


Implications for individual differences research
These challenges to classic dual-process theories have far-reaching implications for the interpretation of results from individual differences research on belief bias. If belief-and rulebased problem features are processed in parallel, rule-based responses by individuals with certain personality traits and thinking dispositions may reflect that these individuals were either more likely to give a rule-based default response or that they successfully inhibited a beliefbased default response and responded with a rule-based deliberate response.
In a recent study, 
Thompson et al. (2018)
 found that, in conflict trials (where the logical validity of a conclusion was incongruent with its believability) , individuals with higher cognitive abilities performed worse when asked to assess the believability of a conclusion than when asked to assess the logical validity. Conversely, individuals with lower cognitive abilities were better at evaluating the believability of a conclusion than its validity. These results suggest that default responses may differ between high and low ability reasoners, and that at least part of the association between cognitive abilities and rule-based reasoning can be attributed to individual differences in the availability of belief-and rule-based default responses. This notion is further supported by previous research showing that more intelligent individuals not only engage in more Type 2-processing, but are also more likely to give rulebased responses as a first, intuitive answer, as compared to less intelligent individuals 
(Thompson & Johnson, 2014)
. Together, these results suggest that individuals with higher cognitive abilities are less afflicted by the belief bias because they are more likely to generate a rule-based default response and are also better at inhibiting and overriding belief-based default responses than individuals with lower cognitive abilities.
Despite these compelling results, which challenge core predictions of sequential dualprocess theories of reasoning, some open questions remain. A detailed inspection of the results by 
Thompson et al. (2018)
 reveals that only participants falling into the first quartile of the ability distribution showed impairments in their reasoning performance when asked to evaluate conclusions with a conflict between belief-and rule-based responses according to their validity than when they were asked to evaluate these conclusions according to their believability. In comparison, all other cognitive ability groups (i.e., both moderately and highly able participants) were better at assessing the validity of a problem than its believability when there was a conflict between the two sources of information. This suggests that not only individuals with very high cognitive abilities, but almost everyone was better at assessing a problem's logical validity than they were at assessing its believability. An implication of this result would be that the rule-based response was the near-universal dominant default response for everyone except those individuals with very low cognitive abilities. This conclusion is seemingly at odds with the robust phenomenon of belief bias and the deterioration of performance in conflict trials in comparison to trials without a conflict between rule-and belief-based responding 
(Evans et al., 1983)
.
However, this resultthat only the lowest ability group differed from the rest of the sample in their response behaviormay be an artifact of sample characteristics and the cognitive ability tests employed by 
Thompson et al. (2018)
. Because their sample consisted of university students who are likely to show above-average cognitive abilities due to selection processes, the intelligence test used in their study (the 
Shipley-2;
Shipley et al., 2009)
 may not have been difficult enough to differentiate across their sample's whole range of cognitive abilities. In addition, they used a numeracy test that has been shown to differentiate in highly educated samples but that has not been related to standard intelligence tests 
(Lipkus et al., 2016)
.
Therefore, it would be important to demonstrate that these results replicate with an intelligence test that is appropriate for highly educated samples and that shows a high loading on general intelligence (g). In the present paper, we therefore included Raven's Advances Progressive Matrices (APM; 
Raven et al., 1994)
 as an intelligence test, which is suited for measuring intelligence in gifted adolescents and adults and has been shown to have a very high gloading 
(Gignac, 2015)
.
Moreover, the present research tests whether the same kind of moderation is found for any kind of cognitive ability measure, or whether different interaction patterns emerge for different cognitive abilities and thinking styles (e.g., fluid intelligence , working memory capacity, critical thinking disposition and ability). Justified by results from a principal component analysis, 
Thompson et al. (2018)
 collapsed all individual differences variables (intelligence, numeracy, reflective thinking, and actively open-minded thinking) into a single cognitive ability variable. This approach leads to an increase in the reliability of the aggregate cognitive ability variable, but comes with a loss of granularity. An inspection of component loadings shows considerable heterogeneity in component loadings with the first principal component being dominated by the intelligence test. This suggests that interactions between cognitive ability variables and performance in the belief bias paradigm may differ substantially across different measures of cognitive ability. In particular, reasoners with high cognitive abilities (i.e., high fluid intelligence and working memory capacity) may be able to quickly create a rule-based default response after reading a problem because they are better at creating higherlevel relational structures that temporarily bind elemental perceptual or memorial representations and that can be effectively abstracted from intrinsic features of low-level representations 
(Chuderski, 2014;
Oberauer et al., 2008)
. Hence, these reasoners might immediately generate a rule-based response even when asked to evaluate the believability of a problem, due to their fast and efficient processing of higher-order relational structures, subsequently resulting in substantial interference from rule-based Type 1 processes. In comparison, individuals with a disposition to engage in reflective thinking but low cognitive abilities may not generate a dominant rule-based default response as quickly and may therefore experience less interference from rule-based Type 1 processes when asked to assess the believability of a problem. In order to test whether individuals with high cognitive abilities and individuals with a disposition to engage in reflective thinking differ, the moderating effects of both measures on ruleand belief-based responding need to be considered separately. In the present paper, we therefore measured several indicators of cognitive abilities and thinking styles to comprehensively investigate how individual differences in both traits moderate rule-and belief-based reasoning in a transitive reasoning task.


Performance measures in transitive reasoning tasks
Transitive reasoning is a form of inferential reasoning that subsumes relational inferences made from relations between objects, individuals, or entities 
(Piaget, 1928;
Sternberg, 1980b
). If A > B and B > C, it can be inferred that A > C. It plays an important role in normal development and has been demonstrated to underlie many socio-cognitive processes including mathematical reasoning, text processing, trusting others, and developing relationships 
(Wright, 2001;
Wright & Smailes, 2015)
. Successful transitive reasoning in tasks comparable to the ones typically used in research on belief bias requires verbal and spatial in addition to operational reasoning abilities 
(Piaget et al., 1960;
Sternberg, 1980a)
.
One limitation of previous research on the belief bias is that most studies only considered accuracy as the performance measure in reasoning tasks while largely ignoring response times despite their theoretical importance in distinguishing between fast intuitive and slow deliberate processing (e.g., 
Evans et al., 2009;
Thompson et al., 2018)
. The studies that did assess response times typically found that they were longer in trials with a conflict between rule-and belief-based responding than in no-conflict trials, although evidence for effects of increased processing demands and its moderation by task instructions was less compelling and more inconsistent for response times than for accuracy 
(Handley et al., 2011;
Thompson et al., 2011;
Trippas et al., 2017)
. At the same time, several studies that used varying presentation times and response deadlines have contributed to theory development by challenging predictions from classical dual process accounts (e.g., 
Evans et al., 2009;
Newman et al., 2017)
.
Hence, research on the belief bias would likely benefit from simultaneously considering responses and response times in order to assess how much processing time is needed for beliefbased and rule-based responding.
One reason why previous research has found rather inconsistent results for response time measures may lie in speed-accuracy tradeoffs that can lead to substantial shifts in response time distributions. Even when instructed to respond as quickly and accurately as possible, individuals still differ in the degree to which they put relative emphasis on speed and accuracy in experimental tasks 
(Heitz, 2014)
. These shifts are not easily detectable by visual inspection or traditional methods of statistical inference, because substantial shifts in response time (e.g., an increase of 200 ms) are often accompanied by very subtle changes in accuracy (e.g., an increase of only 3 percent), which can lead to the erroneous conclusion that there are no speed-accuracy tradeoffs 
(Forstmann et al., 2011;
Pew, 1969)
. This concern is particularly relevant in individual differences research, where individuals with higher abilities may strategically adjust their decision criteria differently than individuals with lower abilities to adapt to task demands 
(Draheim et al., 2016
(Draheim et al., , 2019
Schmiedek et al., 2007)
. Therefore, it is advisable to explicitly consider speed-accuracy tradeoffs by either using measures that integrate response times and accuracies 
(Hughes et al., 2014;
Liesefeld & Janczyk, 2019;
Vandierendonck, 2017)
, or by accounting for them by using mathematical models of decision making 
(Brown & Heathcote, 2008;
Forstmann et al., 2011;
Ratcliff, 1978
).
The use of mathematical models of decision making has further benefits, as it transforms observed performance measures into parameters of process models. Non-overlapping interactions between two experimental factors can be removed through nonlinear transformations of the measurement scale 
(Bamber, 1979;
Loftus, 1978)
, severely limiting the interpretability of these interactions. Therefore, it is crucial to make informed decisions about the measurement scales of cognitive processes, as conclusions may not generalize beyond the specific scales used in data analyses. Because mathematical process models allow to specify transformations between a specific dependent variable and a latent process of interest, using such process models permits to interpret non-overlapping interactions in terms of the modeled latent psychological constructs 
(Wagenmakers et al., 2012)
.
We therefore analyzed participants' performance by means of diffusion modeling. The diffusion model is a mathematical model of decision making that uses the full distribution of responses and response times to estimate latent model parameters describing the decision process 
(Ratcliff, 1978;
Voss et al., 2013)
. It has never before, to the best of our knowledge, been used to study belief bias, because typically sparse trial numbers in reasoning tasks complicate model fitting 
(Trippas et al., 2018
). The diffusion model assumes that in two alternative THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 11 forced-choice problems, evidence is continuously accumulated in a random walk evidence accumulation process until one of two decision thresholds is reached. This evidence accumulation consists of a systematic component, the drift v, and normally distributed random noise s².
The path of the diffusion process varies from trial to trial due to random noise, resulting in different process durations and outcomes reflected in response time distributions for the two decision alternatives. Under the assumption that individuals do not show an a-priori bias for any of the two decision alternatives, the two decision thresholds can be recoded to reflect correct and incorrect responses, which allows to interpret the drift rate v as an ability parameter 
(Frischkorn & Schubert, 2018;
Schubert et al., 2019;
van der Maas et al., 2011;
Voss et al., 2013
Voss et al., , 2015
.


Figure 1 Sample diffusion processes
Note. This figure depicts sample diffusion processes when belief-and rule-based information converge (non-conflict processing) and when belief-and rule-based information diverge (conflict processing) given an invariant boundary separation a across both conditions.
Most importantly, the model distinguishes between the speed and efficiency of information processing, which are reflected in the strength and direction of the drift rate v, and decision criteria, which are reflected in the threshold separation parameter a that quantifies how much evidence needs to be accumulated before making a decision. Together, these two parameters predict the speed and accuracy of decision making, with the drift rate parameter indicating how much evidence is accumulated per unit of time, and the threshold separation pa-rameter indicating how much evidence needs to be accumulated before the evidence accumulation process is terminated. As illustrated in 
Figure 1
, when the processing of rule-and belief-based problem features converges, drift rates should be large (predicting high accuracies and fast responses), whereas when the processing of rule-and belief-based problem features diverges, drift rates should be small (predicting low accuracies and slow responses). In addition, individuals may adjust their decision criteria and hence their decision thresholds when evaluating problems with a conflict between rule-and belief-based processing or when asked to judge the believability of a problem instead of its logical validity. In the present paper, we therefore used drift rates as an integrative performance measure of both responses and response times that is not affected by speed-accuracy tradeoffs to investigate how individual differences in cognitive abilities and thinking styles moderate rule-and belief-based reasoning.
It has previously been suggested that the diffusion model should only be applied to experimental tasks with mean reaction times not exceeding 1.5 seconds , because the underlying model assumptions of single-stage decision making and parameter constancy over time are more likely to be violated in more complex decision tasks. This is relevant for the present paper, because decision making and responding in belief bias tasks may exceed this threshold of 1.5 seconds. Recent work has shown, however, that the diffusion model can be successfully applied to cognitive tasks with reaction times exceeding 1.5 seconds and is quite robust against violations of model assumptions 
(Lerche et al., 2020;
Lerche & Voss, 2016
Ratcliff, 2002;
van Ravenzwaaij et al., 2017)
. 
Lerche and Voss (2019)
 argued that the often-repeated 1.5 seconds rule is based on an arbitrary value and that while the assumptions of single-stage processing and parameter constancy are more likely to be violated in slower response tasks, multi-stage evidence accumulation can be accounted for by a chain of diffusion processes that can be approximated by a single diffusion process. They tested the extensibility of the diffusion model to slower RT tasks in three experimental validation studies and found that the results of selective manipulations of model parameters were similar to findings from experimental validation studies in fast RT tasks, which allowed them to conclude that the model can be extended to slower RT tasks. This conclusion is further supported by simulation studies, which demonstrated that the diffusion model still provides a good account of the data if it is specified in a more parsimonious way than the data-generating model 
(Lerche & Voss, 2016;
Ratcliff, 2002;
van Ravenzwaaij et al., 2017)
. Taken together, these results from simulation and validation studies and the successful application of the diffusion model in studies with more complex decision tasks (e.g., 
Aschenbrenner et al., 2016;
Lerche et al., 2020;
von Krause et al., 2020)
 suggest that the model can be applied to describe the conflict between rule-and belief-based reasoning even if reaction times are relatively slow.
The diffusion model is widely popular in individual differences research and has been successfully applied to shed light on individual differences in reasoning 
(Lerche et al., 2020;
Schmiedek et al., 2007;
Schmitz & Wilhelm, 2016;
Schubert et al., 2015
Schubert et al., , 2019
, affective processing 
(Vandekerckhove, 2014)
, and cognitive aging 
(Ratcliff et al., 2010
(Ratcliff et al., , 2011
Schulz-Zhecheva et al., 2016;
von Krause et al., 2020)
. In particular, several studies have demonstrated that more intelligent individuals show advantages in the speed of evidence accumulation 
(Lerche et al., 2020;
Schmiedek et al., 2007;
Schmitz & Wilhelm, 2016;
Schubert et al., 2015
Schubert et al., , 2019
, whereas age-related differences in cognitive abilities can be mostly attributed to shifts in decision criteria and the slowing of non-decisional processes 
(Ratcliff et al., 2010
(Ratcliff et al., , 2011
Schubert et al., 2020;
Schulz-Zhecheva et al., 2016;
von Krause et al., 2020)
.
Moreover, psychometric studies have identified which conditions need to be met to allow the reliable assessment of trait-like individual differences in diffusion model parameters 
(Lerche & Voss, 2017;
Schubert et al., 2016)
. Taken together, there is a great potential for utilizing diffusion models to shed light on individual differences in reasoning. The present paper builds upon these previous studies to assess which process parameters of rule-and belief-based reasoning are moderated by individual differences in cognitive abilities and thinking styles.


Study 1
The aim of Study 1 was to demonstrate that the diffusion model can account for the conflict between rule-and belief-based processing and to explore which model parameters relate to individual differences in cognitive abilities and thinking styles. We expected to observe smaller drift rates when the processing of rule-and belief-based problem features diverges, as this pattern of results would reflect a competition between the processing of rule-based and belief-based information as suggested by the parallel dual-process model. Moreover, we expected individuals with greater cognitive abilities (in this first study operationalized in terms of working memory capacity) to show higher drift rates in trials with and without a conflict between rule-and belief-based processing than individuals with lower cognitive abilities, because their greater general efficiency of information-processing should result in faster and more accurate responses regardless of the presence or absence of a conflict between rule-and belief-based problem features. In addition, we expected individuals with greater reflective thinking ability (measured by the cognitive reflection test; 
Frederick, 2005)
 and greater disposition to engage in intellectual activity (measured by the need for cognition scale; 
Cacioppo & Petty, 1982;
Cohen et al., 1955)
 to show higher drift rates specifically in trials with a conflict between rule-and belief-based problem features than individuals with lower reflective thinking ability and lower need for cognition, due to their greater ability to disregard belief-based response tendencies that are compelling but incorrect.


Materials and Methods


Participants
We recruited N = 41 participants (33 females, 8 males) between 18 and 65 years (M = 26.7, SD = 9.0) with an academic background from the participant pool from the Heidelberg Psychology Department.


Materials


Relational reasoning problems. Participants completed 192 problems (adapted from
Banks & Hope, 2014). Each problem consisted of three premises presented sequentially for 3s each, followed by a conclusion that was either believable or unbelievable. The conclusion was shown for 2s with the final word omitted, which was subsequently presented until participants gave their response by key-press (see 
Figure 2
 for details on the task procedure). Following Banks and Hope (2014), we had participants respond to a screen that showed only the final word of the conclusion to ensure that their response times reflected only the speed of evidence accumulation in working memory, but not their reading speed or their willingness to read the premises for a second time. Although this feature of the experimental procedure placed demands on participants' working memory, it did so in a similar way for problems with and without a conflict between rule-and belief-based processing.


Figure 2
Procedure of the relational reasoning task
The validity of the conclusions was manipulated by changing the premises, whereas the believability of the conclusion was manipulated by reversing the elements in the conclusion. 50 % of the trials contained problems with a conflict between rule-and belief-based information, whereas the other 50 % of trials were no-conflict problems (see examples in 
Table 1
).
Participants completed 12 practice trials with feedback before the start of each of the two experimental blocks. Both mean RTs and accuracies in this task showed acceptable reliabilities, as estimated by Spearman-Brown corrected correlations based on odd-even splits ranging from r = .83 to r = .95.


Table 1
Sample items from 
Banks and Hope (2014)
 


for trials without and with a conflict between ruleand belief-based information
Example for a relational reasoning task item without a conflict between rule-and belief-based information (non-conflict)
Example for a relational reasoning task item with a conflict between rule-and belief-based information (conflict) premises cars are faster than skateboards delps are slower than cars delps are faster than bicycles bicycles are faster than skateboards delps are slower than bicycles delps are faster than cars conclusion cars are faster than bicycles bicycles are faster than cars
Covariates. We measured working memory capacity (WMC) with the operation span task 
(Turner & Engle, 1989)
, following the recommendations for partial scoring by 
Conway et al. (2005)
. On average, participants recalled 85 % (SD = 11 %) of the items correctly. In addition, we assessed participants' cognitive and reflective abilities with the CRT 1 
(Frederick, 2005)
, which included six additional problems (Shane Frederick, personal correspondence, 02
February 2016). On average, participants solved 4.10 (SD = 2.63) problems correctly. We assessed participants' disposition to engage in critical thinking with the need for cognition (NFC) scale of the Rational-Experiential Inventory (REI; 
Epstein et al., 1996;
Keller et al., 2000)
. Participants' mean NFC score was 5.01 (SD = 0.68). Participants also completed the 
1
 The current debate regarding the CRT reflects uncertainty concerning its validity: While some argue that the CRT measures individual differences in reflective thinking ability 
(Frederick, 2005)
, others argue that it largely measures cognitive and numerical abilities 
(Blacksmith et al., 2019;
Erceg et al., 2020)
. Undoubtedly, however, performance in the CRT predicts susceptibility to biases and logical fallacies (e.g., 
Bialek & Pennycook, 2018;
Pennycook & Ross, 2016)
, making it an important measure to consider in any study on individual differences in bias susceptibility.
faith in intuition scale of the REI, which is not reported in the present study. All measures showed moderate-to-good internal consistencies, ranging from α = .78 for the CRT score over α = .80 for the NFC scale to α = .87 for the operation span score.


Procedure
We administered the relational reasoning task followed by the operation span task, the CRT, and the REI. Each session took approximately 1.5 hours. While participants completed the relational reasoning task, their EEG was recorded (data not reported here). Participants sat in a dimly-lit, sound-attenuated cabin and were instructed to avoid large movements (e.g., stretching, drinking) except during breaks or inter-trial-intervals.


Data analysis
RTs faster than 100 ms or slower than 5000 ms, or with logarithmized RTs exceeding ± 3 SDs of the mean of each condition were discarded.
A model comparison approach was used to identify which diffusion model parameters differed between experimental conditions in the relational reasoning task. For this purpose, we fit four models to the data: In model a) no diffusion model parameter was allowed to vary across conditions; in b) drift rate was allowed to vary across conditions; in c) boundary separation was allowed to vary across conditions; and in d) both drift rate and boundary separation were allowed to vary across conditions. We estimated model parameters with fast-dm-30.2 
(Voss & Voss, 2007)
 using the maximum likelihood approach. In addition to drift rate v and boundary separation a, we also estimated the non-decision time t0 and the trial-to-trial variability of the non-decision time s t0 . The starting point z was fixed to a/2 and all other model parameters were fixed to zero. These model estimation settings follow recommendations by Lerche and . Model fits were compared based on the Akaike information criterion (AIC; 
Akaike, 1973)
 and using Akaike weights 
(Wagenmakers & Farrell, 2004)
.


Results


THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 18
Behavioral data 
Table 2
 shows mean RTs and accuracies. There was a main effect of conflict on accuracy rates, F(1,40) = 43.46, p < .001, ω² = .34, such that performance was worse in conflict (M = 0.58) than non-conflict (M = 0.80) trials. There was no RT difference between conflict and non-conflict trials, F(1,40) = 0.07, p = .790, ω² = .00. 2 


Diffusion model analysis
Mean AICs were lowest and nearly identical for the model with only drift rate and the model with drift rate and boundary separation allowed to vary across experimental conditions, AIC none = 575, AIC v = 556, AIC a = 576, AIC v& a = 556. An inspection of individual Akaike weights showed that the model with only drift rate varying between conditions provided the best account of the data for 49 % of the participants, whereas the model with both drift rate and boundary separation varying between conditions provided the best account of the data for 29 % of the participants. 3 All subsequent analyses were therefore conducted with parameter estimates from the model in which only drift rates varied between conditions. Statistical analyses confirmed a main effect of conflict on drift rates, F(1,40) = 32.87, p < .001, ω² = .29, such that drift rates were lower in conflict (M = 0.16) than non-conflict (M = 0.73) trials.
We subsequently evaluated the fit of this model using a simulation study and graphical analyses of model fit and found that the diffusion model provided an excellent account of the observed data. There was no evidence that model predictions were systematically biased (for details, see Appendix A).
To evaluate the reliability of model parameters, we conducted odd-even splits of the observed data and estimated model parameters from the resulting data sets using the same model specifications. All model parameters showed acceptable reliabilities, as estimated by Spearman-Brown corrected correlations based on the resulting parameter estimates ranging from r = .85 to r = .96. Correlations between model parameters and observed variables are shown in 
Table B
.1.


Moderation by covariates
Accuracy rates. When individual-difference measures were introduced as continuous covariates into an ANCOVA, the experimental effect on accuracy rates was moderated by individual differences in cognitive reflection and need for cognition, but not working memory capacity. Individuals with better performance in the CRT showed a smaller decrease in accuracy rates than individuals with lower performance in the CRT when there was a conflict between rule-and belief-based processing (see 
Figure 3A
 for an illustration based on a median split), F(1,39) = 8.95, p = .005, ω² = .10. We observed the same tendency for individuals high in need for cognition (see 
Figure 3B
 for an illustration based on a median split), who tended to be less affected by conflicts between rule-and belief-based processing than individuals low in need for cognition, F(1,38) = 3.25, p = .079, ω² = .03. Working memory capacity, however, did not moderate conflict resolution, F(1,38) = 0.00, p = .987, ω² = .00.


THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 20
Reaction times. The experimental effect on RTs was not moderated by any of the covariates, all Fs ≤ 0.68, all ps ≥ .144, all ω²s = .00 Drift rates. To assess how the covariates moderated the experimental effect on drift rates, we entered them as continuous variables into an ANCOVA. The experimental effect on drift rates was moderated by individual differences in cognitive reflection and need for cognition, but not working memory capacity. Individuals with better performance in the CRT showed a smaller decrease in drift rates when there was a conflict between rule-and beliefbased processing than individuals with lower performance in the CRT (see 
Figure 3A
 for an illustration based on a median split), F(1,39) = 8.89, p = .005, ω² = .11.


Figure 3
Moderation of the experimental effect by A) cognitive reflection (CR) and B) need for cognition 
(NFC)
 Note. CR = cognition reflection; NFC = need for cognition. The median split was only conducted to illustrate the interaction, but that all statistical analyses used continuous covariates.
We observed the same tendency for individuals high in need for cognition, who tended to be less affected by conflicts between rule-and belief-based processing than individuals low in need for cognition (see 
Figure 3B
 for an illustration based on a median split), F(1,38) = 3.42, p = .072, ω² = .03. Working memory capacity did not moderate conflict resolution, F(1,38) = 0.00, p = .962, ω² = .00. in both conflict-and non-conflict trials, whereas individuals with greater cognitive reflection and need for cognition showed higher drift rates than individuals with lower cognitive reflection and need for cognition specifically in trials with a conflict between rule-and beliefbased processing (see 
Table 3
). None of the individual differences variables was significantly associated with the boundary separation parameter a, suggesting that a disposition to engage in critical thinking did not predict greater decision caution.


Discussion
In the first study, we used a diffusion model approach to account for the conflict between rule-and belief-based processing. We found that drift rates were smaller when ruleand belief-based problem features conflicted than when they aligned, whereas we found evidence for a criterion shift between conflict and non-conflict problems only in a limited number of participants. This result suggests that the interference caused by a conflict between rule-and belief-based processing impaired evidence accumulation.
The extent to which the conflict between rule-and belief-based processing interfered with evidence accumulation was moderated by individual differences in cognitive abilities and thinking styles. We found dissociations in the way drift rates related to individual differences that may be accounted for in terms of algorithmic and reflective thinking 
(Stanovich, 2009)
. While individual differences in reflective thinking (as assessed by the CRT) and the disposition to engage in reflective thinking (as assessed by the NFC scale) were positively related to drift rates specifically in conflict trials, individual differences in algorithmic ability (as assessed by working memory capacity) were related to drift rates in both conflict and nonconflict trials. This association between working memory capacity and overall drift rates is not surprising, as the relational reasoning task placed substantial demands on participants'
working memory. In sum, individuals who scored higher on the CRT and who showed greater need for cognition were found to process rule-based information more efficiently and to be less distracted by interfering belief-based information than individuals with lower reflective thinking. It is important to note, however, that these interactions did not overlap and could therefore be removed by transformations of the measurement scale 
(Bamber, 1979;
Loftus, 1978
). This is not necessarily problematic, as drift rates are process model parameters that closely reflect the latent cognitive process of interest 
(Wagenmakers et al., 2012)
. Nevertheless, it is important to keep in mind that these interactions may still be removed if the data are nonlinearly transformed to another measurement scale.
There are three limitations to Study 1. Because the aim of the Study 1 was to demonstrate that the diffusion model can adequately account for the conflict between rule-and belief-based processing, the sample size was comparatively small for investigating how individual differences variables incrementally contribute to transitive reasoning performance. Hence, we could not test whether individual differences in cognitive reflection and need for cognition jointly or independently moderated conflict resolution. Because successfully solving CRT items requires both algorithmic and reflective thinking 
(Erceg et al., 2020;
Stanovich, 2009
Stanovich, , 2010
Toplak et al., 2011)
, it is possible that CRT solution rates only predicted successful conflict resolution to the degree that it measured reflective thinking ability. However, to test this interpretation of our results, a larger sample size suited for multiple regression analysis would be needed.
Second, we assessed cognitive abilities with a measure of working memory capacity, which does not show as high a loading on general intelligence as proper tests of fluid intelligence 
(Carroll, 1993)
.
Third, we only analyzed how individual differences in cognitive abilities and thinking styles moderated performance when participants were instructed to assess the logical validity of a conclusion, but not how individual-difference variables moderated participants' performance in assessing the believability of a conclusion.
In Study 2, we therefore recruited a larger sample of 145 participants who evaluated transitive reasoning problems both on the basis of validity and on the basis of belief, and who in addition to the individual-differences measures included in Study 1 also completed a fluid intelligence test with a high g-loading.


Study 2
Building on the results from Study 1, when asked to assess a problem based on its validity, we expected cognitive reflection and need for cognition to predict successful conflict resolution (i.e., be associated with the conflict-induced decrease in drift rates), whereas we expected cognitive abilities to be related to the overall rate of evidence accumulation.
When asked to assess a problem based on its believability, however, we expected individuals with greater cognitive abilities to be more affected by interference between rule-and belief-based processing (i.e., to show a larger decrease in drift rates) than individuals with lower cognitive abilities, as was previously found by 
Thompson et al. (2018)
. Because 
Thompson et al. (2018)
 did not distinguish between cognitive abilities and thinking styles, we made no prediction about a possible dissociation regarding the propensity to be affected by interfering rule-based information when assessing a problem based on belief. We used multivariate analyses to analyze how each individual-differences variable incrementally contributed to transitive reasoning performance.


Materials and Methods


Participants
We recruited N = 145 participants from the participant pool from the Heidelberg Psychology Department. Of these, four did not complete the experiment. Data from two participants was lost due to technical problems. In addition, six participants were excluded because they took notes during the transitive reasoning task or reported either not understanding the task or forgetting task instructions. The remaining sample consisted of N = 133 participants THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 26 (91 females, 41 males, 1 no information) between 18 and 69 years (M = 25.1, SD = 9.07).
Most participants had an academic background.


Materials
Relational reasoning problems. Participants completed the same 192 reasoning problems as in Study 1. However, in half of the trials, they now had to assess the problems based on the believability of their conclusions, whereas in the other half they had to assess the problems based on the logical validity of their conclusions. This experimental factor ("Instruc- Covariates. To assess individual differences in cognitive abilities, we measured working memory capacity with the operation span task 
(Turner & Engle, 1989)
 following the rec- tended version of the CRT that included four additional problems 
(Frederick, 2005;
Toplak et al., 2014)
 and need for cognition with the NFC scale of the Rational-Experiential Inventory (REI; 
Epstein et al., 1996;
Keller et al., 2000)
. Participants also completed the faith in intuition scale of the REI, which is not reported in the present study. On average, participants solved 4.21 (SD = 2.15) of the CRT problems correctly and had a NFC score of 2.94 (SD = 0.94). All measures showed moderate-to-good internal consistencies, ranging from α = .75 for the CRT score over α = .78 for the APM score to α = .88 for the NFC scale and α = .88 for the operation span score.


Procedure
We administered the relational reasoning task followed by CRT, the REI, the operation span task, and the APM. Each session took approximately two hours.


Data analysis
RTs faster than 100 ms or slower than 5000 ms, or with logarithmized RTs exceeding ± 3 SDs of the mean of each condition, were discarded.
A model comparison approach was used to identify which diffusion model parameters differed between instruction conditions in the relational reasoning task. For this purpose, we estimated different models with either no parameter, drift rate, boundary separation, non-decision time, or any combination of the three parameters varying across instruction conditions. In addition, we allowed the drift rate to vary across congruency conditions in all estimated models based on the results from Study 1. Hence, we estimated a total of eight models. We estimated diffusion model parameters as in Study 1. Model fits were compared based on the Akaike information criterion (AIC; 
Akaike, 1973)
 and using Akaike weights 
(Wagenmakers & Farrell, 2004)
. 
Table 4
 shows mean RTs and accuracies. There was a main effect of conflict on accuracy rates, F(1,132) = 249.40, p < .001, ω² = .46, which were lower in conflict (M = 0.71) than in non-conflict (M = 0.88) trials. In addition, there was a main effect of instruction on accuracy rates, F(1,132) = 166.14, p < .001, ω² = .25, which were lower in the validity (M = 0.75) than in believability (M = 0.84) condition. Moreover, we observed a significant interaction between the factors Congruency and Instruction, F(1,132) = 23.45, p < .001, ω² = .04, such that the detrimental effect of the conflict between rule-and belief-based information was amplified when participants evaluated problems based on their logical validity (ΔACC = 0.20) compared to when they evaluated problems based on their believability (ΔACC = 0.13). Once again, there was no main effect of conflict on RTs, F(1,132) = 0.05, p = .833, ω² = .00. We observed a significant main effect of instruction on RTs, F(1,132) = 45.27, p < .001, ω² = .07, which were slower in the validity (M = 1.37) than in believability (M = 1.07) condition. We observed no significant interaction between the two experimental factors, F(1,132) = 3.00, p = .086, ω² = .00. 4


Results


Behavioral data
4 Again, we analyzed how time-on-task affected the two experimental effects Congruency and Instruction by splitting trials into four blocks. Reaction times decreased linearly across the course of the experimental task, F(3,393) = 58.49, p < .001, ω² = .04, and we observed a corresponding increase in accuracy rates that was most pronounced from the first to the second block, F(3,396) = 26.38, p < .001, ω² = .06. Participants' performance in trials with a conflict between rule-and belief-based reasoning got better over time, which resulted in a decrease of the estimated effect size of the two-way interaction between Congruency and Instruction on accuracy rates across the four blocks (block one: ω² = .11, block two: ω² = .04, block three: ω² = .00, block four: ω² = .01),


Diffusion model analysis
Mean AICs were lowest for the model with drift rate varying across Congruency conditions (conflict vs. non-conflict) and all three diffusion model parameters (i.e., drift rate, boundary separation, and non-decision time) varying across Instruction conditions (see 
Table   5
). This model provided the best account of the data for 33.83 % of the sample, whereas for 25.56 % of the sample, varying instructions only had an effect on drift rate and boundary separation, but not on non-decision time 5 . All subsequent analyses were therefore conducted with parameter estimates from the model in which only drift rates varied between congruency conditions and all three parameters varied between instruction conditions.  Overall, this pattern of results suggests that participants benefitted from training effects during the course of the experimental task that were most pronounced when they had to evaluate problems according to their logical validity. 
5
 Participants whose data were best described by the model with only drift rate and boundary separation varying between instruction conditions did not significantly differ in any of the covariates from participants whose data were best described by the model with all three parameters varying between instruction conditions, all Fs ≤ 1.68, all ps ≥ .200, all ω²s ≤ .01.
We subsequently evaluated the fit of this model using a simulation study and graphical analyses of model fit and found that the diffusion model provided a good account of the observed data (for details, see Appendix A). Only accuracies in the congruent condition under belief instructions could not be predicted with a high degree of precision, probably because error rates were extremely low in this condition. There was no evidence that model predictions were systematically biased.
To evaluate the reliability of model parameters, we conducted odd-even splits of the ob-  
Table B
.2.
There was a main effect of conflict on drift rates, F(1,132) = 122.44, p < .001, ω² = .11, such that drift rates were lower in conflict (M = 0.71) than non-conflict (M = 0.95) trials. In addition, there was a main effect of instruction on drift rates, F(1,132) = 311.25, p < .001, ω² = .44, which were lower in the validity (M = 0.48) than in believability (M = 1.18) condition. We also found a small but significant interaction between Congruency and Instruction, F(1,132) = 6.51, p = .012, ω² = .01, suggesting that conflict between ruleand belief-based processing led to a greater decrease in drift rates when participants evaluated problems based on their logical validity (Δv = 0.30) than based on their believability (Δv = 0.18). Analyses also confirmed a main effect of instruction on boundary separations, F(1,132) = 20.43, p < .001, which were higher in the validity (M = 2.26) than in believability (M = 2.02) condition. Finally, analyses failed to confirm a main effect of instruction on nondecision times, F(1,132) = 0.12, p = .730, ω² = .00, suggesting that the effect of instruction on non-decision times (i.e., encoding and motor response speed) was very nuanced and only held for a certain part of the sample.


Moderation by covariates
To investigate how individual differences in cognitive abilities and thinking styles moderated the processing of incongruent information under logic-and belief-instruction, we introduced working memory capacity, fluid intelligence, cognitive reflection, and need for cognition as continuous covariates. Because 19 participants had missing values in at least one of the covariates 6 , they were excluded from the following analyses. Four more participants were identified as multivariate outliers based on their Mahalanobis distance and removed from the following analyses. 7
Accuracy rates. We simultaneously introduced all four continuous between-subject covariates into a 2x2 ANCOVA with the within-subject factors Congruency and Instruction. After the four continuous covariates were introduced, we observed a significant three-way-interaction between need for cognition, Congruency, and Instruction, F(1,105) = 6.05, p = .016, ω² = .02. In addition, we observed a significant two-way interaction between fluid intelligence and Instruction, F(1,105) = 21.41, p < .001, ω² = .09, and between need for cognition and Congruency, F(1,105) = 4.50, p = .036, ω² = .01. 
6
 One participant did not complete the NFC scale, performed below guessing probability in the APM, correctly recalled no more than 10 % of the memory items in the operation span task, and responded correctly to fewer than 85 % of the processing items in the operation span task. Four participants scored below guessing probability in the APM and responded correctly to fewer than 85 % of the processing items in the operation span task. Four more participants scored below guessing probability in the APM, one other participant recalled no more than 10 % of the memory items in the operation span task, and nine other participants responded correctly to fewer than 85 % of the processing items in the operation span task. 
7
 The remaining 110 participants showed comparable effects of Congruency and Instruction on behavioral data as the complete sample, with a significant main effect of Congruency on accuracy rates, F(1,109) = 144.06, p < .001, ω² = .29, but not RTs, F(1,109) = 0.13, p = .721, ω² = .00, a significant main effect of Instruction on both accuracy rates, F(1,109) = 203.73, p < .001, ω² = .49, and RTs, F(1,109) = 41.86, p < .001, ω² = .09, and a significant interaction effect on accuracies, F(1,109) = 22.27, p < .001, ω² = .06, and RTs, F(1,109) = 6.97, p = .010, ω² = .00. They also showed comparable effects of Congruency and Instruction on drift rates as the complete sample, with a significant main effect of Congruency, F(1,109) = 112.23, p < .001, ω² = .13, a significant main effect of Instruction, F(1,109) = 291.82, p < .001, ω² = .48, and a significant interaction between the two factors, F(1,109) = 8.51, p = .004, ω² = .01.
There were no further interactions between any of the covariates and any of the experimental effects, all Fs ≤ 3.57, all ps ≥ .062, all ω²s ≤ .01. In addition, we found that more intelligent participants showed overall higher accuracy rates than less intelligent ones independent of experimental conditions, F(1,105) = 14.67, p < .001, ω² = .11.
To further inspect how participants' intelligence and need for cognition moderated the experimental effects, we calculated differences scores (ΔACC) between the accuracy rates in conflict and non-conflict trials separately for both instruction conditions (see 
Table 6
 for correlations between these difference scores and all covariates). Larger differences scores indicate a greater decrease in accuracies (i.e., greater impairment) when there was a conflict between rule-and belief-based problem features.


Figure 4
Moderation of the average difference in accuracy rates between non-conflict and conflict trials by Instruction condition and A) fluid intelligence (gf) and B) need for cognition (NFC)
Note. The median split was only conducted to illustrate the interaction, but that all statistical analyses used continuous covariates.
Participants' intelligence did not predict how well they processed interfering beliefbased information (see 
Figure 4A
 for an illustration based on a median split, where a smaller ΔACC indicates a smaller decrease in accuracy rates), r = -.09, p = .376. Participants with a higher need for cognition, however, showed a smaller difference in accuracy rates when evaluating problems based on their logical validity than individuals with a lower need for cognition, r = -.20, p = .033. This smaller difference indicated that their performance was less impaired when there was a conflict between rule-and belief-based problem features and that they were less affected by interfering belief-based information than individuals with a lower need for cognition (see 
Figure 4B)
. The difference in correlations Δr between need for cognition and intelligence was not significant, Δr = .11, z = 0.75, p = .227.
None of the covariates predicted how interfering rule-based information affected participants when they were evaluating problems based on their believability, but a marginally significant correlation between participants' intelligence and the difference in accuracy rates supported the notion that more intelligent individuals may have been somewhat more affected by interfering rule-based information than less intelligent ones, r = .18, p = .060.
Reaction times. None of the covariates moderated any of the experimental effects (see 
Table 6
), all Fs ≤ 1.78, all ps ≥ .185, all ω²s = .00.
Drift rates. We observed a significant three-way-interaction between fluid intelligence, Congruency, and Instruction (see 
Figure 5A
 for an illustration based on a median split), F(1,105) = 4.11, p = .045, ω² = .00, and a significant three-way-interaction between need for cognition, Congruency, and Instruction (see 
Figure 5B
), F(1,105) = 9.48, p = .003, ω² = .01.
Working memory capacity and cognitive reflection did not moderate the experimental effects, all Fs ≤ 0.52, all ps ≥ .474, all ω²s = .00.
There were no two-way-interactions between any of the covariates and any of the experimental effects, all Fs ≤ 3.58, all ps ≥ .061, all ω²s ≤ .01. In addition, we found a main effect of intelligence, F(1,105) = 11.31, p = .001, ω² = .09, which suggests that participants with greater cognitive abilities showed a greater general efficiency of information processing as reflected in higher drift rates in both conflict and non-conflict trials than participants with lower cognitive abilities. To further inspect how participants' intelligence and need for cognition moderated the experimental effects, we calculated differences scores Δv between the drift rate in conflict and non-conflict trials separately for both instruction conditions (see 
Table 6
 for correlations between these difference scores and all covariates). Larger differences scores indicated a greater decrease in drift rates (i.e., greater impairment) when there was a conflict between rule-and belief-based problem features than when there was no conflict.


Figure 5
Moderation of the average difference in drift rates (v) between non-conflict and conflict trials by Instruction condition and A) fluid intelligence (gf) and B) need for cognition (NFC)
Note. Δv = difference in drift rates between congruency conditions; gf = fluid intelligence; NFC = need for cognition. A greater Δv indicates a greater decrease in drift rates when there was a conflict between rule-and belief-based problem features. Note that the median split was only conducted to illustrate the interaction, but that all statistical analyses used continuous covariates.
More intelligent participants showed a greater decrease in drift rates than less intelligent ones when asked to assess the believability of a conclusion (see 
Figure 5A
 for an illustration based on a median split), r = .19, p = .044. This greater decrease indicated that their performance was more impaired when there was a conflict between rule-and belief-based problem features and that they were more affected by interfering rule-based information than less intelligent reasoners. When asked to assess the logical validity of a problem, however, more intelligent participants did not differ from less intelligent ones in the way that conflict between rule-and belief-based information affected their performance, r = .00, p = .985. These results suggest that more intelligent participants were more strongly affected by interfering rulebased information than less intelligent ones when evaluating the believability of a problem.
We observed the reverse effect for participants with a high need for cognition, who showed a smaller difference in drift rates than participants with a low need for cognition when asked to assess the logical validity of a problem, r = -.26, p = .006. This smaller difference indicated that their performance was less impaired when there was a conflict between ruleand belief-based problem features and that they were less affected by interfering belief-based information than individuals with a low need for cognition (see 
Figure 5B
 for an illustration based on a median split). When asked to assess a problem based on its believability, however, participants with a high need for cognition did not differ from participants with a low need for cognition in the way that a conflict between rule-and belief-based information affected their performance, r = .09, p = .359. These results suggest that participants with a higher need for cognition were less strongly affected by interfering belief-based information than participants with a lower need for cognition when evaluating the logical validity of a problem.
Finally, we tested if the data contained sufficient evidence to support a full double dissociation. The correlation between Δv and intelligence was not significantly larger than the correlation between Δv and need for cognition when participants were asked to evaluate problems based on their believability, Δr = .09, z = 0.71, p = .238. However, the correlation between Δv and need for cognition was larger than the correlation between Δv and intelligence when participants were asked to evaluate problems based on their logical validity, Δr = -.26, z = -1.77, p = .039. Taken together, these results show that participants' need for cognition was more predictive of successfully ignoring interfering belief-based information than their intelligence. However, they fail to conclusively demonstrate that participants' intelligence was more predictive of difficulties ignoring interfering rule-based information than their need for cognition.


Discussion
Consistent with results from the first study, we found that participants with a greater need for cognition were less affected by interference between rule-and belief-based processing (i.e., showed a smaller decrease in drift rates) than participants with a lower need for cognition when assessing problems based on their logical validity. When assessing problems based on their believability, however, we found that more intelligent participants were more affected by interference between rule-and belief-based processing (i.e., showed a higher decrease in drift rates) than less intelligent participants. These results tentatively point to a double dissociation in the way that individual differences in fluid intelligence and thinking styles moderate the processing of conflicting rule-and belief-based information. This double dissociation would suggest that different cognitive mechanisms may mediate the associations of fluid intelligence and need for cognition with transitive reasoning performance.


Intelligence and working memory capacity
Individuals with high cognitive abilities seemed to generate a rule-based default response, which then interfered with their ability to evaluate conclusions based on believability, while less intelligent individuals seemed to generate no or weaker logical intuitions. This conclusion is in line with previous findings which demonstrated that more intelligent individuals were more likely to give rule-based responses as a first, intuitive answer than less intelligent individuals 
(Thompson & Johnson, 2014)
. Moreover, it is supported by a recent study from 
Thompson et al. (2018)
, which also found that individuals with high cognitive abilities performed worse when asked to assess the believability of a conclusion than when asked to assess its logical validity.
Process theories of intelligence can shed some light on why more intelligent reasoners may be more likely to generate a rule-based default response than less intelligent reasoners.
Smarter reasoners may more readily and quickly generate a rule-based response, because they are better at creating higher-level relational structures that temporarily bind elemental perceptual or memorial representations and that can be effectively abstracted from intrinsic features of low-level representations 
(Chuderski, 2014;
Oberauer et al., 2008)
. Hence, their greater ability to generate abstract relational bindings may actually bias them against evaluating conclusions based on intrinsic and concrete problem features. Surprisingly, however, more intelligent reasoners did not show a corresponding advantage at resolving the conflict between ruleand belief-based information when assessing problems based on their logical validity. Instead, they only showed an overall higher drift rate irrespective of congruency condition, replicating our results from Study 1 and suggesting that individuals with high cognitive abilities benefit from a high general efficiency of information processing as reflected in drift rates in both conflict and non-conflict trials.
It is also important to note that the moderating effect of cognitive abilities on resolving the conflict between rule-and belief-based information only emerged for our fluid intelligence measure, but not for our working memory capacity measure. Although both measures predicted overall drift rates irrespective of congruency and instruction condition, only fluid intelligence moderated the experimental effects. This is not surprising, as the two measures (APM test scores and operation span scores) have been shown to exhibit fundamentally different loadings on a general factor of intelligence 
(Carroll, 1993)
 and were also only weakly correlated in the present study, r = .15. While working memory capacity may sometimes be isomorphic with fluid intelligence 
(Chuderski, 2013;
Kyllonen & Christal, 1990)
, substantial correlations between working memory capacity and fluid intelligence are typically only found on the latent construct level 
(Chuderski, 2013;
Conway et al., 2005)
, as the validity of single working memory measures depends on task properties 
(Conway et al., 2005;
Turner & Engle, 1989;
Wilhelm et al., 2013)
. Hence, our results only highlight that individual differences in cognitive abilities either need to be assessed with a comprehensive test battery of working memory tasks that allows latent variable modeling or with a cognitive ability test that shows a sufficiently high g-loading to allow economic assessment.


Need for cognition and cognitive reflection
Need for cognition predicted successful conflict resolution when evaluating conclusions based on their logical validity. In addition, participants with a high need for cognition did not
show a bias towards rule-based responding when evaluating conclusions based on their believability, which suggests that different cognitive mechanisms may account for the associations of rule-based reasoning with cognitive abilities and reflective thinking styles, respectively. Because participants with a high need for cognition had no disadvantage in evaluating conflict problems based on their believability, it is unlikely that they intuitively generated a dominant rule-based default response. Instead, our results suggest that individuals with a disposition to engage in reflective thinking may be as likely to generate a dominant belief-based default response as other individuals, but may be better at successfully inhibiting and overriding the incorrect belief-based response.
Again, it should be noted that the moderating effect of reflective thinking only emerged for need for cognition as a measure of dispositional reflective thinking, but not for cognitive reflection test performance as a measure of reflective thinking ability. We also found that CRT performance was more strongly related to intelligence test performance, r = .38, than to need for cognition, r = -.24. This finding is not a fluke in our data, but rather it is consistent with a recent study that systematically evaluated the validity of two CRT versions and concluded that they did not access the construct of cognitive reflection, but rather individual differences in intelligence and numerical abilities 
(Erceg et al., 2020)
. This conclusion is also supported by a recent study by 
Chuderski and Jastrzębski (2018)
, who found strong relations between participants' performance in CRT items and their fluid intelligence. Moreover, Erceg et al. (2020) also failed to find any association between CRT scores and reasoning performance in a belief bias task once individual differences in intelligence and numerical abilities were accounted for. In addition, recent findings have indicated that the CRT may only measure individual differences in cognitive reflection in less intelligent individuals who do not possess strong logical intuitions , but that it is not an accurate measure of cognitive reflection in more intelligent individuals with strong numerical abilities 
(Erceg et al., 2019)
. Hence, our findings chime in with other studies questioning the validity of the CRT as a measure of reflective thinking. In light of this, it is surprising that we found evidence for convergent validity between the CRT and NFC in Study 1, r = .46. This discrepancy between studies may reflect differences in the sample composition and the specific CRT items included in the two studies 8 . Because we used a published extension of the CRT in the second study, whereas we included items only conveyed by personal communication in the first one, and because participant numbers were much larger and the sample composition more diverse in the second study than the first, we believe that the data from the second study contain more compelling evidence than the data from the first study, underscoring ours and others' notion that the CRT largely measures cognitive abilities 
(Chuderski & Jastrzębski, 2018;
Erceg et al., 2019
Erceg et al., , 2020
.


General Discussion
In two studies, we used stochastic diffusion models to analyze how individuals processed information during a transitive reasoning task when the logical validity of a conclusion conflicted with its believability. We found that the belief bias effect could be mapped onto the drift rate parameter of the diffusion model, which describes the speed and efficiency of the evidence accumulation process during decision making. Drift rates were larger when the processing of rule-and belief-based problem features converged, whereas they were smaller when they diverged. In addition, we found that only a limited number of individuals adjusted their decision criteria to process problems with a conflict between rule-and belief-based information more carefully.
By using innovative analytical approaches such as mathematical models of decision making, researchers are able to use the complete information of the entire response time distributions of correct and incorrect responses to infer latent process parameters describing decision making and conflict resolution. Hence, using diffusion models may help to disambiguate inconsistent results regarding the effect of increased processing demands and varying task instructions on response times in belief bias tasks 
(Handley et al., 2011;
Thompson et al., 2011;
Trippas et al., 2017)
.
In the second study, we found that when assessing problems based on the believability of their conclusions, participants did not only show higher drift rates (i.e., a greater speed and efficiency of information-processing), but also a shift in decision criteria that made them respond less cautiously under belief than under logic instruction. Because variations in drift rate and boundary separation parameters may affect different parts of the response time distribution , previous studies may have found conflicting results considering only effects on mean response times. This benefit of distinguishing between the speed of evidence accumulation and decision cautiousness is particularly useful for individual differences research, as individuals with higher abilities have been shown to strategically adjust their decision criteria differently from individuals with lower abilities to adapt to task demands 
(Draheim et al., 2016
(Draheim et al., , 2019
Schmiedek et al., 2007)
. We found some evidence for a relationship between cognitive abilities and decision boundaries in Study 1 (r = -.22), but not in Study 2 (all rs ≤ |.05|).
Other innovative modeling approaches that have been used to shed light on transitive reasoning performance include signal detection theory 
(Dube et al., 2010)
, equal-variance signal detection theory 
(Trippas et al., 2018)
, and multinomial models 
(Klauer, 2010;
Klauer et al., 2000)
. Previous studies using variations of signal detection theory have called into question whether a conflict between rule-and belief-based problem features affects the ability to discriminate between valid and invalid syllogisms 
(Dube et al., 2010;
Trippas et al., 2018)
.
Those results are only partly in line with ours, as we found that the large majority of participants showed an effect of congruency on drift rates (i.e., discrimination ability). One reason for this divergence may be found in the simultaneous consideration of response times and accuracies in the present study, whereas signal detection models only consider accuracy rates irrespective of response speed. It would be interesting to see if analyses based on multinomial models that may be extended to fit response time distributions arrive at similar conclusions in future studies 
(Heck & Erdfelder, 2016;
Klauer & Kellen, 2018)
. In addition, it would be interesting to compare our results with results from diffusion models that specifically model the conflict between rule-and belief-based processing. However, dual-process diffusion models have so far only been used as cognitive process models but not as measurement models, and can therefore only predict empirical data but not fit them 
(Alós-Ferrer, 2018;
Caplin & Martin, 2016)
. In comparison, the diffusion model for conflict tasks has been developed to fit empirical data 
(Ulrich et al., 2015)
, but will likely need many more trials than used in the present study to describe the conflict resolution process reliably 
(White et al., 2018)
.


Cognitive abilities and thinking styles affect transitive reasoning performance through different mechanisms
Our results strengthen the conceptual distinction between the algorithmic and reflective mind 
(Stanovich, 2009
(Stanovich, , 2010
, as we found suggestive evidence for a double dissociation in the way that individual differences in algorithmic and reflective thinking moderated the processing of conflicting rule-and belief-based information. On the one hand, intelligent individuals seemed to quickly generate abstract representations of reasoning problems bereft of lowlevel intrinsic problem features. This impaired their ability to evaluate the believability of conclusions, either because they could not access these low-level intrinsic problem features as readily as individuals who did not automatically strip them from their mental problem repre-sentations or because these quickly generated abstract representations interfered with simultaneously held belief-based representations. On the other hand, individuals with a disposition to engage in reflective thinking seemed to be as likely to generate a belief-based default response as other individuals, but inhibited and overrode the incorrect belief-based response more quickly and successfully. Hence, they showed no disadvantage when asked to assess problems based on their believability, but outperformed individuals with a lower need for cognition when asked to assess problems based on their logical validity.
Taken together, and to the extent that fluid intelligence and need for cognition are involved in the key functions of the algorithmic mind and the reflective mind 
(Stanovich, 2009
(Stanovich, , 2010
, these results confirm the idea that algorithmic and reflective thinking abilities affect the processing of conflict information through different mechanisms and at different stages of information-processing. In particular, our results suggest that algorithmic abilities may facilitate the creation of decoupled problem representations (for similar findings, see 
Thompson & Johnson, 2014)
, whereas a disposition to engage in critical thinking may facilitate the successful override of Type 1 responses. As such, our results are consistent with previous research that suggested that different individuals approach and solve reasoning tasks in qualitatively different ways 
(Erceg et al., 2019)
. All in all, results from individual-differences studies suggest that in different individuals, performance in reasoning tasks may be affected by different cognitive processes. Crucially, only individual differences in the reflective, but not in the algorithmic mind, predicted successful conflict resolution in the standard belief bias condition. Across both studies, we found that individual differences in algorithmic abilities were instead positively related to the general efficiency of information processing irrespective of congruence or incongruence between rule-and belief-based problem features.


Implications for dual-process accounts of reasoning
Our results contribute to the growing body of evidence challenging standard dual-process theories of reasoning (e.g., 
De Neys, 2012
Evans et al., 2009;
Handley et al., 2011;
Handley & Trippas, 2015;
Newman et al., 2017;
Pennycook et al., 2015;
Trippas et al., 2017)
. In particular, there are two findings that are not easily reconciled with the default interventionist account 
(Evans, 2008)
. First, standard dual-process theories cannot explain why participants asked to evaluate the believability of a conclusion performed worse when rule-and belief-based problem features were in conflict than when they were aligned. Second, they cannot explain why this impairment in performance was even greater for more intelligent individuals than less intelligent ones, as during initial stages of reasoning only belief-based default responses should be generated, which should not be harder to access for more intelligent reasoners. Instead, both findings are consistent with recent theoretical developments proposing that rule-and belief-based problem features are processed in parallel and that reasoning problems that elicit a conflict between rule-and beliefbased may also elicit more than one Type 1 response 
(De Neys, 2012
Handley & Trippas, 2015;
Pennycook et al., 2015)
. However, our results may also be reconciled with a modified default-interventionist model that assumes that different individuals generate different Type 1 default responses. As such, our results do not pose a severe challenge to the basic architecture and processing assumptions of the default interventionist account, but rather suggest that the default may vary between persons.


Limitations
One limitation of our studies is that we used ability measures for assessing individual differences in algorithmic thinking ability and self-report measures for assessing individual differences in reflective thinking. These measures are widely used to assess the two constructs in reasoning research, which allowed us to directly relate results from our study to a large number of previous findings. Nevertheless, it is important to note that constructs were confounded with measurement methods in the present studies. This is particularly problematic as ability and self-report measures of several constructs, including intelligence, self-control, selfregulation, prospective memory, and many more, have been repeatedly shown to be only weakly related 
(Eisenberg et al., 2019;
Hedge et al., 2020;
Herreen & Zajac, 2017;
Jacobs & Roodenburg, 2014;
Pennycook et al., 2017;
Saunders et al., 2018;
Uttl & Kibreab, 2011;
Wennerhold & Friese, 2020)
. Future research should therefore strive to disambiguate this confounding factor and include objective measures of reflective thinking such as probabilistic reasoning and counter-example seeking.
Another limitation is the composition of our samples, which consisted mostly of university students. Range restriction in cognitive abilities could therefore have affected the size and direction of correlations between covariates and experimental effects. To prevent this, we recruited participants from a broad range of majors and included only a limited number of participants who were currently enrolled in highly competitive majors. The success of our recruitment strategy is reflected in mean performance in the APM, which, when extrapolated from 18 to 36 items, corresponds to an average of 22 correctly solved items. According to the norms presented in the manual, participants between 23 and 27 years who solved 22 items were placed in the 50th percentile of the ability distribution 
(Raven et al., 1994)
. This suggests that our sample was not particularly restricted in cognitive abilities.
A further limitation of the present study lies in the relative weak evidence we found for a full double dissociation between cognitive and reflective abilities. Whereas we could show that participants' need for cognition was more predictive of successfully ignoring interfering belief-based information than their intelligence, we could not conclusively demonstrate that participants' intelligence was more predictive of difficulties ignoring interfering rule-based information than their need for cognition. Many factors, including the low power of the z-test for comparing correlations, the measurement of cognitive and reflective abilities on a manifest variable level, and the moderate sample size, may have limited the conclusiveness of our results. We therefore suggest that future studies build upon our approach and measure cognitive and reflective abilities with multiple indicators in larger (and possibly more heterogeneous) samples, allowing the testing of latent correlations and latent correlation differences. Ideally, THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 46 model parameters and their associations with cognitive and reflective abilities should then be estimated in a hierarchical Bayesian cognitive latent variable framework, as this would further increase the stability of latent parameter estimates 
(Schubert et al., 2019;
Vandekerckhove, 2014)
.


Conclusions
By using the diffusion model, we simultaneously took into account both accuracy rates and response times to describe transitive reasoning performance. This enabled us to distinguish between experimental effects on the speed of evidence accumulation, on the speed of non-decision processes, and on decision criteria. Across two studies we found that individual differences in need for cognition predicted successful conflict resolution under logic instruction, as reflected in the decrease in drift rates from non-conflict to conflict trials. This suggests that a disposition to engage in reflective thinking facilitates the inhibition and successful override of Type 1 responses. Intelligence, however, was negatively related to successful conflict resolution under belief instruction, which suggests that individuals with high cognitive abilities quickly generated a higher-level logical response that interfered with their ability to evaluate lower-level intrinsic problem features. Taken together, this tentative double dissociation suggests that algorithmic and reflective thinking abilities may affect the processing of conflict information through different mechanisms and at different stages of information-processing:
Greater cognitive abilities facilitate the creation of decoupled problem representations, whereas a greater disposition to engage in critical thinking facilitates the successful override of Type 1 responses.
(correlations between observed and predicted data ranged from .84 to .96). There was no evidence for systematic bias in model predictions.  
tion") was manipulated block-wise and the order of presentation was counterbalanced across participants. Within each block and identical to Study 1, 50 % of the trials contained problems with a conflict between rule-and belief-based information, whereas the other 50 % were nonconflict problems. Participants completed six practice trials with feedback before the start of each of the two experimental blocks.Mean RTs showed good reliabilities, as estimated by Spearman-Brown corrected correlations based on odd-even splits ranging from r = .92 to r = .97. In comparison, mean accuracies showed lower reliabilities, with corresponding correlations ranging from r = .55 to r = .56 under belief instructions and from r = .81 to r = .82 under logic instructions.


ommendations for partial scoring by
Conway et al. (2005)
 and fluid intelligence with an 18item version of Raven's Advanced Progressive Matrices (APM; Raven et al., 1994). On average, participants recalled 80 % (SD = 11 %) of the items in the operation span and solved 11.01 (SD = 2.27) items of the APM correctly. Cognitive reflection was assessed with an ex-


F( 3
3
,396) = 7.55, p < .001, ω² = .02. Pairwise comparisons supported this interpretation, as the experimental effect of Congruency remained relatively stable over time in the belief condition (block one: d = 0.62, block two: d = 0.89, block three: d = 0.60, block four: d = 0.62), while it decreased over time in the logic condition (block one: d = 1.34, block two: d = 1.34, block three: d = 0.75, block four: d = 0.85).


served data and estimated model parameters from the resulting data sets using the same modelspecifications. Most parameters showed acceptable reliabilities, as estimated by Spearman-Brown corrected correlations based on the resulting parameter estimates ranging from r = .69 to r = .92. Lowest estimates of reliability were observed for drift rates under belief instructions (.69 and .74), which may again reflect the low variability in accuracies in this condition, and the s t0 parameter (.76). Correlations between model parameters and observed variables are shown in


Figure A. 2
2
Model fit based on the comparison of statistics (accuracy rate, 25 %, 50 %, and 75 % RT quantile) of observed and predicted data Note. Each data point represents one participant. Diagonal lines indicate perfect model fit.


Table 2
2
Mean accuracies (ACC), mean RTs in seconds (RT), mean drift rates (v), mean boundary sep-
arations parameters (a), mean non-decision time parameters (t0), and mean inter-trial varia-
bilities of the non-decision times (s t0 ) with SDs in parentheses.
ACC
RT
v
a
t0
s t0
Conflict
0.58 (0.20) 1.33 (0.33) 0.16 (0.53)
Non-con-
0.80 (0.10) 1.32 (0.35) 0.73 (0.32)
2.16 (0.38) 0.38 (0.26) 0.31 (0.41)
flict


Table 3
3
Correlations between accuracy rates (ACC), reaction times (RT) and drift rates v in non-conflict-and conflict trials with boundary separation a,working memory capacity (WMC), Cognitive reflection test (CRT) scores, and Need for cognition (NFC) scores. p < .05; ** p < .01; *** p < .001 ACC nonconflict ACC conflict ACC difference RT non-conflict Instead, individuals with high working memory capacity tended to show high drift rates
RT conflict RT difference v non-conflict
v conflict
v difference
a
WMC CRT
*


Table 4
4
Mean accuracies (ACC), mean RTs in seconds (RT), mean drift rates (v), mean boundary sep-
arations parameters (a), mean non-decision time parameters (t0), and mean inter-trial varia-
bilities of the non-decision times (s t0 ) with SDs in parentheses.
Instruction Congruency
ACC
RT
v
a
t0
s t0
Logic
Conflict
0.65
1.36
0.33
(0.15)
(0.63)
(0.44)
2.26
0.33
Belief
Non-conflict
0.85 (0.07)
1.38 (0.58)
0.63 (0.43)
(0.59)
(0.37)
0.28 (0.56)
Conflict
0.77
1.08
1.09
(0.13)
(0.52)
(0.41)
2.02
0.34
Non-conflict
0.90
1.05
1.27
(0.55)
(0.37)
(0.05)
(0.52)
(0.45)


Table 5
5
Mean Akaike information criterion (AIC) for each specified model and percentage of the sam-
ple whose data were best described by the model. v = drift rate; a = boundary separation,
t 0 = non-decision time.
Model parameter varying across
mean AIC
Best-fitting model for % of
experimental conditions
participants
Congruency
Instruction
v
none
504
2.26 %
v
v
482
14.29 %
v
a
482
3.76 %
v
t 0
494
3.01 %
v
v & a
468
25.56 %
v
v & t 0
473
13.53 %
v
a & t 0
478
3.76 %
v
v & a & t 0
464
33.83 %


Table 6
6
Correlations between the difference in accuracies ΔACC, reaction times ΔRT and drift rates Δv between congruency conditions, Advanced Progres-
sive Matrices (APM) scores, working memory capacity (WMC), Cognitive reflection test (CRT) scores, and Need for cognition (NFC) scores.
ΔACC
ΔRT
Δv
APM
WMC
CRT
logic in-
belief in-
logic in-
belief in-
logic in-
belief in-
struction
struction
struction
struction
struction
struction
ΔACC logic instruc-
tion
ΔACC belief instruc-
.06
tion
ΔRT logic instruction
-.07
.04
ΔRT belief instruction
-.14
-.13
-.13
Δv logic instruction
.86 ***
.00
-.19
-.05
Δv belief instruction
.07
.56 ***
.14
-.52 ***
.05
APM
-.09
.18
.09
-.05
.00
.19 *
WMC
-.02
.00
.01
-.15
.02
.10
.15
CRT
-.03
-.07
.07
.07
.02
-.07
.38 ***
.08
NFC
-.20 *
.02
-.14
.06
-.26 **
.09
-.20 *
-.13
-.24 *
* p < .05; ** p < .01; *** p < .001


Correlations between model parameter (drift rate v, boundary separation a, non-decision time t 0 , and trial-to-trial variability of the non-decision time s t0 ) with observed variables (accuracy rates ACC and mean response times RT
Table B.2
v non-conflict
v conflict
a
t 0
s t0
ACC non-conflict
ACC conflict
RT non-conflict
RT conflict
logic
belief
logic
belief
logic
belief
logic
belief
logic
belief
logic
belief
logic
belief
logic
v non-conflict
logic
belief
.39
v conflict
logic
.55
.25
belief
.31
.73
.10
a
t 0
logic
.25 -.19
-.28
-.40 .15
.26
.50
s t0
belief
.18 -.41
-.63
-.37 -.17
-.58
-.40 .40
.93
t 0
ACC non-conflict logic
.87 -.17
.03
.01 -.39
.17
-.09 -.15
-.11
.09
.07
ACC conflict
belief
-.06 -.16
.02
.98 -.40
.09
.07 -.22
-.04
-.38
.87
-.34
.02
RT non-conflict ACC non-s t0 logic
-.52 -.20 .85
-.04 .31
-.28 -.34 .45
.01 .24
.62 -.18 .08
.00 -.29
.25
.86 -.17
.31 .92 -.17
-.23
-.35
-.22
RT conflict conflict
belief
-.41 .14
.51
-.48 .10
.41
.57 .14
.06
.25
-.03
.26 -.06
-.05
-.29 .24
-.45
.79
ACC conflict logic
.46
.20
.91
.17
.18
-.17
-.24
-.29
-.26
.43
.08
belief
.02
.35
-.03
.61
.10
.02
.01
-.06
-.08
.05
.67
.01
RT non-con-
logic
-.48
-.25
-.30
-.15
.67
.35
.51
.40
.43
-.24
.12
-.16
.13
flict
belief
-.40
-.52
-.38
-.40
.14
.67
.52
.66
.66
-.34
-.10
-.29
-.15
.52
RT conflict
logic
-.42
-.25
-.35
-.09
.59
.28
.61
.46
.49
-.27
.06
-.23
.12
.89
.53
belief
-.39
-.47
-.39
-.44
.14
.65
.69
.69
.67
-.31
-.09
-.32
-.17
.53
.96
.52


To analyze how time-on-task affected the experimental effect, we split trials into four blocks. Accuracy rates increased across the course of the experimental task, F(3,120) = 3.33, p = .022, ω² = .01, while reaction times decreased, F(3,120) = 11.74, p < .001, ω² = .05. These training effects were most pronounced from the first to the second block. In addition, we observed an interaction between time-on-task and the congruency condition on reaction times, which reflected that reaction times were faster in the congruent than in the incongruent condition in all blocks but the third, F(3,120) = 4.76, p = .004, ω² = .01.3  Participants whose data was best described by the model with only drift rate varying between conditions did not differ in any of the covariates from participants whose data was best described by the model with both drift rate and boundary separation varying between conditions, all Fs ≤ 0.43, all ps ≥ .516, all ω²s = .00.


A list of specific items included in the two studies and their German translations will be provided upon request.








Appendix A Study 1
We evaluated the fit of the best-fitting model using a simulation study. We generated 1,000 parameter sets from truncated multivariate normal distributions with means, variances, and covariances based on the estimated parameters. We subsequently simulated one random data set from each set of parameters to generate data with a comparable mean, variance, and covariance structure to the observed data that were undoubtedly generated by a diffusion process. We then re-estimated model parameters from these simulated data sets using the same model specifications. The idea of this simulation study was to test if the fit values of the observed data were comparable to the fit values of the simulated data. If the fit values of the observed data were worse than the fit values of the simulated data, this would indicate that the data were unlikely to result from a diffusion processes and that any inferences based on model parameter estimates might be invalid. We specified the 99 % quantile of worst fit values of the simulated data as a criterion for poor model fit (note that lower fit values indicate better model fit in a maximum likelihood approach that operates with negative likelihoods). Not a single model fitted to the observed data exceeded this criterion, which suggests that the diffusion model provided an excellent account of the observed data.
We also conducted graphical analyses of model fit based on the precision of predictions for accuracy rates and RT quartiles (see 
Figure A
.1), which echoed our results from the simulation study and showed that the model provided an excellent account of participants' accuracies (all correlations between observed and predicted data were larger than .91). Moreover, the model provided an adequate account of participants' RT distributions (correlations between observed and predicted data ranged from .71 to .92). There was no evidence that model predictions were systematically biased.


THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 62


Figure 3
Model fit based on the comparison of statistics (accuracy rate, 25 %, 50 %, and 75 % RT quantile) of observed and predicted data Note. Each data point represents one participant. Diagonal lines indicate perfect model fit.


Study 2
We evaluated the fit of the best-fitting model model. For this purpose, we generated data sets from 1,000 simulated model parameter sets from truncated multivariate normal distributions with means, variances, and covariances based on the estimated parameters, following the same rationale as in the first study. Once again, not a single model fitted to the observed data exceeded the 99 % quantile of worst fit values of the simulated data sets, which suggests that the diffusion model provided an excellent account of the observed data. Graphical analyses of model fit based on the precision of predictions for accuracy rates and RT quartiles (see 
Figure A.
2) confirmed this overall evaluation. The diffusion model provided an adequate account of participants' accuracies (correlations between observed and predicted data ranged from .74 to .92). Note that the relatively low correlation of .74 was observed for accuracies in the congruent condition under belief instructions, where error rates were extremely low (see 
Table 4
) and could therefore not be recovered with high precision by the model. Moreover, the model provided a good account of participants' RT distributions Appendix B 
Table B
.1 Correlations between model parameter (drift rate v, boundary separation a, non-decision time t 0 , and trial-to-trial variability of the non-decision time s t0 ) with observed variables 
(
 
 










Information theory and an extension of the maximum likelihood principle




H
Akaike




B. N. Petrov & F. Csaki






Akademiai Kiado














A dual-process diffusion model




C
Alós-Ferrer




10.1002/bdm.1960








Journal of Behavioral Decision Making




31


2
















A Diffusion Model Analysis of Episodic Recognition in Individuals with a Family History for Alzheimer Disease: The Adult Children Study




A
J
Aschenbrenner






D
A
Balota






B
A
Gordon






R
Ratcliff






J
C
Morris




10.1037/neu0000222








Neuropsychology




30


2
















Conflict and dual process theory: The case of belief bias




L
J
Ball






V
A
Thompson






E
J N
Stupple








Routledge/Taylor & Francis Group








In Dual process theory 2.0.








State-trace analysis: A method of testing simple theories of causation




D
Bamber




10.1016/0022-2496








Journal of Mathematical Psychology




19


2
















Heuristic and analytic processes in reasoning: An event-related potential study of belief bias




A
P
Banks






C
Hope








Psychophysiology




51


3


















10.1111/psyp.12169














The cognitive reflection test is robust to multiple exposures




M
Bialek






G
Pennycook




10.3758/s13428-017-0963-x








Behavior Research Methods




50


5
















Assessing the validity of inferences from scores on the cognitive reflection test




N
Blacksmith






Y
Yang






T
S
Behrend






G
A
Ruark




10.1002/bdm.2133








Journal of Behavioral Decision Making




32


5




















The






Of






Styles




REASONING 48












The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive Psychology




57


3


















10.1016/j.cogpsych.2007.12.002














The need for cognition




J
T
Cacioppo






R
E
Petty




10.1037/0022-3514.42.1.116








Journal of Personality and Social Psychology




42


1
















The dual-process drift diffusion model: Evidence from response times




A
Caplin






D
Martin








Economic Inquiry




54


2


















10.1111/ecin.12294














Human cognitive abilities: A survey of factor-analytic studies




J
B
Carroll




10.1017/CBO9780511571312








Cambridge University Press












When are fluid intelligence and working memory isomorphic and when are they not?




A
Chuderski




10.1016/j.intell.2013.04.003








Intelligence




41


4
















The relational integration task explains fluid reasoning above and beyond other working memory tasks




A
Chuderski








Memory & Cognition




42


3


















10.3758/s13421-013-0366-x














Much ado about aha!: Insight problem solving is strongly related to working memory capacity and reasoning ability




A
Chuderski






J
Jastrzębski




10.1037/xge0000378








Journal of Experimental Psychology. General




147


2
















An experimental investigation of need for cognition




A
R
Cohen






E
Stotland






D
M
Wolfe








The Journal of Abnormal and Social Psychology




51


2


















10.1037/h0042761














Working memory span tasks: A methodological review and user's guide




A
R A
Conway






M
J
Kane






M
F
Bunting






D
Z
Hambrick






O
Wilhelm






R
W
Engle




















10.3758/BF03196772








Psychonomic Bulletin & Review




12


5














Dual processing in reasoning: Two systems but one reasoner




W
De Neys




10.1111/j.1467-9280.2006.01723.x








THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 49






17














Bias and conflict: A case for logical intuitions




W
De Neys




10.1177/1745691611429354








Perspectives on Psychological Science




7


1
















Conflict detection, dual processes, and logical intuitions: Some clarifications




W
De Neys








Thinking & Reasoning




20


2


















10.1080/13546783.2013.854725














Combining reaction time and accuracy: The relationship between working memory capacity and task switching as a case example




C
Draheim






K
L
Hicks






R
W
Engle








Perspectives on Psychological Science


















10.1177/1745691615596990














Reaction time in differential and developmental research: A review and commentary on the problems and alternatives




C
Draheim






C
A
Mashburn






J
D
Martin






R
W
Engle








Psychological Bulletin




145


5


















10.1037/bul0000192














Assessing the belief bias effect with ROCs: It's a response bias effect




C
Dube






C
M
Rotello






E
Heit




10.1037/a0019634








Psychological Review




117


3
















Uncovering the structure of self-regulation through datadriven ontology discovery




I
W
Eisenberg






P
G
Bissett






A
Zeynep Enkavi






J
Li






D
P
Mackinnon






L
A
Marsch






R
A
Poldrack




10.1038/s41467-019-10301-1








Nature Communications




10


1


2319














Individual differences in intuitiveexperiential and analytical-rational thinking styles




S
Epstein






R
Pacini






V
Denes-Raj






H
Heier




10.1037//0022-3514.71.2.390








Journal of Personality and Social Psychology




71


2
















Who detects and why? Individual differences in abilities, knowledge and thinking dispositions among different types of problem solvers THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING 50 and their implications for the validity of reasoning tasks




N
Erceg






Z
Galic






A
Bubić




10.31234/osf.io/w5zau












Preprint








A reflection on cognitive reflection -testing convergent validity of two versions of the Cognitive Reflection Test




N
Erceg






Z
Galic






M
Ružojčić




10.31234/osf.io/ewrtq


















Dual-processing accounts of reasoning, judgment, and social cognition




J
Evans






B T
St




10.1146/an-nurev.psych.59.103006.093629








Annual Review of Psychology




59


1
















On the conflict between logic and belief in syllogistic reasoning




J
Evans






B T
St






J
L
Barston






P
Pollard








Memory & Cognition




11


3


















10.3758/BF03196976














Reasoning under time pressure




J
Evans






B T
St






S
J
Handley






A
M
Bacon




10.1027/1618-3169.56.2.77








Experimental Psychology




56


2
















The speed-accuracy tradeoff in the elderly brain: A structural model-based approach




B
U
Forstmann






M
Tittgemeyer






E.-J
Wagenmakers






J
Derrfuss






D
Imperati






S
Brown








Journal of Neuroscience




47


















10.1523/JNEUROSCI.0309-11.2011














Cognitive reflection and decision making




S
Frederick




10.1257/089533005775196732








Journal of Economic Perspectives




19


4
















Cognitive models in intelligence research: Advantages and recommendations for their application




G
T
Frischkorn






A.-L
Schubert




10.3390/jintelligence6030034








Journal of Intelligence




6


3














Raven's is not a pure measure of general intelligence: Implications for g factor theory and the brief measurement of g




G
E
Gignac








Intelligence




52


















10.1016/j.intell.2015.07.006


















The






Of






Styles




REASONING 51












Logic, beliefs, and instruction: A test of the default interventionist account of belief bias




S
J
Handley






S
E
Newstead






D
Trippas








Journal of Experimental Psychology. Learning, Memory, and Cognition




37


1


















10.1037/a0021098
















S
J
Handley






D
Trippas




Dual processes and the interplay between knowledge and structure: A new parallel processing model. Psychology of Learning and Motivation -Advances in Research and Theory






62
















10.1016/bs.plm.2014.09.002














Extending multinomial processing tree models to measure the relative speed of cognitive processes




D
W
Heck






E
Erdfelder




10.3758/s13423-016-1025-6








Psychonomic Bulletin & Review




23


5
















Self-reported impulsivity does not predict response caution




C
Hedge






G
Powell






A
Bompas






P
Sumner




10.1016/j.paid.2020.110257








Personality and Individual Differences




167


110257














The speed-accuracy tradeoff: History, physiology, methodology, and behavior




R
P
Heitz




10.3389/fnins.2014.00150








Frontiers in Neuroscience




8














The Reliability and Validity of a Self-Report Measure of Cognitive Abilities in Older Adults: More Personality than Cognitive Function




D
Herreen






I
T
Zajac




10.3390/jintelligence6010001








Journal of Intelligence




6


1














The logic-bias effect: The role of effortful processing in the resolution of belief-logic conflict




S
Howarth






S
J
Handley






C
Walsh




10.3758/s13421-015-0555-x








Memory & Cognition




44


2
















Alternatives to switch-cost scoring in the task-switching paradigm: Their reliability and increased validity




M
M
Hughes






J
A
Linck






A
R
Bowles






J
T
Koeth






M
F
Bunting








Behavior Research Methods




46


3


















10.3758/s13428-013-0411-5


















The






Of






Styles




REASONING 52












The development and validation of the Self-Report Measure of Cognitive Abilities: A multitrait-multimethod study




K
E
Jacobs






J
Roodenburg




10.1016/j.intell.2013.09.004








Intelligence




42
















Intuitive und heuristische Urteilsbildung-verschiedene Prozesse? Präsentation einer deutschen Fassung des "Rational-Experiential Inventory" sowie neuer Selbstberichtskalen zur Heuristiknutzung. [Intuitive and heuristic judgment-different processes? Presentation of a German version of the Rational-Experiential Inventory and of new self-report scales of heuristic use




J
Keller






G
Bohner






H.-P
Erb




















10.1024//0044-3514.31.2.87








Zeitschrift Für Sozialpsychologie




31


2














Hierarchical multinomial processing tree models: A latent-trait approach




K
C
Klauer




10.1007/s11336-009-9141-0








Psychometrika




75


1
















RT-MPTs: Process models for response-time distributions based on multinomial processing trees with applications to recognition memory




K
C
Klauer






D
Kellen








Journal of Mathematical Psychology




82


















10.1016/j.jmp.2017.12.003














On belief bias in syllogistic reasoning




K
C
Klauer






J
Musch






B
Naumer




10.1037/0033-295X.107.4.852








Psychological Review




107


4
















Reasoning ability is (little more than) workingmemory capacity?




P
C
Kyllonen






R
E
Christal




10.1016/S0160-2896








! Intelligence




14


4
















Diffusion modeling and intelligence: Drift rates show both domain-general and domain-specific relations with intelligence




V
Lerche






M
Von Krause






A
Voss






G
T
Frischkorn






A.-L
Schubert






D
Hagemann




10.1037/xge0000774








Journal of Experimental Psychology. General




















The






Of






Styles




REASONING 53












Model complexity in diffusion modeling: Benefits of making the model more parsimonious




V
Lerche






A
Voss




10.3389/fpsyg.2016.01324








Frontiers in Psychology
















Retest reliability of the parameters of the Ratcliff diffusion model




V
Lerche






A
Voss




10.1007/s00426-016-0770-5








Psychological Research




81


3
















Experimental validation of the diffusion model based on a slow response time paradigm




V
Lerche






A
Voss








Psychological Research




83


6


















10.1007/s00426-017-0945-8














Combining speed and accuracy to control for speedaccuracy trade-offs(?)




H
R
Liesefeld






M
Janczyk








Behavior Research Methods






51
















10.3758/s13428-018-1076-x














General performance on a numeracy scale among highly educated samples: Medical Decision Making




I
M
Lipkus






G
Samsa






B
K
Rimer




















10.1177/0272989X0102100105














On interpretation of interactions




G
R
Loftus




10.3758/BF03197461








Memory & Cognition




6


3
















Rule-based reasoning is fast and beliefbased reasoning can be slow: Challenging current explanations of belief-bias and baserate neglect




I
R
Newman






M
Gibb






V
A
Thompson




10.1037/xlm0000372








Journal of Experimental Psychology: Learning, Memory, and Cognition




43


7
















Which working memory functions predict intelligence?




K
Oberauer






H.-M
Süβ






O
Wilhelm






W
W
Wittmann








Intelligence




36


6


















10.1016/j.intell.2008.01.007


















The






Of






Styles




REASONING 54












What makes us think? A threestage dual-process model of analytic engagement




G
Pennycook






J
A
Fugelsang






D
J
Koehler








Cognitive Psychology




80


















10.1016/j.cogpsych.2015.05.001














Commentary: Cognitive reflection vs. calculation in decision making




G
Pennycook






R
M
Ross




10.3389/fpsyg.2016.00009








Frontiers in Psychology
















Dunning-Kruger effects in reasoning: Theoretical implications of the failure to recognize incompetence




G
Pennycook






R
M
Ross






D
J
Koehler






J
A
Fugelsang




















10.3758/s13423-017-1242-7








Psychonomic Bulletin & Review




24


6














The speed-accuracy operating characteristic




R
W
Pew




10.1016/0001-6918(69








Acta Psychologica




30
















Judgment and reasoning in the child (pp. viii, 260). Harcourt, Brace




J
Piaget




10.4324/9780203207260


















The child's conception of geometry (pp. vii, 411)




J
Piaget






B
Inhelder






A
Szeminska












Basic Books








A theory of memory retrieval




R
Ratcliff




10.1037/0033-295X.85.2.59








Psychological Review




85


2
















A diffusion model account of response time and accuracy in a brightness discrimination task: Fitting real data and failing to fit fake but plausible data




R
Ratcliff




10.3758/BF03196283








Psychonomic Bulletin & Review




9


2
















The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks




R
Ratcliff






G
Mckoon








Neural Computation




20


4


















10.1162/neco.2008.12-06-420


















The






Of






Styles




REASONING 55












A diffusion model explanation of the worst performance rule for reaction time and IQ




R
Ratcliff






F
Schmiedek






G
Mckoon








Intelligence




36


1


















10.1016/j.intell.2006.12.002














Individual differences, aging, and IQ in twochoice tasks




R
Ratcliff






A
Thapar






G
Mckoon








Cognitive Psychology




60


3


















10.1016/j.cogpsych.2009.09.001














Effects of aging and IQ on item and associative memory




R
Ratcliff






A
Thapar






G
Mckoon








Journal of Experimental Psychology: General




140


3


















10.1037/a0023810














Manual for Raven's progressive matrices and mill hill vocabulary scales. Advanced progressive matrices




J
C
Raven






J
H
Court






J
Raven








Oxford University Press












Reported Selfcontrol is not Meaningfully Associated with Inhibition-related Executive Function: A Bayesian Analysis




B
Saunders






M
Milyavskaya






A
Etz






D
Randles






M
Inzlicht




10.1525/colla-bra.134








Collabra: Psychology




4


1


39














Individual differences in components of reaction time distributions and their relations to working memory and intelligence




F
Schmiedek






K
Oberauer






O
Wilhelm






H.-M
Süß






W
W
Wittmann




10.1037/0096-3445.136.3.414








Journal of Experimental Psychology: General




136


3
















Modeling mental speed: Decomposing response time distributions in elementary cognitive tasks and correlations with working memory capacity and fluid intelligence




F
Schmitz






O
Wilhelm




10.3390/jin-telligence4040013








Journal of Intelligence




4


4














Trait characteristics of diffusion model parameters




A.-L
Schubert






G
T
Frischkorn






D
Hagemann






A
Voss




10.3390/jintelligence4030007








Journal of Intelligence




4


3




















THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING




56












Disentangling the Effects of Processing Speed on the Association between Age Differences and Fluid Intelligence




A.-L
Schubert






D
Hagemann






C
Löffler






G
T
Frischkorn




10.3390/jintelli-gence8010001








Journal of Intelligence




8


1














Decomposing the relationship between mental speed and mental abilities




A.-L
Schubert






D
Hagemann






A
Voss






A
Schankin






K
Bergmann




10.1016/j.intell.2015.05.002








Intelligence




51
















Individual differences in cortical processing speed predict cognitive abilities: A model-based cognitive neuroscience account




A.-L
Schubert






M
D
Nunez






D
Hagemann






J
Vandekerckhove








Computational Brain & Behavior




2


2


















10.1007/s42113-018-0021-5














Predicting Fluid Intelligence by Components of Reaction Time Distributions from Simple Choice Reaction Time Tasks




Y
Schulz-Zhecheva






M
C
Voelkle






A
Beauducel






M
Biscaldi






C
Klein




10.3390/jintelligence4030008








Journal of Intelligence




4


3














Shipley-2




W
C
Shipley






P
G
Christian






T
A
Martin






A
M
Klein








Western Psychological Services
















Distinguishing the reflective, algorithmic, and autonomous minds: Is it time for a tri-process theory




K
E
Stanovich








Two Minds: Dual Processes and Beyond


K. Frankish & J. S. B. T. Evans




Oxford University Press
















Rationality and the reflective mind




K
E
Stanovich




10.1093/ac-prof:oso/9780195341140.001.0001/acprof-9780195341140








Oxford University Press












Individual differences in reasoning: Implications for the rationality debate?




K
E
Stanovich






R
F
West








Behavioral and Brain Sciences




23


5


















10.1017/S0140525X00003435




















THE ROLE OF COGNITIVE ABILITIES AND THINKING STYLES IN REASONING




57














K
E
Stanovich






R
F
West






M
E
Toplak




The rationality quotient: Toward a test of rational thinking




MIT Press Ltd








1st ed.








The development of linear syllogistic reasoning




R
J
Sternberg




10.1016/0022-0965








Journal of Experimental Child Psychology




29


2
















Representation and Process in Linear Syllogistic Reasoning




R
J
Sternberg








Journal of Experimental Psychology: General




109


2
















Conflict, metacognition, and analytic thinking




V
A
Thompson






S
C
Johnson








Thinking & Reasoning




20


2


















10.1080/13546783.2013.869763














Do smart people have better intuitions




V
A
Thompson






G
Pennycook






D
Trippas






J
Evans






B T
St




10.1037/xge0000457








Journal of Experimental Psychology: General




147


7
















Intuition, reason, and metacognition




V
A
Thompson






J
A
Turner






G
Pennycook








Cognitive Psychology




63


3


















10.1016/j.cogpsych.2011.06.001














The cognitive reflection test as a predictor of performance on heuristics-and-biases tasks




M
E
Toplak






R
F
West






K
E
Stanovich




10.3758/s13421-011-0104-1








Memory & Cognition




39


7


1275














Assessing miserly information processing: An expansion of the Cognitive Reflection Test




M
E
Toplak






R
F
West






K
E
Stanovich




10.1080/13546783.2013.844729








Thinking & Reasoning




20


2
















Characterizing belief bias in syllogistic reasoning: A hierarchical Bayesian meta-analysis of ROC data




D
Trippas






D
Kellen






H
Singmann






G
Pennycook






D
J
Koehler






J
A
Fugelsang






C
Dubé




10.3758/s13423-018-1460-7








Psychonomic Bulletin & Review




25


6




















The






Of






Styles




REASONING 58












When fast logic meets slow belief: Evidence for a parallel-processing model of belief bias




D
Trippas






V
A
Thompson






S
J
Handley




10.3758/s13421-016-0680-1








Memory & Cognition




45


4
















Is working memory capacity task dependent?




M
L
Turner






R
W
Engle




10.1016/0749-596X(89








Journal of Memory and Language




28


2
















Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions




R
Ulrich






H
Schröter






H
Leuthold






T
Birngruber








Cognitive Psychology




78


















10.1016/j.cogpsych.2015.02.005














Self-report measures of prospective memory are reliable but not valid




B
Uttl






M
Kibreab




10.1037/a0022843








Canadian Journal of Experimental Psychology = Revue Canadienne De Psychologie Experimentale




65


1
















Cognitive psychology meets psychometric theory: On the relation between process models for decision making and latent variable models for individual differences




H
L J
Van Der Maas






D
Molenaar






G
Maris






R
A
Kievit






D
Borsboom




10.1037/a0022749








Psychological Review




118


2
















The EZ diffusion model provides a powerful test of simple empirical effects




D
Van Ravenzwaaij






C
Donkin






J
Vandekerckhove




10.3758/s13423-016-1081-y








Psychonomic Bulletin & Review




24


2
















A cognitive latent variable model for the simultaneous analysis of behavioral and personality data




J
Vandekerckhove








Journal of Mathematical Psychology




60


















10.1016/j.jmp.2014.06.004














A comparison of methods to combine speed and accuracy measures of performance: A rejoinder on the binning procedure




A
Vandierendonck




10.3758/s13428-016-0721-5








Behavior Research Methods




49


2




















The






Of






Styles




REASONING 59












Do Non-Decision Times Mediate the Association between Age and Intelligence across Different Content and Process Domains




M
Von Krause






V
Lerche






A.-L
Schubert






A
Voss




10.3390/jintelli-gence8030033








Journal of Intelligence




8


3


33














Diffusion models in experimental psychology: A practical introduction




A
Voss






M
Nagler






V
Lerche








Experimental Psychology




60


6


















10.1027/1618-3169/a000218














Fast-dm: A free program for efficient diffusion model analysis




A
Voss






J
Voss




10.3758/BF03192967








Behavior Research Methods




39


4
















Assessing cognitive processes with diffusion model analyses: A tutorial based on fast-dm-30




A
Voss






J
Voss






V
Lerche




10.3389/fpsyg.2015.00336








Frontiers in Psychology




6














AIC model selection using Akaike weights




E.-J
Wagenmakers






S
Farrell




10.3758/BF03206482








Psychonomic Bulletin & Review




11


1
















On the interpretation of removable interactions: A survey of the field 33 years after Loftus




E.-J
Wagenmakers






A.-M
Krypotos






A
H
Criss






G
Iverson




10.3758/s13421-011-0158-0








Memory & Cognition




40


2
















Why Self-Report Measures of Self-Control and Inhibition Tasks Do Not Substantially Correlate




L
Wennerhold






M
Friese




10.1525/collabra.276








Collabra: Psychology




6


1














Testing the validity of conflict drift-diffusion models for use in estimating cognitive processes: A parameter-recovery study




C
N
White






M
Servant






G
D
Logan




















10.3758/s13423-017-1271-2








Psychonomic Bulletin & Review




25


1


















The






Of






Styles




REASONING 60












What is working memory capacity, and how can we measure it? Frontiers in Psychology




O
Wilhelm






A
H
Hildebrandt






K
Oberauer




10.3389/fpsyg.2013.00433


















Reconceptualizing the Transitive Inference Ability: A Framework for Existing and Future Research




B
C
Wright








Developmental Review




21


4


















10.1006/drev.2000.0525














Factors and processes in children's transitive deductions




B
C
Wright






J
Smailes








Journal of Cognitive Psychology




27


8


















10.1080/20445911.2015.1063641















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]