You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
A large body of decision-making research suggests that humans learn from experience the absolute expected value of different available options 
(Kable and Glimcher, 2009;
Montague and Berns, 2002;
Padoa-Schioppa, 2011;
O'Doherty, 2014;
Levy and Glimcher, 2012)
. That is, as people try out different options and observe their reward outcomes, it is thought that they track the average reward associated with each option, and this allows them to choose options with higher expected value. The main benefit of this so-called value learning is that it makes it easy to choose from any set of options, including options that have not been previously considered in relation to one another, simply by comparing their absolute expected values. In this sense, expected values serve as a "common currency" that serves to generalize preferences across contexts that offer different combinations of options 
(Mongillo et al 2014
, Hayden Niv et al 2020
, Bennett et al 2021
. Evidence that value learning is implemented by the brain emerged from early foundational work on primates indicating that brainstem dopaminergic neurons instantiate prediction errors -differences between actual and expected reward -that are well suited for algorithmic implementations of value learning 
(Schultz et al 1997)
. Since then, many human brain imaging studies have shown that activation in the orbitofrontal cortex (OFC) and other regions is correlated with expected value during reward learning and other types of economic decision-making tasks (O'Reilly 2020; 
Bartra et al., 2013;
Levy & Glimcher, 2012)
.
New evidence, however, has cast into question whether humans indeed learn absolute expected values or may be instead learning relative preferences among limited sets of options. Two recent studies showed that people's choices reflect relative preferences because when they are rewarded for choosing one out of two options, they do not only form a preference in favor of the option they chose, but also a preference against the option they did not choose 
(Li & Daw 2011;
 Biderman and Shohamy 2021). Neural data reveal a similar picture. Neural firings in areas considered to encode value such as the OFC and the striatum have been found to encode normalized values that, in fact, have no absolute meaning and can only be interpreted as relative preferences compared to other options sampled in the same context 
(Zimmerman 2018
, Padoa-Schioppa 2009
. Such relative preference encoding is evident even when each individual option is encountered separately 
(Azab and Hayden, 2018;
Hunt et al., 2018)
. These studies among others 
(Vlaev et al., 2011;
Gigerenzer and Gaissmaier, 2011;
Lichtenstein and Slovic, 2006)
, have led researchers to propose alternative learning models, according to which humans learn preferences between options without encoding the absolute value of each option (Lowenstein et al 2014, 
Niv 2020, Bennett et al 2021)
.
Here, we test a novel hypothesis that humans flexibly adapt the degree to which they form absolute expected values and relative preferences based on the opportunities and incentives afforded by the environment. It is well established, across a wide range of machine learning applications, that learning environments that provide a more diverse set of learning exemplars aid generalization of learned information to new input patterns and unfamiliar contexts 
(Gong et al 2018
, Lee et al 2018
. In the case of learning to maximize reward, the set of exemplars corresponds with the set of possible options, and learning about a broader set of options could make it more clearly evident that the value of an option does not depend on the other options it is pitted against -that is, that each option has an absolute value. Additionally, the broader the set of options, the greater is the space of possible choice sets (i.e., a choice set is a set of simultaneously available options from which one chooses), some of which have yet to be encountered. The prospect of having to choose among unfamiliar sets of familiar options makes it worthwhile to form preferences that can be used to choose among such sets, which is precisely what absolute values are best suited for. By contrast, learning relative preferences produces suboptimal choices among unfamiliar choice sets, since it only keeps track of how valuable options were relative to the other options they were previously pitted against 
(Li and
Daw 2011, Bavard et. al 2021)
.
These considerations suggest at least two types of training diversity may support and incentivize value learning. The first relates to how many options are offered by the immediate learning context, which determines the number of options a person learns about concurrently (henceforth, concurrent diversity). The second is the number of options each option is cumulatively pitted against across the entire course of learning (henceforth, cumulative diversity). These two types of diversity are dissociable since an option can be learned concurrently with fewer other options, yet across multiple learning sessions it may be cumulatively pitted against a larger number of other options.
To test the impact of concurrent and cumulative diversity on the formation of absolute values, we designed a novel multi-day reward learning experiment comprising twenty learning sessions, with consecutive sessions separated by twelve hours on average. In each session, subjects' goal was to maximize their reward by choosing among pairs of images, each of which was associated with a fixed probability of reward. The probabilities were not told to subjects and could only be learnt through trial and error, by observing the reward outcomes of chosen images. To manipulate concurrent diversity, we varied how many images subjects learned about concurrently in each learning session. To manipulate cumulative diversity, we varied the total number of other images each image was pitted against over two separate learning sessions.
Critically, the multi-session design allowed us to assess the formation of absolute values by asking subjects to choose between images that were never directly paired together during learning. To enhance the distinction between absolute values and relative preferences, we had images with the same reward probability (i.e., equal absolute value) learned against other images 6 with mostly lower reward probabilities (i.e., high relative value) or mostly higher reward probabilities (i.e., low relative value). An absolute value learner would have no preference among these images, whereas a relative preference learner would prefer the option that ranked higher in its original learning context.


Results
27 subjects (ages to 30; Mean=24 ±.5) completed two learning sessions a day of a reward learning task over a period of ten days 
(Figure 1
). In low-concurrent-diversity sessions, subjects learned about three images within a single session, whereas high-concurrent-diversity sessions involved learning about six images within a single session. Every image appeared in two learning sessions, but low-cumulative-diversity images were pitted against the same images in both sessions whereas high-cumulative-diversity images were pitted against different images. On each trial, subjects were asked to choose one of two circular images. Following their choice, subjects either received or did not receive a reward of 1 coin based on a fixed reward probability associated with the chosen image (0, 1/3, 2/3, or 1). Each game consisted of 48 such 'learning' trials, interleaved with 24 'testing' trials wherein subjects chose between images about which they learned in prior sessions. Outcomes were not revealed in testing trials to prevent further learning. Every image first appeared in 64 learning trials over two sessions before subjects were tested on it. ITI: inter-trialinterval. (B) Daily schedule. Each day, subjects performed two experimental sessions on a specially designed mobile phone app 
(Eldar et al., 2018)
, one in the morning (on average, at 8:56 am, and no earlier than 6:00 am) and one in the evening (on average, at 6:56 pm, and no earlier than 4:00 pm). In each session, subjects played two games in which they learned about a total of six images. (C) Experimental conditions. Four experimental conditions were implemented along 10 days of learning, each lasting two to three days. Each condition is illustrated via a representative selection of six pairs of images subjects chose between in two different sessions. Games with low concurrent diversity included three images (left two columns) whereas games with high concurrent diversity included six images (right two columns). In the low cumulative diversity condition, each image was pitted against the same two images in two learning sessions, whereas in the high cumulative diversity condition, images were pitted against different images in the two learning sessions. Three days of training, as opposed to two, were required for highcumulative-diversity conditions so that each image could be pitted against different images in its two learning sessions (see Methods for details). Conditions were randomly ordered, and equal in terms of average reward probability and number of images learned per session. Testing trials involved choosing between two images the subject already chose between during learning (Learnt-Pair Trials) or novel pairings of images learned separately (Novel-Pair Trials). Solid color circles are merely for demonstration, actual stimuli were distinguishable abstract images as shown in panels (A) and (B).


Subjects formed preferences in favor of more rewarding images
To validate our task, we first determined whether subjects successfully learned to choose images associated with higher reward probabilities. Subjects' choices showed that they indeed tended to choose the more rewarding image out of each pair, doing so in 87% (SEM ±1%) of trials ( 
Figure 2A
; chance performance = 50%). As can be expected, subjects' performance was lower in the conditions that required learning about more images (i.e., high concurrent diversity; 
Figure    2b
, left panel) or about more pairs of images (i.e., high cumulative diversity; 
Figure 2b
, right panel). However, subjects performed considerably above chance in all conditions, and showed gradual improvement as they tried out choosing the different images and observed their outcomes ( 
Figure 2c
). These results confirm that the task was effective in getting subjects to form preferences among images based on how often each image was rewarded. A choice was considered accurate if the subject selected the image with the higher reward probability. Subjects performed significantly above chance (50%) in all trial types and conditions (t(51.8) p<.0001). The plot shows total (vertical lines) and interquartile (boxes) ranges and medians (horizontal lines). Also shown are mean accuracies predicted by a computational model that was fitted to subjects' choices (circles; see details under Computational Formalization below). B) Effect of training diversity on accuracy during learning. Accuracy was higher in sessions with low (91% SEM ±1%) compared to high (87% SEM ±1%) concurrent diversity (p=.024, bootstrap test), and trended higher in sessions with low (90% SEM ±1%) compared to high (88% SEM ±1%) cumulative diversity (p=.078, bootstrap test). The plot shows individual subject accuracy (circles), group distributions of accuracy levels (violin), group means (thick lines) and standard errors (gray shading). C) Learning curves for each experimental condition. Accuracy in trials involving a given image as a function of how many trials the image previously appeared in. A drop-off in accuracy can be observed for high-cumulative-diversity images (dark) at the beginning of the second session, because these images were pitted against new images. The plot shows group means (circles), standard errors (vertical lines), local polynomial regression lines 
(Cleveland et al 1979;
curves)
 and confidence intervals (shading). Given that performance was always above chance (50%), y-axes in panels B and C focus on this range.


Concurrent diversity increased generalization
A hallmark of value learning is the ability to generalize learned preferences to novel settings. To test generalization, we had subjects choose between images that had not been previously pitted against each other ('novel-pair' testing trials). We compared subjects' accuracy on these trials to accuracy in choosing between images that subjects had encountered during learning ('learnt-pair' testing trials).
We found that subjects successfully chose the image with the higher reward probability in 83% of novel-pair trials (SEM ±2%; chance performance = 50%). This level of accuracy, however, was significantly lower than the accuracy subjects demonstrated on learnt-pair trials (Mean = 87% SEM ±3%; bootstrap p=.02). The performance difference cannot be explained by differences in the relative timing of the two types of trial, since learnt-pair and novel-pair trials were interleaved throughout the experiment and similarly separated in time from learning (on average, 1.8 days ±.06 following learning). Thus, this result shows that subjects generalized their preferences well, but did not do so perfectly.
We thus asked whether success in generalization was affected by the diversity of learning experiences. Strikingly, we found that for images about which subjects learned in conditions of high concurrent diversity, there was no significant difference in accuracy between novel-pair and learnt-pair trials 
(Figure 3
; Mean= -1% ±2%; p=.34, bootstrap test). Conversely, for lowconcurrent-diversity images, subjects performed substantially worse in choosing between novel pairs (Mean=-7% ±2%; p=.004, bootstrap test).
In contrast, cumulative diversity did not impact the performance drop-off from learnt-pair to novel-pair trials (-3% ±2% vs -4.8% ±2% p=.29 bootstrap test). This latter result is especially surprising given that pairs of images from conditions of high cumulative diversity were encountered during learning half as many times as pairs of images in low-cumulative-diversity conditions. For this reason, we expected that learnt-pair performance would be compromised by cumulative diversity, and this was indeed the case (as evident by comparing accuracy on learntpair trials to the level of accuracy achieved at the last 5 trials of learning; Meanlow=-2.3% SEM ±1%, Meanhigh=-4.2% ±1%, p=.03 bootstrap test). However, accuracy on novel-pair trials was similarly compromised by cumulative diversity (Meanlow=-5.6% ±2%, Meanhigh=-7.9% ±2%, p=.026 bootstrap test), and thus no benefit to generalization was observed.
These results show that increasing the number of options about which a person concurrently learns improves their ability to generalize their learned preferences to novel choice sets. 
Figure 3
. Generalization performance. n = 27 subjects. Drop-off in accuracy in novel-pair compared to learnt-pair testing trials, as a function of training diversity. Concurrent diversity had a significant effect on this measure of generalization (p=.004, bootstrap test) whereas cumulative diversity did not significantly affect it (p=.23, bootstrap test). The plot shows individual subject accuracy (circles), group distributions of accuracy levels (violin), group means (thick lines) and standard errors (gray shading).


Concurrent diversity reduces influence of other options' outcomes
The observed improved generalization suggested that concurrent diversity enhanced absolute value learning. To further investigate this possibility, we examined another key consequence of absolute value learning, namely, that the preferences it forms depend only on the available images' prior outcomes. By contrast, relative preferences also account for the outcomes of other images against which the presently available images were pitted during learning. These latter outcomes determined how each image compared with other images during learning. Accounting for these outcome means that a relative preferences learner would tend to favor similarly rewarded images that ranked higher in their original learning context. Examining subjects' choices between similarly rewarded images from low-concurrent-diversity conditions (i.e. less than 10% difference) showed that subjects indeed preferred images that ranked higher during learning (i.e., that were pitted against images with lower reward probabilities; Meanlow=63% SEM ±6%, p=006, with 50% representing no preference between images). This, however, was not the case for images from high-concurrent-diversity conditions.
Preference among the latter images was not affected by other images' outcomes during learning (Mean high==49% SEM ±5%, p =.24). Here too, no difference in rank bias was found as a function of cumulative diversity (Mean low=57% ±7% p=06; Mean high=48% ±8% p=.20) .
One problem interpreting a rank bias is that it may be confounded by the fact that higher ranking images were chosen more during learning. If an image is pitted against images associated with lower probabilities of reward, then it will naturally be chosen more and thus more outcomes will be observed for it. Subjects may therefore develop greater confidence regarding the value of higher-ranking images. To control for this confound, we used a Bayesian logistic mixed model that predicted subjects' choices based on differences between the currently available images' own reward history ( !"# ; i.e., proportion rewarded), the number of times each image was chosen ( $%&'( *+,('# i.e., sampling bias), and the reward history from trials in which the currently available images were rejected in favor of other images. The latter was separated into two separate regressors, for prior outcomes of rejecting a presently available image in favor of the other presently available image ( *-..'#/ 01/'.#0/%2' ) and of rejecting it in favor of any other image ( !/+'. ). Importantly, only the latter regressor unequivocally captures the effect of other images' outcomes that are irrelevant for inferring absolute expected value. To determine whether the effects of different types of outcomes was modulated by training diversity, we included regressors for concurrent and cumulative diversity and interactions between both types of diversity and each type of reward history.
The results again showed that in addition to the strong impact of an image's own reward history  Y-axis shows the percent of trials involving similarly rewarded images in which subjects chose the image that ranked higher during learning. The plot depicts individual subjects' choices (circles), group distributions (violin), group means (thick lines) and standard errors (gray shading). B) Effect of reward histories on choice. The plot shows the log odds effect on choice of three types of reward. Own: differences in reward history of currently available images. Current alternative: differences in reward history when rejecting one available image in favor of the other available image. Other: differences in reward history when rejecting one of the available images in favor of any other image.
( !"# =1.97 [1.63, 2.31]),


Computational formalization of value and preference learning formalization
Our results evidenced signs of both absolute value and relative preference learning. On one hand, subjects successfully learned reward maximizing choices and generalized well to novel choice sets, as consistent with absolute value learning. On the other hand, subjects performed still better at choosing among familiar choice sets, and they preferred images that were relatively more valuable in their original learning context, as consistent with relative preference learning.
Critically, learning about more images concurrently diminished or even eliminated the signs of relative preference learning.
We next tested whether this set of results be coherently explained as reflecting the operation of two learning processes -absolute value and relative preference learning -the balance between which changes as a function of concurrent diversity. To do this, we fitted subjects' choices during both learning and testing with a computational model that combines value and preference learning. We then examined the best-fitting values of the model's parameters to determine the degree to which value and preference learning were each employed in each experimental condition.
To formalize absolute value learning, the model represents subject beliefs about the absolute values of images as beta distributions, defined by two parameters 6 and 6 . This beta distribution represents the reward probability that is believed to be associated with each image given the outcomes obtained for choosing it. Thus, 6 and 6 accumulate the number of times that choice of image was rewarded and not rewarded:
6 ← 6 + (equation 1) 6 ← + (1 − ) (equation 2)
where = 1 if the choice was rewarded and = 0 if it was not. Here, serves as a leak parameter allowing for the possibility that more recent outcomes have a greater impact on subjects' beliefs ( = 1 entails that outcomes are equally integrated, whereas < 1 entails overweighting of recent outcomes). The key decision variable provided by this form of learning is the absolute value ( ) of image , which is estimated as the expected value of the image's beta distribution:
( ) = 7 ! 7 ! 89 ! (equation 3)
To isolate the key computation distinguishing relative preference learning from value learning, we use the same learning rules defined above to generate a relative preference ( , ), for image over image , that accounts for all outcomes observed for choosing between the two images. When applying Eqs. 1 and 2 for preference learning, = 1 if image was chosen and rewarded or if image was chosen and not rewarded, and = 0 if image was chosen and rewarded or if image was chosen and not rewarded. To enable preference learning to exhibit preferences among previously unencountered pairs of images, a general relative preference ( ) was computed for each image by accumulating in the same fashion the outcomes of choosing image or the other image, across all learning trials involving image .
Accounting for the outcomes of the other images distinguishes ( ) from ( ), making it an index of relative preference as opposed to absolute value.
When facing a choice between image and image , the probability that the model will choose either image is computed based on a weighted sum of the images' absolute value and relative preference:
P(choice = ) = : " #$%&' ((!)+" ,-'.'-'/0' 1 2 (!)
: " #$%&' ((!)+" ,-'.'-'/0' 1 2 (!) 8:
" #$%&' ((3)+" ,-'.'-'/0' 1 2 (3) (equation 4)
where ; ( ) is itself a weighted sum of ( ) and ( , ), with a free parameter controlling their relative weights. Importantly, the inverse temperature parameters for value and preference learning, 201-' and <.'='.'#4' , determine the degree to which absolute values and relative preferences influence choice. Thus, the values of these parameters that best fit subjects' choices can be used to quantify the degree to which value and preference learning manifested in each experimental condition 
(Findling et al 2019
, Daw et al 2006
. To this end, we allowed the two inverse temperature parameters to vary as a function of concurrent and cumulative diversity, for either learning or testing trials.


Subjects combine value and preference learning
To determine whether a combination of value and preference learning was needed to explain subjects' choices, we compared the full model to two sub-models, one that only learns absolute values ( <.'='.'#4' = 0) and one that only learns relative preferences ( 201-' = 0), as well as to a number of additional alternative learning models (see methods pg. 38). We found that the full model accounted for subject choices significantly better than the alternative models (figure 5).
Moreover, posterior predictive checks showed that our full model uniquely accounted for all the behavioral findings, including generalization performance, effect on choice of outcomes for other images, and rank bias (see 
figure S1
)


Concurrent diversity enhances value learning
Examining the values of the parameters that best-fitted subjects' choices showed that value learning generally predominated over relative preference learning (β <.'='.'#4' =4 ±.28 vs  


Discussion
We find that increasing the number of options a person concurrently learns about shapes reward learning in several ways. It first reduces performance during learning, but then leads to more successful generalization, removes a bias in favor of options that ranked higher during learning, and generally decreases the degree to which preference for an option is influenced by presently learning is promoted by a more limited set of options which reduces the likelihood of encountering novel choice sets. In this sense, the shift between preference and value learning in our experiment can be thought of as a rational adaptation. This perspective is supported by a very recent finding that value learning is enhanced by expectations of having to choose between options from different learning contexts 
(Juechems et al 2021)
. Importantly, we demonstrate that concurrent diversity is sufficient to promote absolute value learning even in the absence of a direct manipulation of subjects' expectations of needing to choose across contexts. Conversely, in conditions of low concurrent diversity, subjects display relative preference learning despite being aware of the need to choose among options from different learning contexts.
Our findings agree with prior work showing that emphasizing comparisons between a limited number of specific images, for instance by repeatedly presenting subjects with a choice between the same two options and providing reward information about the foregone option, promotes learning of relative values 
(Bavard et al 2021)
. Importantly, however, the process by which the formation of relative values in the latter experiments has so far been explained -namely, normalization to the range of outcomes experienced during learning -cannot explain relative preference learning in our experiment. This is because the range of outcomes in our experiments was the same in all learning sessions. By contrast, the model we proposed here for relative preference learning may coherently account for both our findings and the findings that had previously been attributed to normalization. 


Conclusion
Our findings contribute to the ongoing debate concerning the extent to which people learn absolute values versus relative preferences. We show that absolute value learning depends on a characteristic of the immediate learning context, namely, the diversity of learning experiences it offers. We find that increased diversity, despite impairing performance in the short term, has the effect of enhancing learning of absolute values which generalize well to novel contexts. Such generalization is essential for making decisions in real-world settings where we must rely on limited experience in each of many different contexts.


Contact for resource sharing
Further information and requests for resources or raw data should be directed to and will be fulfilled by the Lead Contact, Levi Solomyak (levi.solomyak@mail.huji.ac.il).


Methods


Subjects
27 human subjects (14 male,13 female), aged 20 to 30 (Mean=24 SEM ±.5), completed the experiment which consisted of 3556 trials. Given the size of the dataset obtained for each subject (an order of magnitude greater than in typical learning experiments) and the effect size found in similar prior literature 
(Bavard et al 2021)
, we expected that a meaningful finding would manifest as at least a large effect 
(Cohen's D=.8;
Cohen 1988)
. We thus selected a sample size that would provide at least 80% power of detecting such an effect (i.e., n>=26). The experiment was discontinued midway for 3 additional subjects due to failure to complete learning sessions or evidence of random choosing. Subjects were recruited from a subject pool at Hebrew University of Jerusalem as well as from the Jerusalem area. Before being accepted to the study, each subject was queried regarding each of the study's inclusion or exclusion criteria.
Inclusion criteria included fluent Hebrew or English and possession of an Android smartphone that could connect to wearable sensors via Bluetooth Low Energy. Exclusion criteria included age (younger than 18 or older than 40), impaired color discrimination, use of psychoactive substances (e.g., psychiatric medications), and current neurological or psychiatric illness.
Subjects were paid 40 Israeli Shekels (ILS) per day for participation and 0.25 ILS for each coin they collected in the experimental task, which together added up to an average sum of 964 ±42
ILS over the entire duration of the study. The experimental protocol was approved by the Hebrew University local research ethics committee, and informed consent was obtained from all subjects.
Subjects who missed two sessions of the experiment or who displayed patterns of making random choices were automatically excluded from the study. Random choosing was indicated by chance-level performance or by reaction times below 1400 ms which indicated that subjects were giving no thoughts to their response.


Experimental design
To test for value and preference learning, we had subjects perform a trial-and-error learning task over a period of 10 days. On each trial, subjects chose from one of two available images, and then collected a coin reward with a probability associated with the chosen image. Each game consisted of 48 such trials involving a set of 3 images with reward probabilities of either {0,
.33,.66} or {.33, .66 and 1}. These probabilities were never revealed to the subjects. Subjects were only instructed that each image was associated with a fixed probability of reward. Subjects played four games a day, two in a morning session and another two in an evening session.
To assess whether concurrent or cumulative diversity promotes absolute value learning, we tested subjects on four experimental conditions involving either low or high levels of each type of diversity. Task conditions were randomly ordered across days in order to avoid confounds related to fatigue or gradual improvement in learning strategy. Images learned in low cumulative diversity conditions were learned over the span of two consecutive days. To satisfy the constraints of high cumulative diversity concerning which images are pitted against which, high cumulative diversity conditions spanned three days (see below). All four conditions yielded the same expected payout, since the average reward probability associated with images within each condition was .5.
To enhance the distinction between absolute values and relative preferences, we had images with the same absolute value (i.e., equal reward probability) learned against other images with mostly lower reward probabilities (i.e., in games where the probabilities were {0, .33,.66}; low reward context) or mostly higher reward probabilities (i.e., in games where the probabilities were {.33, .66 and 1}; high reward context).


Concurrent diversity
Low: In these conditions, the two games within each learning session were independent of one another, each involving a distinct set of three images, with each trial randomly pairing two of the three images. Thus, the number of images subjects had to concurrently track in these conditions was limited to three.
High: Each session involved a set of six images, all of which were encountered in both games.
Consequently, in these conditions subjects had to concurrently track six images. To equalize low and high concurrent diversity conditions in terms of the number of images each image was pitted against within each game (two), as well as in terms of the total number of pairs of images between which subjects chose within each session, the six images formed only six different pairs.
To enhance the impact of high concurrent diversity, unlike in the low concurrent diversity condition, images that were pitted against each other never had a common image that they were both pitted against 
(Table 1)
. 


12
.33 10 and 11 Never


Cumulative diversity
Low: Every image was pitted against the same two other images in two consecutive learning sessions. Thus, in total, subjects chose between each pair of images 32 times. In total, subjects learned about twelve images in the span of two days.
High: Every image was pitted against two different pairs of images in two different learning sessions (i.e., against a total of four other images). Thus, over the same number of trials involving the same number of images, subjects encountered twice as many image pairs compared to the low cumulative diversity condition. Correspondingly, subjects chose between each pair of images 16 times. Implementing these criteria made it impossible to have subjects learn about twelve images in the span of two days. Thus, to appropriately pair the different images, in this condition we had subjects learn about 18 images across the span of 3 days 
(Table 2)
. To ensure that the opportunity to learn relative preference was not hindered by a change in reward context midway through learning, reward context was always the same (i.e., either high or low) in both learning sessions of a given image. 463 
Table 2
: Example arrangement of images across days in conditions of high cumulative diversity. Each number corresponds to an image subjects learned about. Brackets group images that were pitted against each other. A) Low concurrent. Each game consisted of three images which were all pitted against each other. B) High concurrent. Each game consisted of six images, but every image was only pitted against two other images across trials. To assess the formation of absolute values, we had subjects choose between images about which they had learned in two previous sessions 
('testing' trials)
. Through the entire course of learning, learning trials (trials in which subjects could see if they collected a coin or not) were interleaved with testing trials (every 3rd trial, with no reward feedback provided). Testing trials presented subjects with choices between pairs of images between which subjects had already chosen during learning ('Learnt Pair Trials') or novel pairs of images about which subjects learned separately ('Novel Pair trials'). Unlike learnt pair trials, which always involved images of differing expected values, some novel pair trials (on average 22% of all testing trials) presented a choice between two images with the same expected value, and these choices were not used in measuring choice accuracy. On the final day of the experiment (day 11), all four games consisted only of testing trials, involving images drawn from all conditions. In the first two days, as subjects were still learning about the first sets of images, testing trials involved 'dummy' images from an initial training session performed in the lab. Thereafter, familiar images were those subjects learned about during the week.


A) Low concurrent


Mobile platform
To test learning across multiple well-separated sessions, we modified an app developed by Eldar which are regularly uploaded via the phone's data connection to a dedicated cloud.


Daily schedule
Subjects first visited the lab to receive instructions, test the app on their phones, and try out the experimental task (see Initial lab visit section below). Starting from the next day, subjects performed two experimental sessions a day, one in the morning and one in the evening, over a period of 10 consecutive days followed by a rest day, and a final day of testing. Each session began with a 5-minute heart rate measurement during which subjects were asked to remain seated. Following this, subjects put on the EEG sensor and played two games of the experimental task. The app allowed subjects to perform the morning session from either 6AM, 7AM, 8AM or 9AM, as best fitted the subject's daily schedule, and the evening session from 8 hours following this time. Subjects were allowed to adjust the timing of the sessions according to their daily schedule but were required to ensure a gap of at least 6 hours between successive sessions. On average, subjects performed the morning session at 8:56AM (mean SD ± 40 min) and the evening session at 6:12 PM (mean SD± 32 min). Subjects who were religiously observant were allowed to suspend the experiment due to holiday observance as long as they resumed it the following day. As part of a larger data collection effort, subjects were also asked to report their mood prior to playing each game as well as twice more throughout the day.


Materials
The experiment involved 60 images, which were abstract patterns collected from various internet sources. To ensure that images were sufficiently distinguishable from one another we ran a structural similarity analysis that assesses the visual impact of three characteristics of images:
luminance, contrast, and structure 
(Zhou et al 2004)
. We considered as sufficiently distinguishable images with a similarity index of at most .6, and this was verified by visual inspection.


Regression analyses
Regression analyses were performed in R using RStudio. All results were analyzed with generalized linear models using the "brms" package, which performs approximate Bayesian inference of regression coefficients using Hamiltonian Monte Carlo estimation. We used default priors and sampled two chains of 10000 samples each. 1000 samples per chain were used as warm-up. To ensure convergence, we required an effective sample size of at least 10000 and a Rhat statistic of at most 1.01 for all regression coefficients. To evaluate an effect of interest, we report the median of the posterior samples of the relevant regression coefficient and their 95%
high density interval (HDI). A reliable relationship is said to exist between a predictor and an outcome if the 95% HDI excludes zero.


Examining rank bias
To calculate whether subjects preferred images ranked higher in the original learning context, we calculate each subject's ranking of each image based on how many times they chose the image relative to the other images it was pitted against (best, second-best, or worst). If the difference in choice frequency between two images that were pitted against each other was below 10%, indicating no established ranking between them, then both images were excluded from the analysis (8% of trials). These rankings were then averaged across the two learning sessions in which an image was learned about to generate an overall rank for the image. Finally, we tested for a ranking bias by examining subjects' choice between differently ranked but similarly rewarded images (defined as less than a 10% difference in percent rewarded outcomes). Bias was defined as a tendency to choose the image that had been ranked higher during learning.


Examining the influence of other options' outcomes
To determine whether training diversity modulated the influence of other options' outcomes that were irrelevant for computing presently available images' absolute, we used a Bayesian logistic mixed model predicting subjects' choices. The predictors are described below for a choice between image A and image B:
1) -Reward history of the currently available images in the current pair, computed as the proportion of rewarded trials when option A was chosen versus any image minus the proportion of rewarded trials when option B was chosen versus any image.
2) -Reward history of the currently available images in the previous times the same images were pitted against each other, computed as the proportion of rewarded trials when option A was rejected in favor of option B minus the proportion of rewarded trials when option B was rejected in favor of option A.
3) ℎ -Reward history of choosing against the currently available images, computed as the proportion of rewarded trials when option A was rejected versus any image other than B minus the proportion of rewarded trials when option B was rejected versus any image other than A.


4)
ℎ -Computed as the number of trials in which option A was selected out of all trials that included option A minus the number of trials when option B was selected out of all trials that included option B.


5)
-concurrent diversity condition.
6) -cumulative diversity condition.


7)
All two-way interactions between each type of reward history and each type of diversity condition:
× , × , × ℎ , × , × , × ℎ .
The probability of choosing image i over image was thus modelled as: 
P
+ *,#4-..'#/ × !"# × 6?@ + *,#4-..'#/ × *-..'#/ 01/'.#0/%2' × 6?@ + *,#4-..'#/ × !/+'. × ℎ 6?@ + *-&-10/%2' × !"# × 6?@ + *-&-10/%2' × *-..'#/ 01/'.#0/%2' × 6?@ + *-&-10/%2' × !/+'. × ℎ 6?@ ) (equation 6)
where represents the logistic function. To account for between-subject variation, we included random intercepts as well as random slopes for all predictors.
#$%&' ,-'.'-'/0' × %'$-/7/8 9':97/8
is a ratio that represents the relative influence of value and preference learning on choice differs in learning and testing trials. The more this ratio diverges from one, the more preference and value learning are differentiated in the sense that one algorithm influences choices more during learning trials and the other algorithm influences choices more during testing trials.
Using these main effect and interaction parameters, <.'='.'#4' and 201-' can be computed for each trial type. Thus, for example, in a testing trial of low concurrent but high cumulative diversity we can calculate the inverse temperature for the value and preference algorithm as follows:
<=>?>=>@A>B <=>?>=>@A>CDEF>GH@>
GIJ KHLK AI@AM==>@N G>E=@H@L N>FNH@L GIJ KHLK AMOMGENHP> GIJ KHLK AMOMGENHP> × PEGM> <=>?>=>@A> PEGM> <=>?>=>@A> × G>E=@H@L N>FNH@L GIJ KHLK AMOMGENHP>× G>E=@H@L N>FNH@L GIJ KHLK AI@AM==>@N × PEGM> <=>?>=>@A> GIJ KHLK AI@AM==>@N× G>E=@H@L N>FNH@L PEGM>B PEGM>CDEF>GH@> R !"# $%&$ (")(*++,)- R !,.+)%)& -,/-%)& R !"# $%&$ (*0*!.-%1, R !"# $%&$ (")(*++,)-× 1.!*, 3+,4,+,)(, R !"# $%&$ (*0*!.-%1,× !,.+)%)& -,/-%)& R !"# $%&$ (*0*!.-%1, × 1.!*, 3+,4,+,)(, R 1.!*, 3+,4,+,)(, × !,.+)%)& -,/-%)& R !"# $%&$ (")(*++,)-× !,.+)%)& -,/-%)& (equation 7)
Alternative models
To identify the computations that guided subjects' choices we compared the model presented in the main text to several variations of this model, in terms of how well each fitted subjects'
choices. These included a model that only learns absolute value ( <.'='.'#4' = 0), a model that only learns relative preference ( 201-' = 0), a model with leak parameters ( <.'='.'#4' and 201-' ) that vary across conditions (BIC +240), and a non-Bayesian learning model that, instead of beta distributions, only learns expected values and relative preferences based on a Rescorla-Wagner update rule with fixed learning rate 
(Rescorla, R.A. & Wagner, A.R. (1972)
; BIC +5600 relative to winning model).


Model fitting
We fit model parameters to subjects' choices using an iterative hierarchical importance sampling approach 
(Eldar et al 2018)
. We first used 2.5 × 10 5 random settings of the parameter from predefined group-level distributions to compute the likelihood of observing subjects' choices given each setting. We approximated posterior estimates of the group-level prior distributions for each of our parameters by resampling the parameter values with likelihoods as weights, and then re-fit the data based on the updated priors. These steps were repeated iteratively until model evidence ceased to increase. To derive the best-fitting parameters for each individual subject, we computed a weighted mean of the final batch of parameter settings, in which each setting was weighted by the likelihood it assigned to the individual subject's decision.


Parameter initialization
Across both models, <.'='.'#4'?A0('1%#' and 201-'?A0('1%#' are initialized by sampling from a gamma distribution ( = 1, = 1), leak parameters ( <.'='.'#4' and 201-' ) are initialized by sampling from a beta distribution with ( = 9 = 1) and all other parameters are initialized by sampling from a lognormal distribution ( = 0, = 1).


Model comparison
For each model we estimated the optimal parameters by likelihood maximization. We then applied the Bayesian Information Criterion (BIC) to compare the goodness of fit and parsimony of each model. The BIC can be computed as follows: BIC = -2 ln L + k ln n, where L is the evidence in favor of each model, which was estimated as the mean likelihood of the model given random parameter settings drawn from the fitted group-level priors, k is the number of fitted group-level parameters and n is the number of subject choices used to compute the likelihood.
We validated the model comparison procedure by simulating data using each model and using the model comparison procedure to recover the correct model 
(Table S1
).


Statistical tests of parameter fits
Statistical significance of each interaction parameter was measured using a two tailed permutation test. First, we calculated the mean of the log fit across the 27 subjects to generate a summary statistic of how much the parameter deviates from 1. A mean of zero indicates that the parameter of interest does not significantly scale the inverse temperature in either direction. A mean significantly different from zero indicates the condition modulates the inverse temperature parameters in favor of either absolute value or relative preferences.
Thus, for each parameter of interest, we generated a null distribution composed of 1000 random permutations of the data, randomly shuffling the condition of interest (e.g. whether the subject is in low or high concurrent diversity, which corresponds to inverting the impact of the parameter).
We then applied the full model fitting procedure to each permuted data set and computed the p value by comparing the actual parameter fit to the distribution of parameter fits for the permuted data. We validated our parameter fits through simulating data using the best fitting parameters 649 for each subject and then recovering those parameters 
(Table S2
). 
Figure S1
. Data simulated using the combined value and preference learning model demonstrates all key behavioral findings. To determine whether the model successfully captured individual differences in our experiment, we examined how parameter fits correlated with model-agnostic measures of behavior. As expected, we found that 201-' was significantly correlated with generalization performance (r=.7) while <.'='.'#4' correlated with our measure of rank bias (r=.5). We then validated the best-fitting model thoroughly by simulating, for each subject, 1000 data sets using their best fitting parameters and analyzing the simulated data in the same fashion in which we analyzed the real data. This procedure showed the model uniquely accounted for all of our behavioral findings (figure S1) (A) In learning trials, performance is better in conditions of low concurrent (Mean 1," =88% ±1% vs Mean +%^+ = 85% ± 1 ) and cumulative (Mean 1," = 85% ± 1% vs Mean +%^+ = 82% ± 1% figure 8a) diversity. (B) Concurrent but not cumulative diversity leads to better generalization (Concurrent: Mean 1," =−7.8% ± 1% vs Mean +%^+ =−1.2% ± 1 figure 8b p<.001 bootstrap test; Cumulative Mean 1," =−4% ± 1% vs Mean +%^+ =−6% ± 1 figure 8b p=.23). (C) Concurrent but not cumulative diversity diminishes ranking bias (Concurrent: Mean 1," =59% ±1% vs Mean +%^+ =53% ± 1, p=.01, Cumulative: Mean 1," =56% ±1% vs Mean +%^+ = 54% ± 1) . (D) The simulated choices show that preference for an image is inversely influenced by the outcomes of both the current alternative ( *-..'#/ 01/'.#0/%2' = -.46, CI= 
[-.56, -.35]
) and of the other images it had previously been pitted against ( !/+'. =-.42, CI= 
[-.54, -.30]
). Furthermore, the influence of other images' reward history is reduced by high concurrent diversity ( 4,#4-..'#/×,/+'. =. 31, CI= 
[.22, .40]
).
Figure 1 .
1
Experimental design. (A) Reward learning game.


Figure 2 .
2
Overall performance. n = 27 subjects. A) Choice accuracy as a function of trial type and condition.


preference for an image was inversely influenced by the outcomes of both the current alternative ( *-..'#/ 01/'.#0/%2' = -.43[-.64, -.21]) and of the other images it had previously been pitted against ( !/+'. =-.53[-.75, -.32]). Thus, the more subjects were rewarded when not choosing an image, the less likely they were to prefer it on subsequent trials. Most importantly, the influence of other images' reward history was reduced by high concurrentdiversity ( !/+'. × *,#4-..'#/ = .39 CI=[.30, .49]). No interaction was found between concurrent diversity and an image's own outcomes ( !"# × *,#4-..'#/ = .17 CI=[-.10, .44] ) nor with the current alternative's outcomes ( *-..'#/ 01/'.#0/%2' × *,#4-..'#/ = .06 CI=[-08,.20]). We also did not find interactions with cumulative diversity ( *-..'#/ 51/'.#0/%2' × *-&-10/%2' = −.06 CI=[-.17, .05], !/+'. × *-&-10/%2' = .11CI=[-.02 .23]), except with the impact of an image's own outcomes ( !"# × *-&-10/%2' = −.20 CI=[-.37 -.02]), as consistent with our previous finding that cumulative diversity generally decreased performance. Thus, concurrent, but not cumulative, diversity reduced the influence of other options' rewards that are irrelevant for inferring absolute value.


Figure 4 .
4
Effect of other options' outcomes. A) Rank bias as a function of training diversity.


<
.'='.'#4' =1.88 ±.22), as befitting a task that involves frequent choices between options from different learning contexts. Importantly, however, preference learning manifested to a greater extent in conditions of low concurrent diversity (low concurrent: <.'='.'#4' =2.6 ±.29; high concurrent: <.'='.'#4' =1.44 ±.21; < .001 permutation test), whereas value learning manifested to a greater extent in conditions of high concurrent diversity (low concurrent: 201-' = 3.84 ±.31; high concurrent: 201-' = 4.33 ± .2 < .001 permutation test).By contrast to concurrent diversity, cumulative diversity inhibited value learning (low cumulative: =4.7 ±.5; high cumulative: 201-' =3.5 ±.4; p<.001 permutation test) and had no significant impact on preference learning (low cumulative: <.'='.'#4' = .89 ± .13; high cumulative: <.'='.'#4' = .92 ± .1; p=.32 permutation test) . Additionally, the manifestation in choice of the two forms of learning did not significantly differ between learning and testing trials (testing: 201-' = 4.0±.3, <.'='.'#4' = .87±.1; learning: 201-' = 4.02 ±.3, <.'='.'#4' =.88 ±.1).Thus, the results indicate that concurrently learning about a broader set of options enhances the use of absolute values for making choices.


Figure 5 .
5
n = 27 subjects. A) Model comparison. Comparison of the combined Preference+Value model to models that learn only absolute values or relative preferences. The models are compared by means of the Bayesian Information Criterion (BIC; Kass et al 1995). Lower BIC values indicate a more parsimonious model fit. B) Modeled Utilization of Value and Preference Learning. Individual subject model parameter fits showing the effects of concurrent and cumulative diversity on the degree to which preference and value learning manifested in subjects' choices. Each dot represents a subject. Dashed lines mark where utilization of the form of learning is equal for low and high diversity.


irrelevant options' outcomes. Computational modeling shows that all of these effects are coherently explained by a shift away from relative preference and towards absolute value learning. These findings offer a meaningful extension of previous demonstrations of absolute value
(Kable and Glimcher, 2009;
 Berns, 2002, Levy and
Glimcher, 2012)
 and relative preference (Biderman and Shohamy 2021, Li and Daw 2011) learning in humans, by identifying key conditions under which the former is diminished in favor of the latter, namely, conditions of high concurrent training diversity.The enhancement of absolute values and the inhibition of relative preferences that we demonstrated are best understood in light of past suggestions that encoding context-specific information aids performance as long as the agent remains within the learning context but is ill suited for generalizing policies to other learning contexts(Polania et al 2019, Hunter and Daw    2018). Indeed, relative preference learning is inherently specific to the learning context while impairing generalization to novel choice sets. Our findings show that such context-specific


et al (2018) for Android smartphones using the Android Studio programming environment (Google, Mountain View, CA). The app asks users to perform experimental tasks according to a predetermined schedule. Additional features of the app not relevant for the present work include probing of changes in subjects' mental state, including regular mood self-report questionnaires and life events and activities logging, and recording of electroencephalographic (EEG) and heart rate signals derived from wearable sensors connected using Bluetooth. All behavioral and physiological data are saved locally on the phone as SQLite databases (The SQLite Consortium),


Second, this result suggests that the formation of absolute values is not promoted by the global diversity of learning exemplars encountered during the entire course of learning, but rather, by the local diversity that characterizes the immediate learning context. While our findings show that concurrent diversity promotes absolute value learning, there are two features of concurrent diversity that we cannot yet tease apart. Namely, the effects of learning about a larger number of images are inherently coupled with the effects of having consecutive sampling of an image separated by a larger number of intervening trials. Notably, a larger number of intervening trials increases both working memory load
(Collins et al 2017) and
 the time interval that passes between consecutive sampling of the same image. Future experiments could disentangle the effects of number of images, inter-encounter intervals, and working memory load by introducing unrelated tasks during learning, which could serve to either create larger delays between encounters with an image or increase working memory while maintaining the same number of images subjects learn about concurrently. However, it is yet unknown how the brain arbitrates between preference and value learning. One relevant line of work comprises studies on how concurrent diversity influences the brain regions recruited for learning
(Collins et al 2017)
. Though this work has not examined absolute value and relative preference learning, it has shown that increasing the number of items people concurrently learn about strengthens activation in a striatal-frontoparietal network implicated in value learning. Future studies could investigate the involvement of this network and other regions in arbitrating between value and preference learning as environmental conditions change.
Another open question remains as to a full functional description of the relationship between
concurrent diversity and absolute value learning. Our model, which was tested on three (low
diversity) or six (high diversity) concurrently learnt images does not allow us to extrapolate to
learning with other set sizes. Clarifying the full functional relationship between diversity and
value learning can be aided by extending the current experimental approach to testing additional
levels of diversity, as well as by further developing a mechanistic understanding of how diversity
promotes value learning.
Several studies have investigated the neural basis of value (Schultz et al 1997, O'Reilly 2020;
Bartra et al. 2013; Levy & Glimcher, 2012) and preference (Li and Daw 2011, Zimmerman
2018, Padoa-Schioppa 2009) learning in isolation, and the potential instantiation of relative
preferences via sampling from memory during choice (Bornstein et al 2017a, Bornstein &
Norman, 2017b, Rahul Bhui and Gershman, 2018, Ronayne & Brown 2017; Gershman & Daw,
2017).
Though both concurrent and cumulative diversity increased task difficulty, as evident by poorer performance during learning, cumulative diversity did not have the effect of improving generalization. This result has two key implications. First, this result contradicts previous suggestions that it is task difficulty per-se that promotes absolute value learning (Bavard et al 2021).


Table 1 : Example image pairings in conditions of low and high concurrent diversity and reward context.
1
Stimulus
Expected value
Pitted against
Optimal choice frequency
1
0
2 and 3
Never
2
.33
1 and 3
Half (over 1)
3
.66
2 and 3
Always
B) Concurrent diversity: Low; Reward context: High
Stimulus
Expected value
Pitted against
Optimal choice frequency
4
.33
5 and 6
Never
5
.66
4 and 6
Half (over 4)
6
1
4 and 5
Always
C) Concurrent diversity: High; Reward context: Low
Stimulus
Expected value
Pitted against
Optimal choice frequency
1
0
2 and 3
Never
2
.33
1 and 4
Half (over 1)
3
.66
1 and 5
Always
4
.66
2 and 6
Always
5
.33
3 and 7
Half (over 6)
6
0
4 and 5
Never
A) Concurrent diversity: Low; Reward context: Low








Acknowledgements
We thank Elisa Milwer and Alina Ryabtev for help with the data collection process. This work has been made possible by NIH grants R01MH124092 and R01MH125564, ISF grant 1094/20 and US-Israel BSF grant 2019802






Computational formalization
Whereas the main components of the computational model are described in the main text, here we detail precisely how preference and value learning were influenced by diversity conditions.
On each trial, a set of per-subject <.'='.'#4'?A0('1%#' and 201-'?A0('1%#' parameters were modulated by the following main effects and interaction parameters. %'$-/7/8 9':97/8 is a ratio that represents how the influence of prior outcomes on choice differs between learning and testing trials.


Main effects
are ratios that represent the impact of training diversity on the relative influence of value and preference learning on choice. The more either ratio diverges from 1, the greater the impact diversity has on the balance between value and preference learning. are ratios that represent the impact of training diversity on the relative influence of prior outcomes on choices in learning compared to testing trials; the more either ratio diverges from 1, the greater the impact diversity has on the relative influence of prior outcomes on choices in learning compared to testing trials.


Author Contributions
LS, EE designed the research; LS conducted the research; LS, EE analyzed the data; PS contributed materials/analysis tools, LS, EE, PS wrote the paper.


Competing Interests
The authors declare no competing interests. 
Table S1
. Model validation. full experimental datasets were simulated using each model. Rows indicate the model used to simulate data and columns indicate the model recovered from the data using the model comparison procedure.  
Table S2
. Validation of Parameter Recovery. We validated our parameter fits through simulating data using the best fitting parameters for each subject and then recovering those parameters. Our correlation between simulated and recovered parameters was at least .74 for all parameters of interest that capture the effects of the experimental conditions, and at least .51 for all other parameters. 


Supplementary Material


Preference+Value


Parameter Correlation between Simulated and Recovered Parameters
 










Correlates of decisional dynamics in the dorsal anterior cingulate cortex




H
Azab






B
Y
Hayden








PLoS biology




15


11


2003091














The valuation system: a coordinate-based meta-analysis of BOLD fMRI experiments examining neural correlates of subjective value




O
Bartra






J
T
Mcguire






J
W
Kable








Neuroimage




76
















Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning. Sci Adv




S
Bavard






A
Rustichini






S
Palminteri




10.1126/sciadv.abe0340


33811071






7


340












Reference-point centering and range-adaptation enhance human reinforcement learning at the cost of irrational preferences




S
Bavard






M
Lebreton






M
Khamassi






G
Coricelli






S
Palminteri








Nat Commun




9
















Value-free reinforcement learning: Policy optimization as a minimal model of operant behavior




D
Bennett






Y
Niv






A
Langdon




10.31234/osf.io/ew58m


















Memory and decision making interact to shape the value of unchosen options




N
Biderman






D
Shohamy








Nat. Commun




12


4648














Decision by sampling implements efficient coding of psychoeconomic functions




R
Bhui






S
J
Gershman




10.1037/rev0000123








Psychological Review




125


6
















Ventromedial prefrontal and anterior cingulate cortex adopt choice and default reference frames during sequential multi-alternative choice




E
D
Boorman






M
F
Rushworth






T
E
Behrens








J Neurosci




33
















Reminders of past choices bias decisions for reward in humans




A
Bornstein






M
Khaw






D
Shohamy




10.1038/ncomms15958








Nat Commun




8


15958














Reinstated episodic context guides sampling-based decisions for reward




A
M
Bornstein






K
A
Norman








Nature neuroscience




20


7
















Robust Locally Weighted Regression and Smoothing Scatterplots




William
S
Cleveland




10.2307/2286407.JSTOR2286407.MR0556476






Journal of the American Statistical Association




74


368
















Statistical Power Analysis for the Behavioral Sciences




J
Cohen








Lawrence Erlbaum Associates


Hillsdale






2nd ed








Working Memory Load Strengthens Reward Prediction Errors




Age
Collins






B
Ciullo






M
J
Frank






D
Badre




10.1523/JNEUROSCI.2700-16.2017






J Neurosci




37


16
















The Magical Mystery Four: How is Working Memory Capacity Limited, and Why?




N
Cowan




10.1177/0963721409359277






Curr Dir Psychol Sci




19


1
















Cortical substrates for exploratory decisions in humans




N
D
Daw






J
P
O'doherty






P
Dayan






B
Seymour






R
J
Dolan








Nature




441
















Decodability of Reward Learning Signals Predicts Mood Fluctuations Highlights Current Biology




Eldar




10.1016/j.cub.2018.03.038








Elsevier Ltd


28






The Author(s). Published by








Computational noise in reward-guided learning drives behavioral variability in volatile environments




C
Findling






V
Skvortsova






R
Dromnelle




10.1038/s41593-019-0518-9








Nat Neuroscience




22
















Decomposing the effects of context valence and feedback information on speed and accuracy during reinforcement learning: a meta-analytical approach using diffusion decision modeling




L
Fontanesi






S
Palminteri






M
Lebreton








Cogn Affect Behav Neurosci




19
















Context, learning, and extinction




S
J
Gershman






D
M
Blei






Y
Niv








Psychol Rev




117


197














Reinforcement learning and episodic memory in humans and animals: an integrative framework




S
J
Gershman






N
D
Daw








Annual review of psychology




68
















Heuristic decision making. Annual review of psychology




G
Gigerenzer






W
Gaissmaier








62














Neuroeconomics: Decision making and the brain




P
W
Glimcher




& Fehr, E.






Academic Press














Zhiqiang
&
Gong






Ping
&
Zhong






Weidong
Hu




Machine Learning
















The case against economic values in the brain




B
Hayden






Y
Niv








PsyArXiv
















Reference-dependent preferences arise from structure learning




L
E
Hunter






S
J
Gershman






2018


252692












Triple dissociation of attention and decision computations across prefrontal cortex




L
T
Hunt






W
N
Malalasekera






A
O
De Berker






B
Miranda






S
F
Farmer






T
E
Behrens






S
W
Kennerley








Nature neuroscience




21


10
















Where does value come from?




K
Juechems






C
Summerfield








Trends in cognitive sciences




23


10
















Human value learning and representation reflects rational adaption to task demands




T
Juechems






Altun






Hira






Jarvstad














The neurobiology of decision: consensus and controversy




J
W
Kable






P
W
Glimcher








Neuron




63


6
















Bayes factors




R
E
Kass






A
E
Raftery








J. Am. Stat. Assoc




90
















Evidential diversity increases generalization in predictive learning




J
C
Lee






P
F
Lovibond






B
K
Hayes




10.1177/1747021819857065


31144583






Q J Exp Psychol (Hove)




72


11
















The root of all value: a neural common currency for choice




D
J
Levy






P
W
Glimcher








Current opinion in neurobiology




22


6
















Signals in human striatum are appropriate for policy update rather than value prediction




J
Li






N
D
Daw








J Neurosci




31
















The construction of preference


Lichtenstein, S., & Slovic, P.






Cambridge University Press












Normalization is a general neural mechanism for contextdependent decision making




K
Louie






M
W
Khaw






P
W
Glimcher








Proc Natl Acad Sci U S A




110
















The Misbehavior of Reinforcement Learning. Proceedings of the IEEE




Gianluigi
&
Mongillo






Hanan
&
Shteingart






Yonatan
Loewenstein








102


















/
Jproc


















Neural economics and the biological substrates of valuation




P
R
Montague






G
S
Berns








Neuron




36


2
















Reinforcement learning in the brain




Y
Niv








Journal of Mathematical Psychology




53


3
















The problem with value




J
P
O'doherty








Neuroscience & Biobehavioral Reviews




43
















Unraveling the Mysteries of Motivation




R
C
O'reilly








Trends in Cognitive Sciences
















Range-adapting representation of economic value in the orbitofrontal cortex




C
Padoa-Schioppa








J Neurosci




29
















Contextual modulation of value signals in reward and punishment learning




S
Palminteri






M
Khamassi






M
Joffily






G
Coricelli








Nat Commun




6
















Efficient coding of subjective value




R
Polania






M
Woodford






C
C
Ruff








Nat Neurosci




22
















Multi-attribute decision by sampling: An account of the attraction, compromise and similarity effects




D
Ronayne






G
D
Brown








Journal of Mathematical Psychology




81
















A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement, Classical Conditioning II




R
A
Rescorla






A
R
Wagner




A.H. Black & W.F. Prokasy






Appleton-Century-Crofts














A range-normalization model of context-dependent choice: a new model and evidence




A
Soltani






De
Martino






B
Camerer






C








PLoS Comput Biol




8


1002607














A neural substrate of prediction and reward




W
Schultz






P
Dayan






P
R
Montague




10.1126/science.275.5306.1593


9054347






Science




275


5306
















Choice-theoretic foundations of the divisive normalization model




K
Steverson






A
Brandenburger






P
Glimcher








J Econ Behav Organ




164
















Reward value comparison via mutual inhibition in ventromedial prefrontal cortex




C
E
Strait






T
C
Blanchard






B
Y
Hayden




10.1016/j.neuron.2014.04.032






Neuron




82


6
















Counterfactual thinking: an fMRI study on changing the past for a better future




N
Van Hoeck






N
Ma






L
Ampe






K
Baetens






M
Vandekerckhove






F
Van Overwalle




10.1093/scan/nss031






Soc Cogn Affect Neurosci




8


5
















Does the brain calculate value?




I
Vlaev






N
Chater






N
Stewart






G
D
Brown








Trends in cognitive sciences




15


11
















Multiple timescales of normalized value coding underlie adaptive choice behavior




J
Zimmermann






P
W
Glimcher






K
Louie








Nature communications




9


1
















Image Quality Assessment: From Error Visibility to Structural Similarity




W
Zhou






A
C
Bovik






H
R
Sheikh






E
P
Simoncelli








IEEE Transactions on Image Processing




13

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]