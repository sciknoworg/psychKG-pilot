You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
We have all experienced the pressure of making decisions under limited time. For instance, choosing what to order at a restaurant while the waiter waits impatiently behind your shoulder. Or deciding which analyses to run as a paper submission deadline looms near. With less time to think, we have less opportunity to perform costly computations. But does time pressure merely make us more noisy as we deal with the speed-accuracy trade-off 
1,
2
 ? Or are we able to adapt our decision-making processes, to make the best use of our cognitive resources given external constraints on our computational capacity 
[3]
[4]
[5]
[6]
 ?
Here, we are interested in the cognitive processes involved in navigating the exploration-exploitation dilemma 
[7]
[8]
[9]
 , which plays a key role when learning through interactions with the environment, such as in reinforcement learning 
10
 (RL) problems. Should you exploit your usual menu option or should you explore something new? The usual option may yield a predictably rewarding outcome, but forgoes the opportunity of learning about other menu items. A new option could lead to either a pleasant or unpleasant surprise, but will likely be informative for future decisions and could improve future outcomes.
Since optimal solutions to the exploration-exploitation dilemma are generally unobtainable 
11,
12
 except in limiting cases 
[13]
[14]
[15]
 (e.g., infinite time horizons among other assumptions), there is great interest in understanding the strategies that humans use 
16,
17
 . Empirical evidence from a variety of experiments 
8,
[18]
[19]
[20]
[21]
 and real-world consumer data 
22
 suggests people use a mix of two strategies: random and directed exploration. Random exploration increases the diversity of choices by adding stochasticity to the agent's behavioral policy, instead of only maximizing expected value. If you have only ever tried a handful of items on the menu, then you might have an imperfect picture of which options are good. Thus, adding In unlimited time rounds, participants could take as long as they wanted to make each selection and received positive feedback (happy face) and were shown the value of the acquired payoff for 400ms (feedback period). In limited time rounds, participants were only given 400 ms to make each selection. If they exceeded the time limit, they earned no rewards and received negative feedback (sad face) with the value of the payoff they could have earned crossed out. We used the same feedback period duration of 400ms before the next trial automatically began and participants were shown the choice screen. Inputs during the feedback period had no effect. c) Each payoff condition specifies a normal payoff distribution for each option, with the means and variances described numerically in 
Table 1
. The reward distributions are designed to compare how differences in reward expectations and differences in uncertainty influence choices, where IGT refers to a payoff distribution inspired by the Iowa Gambling Task (see Methods). Dots and the Tukey boxplots describe 100 randomly drawn payoffs, while the half violin plots show the generative distribution, with the diamond indicating the mean.
more variability to your choices may give you a better perspective about which options you should value. In contrast, directed exploration adds an exploration bonus to each option, proportional to the agent's level of uncertainty 
23
 . Rather than simply behaving more randomly, directed exploration is more strategic, prioritizing choices with the highest uncertainty to gain more information 
24,
25
 . Perhaps there is an item on the menu you have never tried before. Directing your exploration to that novel item would be more effective at achieving an information maximization goal than choosing randomly. But since representations of uncertainty need to be factored into the decision-making process, this may be computationally more costly.


Limiting Decision Time
We manipulate decision time as a method for imposing external limitations on cognitive resources, to better understand the differential cognitive costs associated with random and directed exploration. With less time "budgeted" for costly computations, resource-rational decision makers 4, 5 might be expected to choose cheaper strategies in order to achieve a better trade-off between the costs of computation and the benefits in terms of reward. One line of research on human decision-making commonly assumes that time pressure causes participants to rely more on "intuitive decision making" 
26
 , making immediate outcomes more salient 
27
 , and making people more reliant on fast, recognition-based processes as compared to slower, more analytical processes 
28
 . Research using formal computational models has also related time pressure to changes in the speed-accuracy trade-off 
29
 , yielding faster, less accurate decisions, but nevertheless still achieving an efficient rate of rewards 
30,
31
 . However, there is disagreement in the literature about how time pressure changes exploration patterns. On the one hand, taxing cognitive capacities has been shown to increase exploration, producing less consistent and fewer expected value-maximizing decisions 
32,
33
 . Similarly, people and monkeys placed under time pressure become more eager to select uncertain options, independent of outcome value 
34,
35
 . Time pressure has also been linked to making people become more risk-seeking 
[36]
[37]
[38]
 , although recent modeling work has challenged the reliability of this shift in risk preferences 
33
 . Nevertheless, a common thread is that limiting cognitive capacity reduces the scope or detail with which people evaluate different options 
[39]
[40]
[41]
 , producing more impulsive decisions or a switch to simpler, heuristic decision-making strategies 
42
 , both with similar patterns of increased exploration.
On the other hand, time pressure has also been shown to decrease exploration, leading to more repeat choice behavior and a reduced preference for uncertain options. Participants under time pressure are more likely to repeat previous actions 
43
 , even to the detriment of producing more costly errors. This can also be related to a trade-off between reward and policy complexity 
44
 , where less complex and cheaper-to-encode policies will lead to higher rates of choice perseveration (i.e., repeat choices). Time pressure has also been shown to increase participants' preferences for a known payoff over an uncertain alternative in the domain of gains 
45
 , although the inverse was true in the domain of losses. There are also similar findings from description-based gambles, where time pressure can increase risk aversion in the domain of gains 
46
 .
These divergent results could be interpreted through the lens of early work on coping mechanisms people use when put to the limits of their cognitive abilities 
47
 . One mechanism is acceleration, where information is processed at a faster rate. Combined with lower evidence thresholds, acceleration can lead to more frequently choosing options that would otherwise be ignored, consistent with increased random exploration. Recent work using drift diffusion models has supported this hypothesis by connecting random exploration to lowered evidence thresholds and increased drift rates 
48
 . Conversely, longer response times have been related to the ability to mentally simulate a greater number of future outcomes 
49
 , producing more directed exploration but decreased random exploration 
50
 . Acceleration as a response to time pressure could thus produce a trade-off between different forms of exploration.
Another potential mechanism is repetition, where previous actions are repeated or recycled 
44,
51
 , since it may not always be cost effective to simulate any future outcomes at all. This can be related to value-free habits 
52
 , where not all decisions justify the cognitive costs of using value expectations (both rewards and uncertainty) to select new actions. Whereas you might normally enjoy exploring new restaurants in a new city, limits on decision time, such as an imminent departure at the airport, might motivate you to default to a previously visited restaurant, instead of weighing the alternatives and selecting a new option.


Goals and Scope
We present a rich experimental setting, where we use a within-subject design manipulating the presence or absence of time pressure to gain insights into the cognitive processes underlying exploration. We use multiple four-armed bandit tasks, where across four payoff conditions (within-subject), we independently manipulate reward expectations and uncertainty across different options 
(Fig. 1
). This allows us to dissociate value-directed and uncertainty-directed choices, where compared to previous studies with twoarmed bandit tasks 
19,
24
 , the richer set of options makes efficient exploration more relevant and observable over more trials. Given less decision time, participants can be expected to have less access to costly computations, leading to less value-maximizing choices and more random exploration. Simultaneously, time pressure may limit the capacity for reasoning about the uncertainty of each option, thus leading to less uncertainty directed exploration.
As predicted, time pressure made participants less sensitive to reward values (more random exploration) and less likely to select options with high relative uncertainty (less directed exploration). We then estimated three hierarchical Bayesian models to understand how expectations of reward and subjective uncertainty influenced choices, reaction times (RTs), and evidence accumulation, which reaffirmed our behavioral analyses, with additional insights into the decision-making and evidence accumulation process. Time pressure diminished uncertainty-directed exploration through several mechanisms: i) reducing the selection of uncertain options during early trials, ii) encouraging more aggressive exploitation of known options in later trials, and iii) heightening the tendency to repeat previous choices.
Our analysis of the RT data revealed how this shift in exploration is related to the computational costs of different exploration strategies. High reward expectations corresponded to faster choices, while high uncertainty (both relative and total) were associated with slower choices. Under time pressure, participants selected highly rewarding options even faster, but slowed down less when selecting highly uncertain options-independent of having faster choices in general. These changes in RT can be linked to the evidence accumulation process. While time pressure did not change how reward expectations influenced evidence accumulation (faster choices were due to lower decision-thresholds), it reduced the extent that relative uncertainty dampened the rate of evidence accumulation.
Our findings indicate that time pressure selectively impacts how uncertainty is integrated into decisions. Put under time pressure, people are less influenced by uncertainty, less value-directed, and more likely to repeat previous choices. This is a simpler strategy and comes at lower costs, representing a potentially resource-rational adaptation to time pressure. These results enrich our understanding of human exploration strategies under changing task demands, providing insights into the cognitive costs of reasoning about and acting on uncertainty.


Results
We conducted an online experiment on MTurk (n = 99; 36 female; M age =34.82; SD age =10.1)) to study how time pressure influences exploration behavior ( 
Fig. 1
). Our "Time Bandit" experiment employed repeated four-armed bandit tasks, where we independently manipulated expected reward and uncertainty across four payoff conditions ( 
Fig. 1c
; 
Table 1
), along with time pressure (limited vs. unlimited time). This allowed us to disentangle how relative differences in reward expectations and uncertainty influence choices, and how time pressure modulates this influence, in a single within-subject design (see 
Methods)
.


Behavioral Analyses
To analyze the influence of time pressure and payoff conditions on performance and choice behavior, we constructed a series of Bayesian mixed-effects regression models. Specifically, we estimated how average rewards ( 
Fig. 2a
), the entropy of choices 
(Fig. 2b)
, the number of repeat choices 
(Fig. 2c)
, and the probability of making a repeat choice conditioned on payoff ( 
Fig. 2e
), were influenced by time pressure and payoff conditions, whilst also accounting for individual differences in the random effects structure. This allows us to describe the influence of either time pressure or payoff conditions in terms of the estimated marginal means (∆ EMM ), which uses contrast analyses to quantify differences in the dependent variable marginalized over the other independent variables. For instance, examining how average rewards were influenced by time pressure, marginalized over the four payoff conditions, or vice versa. The raw posterior estimates are provided in 
Table S1
 and visualized in 
Figure S1
, while 
Figure S2
 provides the raw data separated by payoff condition.   
Fig S1c)
, perhaps because participants were able to more quickly identify and exploit the highest rewarding arm with less variance in observed outcomes.


4/21
Lastly, we also included a variant of the repeat choice model, which included the (unshifted) value of the previous reward as an additional predictor 
(Fig 2d)
. Here, we modeled the probability of each choice (after the first trial) being a repeat using logistic regression. We find the same influence of the experimental manipulations on repeat behavior as above (see 
Table S1
), but also find an interaction between time pressure and previous reward value. Participants were more likely to repeat a choice with higher rewards in unlimited time (OR= 1.24 
[1.19, 1.29]
). Put differently, time pressure reduced participants' sensitivity to reward value in their repeat behavior, as evidenced by the flatter response curve in 
Figure 2d
. 
Figure 2e
 visualizes the aggregate choice proportions to get a better sense of patterns related to reward expectations and uncertainty. These bars indicate the aggregate choice frequency of each option relative to chance, where bars above zero indicate the option was chosen more frequently, and bars below zero indicate the option was chosen less frequently. The difference between the orange and green bars illustrates the differences in choice behavior as a function of time pressure. We aggregate the data using the canonical mapping of reward distributions (see inset plots in 
Fig. 2a
 for reference) to the [Q,W, O, P] keys, although the keys were randomly mapped in each round for participants. To provide statistical support for choice differences, we use Bayesian mixed-effects logistic regression to model how time pressure influenced the probability of choosing a given option. We focus on two informative cases.


Choice Patterns
In the IGT condition (named for mimicking the structure of the so-called Iowa Gambling Task 
54
 ), there were two high reward and two low reward options, with each pair having either a low or high variance.
We focused on the two high reward options (indicated as 'O' and 'P' in 
Fig. 2e
), and modeled whether time pressure influenced the likelihood of choosing the riskier, high variance option 'P' over the safer low variance option 'O', as a simple test of how decision time can influence the role of relative uncertainty (see 
Fig. 2e
 inset for the raw data). We found that overall, participants chose the high variance option ('P') more frequently in unlimited time (Odds Ratio: OR = 1.11 [.80, 1.53]; 
Table S2
), although the estimates overlapped with chance (OR = 1). However, there was also an interaction with round number, where the difference between time conditions widened over successive rounds. Participants in the unlimited time condition increased their likelihood of selecting the high variance option over rounds (OR = 1.39 
[1.23, 1.57]
). This effect tended towards the opposite direction for limited time rounds, where participants selected the high variance option less frequently over rounds (OR = 0.83 [0.68, 1.02]). We find the clearest differences arising from the time-pressure manipulation in the Equal Means condition ( 
Fig. 2e inset)
, where compared against all other options, participants were more likely to select the highest variance option ('P') in the unlimited time condition (OR = 1.44 [1.12, 1.86]; 
Table S2
). This illustrates a clear shift in preferences away from uncertain options when time pressure is introduced. Whereas participants tend to be risk-seeking and choose more uncertain options under unlimited time, they become more risk-averse and choose them less often under time pressure.


Interim Discussion
Altogether, we find behavioral evidence that time pressure reduced exploration. There were less diverse and more repeat choices, which ultimately resulted in lower reward outcomes. From these analyses, we find two important behavioral signatures of the underlying cognitive processes that produced this shift in exploration. First, time pressure reduced participants' sensitivity to reward values in repeating previous choices, making them more likely to repeat a low-reward choice 
(Fig. 2d
). Second, participants were less likely to select options with higher relative uncertainty under time pressure ( 
Fig. 2e
). In the next section, we employ model-based analyses, which use RL models to explicitly track expected reward and uncertainty estimates. We then use these estimates to model choice behavior, reaction times, and evidence accumulation (i.e., drift rate).


Model-Based Analyses
To model learning and decision making in our task, we use a Bayesian mean tracker (BMT) as an RL model for estimating expected rewards and associated uncertainties, which are then updated based on prediction errors (see Methods). The BMT is a special case of the Kalman filter, which assumes time-invariant reward distributions (as was the case in our experiment). The BMT provides a Bayesian analogue 
55
 to the classic Rescorla-Wagner 56 model of associative learning, and has described human behavior in a variety of multi-armed bandit and decision-making tasks 
19,
20,
24,
[57]
[58]
[59]
 .
We generated predictions from the BMT using participant choices and reward observations at each trial t to compute posterior distributions of the average reward of the options, and then using these as prior predictive distributions at trial t + 1. These prior predictive distributions are all normally distributed, and we used the mean and standard deviation as measures of predicted reward and the associated uncertainty, respectively (see 
Fig. 4
 and 
Fig. S5
), which we use to conduct three model-based analyses predicting choices, reaction times, and evidence accumulation ( 
Fig. 3
).


Choices.
In our first analysis, we assessed how reward expectations and uncertainty estimates influenced the likelihood of an option being chosen on each trial. We applied hierarchical Bayesian inference to estimate the parameters of a softmax policy (see Methods), under the assumption that a participant's choice on each  trial is influenced by both the predicted mean and uncertainty of an option. Each participant's parameters are assumed to be jointly normally distributed and assumed to interact with time pressure. The probability of choosing option j on trial t is a softmax function of its decision value Q j,t :
P(C t = j) = exp(Q j,t ) ∑ 4 k=1 exp(Q k,t ) .
(1)
The decision value Q j,t is a function of the prior predictive mean m j,t and uncertainty √ v j,t (standard deviation) of each option according to the BMT, with an additional stickiness bonus for the most recently chosen option (δ j,t−1 = 1 if option j was chosen on trial t − 1; see Methods):
Q j,t = α(m j,t + β √ v j,t ) + γδ j,t−1 .
(2)
We computed hierarchical Bayesian estimates for the value-directed component α (factoring in both rewards and uncertainty), the uncertainty bonus β (governing the trade-off between exploitation and exploration), and the stickiness bonus γ, including interactions with the time pressure manipulation (limited vs. unlimited). Larger α estimates indicate more value-directed choices, whereas lower α suggest more random choices, which are not explainable by reward expectations or uncertainty estimates (i.e., random exploration). More positive β estimates indicate a higher level of uncertainty-directed exploration.
Higher estimates of γ indicate more perseveration in choice behavior, with more frequent repetitions of previous choices. 
Figure 3a
 shows the posterior estimates of the model (see 
Fig. S7
-S8 for comparison to alternative models). We find less value-directed choice behavior under time pressure (α Unlimited −α Limited = 1.90 [1.24, 2.56]), with positive estimates in both conditions (α Unlimited = 9.21 [8.31, 10.1]; α Limited = 7.31 
[6.23, 8.44]
). This pattern can be seen in the raw BMT predictions 
(Fig. 4a
), where chosen options had both higher relative reward expectations and relative uncertainty in unlimited time. By definition, the inverse of the value-directed component defines the level of random exploration, with the interpretation that participants' choices were less predictable and more random when given less time to deliberate (limited time). This may seem at odds with the behavioral results showing reduced entropy under time pressure, but the lack of correlation between α and choice entropy under time limitations (see 
Fig. S6b
) suggests that participants consistently chose non-value maximizing options (i.e., repeating low-value choices; 
Fig. 2e
). In contrast, α estimates were correlated with higher average rewards in both conditions (see 
Fig. S6a
).
Time  
Figure 4b
 provides additional clarity about this result. Participants with unlimited time experienced an early uptick in selecting relatively uncertain options around trial 3, suggestive of an "exploration phase". Afterwards, there was a gradual shift towards exploitation, indicated by the monotonic decay of the relative uncertainty of chosen options, indicating an increasing preference for relatively less uncertain options. Under time pressure, there is a similar trend, yet the early exploration phase has almost vanished (the relative uncertainty of the chosen option on trial 3 is indistinguishable from 0: t(98) = 0.9, p = .387, d = 0.1, BF = .16) and later trials are associated with more strongly negative relative uncertainty. Thus, a reduced exploration bonus under time pressure appears to be a combination of less exploration in early trials, and more aggressive exploitation in later trials, which is also apparent in the higher levels of total uncertainty during limited time rounds 
(Fig. 4c
). This reduction in directed exploration may also be related to the lower overall performance under time pressure, since higher β estimates in the limited time condition were associated with higher rewards (see 
Fig. S6a
).
In addition to these changes in exploration, time pressure increased the stickiness of choices (γ Unlimited − γ Limited = −0.32 [−0.46, −0.19]), with positive estimates in both conditions (γ Unlimited = 1.58 [1.36, 1.80]; γ Limited = 1.91 
[1.64, 2015]
). This increase in choice perseveration is consistent with the reduced entropy and higher repeat choice probabilities found in the behavioral data, but estimates of γ were unrelated to average reward (see 
Fig. S6
).
Overall, time pressure reduced the value-directedness of choices, reduced uncertainty-directed exploration, and increased the stickiness of choices. We now turn to modeling reaction times (RTs) to better understand how reward expectations and uncertainty influenced the speed of decisions.
Reaction Time.
Our second analysis looked at how RTs (see 
Fig S3 for
 raw RT analysis) were influenced by expectations of rewards and estimated uncertainties. We first computed the relative reward and relative uncertainties of the BMT predictions using the difference between the chosen option and the average of the unchosen options on each trial. Thus, positive values indicate that the expected reward or uncertainty were larger than the mean of the unchosen options. We also computed total uncertainty, based on the sum of uncertainty estimates across the four options on any given trial. We then regressed relative mean, relative uncertainty, total uncertainty, and round number onto log-transformed RTs in a Bayesian mixed effects regression (see Methods).
The resulting posterior parameter estimates ( . Thus, higher reward expectations made people faster, whereas uncertainty (both relative and total) slowed them down. Both effects were less pronounced under time pressure.
There was also a notable interaction between predictors (see 
Fig. S9
 for the full model and Figs. S10-S11 for interaction plots). While high relative reward expectations generally sped up choices, this pattern was inverted when high rewards were also accompanied by high relative uncertainty, with participants slowing down instead of speeding up (b = .04 [.001, .08]; no difference between time conditions). Thus, certainty about high rewards produced rapid decisions, whereas uncertainty about high rewards produced slower choices.
Overall, more exploitative choices (with higher relative reward expectations) were faster, while more explorative choices (with both higher relative uncertainty or higher total uncertainty) were slower. This differs from previous findings using two-armed bandits 
19
 , in which higher relative uncertainty was related to faster decisions. Here, we find that uncertainty is not just a bonus that adds to the decision signal, making choices easier and faster. Rather, grappling with uncertainty takes time.
Evidence Accumulation.
In our third analysis, we used a Linear Ballistic Accumulator 
60,
61
 (LBA) to model choices and RTs simultaneously (see Methods). This model assumes that choices are the result of an evidence accumulation process, where evidence for each option accumulates as a function of drift rate, which is independently estimated for each option. Whichever option first exceeds the decision threshold is chosen. The interplay between the drift rate and evidence threshold captures how participants trade response speed for accuracy, with higher thresholds requiring more evidence and producing more value maximizing choices, yet slower responses. Thus, we can use the LBA to separate out how time pressure impacts evidence accumulation in terms of the rate of evidence accumulation and the amount of evidence collected.
Consistent with the need to arrive at decisions more quickly, we observed both lower relative evidence thresholds k (t(98) = −5.2, p < .001, d = 0.5, BF > 100; 
Fig. S12
) and higher mean drift rates (t(98) = 7.1, p < .001, d = 0.7, BF > 100) under time pressure. This suggests an accelerated processing of information (faster drift rates), which is also more prone to errors (lower threshold). The maximum pairwise difference between drift rates was also larger under limited time (t(98) = 6.0, p < .001, d = 0.6, BF > 100), suggesting larger separation between different options, which is also related to lower choice entropy and more frequent repeat choices (see 
Fig. S13
). Additionally, participants had shorter nondecision times τ (t(98) = −4.6, p < .001, d = 0.5, BF > 100) and less maximum starting evidence A (t(98) = −7.8, p < .001, d = 0.8, BF > 100), when placed under time pressure. All parameters where strongly correlated across time conditions (Kendall rank correlations; all r τ > .40; BF > 100; 
Fig. S12
), are recoverable 
(Fig. S14)
, and can be used to simulate realistic choice and RT patterns 
(Fig. S15)
. Thus, our LBA results confirm the intuition that participants reached faster decisions at lower evidence thresholds when time limitations were imposed, but they also accumulated evidence faster and with larger separation between options.
In a final step, we sought to better understand how expectations of reward and uncertainty influence the evidence accumulation process and how time pressure may impact this relationship. Thus, we regressed the BMT predictions of relative expected reward, relative uncertainty, and total uncertainty for each option onto its estimated drift rate using a Bayesian mixed effects regression. Note that the LBA parameters are estimated on each round, thus the BMT predictions are averaged over trials, but nevertheless capture differences in the trajectory of learning and the independent manipulations of expected rewards and uncertainty in the four payoff conditions.
The result of this analysis 
(Fig. 3c)
  The main interaction between predictors (see 
Fig. S16
 for the full model and Figs. S17-S18 for interaction plots), was that the effect of total uncertainty could be inverted depending on relative reward  
Fig. S17h
). Total uncertainty amplified evidence accumulation when the stakes were low (low relative rewards or low relative uncertainty), but dampened evidence accumulation instead when the stakes were high (high relative rewards or relative uncertainty). Since total uncertainty is the same across all options, amplified evidence accumulation under low stakes corresponds to faster, more random choices, consistent with little benefit from increased deliberation in these settings. Conversely, dampened evidence accumulation under high stakes corresponds to slower, and more reward-or uncertainty-directed choices.
Overall, we find that reward-modulated increases in evidence accumulation were unaffected by time pressure. However, uncertainty-driven decreases in evidence accumulation were less pronounced under time pressure, with drift rates less influenced by uncertain options. We also found an influence of high total uncertainty, which was modulated by expectations of rewards and relative uncertainty. When more was at stake, total uncertainty dampened drift rates and produced slower decisions. But when relative differences in reward expectations were minor, higher total uncertainty amplified drift and produced faster decisions.


Discussion
How is exploration and decision-making constrained by cognitive limitations imposed through time pressure? We investigated this question using several variants of a four-armed bandit task, designed to independently manipulate differences in reward expectations and uncertainty. We then used a time pressure manipulation to either give participants unlimited decision time or to limit decision time to less than 400 ms for each choice. Both payoff and time pressure manipulations were conducted within-subjects, allowing us to use hierarchical modeling to achieve a high level of detail into the interplay between learning strategies and cognitive limitations imposed by time pressure.
Our behavioral results show that time pressure induced participants to earn fewer rewards, made them less sensitive to reward values in their repeat choice behavior, and less likely to select options associated with higher uncertainty. We then used RL models to analyze how reward expectations and uncertainty affected choices, RTs, and the rate of evidence accumulation.
High reward expectations made participants more likely to select options, producing faster RTs for such exploitative choices, and amplifying the rate of evidence accumulation. Adding time pressure reduced the value-directedness of choices, but increased their tendency to speed up when choosing options with high relative reward expectations (i.e., exploitation), and made them more likely to repeat previous choices.
In contrast, while uncertainty also made participants more likely to select options, choices with higher relative (and to some extent total uncertainty) were associated with slower choices and reduced evidence accumulation rates. Adding time pressure reduced uncertainty-directed exploration in choice behavior and also reduced the influence of uncertainty on RTs. This is consistent with the notion that uncertainty takes time to process and deploy strategically. Without the necessary time to grapple with uncertainty, participants shifted to exploiting known options and repeating previous choices, rather than integrating the value of exploring uncertain options.
Similar reductions in directed exploration have also been observed when participants were placed under working memory load 
62
 . The resulting behavior may thus be seen as a resource-rational 4, 5 adaptation to externally imposed limitations on cognitive resources, consistent with other findings showing that people are sensitive to the cost-benefit tradeoffs of different learning strategies 
63,
64
 . Indeed, the interactions of our LBA model 
(Fig. S17g-h
) suggest that people are sensitive to the cost-benefit trade-off of increased deliberation, producing faster more random decisions when the stakes are low, but slowing down and deliberating longer when the stakes are high. Future research should examine the underlying mechanisms of the arbitration between strategies and the neural locus of cognitive control.


Limitations and extensions
One limitation is that we only account for how time pressure influences exploration strategies, but not for changes in learning. Time pressure might not only change which computations we engage in when deciding how to explore or exploit, but it might also influence the richness of the representations we form during learning or the extent to which these representations are updated in response to new information.
Indeed, previous work in economics has shown a reduced efficacy of training 
65
 . However, our use of Bayesian RL in modeling choices and RTs may not be able to differentiate between these hypotheses, although similar models in related tasks have been used to predict directly elicited participant judgments about reward expectations and confidence 
[66]
[67]
[68]
[69]
[70]
 . Future studies may consider modeling not only choices and RTs, but also participant judgments about future outcomes in a similar time pressure manipulation.
Our current results also only examined uncertainty about reward expectations. However, there exist several alternative measures of uncertainty such as confidence 
71,
72
 , perceptual uncertainty 
73,
74
 , and computational uncertainty induced by cognitive load 
75
 , all of which could influence exploration behavior in different ways. Thus, we expect future studies to increasingly focus on disentangling different sources of uncertainty and their effects on the exploration-exploitation dilemma.
Additionally, while our four-armed bandit task was designed to provide a richer choice set beyond two options, magnifying the difference between directed and random exploration, it still pales in comparison to the complexity of many real world problems. Since participants may be more likely to engage in directed exploration in highly complex or highly structured domains 
21,
22,
70
 , an important future direction will be to understand how environmental structure modulates changes in learning as a function of cognitive limitations.
Lastly, we have also only looked at multi-armed bandits in which participants only gain positive rewards or earn nothing when exceeding the time limit. We did not, however, probe how exploration behavior changes in the domain of losses 
76,
77
 or risky outcomes 
33,
78
 . Since the distribution of rewards can affect participants' learning 
79
 and losses have been shown to produce risk-seeking under time pressure 
45,
46
 , studying this domain will be a crucial next step.


Conclusions
We studied the interplay of human exploration strategies and cognitive limitations imposed by time pressure, showing that participants are sensitive to the costs and benefits of different computations. Put under time pressure, people were less influenced by uncertainty, less value-directed, and repeated past choices more often. These behavioral changes are linked to the cognitive costs of reasoning about rewards and uncertainty. Exploitative choices (i.e., high reward expectations) were generally faster, while exploratory choices (i.e, high relative or total uncertainty) were slower. Taken together, our results suggest that people display a resource-rational sensitivity to the cost-benefits of different exploration strategies under externally imposed limitations on cognitive resources.


Methods


Participants and Design.
We recruited 99 participants (36 female, aged between 21 and 69 years; M=34.82; SD=10.1) on Amazon Mechanical Turk (requiring 95% approval rate and 100 previously approved HITs). Participants were paid $3.00 for taking part in the experiment and a performance contingent bonus of up to $4.00 (calculated based on the performance of one randomly selected round). Participants spent 13.0 ± 5.6 minutes on the task and earned $5.87 ± $0.91 in total. The study was approved by the Ethics Committee of the Max Planck Institute for Human Development and all methods were carried out in accordance with relevant guidelines and regulations.. Informed consent was obtained from all subjects.
We used a 2 × 4 within-subject design to examine how the presence or absence of time pressure and the payoff structure of the task (see 
Fig. 1c
 and Tab. 1) influenced choices and reaction times. In total, the experiment consisted of 40 rounds with 20 trials each. In each round, a condition was sampled (without replacement) from a pre-randomized list, such that each combination of time pressure and payoff structure 
Table 1
. Payoff Conditions. Means shown are unshifted. In the experiment, a random value between 30 and 60 was added to all rewards of all options, and actual rewards were always positive. IGT refers to payoffs inspired by the Iowa Gambling Task 
54,
80
 . Participants were required to complete three comprehension questions and two practice rounds (one with unlimited time and one with limited time) consisting of 5 trials each before starting the experiment. Each of the 40 rounds was presented as a four-armed bandit task, where the four options were randomly mapped to the [Q,W, O, P] keys on the keyboard 
(Fig. 1a)
. Selecting an option by pressing the corresponding key yielded a reward sampled from a normal distribution, where the mean and variance was defined by the round's payoff structure 
(Fig. 1c
 and Tab. 1). Participants completed 20 trials in each round and were told to acquire as many points as possible.


Payoff
Before starting a round, participants were informed whether it was an unlimited or a limited time round. In unlimited time rounds, participants could spend as much time as they needed to reach a decision, upon which they were given feedback about the obtained reward (displayed for 400 ms) before continuing to the next trial 
(Fig. 1b)
. In limited time rounds, participants were instructed to decide as fast as possible. If a decision took longer than 400 ms, they forfeited the reward they would have earned (presented to them as a crossed-out number with an additional sad smiley; 
Fig. 1b)
. We used the same feedback period of 400 ms to display feedback about obtained rewards in both limited and unlimited time rounds.
We applied a random shifting of rewards across rounds (i.e., different minimum and maximum reward) to prevent participants from immediately recognizing when they had chosen the optimal option. For each round, we sampled a value from a uniform distribution U 
(30,
60)
, which was then added to the rewards. Together with random shifting, we also truncated rewards such that they were always larger than zero. In order to convey intuitions about the random shift of rewards, payoffs were presented using a different fictional currency in each round (e.g., ß, Þ, ϑ ), such that the absolute value was unknown, but higher were always better.
At the end of each round, participants were given feedback about their performance in terms of the bonus they would gain (in USD) if this was the round selected for determining the bonus. The bonus was calculated as a percentage of the total possible performance, raised to the power of 4 to accentuate differences in the upper range of performance: Bonus = total reward gained mean reward of best option×20 trials 4 × $4.00


Payoff conditions
We used four different payoff conditions as a within-participant manipulation 
(Table 1
 and 
Fig. 1c
). Each payoff condition specified the mean µ j and variance σ 2 j of the reward distribution R j ∼ N (µ j , σ 2 j ) for each option j. Each distribution was randomly mapped to one of the four [Q,W, O, P] keys of the keyboard in each round. The Iowa Gambling Task (IGT) is a classic design that has been related to a variety of clinical and neurological factors affecting decision-making 
54,
80
 . We implemented a reward condition inspired by the IGT such that there are two high and two low reward options, with a low and high variance version of each. We also constructed two conditions with equally spaced means, but with either uniformly low variance or uniformly high variance. Lastly, the equal means condition had identical means and gradually increasing variance, such that we can observe the influence of uncertainty independent of mean reward.


Model-based analyses


Bayesian mean tracker
The Bayesian mean tracker (BMT) learns a posterior distribution over the mean reward µ j for each option j. Rewards are assumed to be normally distributed with a known variance but unknown mean. The prior distribution of the mean is also a normal distribution. This implies that the posterior distribution for each mean is also a normal distribution:
p t (µ j |D t−1 ) = N (m j,t , v j,t )
(3)
where p t is the posterior distribution at trial t and D t−1 denotes the observed rewards and choices up to and including trial t (for all options). For a given option j, the posterior mean m j,t and variance v j,t at trial t are only updated when it has been selected at trial t:
m j,t = m j,t−1 + δ j,t G j,t y t − m j,t−1 (4) v j,t = 1 − δ j,t G j,t v j,t−1 (5)
where δ j,t = 1 if option j is chosen on trial t, and 0 otherwise. Additionally, y t is the observed reward at trial t, and G j,t is defined as:
G j,t = v j,t−1 v j,t−1 + θ 2 ε (6)
where θ 2 ε , referred to as the error variance, is the variance of the rewards around the mean. Intuitively, the estimated mean of the chosen option m j,t is updated based on prediction error, which is the difference between the observed reward y t and the prior expectation m j,t−1 , multiplied by learning rate G j,t ∈ [0, 1]. At the same time, the estimated variance v j,t of the chosen option is reduced by a factor 1 − G j,t . The error variance (θ 2 ε ) can be interpreted as an inverse sensitivity, where smaller values result in more substantial updates to the mean m j,t , and larger reductions of uncertainty v j,t . We set the prior mean to m j,0 = 0 based on the (unshifted) expectation across payoff conditions, and the prior variance is set to v j,0 = 55 * 20, which is also the expectation across payoff conditions, scaled by a constant multiple of 20. We use unshifted reward values (i.e., before adding the shift ∼ U 
(30,
60)
 were observed by participants), with the means in each condition centered on 0. For our model-based analysis, the error variance θ 2 ε was set to the true underlying variance of the chosen option.


Hierarchical Bayesian Regression models


Mixed effects regressions
All Bayesian mixed effects regression models used Hamiltonian Markov chain Monte Carlo (MCMC) with a No-U-Turn sampler 
81
 and were implemented using brms 
82
 . All models used generic, weakly informative priors ∼ N (0, 1) with the proposal acceptance probability set to .99. In all cases, participants were assigned a random intercept and all fixed effects also had corresponding random effects following recommendations to apply a maximal random-effects structure 
83
 . All models were estimated over four chains of 4000 iterations, with a burn-in period of 1000 samples.


Softmax choice model
The softmax choice model was estimated hierarchically using custom code written in STAN. Formally, we assume that the α-and β -coefficients (see Equation 2) for each participant are drawn independently from a normal distribution:
α limited i , α unlimited i , β limited i , β unlimited i , γ limited i , γ unlimited i ∼ N (µ 0 , σ 2 0 ).
(7)
For simplicity, we use α i , β i , and γ i in Equation 2 to refer to
θ i = θ limited i + 1θ unlimited i , where θ ∈ [α, β , γ]
and 1 = 1 for unlimited time rounds, and 0 otherwise. We used Hamiltonian MCMC with a No-U-Turn sampler 
81
 to estimate the group-level mean µ 0 and variance over participants σ 2 0 for α, β , and γ, and their interaction with time pressure. We used the following priors on the group-level parameters:
µ 0 ∼ N (0, 1) (8) σ 2 0 ∼ N (0, 1) ∈ (0, ∞)
(9)
The posterior mean and uncertainty estimates of the BMT were standardized between [0,1] before being entered into the regression. The model was estimated over four chains of 4000 iterations, with a burn-in period of 1000 samples, and with the proposal acceptance probability set to .99.


RTs
The RT regression used the same Bayesian mixed effects framework as above, with log-transformed RTs as the dependent variable. 1 ms was added to each RT to avoid log(0), with the raw RTs truncated at a maximum of 5000 ms. Both dependent and independent variables were standardized to a mean of 0 and unit variance.


LBA
Formally, the LBA assumes that, after an initial period of non-decision time τ, evidence for option j accumulates linearly at a rate of v j , starting from an initial evidence level p j ∼ U(0, A). Evidence accumulates for each option j until a threshold b = A + k is reached. We follow the Bayesian implementation proposed by Ref 
61
 and assume that the priors for the drift rates stem from truncated normal distributions
v j ∼ N (2, 1) ∈ (0, ∞).
(10)
Additionally, we assume a uniform prior on non-decision time
τ ∼ U(0, 1),
(11)
and a truncated normal prior on the maximum starting evidence
A ∼ N (0.5, 1) ∈ (0, ∞).
(12)
Finally, we reparameterized the model by shifting b by k units away from A, and put a truncated normal distribution as the prior on the resulting relative threshold k:
k ∼ N (0.5, 1) ∈ (0, ∞).
(13)
We estimated the LBA parameters (see 
Fig. S12
) for each participant in every round separately using No-U-Turn Hamiltonian MCMC 
81
 , with reaction times truncated at 5000 ms. The drift rate regression used the same Bayesian mixed effects framework as above, with both DVs and IVs standardized to a mean of 0 and unit variance.


Supporting information
Time pressure changes how people explore and respond to uncertainty Both frequentist and Bayesian statistics are reported throughout this paper. Frequentist tests are reported as Student's t-tests (specified as either paired or independent). Each of these tests are accompanied by a Bayes factors (BF) to quantify the relative evidence the data provide in favor of the alternative hypothesis (H A ) over the null (H 0 ). This is done using the default two-sided Bayesian t-test for either independent or dependent samples, where both use a Jeffreys-Zellner-Siow prior with its scale set to √ 2/2, as suggested by Ref 
85
 . All statistical tests are non-directional as defined by a symmetric prior.
Correlations.
For testing linear correlations with Pearson's r, the Bayesian test is based on Jeffreys 
86
 test for linear correlation and assumes a shifted, scaled beta prior distribution B( 1 k , 1 k ) for r, where the scale parameter is set to k = 1 3 
87
 . Note that when performing group comparisons of correlations computed at the individual level, we report the mean correlation and the statistics of a single-sample t-test comparing the distribution of z-transformed correlation coefficients to µ = 0.
For testing rank correlations with Kendall's tau, the Bayesian test is based on parametric yoking to define a prior over the test statistic 
88
 , and performing Bayesian inference to arrive at a posterior distribution for r τ . The Savage-Dickey density ratio test is used to produce an interpretable Bayes Factor.


ANOVA.
We use a two-way analysis of variance (ANOVA) to compare the means of p ≥ 2 samples based on the F distribution. In general terms, we can define ANOVA as a linear model:
y = µ1 + σ Xθ θ θ + ε ε ε (14)
where y is a vector of N observations, µ is the aggregate mean, 1 is a column vector of length N, σ is the scale factor, X is the N × p design matrix, θ θ θ is a column vector of the standardized effect sizes, and ε ε ε is a column vector containing the i.i.d. errors where ε i
i.i.d ∼ N (0, σ 2 )
. We assume independent g-priors 
89
 for each effect size
θ 1 ∼ N (0, g 1 σ 2 ), • • • , θ p ∼ N (0, g p σ 2 )
, where each g-value is drawn from an inverse chi-square prior with a single degree of freedom g i
i.i.d ∼ inverse-χ 2 (1).
For µ and σ 2 we assume a Jeffreys 
90
 prior. Following Ref 
91
 , we compute the Bayes factor by integrating the likelihoods with respect to the prior on parameters, where Monte Carlo sampling was used to approximate the g-priors. The Bayes factor reported in the text can be interpreted as the log-odds of the model relative to an intercept-only null model.


Supplementary Behavioral results
Raw RTs. 
Figure S3a
 shows the distribution of participant reaction times (RTs) split by time pressure and payoff conditions. Using a two-way within subject ANOVA, we found that participants (unsurprisingly) responded faster in limited time (F(1, 98) = 13.8, p < .001, η 2 = .016, BF > 100), but with no differences across payoff conditions (F(3, 98) = 0.684, p = .562, η 2 = .002, BF = 0.005). Additionally, we find that participants sped up over trials (average correlation:r = −.54; one-sample t-test against zero using  
Figure S1
. Regression coefficients. Visualization of regression coefficients, corresponding to the models in 
Table S1
. The vertical grey line represents chance, and estimates above are in blue, while estimates below are in red (irrespective of significance). The inner horizontal line indicates the 50% HDI and the outer line indicates 89% HDI.
z-transformed correlation coefficients: t(98) = −17.6, p < .001, d = 1.8, BF > 100; 
Fig. S3b
), with a strong speed up in unlimited time (paired t-test comparing z-transformed correlation coefficients: t(98) = 4.5, p < .001, d = 0.5, BF > 100). We see a similar speed-up over rounds (average correlation: t(98) = −9.4, p < .001, d = 0.9, BF > 100; 
Fig. S3c
), which was also more pronounced under unlimited time (t(98) = 4.4, p < .001, d = 0.5, BF > 100).


Too slow analyses
We also analysed the pattern of "too slow" responses that exceeded 400ms during the limited time condition, where participants received no reward. Using a one-way within subject ANOVA, we found no differences in the number of "too slow" responses across payoff conditions (F(3, 98) = 0.088, p = .966, η 2 = .001, BF = 0.012; 
Fig. S4a
). Too slow responses tended to follow lower-valued reward observations (paired t-test: t(93) = −6.2, p < .001, d = 0.8, BF > 100; omitting 5 participants who never had slow responses; 
Fig. S4b
). However, this effect is mediated by trial number, since the vast majority of "too slow" responses occurred on the second trial 
(Fig. S4c
). Since the learning curves in 
Fig. 2a
 indicate gradual increases in reward over trials (except for the Equal Means condition), a tendency for "too slow" responses to take place during early trials will generally correspond to lower reward observations. In order to simultaneously model the influence of payoff conditions, trials, and reward observations on the tendency to produce a "too slow" response, we fit a Bayesian logistic mixed-effects model 
(Fig. S4d
). We again found no influence of payoff condition (all estimates overlapping with an odds ratio of 1). However, we find a strong effect of trial , such that both later trials and larger rewards observations were less likely to produce "too slow" responses.  , where DV is the dependent variable (columns), and PayoffConditions were defined using dummy coding, with the baseline being the Equal Means condition. We report the posterior mean and 95% highest posterior density (HPD) interval below in brackets. The 'Repeats' model is a Binomial regression based on 19 successive Bernoulli trials (since the first trial cannot be a repeat). The 'P(Repeat)' model is a logistic regression, with the previous reward value added as an additional predictor. Both the 'Repeats' and 'P(Repeat)' models are reported as Odds Ratios. σ 2 indicates the individual-level variance, τ 00 indicates the variation between individual intercepts and the average intercept, and ICC is the intraclass correlation coefficient. Model coefficients are visualized in 
Figure S1
. , where DV is the dependent dependent binary variable representing whether the highest variance option was chosen. In the IGT regression, we only consider choices where the two highest mean reward options were chosen ('O' and 'P'), where DV = 1 when 'P' was chosen, and zero otherwise. For the Equal Means condition, we include all choices. We report the posterior odds ratio and 95% highest posterior density (HPD) interval below in brackets. σ 2 indicates the individual-level variance, τ 00 indicates the variation between individual intercepts and the average intercept, and ICC is the intraclass correlation coefficient.  
Figure S5
. BMT predictions about the chosen option simulated for all participants. Lines indicate group means, with ribbons showing the 95% CI. a) Expected rewards (posterior mean) increase over successive trials, showing how the model tracks learning. The uptick in the Equal Means condition, followed by a decay back to zero indicates participants persevered after high reward observations stemming from the underlying variance, which then regressed back to the mean of 0. b) Posterior uncertainty (stdev) decays as participants exploit options with diminishing uncertainty. c) Relative reward shows the difference between the posterior mean of the chosen option and the average posterior mean of the unchosen options. Relative reward is always valued positively (dashed line indicates 0). d) Relative uncertainty shows the difference between the posterior uncertainty (stdev) of the chosen option and the average posterior uncertainty of the unchoen options. The early upticks indicates uncertainty directed exploration (substantially less in limited time), followed by exploitation as this value decays below zero (dashed line). e) Total uncertainty (stdev) decays monotonically, with a faster decline in unlimited time due to more uncertainty directed exploration.  
Figure S6
. Comparison of Softmax parameters and behavior. Each dot shows the mean posterior for each participant in each time condition, while the lines and ribbons are a linear regression and 95% CI. a) Higher α estimates correspond to higher rewards in both conditions (unlimited time: r τ = .55, p < .001, BF > 100; limited time: r τ = .55, p < .001, BF > 100). However, we only find a reliable effect of β under time pressure (r τ = .25, p < .001, BF > 100), but not with unlimited time (r τ = .13, p = .055, BF = .81). This suggests that the lower overall performance under time pressure, may have a result of the reduction in uncertainty directed exploration 
(Fig. 3a)
. We find no relationship between stickiness and rewards (unlimited time: r τ = .13, p = .052, BF = .85; limited time: r τ = .08, p = .265, BF = .24). b) We find no correlation between α and choice entropy (unlimited time: r τ = −.13, p = .053, BF = .84; limited time: r τ = −.01, p = .835, BF = .13). However, higher β estimates generated higher entropy choices in both conditions (unlimited time: r τ = .26, p < .001, BF > 100; limited time: r τ = .36, p < .001, BF > 100), while higher γ were related to lower entropy (unlimited time: r τ = −.53, p < .001, BF > 100; limited time: r τ = −.41, p < .001, BF > 100). c) Similar to choice entropy, we find no relationship between α and the frequency of repeat choices (unlimited time: r τ = .06, p = .384, BF = .19; limited time: r τ = −.04, p = .545, BF = .16). However, higher β estimates were correlated with less repeat choices in limited time (r τ = −.30, p < .001, BF > 100), and more weakly correlated in unlimited time (r τ = −.19, p = .006, BF = 5.4). Stickiness γ was unsurprisingly correlated with more repeat choices in both conditions (unlimited time: r τ = .73, p < .001, BF > 100; limited time: r τ = .65, p < .001, BF > 100). In all plots, Tukey's fence has been applied to omit outliers for clearer visualizations, but all data are included in the statistical tests. b) Variant, also without stickiness, but where the value-directed component was scaled by the total uncertainty (across options) as a method to regulate higher random exploration when the total uncertainty is high (following Ref 
24
 ; see 
Fig. S8 for details)
. Here, we get negative uncertainty bonus β estimates for both conditions. Both models provide worse fits to the data 
(Fig. S8)
 compared to the sticky softmax model reported in 
Figure 3a
 .


2/21


4/21
11/21 The sticky model is reported in the main text, while the non-sticky model omits the γ term. Lastly, the total uncertainty scaled model (TUscaled) also omits the stickiness parameter, but rescales the value estimates going into the softmax function by the total uncertainty across all four options to account for changes in random exploration as a function of total uncertainty 
24
 :
Q j,t = α(m j,t +β √ v j,t ) ∑ k √ v k,t
. Each dot is a single participant (connected by lines across models),with overlaid Tukey boxplots and the diamond indicating the group mean. The significance tests are Bayes  
Figure S12
. LBA parameters. Mean posterior parameter estimates of the LBA model, where each pair of connected dots is a single participant. Tukey boxplots show the group statistics, with the diamond indicating group means. τ is the non-decision time, A is the maximum starting evidence, k is the relative threshold, mean_drift is the average drift rate across all four options 1 4 ∑ j v j , and max_drift_diff is the largest pairwise difference in drift rates max i = j |v i − v j |.  
Figure S13
. Comparison of LBA parameters and behavior. Each dot shows the mean posterior for each participant in each time condition, while the lines and ribbons are a linear regression and 95% CI. τ is the non-decision time, A is the maximum starting evidence, k is the relative threshold, mean_drift is the average drift rate across all four options 1 4 ∑ j v j , and max_drift_diff is the largest pairwise difference in drift rates max i = j |v i − v j |. a) The only meaningful correlation between rewards and LBA parameters was found for maximum starting evidence A under time pressure (r τ = −.25, p < .001, BF = 89), where participants who were closer to making a decision prior to the start of a trial, earned lower payoffs. b) We find the strongest relationships between both drift rate variables and choice entropy, which were similar across time conditions. Participants with higher mean drift had more entropic choices (unlimited: r τ = .37, p < .001, BF > 100; limited: r τ = .32, p < .001, BF > 100), whereas participants with larger differences in drift rate were less entropic (unlimited: r τ = −.49, p < .001, BF > 100; limited: r τ = −.57, p < .001, BF > 100). We also find a weak correlation where higher maximum starting evidence was correlated with higher entropy for limited time rounds (r τ = .15, p = .024, BF = 1.6), and a moderate correlation where longer non-decision time corresponded to more entropic choices in unlimited time rounds (r τ = .22, p = .002, BF = 18). c) Similar to choice entropy, we again find the strongest relationship between the drift rate variables and the frequency of repeat choices, where higher mean drift produced less repeats (unlimited: r τ = −.41, p < .001, BF > 100; limited: r τ = −.28, p < .001, BF > 100), and larger differences in drift rate produced more repeat choices (unlimited: r τ = .49, p < .001, BF > 100; limited: r τ = .58, p < .001, BF > 100). We also find that higher starting evidence was correlated with more repeat choices in limited time rounds (r τ = .58, p < .001, BF > 100). In all plots, Tukey's fence has been applied to omit outliers for clearer visualizations, but all data are included in the statistical tests. Generating Parameter


15/21


Recovered Parameter
Limited Time
Unlimited Time 
Figure S14
. LBA recovery. Parameter recovery analysis, where the generating parameters on the x-axis were used to simulate participant choices and reaction times, upon which we used the same LBA estimation procedure and recovered the parameter estimates shown on the y-axis. Each dot is the posterior mean of a single participant (separated by time condition, but averaged across payoff conditions), with the line and ribbon showing a linear regression ± 95% CI. All parameters were recoverable in each time condition (all r τ > .59; BF > 100). τ is the non-decision time (unlimited time: r τ = .76, p < .001, BF > 100; limited time: r τ = .76, p < .001, BF > 100). A is the maximum starting evidence (unlimited time: r τ = .88, p < .001, BF > 100; limited time r τ = .86, p < .001, BF > 100). k is the relative threshold (unlimited time: r τ = .84, p < .001, BF > 100; limited time: r τ = .84, p < .001, BF > 100). Drift is the drift rate for each of the four options v j (unlimited time: r τ = .59 p < .001, BF > 100; limited time: r τ = .66, p < .001, BF > 100). max_drift_diff is the largest pairwise difference in drift rates max i = j |v i − v j | (unlimited time: r τ = .64, p < .001, BF > 100; limited time: r τ = .63, p < .001, BF > 100).
Figure 1 .
1
Experimental design. a) Time bandit task, where each option was randomly mapped to the [Q,W, O, P] keys on the keyboard, with a different mapping each round. Participants completed 40 rounds (each containing 20 trials), where we manipulated time pressure (panel b) and payoff conditions (panel c) in a crossed, within-subject design. b)


Figure 3 .
3
Posterior estimates of model-based analyses. a) Hierarchical softmax model. Expected rewards and uncertainties were regressed onto choice probability (Eqs 1-2). The top row shows the value-directed component (α), the middle row shows the uncertainty bonus (β ), and the bottom row shows stickiness (γ). b) Hierarchical RT model. The influence of relative reward (top), relative uncertainty (middle), and total uncertainty (bottom) on RTs. c) LBA drift regression. Relative reward (top), relative uncertainty (middle), and total uncertainty (bottom) were used to predict the drift rate of an LBA. In all plots, the vertical dashed line indicates an effect of 0, while the black dot indicates the mean effect and confidence intervals show the 66% (thick) and 95% (thin) highest density interval.


pressure also reduced uncertainty-directed exploration (β Unlimited − β Limited = 0.09 [0.04, 0.15]), with positive estimates in both conditions (β Unlimited = .26 [.20, .32]; β Limited = .16 [.08, .24]).


Figure 4 .
4
Fig. 3b) indicate that higher relative reward expectations BMT predictions about the chosen option simulated for all participants (seeFig S5 formore detailed plots, separated by payoff condition). Lines indicate group means, with ribbons showing the 95% CI. a) Relative reward shows the difference between the posterior mean of the chosen option and the average posterior mean of the unchosen options. Relative reward is always valued positively (dashed line indicates 0). b) Relative uncertainty shows the difference between the posterior uncertainty (stdev) of the chosen option and the average posterior uncertainty of the unchosen options. The early upticks indicates uncertainty-directed exploration (substantially less in limited time), followed by exploitation as this value decays below zero (dashed line). c) Total uncertainty (average stdev) decays monotonically, with a faster decline in unlimited time due to more uncertainty directed exploration.produced faster choices under limited time (b Limited = −.08 [−.11, −.05]), but with a weaker effect under unlimited time (b Unlimited = −.02 [−.04, .01]) that overlapped with zero. In contrast, both relative and total uncertainty slowed down choices (relative uncertainty: b Unlimited = .15 [.10, .19]; total uncertainty: b Unlimited = .22 [.19, .26]), with the latter having a larger effect. In both cases, this uncertainty-related slowdown was reliably less pronounced when placed under time pressure (relative uncertainty: b unlimited − b limited = −.10 [−.14, −.06]; total uncertainty: b unlimited − b limited = −.11 [−.14, −.08])


revealed that higher relative reward expectations amplified evidence accumulation equally for limited and unlimited time (b = .36 [.31, .41]; no interaction with time pressure: b Unlimited -Limited = −.005 [−.061, .051]). Thus, options with higher relative reward expectations were more likely to be chosen and with faster decision times. Conversely, relative uncertainty (specific to each option) had a negative effect on drift rate, thus dampening evidence accumulation (b Unlimited = −.39 [−.44, −.34]), with a reliably smaller effect under time pressure (b Limited = −.31 [−.36, −.27]; b Unlimited -Limited = −.08 [−.14, −.01]). Lastly, total uncertainty (computed across all options) also dampened evidence accumulation in limited time rounds (b Limited = −.06 [−.09, −.03]), but did not produce a reliable effect in unlimited time (b Unlimited = −.03 [−.07, .01]). Thus, rewards increased evidence accumulation, while uncertainty (in general) slowed down evidence accumulation.


(no interaction with time pressure: b = −.08 [−.14, −.02]; Fig. S17g) and relative uncertainty (b Limited = −.15 [−.18, −.12]; b Unlimited = −.09 [−.13, −.06];


Charley M. Wu, Eric Schulz, Timothy J. Pleskac, & Maarten Speekenbrink Statistics Comparisons.


Figure S2 .Figure S3 .
S2S3
Raw Behavioral Data Split By Conditions. a) Average performance across payoff and time conditions. Each dot is a single participant, with overlaid Tukey boxplots and diamonds indicating the group means. b) Entropy of choices by payoff and time conditions, where the dashed line indicates a random baseline. c) Repeat clicks as a function of trial, where each dot is the group mean and the lines are a locally smoothed regression. d) Repeat clicks as a function of previous reward value. Ribbons indicate the 95% CI. Reaction Times (RTs). All RTs are shown in milliseconds (ms) and on a log scale. Outliers greater than 5000ms are omitted from the plots, but not from the analyses. a) RT distributions separated by payoff and time conditions. The dashed line indicates the 400 ms limit for limited time choices. b) RTs as a function of trial. Each dot is the aggregate mean, with the lines and ribbons indicating the mean and 95% confidence intervals of a generalized additive regression. c) RTs as a function of Round.


Figure S4 .
S4
"Too slow" analyses. All analyses were performed solely on limited time rounds. a) The average number of slow trials (>400ms), separated by payoff condition. Bars indicate group means, while error bars show the 95% CI. b) Average reward observation immediately preceding a trial that was either too slow (orange) or not (green), separated by Payoff condition. c) The probability of a response being too slow as a function of trial, with each line showing the group means for each payoff condition and the ribbons indicating the 95% CI. d) Regression coefficients of the Bayesian logistic mixed effects model, depicted as odds ratios with the vertical grey line indicating chance. The inner horizontal line indicates the 50% HDI and the outer line indicates the 89% HDI.


Figure S7 .
S7
Alternative Softmax Model Posteriors. Posterior estimates for alternative formulations of the softmax model. a) Variant without stickiness, which yields negative uncertainty bonus β estimates for limited time.


Figure S8 .
S8
Model comparison. Comparing three variants of the hierarchical softmax choice model using Deviance Information Criterion 92 DIC = −2 log E θ p(y|θ ) + 2p D , where the effective number of parameters is defined as p D = V θ (−2 log p (y|θ )).


Figure S11 .
S11
Factors (BF) corresponding to paired Bayesian t-tests. The sticky model beats the non-sticky model (t(98) = −13.2, p < .001, d = 0.7, BF > 100), the sticky model beats the TUscaled model (t(98) = −15.6, p < .001, d = 0.7, BF > 100), and there are no reliable differences between the non-sticky and TUscaled models (t(98) = −2.1, p = .040, d = 0.0, BF = .87). relSig = high & totalSig = low relSig = high & totalSig = med relSig = high & totalSig = high relSig = med & totalSig = low relSig = med & totalSig = med relSig = med & totalSig = high relSig = low & totalSig = low relSig = low & totalSig = med relSig = low & totalSig = & totalSig = low relMu = high & totalSig = med relMu = high & totalSig = high relMu = med & totalSig = low relMu = med & totalSig = med relMu = med & totalSig = high relMu = low & totalSig = low relMu = low & totalSig = med relMu = low & totalSig = & relSig = low relMu = high & relSig = med relMu = high & relSig = high relMu = med & relSig = low relMu = med & relSig = med relMu = med & relSig = high relMu = low & relSig = low relMu = low & relSig = med relMu = low & relSig = Four-way interactions for RT mixed effects regression. Four-way interactions of the RT Bayesian mixed effects regression illustrated in Fig. S9. Interactions are grouped in terms of relative means (a) and relative uncertainty (b). Continuous variables are split into discrete [low, med, high] levels, based on [mean − sd, mean, mean + sd], with relative uncertainty (relSig) increasing top to bottom (rows) and total uncertainty (totalSig) increasing from left ro right (columns). RelMu: relative reward; RelSig: relative uncertainty; TotalSig: total uncertainty.


Looking first at average reward(Fig 2a), we find that time pressure played a reliable role in reducing rewards (∆ EMM = −.19 [−0.29, −0.10]; all Bayesian estimates include the 95% Highest Density Interval in square brackets). There was also substantial variation across payoff conditions. Participants performed better in the IGT-like condition than in the Low Var condition (∆ EMM = 0.12 [0.04, 0.20]). We see an even larger difference when comparing the Low Var and High Var conditions (∆ EMM = .
24
 [0.15, 0.33]), where despite having the same expected rewards for each option, participants performed substantially better with lower variance. Lastly, participants performed better in High Var than in the Equal Means condition (∆ EMM = 1.02 [0.93, 1.11]), which is intuitive since improvement is not possible if all arms have the same expected reward. Binomial regression, modeling the number of repeats as the result of 19 independent Bernoulli trials, since the first choice cannot be a repeat by definition. Participants made overwhelmingly more repeat choices under time pressure (Odds Ratio (OR): ∆ EMM = 1.40 [1.22, 1.58]). While we see relatively small variation across payoff conditions, the Low Var condition had more repeats than the High Var condition (OR: ∆ EMM = 1.34 [1.23, 1.47]; see
Learning curves
Entropy and Repeat Choices
d
Repeat Response Curve
Next, we assessed the overall diversity of choices by calculating the Shannon entropy 53 of choice
distributions in each round (Fig 2b). Participants made less diverse and lower entropy choices under
limited time (∆ EMM = −0.12 [−.23 − 0.01]). This provides initial evidence for reduced exploration under time pressure. We also find largely overlapping entropy levels among the different payoff conditions, but P(repeat) with Equal Means having the most diverse choices (compared against High Var: ∆ EMM = 0.34 [0.27, 0.42]).
This suggests that in the face of indiscernible differences in reward expectations, participants increased
Limited Time Unlimited Time Additionally, we modeled the number of repeat choices in each round as a measure of sequential their exploration.
dependency between choices (Fig 2c). We used a
Previous Reward
e
Arm frequency
IGT
Low Var
High Var
Equal Means
p(choice)−p(chance)
−2% 0% 2% 4%
P>O
−4% −2% 0% 2%
P>Other
−1% 0% 1% 2%
Limited Time Unlimited Time
Q
W
O
P
Q
W
O
P
Q
W
O
P
Q
W
O
P
Arms
Figure 2. Behavioral results. a) Learning curves depicting average participant performance (lines) ± standard
error of the mean (ribbons) over trials (using unshifted rewards), faceted by payoff condition. The inset figures show
the expected reward ± standard deviation of each payoff condition for reference. IGT refers to payoffs inspired by
the Iowa Gambling Task (see Methods). b) Choice entropy in each round, where higher entropy corresponds to more
diverse choices and dotted lines indicate random chance (i.e., playing each arm with uniform probability). Each
connected dot represents a participant, and overlaid are Tukey boxplots with the diamond indicating the group mean.
c) The proportion of repeat clicks across time conditions, where each connected dot is a single participant, with
overlaid Tukey boxplots and the diamond indicating the group mean. d) Repeat choices as a function of the previous
(unshifted) reward value. Each dot is the aggregate mean, and lines represent a locally smoothed Generalized
Additive Model regression estimate, with the ribbon indicating the 95% confidence interval. e) Aggregate choice
proportions (normalized for chance) for each option, mapped to the canonical ordering shown in panel a (inset).
Error bars indicate the 95% CI. The inset plots show a preference for the 'P' option over the 'O' option in the IGT
condition, and a preference for the 'P' option over all others in the Equal Means condition. See Fig. S1 for a
Bayesian mixed effects regression of the behavioral results, and Figs. S2-S4 for additional behavioral analyses.


(OR: 0.88 [.86, .91]) and a reliable but small effect of previous reward (OR: 0.99 [.98, .99])


Table S1 .
S1
Bayesian Mixed Effects Regression: Experimental Manipulations
Avg. Reward Choice Entropy
Repeats
P(Repeat)
Estimate
Estimate
Odds Ratio
Odds Ratio
σ 2
0.13
0.41
12.69
0.01
τ 00
0.88
0.59
4.47
0.21
ICC
0.13
0.41
0.72
0.04
N Participant
99
99
99
99
Observations
3960
3960
3960
76240
Bayesian R 2
0.43
0.46
0.65
0.26
Note: Each model was defined as DV ∼ TimePressure * PayoffConditions * Round + (1
+ TimePressure + PayoffConditions + Round | Participant)


Table S2 .
S2
Bayesian Mixed Effects Logistic Regression: Choice Probability for Highest Variance Option
IGT
Equal Means
Odds Ratio
Odds Ratio
Intercept
0.91
0.24
[0.69, 1.20]
[0.18, 0.33]
UnlimitedTime
1.11
1.45
[0.80, 1.53]
[1.11, 1.87]
Round
0.83
1.00
[0.68, 1.02]
[0.99, 1.02]
UnlimitedTime:Round
1.39
0.99
[1.23, 1.57]
[0.98, 0.99]
Random Effects
σ 2
0.00
0.02
τ 00
0.25
0.17
ICC
0.00
0.00
N Participant
99
99
Observations
10230
19800
Bayesian R 2
0.194
0.134
Note: Each model was defined as DV ∼ TimePressure * Round + (1 + TimePressure
+ Round|Participant)








Acknowledgments
We thank Rahul Bhui and Irene Cogliati Dezza for helpful feedback on earlier drafts of the manuscript, and Kimberly Gerbaulet for help with data collection. CMW is supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A and funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy-EXC2064/1-390727645. An early version of this work was presented at the 41st Annual Conference of the Cognitive Science Society 
84
 .






Data and Code Availability
Code and data are publicly available at https://osf.io/v4dua/.


Author Contributions
All authors designed the experiment. CMW collected and analyzed the data, with contributions from ES. CMW drafted the initial manuscript with input from ES, TJP, and MS. All authors contributed to final manuscript.


Competing financial interests
The authors declare no competing financial interests.   
Figure S9
. Coefficient plot of RT mixed effects regression. Posterior estimates of the Bayesian mixed effects regression predicting (log) RT. The mean posterior estimate is displayed numerically and indicated by the black dot, while the 95% HPD is illustrated by the length of the horizontal line. Coefficients are sorted by largest to smallest, with blue and red colors corresponding to estimates that are above or below 0, respectively, but do not indicate whether the difference is meaningful. See Figs. S10-S11 for interaction plots. RelMu: relative reward; RelSig: relative uncertainty; TotalSig: total uncertainty.   
Figure S15
. LBA simulations. a) Observed vs. simulated choice probabilities. For each set of participant parameter estimates, we generated 10k simulated choices that we used to define choice probabilities for each option (y-axis), which we compared against participants' observed choice probabilities (x-axis). Each dot represents the choice probabilities for each option for each set of parameter estimates. The colored line is a linear regression, with the ribbon showing the 95% CI. b) Observed vs. simulated RTs Using the same set of simulated data as above, we created a matched dataset where we sampled a simulated RT value yoked to each participant set of observed choices (i.e., an RT corresponding to the chosen option rather than any of the four options). We then aggregated the data by payoff condition and time pressure, and computed 20-quantiles along the participant RTs (x-axis), plotting the median (dot) and 95% CI (error bar) for the corresponding simulated RTs.  


Supplementary Model Results
0


13/21
 










The neural basis of the speed-accuracy tradeoff




R
Bogacz






E.-J
Wagenmakers






B
U
Forstmann






S
Nieuwenhuis








Trends neurosciences




33
















Speed-accuracy tradeoff and information processing dynamics




W
A
Wickelgren








Acta psychologica




41
















Rational approximations to rational models: alternative algorithms for category learning




A
N
Sanborn






T
L
Griffiths






D
J
Navarro








Psychol. review




117


1144














Resource-rational analysis: understanding human cognition as the optimal use of limited computational resources




F
Lieder






T
L
Griffiths








Behav. Brain Sci




43














Resource-rational decision making




R
Bhui






L
Lai






S
J
Gershman








Curr. Opin. Behav. Sci




41


















R
Hertwig






T
J
Pleskac




Pachur, T. & Center for Adaptive Rationality. Taming uncertainty




Mit Press














Unpacking the exploration-exploitation tradeoff: A synthesis of human and animal literatures




K
Mehlhorn








Decis




2


191














The algorithmic architecture of exploration in the human brain




E
Schulz






S
J
Gershman








Curr. Opin. Neurobiol




55
















Should i stay or should i go? how the human brain manages the trade-off between exploitation and exploration




J
D
Cohen






S
M
Mcclure






A
J
Yu








Philos. Transactions Royal Soc. B: Biol. Sci




362
















Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








MIT press






second edn








Nonapproximability results for partially observable markov decision processes




C
Lusena






J
Goldsmith






M
Mundhenk








J. Artif. Intell. Res




14
















Modeling human decision making in generalized gaussian multiarmed bandits




P
B
Reverdy






V
Srivastava






N
E
Leonard








Proc. IEEE


IEEE






102














A dynamic allocation index for the discounted multiarmed bandit problem




J
C
Gittins






D
M
Jones








Biom




66
















Bandit processes and dynamic allocation indices




J
C
Gittins








J. Royal Stat. Soc. Ser. B (Methodological


















Asymptotically efficient adaptive allocation rules




T
L
Lai






H
Robbins








Adv




6
















Learning models in decision making




T
J
Pleskac








The Wiley Blackwell Handbook of Judgment and Decision Making


Keren, G. & Wu, G.




Wiley Blackwell




2














Balancing exploration and exploitation with information and randomization




R
C
Wilson






E
Bonawitz






V
D
Costa






R
B
Ebitz








Curr. Opin. Behav. Sci




38
















Humans use directed and random exploration to solve the explore-exploit dilemma




R
C
Wilson






A
Geana






J
M
White






E
A
Ludvig






J
D
Cohen








J. Exp. Psychol. Gen




143
















Uncertainty and exploration




S
J
Gershman








Decis
















Uncertainty and exploration in a restless bandit problem




M
Speekenbrink






E
Konstantinidis








Top. Cogn. Sci




7
















Generalization guides human exploration in vast decision spaces




C
M
Wu






E
Schulz






M
Speekenbrink






J
D
Nelson






B
Meder




10.1038/s41562-018-0467-4






Nat. Hum. Behav




2
















Structured, uncertainty-driven exploration in real-world consumer choice




E
Schulz








Proc. Natl. Acad. Sci. 201821028


Natl. Acad. Sci. 201821028
















Dopamine: generalization and bonuses




S
Kakade






P
Dayan








Neural Networks




15
















Deconstructing the human algorithms for exploration




S
J
Gershman








Cogn




173
















Gaussian process optimization in the bandit setting: No regret and experimental design




N
Srinivas






A
Krause






S
M
Kakade






M
Seeger




arXiv:0912.3995










arXiv preprint








Representativeness revisited: Attribute substitution in intuitive judgment




D
Kahneman






S
Frederick








Heuristics biases: The psychology intuitive judgment






49


81












A timely account of the role of duration in decision making




D
Ariely






D
Zakay








Acta psychologica




108
















Sources of error in naturalistic decision making tasks




G
Klein








Proceedings of the Human Factors and Ergonomics Society Annual Meeting


the Human Factors and Ergonomics Society Annual Meeting
Sage CA; Los Angeles, CA




SAGE Publications




37














Assessing the speed-accuracy trade-off effect on the capacity of information processing




C
Donkin






D
R
Little






J
W
Houpt








J. Exp. Psychol. Hum. Percept. Perform




40


1183














Tuning the speed-accuracy trade-off to maximize reward rate in multisensory decision-making




J
Drugowitsch






G
C
Deangelis






D
E
Angelaki






A
Pouget








Elife




4


6678














Do humans produce the speed-accuracy trade-off that maximizes reward rate? The Q




R
Bogacz






P
T
Hu






P
J
Holmes






J
D
Cohen








J. Exp. Psychol




63
















Taxing cognitive capacities reduces choice consistency rather than preference: A model-based test




S
Olschewski






J
Rieskamp






B
Scheibehenne








J. Exp. Psychol. Gen




147


462














Distinguishing three effects of time pressure on risk taking: Choice consistency, risk preference, and strategy selection




S
Olschewski






J
Rieskamp








J. Behav. Decis. Mak
















Rapid makes risky: Time pressure increases risk seeking in decisions from experience




C
R
Madan






M
L
Spetch






E
A
Ludvig








J. Cogn. Psychol




27
















Temporal discounting predicts risk sensitivity in rhesus macaques




B
Y
Hayden






M
L
Platt








Curr. Biol




17
















The effect of emotion and time pressure on risk decision-making




Y
Hu






D
Wang






K
Pang






G
Xu






J
Guo








J. Risk Res




18
















Time pressure in risky decision-making: effect on risk defusing




O
Huber






U
Kunz








Psychol. Sci




49


415














Tempus fugit: time pressure in risky decisions




M
G
Kocher






J
Pahlke






S
T
Trautmann








Manag. Sci




59
















Effects of time-pressure on decision-making under uncertainty: changes in affective state and information processing strategy




A
J
Maule






G
R J
Hockey






L
Bdzola








Acta psychologica




104
















Decision making under time pressure, modeled in a prospect theory framework. Organ. behavior human decision processes 118




D
L
Young






A
S
Goodie






D
B
Hall






E
Wu




















Rationally inattentive intertemporal choice




S
J
Gershman






R
Bhui








Nat. communications




11
















Ssl: a theory of how people learn to select strategies




J
Rieskamp






P
E
Otto








J. Exp. Psychol. Gen




135


207














Oops, i did it again-relapse errors in routinized decision making. Organ. behavior human decision processes 93




T
Betsch






S
Haberstroh






B
Molter






A
Glöckner




















Origin of perseveration in the trade-off between reward and complexity




S
J
Gershman








Cogn




204


104394














Decision making under uncertainty: a comparison of simple scalability, fixed-sample, and sequential-sampling models




J
R
Busemeyer








J. Exp. Psychol. Learn. Mem. Cogn




11


538














Risk and reward preferences under time pressure




A
D
Nursimulu






P
Bossaerts








Rev. Finance




18
















Information input overload and psychopathology




J
G
Miller








Am. journal psychiatry




116
















The dynamics of explore-exploit decisions reveal a signal-to-noise mechanism for random exploration




S
F
Feng






S
Wang






S
Zarnescu






R
C
Wilson








Sci. reports




11
















Where do hypotheses come from?




I
Dasgupta






E
Schulz






S
J
Gershman








Cogn. psychology




96
















Deep exploration as a unifying account of explore-exploit behavior




R
Wilson






S
Wang






H
Sadeghiyeh






J
D
Cohen








PsyArXiv




















L
Thorndike








Animal intelligence: Experimental studies
















Habits without values




K
J
Miller






A
Shenhav






E
A
Ludvig








Psychol. review




126


292














A mathematical theory of communication




C
E
Shannon








The Bell System Technical Journal




27
















Insensitivity to future consequences following damage to human prefrontal cortex




A
Bechara






A
R
Damasio






H
Damasio






S
W
Anderson








Cogn




50
















A unifying probabilistic view of associative learning




S
J
Gershman








PLoS Comput. Biol




11


1004567














A theory of pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement




R
A
Rescorla






A
R
Wagner








Class




2
















Expected and unexpected uncertainty: ACh and NE in the neocortex




A
J
Yu






P
Dayan








Advances in Neural Information Processing Systems


















Learning and decisions in contextual multi-armed bandit tasks




E
Schulz






E
Konstantinidis






M
Speekenbrink








Thirty-Seventh Annual Conference of the Cognitive Science Society
















Learning and selective attention




P
Dayan






S
Kakade






P
R
Montague








Nat. neuroscience




3
















The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cogn. Psychol




57
















Bayesian inference with Stan: A tutorial on adding custom distributions




J
Annis






B
J
Miller






T
J
Palmeri








Behav. Res. Methods




49
















Should we control? the interplay between cognitive control and information integration in the resolution of the exploration-exploitation dilemma




I
D
Cogliati






A
Cleeremans






W
Alexander








J. experimental psychology. Gen
















Cost-benefit arbitration between multiple reinforcementlearning systems




W
Kool






S
J
Gershman






F
A
Cushman








Psychol. science




28
















In pursuit of knowledge: Preschoolers expect agents to weigh information gain and information cost when deciding whether to explore




R
Aboody






C
Zhou






J
Jara-Ettinger








Child Dev
















Time pressure, training and decision effectiveness




D
Zakay






S
Wooler








Ergonomics




27
















Confidence modulates exploration and exploitation in value-based learning




A
Boldt






C
Blundell






B
De Martino








236026












It's new, but is it good? how generalization and uncertainty guide the exploration of novel options




H
Stojić






E
Schulz






P
Analytis






M
Speekenbrink








J. Exp. Psychol. Gen




149


1878














Uncertainty in learning, choice, and visual fixation




H
Stojić






J
L
Orquin






P
Dayan






R
J
Dolan






M
Speekenbrink








Proc. Natl. Acad. Sci




117
















Similarities and differences in spatial and non-spatial cognitive maps




C
M
Wu






E
Schulz






M
M
Garvert






B
Meder






N
W
Schuck




10.1371/jour-nal.pcbi.1008149






PLOS Comput. Biol




16
















Inference and search on graph-structured spaces




C
M
Wu






E
Schulz






S
J
Gershman




10.1007/s42113-020-00091-x.20/21






Comput. Brain & Behav




4
















Doubly bayesian analysis of confidence in perceptual decision-making




L
Aitchison






D
Bang






B
Bahrami






P
E
Latham








PLoS computational biology




11


1004519














Two-stage dynamic signal detection: a theory of choice, decision time, and confidence




T
J
Pleskac






J
R
Busemeyer








Psychol. review




117


864














Perceptual decision making: drift-diffusion model is equivalent to a bayesian model




S
Bitzer






H
Park






F
Blankenburg






S
J
Kiebel








Front. human neuroscience




8


102














Belief states and categorical-choice biases determine reward-based learning under perceptual uncertainty




R
Bruckner






H
R
Heekeren






D
Ostwald




10.1101/2020.09.18.303495
















The effect of cognitive load on economic decision making: A survey and new experiments




C
Deck






S
Jahedi








Eur. Econ. Rev




78
















Decomposing the effects of context valence and feedback information on speed and accuracy during reinforcement learning: a meta-analytical approach using diffusion decision modeling




L
Fontanesi






S
Palminteri






M
Lebreton








Cogn. Affect. & Behav. Neurosci




19
















Contextual modulation of value signals in reward and punishment learning




S
Palminteri






M
Khamassi






M
Joffily






G
Coricelli








Nat. communications




6
















Generalization and search in risky environments




E
Schulz






C
M
Wu






Q
J
Huys






A
Krause






M
Speekenbrink








Cogn. science




42
















Do learning rates adapt to the distribution of rewards?




S
J
Gershman








Psychon. bulletin & review




22
















Using cognitive models to map relations between neuropsychological disorders and human decision-making deficits




E
Yechiam






J
R
Busemeyer






J
C
Stout






A
Bechara








Psychol. Sci




16
















The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo




M
D
Hoffman






A
Gelman








J. Mach. Learn. Res




15
















Advanced bayesian multilevel modeling with the r package brms




P.-C
Bürkner








The R J




10
















Random effects structure for confirmatory hypothesis testing: Keep it maximal




D
J
Barr






R
Levy






C
Scheepers






H
J
Tily








J. memory language




68
















Under pressure: The influence of time limits on human exploration




C
M
Wu






E
Schulz






K
Gerbaulet






T
J
Pleskac






M
Speekenbrink








Proceedings of the 41st Annual Conference of the Cognitive Science Society


Goel, A., Seifert, C. & Freksa, C.


the 41st Annual Conference of the Cognitive Science Society
Montreal, QB


















Bayesian t tests for accepting and rejecting the null hypothesis




J
N
Rouder






P
L
Speckman






D
Sun






R
D
Morey






G
Iverson








Psychon. Bull. & Rev




16


















H
Jeffreys




The Theory of Probability


Oxford, UK




Oxford University Press














Harold jeffreys's default bayes factor hypothesis tests: Explanation, extension, and application in psychology




A
Ly






J
Verhagen






E.-J
Wagenmakers








J. Math. Psychol




72
















Bayesian inference for kendall's rank correlation coefficient. The




J
Van Doorn






A
Ly






M
Marsman






E.-J
Wagenmakers








Am. Stat




72
















Posterior odds ratios for selected regression hypotheses




A
Zellner






A
Siow








Bayesian Statistics: Proceedings of the First International Meeting


Bernardo, J. M., Lindley, D. V. & Smith, A. F. M.


Valencia (Spain












University of Valencia












An invariant form for the prior probability in estimation problems




H
Jeffreys








Proc. Royal Soc. Lond. Ser. A. Math. Phys. Sci




186
















Default bayes factors for anova designs




J
N
Rouder






R
D
Morey






P
L
Speckman






J
M
Province








J. Math. Psychol




56
















Bayesian data analysis




A
Gelman




1.46] [-0.52, -0.30






CRC press






1.29, 1.61









"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]