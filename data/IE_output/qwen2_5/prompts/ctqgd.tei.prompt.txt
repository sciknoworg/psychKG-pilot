You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Public significance statements
A theoretical study and four empirical experiments found that forcing medical experts to wait for 1.0 seconds before answering a binary labelling question of a medical image increased their job performance. The results consistent with the framework of resource rationality and suggest that there exists an optimal decisionmaking time. These results can be applied to a wide variety of decision-making scenarios.


Introduction


The need for simple and cost-effective interventions to enhance human's performance
Making rational judgments is not always an easy task for humans 
(Skirzynski et al., 2021)
. Recently, as the workforce have become increasingly dispersed on the internet due to the COVID-19 pandemic and the popularity of crowdsourcing platforms such as Amazon Mechanical Turk 
(Liithje, 2005)
, ensuring the quality of outputs from the geographically dispersed workforce has become a challenge, partly due to restricted teamwork capabilities and a lack of communication between employees 
(Lin et al., 2012;
Sheng & Zhang, 2019;
Yaron & Manas, 2013)
. Many methods for quality control of output made by the dispersed workforce on the internet have been proposed 
(Alagarai et al., 2014;
Daniel et al., 2018;
Faltings et al., 2014;
Heer & Bostock, 2010;
Hettiachchi et al., 2022;
Morris et al. 2012)
; however, they involve complex implementations that require substantial time and effort as well as explicit costs (i.e. issuing bonus payments to employees) and indirect cost (i.e. infrastructure cost to capture and analyse employee behaviour data) often attached to quality improvement methods. Thus, they are hardly practical.
Therefore, we propose the following research question: Can people's rational judgments be enhanced in a cost-effective manner?


Decision time and rational judgments
The speed-accuracy trade-off 
(Heitz, 2014;
Wickelgren, 1977)
 in human decision-making has been identified in previous studies. People are assumed to reduce various errors by taking more time to make decisions 
(Bruno et a., 2015;
Bye et al., 2021;
Fawver et al., 2020;
Inbar et al., 2010;
Kahneman 2003;
Kolvoort & van Maanen, 2021;
Phillips et al., 2016;
Rosenkrantz & Bansal, 2016)
. However, humans' cognitive resources are limited. Thus, the framework of resource rationality 
(Griffiths et al., 2015;
Lieder et al., 2018;
Lieder & Griffiths, 2020)
, which is a model that explains how human beings make good judgments with finite computational resources (i.e. processing power, memory, and attention), was developed. On the basis of resource rationality, the following two characteristics are expected for the relationship between decision time and decision-making. First, there may be an upper limit to the increase in correct answer rate, even with longer decision time. Second, taking too much time to complete a task may increase the cognitive load on people. That is, there may be a trade-off between improvements of people's correct answer rates and increases in their cognitive load as a function of decision time.
Therefore, in this study, we hypothesise that when people take an appropriate length of time to make decisions, the trade-off between improvements of correct response rates and increases in their cognitive load can be balanced, which, in turn, would improve job performance.


Objects
This research tests a hypothesis concerning the existence of a rational decision time that balances the tradeoff between an improvement in a people's correct answer rate and an increase in their cognitive load. Moreover, to propose a cost-effective intervention for enhance humans' rational judgments, this research experimentally tests a hypothesis claiming that an intervention that encourages people to wait an appropriate length of time before making decisions increases people's job performance.
The purposes of this research are as follows.
(1) Through the theoretical study, we confirm the existence of an optimal length of decision time that improves human's performance by balancing the trade-off between an improvement in human's judgments' accuracy and an increase in cognitive load.
(2) Through the large-scale behavioural experiments, we verify that prompting an appropriate decision time improves job performance. In particular, these empirical studies discuss whether peoples' job performance can be improved by disallowing them to make a judgment for a certain period of time after being presented with a task.
In this paper, the period of time during which people cannot make a judgment (i.e., the length of time during which people are forced to wait and prompted to think) is called the 'forced decision time'. Participants were made to wait and were disallowed to respond to the task for a certain forced decision time after the task is presented. This intervention was designed in reference to the boost framework 
(Folke et al., 2021;
Hertwig & Grüne-Yanoff, 2017)
, an intervention that aims to foster competence in making rational choices. These empirical studies focused on a real social situation in which medical experts labelled the medical image to identify the presence or absence of disease 1 .
In this manuscript, Section 3 shows the theoretical study. In Section 4, we provide an overview of the behavioural experiments. Section 5 details the empirical studies conducted with nurses, while Section 6 shows the empirical studies conducted with medical doctors. Section 7 covers the general discussion. Lastly, Section 8 is the conclusion.


Theoretical study: What is the optimal decision time for boosting performance?
In this section, we conduct a theoretical analysis of the optimal allocation of cognitive resources of humans in order to confirm that there is an optimal decision time. Our framework for this analysis comprises two components pertaining to task performance: cognitive utility and cognitive frugality.


Theoretical framework


Cognitive utility
Since various human errors may be reduced if humans spend more time thinking 
(Bruno et a., 2015;
Bye et al., 2021;
Fawver et al., 2020;
Inbar et al., 2010;
Kahneman 2003;
Kolvoort & van Maanen, 2021;
Phillips et al., 2016;
Rosenkrantz & Bansal, 2016)
, it is assumed that people become likelier to provide a correct judgment when thinking time is increased. However, based on the limited cognitive resources of a respondent, there is a certain upper limit to their rate of correct judgment. Furthermore, people's psychological responses are not represented with a linear relationship from a given dimension. One typical example is the utility involved in people's economic behaviour. It is known that the relationship between gain (or loss) and utility is not linear, and that the degree to which utility increases gradually decreases as gain (or loss) increases 
(Kahneman & Tversky, 1979)
. We assume 1 A detailed description of the social scenario assumed in this study is provided as follows: geographically dispersed medical experts label medical images to determine the presence or absence of disease. The detailed social scenario assumed in this study, including why the binary labelling of medical images was used, was explained in Supplement 1.
that there is a similar relationship between the length of time spent thinking (hereafter, decision time) and the accuracy of judgments (here called cognitive utility). In particular, for decision time , we denote cognitive utility as ( ). We represent ( ) with the following equation:
( ) = ℎ 1 + exp(− ) (Eq. 1)
where ℎ is the upper limit of the rate of correct judgments by the respondent, 0 ≤ ℎ ≤ 1, with 1.0 being the best performance. ( > 0) represents the individual difference in decision time when the rate of correct judgments reaches almost the maximum value.


Cognitive frugality
Frugality in cognitive processes has been noted from various perspectives. For example, Gigerenzer, in the discussion of fast-and-frugal heuristics, operationally defines frugality in terms of the amount of information considered in the decision process and analyses the impact of frugality on decision-making 
(Gigerenzer & Brighton, 2009)
. In this study, we consider cognitive frugality in the decision process in terms of thinking time.
Since humans have limited computational resources available 
(Griffiths et al., 2015;
Lieder et al., 2018;
Lieder & Griffiths, 2020)
, the more time humans spend thinking, the greater the cognitive burden they would have. In contrast, spending less time on thinking may lead to reductions in cognitive load. There is a limit to the reduction in cognitive load, and the degree to which the cognitive load is reduced due to shorter thinking times becomes progressively smaller as the thinking time becomes shorter. We call this reduction in cognitive load cognitive frugality. Cognitive frugality, ( ), is represented as follows with parameters (controlling curvature, > 0)
and (controlling location, > 0):
( ) = 1 1 + exp8 ( − )9 (Eq. 2)
where is the invariant point and represents the decision time when cognitive frugality equals a moderate degree.
represents the individual difference in decision time when the rate of correct judgments reaches almost the minimum value.


Task performance of respondents
According to the two components above, ( ) and ( ), we can represent the quality of job performance in the form of expected job performance, ( ), with the following equation:
( ) = ( ) × ( ) (Eq. 3).
In this framework, the optimal decision time to achieve the best job quality (i.e. the decision time that results in the highest expected job performance) is determined by the trade-off between cognitive utility and frugality. ( ) peaks when the respondent uses the appropriate length of time for decision making, and ( ) peaks out and then declines when the respondent uses too long a time for decision making. This result indicates that taking too long to decide is not optimal for achieving the highest job quality when considering both cognitive utility and frugality. Thus, an optimal decision time (i.e. one that is neither too short nor too long) exists for maximizing ( ).


Optimal time allocation in the trade-off between cognitive utility and frugality
Next, we provide a more thorough analysis of the effects of and on ( ). We controlled the values of and more extensively. In particular, the values of and were constrained within 0.5-3.5, and we examined how ( ) changes as a function of decision time. 
Figure 1(C)
 shows the results of this analysis, where ( ) is standardised such that ( ) equals to 1.0 for the maximum value at each combination of parameter values (α or β). The bold black line indicates the decision time in which ( ) equals the maximum value for each parameterin other words, the optimal decision time for maximising ( ). According to Figure 1(C), when decision time is too long, expected job performance is reduced to less than half of its maximum. However, as shown in 
Figure   1
(C) that the black bold line where ( ) is maximised does not appear when decision time is too short (e.g., 0.5), expected job performance is not maximised when decision time is too short. In other words, by spending a certain length of time, the expected job performance calculated from cognitive utility and frugality is maximised. These results were the same over broad parameter ( or ) settings.


Discussion
Our theoretical analyses based on two components pertaining to task performance-cognitive utility and frugality-showed that there is an optimal length of decision time that maximises work performance. Taking a certain length of decision time is rational because of the ability to balance the trade-off between an increase in judgment accuracy and an increase in people's cognitive load. Our theoretical analyses suggested that the reason task performance is not always maximised when the time used for decision making is too long or too short is that resources are not allocated optimally, as assumed by the resource rationality framework.


Overview of behavioural experiments
We conducted behavioural experiments to verify that an appropriate forced decision time (i.e. a rational period of time to keep respondents waiting and prompt respondents to think before answering a question) improves job performance. This study used a task in which participants judged whether a medical image of the inside of a bladder was normal (no disease included in the image) or abnormal (cancer included in the image) called the medical image experiment.


General procedure: Medical image experiment
This section describes the general procedure of the medical image experiment. Each participant responded to each of the 100 binary judgment tasks after three tutorials; the 100 binary judgment tasks consisted of 50 normal and 50 abnormal images, which were presented randomly. The experiments were conducted entirely online, and the experimental web screen is shown in 
Figure 2
(A). For each task, a single image was presented on the experimental web screen, and the participant clicked on the answer button for either 'normal' or 'abnormal' on the same screen. Then, a fixation point (a cross) was automatically displayed for 1.0 s, and the screen automatically moved to the next task. The participants responded in a self-paced manner throughout the experiment.
Participants could opt to drop out of the experiment at any time before completing the 100 tasks. If participants closed the experimental web screen, they dropped out. Participants who had once dropped out could not restart the experiments. The following information was not explained to the participants: the ratio of normal to abnormal images for the 100 tasks and the purpose of the analysis of the results of the experiments.
In this research, two experimental settings were used.
(1) The medical image experiment without forced decision time: To check the basic characteristics of the judgments, we conducted an experiment without a forced decision time. In other words, the participant could click the answer button soon after the task image was displayed ( 
Figure 2
(B) (1)). In summary, this experimental setting consists of a general binary classification task.
(2) The medical image experiment with forced decision time: We examined the effect of forced decision time on job performance. In other words, for a certain period of time after the task image was displayed, the answer button was greyed out and inoperative, so participants could not click on the answer button. After a certain period of time, the answer button turns from greyed out to blue, and participants could click on the answer button. By forcing participants to wait until being able to answer the task, they were encouraged to think about the task for a certain period of time known as the 'forced decision time' (Figure 2(B) (2)).
Here, we detail the relevance of our theoretical study, which were shown in Section 3, to our behavioural experiments. In our behavioural experiments, we considered that an increased correct answer rate reflected the cognitive utility in the theoretical study. We also considered that a decreased rate of dropout from the experiment reflected the cognitive frugality in the theoretical study, given the characteristics of the crowdsourcing tasks, which are often designed to allow employees who feel overwhelmed to drop out of the experiment before their completion 
(Benbunan-Fich, 2023;
Crump et al., 2013;
Zhou & Fishbach, 2016)
.


Participants, analysis, and ethics
In this paper, two different sets of participants were recruited: nurses (Empirical Study 1) and medical doctors (Empirical Study 2). This was done to assess differences in the effects of forced decision time by participants with different background knowledge about medical images of the inside of a bladder: nurses are novices, while medical doctors are semi-professionals 2 . For each study, two types of experimental settings (the medical image experiment without a forced decision time and the medical image experiment with a forced decision time) were used. Details of experimental settings are shown in Sections 5 and 6. All participants were recruited through Rakuten Insight, Inc, which is the Japan's only survey platform with large scale registration of nurses and medical doctors; we considered that this ensured diversity among nurses and physicians participating in this experiment as much as possible. Empirical Studies 1 and 2 were conducted in 2021. The participants in the experiment were native speakers of Japanese, and all the experiments were conducted in Japanese. In this paper, the experimental screens are shown in English for readability. More detailed description of the participants was shown in the footnote 3 3 .
The sample size for each experiment was determined as follows: For the medical image experiment without a forced decision time (Experiments 1-1 and 2-1), since the purpose of this experiment is to examine exploratively the characteristics of the without forced decision time condition and there is no comparison between groups, the number of participants recruited was determined based on a policy of recruiting as many participants as possible,
given research resources. For the medical image experiment with a forced decision time (Experiments 1-2 and 2-2), assuming a comparison of participants' correct answer rate between groups based on differences in forced decision time, even accounting for the possibility that some participants might drop out, participants were recruited so that there would be at least 96 participants per group who would complete the 100 tasks. The sample size was 96 when the effect size was medium ( = 0.5), level was 0.01, and the power was 0.8. The significance level was set at 0.1%. The details of the actual sample sizes are shown in Sections 5 and 6.
We confirm that the experiments adhere to ethical guidelines specified in the APA Code of Conduct and that it was approved by the Medical Ethics Committee of the University of Tsukuba (Approval No. 1616). All the participants provided informed consent on web screen before starting the experiment. Participants who completed the 100 tasks received redeemable coupons that can be used in the online shopping service in Japan as rewards.
No reward was given to participants who dropped out; this was fully informed to participants prior to their participation in the experiments 4 .


Transparency and Openness
All measures, manipulations, exclusions, as well as the method of determining the final sample size in the experiments are disclosed in the respective method sections. Data and analytic code for all the experiments reported in this paper are available at https://osf.io/9qu3w/?view_only=f59ab322714a4b8c8b77d663dbc31e6d.
We used medical images registered as intellectual property by the University of Tsukuba . For subsequent data analyses, Python 3.9.5, pandas 1.2.4, NumPy 1.20.3, SciPy 1.6.3, R 4.1.0, and RStan 2.21.2 were used. Our empirical studies' design and their analyses were not pre-registered.


Empirical study 1: Nurses as participants


Experiment 1-1: Without a forced decision time
First, to check the relationship between total decision time and correct answer rate without forcing decision time for nurses, we conducted an experiment without a forced decision time.


Methods
Participants: A total of 217 nurses (171 women and 46 men; Mage = 37.1 years and SDage = 9.43 years)
completed the experiment (i.e. the 100 binary judgment tasks) 5 .
Experimental procedure: The experimental setting used in Experiment 1-1 was the medical image experiment without a forced decision time (shown in Section 4.1).


Operational definition of the total decision time:
Since the recorded response time reflects factors irrelevant to actual thinking about the judgments (i.e. the time needed for peripheral processes, such as stimulus encoding and response execution 
(Dutilh & Rieskamp, 2016)
), the total decision time (i.e. the time a participant spent thinking about a judgment) was calculated as the recorded response minus the time that was irrelevant to thinking about the judgments, which was called non-decision time. The method of calculating non-decision time and other details of calculating total decision time are given in Supplement 4(1).
(total decision time) = (recorded response time) -(non-decision time). (Eq. 4)
Statistical analysis: First, the mean correct answer rate of all judgments was defined as . We estimated using a model assuming the participants' individual difference level (details are shown in Supplement 5).
Second, this experiment analysed the relationship between total decision time and correct answer rate using a state-space model 
(Durbin & Koopman, 2012)
, assuming the participants' individual difference levels. For the judgment !,# made by -th participant at -th total decision time window, !,# = 1 meant the judgment was correct and !,# = 0 meant the judgment was incorrect. The time window was 0.40 s (1st time window means 0.0-0.40 s, 2nd time window means 0.40-0.80 s, …) 6 . # means correct answer rate at -th total decision time window, !,# means correct answer rate of -th participant at -th total decision time window, σ means the standard deviation of correct answer rate. The model parameters were fitted with three Markov chain Monte Carlo (MCMC) chains with 140,000 iterations and 130,000 burn-in samples, and a thinning parameter of one. The R hats for all parameters were under 1.05.
# ∼ 8 #$% , & ) !,# ∼ 8 # , & ) (Eq. 5) !,# ∼ S 8 !,# 9V


Results
Section 5.1.2 mentions the results of Experiment 1-1 with respect to confirming the relationship between total decision time and the correct answer rate of nurses, which is the objective of Experiment 1-1. The nondecision time is shown in Supplement 6.
To exclude from the analysis any judgments that infer that there may have been a problem with data transmission or that the participant may have interrupted their thinking during the decision-making process, judgments with a total decision time of longer than 1,000 s were excluded from the analysis because it is difficult to assume that the person was actively thinking about the task for more than 1,000 s. Then, 21,695 of 21,700 judgments were used for the analysis. The estimated mean correct answer rate of all judgments and 95% credible intervals was 0.580 [0.565, 0.594].
Figure 3 (top) shows the relationship between total decision time and the correct answer rate. The correct answer rate of the judgments was significantly lower than the estimated mean correct answer rate (0.580) when the total decision time was 0.0-0.8 s, while the correct answer rate was significantly higher than 0.580 when the total decision time was 1.2-9.6 s. The total decision time at which the correct answer rate peaked was 4.8-5.2 s, and it was confirmed that the correct answer rate did not continue to increase as the total decision time increased The median length of total decision time was 1.28 s.


Discussion
The experiment showed that the correct answer rate with a short total decision time was significantly lower than the mean correct answer rate. The experiment also showed that there is a peak in the correct answer rate at a total decision time; once the correct answer rate reached a certain value, the correct answer rate did not necessarily increase even if the total decision time increased. These findings were consistent with the results of our theoretical analysis (Section 3). The results experimentally suggested that there is an optimal decision time for nurses.


Experiment 1-2: The effect of forced decision time on performance
The next experiment was conducted to examine the effect of forced decision time on the job performance of nurses.


Methods
Participants: A total of 434 nurses (360 women and 74 men; Mage = 36.6 years and SDage = 9.78 years)
completed the experiment 7 .
Experimental procedure: The experimental setting used in Experiment 1-2 was the medical image experiment with a forced decision time (shown in Section 4.1).
The nurses who participated in Experiment 1-2 were randomly assigned to one of five experimental groups based on the forced decision time (seconds): a group without the setting of a forced decision time (i.e., = 0.0) and four groups with a forced decision time (i.e., = 1.0, . . . , 4.0). The experimental group without the setting of a forced decision time was called the 'without-intervention group'. The intervention of forced decision time was not explained to the participants; in other words, the participants were not notified that they could not make a judgment for a certain length of time (forced decision time) after being presented with each task.
Operational definition of total decision time: Eq. 4 was used to calculate the total decision time: this is the same as in Experiment 1-1. The difference from Experiment 1-1 was that the non-decision time was calculated for each of the five experimental groups ( = 0.0, ..., 4.0). Additional details are given in Supplement 4(2).
7 As described in the following experimental procedure, we designed five types of forced decision time ( = 0.0, 1.0, 2.0, 3.0, and 4.0). Medical doctors were recruited so that the number of participants in each of the five groups who completed 100 tasks exceeded 96 each. As described in Section 4.1, this experiment allowed participants to drop out of the experiment before completing 100 tasks. Within 10 days of the start of participant recruitment, 423 nurses completed 100 tasks. Since the number of participants who completed 100 tasks increased by only 11 over the next three days, the recruitment of medical doctors was terminated at this point in consideration of the study's feasibility. The nurses' specialty and there are of residence were shown in Supplement 3.


Evaluation of job performance:
In Experiment 1-2, to discuss the existence of an optimal decision time that improves peoples' performance by balancing the trade-off between cognitive utility and cognitive frugality, which is shown in our theoretical study (Section 3), job performance should be evaluated using the metrics that assess the balance between the trade-off between cognitive utility and cognitive frugality.
Therefore, this study defines job performance as the expected value of the correct judgments at the forced decision time . This is because we considered the people's correct answer rate as cognitive utility and the small number of participants who dropped out of the tasks (i.e., the large number of participants who completed the 100 binary judgment tasks) as a reflection of the cognitive frugality. We call this metric of job performance ( ) 8 , which is defined as follows:
( ) = ( ) × ( ). (Eq. 6)
The number of participants who completed the 100 binary judgment tasks as ( ). This study defined the correct answer rate under the forced decision time as ( ) . To estimate ( ) assuming participants'
individual differences, we used the same model as Experiment 1-1, except that there are four groups with the forced decision time (details of the model and parameter fittings are shown in Supplement 5).
Assuming the participants' individual difference levels for each of five experimental groups, we used the state-space model to analyse the relationship between total decision time and correct answer rate (Eq. 5). The settings for parameter fitting were the same as in Experiment 1-1 and were used for each group.


Results
Section 5.2.2. describes the results with respect to confirming the effect of forced decision time on the performance of nurses, which was the objective of Experiment 1-2. The non-decision times for each of the five experimental groups are shown in Supplement 6.
Similar to Experiment 1-1, to exclude from the analysis judgments that infer that there may have been a problem with data transmission or that the participant may have interrupted their thinking during the decisionmaking process, the judgments with a total decision time of longer than 1,000 s were excluded from the analysis because it is difficult to assume that the person was actively thinking about the task for more than 1,000 s. Then, 43,382 of the 43,400 judgments were used for the analysis.
( ), ( ), ( ) ( = 0.0, ..., 4.0) are shown in 
Table 1
. For ( ), the estimation value and 95% credible interval of ( ) showed that ( ) of the forced decision time groups of = 1.0 2.0 significantly increased compared to the without-intervention group ( = 0.0), and ( ) peaked for the forced decision time groups of = 1.0. The ( ) of the forced decision time groups of = 3.0 4.0 were smaller than the ( ) of the forced decision time group of = 2.0. ( ) of the forced decision time groups of = 3.0 4.0 was smaller than that of the without-intervention group. ( ) of the forced decision time group of = 4.0 was significantly larger than that of the without-intervention group, and ( ) of the forced decision time groups of = 1.0, 2.0, 3.0 tended to be larger than that of the withoutintervention group.
As to the change in ( ) due to the implementation of a forced decision times, we conducted a more detailed analysis of the reason for the ( ) of each of the four forced decision time groups' tendency to be larger than that of the without-intervention group. Then, we analysed the relationship between total decision time and the correct answer rate for each group, as shown in 
Figure 4
. For the without-intervention group 
(Figure 4, top)
, the correct answer rate of the judgments with the total decision times ranging from 0.0-0.8 s was significantly lower than the estimated mean correct answer rate (0) (0.592 as shown in 
Table 1
), and the median length of total decision time was 1.39 s. The result that the correct answer rate for decisions with too short total decision times (e.g., 0.0-0.8 s) was significantly lower than the estimated mean correct answer rate and the median value of decision time was the same trend as in Experiment 1-1; in other words, the results of Experiment 1-2 showed that the same trends as in Experiment 1-1 were reproduced. However, for the four forced decision time groups 
(Figure 4
, middle and bottom), there was no total decision time in which the correct answer rate was significantly lower than the mean (0) (0.592).


Discussion
When the nurses worked on the task of labelling medical images with binary labels, their job performance ( ) peaked with a forced decision time of 1.0 s. The trend observed in Experiments 1-2, where forced decision times that were neither too long nor too short resulted in significantly higher job performance, was consistent with that observed in our theoretical analysis (Section 3). Experiment 1-2 showed a trend towards higher ( ) and smaller ( ) when the forced decision time was 3.0 s or 4.0 s, which also supports the assumption of our theoretical study that cognitive utility increases and cognitive frugality decreases when the time spent on a task increases.
There are two possible reasons why the nurses' job performance peaked when the forced decision time was 1.0 s. First, as suggested by 
Figure 4
, forced decision time was speculated to prevent nurses from making impulsive judgments (e.g. judgments with a total decision time of 0.0-0.8 s), which tend to lead to inaccurate judgments. Second, 1.0 s was shorter than the median length of total decision time (1.39 s); thus, it was assumed that the nurses did not feel greatly burdened by being forced to wait for 1.0 s and that not so many nurses dropped out. The result showing that ( ) of the forced decision time groups of = 3.0 and = 4.0 was smaller than that of without-intervention group was suggested to reflect the fact that the longer the forced decision time, the more nurses felt burden too much and dropped out from the experiment. In summary, the results of Experiment 1-2 supported the hypothesis that there is a rational total decision time that appropriately balances the trade-off
between the correct answer rate and the cognitive load reflected in the number of dropouts.


Empirical study 2: Medical doctors as participants
To confirm whether the effects of a forced decision time on the job performance, which were showed with nurses in Empirical Study 1, were also valid in participants with different background knowledge, the similar experiments were conducted with medical doctors.


Experiment 2-1: Without a forced decision time
First, to check the relationship between total decision time and correct answer rate without forcing decision time for medical doctors, we conducted an experiment without a forced decision time similar to Experiment 1-1. 


Methods


Participants


Results
This section describes the results of Experiment 2-1 with respect to confirming the relationship between total decision time and the correct answer rate of medical doctors, which was the objective of Experiment 2-1. The non-decision time is shown in Supplement 6.
Similar to Experiment 1-1, judgments with a total decision time of longer than 1,000 s were excluded from the analysis. Thus, 22,394 of the recorded 22,400 judgments were analysed. The estimated mean correct answer rate of all judgments and 95% credible intervals was 0.685 [0.667, 0.705].
Figure 5 (top) shows the relationship between total decision time and the correct answer rate. The correct answer rate was significantly lower than the estimated mean correct answer rate (0.685) when the total decision time was 0.0-0.8 s. On the other hand, the correct answer rate was significantly higher than 0.685 when the total decision time was 1.2-2.0 s and 10.0-10.4 s. The total decision time at which the lower bound value of the 95% credible interval of correct answer rate peaked was 1.6-2.0 s, and it was also confirmed that the lower bound value of the 95% credible interval of correct answer rate did not continue to increase as the total decision time got longer than 1.6-2.0 s. 
Figure 5 (bottom)
 shows the observed number of judgments according to total decision time. The median length of total decision time was 1.80 s.


Discussion
In the experiment conducted with medical doctors, the correct answer rate with the short total decision time was significantly lower than the mean correct answer rate. Moreover, there is a peak in the correct answer rate at a total decision time.
Once the correct answer rate reached a certain value, the correct answer rate did not necessarily increase even if the total decision time increased. These results were consistent with the results of the theoretical analysis (Section 3) and empirical studies conducted with nurses (Experiment 1-1).


Experiment 2-2: Effect of forced decision time on job performance
The experiment was conducted to examine the effect of forced decision time on the job performance of medical doctors, similar to Experiment 1-2. Moreover, in Experiment 2-2, considering the actual social implementation of our proposed boost, which encourages thinking by forcing decision-makers to wait a set amount of time before proceeding with the task at hand, we checked whether the cognitive load of medical doctors who completed 100 tasks increased due to our proposed boost. This is because our proposed boost would not be appropriate if the cognitive load on participants who completed the tasks is too great when a forced decision time is set.


Methods
Participants: 404 medical doctors (50 women and 354 men; Mage = 50.3 years and SDage = 12.0 years) completed the experiment 10 .
Experimental procedure: The experimental setting used in Experiment 2-2 was the medical image experiment with a forced decision time. The results of Experiments 1-1 and 2-1 showed that the median length of total decision time was longer for medical doctors (1.80 s) than for nurses (1.28 s) when there was no forced decision time. Therefore, in Experiment 2-2, the forced decision time was set up to 6.0 s to examine the effect of a longer forced decision time than in Experiment 1-2. The medical doctors who participated in this experiment
were randomly assigned to one of seven experimental groups: a without-intervention group ( = 0.0) or groups with forced decision times of 1.0, 2.0, 3.0, 4.0, 5.0, or 6.0 s.
In Experiment 2-2, a questionnaire was distributed to participants who completed 100 tasks to measure their cognitive load during the tasks. The methods and results related to cognitive load are shown in Section 6.2.3.
Operational definition of the total decision time and evaluation of job performance: The method to calculate the total decision time for each judgment (please refer to Supplement 4(2)), the evaluation of job performance using ( ) and estimation of ( ) (please refer to Supplement 5) was the same as in Experiment 1-2.
To analyse the relationship between total decision time and correct answer rate, the state-space model assuming participants' individual difference used for each of seven experimental groups was the same as Experiment 1-2 except for the following two points. The time window was set to 0.25 s (i.e. the first time window was 0.0-0.25 s, the second time window was 0.25-0.50 s, …) considering the convergence of parameter fitting, and times of iteration and burn-in (please refer to Supplement 8).


Results
Section 6.2.2 describes the results of Experiment 2-2 with respect to confirming the effect of forced decision time on the performance of medical doctors, which is the objective of Experiment 2-2. The non-decision times are shown in Supplement 6. Similar to Experiment 1-2, judgments with a total decision time of longer than 1,000 s were excluded from the analysis. Thus, 40,362 of 40,400 judgments were used for the analysis.
( ), ( ), ( ) ( = 0.0, ..., 6.0) are shown in 
Table 1
. For ( ), the estimation value and 95% credible interval of ( ) showed that ( ) of the forced decision time group of = 1.0 tended to increase compared to the without-intervention group ( = 0.0), while the groups with longer forced decision time tended to show smaller ( ) when forced decision time was longer than 2.0 s. The ( ) of the six forced decision time groups ( > 1.0) was smaller than that of without-intervention group, and the groups with longer forced decision times tended to have smaller ( ). The ( ) of each of the six forced decision time groups tended to be larger than that of the without-intervention group. These results supported our theoretical study and were consistent with those of Experiment 1-2. The finding that groups with longer forced decision times tended to have smaller ( ) suggests that the longer the forced decision time, the more medical doctors felt overburdened and thus dropped out from the experiment.
As to the change in ( ) due to the implementation of a forced decision times, we conducted more detailed analysis of the reason of that ( ) of each of the six forced decision time groups tended to be larger than that of the without-intervention group. Then, we analysed the relationship between total decision time and the correct answer rate for each group, as shown in 
Figure 6
. For the without-intervention group ( = 0.0) ( 
Figure   6, top)
, the correct answer rate of the judgments with total decision times ranging from 0.0-0.75 s was significantly lower than the estimated mean correct answer rate (0) (0.696 as shown in 
Table 1
), and the median length of total decision time was 1.51 s. The result that the correct answer rate for decisions with too short total decision times (e.g., 0.0-0.75 s) was significantly lower than the estimated mean correct answer rate and the median value of decision time was the same trend as in Experiment 2-1; in other words, the results of Experiment 2-2 showed that the same trends as in Experiment 2-1 were reproduced.. However, for the six forced decision time groups ( 
Figure 6
, middle and bottom), there were no total decision times in which the correct answer rate was significantly lower than the mean (0) (0.696); this effect of the forced decision time also reproduced the same tendency found in Experiment 2-1.


The cognitive load of the participants who completed the tasks
In this section, we analyse the extent to which participants who completed our tasks felt a greater cognitive load due to the implementation of a forced decision time.
Methods: After completing 100 binary judgment tasks, the participants were asked to rate the NASA Task Load Index (NASA-TLX) [raw-TLX (0: the smallest mental workload; 600: the maximum mental workload) 
(Hart, 2006)
 in Japanese 
(Miyake & Kumashiro, 1993)
] and their subjective stress in their judgments (0: nonstressful -100: very stressful) using a 101-point Likert scale. Participants were asked about subjective stress in addition to the NASA-TLX because the cognitive load perceived by participants due to the implementation of a forced decision time may not correspond to the mental workload that can be measured by the NASA-TLX.
Results: First, significance tests were conducted on the null hypothesis that the mean score of the NASA-TLX of each group with a forced decision time ( = 1.0, … , 6.0) would be equal to that of the without-intervention group ( = 0.0). The mean and standard deviation of NASA-TLX scores were 330.7 (± 97.5), 328.6 (± 90.9), 336.5 (± 90.5), 337.1 (± 109.0), 331.3 (± 87.0), 317.1 (± 76.7), and 319.9 (± 97.2) for each group with a forced decision time of 0.0-6.0 s, respectively 
(Figure 7 (top)
). In the Shapiro-Wilk test, the null hypothesis that the distribution of NASA-TLX values is normally distributed was not rejected for all groups. In Bartlett's test, the null hypothesis that the population variance of the NASA-TLX values of each forced decision time group would be equal to the population variance of the NASA-TLX values of the without-intervention group was not rejected among all groups. Therefore, Dunnett's test was used. The p-values were 1.00, 0.999, 0.998, 1.00, 0.940, and 0.980 for each group with a forced decision time of 1.0-6.0 s, respectively. The effect sizes were 0.022, -0.061, -0.062, -0.007, 0.152, and 0.111 for each group with a forced decision time of 1.0-6.0 s, respectively. The results showed that there was no significant difference in the mean value of NASA-TLX for each group with a forced decision time compared to that of the without-intervention group. We must note that the NASA-TLX values were obtained only for those who completed the 100 tasks.
For subjective stress, significance tests were conducted on the null hypothesis that the mean score of the subjective stress of each group with a forced decision time ( = 1.0, … , 6.0) would be equal to that of the withoutintervention group ( = 0.0). The mean and standard deviation was 62.41 
(
 .070, and -0.106 for each group with a forced decision time of 1.0-6.0 s, respectively. The results showed that there was no significant difference in the mean value of subjective stress of each group with a forced decision time compared to that of the without-intervention group.
Discussion: These results showed that our proposed boost of setting a forced decision time increased job performance without increasing mental workload or subjective workload compared to the without-intervention group, at least for medical doctors who completed our tasks. Longer forced decision times were considered to increase the cognitive load on participants; however, the results suggest that the increase in cognitive load was actually reflected in the increasing number of dropouts (i.e. the decreasing number of participants who completed the 100 binary judgment tasks). In summary, our proposed boost of forcing a certain short minimum decision time is suggested to increase the mental workload or subjective stress of the people who completed the tasks.


Discussion
Factoring in the results of Empirical Studies 1 and 2 for in terms of the binary labelling for medical images, job performance peaked when people are forced to wait for 1.0 s, which was shorter than the median length of total decision time, before allowing them to answer a question. This result was observed whether the participants were nurses and medical doctors, i.e., independent of the background knowledge of participants. In an environment that allowed participants who felt too high a cognitive load to drop out, our proposed boost forcing a certain short additional decision time (e.g., 1.0 s) was a cost-effective intervention in that it improved job performance without increasing the mental workload or subjective stress of participants who completed the tasks.
Based on the results of analysis of the relationship between total decision time and correct answer rate, it was inferred that forcing participants to wait and prompting participants to think for a short and rational period of time before answering a question would prevent impulsive decisions, which tend to lead to inaccurate judgments, thus improving job performance. These results support the hypothesis that there is a rational total decision time that appropriately balances the trade-off between correct answer rate and the cognitive load reflected in the number of dropouts.


General Discussion
In this study, referring to the resource-rationality perspective, we formulated a trade-off between cognitive utility and cognitive frugality according to the changes in decision time, and conducted large-scale behavioural experiments to verify that there exists an optimal decision time to improve people's job performance. Moreover, the behavioural experiments with a binary judgment task of medical images confirmed that job performance improved without increasing the mental workload and subjective stress of people who completed the tasks when people were forced to wait for only 1.0 s before they were allowed to make a judgment for a task. Job performance decreased when people were disallowed to make a decision for more than 2.0 s after being presented with a task.
Note that an additional 1.0 s, which was shorter than the median length of decision time, was assumed to promote the rational allocation of cognitive resources and prevent impulsive judgments, independent of the participants' background knowledge (nurses and medical doctors). Based on the results, we propose a boost in which people are forced to wait to make decisions for a short time, such as only one second.


Constraints on generality
By using the binary labelling of medical images, this study showed that providing an appropriate forced decision time improved job performance without increasing the burden of people who completed the tasks. Our proposed boost, in which the people are forced to wait to make decisions for a short time, is quite simple and non-domain-specific-it can improve cognitive ability via web screen design without explicit instructions; therefore, our proposed boost was assumed to be applicable in any domain, not just for the binary labelling of medical images. In the future, it is necessary to confirm the effectiveness of our proposed boost in a wide variety of decision-making scenarios other than the binary labelling of medical images.
The setting of an appropriate forced decision time could vary not only depending on the task or background knowledge of the participants but also on cognitive style preference 
(Cools & Van den Broeck, 2007)
 and
individual differences in the reasoning strategy 
(Thompson, & Markovits, 2021)
. In the future, we will attempt to clarify differences according to the characteristics of the participants and develop a method for predicting the appropriate decision times for each participant.
The study targeted the specialized task of binary judgment of medical images of the inside of a bladder, and participants were recruited exclusively from nurses or physicians. For recruiting the participants, we used Rakuten Insight, Inc, which is the Japan's only survey platform with large scale registration of nurses and medical doctors;
therefore, we believe that the greatest possible attention was given to ensuring the diversity of sample demographics, given that the nurses and physicians who participated in the experiment were diverse in their specialties and area of residence (Supplement 3). However, we cannot deny the possibility that there may be some unknown issues due to the fact that the experiments were conducted in Japanese and the participants in the experiment were native speakers of Japanese.


Applicability, limitations, and future work
A well-known intervention for improving judgment similar to the boost framework is the nudge framework, which induces humans to make better decisions by changing their choice architecture 
(Johnson & Goldstein, 2003;
Thaler & Sunstein, 2008;
de Ridder et al., 2022)
. Compared to nudges, boosts are considered to have fewer concerns 
(Fieseler et al., 2019;
Zhou & Fishbach, 2016)
 in terms of undermining autonomy and transparency because it is considered that the boost method does not steer people towards particular choices and preserves decision-making autonomy. A previous study 
(van Gestel et al., 2020)
 improved humans' cognitive abilities by instructing them to think thoroughly about their decisions. However, to the best of our knowledge, few studies have examined whether cognitive ability can be improved by the design of a web screen without issuing explicit instructions to think longer, as our proposed boost. We would like to investigate whether our proposed boost can be applied to a greater variety of scenarios, such as data collection in citizen science or to determine whether people refrain from being deceived by fake information.
In this study, it was not possible to collect data on the cognitive load of participants who dropped out of the experiment (our experiments were set up in such a way that participants could drop out of the experiment by closing the screen). Future work should examine the relationship between decision time, the cognitive load of participants who dropped out, and the number of dropouts. Technology to infer the thought processes of participants from data, such as mouse trajectories, which can be easily measured in an online environment, should be developed.


Conclusion
We proposed an intervention for forcing minimum decision time as a simple and cost-effective boost to improve one's ability to make good judgments. Through theoretical analyses and large-scale behavioural experiments conducted with nurses and medical doctors, we showed that preventing people from making judgements within a certain short time (i.e. only 1.0 s) after being presented with a task improved job performance in binary judgment tasks without increasing the cognitive load of respondents who completed the tasks. By analysing the relationship between total decision time and the correct answer rate, it was inferred that prompting people to think by forcing people to wait for a certain short time improved job performance by preventing impulsive decisions. We believe that focusing on the framework of resource rationality could lead to simple and cost-effective solutions to real-world problems by boosting people's cognitive competence. Note: (A) or equals 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, or 3.5. We set and h to 3.0 and 1.0, respectively. (B) We set , , , and h to 1.0, 2.0, 3.0, and 1.0, respectively. (C) ( ) is standardized to be at maximum 1.0 for each combination of parameter values ( or ). The bold black line indicates the decision time t wherein ( ) equals to the maximum value at the parameter ( or ) value. Note: Participants answered each task by clicking either the normal or abnormal button. All experiments were conducted in Japanese, and the experimental computer screen was written in Japanese. In this paper, the experimental screen is presented in English for readability. For both experimental settings, when either the normal or abnormal button was clicked, a fixation point was automatically displayed for 1.0 s; then, the screen automatically moved to the next task. Note: (Top) The black bold line represents the correct answer rates, while the shaded area represents the 95% credible interval, which were estimated using a state-space model. The dashed line indicates the estimated average correct answer rate of all judgments Acc (0.580). The dots mean the observed correct answer rates for every 0.5 s of total decision time.


Figure 4
For each of five experimental groups of Experiment 1-2, (upper figure) the relationship of total decision time and correct answer rate, and (lower figure) the observed number of judgments according to total decision time.
Note: For the upper figure for each group, the black bold line represents the correct answer rates, while the shaded area represents the 95% credible interval, which were estimated using a state-space model. The dashed line indicates the correct answer rate estimated using a model over all judgments of the without-intervention group Acc(0) (0.592). The dots mean the observed correct answer rates for every 0.5 s of total decision time. (Bottom) The observed number of judgments according to total decision time (Experiment 2-1).


Note:
The black bold line refers to the correct answer rates and the shaded area is the 95% credible interval.
Both were estimated using a state-space model. The dashed line indicates the estimated average correct answer rate of all judgments Acc (0.685). The dots refer to the observed correct answer rates for every 0.5 s of total decision time.


Figure 6
For each of seven experimental groups of Experiment 2-2, (upper figure) the relationship of total decision time and correct answer rate, and (lower figure) the observed number of judgments according to total decision time.
Note: For the upper figure for each group, the black bold line represents the correct answer rates, while the shaded area is the 95% credible interval. These were estimated using a state-space model. The dashed line
indicates the correct answer rate estimated using a model over all judgments of the without-intervention group (0) (0.696). The dots represent the observed correct answer rates for every 0.5 s of total decision time.
Figure 1 (
1
A) confirms that cognitive utility ( ) and frugality ( ) show different trends depending on the length of decision time when we set and ℎ to 3.0 and 1.0, respectively. As people take more time to make a decision, cognitive utility increases and cognitive frugality decreases. Hereafter, we report how expected job performance ( ) is affected by the features of cognitive utility ( ) and frugality ( ). First, we present a typical result.Figure 1(B)shows ( ), ( ), and ( ) as functions of decision time when we set , , , and ℎ to 1.0, 2.0, 3.0, and 1.0, respectively. According toFigure 1(B), ( ) does not take the form of a monotonic function.


beyond 4.8-5.2 s.Figure 3 (bottom) shows the observed number of judgments according to total decision time.


. 224 medical doctors (32 women and 192 men; Mage = 46.6 years and SDage = 8.04 years) completed the experiment 9 .Experimental Procedure and Operational Definition of the Total Decision Time. The experimental setting used in Experiment 2-1 was the medical image experiment without a forced decision time. The total decision time was calculated as the recorded response minus the time that was irrelevant to thinking about the judgments, similar to Experiment 1-1 (please refer to Supplement 4(1)).Statistical Analysis. For estimation of the correct answer rate of all judgments, we used the same model as Experiment 1-1 (Supplement 5). For analysis of the relationship between total decision time and correct answer rate, we used the same state-space model assuming participants' individual difference as Experiment 1-1 (shown in Section 5.1.1). The settings for parameter fitting were the same as Experiment 1-1 except for times of iteration and burn-in (Please refer to Supplement 8).


± 23.62), 57.80 (± 25.08), 59.33 (± 25.61), 61.59 (± 26.45), 60.91 (± 23.50), 64.34 (± 23.03), and 57.35 (± 27.09) for each group with a forced decision time of 0.0-6.0 s, respectively (Figure 7 (bottom)). In the Shapiro-Wilk test, the null hypothesis that the distribution of NASA-TLX values is normally distributed was rejected for the 1.0 s, 3.0 s, and 5.0 s intervention groups. Then, the Steels test was used. The p-values were 0.699, 0.885, 0.999, 0.980, 0.980, and 0.882 for each group with a forced decision time of 1.0-6.0 s, respectively. The effect sizes were -0.126, -0.103, -0.022, -0.067, 0


Figure 1 (
1
A) Cognitive utility ( ) and frugality ( ) as a function of decision time. (B) Cognitive utility ( ), frugality ( ), and expected job performance ( ) as a function of decision time . (C) Heatmap of ( ). (C-1) Heatmap of ( ) as a function of parameter value . (C-2) Heatmap of ( ) as a function of parameter value .


Figure 2 (
2
A) An example of the experimental computer screen. (B) Overview of the medical image experiment.


Figure 3 (
3
Top) The relationship of nurses' total decision time and correct answer rate (Experiment 1-1). (Bottom) The observed number of judgments according to total decision time (Experiment 1-1).


Figure 5 (
5
Top) The relationship of medical doctors' total decision time and correct answer rate (Experiment 2-1).


In our behavioural experiments, the term 'medical doctors' refers to non-urologists. Urologists are experts of the bladder and judge whether a medical image of the inside of a bladder was normal or abnormal in daily clinical practice. A more detailed explanation of why nurses and medical doctors (non-urologists) were targeted in this study is provided in Supplement 2.3  The purpose of recruiting participants for the empirical studies in this study was to collect two types of experimental participants, nurses and medical doctors, who had different background knowledge about the binary judgment of medical images of the inside of a bladder. The study targeted the specialized task of binary judgment of medical images of the inside of a bladder, and participants were recruited exclusively from nurses or physicians. Given the purpose of this study, we obtained approval to collect information on age, sex, specialty as nurses or medical doctors, and region of their residence (prefecture) in the ethical review (please refer to Section 4.3. Transparency and Openness). Details of these data are disclosed in the respective method sections (Sections 5 and 6).We did not collect information on gender, racial identity, ethnicity, nativity or immigration history, or socioeconomic status because we considered these data to be too sensitive for the purpose of this study. However, since all the participants were native Japanese speakers, it is presumed that almost all of them were Asian in racial identity and Japanese in ethnicity. Since the participants are nurses or medical doctors, we assume that almost all of them have a high socioeconomic status.


It has been found that not rewarding crowdsourced research participants who quit before completing an experiment protects the benefits of the participants who complete the experiment and upholds the overall value of the research enterprise for advancing knowledge (Benbunan-Fich, 2023).


In Experiment 1-1, the goal was to have 250 nurses participate, given the research resources. During the first 14 days of recruitment, 208 nurses completed the 100 tasks, but since the number of nurses who completed the 100 tasks increased by only nine during the following three days, the recruitment of nurses was ended at that point in consideration of the study's feasibility. The nurses' specialty and there are of residence were shown in Supplement 3.6  The time window considered appropriate for convergence in parameter estimation was set. For the state-space model, judgments with a decision time within 10.0 s were used for convergence in parameter estimation was set. For Experiment 1-1, the number of judgments whose decision time was within 10.0 s was 21,055 of 21,700. We concluded that the judgments whose decision time was within 10.0 s were sufficient for examining the relationship between total decision time and correct answer rate for nurses.


Assuming the real-world application scenario adopted in this paper (please refer to footnote 1 and Supplement 1), it is also still appropriate to use the expected value of the correct judgments as evaluation metric of the job performance. Additional details are provided in Supplement 7.


In Experiment 2-1, the goal was to collect 250 medical doctors, given the research resources. During the first 14 days of recruitment, 224 medical doctors completed the 100 tasks, but since the number of medical doctors who completed the 100 tasks did not increase during the following three days, the recruitment of medical doctors was ended in consideration of the study's feasibility. The medical doctors' specialty and there are of residence were shown in Supplement 3.


As described in the following experimental procedure, we designed seven types of forced decision time ( = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, and 6.0). Medical doctors were recruited so that the number of participants in each of the seven groups who completed 100 tasks exceeded 96 each. As described in Section 4.1, this experiment allowed participants to drop out of the experiment before completing 100 tasks. Within seven days of the start of participant recruitment, 402 medical doctors completed 100 tasks. Since the number of participants who completed 100 tasks increased by only two over the next seven days, the recruitment of medical doctors was terminated at this point in consideration of the study's feasibility. The medical doctors' specialty and there are of residence were shown in Supplement 3.








Acknowledgements
We removed detailed funding source information according to the Guidelines for Anonymizing Submissions.






 
Table 1
 
Results of Experiments 1-2 (nurses) and 2-2 (medical doctors).
Forced decision time ( ) ( ) (  
5 [33.8, 37.3]
 Note: For ( ) and ( ), the estimation value and 95% credible interval were showed.


Figure 7
The cognitive load for each group of Experiment 2-2 (top) NASA-TLX (raw-TLX), (bottom) subjective stress.
 










Cognitively inspired task design to improve user performance on crowdsourcing platforms




Alagarai
Sampath






H
Rajeshuni






R
Indurkhya






B








Proceedings of the SIGCHI Conference on Human Factors in Computing Systems


the SIGCHI Conference on Human Factors in Computing Systems


















To pay or not to pay? handling crowdsourced participants who drop out from a research study




R
Benbunan-Fich








Ethics and Information Technology




25


3


34














Understanding and confronting our mistakes: The epidemiology of error in radiology and strategies for error reduction




M
A
Bruno






E
A
Walker






H
H
Abujudeh








Radiographics




35
















Calibration information reduces bias during estimation of factorials: A (partial) replication and extension of Tversky and Kahneman




J
K
Bye






V
Marupudi






J
Park






S
Varma








PNAS


43












Development and validation of the cognitive style indicator




E
Cools






Van Den






H
Broeck








The Journal of Psychology




141


4
















Evaluating Amazon's Mechanical Turk as a tool for experimental behavioral research




M
J
Crump






J
V
Mcdonnell






T
M
Gureckis








PLOS One




8


3


57410














Quality control in crowdsourcing: A sur vey of quality attributes, assessment techniques, and assurance actions




F
Daniel






P
Kucherbaev






C
Cappiello






B
Benatallah






M
Allahbakhsh








ACM Computing Survey




51


1
















Nudgeability: Mapping conditions of susceptibility to nudge influence




D
De Ridder






F
Kroese






L
Van Gestel








Perspectives on Psychological Science




17


2
















Comparing perceptual and preferential decision making




G
Dutilh






J
Rieskamp








Psychonomic Bulletin & Review




23


3


















J
Durbin






S
J
Koopman




Time Series Analysis by State Space Methods


Oxford




Oxford University Press








nd ed.








Incentives to counter bias in human computation




B
Faltings






R
Jurca






P
Pu






B
D
Tran








HCOMP
















Seeing isn't necessarily believing: Misleading contextual information influences perceptual-cognitive bias in radiologists




B
Fawver






J
L
Thomas






T
Drew






M
K
Mills






W
F
Auffermann






K
R
Lohse






A
M
Williams








Journal of Experimental Psychology: Applied




26


4


579














Unfairness by design? The perceived fairness of digital labor on crowdworking platforms




C
Fieseler






E
Bucher






C
P
Hoffmann








Journal of Business Ethics




156


4
















Boosting promotes advantageous risk-taking




T
Folke






G
Bertoldo






D
Souza






S
Alì






F
Stablum






K
Ruggeri
























Humanit. Soc. Sci. Commun




8


270












Homo heuristicus: Why biased minds make better inferences




G
Gigerenzer






H
Brighton








Topics in Cognitive Science




1


1
















Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic




T
L
Griffiths






F
Lieder






N
D
Goodman








Topics in cognitive science




7


2
















Nasa-Task load index (NASA-TLX); 20 years later




S
G
Hart








Annual Meeting Proceedings




Human Factors and Ergonomics Society




50














Crowdsourcing graphical perception: Using mechanical turk to assess visualization design




J
Heer






M
Bostock








P roceedings of the SIGCHI conference on human factors in computing systems


















The speed-accuracy tradeoff: history, physiology, methodology, and behavior




R
P
Heitz








Frontiers in Neuroscience




8


150














Nudging and boosting: Steering or empowering good decisions




R
Hertwig






T
Grüne-Yanoff








Perspectives on Psy chological Science




12


6
















A survey on task assignment in crowdsourcing




D
Hettiachchi






V
Kostakos






J
Goncalves








ACM Computing Surv ey




55


3
















People's intuitions about intuitive insight and intuitive choice




Y
Inbar






J
Cone






T
Gilovich








Journal of Personality and Social Psychology




99


2
















Do defaults save lives?




E
J
Johnson






D
G
Goldstein








Science




80


302
















A perspective on judgment and choice




D
Kahneman








American Psychologist




58


9
















Prospect Theory: An Analysis of Decisions under Risk




D
Kahneman






A
Tversky








Econometrics




47
















Causal reasoning under time pressure: testing theories of systematic non-normativ e reasoning patterns




I
R
Kolvoort






L
Van Maanen








Proceedings of the Annual Meeting of the Cognitive Science Society


the Annual Meeting of the Cognitive Science Society






43














The anchoring bias reflects rational use of cognitive resources




F
Lieder






T
L
Griffiths






Q
J
Huys






N
D
Goodman








Psychonomic Bulletin & Review




25


1
















Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources




F
Lieder






T
L
Griffiths








Behav. Brain Sci




43
















IT and the changing social division of labor: The case of electronics contract manufacturing




B
Liithje








The MIT Press


342






In Transforming enterprise: The economic and social implications of information technology








Crowdsourcing Control: Moving Beyond Multiple Choice




C
Lin






M
Mausam






D
Weld








Proceedings of the AAAI conference on artificial intelligence


the AAAI conference on artificial intelligence






26














Subjective mental workload assessment technique: An introduction to NASA-TLX and SWAT and proposal of simple scoring methods




S
Miyake






M
Kumashiro








The Japanese Journal of Ergonomics




29
















Priming for better performance in microtask crowdsourcing environments




R
R
Morris






M
Dontcheva






E
M
Gerber








IEEE Internet Computing




16


5
















Thinking styles and decision making: A meta-analysis




W
J
Phillips






J
M
Fletcher






A
D G
Marks






D
W
Hine


















Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic




T
L
Griffiths






F
Lieder






N
D
Goodman








Topics in cognitive science




142


3










Psychological Bulletin








Diagnostic errors in abdominopelvic CT interpretation: Characterization based on report addenda




A
B
Rosenkrantz






N
K
Bansal








Abdominal Radiology




41
















Machine learning with crowdsourcing: A brief summary of the past research and future directions




V
S
Sheng






J
Zhang








Proceedings of the AAAI conference on artificial intelligence


the AAAI conference on artificial intelligence






33














Automatic discovery and description of human planning strategies




J
Skirzynski






Y
R
Jain






F
Lieder








Behavior Research Methods


















Reasoning strategy vs cognitive capacity as predictors of individual differences in reasoning performance




R
H
Thaler






C
R V A
Sunstein






H
Markovits








Cognition




217




Yale University Press Thompson






Nudge: Improving decisions about health, wealth, and happiness








Do nudges make use of automatic processing? Unraveling the effects of a default nudge under type 1 and type 2 processing




L
C
Gestel






M
A
Adriaanse






D
T D
De Ridder








Comprehensive Results in Social Psychology


















Speed-accuracy tradeoff and information processing dynamics




W
A
Wickelgren








Acta Psychologica




41


1
















Pricing mechanisms for crowdsourcing markets




S
Yaron






M
Manas








Proceedings of the 22nd international confer ence on World Wide Web


the 22nd international confer ence on World Wide Web


















The pitfall of experimenting on the web: How unattended selective attrition leads to surprising (yet false) research conclusions




H
Zhou






A
Fishbach








Journal of Personality and Social Psychology




111


4


493















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]