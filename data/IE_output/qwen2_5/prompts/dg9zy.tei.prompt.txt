You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
The human brain operates in a perpetual state of activity, whether it is focused on a particular task or wandering in the inner world of thoughts. This activity reflects the non-stationary nature of neuronal dynamics, which are characterized by a complex interplay between transient, evoked states, and ongoing spontaneous fluctuations 
(Galadí et al., 2021;
Melanson et al., 2017)
. The complex cognitive processes that emerge from this neuronal activity also tend to exhibit non-stationary dynamics 
(Craigmile et al., 2010;
Sebastian Castro-Alvarez & Tendeiro, 2023;
Van Orden et al., 2003;
Wagenmakers et al., 2004)
. In other words, proverbial cognitive processes, such as attention, memory, and decisionmaking, are not constant over time, but instead undergo fluctuations, shifts, and alterations in their functions.
Lapses of attention are a canonical cause of such non-stationary dynamics. Even when actively engaged in a task, our focus can drift or momentarily falter 
(Weissman et al., 2006)
. Moreover, our capacity to sustain attention and concentrate may vary, influenced by factors such as fatigue, motivation, and external distractions 
(Esterman & Rothlein, 2019;
Ratcliff & Van Dongen, 2011;
Walsh et al., 2017)
. These fluctuations can have a significant impact on our cognitive functioning, but they are often overlooked or simplified in traditional models of cognition. And while these often assume cognitive processes to be stable and time-invariant, there has been a growing recognition that traditional models do not fully capture the complexity and variability of real-world cognition 
(Beer, 2023;
Cochrane et al., 2023;
Evans & Brown, 2017;
Gunawan et al., 2022;
Kucharský et al., 2021;
Li et al., 2023;
. Common approaches to address variability in the components of cognitive models can be broadly classified into four categories: stationary variability, trial binning, regression approach, and frontend-backend models.
The first approach assumes random fluctuations around a stable mean, referred to as stationary variability (see 
Figure 1A)
. A prominent example of this approach is the "full" diffusion decision model (DDM), which allows for inter- igure 1: A conceptual illustration of the five main approaches to model temporal variation in the parameters θ of a cognitive model G. A Stationary variability, also known as inter-trial variability, assumes that parameter values fluctuate around a stable mean. B Trial binning involves organizing the data into distinct bins and fitting a cognitive model G to each bin individually. C Regression approach employs time (and sometimes additional contextual variables) as predictors for the parameters θ. D Frontend-backend models employ a mechanistic model, referred to as the frontend, to elucidate the dynamics of the parameter of the cognitive model (i.e., the backend). E Superstatistics involve a superposition of multiple stochastic processes operating on different temporal scales. They comprise a low-level observation model G and a high-level transition model T that specifies how the parameters θ t evolve stochastically.
trial variability of its core parameters 
(Ratcliff & Rouder, 1998;
Ratcliff & Tuerlinckx, 2002)
. However, stationary inter-trial variability mainly improves in-sample model fit and cannot identify systematic changes or sudden shifts in core model parameters. Moreover, the resulting model family still treats behavioral data as independent and identically distributed (IID) responses, making it unsuitable for investigating systematic changes in cognitive constructs.
Another approach for detecting systematic changes in cognitive model components is trial binning 
(Evans & Brown, 2017;
Evans & Hawkins, 2019;
Kahana et al., 2018)
. This method involves organizing data into discrete bins and then applying a stationary model to each of these data subsets separately (see 
Figure 1B
). One can then examine variations in parameter estimates across these bins. The challenge in employing this approach is the selection of the number of time steps within each bin, which introduces an unwelcome trade-off between temporal resolution and estimation quality. For instance, if only a few time steps are chosen, the analysis can yield relatively fine-grained, but very uncertain estimates due to the low number of data points. A further shortcoming of trial binning is that estimates within a specific bin are not informed by data from neighboring bins. However, the appeal of dynamic modeling lies in the distinctive capability to utilize both past and future data to constrain the estimated parameter trajectories.
The third approach involves a generalized linear model (GLM) with time (and possibly other contextual factors) as a predictor of model parameters 
(Cochrane et al., 2023;
Evans et al., 2018)
. The GLM approach is more appealing than trial binning, as it can detect linear or non-linear changes in model parameters without loss of resolution (see 
Figure 1C
). However, the underlying regression function makes strong assumptions about the nature of the relationship between model parameters and time. Thus, even though a modeler will typically fit and compare a few plausible specifications (e.g., linear vs. exponential), it is often difficult to determine all plausible specifications a priori, and the overall flexibility of the GLM model as a process characterization remains severely limited 
(Gunawan et al., 2022)
.
Differently, the frontend-backend approach aims to account for changes in model parameters, while providing a mechanistic explanation for the dynamic nature of the target system (see 
Figure 1D)
. Here, the backend model pertains to the cognitive model which formalizes how the behavioral data is generated (e.g., a DDM). The frontend constitutes a mechanistic model, elucidating how the parameters of the backend model adapt over time, in different contexts and in response to additional factors 
Fontanesi et al., 2019;
Osth et al., 2018;
. This approach has several advantages, as it not only accommodates the dynamic nature of the parameters, but also provides a mechanistic description for their temporal variation through a set of static parameters and deterministic functions. For instance, there has been a recent trend to use reinforcement learning models as a frontend model to inform changes in DDM parameters due to reward-based learning 
(Fontanesi et al., 2019;
McDougle & Collins, 2021;
Miletić et al., 2021)
. Nevertheless, detailed frontend models are often challenging to develop, estimate, and compare.
Recently, we proposed an alternative approach that infers non-stationary parameter trajectories directly from the data, while imposing minimal constraints on how parameters change over time . Our approach leverages a framework known as superstatistics 
(Beck & Cohen, 2003;
Beck, 2004;
Mark et al., 2018)
, which involves a superposition of multiple stochastic processes operating on distinct time scales (see 
Figure 1E
). At its core, this model comprises a low-level observation model and a high-level transition model. The former describes how data at a specific time point is generated, akin to the backend model. Like the frontend model, the transition model characterizes how the parameters change over time. However, from a superstatistics perspective, the transition model is inherently a stochastic process, exemplified, for instance, by a Gaussian random walk or a regime switching process.
The superstatistics approach effectively addresses the limitations of prior methodologies. Unlike stationary models, superstatistical models can readily generate non-stationary variations in the parameters of the low-level model, facilitating gradual or sudden transitions between different states. Furthermore, parameter estimates are contingent on past data points, thereby treating the data no longer as IID. In contrast to the trial-binning approach, models within the superstatistics framework leverage the entirety of available data, mitigating concerns about insufficient data points for parameter estimation. Different from GLM approaches, our superstatistics method imposes minimal assumptions on potential parameter trajectories, making it significantly less restrictive.
In contrast to frontend-backend models, superstatistics do not offer mechanistic explanations for parameter dynamics but provide greater flexibility in their estimation. Although mechanistic explanations are central to psychological research, there are cases where suitable explanations are lacking or are applicable only to specific parameters. Therefore, we consider these two approaches as complementary. The superstatistical framework takes a bottom-up, exploratory approach, functioning as a tool for generating hypotheses. In subsequent stages, one could potentially formulate plausible frontend models based on insights from parameter trajectories inferred with a superstatistical model. Additionally, superstatistical models can serve as benchmarks for testing and validating competing frontend-backend models by comparing resulting parameter trajectories from both methods.
Having laid out the potential benefits of the superstatistics framework and its applicability in the realm of cognitive process models , a pivotal question arises: Do the inferred parameter trajectories genuinely reflect shifts in the cognitive constructs they aim to represent, or are they merely a modeling artefact? To address this inquiry, we embark on an experimental validation study. In this study, we manipulate the experimental context in a manner that allows us to confidently anticipate how individuals and, consequently, their inferred cognitive constructs, will respond. In other words, if the inferred parameter time series mirror the alterations in the experimental context, we garner substantial evidence that these trajectories indeed reflect changes in the psychological constructs.
Throughout, we employ the well-established 4-parameter DDM 
(Ratcliff, 1978)
 as a low-level observation model. The DDM is a mathematical model that simultaneously accounts for response time (RT) and choice data obtained from two-alternative decision tasks. Fundamentally, it posits that, in forced-choice binary decision task, individuals accumulate evidence for the decision alternatives until a certain threshold is met, triggering a decision. Each of the DDM's four core parameters corresponds to a specific psychological construct: (i) the drift rate v signifies the average speed of information uptake; (ii) the threshold a serves as a proxy for decision caution; (iii) the relative starting point β represents a priori decision preferences; and (iv) the additional constant τ accounts for the duration of all processes taking place prior and following a decision, such as stimulus encoding or motor action (but see 
Verdonck et al., 2021)
.
A primary reason for our choice of the DDM as the observation model lies in its rigorous prior validation 
(Arnold et al., 2015;
Lerche & Voss, 2019;
Voss et al., 2004)
. These prior studies have convincingly demonstrated that the DDM's parameters are valid reflections of the intended psychological constructs. Moreover, the manipulation of experimental conditions leading to systematic alterations in specific DDM parameters is well-documented and comprehensively understood . For example, varying the difficulty of an experimental task alters the drift rate parameter, whereas providing verbal instructions to prioritize either speed or accuracy during task-solving leads to observable shifts in the threshold parameter and sometimes also in the non-decision time 
(Lerche & Voss, 2018)
.
In this study, we focus on the aforementioned experimental manipulations targeting the drift rate and the threshold parameters. We employed a color discrimination task, which was also utilized in the validation study by 
Voss et al. (2004)
. During this task, individuals must decide whether there are more blue or more orange pixels in a patch of pixels. The difficulty of the task can be easily manipulated by adjusting the ratio of blue and orange pixels. The farther the ratio is from 1:1, the easier the task becomes. Additionally, we manipulated the emphasis on speed or accuracy by verbally instructing participants to prioritize one over the other.
Systematic changes in cognitive model parameter can appear in different ways, ranging from changing slowly and gradually to more rapid and large shifts. In our experiment, we focus on two different types. Firstly, task difficulty changes frequently to the next easier or harder level, imitating gradual changes. Secondly, the speed-accuracy emphasis changes less regularly after each trial block, resembling sudden shifts. The primary aim of our experiment is to investigate whether the parameter trajectories inferred with a non-stationary DDM (NSDDM) match these changing patterns of the experimental conditions. Specifically, we expect the drift rate parameter to mirror the gradual changes of the task difficulty. Additionally, the threshold parameter should show sudden shifts when the priority switches between speed and accuracy. It is crucial to understand that in this application, the NSDDM does not have information about the experimental context and has to infer the parameter trajectory solely on the behavioral data.
When dealing with different types of fluctuations, another crucial question arises: What kind of transition model is most suitable for capturing the expected dynamics? To address this question, we implement different NSDDMs that solely differ in their transition model for the drift rate and the threshold parameter. Specifically, we compare four different transition models: (i) a Gaussian random walk; (ii) a mixture between a Gaussian random walk and uniformly distributed regime changes; (iii) a Lévy flight; and (iv) a regime switching function, where parameters either remain the same as in the previous time step or shift uniformly. These four transition models differ in their complexity (i.e., number of high-level parameters) and their ability to account for different types of temporal shifts.
Performing Bayesian model comparison and parameter estimation with superstatistical models can be computationally challenging . Therefore, we employ simulation-based inference 
(SBI, Cranmer et al., 2020)
 as implemented in the BayesFlow framework 
(Radev et al., 2023)
. BayesFlow enables us to carry out a principled Bayesian workflow utilizing simulation-based calibration 
(SBC, Säilynoja et al., 2022;
Talts et al., 2020)
 and other validation methods 
(Gelman et al., 2020;
Schad et al., 2021
) that would otherwise be excessively time-consuming. The contributions of the present study can be summarized as follows:
1. We perform an experimental validation of different non-stationary instantiations of the diffusion decision model.


2.
We propose an amortized method for Bayesian model comparison of non-stationary models via deep ensembles.
3. We showcase the potential of amortized Bayesian inference for increasing the aspirations of cognitive modeling.


Materials and Methods


Participants
A total of 14 participants (9 female, 5 male) were recruited for the experiment. The participants had an average age of 23.14 years (SD = 1.29, Range = 
[22,
26]
). Every individual provided informed consent to participate in the study, and the research protocol received approval from the local ethics committee. The entire study was conducted in accordance with the ethical principles outlined in the Helsinki Declaration.


Task
The participants completed a total of 800 trials in a color discrimination task, including 32 practice trials. In each trial, individuals were presented with a rectangular patch containing blue and orange pixels and had to determine whether there were more blue or orange pixels. Prior to the patch presentation, a fixation cross was displayed for 300 ms. All stimuli were presented on a gray background.
Task difficulty was manipulated by varying the proportion of blue/orange pixels in the patch. The following ratios were utilized: 50.5:49.5; 52.25:47.75; 53.5:46.5; and 55:45. Half of the trials featured orange as the dominant color, while the other half featured blue. The difficulty level remained constant for either 8 or 16 trials before transitioning to the next level of difficulty. In addition to manipulating task difficulty, participants received two types of instructions which changed every 48 trials. In the "accuracy" condition, individuals were instructed to prioritize accuracy in their responses. Conversely, in the "speed" condition, participants were directed to emphasize speed while maintaining a reasonable level of accuracy. Feedback was provided after each trial to make participants aware of their performance: a green cross for correct responses, a red minus for incorrect responses, and a red clock for responses slower than 700 ms in the speed condition.


Superstatistics Framework
To represent non-stationary changes in DDM parameters, we adopt a superstatistics framework 
(Beck & Cohen, 2003;
Mark et al., 2018)
. Within this framework, each generative model comprises (at least) a low-level observation model G characterized by time-dependent local parameters θ t ∈ R K that vary according to a high-level transition model T with static high-level parameters η ∈ R D . These models simulate parameters and observable data x t ∈ X according to the following general recurrent system
θ t = T (θ 0:t−1 , η, ξ t ) with ξ t ∼ p(ξ | η), θ 0 ∼ p(θ) x t = G(x 1:t−1 , θ t , z t ) with z t ∼ p(z | θ t ),
(1)
where T represents an arbitrary high-level transition function parameterized by η, and G is a (non-linear) transformation that encapsulates the functional assumptions of the low-level model. The random variates ξ t and z t govern the stochastic nature of the two model components through noise outsourcing. The initial parameter configuration θ 0 adheres to a prior distribution θ 0 ∼ p(θ) encoding the available information about feasible starting parameter values.
The above formulation is very abstract and general, highlighting the flexibility of the superstatistics framework. Moreover, it does not assume that the corresponding transition or likelihood densities, given by
T(θ t | η, θ 0:t−1 ) = p(θ t , ξ | η, θ 0:t−1 ) dξ (implied transition density) (2) p(x t | θ t , x 1:t−1 ) = p(x t , z | θ t , x 1:t−1 ) dz (implied likelihood density),
(3)
are tractable or available in closed-form, situating our approach in the context of simulation-based inference 
(SBI, Cranmer et al., 2020)
. Here, we build on SBI with neural networks 
(Ardizzone et al., 2018;
Greenberg et al., 2019;
 as a principled approach to perform fully Bayesian inference by using only samples from the generative system defined by Equation 1. Importantly, our estimation methods overcome key limitations of previous approaches related to the curse of dimensionality 
(Mark et al., 2018)
.


Low-Level Model
In this work, we use the same standard DDM implementation as a low-level observation model G for all NSDDMs. The low-level dynamics of the evidence accumulation process are described by the following stochastic ordinary differential equation:
dx n = vdt s + z dt s with z ∼ N (0, 1)
(4)
Accordingly, the evidence x n on a given trial n follows a random walk with drift v and Gaussian noise z, where t s represents time on a continuous time scale. The core assumption of the DDM is that evidence is accumulated with a fixed rate v until one of two thresholds, a or 0, is reached, and the corresponding decision D n is made:
D n = 1, if x n ≥ a 0, if x n ≤ 0 .
(5)
Furthermore, the DDM incorporates an additive constant τ , which represents the time allocated to all non-decisional processes (i.e., stimulus encoding and motor action). Consequently, the DDM encompasses three distinct free parameters, namely θ = (v, a, τ ). We fixed the starting point of the evidence accumulation process at a/2, since, in our case, the two boundaries of the accumulation process correspond to correct and incorrect responses, respectively. Thus, it is unwarranted to estimate any potential a priori bias towards either of these boundaries 
(Voss et al., 2013)
.


High-Level Models
We formulate and compare four different high-level transition models, denoted as T 1 , ..., T 4 , which govern the trialby-trial changes in local DDM parameters θ 1:T . These transition models vary in terms of their flexibility in allowing changes to the low-level parameters and their underlying complexity, including the number of high-level parameters
Trial (t) Parameter
Random walk
Trial (t)
Mixture random walk
Trial (t)
Levy flight
Trial (t)
Regime switching 
Figure 2
: An example illustration of the four high-level (transition) models considered in our study, governing the temporal variation of a hypothetical low-level model parameter.
involved (see 
Figure 2
 for an exemplar trajectory generated by each transition model). To ensure that the low-level parameters remain within plausible ranges, we impose both lower and upper bounds on their trajectories. Specifically, we set the upper bounds for the parameters v, a, and τ to 8, 6, and 4, respectively. Additionally, since negative parameter values are not meaningful for our DDM specification, we set the lower bounds for all parameters to 0. For all transition models, we assume independence between the trajectories of the local DDM parameters.
Random Walk The first transition model (T 1 ) convolves the low-level model's parameters with a Gaussian distribution, resulting in a gradual change that follows a random walk:
T 1 (θ k,t | θ k,t−1 , σ k ) = N (θ k,t | θ k,t−1 , σ k )
(6)
According to this transition model, the current value of each parameter θ k,t is only influenced by its previous value θ k,t−1 , generating more or less auto-correlated and gradual changes.
Mixture Random Walk The second transition model (T 2 ) corresponds to a mixture distribution between a random walk (cf. Equation 6) and uniformly distributed shifts:
T 2 (θ k,t | θ k,t−1 , ρ k , σ k , a k , b k ) = ρ k N (θ k,t | θ k,t−1 , σ k ) + (1 − ρ k ) U(a k , b k )
(7)
where ρ indicates the probability of the type of change (gradual change or shift) as a mixing coefficient for the two states. The upper and lower bounds of the uniform distribution, denoted as a and b, are set to cover plausible parameter ranges and are not treated as free parameters themselves.


Lévy-Flight
The Lévy flight transition model (T 3 ) is similar to the Gaussian random walk. However, instead of assuming normally distributed noise, it assumes an alpha-stable transition for each component of θ:
T 3 (θ k,t | θ k,t−1 , σ k , α k ) = Alpha-Stable(θ k,t | θ k,t−1 , σ k , β = 0, α k )
(8)
where 0 < α ≤ 2 governs the heaviness of the noise distribution's tails. If α k = 2 then the distribution is equivalent to a Gaussian distribution. Notably, as the value of α decreases, the distribution's tails get heavier, allowing for larger shifts in the parameter values. When simulating from the Lévy flight transition model, we use a scale of σ k / √ 2, such that the corresponding Gaussian distribution for α k = 2 has a standard deviation of σ k .
Regime Switching Finally, the regime switching transition model (T 4 ) is a simpler version of the mixture random walk. The parameter's trajectory adheres to one of two possibilities: it either maintains its previous value or undergoes a uniform shift:
T 4 (θ k,t | θ k,t−1 , ρ k , a k , b k ) = ρ k δ(θ k,t − θ k,t−1 ) + (1 − ρ k ) U(a k , b k ),
(9)
where δ(•) is the Dirac delta distribution indicating that the parameter either does not change at all with probability ρ or undergoes a sudden change with probability 1 − ρ.
Strictly speaking, some of the above transition models can effectively be transformed into others by employing specific high-level parameter configurations. For instance, the mixture random walk with σ = 0 reduces to the regime  
Figure 3
: A conceptual illustration of our amortized Bayesian inference training setup. A Parameter estimation A recurrent summary network processes the synthetic time series x 1:T and learns maximally informative temporal summary statisticsx 1:T . An inference network (i.e., normalizing flow) learns to approximate the joint posterior distribution of time-varying low-level parameters θ 1:T and static high-level parameters η given the learned summaries. B Model comparison A transformer summary network consumes time series simulated from competing models and learns maximally informative summary vectorsx. An inference network (i.e., a probabilistic classifier) learns to approximate posterior model probabilities (PMPs) given the summary vectors. Once trained, the networks can be efficiently validated using principled Bayesian methods and applied to the observed data. switching transition function. Conversely, when ρ = 0 it reduces to a simple Gaussian random walk. Also, the Lévy flight transition model with α = 2 turns into a random walk transition function. The mixture random walk and the Lévy flight transition function both have two high-level parameters and can thus be regarded as more complex and more flexible than the other two transition models, which only have a single high-level parameter. Notably, the random walk transition model is the only one that cannot generate relatively large sudden shifts in parameter values.


Model Comparison Setup
One of the major aims of this study is to compare four NSDDMs sharing the same low-level diffusion model but differing in their assumptions about the type of stochastic variation of the drift rate (v) and threshold (a) parameters. All four NSDDMs employ the same Gaussian random walk model T 1 for the non-decision time parameter (τ ). We base this decision on previous research  and the rationale of our experimental manipulations, which should not imply sudden large shifts in the τ parameter. For M 1 , the drift rate and threshold parameter also follow a Gaussian random walk, resulting in three high-level parameters, η = (σ v , σ a , σ τ ). In M 2 , both v and a follow a mixture between a Gaussian random walk and uniform shifts (T 2 ), which results in a total of five highlevel parameters, η = (σ v , σ a , σ τ , ρ v , ρ a ). In contrast, M 3 introduces a trajectory for the drift rate and threshold parameters characterized by a Lévy flight (T 3 ), which has five free high-level parameters,
η = (σ v , σ a , σ τ , α v , α a ).
Lastly, for M 4 , the two parameters v and a either remain the same as in the previous time point or shift uniformly (T 4 ). This model has a total of three high-level parameters, η = (σ τ , ρ v , ρ a ). A listing of the weakly informative prior distributions assigned to the model parameters can be found in the Appendix.


Amortized Bayesian Inference
Amortized Bayesian inference (ABI) is a flexible framework for estimating, comparing, and validating complex models through simulation-based training of specialized neural networks 
(Radev et al., 2023)
. ABI consists of (i) a training phase where the networks learn a surrogate distribution, and (ii) an inference phase where the networks infer the target quantities (e.g., model parameters or model posterior probabilities) in real-time for any new data set supported by the model(s). The neural networks are trained purely on simulations from the generative model and do not require an explicit likelihood or numerical integration. Thus, ABI re-casts expensive Bayesian inference into a neural network prediction task, such that sampling from the target posterior and model refits happen almost instantaneously.
Amortized Parameter Estimation Our deep learning approach for jointly estimating time-varying and static parameters follows , who extend ideas from ABI with static parameters 
(Gonçalves et al., 2020;
 to non-stationary Bayesian models. Accordingly, our goal is not only to infer the tra-jectories of all three model parameters {θ t } T t=1 , but also to estimate the posterior distribution for the static high-level parameters η of the transition model. Thus, we are interested in recovering the full joint posterior p(θ 1:T , η | x 1:T ) from the observed time series {x t } T t=1 :
p(θ 1:T , η | x 1:T ) ∝ p(η, θ 0 ) p(x 1 | θ 1 ) T t=2 p(x t | θ t , x 1:t−1 ) T t=1 T(θ t | η, θ 0:t−1 )
(10)
where p(η, θ 0 ) is the joint prior over high-level parameters and initial low-level parameter values. The joint prior typically factorizes as p(η, θ 0 ) = p(η)p(θ 0 ), assuming that η and θ 0 are independent in the absence of any information. Even though our SBI method is applicable to any model of the general form in Eq. 10, our low-level (Low-Level Model) and high-level (High-Level Models) specifications lead to a simplified formulation
p(θ 1:T , η | x 1:T ) ∝ p(η, θ 0 ) T t=1 p(x t | θ t ) T t=1 T(θ t | η, θ t−1 ).
(11)
The simplified formulation follows from the fact that our transition models share the Markov property and the DDM likelihood depends on time only through the current parameter θ t in the latent trajectory θ 1:T .
Following the typical ABI offline training setting (see 
Figure 3A
 for a conceptual illustration), we generate a data set of simulated data sets,
D = {η (b) , θ (b) 1:T , x (b) 1:T } B b=1
, and use the simulated data to train a specialized neural network, F ψ (θ 1:T , η; x 1:T ), which approximates the full joint posterior (i.e., a normalizing flow, see 
Papamakarios et al., 2021)
. In particular, we minimize the following loss in expectation over the full non-stationary generative model (i.e., the right hand-side of Eq. 10)
L(ψ) = E (η,θ 1:T ,x 1:T )∼D [− log q ψ (θ 1:T , η | x 1:T )] ,
(12)
where we approximate the expectation over p(θ 0 ) p(η, θ 1:T , x 1:T ) via our training set D and regularize against overfitting with standard techniques, such as dropout and weight decay. It is also possible to run the simulator(s) indefinitely and perform online training using on-the-fly simulation . In fact, this approach should be preferred for fast simulators, as it makes overfitting hardly possible. Thus, online learning is the approach we pursue for estimating the parameters of our NSDDMs.
In the context of dynamic Bayesian models, we have many choices on how to factorize the joint posterior 
(Särkkä, 2013)
. The two most common choices are to approximate the filtering distribution or the smoothing distribution 
(Mark et al., 2018)
. The filtering distribution corresponds to an online analysis, where the low-level parameters θ t at time step t are only informed by past data points. Differently, the smoothing distribution conditions the posterior of θ t on all past and future data points, and provides potentially sharper estimates. Thus, in this study, we exclusively target the approximate smoothing distribution due to its superior parameter recoverability in an offline analysis. 2 In practice, we employ unidirectional or bidirectional long-short term memory (LSTM) networks 
(Gers et al., 2000)
 with many-to-many input-output relationships as a backbone for approximating the filtering or the smoothing distribution, respectively. We then train four separate neural approximators, such that each network becomes an "expert" in inferring the smoothing distribution of the corresponding NSDDM. The Appendix contains more details on the neural network settings and training hyperparameters.
Amortized Model Comparison To conduct a comparative analysis of the four NSDDMs, we focus on Bayes factors (BFs) and posterior model probabilities (PMPs), which can be classified as prior predictive methods embodying Occam's razor 
(Kass & Raftery, 1995;
MacKay, 2003)
. The efficacy of these measures has been demonstrated in a wide range of psychological modeling studies 
(Heck et al., 2023)
. Nevertheless, an ongoing debate surrounds the preference between the two 
(Tendeiro & Kiers, 2019;
van Ravenzwaaij & Wagenmakers, 2022)
. Since BFs and posterior odds (i.e., ratios between PMPs) are equivalent when all models are assumed to be equally likely a priori, we estimate and analyse both quantities in our study.
Following the common Bayesian terminology (MacKay, 2003), we can refer to the four competing models through an index set
M = {M 1 , M 2 , M 3 , M 4 }.
The aim of prior predictive Bayesian model comparison is to find the simplest most plausible model within M. To this end, we can compute PMPs for each of the competing models
p(M j | x 1:T ) = p(x 1:T | M j ) p(M j ) E p(M) [p(x 1:T | M)] ,
(13)
where p(M) refers to the prior distribution over the discrete model space. The marginal likelihood p(x 1:T | M j ) plays a crucial role in Equation 13, and can be expressed by integrating out all parameters of the joint model,
p(x 1:T | M j ) = p(η, θ 0 ) T t=1 p(x t | θ t , M j ) T t=1 T j (θ t | η, θ t−1 ) dη dθ 0 , . . . , dθ T .
(14)
Importantly, since the marginal likelihood averages the likelihood over the joint prior, it automatically incorporates a probabilistic Occam's razor, favoring models with constrained prior predictive flexibility. When comparing a pair of competing models, M j and M i , we can compute the ratio between their respective marginal likelihood,
BF ji = p(x 1:T | M j ) p(x 1:T | M i ) .
(15)
This ratio is referred to as the Bayes factor (BF). Consequently, a BF ji > 1 signifies a relative preference for model j over model i based on the given data x 1:T 
(Kass & Raftery, 1995)
.
Unfortunately, the marginal likelihood is notoriously hard to approximate 
(Gronau et al., 2017)
 and even doubly intractable for mechanistic models with unknown or unnormalized likelihoods. To circumvent this intractability, we follow the neural method of 
Elsemüller, Schnuerch, et al. (2023)
 and 
Radev, D'Alessandro, et al. (2020)
 which enables amortized Bayesian model comparison for arbitrary computational models (see 
Figure 3B
 for a graphical illustration). This method involves the simultaneous training of two neural networks with different roles: a summary network and an inference network. The summary network learns maximally informative summary statistics from the raw data (e.g., behavioral time series). The inference network approximates the PMPs for the candidate models, q ϕ (M | x 1:T ) given the outputs of the summary network. Here, we subsume all trainable network parameters under ϕ and refer to the composition of the two networks as an evidential network.
The training data for the evidential network consists of all simulations from the candidate models together with the corresponding model index,
D(M) = {x (b) 1:T , M (b) j } B ′ b=1
, where B ′ denotes the total number of simulations from all models. Together, the two networks minimize the standard cross-entropy loss,
L(ϕ) = E (Mj ,x 1:T )∼D(M) − J j=1 I Mj log q ϕ (M j | x 1:T ) ,
(16)
and we approximate the expectation over p(η, θ 1:T , x 1:T ) by our training set D(M), and I Mj denotes an indicator function (i.e., one-hot encoding) for the true model index. In principle, we could use online learning for amortized model comparison as well, but we found offline training to yield sufficiently accurate results.
More recently, 
Elsemüller, Olischläger, et al. (2023)
 demonstrated the importance of gauging the sensitivity of amortized neural approximators, especially in the context of model comparison. The authors suggest to train an ensemble of multiple evidential networks, instead of relying on a single network. Accordingly, we can measure the (lack of) agreement between ensemble members and obtain a hint at the robustness of the approximate PMPs. Here, we trained an ensemble of ten evidential networks and computed the mean and standard deviation of the estimated PMPs across all ten networks. For more details regarding the neural network architecture and training settings, we refer the reader to the Appendix.
Code Availability Complete code for reproducing the results reported in this manuscript is available in the project's GitHub repository https://github.com/bayesflow-org/Non-Stationary-DDM-Validation.


Results


Model Comparison
As a first step, we assess the closed-world (i.e., in silico) performance of our model comparison method in terms of computational faithfulness and accuracy of model recovery. To assess the former, we perform simulation-based B A 
Figure 4
: In silico model comparison and sensitivity results. A Calibration curves of all four NSDDMs aggregated across the neural approximator ensemble. Additionally, the expected calibration error ( ECE) is annotated within each subfigure. The gray histograms depict the relative frequencies of the predicted model probabilities. B Confusion matrix between true data generating model and predicted model. The proportion values were averaged across the ten neural approximator within the ensemble. calibration (SBC; 
Säilynoja et al., 2022;
Talts et al., 2020
) based on 10 000 synthetic data sets per model. 
Figure 4A
 shows the calibration curves for each NSDDM averaged across the ten evidential networks in our deep ensemble. We observe excellent calibration with very minimal expected calibration errors ( ECE) across all models. Thus, we conclude that the approximate posterior probabilities are well-calibrated in the closed-world setting.
Next, we assess the accuracy of our model comparison networks in terms of their ability to correctly identify the ground-truth data-generating model. To this end, we apply the deep ensemble to the 40 000 synthetic data sets we have already simulated for assessing calibration. In 
Figure 4B
, we present the resulting confusion matrix, which illustrates the agreement between true and predicted models averaged across the ten approximators. Among the four models, the random walk DDM is the only one that rarely gets confused with the other models. A possible explanation is that it is the only transition model not capable of generating sudden shifts in parameter values. The remaining models are susceptible to more frequent misclassifications. For example, the mixture random walk DDM is correctly identified only 54% of the time, and it is often confused with the regime switching model, occurring 43% of the time. Notably, the Lévy flight DDM is prone to mimicry with the random walk DDM (on average 30% of the time).
It is essential to emphasize that these results do not imply a deficiency in your model comparison method, but rather underscore the fact that certain pairs of models, such as the mixture random walk and the regime switching DDM, can generate remarkably similar data patterns. For instance, a significant portion of the prior distribution's mass for the α parameter of the Lévy flight transition model centers around 2. If α ≈ 2, then the Lévy alpha-stable distribution closely resembles a Gaussian distribution, with equality in the case of α = 2. Consequently, simulating the Lévy flight DDM would often yield data patterns that could have just as plausibly originated from the simpler random walk DDM.
Similarly, a substantial portion of the prior mass for the σ priors of the mixture random walk transition model clusters around 0, which subsequently transforms it into a regime switching transition model, resulting in large overlap in synthetic data sets. Interestingly, the mixture random walk and the Lévy flight DDM are seldom confused, even though both models can produce subtle local changes and large sudden shifts. This implies that these two transition models generate qualitatively similar but quantitatively easy to distinguish parameter trajectories. In summary, the observation of occasional model confusion is not a limitation of our method; rather, it underscores our method's effectiveness in discerning when two models generate highly similar data, making them less straightforward to differentiate from each B A other. Moreover, the amortization property of our method enables us to easily conduct such simulation studies prior to analyzing real data -estimating 40 000 posterior model probabilities would have been infeasible for any other method.
After successfully validating our model comparison method, we apply the deep ensemble to the empirical data of the 14 participants. Each approximator in the ensemble was used to infer posterior model probabilities (PMP) for each model, considering each individual's data separately. Subsequently, we calculated the mean (points), median (stars), and 75% credibility interval (CI) for the PMPs for all approximators the 14 individuals ( 
Figure 5A
). The analysis reveals that the Lévy flight DDM is the most plausible model with an average PMP of approximately 60%. It was the most plausible model for 9 out of the 14 participants. In contrast, the mixture random walk model collects an average PMP of less than 30%. Nevertheless, it was estimated to be the most plausible model for 5 participants. The random walk DDM and regime switching DDM were consistently less plausible than the other models and did not emerge as superior for any of the participants.
In addition to PMPs, we computed log 10 Bayes factors (BF). 
Figure 5B
 depicts a heatmap of BFs for all one-toone comparisons between our four NSDDMs, averaged across the participants and the evidential networks of the ensemble. Following 
Kass and Raftery (1995)
, an absolute value of log 10 (BF) > 2 indicates decisive evidence, absolute values between 1 to 2 signify strong, and between 0.5 to 1 substantial evidence. An absolute value of log 10 (BF) < 0.5 is labeled as not worth more than a bare mention. The BF patterns in 
Figure 5B
 align with the PMP findings, implying strong evidence for the Lévy flight DDM over the random walk DDM and substantial evidence over the other NSDDMs. Also, both the mixture random walk and the regime switching DDM have substantial evidence over the random walk model. Interestingly, there is little evidence favoring the mixture random walk DDM over the regime-switching model, suggesting comparable performance.
These findings offer two substantive insights. First, the ability of transition models to generate sudden shifts in parameters seems essential, as seen in the random walk DDM's lower plausibility. Moreover, the regime switching DDM, allowing for occasional shifts, but neglecting small gradual changes, performed less effectively than the more complex models. This result underscores the importance of accommodating both gradual as well as sharp changes in model parameters for achieving optimal fit. Consequently, the more complex NSDDMs, particularly the mixture random walk DDM and Lévy flight DDM, emerged as more plausible than their simpler counterparts, despite the implicit penalty for prior complexity imposed by Bayesian model comparison. Posterior Re-simulation
Subsequently, we fit all four variants of the NSDDM to each of the 14 data sets, evaluating the absolute goodnessof-fit of each model. To achieve this, we conducted 500 re-simulations with randomly sampled posterior parameter trajectories for each individual data set. In 
Figure 6A
, we present the median and median absolute deviation (MAD) of response times (RT) across all individuals and re-simulations. We provide these aggregates for each NSDDM, categorized by task difficulty level and the two experimental conditions. Notably, an initial observation reveals that the experimental manipulations were effective on average: empirical median RTs increased with task difficulty, and individuals tended to respond faster in the speed condition compared to the accuracy condition. Remarkably, all four variants of the NSDDM demonstrated an outstanding fit to these empirical data patterns. Solely, RTs in the accuracy condition with the highest task difficult level consistently are underestimated by all NSDDM variants.
The empirical and re-simulated proportion of correct choices (accuracy) are aggregated and presented in the same way as the RTs (see 
Figure 6B
). Again, the empirical data mirror the anticipated patterns resulting from our experimental manipulations. As expected, accuracy diminishes with increasing task difficulty. Individuals are generally less accurate in the speed condition compared to the accuracy condition. Although NSDDMs successfully reproduce the general patterns in the choice data, we observe notably worse re-simulation compared to that of the RTs data. In both accuracy and speed conditions, re-simulated accuracies exhibit a less pronounced decline as a function of difficulty than observed in the empirical data. Further, the difference in accuracy between the two experimental conditions is less pronounced in the re-simulated data compared to the behavioral data. Notably, the random walk DDM underperforms relative to the other three NSDDMs in this analysis.
It is important to highlight that, unlike conventional approaches, the models did not receive any information regarding the specific experimental context an individual faced at any given moment. From these analyses, we conclude that all NSDDM implementations successfully capture the general patterns in the empirical RT data. Individual participant analyses, detailed in the Appendix, affirm the same conclusions.  
Figure 7
: Model fit to response time (RT) time series. The empirical RT time series of two exemplar individuals are shown in black. From trial 1 to 700, the posterior re-simulations (aka retrodictive checks) using the best fitting non-stationary diffusion decision model (NSDDM) for the specific individual are shown in blue and red, respectively. In this instance, the left column showcases results from a Lévy flight DDM, while the right column displays parameter trajectories from a mixture random walk DDM. For the remaining trials, one-step-ahead posterior predictions from the NSDDMs are depicted in cyan and orange, respectively. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.
In addition to the analysis of the absolute model fit on the aggregate level, we evaluated the fit across the RT time series. For every participant, we generated 250 posterior re-simulations for the first 700 trials with the corresponding best fitting NSDDM. The remaining 68 data points were left out for predictive analysis. Employing a one-step-ahead prediction approach, we iteratively forecasted the subsequent data point, followed by a re-fitting of the model in each step. 
Figure 7
 illustrates the empirical and re-simulated RT time series for two exemplary participants. Results for the remaining 12 participants can be seen in the Appendix. The colored lines depict the median and the shaded bands represent to 90% credibility intervals (CI) across the 250 re-simulations. Both the empirical data (solid black lines) and the re-simulated/predicted RTs were smoothed using a simple moving average (SMA) with a period of 5. Yellow shaded regions highlight trials where speed was emphasised over accuracy, whereas blank white areas denote instances where the opposite emphasis was applied. Overall, RTs were slower and more variable in the accuracy condition. Notably, the NSDDM not only closely replicated the empirical time series but also effectively predicted future data points. This suggests that the model does not overfit the data.


Parameter Estimates
At the heart of the current validation study are the inferred parameters, prompting a crucial question: Do these parameter dynamics align with the sequence of experimental manipulations? We address this question by examining both the time-averaged and time-varying estimates.


Aggregate Analysis
We initially examine the parameter estimates averaged across individuals for each difficulty level and condition separately. This provides a comprehensive overview of average effects on model parameters in different experimental contexts, at first, without delving into the temporal aspect. The bottom panel of 
Figure 6
 illustrates the posterior medians and MADs collapsed onto the different experimental contexts for the drift rate ( 
Figure 6C
) and threshold parameter ( 
Figure 6D
).
Analyzing the aggregated drift rate estimates reveal an anticipated pattern. On average, the drift rate decreases as task difficulty increases, observed in both the accuracy and speed conditions. Additionally, slightly higher overall values are estimated in the speed condition compared to the accuracy condition. While all four NSDDMs yield fairly similar Participant 11 Lévy flight DDM Participant 6 Mixture random walk DDM 
Figure 8
: Estimated parameter trajectories of two exemplar individuals corresponding to the respective best-fitting non-stationary diffusion decision model (NSDDM). In this instance, the left column showcases results from a Lévy flight DDM, while the right column displays parameter trajectories from a mixture random walk DDM. Each low-level parameter (drift rate, threshold, and non-decision time) is displayed on a separate row. The solid lines are color-coded (blue for the Lévy flight DDM and red for the mixture random walk DDM) to represent the posterior medians, while the shaded regions mark the median absolute deviation (MAD). The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. The sequences of task difficulty levels are depicted with black lines and overlaid with the drift rate in the top panels.
parameter values, the distinctions in average parameter values between difficulty levels are less pronounced when estimated with the random walk DDM.
With the second experimental manipulation -namely, the instruction to emphasize speed or accuracy -we aimed to manipulate the participants' decision caution, which is assumed to be captured by the threshold parameter. Examining the aggregated estimates of the threshold parameter in 
Figure 6D
, we observe generally increased values in the accuracy condition compared to the speed condition. Interestingly, in the accuracy condition, the threshold parameter also slightly increases with growing task difficulty -a pattern not observed in the speed condition. A comparison between the estimates of the four NSDDMs reveals that the mixture random walk DDM and the Lévy flight DDM yield higher threshold estimates in the accuracy condition compared to the other two NSDDMs. Conversely, all four NSDDMs seem to converge in their threshold parameter estimates in the speed condition.
Parameter Trajectories For a more fine-grained analysis, particularly considering temporal aspects, we present the complete inferred parameter trajectories of the three low-level parameters of a NSDDM for two exemplary individuals in 
Figure 8
. The Appendix contains the inferred parameter trajectories of the remaining 12 participants. Each partici-pant's trajectory is depicted with the posterior median (solid lines) and the median absolute deviation (MAD, shaded bands) across all 768 experimental trials, estimated with the model with the highest posterior model probability for that specific individual. The trajectory of participant 11 corresponds to a Lévy flight DDM, whereas the trajectory of participant 6 comes from a mixture random walk DDM. Shaded blocks along the timeline denote the experimental condition at a given trial, with yellow indicating an emphasis on speed.
The top panel illustrates the estimated trajectories of the drift rate parameter alongside the sequences of task difficulty levels (depicted by black line). Here, 0 corresponds to the most difficult level, while 6 represents the easiest. It is important to note that the absolute values of the difficulty conditions hold no intrinsic meaning. As observed, the drift rates of both participants align with the overarching trend of the difficulty condition sequence. They decrease when the difficulty is high and increase as the task becomes easier.
Regarding the trajectory of the threshold parameter (middle panel), we anticipated that a shift from an accuracy instruction to a speed instruction would lead to a decrease in the threshold parameter, and vice versa. This hypothesized pattern is clearly evident when examining the estimated threshold parameter trajectories of the two participants in the middle panel of 
Figure 8
. For instance, the threshold parameter estimated for participant 11 oscillates around an approximate value of 1 in the speed condition. Moreover, it consistently rises whenever a switch in the accuracy condition takes place. Intriguingly, the parameter's value during accuracy emphasis is not as uniform compared to the speed condition. In some blocks, it fluctuates around 2, while in others it hovers around 1.5 or even lower. Similarly, participant 6 displays pronounced shifts in the threshold parameter when a change in the condition occurs, with these shifts being more pronounced in the first half of the experiment and diminishing in the second half.
Finally, the bottom panel of 
Figure 8
 illustrates the trajectory of the non-decision time parameter. Although our experimental manipulations did not systematically target the dynamics of this parameter, it is sometimes assumed that the manipulation of speed and accuracy instructions may also influence it 
(Arnold et al., 2015;
Voss et al., 2004)
. While both individuals exhibit some fluctuations in τ , no systematic differences between the two conditions are apparent.
Upon reviewing the parameter trajectories of the remaining participants in the Appendix, similar patterns emerge. In summary, both the inferred means and trajectories of the drift rate and threshold parameters align with the sequence of experimental manipulations, as predicted by our design. Moreover, our NSDDMs were able to estimate these trajectories directly from the behavioral data, getting no explicit information whatsoever about the experimental context. Thus, our validation study suggests that NSDDMs are able to detect genuine changes in cognitive constructs.


Discussion
Psychology and cognitive science are witnessing a growing interest in incorporating dynamic aspects into mechanistic models that seek to formalize and explain cognitive processes. In a previous study, we explored a method to estimate plausible trajectories of cognitive process model parameters directly from behavioral data . Nevertheless, an empirical validation of this modeling approach was lacking. Thus, the current study sought to bridge this gap by experimentally examining the validity of the inferred diffusion decision model (DDM) parameter dynamics.


Experimental validation
The present study posed to the following core question: Can non-stationary DDMs (NSDDM) effectively detect experimentally induced changes in cognitive constructs from behavioral data alone? If this holds true, our findings can provide the first substantial evidence for the validity of the superstatistics framework as applied to cognitive models. Notably, our results demonstrated that the NSDDMs indeed reliably identified the sequence of two experimental manipulations, despite the absence of any contextual information. Moreover, posterior re-simulation revealed an outstanding fit to the response data, both on an aggregate level as well as on the level of the raw time series. This performance stands as compelling evidence supporting the validity of NSDDMs.
The trajectory of the drift rate parameter for all individuals closely mirrored the sequence of the task difficulty manipulation. Specifically, the drift rate parameter decreased when task difficulty increased, and conversely, increased as task difficulty decreased. This not only confirms the anticipated impact of the manipulation, but also highlights the NSDDMs' ability to discern these variations directly from the behavioral data, agnostic to additional contextual information.
Interestingly, drift rates increased throughout the experiment, although this was not the case for trials with the highest task difficulty. This observation suggests a practice effect among participants, where task performance generally improved with experience, except under the most challenging condition. Practice effects are a widely recognized phenomenon in various decision-making and memory paradigms 
(Forstmann et al., 2008;
Healey & Kahana, 2014
, 2016
Wagenmakers et al., 2008;
Wynton & Anglim, 2017)
. In fact, practice effects have been studied with various dynamic cognitive modeling approaches 
(Evans et al., 2018;
Evans & Hawkins, 2019;
Gunawan et al., 2022;
Kahana et al., 2018)
. A notable contribution to this field comes from 
Gunawan et al. (2022)
, who conducted a comprehensive re-analysis of three datasets derived from widely cited articles. Their study compared three dynamic models: (i) a smooth polynomial trend, (ii) a non-smooth autoregressive process, and (iii) a regime switching model instantiated by a hidden Markov model (HMM) with two different states.
In their study, 
Gunawan et al. (2022)
 employed a low-level model similar to the DDM, namely the linear ballistic accumulator model (LBA; . However, their transition models, specifically the polynomial trend and the autoregressive process, differed in that they allowed LBA parameters to change only from block-toblock, neglecting trial-to-trial parameter fluctuations (except for the HMM). Their findings indicated that the HMM outperformed the other two dynamic model instantiations. This superiority can possibly be attributed to the model's capacity to flexibly change parameters from trial-to-trial, in contrast to changes occurring only from block-to-block. Even though the trial-by-trial specification of the HMM captures the microstructure of the decision-making process, it is still less flexible than the models we examined in the current study. HMMs assume a pre-defined number of possible states, whereas this is not the case with the implementation of our regime switching model. The advantage of not fixing the number of distinct states beforehand is particularly evident when the exact latent quantity is unknown prior to investigation. Moreover, results from our model comparison clearly favored transition models that account for both, gradual changes as well as sudden shifts. This suggests that regime-switching models may fall short in certain fields of application. Nevertheless, both models have their merits, and the choice between them should be guided by the specific research question at hand and formal model comparison.
As our study focused on experimentally validating parameter trajectories estimated with NSDDMs, we deliberately refrained from further analysing practice effects. However, we suggest that our flexible framework could be a promising alternative for investigating practice effects. Unlike pure regime switching models, it has the capacity to reveal a mixture of practice-related changes, ranging from abrupt shifts to gradual changes. When exploring substantive research questions, such as practice effects, with superstatistical models, it is imperative to depart from the approach taken in the current study. That is, one should always incorporate contextual information from the experimental setting when estimating parameter trajectories. Here the question arises, how to incorporate this information? In a previous study, we simply assumed separate low-level parameters for each experimental condition . This approach is particularly appropriate when conditions randomly change from trial to trial. However, future research could explore alternative ways of including experimental context information with the goal to further inform the parameters.
Concerning the second experimental manipulation, that is, the emphasis on speed or accuracy, their effect on the threshold parameter is more diverse across individuals. While a majority of participants demonstrated shifts in the threshold parameter in response to instructional changes, the consistency and magnitude of these changes varied significantly among individuals. Some participants exhibited only a few adjustments in the threshold parameters, seemingly overlooking the change in instruction on certain occasions. In contrast, others consistently heightened their threshold parameter during accuracy-focused tasks, followed by a subsequent decrease when transitioning to speed-oriented conditions. Meanwhile, some participants displayed rather unsystematic changes in decision caution, suggesting that the participants of reacted differently to the speed-accuracy manipulation. 
Kucharský et al. (2021)
 introduced a dynamic LBA incorporating a hidden Markov transition model with two states, akin to the model proposed by 
Gunawan et al. (2022)
. Their focus centered on scrutinizing the speed-accuracy tradeoff, exploring the hypothesis that individuals dynamically switch between different operating states under varying instruction conditions. By fitting their model to previously collected data, they provided evidence that individuals tend to oscillate between two stable states: a deliberative, stimulus-driven mode emphasizing accuracy and sacrificing speed, and a guessing mode characterized by random and relatively faster choices.
However, our approach for estimating parameter trajectories reveals a more intricate scenario, challenging the assumed binary operational shift. Contrary to expectations, individuals manifest more than two discernible states. At times, they exhibit an extreme adaptation to a change in condition, while at other times, they display little or no reaction to the altered condition. This complexity underscores the necessity for more flexible transition models, as employed in our study. Failing to utilize such adaptive models could potentially obscure the complex unfolding of individuals' cognition and behavior over time.


Model comparison
When implementing non-stationary models, a modeler encounters a myriad of options, ranging from various transition models to decisions about which parameter follows which transition model. In this study, we limited our choices to a small subset of the possibility space. Based on our experimental manipulations we anticipated that the DDM parameters, particularly the threshold parameter, would not only undergo gradual changes, but also manifest more abrupt shifts in response to changing conditions. Consequently, we tested different implementations accommodating such shifts (mixture random walk, Lévy flight, regime switching) against a transition model that does not, namely, the simple Gaussian random walk. The inferred posterior model probabilities (PMPs) and Bayes factors (BFs) consistently favored the Lévy flight and occasionally the mixture random walk transition models. However, in terms of the absolute goodness-of-fit, as assessed through posterior re-simulations, the performance of all four NSDDMs showed remarkable similarity. This leads to two notable conclusions. First, even the models with lower PMPs demonstrated a good fit to the data, likely owing to the inherent flexibility of the superstatistical framework. Second, our Bayesian model comparison method could reliably detect the most favorable model even when the absolute differences were marginal.


Limitations
Psychological research is usually interested in some group or overall estimate of parameters. Thus, it would have been informative to compute and inspect "average" parameter trajectories. Unfortunately, our experiment was designed in a way that the difficulty and the speed-accuracy instruction manipulation was randomized across participants. This made it impossible to average the individual trajectories directly. Instead, we collapsed the estimates by the different experimental conditions and provided an aggregate view across individuals. Although this is certainly a limitation of this study, we argue that the current analysis is sufficient to address our specific research question.
Moreover, despite using many default settings from the BayesFlow software 
(Radev et al., 2023)
, the configuration and training of neural approximators for both parameter inference and model comparison for non-stationary models can still be a challenge. A basic understanding of deep learning principles and simulation-based inference is an essential prerequisite. These requirements may pose obstacles to the adoption of our method, highlighting the necessity for improved software and tutorials addressing these intricacies.


Outlook
Going forward, we see the relevance of our superstatistics framework as twofold. First, superstatistics could become a powerful tool in the methodological toolkit of the researcher interested in temporal changes in cognitive constructs. It is a general framework and provides large flexibility. Thus far, we only used the DDM as a low-level observation model. However, there are many other cognitive process models that could benefit from such an framework. For instance, reinforcement learning model parameters, such as the learning rate or the softmax temperature parameter,likely change over time 
(Li et al., 2023)
. Second, even when the temporal evolution of cognitive parameters is not a central research question, the adoption of non-stationary models may bring advantages over their stationary counterparts . Our analysis of estimated trajectories vividly illustrates discernible changes in parameters. Assuming stationarity would have led to misleading substantive conclusions.
With great flexibility comes a great plethora of choices. In this study, we compared different transition models guided by the contrast between gradual and sudden changes. However, there are more degrees of freedom when implementing superstatistical models, or Bayesian models in general 
(Gelman et al., 2020)
. 
Elsemüller, Olischläger, et al. (2023)
 advocates for the crucial role of sensitivity analysis, illustrating a potent methodology to facilitate informed decisions regarding factors such as the type and shape of prior distributions, neural network architectures, and other pivotal elements. We believe that using such an approach in the context of superstatistics could provide better guidelines for their implementation.
Up to this point, we focused on the estimates of the low-level parameter trajectories. Yet, it is crucial to note that we also obtain posterior distributions for the static high-level parameters. These estimates can also yield valuable insights into individuals' behavior and cognition. Depending on the chosen transition model, these estimates can offer indications of the frequency with which individuals transition between distinct operational states or the variability inherent in their cognitive constructs. Thus, analyzing these high-level parameters could constitute a compelling avenue for future research.


Conclusion
In conclusion, the experimental validation of non-stationary diffusion decision models presented in this study represents a significant step forward in the field of cognitive modeling. Our results provide compelling evidence that the estimated parameter trajectories genuinely reflect tangible changes in the targeted psychological constructs. We hope that our validation opens the door to widespread applications of non-stationary models in future modeling endeavors, offering a more nuanced understanding of cognitive processes across varying time scales.


S2 Appendix. Neural network architectures and training setups
In the following, we outline our implementation of the neural approximators and the training setup used for model comparison and parameter estimation.


Model comparison
For model comparison we trained an ensemble of ten neural approximators. Each approximator consists of a summary network and an inference network. The summary network is a many-to-one transformer architecture for time series encoding 
(Wen et al., 2023)
. The time series transformer has 128 template and 64 summary dimensions. For inference, we use a network that approximates posterior model probabilities (PMPs) as employed in 
Elsemüller, Schnuerch, et al. (2023)
.
We performed offline training for each of the ten neural approximators separately. The training data consisted of 25 000 simulations per model. Training was performed with 25 epochs and a batch size of 16 starting with an initial learning rate of 0.0005. The learning rate was adjusted with a cosine decay from its initial value to 0.


Parameter estimation
For parameter estimation we trained one neural approximator for each of the four NSDDM implementations. Each approximator consists of a hierarchical summary network as employed in 
Elsemüller, Schnuerch, et al. (2023)
 and two inference networks. Three bidirectional long-short term memory (LSTM) networks were used for the hierarchical summary network. The number of hidden units were 512, 256, and 128 respectively.
For inference, we use a composition of two invertible neural networks , one for the low-level and one for the high-level parameters. The network for the low-level parameters has 8 coupling layers with an interleaved affine and spline internal coupling design. The network for the high-level parameters only differs from the former in its number of coupling layers which is 6.
Since our simulators can be run fast, the training of the four neural approximators was performed online, with 75 epochs, 1 000 iterations per epoch, and a batch size of 16. Thus, each approximator was trained on N = 1 200 000 simulated data sets. The initial learning rate was set to 0.0005 and was reduced with a cosine decay function to 0.


VALIDATION AND COMPARISON OF NON-STATIONARY DIFFUSION DECISION MODELS


S3 Appendix. Individual analyses
The following section shows the individual specific posterior re-simulations and parameter estimates for each difficulty level and both conditions separately. The visualizations are constructed in the vain of 
Figure 6
 in the main text.      
Figure 13
: Aggregate results from all models fitted to the data from participant 5. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.    
Figure 17
: Aggregate results from all models fitted to the data from participant 9. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.       From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Participant 3 Mixture random walk DDM 
Figure 25
: Model fit to response time (RT) time series. The empirical RT time series of participant 3 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied 
.   1  96  192  288  384  480  576  672  768   Trial   1   2   3   4
 Response time (s) Participant 7 Mixture random walk DDM 
Figure 28
: Model fit to response time (RT) time series. The empirical RT time series of participant 7 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Response time (s) Participant 10 Lévy Flight DDM 
Figure 31
: Model fit to response time (RT) time series. The empirical RT time series of participant 10 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Participant 12 Mixture random walk DDM 
Figure 32
: Model fit to response time (RT) time series. The empirical RT time series of participant 7 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.


S5 Appendix. Parameter trajectories
In the following, we present the inferred parameter trajectories for the remaining participants. For each visualisation the model with the highest posterior model probability for that specific individual was used.  
Figure 35
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 1 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 36
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 2 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 37
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 3 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 38
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 4 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 39
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 5 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 40
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 7 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 41
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 8 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 42
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 9 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 43
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 10 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 44
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 12 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 45
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 13 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  
Figure 46
: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 14 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.
Figure 5 :
5
Empirical model comparison results. A Aggregate posterior model probabilities (PMP) across the ensemble and the 14 individual participants. Points depict the mean, stars the median, and the error bars indicate the 75% credibility interval (CI). B Heatmap of average log 10 Bayes factors (BF). Both metrics agree on favoring the Lévy flight DDM over the other models.


Figure 6 :
6
Aggregated results from all models fitted to the empirical data. The top row illustrates posterior resimulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated RTs for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individuals and re-simulations.


Figure 9 :Figure 10 :Figure 11 :
91011
Aggregate results from all models fitted to the data from participant 1. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 2. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 3. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 12 :
12
Aggregate results from all models fitted to the data from participant 4. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 14 :
14
Aggregate results from all models fitted to the data from participant 6. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 15 :Figure 16 :
1516
Aggregate results from all models fitted to the data from participant 7. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 8. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 18 :
18
Aggregate results from all models fitted to the data from participant 10. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 19 :Figure 20 :
1920
Aggregate results from all models fitted to the data from participant 11. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 12. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


Figure 21 :Figure 22 :
2122
Aggregate results from all models fitted to the data from participant 13. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 14. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.


S4Figure 23 :
23
Appendix. Response time time seriesIn the following, we present the model fit to the whole response time time series for the remaining 12 participants. Model fit to response time (RT) time series. The empirical RT time series of participant 1 is shown in black.


Figure 24 :
24
Model fit to response time (RT) time series. The empirical RT time series of participant 2 is shown in black.


Figure 26 :
26
Model fit to response time (RT) time series. The empirical RT time series of participant 4 is shown in black.


Figure 27 :
27
Model fit to response time (RT) time series. The empirical RT time series of participant 5 is shown in black.


Figure 29 :
29
Model fit to response time (RT) time series. The empirical RT time series of participant 8 is shown in black.


Figure 30 :
30
Model fit to response time (RT) time series. The empirical RT time series of participant 9 is shown in black.


Figure 33 :
33
Model fit to response time (RT) time series. The empirical RT time series of participant 13 is shown in black.


Figure 34 :
34
Model fit to response time (RT) time series. The empirical RT time series of participant 14 is shown in black.


 Note, that Schumacher et al. (2023)  focused exclusively on the filtering distribution in their benchmarking experiments.








Acknowledgments
We thank Steffen Ernst for his efforts in programming the experiment and collecting the data. L.S., M.S., and A.V. were supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; grant number GRK 2277 "Statistical Modeling in Psychology").






Author contributions
L.S. and M.S conceived the initial idea and the experimental design. L.S. and S.T.R. created and applied the models, and wrote the initial draft of the manuscript. A.V. supervised the project. All authors reviewed and refined the initial draft of the manuscript and agreed to its current version.


Data and Code Availability
All models, data, and scripts for reproducing the results of this work are publicly available in the project's GitHub repository https://github.com/bayesflow-org/Non-Stationary-DDM-Validation. The neural superstatistics method is implemented in the BayesFlow Python library for amortized Bayesian workflows 
(Radev et al., 2023)
.


Appendix S1 Appendix. Prior distributions
In the following we list the prior distributions we used for all four NSDDM's.


DDM Starting Values
For the starting values of the parameter trajectories we used half-normal distributions with a mean µ and a standard deviation σ denoted as HN (µ, σ):
Half-normal distributions were used for the standard deviations of the Gaussian random walk transition model:
We decided to use a relatively narrower prior on σ τ because the non-decision time parameter is not expected to fluctuate as heavily as the other two parameters.


Mixture Random Walk Transition Model
The mixture random walk transition model used the same prior for the Gaussian random walk as described above. Additionally, Uniform distributions denoted as U were used for the mixture proportion parameter ρ:
The Levy flight transition model uses an alpha stable distribution instead of a Gaussian distribution for the transition. We used the same priors for the standard deviations as in the random walk and the mixture random walk. The alpha stable distribution has an additional parameter α, which determines the fatness of the tails. This parameter is bound between 1 and 2. Therefore, we used a Beta distribution denoted as B and added 1 to the sampled values:


Regime Switching Transition Model
The same prior distributions as for the mixture random walk were used for the mixture probabilities of the regime switching transition model.
 












L
Ardizzone






J
Kruse






S
Wirkert






D
Rahner






E
W
Pellegrini






R
S
Klessen






L
Maier-Hein






C
Rother






U
Köthe




arXiv:1808.04730


Analyzing inverse problems with invertible neural networks










arXiv preprint








Empirical validation of the diffusion model for recognition memory and a comparison of parameter-estimation methods




N
R
Arnold






A
Bröder






U
J
Bayen








Psychological Research




79


5


















C
Beck






E
G D
Cohen




Superstatistics. Physica A: Statistical Mechanics and its Applications






322
















C
Beck




Superstatistics: Theory and applications. Continuum mechanics and thermodynamics






16














On the proper treatment of dynamics in cognitive science




R
D
Beer








Topics in cognitive science
















The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote








Cognitive Psychology




57


3
















An integrated model of choices and response times in absolute identification




S
D
Brown






A
A J
Marley






C
Donkin






A
Heathcote








Psychological Review




115


2
















Multiple timescales of learning indicated by changes in evidence-accumulation processes during perceptual decision-making. npj Science of Learning




A
Cochrane






C
R
Sims






V
R
Bejjanki






C
S
Green






D
Bavelier








8














Hierarchical Bayes Models for Response Time Data




P
F
Craigmile






M
Peruggia






T
Van Zandt








Psychometrika




75


4
















The frontier of simulation-based inference




K
Cranmer






J
Brehmer






G
Louppe








Proceedings of the National Academy of Sciences


the National Academy of Sciences






117
















L
Elsemüller






H
Olischläger






M
Schmitt






P.-C
Bürkner






U
Köthe






S
T
Radev




Sensitivity-Aware Amortized Bayesian Inference
















A deep learning method for comparing bayesian hierarchical models




L
Elsemüller






M
Schnuerch






P.-C
Bürkner






S
T
Radev


















Models of sustained attention. Current opinion in psychology




M
Esterman






D
Rothlein








29














People adopt optimal policies in simple decision-making, after practice and guidance




N
J
Evans






S
D
Brown








Psychonomic Bulletin & Review




24


2
















Refining the law of practice




N
J
Evans






S
D
Brown






D
J K
Mewhort






A
Heathcote








Psychological Review




125


4
















When humans behave like monkeys: Feedback delays and extensive practice increase the efficiency of speeded decisions




N
J
Evans






G
E
Hawkins








Cognition




184
















A reinforcement learning diffusion decision model for value-based decisions




L
Fontanesi






S
Gluth






M
S
Spektor






J
Rieskamp








Psychonomic Bulletin & Review




26


4
















Striatum and pre-SMA facilitate decision-making under time pressure




B
U
Forstmann






G
Dutilh






S
Brown






J
Neumann






D
Y
Von Cramon






K
R
Ridderinkhof






E.-J
Wagenmakers








Proceedings of the National Academy of Sciences




105


45
















Capturing the non-stationarity of whole-brain dynamics underlying human brain states




J
A
Galadí






S
Silva Pereira






Y
Sanz Perl






M
L
Kringelbach






I
Gayte






H
Laufs






E
Tagliazucchi






J
A
Langa






G
Deco








NeuroImage




118551
















A
Gelman






A
Vehtari






D
Simpson






C
C
Margossian






B
Carpenter






Y
Yao






L
Kennedy






J
Gabry






P.-C
Bürkner






M
Modrák




arXiv:2011.01808


Bayesian workflow










arXiv preprint








Learning to forget: Continual prediction with lstm




F
A
Gers






J
Schmidhuber






F
Cummins








Neural computation




12


10
















Training deep neural density estimators to identify mechanistic models of neural dynamics




P
J
Gonçalves






J.-M
Lueckmann






M
Deistler






M
Nonnenmacher






K
Öcal






G
Bassetto






C
Chintaluri






W
F
Podlaski






S
A
Haddad






T
P
Vogels








Elife


9


56261












Automatic posterior transformation for likelihood-free inference




D
Greenberg






M
Nonnenmacher






J
Macke








International Conference on Machine Learning


















A tutorial on bridge sampling




Q
F
Gronau






A
Sarafoglou






D
Matzke






A
Ly






U
Boehm






M
Marsman






D
S
Leslie






J
J
Forster






E.-J
Wagenmakers






H
Steingroever








Journal of mathematical psychology




81
















Time-evolving psychological processes over repeated decisions




D
Gunawan






G
E
Hawkins






R
Kohn






M.-N
Tran






S
D
Brown








Psychological review




129


3


438














Is memory search governed by universal principles or idiosyncratic strategies




M
K
Healey






M
J
Kahana








Journal of Experimental Psychology: General




143


2
















A four-component model of age-related memory change




M
K
Healey






M
J
Kahana








Psychological Review




123


1
















A review of applications of the Bayes factor in psychological research




D
W
Heck






U
Boehm






F
Böing-Messing






P.-C
Bürkner






K
Derks






Z
Dienes






Q
Fu






X
Gu






D
Karimova






H
A L
Kiers






I
Klugkist






R
M
Kuiper






M
D
Lee






R
Leenders






H
J
Leplaa






M
Linde






A
Ly






M
Meijerink-Bosman






M
Moerbeek






.
.
Hoijtink






H








Psychological Methods




28


3
















The variability puzzle in human memory




M
J
Kahana






E
V
Aggarwal






T
D
Phan








Journal of Experimental Psychology: Learning, Memory, and Cognition




44


12
















Bayes Factors




R
E
Kass






A
E
Raftery








Journal of the American Statistical Association




90


430
















Hidden Markov Models of Evidence Accumulation in Speeded Decision Tasks




Š
Kucharský






N.-H
Tran






K
Veldkamp






M
Raijmakers






I
Visser








Computational Brain & Behavior




4


4
















Speed-accuracy manipulations and diffusion modeling: Lack of discriminant validity of the manipulation or of the parameter estimates?




V
Lerche






A
Voss








Behavior Research Methods




50


6
















Experimental validation of the diffusion model based on a slow response time paradigm




V
Lerche






A
Voss








Psychological Research




83


6
















Dynamic noise estimation: A generalized method for modeling noise in sequential decision-making behavior




J.-J
Li






C
Shi






L
Li






A
Collins












bioRxiv








Information theory, inference and learning algorithms




D
J
Mackay








Cambridge university press












Bayesian model selection for complex dynamic systems




C
Mark






C
Metzner






L
Lautscham






P
L
Strissel






R
Strick






B
Fabry








Nature Communications




9


1


1803














Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning




S
D
Mcdougle






A
G E
Collins








Psychonomic Bulletin & Review




28


1


















A
Melanson






J
F
Mejias






J
J
Jun






L
Maler






A
Longtin




Nonstationary Stochastic Dynamics Underlie Spontaneous Transitions between Active and Inactive Behavioral States






4












A new model of decision processing in instrumental learning tasks




S
Miletić






R
J
Boag






A
C
Trutti






N
Stevenson






B
U
Forstmann






A
Heathcote




V. Wyart, J. I. Gold, & J. W. de Gee






63055












Modeling the dynamics of recognition memory testing with an integrated model of retrieval and decision making




A
F
Osth






A
Jansson






S
Dennis






A
Heathcote








Cognitive Psychology




104
















Normalizing flows for probabilistic modeling and inference




G
Papamakarios






E
Nalisnick






D
J
Rezende






S
Mohamed






B
Lakshminarayanan








The Journal of Machine Learning Research




22


1
















Amortized Bayesian Model Comparison With Evidential Deep Learning




S
T
Radev






M
D'alessandro






U
K
Mertens






A
Voss






U
Köthe






P.-C
Bürkner








IEEE Transactions on Neural Networks and Learning Systems




34


8
















Bayesflow: Learning complex stochastic models with invertible neural networks




S
T
Radev






U
K
Mertens






A
Voss






L
Ardizzone






U
Köthe








IEEE transactions on neural networks and learning systems






33














Bayesflow: Amortized bayesian workflows with neural networks




S
T
Radev






M
Schmitt






L
Schumacher






L
Elsemüller






V
Pratz






Y
Schälte






U
Köthe






P.-C
Bürkner








Journal of Open Source Software




8


89


5702














A theory of memory retrieval




R
Ratcliff








Psychological Review




85


2
















The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks




R
Ratcliff






G
Mckoon








Neural computation




20


4
















Modeling Response Times for Two-Choice Decisions




R
Ratcliff






J
N
Rouder








Psychological Science




9


5
















Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability




R
Ratcliff






F
Tuerlinckx








Psychonomic Bulletin & Review




9


3
















Diffusion model for one-choice reaction-time tasks and the cognitive effects of sleep deprivation




R
Ratcliff






H
P A
Van Dongen








Proceedings of the National Academy of Sciences




108


27
















Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison




T
Säilynoja






P.-C
Bürkner






A
Vehtari








Statistics and Computing




32


2


32














Bayesian Filtering and Smoothing




S
Särkkä








Cambridge University Press












Toward a principled bayesian workflow in cognitive science




D
J
Schad






M
Betancourt






S
Vasishth








Psychological methods




26


1


103














Neural superstatistics for Bayesian estimation of dynamic cognitive models




L
Schumacher






P.-C
Bürkner






A
Voss






U
Köthe






S
T
Radev








Scientific Reports




13


1


13778














Duration discrimination: A diffusion decision modeling approach. Attention, Perception, & Psychophysics




L
Schumacher






A
Voss








85














A time-varying dynamic partial credit model to analyze polytomous and multivariate time series data




Sebastian
Castro-Alvarez






R
R M
Laura






F
Bringmann






J
N
Tendeiro








Multivariate Behavioral Research




0


0
















Validating Bayesian Inference Algorithms with Simulation-Based Calibration




S
Talts






M
Betancourt






D
Simpson






A
Vehtari






A
Gelman


















A review of issues about null hypothesis Bayesian testing




J
N
Tendeiro






H
A L
Kiers








Psychological Methods




24


6
















Self-organization of cognitive performance




G
C
Van Orden






J
G
Holden






M
T
Turvey








Journal of Experimental Psychology: General




132


3
















Advantages masquerading as "issues




D
Van Ravenzwaaij






E.-J
Wagenmakers








Bayesian hypothesis testing: A commentary on Tendeiro and Kiers






27














The Leaky Integrating Threshold and its impact on evidence accumulation models of choice response time (RT)




S
Verdonck






T
Loossens






M
G
Philiastides








Psychological Review




128


2
















Diffusion models in experimental psychology




A
Voss






M
Nagler






V
Lerche








Experimental psychology
















Interpreting the parameters of the diffusion model: An empirical validation




A
Voss






K
Rothermund






J
Voss








Memory & Cognition




32


7
















Estimation and interpretation of 1/f α noise in human cognition




E.-J
Wagenmakers






S
Farrell






R
Ratcliff








Psychonomic Bulletin & Review




11


4
















A diffusion model account of criterion shifts in the lexical decision task




E.-J
Wagenmakers






R
Ratcliff






P
Gomez






G
Mckoon








Journal of Memory and Language




58


1
















Computational cognitive modeling of the temporal dynamics of fatigue from sleep loss




M
M
Walsh






G
Gunzelmann






H
P A
Van Dongen








Psychonomic Bulletin & Review




24


6
















The neural bases of momentary lapses in attention




D
H
Weissman






K
Roberts






K
Visscher






M
Woldorff








Nature neuroscience




9


7


















Q
Wen






T
Zhou






C
Zhang






W
Chen






Z
Ma






J
Yan






L
Sun




Transformers in Time Series: A Survey
















Abrupt strategy change underlies gradual performance change: Bayesian hierarchical models of component and aggregate strategy use




S
K A
Wynton






J
Anglim








Journal of Experimental Psychology: Learning, Memory, and Cognition




43


10

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]