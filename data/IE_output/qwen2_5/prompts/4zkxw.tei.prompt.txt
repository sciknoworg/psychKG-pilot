You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Misinformation -information that is incorrect or misleading -is a growing concern in politics, science, and public health 
[1]
[2]
[3]
. Significantly, misinformation has been shown to affect peoples' voting intentions, belief in anthropogenic climate change, and perception of vaccine efficacy 
[4]
[5]
[6]
. Understanding the discrepancy between how we form beliefs in the face of misinformation and how we optimally should, is therefore of key importance to a range of disciplines.
Within a normative probabilistic framework, information should be weighted according to its reliability, with more accurate information producing a stronger effect on post hoc beliefs 
[7]
. For example, when reading reviews about a product online, we may intuitively assign a "trustworthiness" score of the different websites we consult. If we suspect one website to have low reliability (e.g., due to its poor reputation), we should assign it a relatively smaller weight to the evidence it provides about the product when making our choice. To maximize choice accuracy, information with no predictive validity should therefore be ignored, while misinformation that is reliably incorrect should be encoded as having below chance reliability and therefore treated as evidence against the conclusion it promotes 
[8]
.
Previous research, however, has shown that we often fail to filter out unreliable information, allowing it instead to bias the decision process 
[9,
10]
. Indeed, mere exposure to a stimulus can affect subsequent decision-making 
[11]
[12]
[13]
. Similarly, misinformation might be encoded as being reliable, even when we have the prior knowledge required to refute the misleading claims 
[14]
. To reduce the impact of misinformation, many prominent online platforms, including Twitter, Facebook, and Google, have implemented warning messages whereby misinformation is explicitly labelled 
[15]
[16]
[17]
. This proactive approach has been shown to diminish beliefs in misinformation 
[18,
19]
, though this effect is only partial 
[20]
.
However, in naturalistic settings, we often have the opportunity to improve our understanding of what is the optimal choice by sampling more information 
[21]
. Given that information-seeking behaviour has been shown to increase with uncertainty 
[22]
[23]
[24]
[25]
, we may expect information-seeking to be upregulated in response to information we know is unreliable. Increases in our appetite for information could mitigate the effects of misinformation -enabling our exposure to other, more accurate information.
While valuable inroads have been made to study misinformation in realworld (or realistic) scenarios, these studies are limited in their ability to reveal the key cognitive principles underlying misinformation processing. The reason is the inherent complexity of all realistic situations in which people encounter misinformation. Here, used a novel experimental approach explicitly designed to isolate how information seeking behaviour is impacted by the presence of misinformation when no other subjective factors (emotional state, attitudes), cognitive component (load, perceptual feature), prior beliefs (knowledge, opinions) or decision context (such as information platforms, modes of interaction, identity of sources) could play a role. Specifically, we aimed to understand how information labelled explicitly as unreliable and misleading affects decision-making and information-seeking behaviour using three experimental paradigms with independent samples. Within each paradigm, participants observed predictions pertaining to which of two colours would win an upcoming binary lottery. These predictions were made by a series of "information sources" that predicted the outcome of the lottery. The reliability of each source was explicitly shown to participants as a percentage indicating the probability that each of its predictions are correct. We then probed their desire for more information by offering them the chance to bid small amounts of money to view the prediction of one additional information source of unknown (but above-chance) reliability. Finally, participants chose which colour they believed would win the upcoming lottery. By providing explicit percentage scores indicating the reliability of each source, we were able to emulate and control the trustworthiness that we implicitly assign to information sources 
[26]
. This removal of real-world contextual factors allowed us to examine information preferences in the absence of ideological or experiential prior beliefs, which can be difficult to precisely estimate or control for in ecological settings.
In experiment 1, which served as the baseline, the predictions of all sources were reliably right (RR; >50% reliability). In experiment 2, the exact same sources were shown to keep the information content the same, but we added an additional source to the end of each sequence whose prediction was unreliable (UR; 50% reliable) and therefore uninformative. In experiment 3, we again used the same sequences of sources as in experiment 1; however, one of the sources was inverted to predict the opposite colour, but with the corresponding belowchance reliability, such that the information it showed was reliably wrong (RW; <50% reliability). This means information content was again identical, but the RW source required participants to reinterpret its prediction as evidence in favour of the opposite colour.
We predicted that, in general, choice accuracy (i.e., the probability of choosing the colour predicted by the more reliable sources) would be a function of the difference in the win probability of sources predicting one colour over the other. Crucially, for an ideal observer able to take into account information reliability optimally, the presence of unreliable or misinformation should in principle have no impact on the decision because the information content remains identical across conditions. This design therefore allows us to directly test whether, paradoxically, the presence of either the UR information source and the RW information source nevertheless led to lower choice accuracy. Such a result would mean that the mere presence of unreliable information impacts choice even when explicitly labelled as such. Such finding would support the idea that misinformation impacts choice behavior beyond its actual content, and that it persists irrespective of explicit labelling, shedding new light on this important societal question.
In case that there was an effect of misinformation on choice accuracy, we further hypothesized that participants would be willing to pay more for additional information when presented with an unreliable information source, to counteract any perception of increased uncertainty. More specifically, given that the prediction of the UR source could be congruent or incongruent with the consensus of the reliable information sources, we hypothesized that information-seeking would increase when seemingly conflicting information was provided. Similarly, we expected that the presence of RW sources would increase information-seeking because they might lead to a subjective perception of increased uncertainty.
Finally, in an additional set of three experiments, participants were again presented with a series of information sources that were either all reliably right (experiment 4) or contained either one unreliable source (experiment 5), or one reliably wrong source (experiment 6). In all three experiments, participants could not seek additional information, but instead immediately chose which colour they believed would win the lottery, then rated their confidence in their choice. This set of experiments served to understand whether choice accuracy and information seeking behaviour in the first three experiments could indeed be explained by changes in subjective confidence when presented with UR or RW information.


Results
Within each trial of the first three experiments, participants were always first provided with six pieces of information concerning which of two colours would win a lottery. Each piece of information took the form of a prediction of the outcome from a "source" (rectangle of the corresponding colour) alongside a percentage corresponding to how reliable that source was. In the Reliably Right experiment, each of these information sources had positive but relatively low reliability (between 51% and 55%) to avoid runaway sequences (see Methods for details, and below for modifications in the other experiments). Sequences of information were designed such that across the six sources, half predicted pink would win, while the other half predicted orange would win. We derived sequences such that the probability of one colour winning was either 0, 4, 8, or 12% higher than the other (see Methods for details). The colour with the higher probability of winning is henceforth termed the "winning" colour, while the colour with the lower probability of winning is termed the "losing" colour. After being shown the first six sources, participants were then given the opportunity to view the prediction of one extra information source of unknown reliability. To do so, participants placed a bid corresponding to the maximum cost they would be willing to pay in order to see the extra information (BDM auction 
[27]
) -between 0 cents and 5 cents in 1-cent increments. A random, hidden "price" of between 1 and 5 cents was then generated by the computer. If the participant's bid was equal to or greater than that price, they paid that price and were shown the extra information. Otherwise, no cost was incurred, and they were instead shown a blank screen for the equivalent length of time. After the extra information was received or denied, participants chose which colour they believed was more likely to win that lottery. Correct choices were rewarded with 15 cents and added to participants' earnings, though participants were not shown the outcome of each lottery (See 
Figure 1
 for an example trial). 
Figure 1
: Information-seeking paradigm. A) Participants were presented with a series of information sources, each predicting a colour. The reliability of each piece of information was displayed as a percentage value corresponding to the probability that the winning colour would correspond to the colour of the square. In the Reliably Right (RR) experiment (i) all sources had a reliability greater than 50%. In the Reliably Wrong (RW) experiment (ii) one of the sources was "inverted" so that it predicted the opposite colour, but with equivalent, below 50% reliability. Finally, in the Unreliable Information (UR) experiment (iii) the six reliable sources were followed by an unreliable information source with 50% reliability. After viewing the predictions, participants placed a bid corresponding to the maximum they would pay (between 0 and 5 cents) in order to view an additional information source. If their bid was higher than a randomly-generated "price", they paid that price and were shown an extra information source. Otherwise, they were shown a blank prediction for the equivalent amount of time. Finally, participants were required to choose which colour they believed would win the lottery, with a 15-cent reward for correct choices. B) Cumulatively, the predictions of the sources suggested that the win-probability of one colour was either 0, 4, 8, or 12% higher than the other.
2.1 Are choice behaviour and information-seeking modulated by source reliability?


Reliably Right (RR) Information Experiment
In this experiment, which served as the baseline condition, no modifications to the paradigm were made. We assessed how participants' choices and desire for additional information were related to the difference in win-probability for each colour as predicted by the six information sources. A one-way repeated-measures ANOVA showed significant differences in choice accuracy (i.e., frequency of choosing the winning colour) between winprobability difference levels F (2, 330) = 109.81, p < .001, η 2 = .107, indicating that choice accuracy indeed increased with the difference in win-probability between colours 
(Figure 2A
). All post hoc t-tests with Bonferonni-Holm correction for multiple comparisons are reported in 
Supplementary Table A1
.
To assess whether participants' desire for more information depended on the relative strength of the information they possessed, we computed how bid magnitude was influenced by the difference in win-probability between colours. Results of a one-way repeated-measures ANOVA showed significant differences, F (3, 495) = 48.59, p < .001, η 2 = .041, reflecting that participants bid more for information when the difference in win-probability was smaller ( 
Figure 2B
; see 
Supplementary Table A2
 for all Bonferonni-Holm corrected post hoc comparisons).


Unreliable Information (UR) Experiment
Identical sequences were used as in the Reliably Right experiment, with the only difference being that the first six sources were immediately followed by a seventh source that had a reliability of 50% (i.e., was fully unreliable; UR). The UR source could either be congruent with the consensus of the reliable sources (predicting the "winning" colour) or could be incongruent with that consensus (predicting the "losing" colour). As a 50% reliable source has no predictive power and is therefore non-informative, the objective information content was identical to the Reliably Right experiment.
First, we replicated the effect of difference in the win-probability of each colour on choice accuracy 
(Figure 2A)
. A one-way repeated-measures ANOVA revealed significant differences in accuracy between the win-probability difference levels, F (2, 322) = 56.67, p < .001, η 2 = .093, again reflecting less accurate choices for smaller differences (see 
Supplementary Table A3
 for all Bonferonni-Holm corrected post hoc comparisons). To further examine whether choice accuracy differed dependent on whether the UR information source challenged the consensus of the other information sources, we conducted a within-subjects t-test. As trials with a win-probability difference of 0 do not have a "winning" colour, for these analyses, only trials in which the win-probability difference was greater than 0 were included. Contrary to expectations, we found that choice accuracy did not differ depending on whether the UR information source predicted the winning or losing colour, t(164) = 0.77, p = .44 ( 
Figure 2C
).
We also replicated the effect of information strength on information seeking, showing significant effect in a one-way repeated-measures ANOVA, F (3, 483) = 19.88, p < .001, η 2 = .009, indicating that bid magnitude again decreased as difference in win probability increased ( 
Figure 2B
; see 
Supplementary Table  A4
 for all post-hoc t-tests with Bonferonni-Holm correction). Averaging across evidence levels, bid size did not differ depending on whether the UR information source predicted the winning colour or the losing colour, t(164) = 0.68, p = .50, contrary to our hypothesis ( 
Figure 2E
).


Reliably Wrong (RW) Information Experiment
This was a direct replication of the Reliably Right experiment, with one change: within the first six sources, one source would be "inverted" such that it would now predict the opposite colour, but with a reliability level below 50%. For example, a source that previously predicted orange with 53% reliability would now predict pink with 47% reliability (see 
Figure 1A
). This meant that each sequence contained two sources predicting one colour and four predicting the other. However, as the lottery outcome was binary, the information communicated by the sources was objectively equivalent to that in the Reliably Right experiment.
We again replicated the effect of the difference in the win-probability of the two colours on choice accuracy, F (2, 328) = 100.22, p < .001, η 2 = .120. A higher accuracy was found with greater difference between the colours ( 
Figure  2A
; post hoc t-tests with Bonferonni-Holm correction are reported in Supplementary 
Table A5
). We also tested whether choice accuracy differed based on whether the inverted source supported the winning or losing colour, finding no significant difference, t(161) = 1.59, p = .11 ( 
Figure 2D
). This suggests that the inverted source was correctly interpreted as evidence in favour of the opposite colour to its prediction. To confirm this, we further constructed two general linear models. In the first model, choice behaviour was predicted from the difference in win-probability between the two colours (Model 1: Choice ∼ totalDiff). In the second model, this predictor was divided such that the effects of the predictions of the reliably wrong, inverted source (invertedDiff) and the other, reliably correct sources (otherDiff) were estimated with different coefficients (Model 2: Choice ∼ invertedDiff + otherDiff). Model fit statistics indicated that separating the reliably wrong information source did not improve the fit of the model, as the simpler Model 1 (AIC = 10335, BIC = 10349) provided a better fit than the more complex Model 2 (AIC = 10337, BIC = 10358), confirming that the RW information was indeed interpreted correctly.
We then again replicated the effect of information source reliability on information seeking, F (3, 492) = 27.77, p < .001, η 2 = .019. ( 
Figure 2B
), reflecting more information-seeking as the likelihood of each colour winning approached 50% (see 
Supplementary Table A6
 for all comparisons). Bid size was again unaffected by which colour the RW information source predicted t(161) = 0.22, p = .82 ( 
Figure 2F
). In summary, the general effect of more accurate choices and less desire for information with increasing differences in the win-likelihood of each colour could be replicated across all three experiments. Participants' choices and information-seeking behaviour were not biased by which colour the UR or RW information sources predicted to win. However, this finding alone does not provide any evidence against the hypothesis that the presence of UR and RW sources still affected these behaviours. To address this, we next conducted crucial comparisons of choice behaviour and information-seeking between experiments. 


Are choices different in the presence of unreliable or reliably wrong information?
For these analyses, we adopted a hierarchical Bayesian framework in which trial-wise choices were modelled (see Methods for model specification). To predict choice behaviour, we used a subject-level main effect of the difference in win-probability and group-level effects for both the main effect of experimental condition and the interaction between condition and win-probability difference. We constructed separate models to compare both the UR information experiment and the RW information experiment with the RR experiment. Parameter estimates for both models revealed main effects of both winprobability difference and experimental condition, with no significant interaction ( 
Figure 3
; left panels). These results confirm that accuracy increased as the difference in probability of the two colours to win increased. They further indicate that choice accuracy was significantly lower in both the UR and RW experiments compared to the RR experiment. Consequently, despite paying more for extra information, participants in the Reliably Right experiment earned more on average per trial (M = 9.64 c, SE = 0.06 c) than those in the Reliably Wrong (M = 8.08 cents, SE = 0.07 c) or Unreliable experiment (M = 9.01 c, SE = 0.06 c). Posterior predictive checks of both models indicated a close fit to the data (Supplementary 
Figure A1
). The two columns on the left show parameter values obtained from the modelling of choice behaviour, while the right two columns show parameters obtained from modelling bid size. Shown are the hyperparameter corresponding to the mean effect of win-probability difference (top panels), parameter corresponding to the effect of the experimental condition (middle panels) and parameter corresponding to the interaction between percentage difference in win-probability and condition (bottom panels). Intervals below each density plot indicate the 95% highest density interval.
2.3 Does information-seeking differ in the presence of unreliable or reliably wrong information? 
Figure 2B
 indicates that, contrary to expectations, participants in the Unreliable and Reliably Wrong experiments bid less for additional information than those in the Reliably Right experiment, indicating less desire to acquire more information, despite poorer choices. To test this statistically, we adopted a hierarchical Bayesian analysis framework in which we modelled trial-wise bid sizing using a cumulative probit model, equivalent to mixed-effects ordinal regression (see Methods for model specification). In this model, we predicted trial-wise bid size from a participant-level main effect of the absolute difference in win-probability and group-level effects of experimental condition and interaction between condition and win-probability. This approach allowed us to limit the response options to the six possible bid sizes 
[28]
. As before, we constructed two models, one comparing the UR experiment to the RR experiment, and one comparing the RW experiment to the RR experiment. Parameter estimates for both models revealed main effects of both win-probability difference and experimental condition ( 
Figure 3
; right panels). For both models, significant, negative estimates for the hyperparameters governing the means of the win-probability parameters confirmed that bid size decreased as the strength of evidence increased. Further, significant, negative estimates for the condition parameters show that bid size was larger in the RR experiment than in either of the UR and RW experiments. There was a significant positive interaction between the two predictors for the RW experiment, though the 95% highest density interval (HDI) contained 0 for the UR experiment. This suggests that increasing the difference in cumulative source reliability affected bid size more strongly in the RR experiment than in the RW experiment. Posterior predictive checks of both models indicated a close fit to the data (see Supplementary 
Figure A2
). In summary, the comparisons between experiments showed that in the presence of an unreliable or reliably wrong source, despite no change in the objective information content, choice behaviour was poorer compared to when these sources were not present. Despite this, and despite that obtaining additional information always improved participants' knowledge, they nevertheless opted to bid less for additional information.
We initially hypothesised that, if the presence of misinformation had an effect on participants' choice accuracy, this could be due to a general mistrust in the information environment, which in turn should be reflected in lower subjective confidence in making a good decision. Finding that participants also sampled less information might either suggest that this mistrust extended to unseen sources of the same decision environment (i.e. the additionally available source), or that paradoxically, participants might have become blind to their low decision accuracy and overconfident in the information they already processed. In other words, after deciphering the true direction of these sources' predictions, participants might have had the illusion that they knew more than in the baseline condition and therefore opted to bid less for additional information, not noticing that their decision performance was worse. If this were true, then participants should have higher confidence in their decisions in the UR and RW experiments than their RR experiment counterparts. The next two experiments were designed to test these hypotheses. Finding lower decision confidence after viewing the initial sources' predictions in the presence of unreliable and misinformation as compared to no presence of misinformation would support the idea that misinformation impacts the perceived quality of the entire information environment, beyond its information content. However, finding higher confidence after the initial sources' predictions in the presence of unreliable and misinformation would support the idea that deciphering misinformation, while still diminishing choice accuracy, also artificially boosts confidence in one's decision-making.


Confidence Experiments
The structure of the three confidence experiments (RR-Conf, UR-Conf, and RW-Conf) mirrored those of the information-seeking experiments, with two key changes. Instead of bidding for an extra piece of information, participants were required to immediately choose which colour they believed would win the lottery. Following this, they provided a confidence rating corresponding to how likely they believed their choice was to be correct on a continuous scale ranging from 0 to 100.


Replication of choice behaviour
As with the previous experiments, we found that choice accuracy increased with the difference in win-probability between the two colours in the RR-Conf  
Figure 4A
). Post hoc t-tests with Bonferonni-Holm correction for multiple comparisons revealed significant differences in accuracy between all pairs of win-probability difference levels in all three experiments (all p < .001; see Supplementary Tables A7, A8, and A9).
As reported in the information-seeking experiments, whether the unreliable sources in the UR-Conf experiment predicted the winning or losing colour to win did not affect choice behaviour, t(149) = 0.90, p = .37 ( 
Figure 4C)
. Similarly, whether the inverted source in the RW-Conf experiment was a source predicting the winning or losing colour had no effect on choice behaviour, t(149) = 0.29, p = .78 ( 
Figure 4D)
.
As observed in the information-seeking experiments, choice accuracy was higher in the RR-Conf experiment than in either the UR-Conf or the RW-Conf experiment ( 
Figure 4A
). The same Bayesian computational modelling approach revealed main effects of both difference in win-probability and of experimental condition. We found positive effects of win-probability, confirming that, in all experiments, choice accuracy increased as the difference in win-probability increased (see 
Figure 5
 for parameter values). Further, the significant, positive main effects of experimental condition suggested that participants' accuracy was higher in the RR-Conf experiment than in either the UR-Conf or the RW-Conf experiment, replicating the previous findings. Finally, unlike in the information seeking experiment, an interaction effect was also found when comparing choice accuracy in the RR-Conf experiment with both the UR-Conf and the RW-Conf experiment. This effect suggested that the increases in percentage difference produced smaller changes in choice accuracy within the UR-Conf and RW-Conf experiments relative to the RR-Conf experiment. Posterior predictive checks of both models indicated a close fit to the data (see 
Supplementary Figure A3
). 


Do unreliable and reliably wrong information sources boost confidence?
To determine whether participants' confidence in their ability to correctly predict the winning outcome was dependent on the relative strength of the information they possessed, we first conducted repeated measures ANOVAs within experiments with confidence ratings as the dependent variable and the absolute difference in win-probability between colours as a factor. Results showed that confidence increased with the difference in win-probability in the  
Figure 4B
). Follow-up pairwise comparisons revealed significant differences between all pairs of differences within each experiment (all p < .01 after Bonferroni-Holm correction; see 
Supplementary Tables A10, A11
, and A12), indicating that participants' confidence in making the correct decision did indeed grow as the strength of the evidence they possessed increased.
We then tested the hypothesis that the presence of a RW or UR information source might lead to overconfidence in one's ability to accurately predict the lottery outcome. Contrary to this prediction, we observed higher confidence ratings in the RR-Conf experiment compared to the UR-Conf and RW-Conf experiments ( 
Figure 4B
). To formally test this, we applied hierarchical Bayesian computational modelling in which we predicted trial-wise confidence ratings from the absolute difference in win-probability between colours and the experimental condition. We applied this model to both the UR-Conf and RW-Conf experiments, comparing each to the baseline RR-Conf experiment. We used subject-level effects for both the constant or baseline level of confidence and for the effect of the difference in win-probability, while group-level effects were used for the effect of condition and the interaction between condition and the difference in win-probability (see Methods for model specification). Posterior distributions of parameters revealed significant main effects of the difference in win-probability, but not of experimental condition (see 
Figure 5
). However, significant interactions between experimental condition and difference in winprobability were found in both models, indicating that the effect of increasing evidence strength on confidence was attenuated in the UR-Conf and RW-Conf experiments compared to the RR-Conf experiment.
Despite no difference in choice behavior depending on whether the UR source predicted the winning or losing colour, we found that confidence ratings in the UR-Conf experiment were slightly but significantly higher when the UR source predicted the losing colour than when it predicted the winning colour, t(149) = 4.10, p < .001 ( 
Figure 4E)
. Similarly, in the RW-Conf experiment, we found that participants were more confident in their choice when a source predicting the winning colour was inverted to predict the losing colour, compared to when a source predicting the losing colour was inverted to predict the winning colour, t(149) = 2.31, p = .022 ( 
Figure 4F
). This means that, on average, participants were more confident in their ability to choose the correct colour when the reliably wrong source predicted the losing colour to win with below-chance reliability. Both of these findings are surprising given that the predictions of the unreliable and reliably wrong sources did not affect choice accuracy.
However, while the predictions of the unreliable and reliably wrong information sources affected confidence, these effects were small. They are therefore insufficient to explain the overall decrease in confidence found in the UR-Conf and RW-Conf experiments relative to the RR-Conf experiment. Indeed, even when examining only trials in which the UR information source predicted the losing colour, confidence ratings in the RR-Conf experiment were still significantly higher than those in the UR-Conf condition, t(278.60) = 3.34, p < .001 (see 
Supplementary Figure A4
). Similarly, confidence ratings for trials in the RW-Conf experiment in which a source predicting the winning colour was inverted were significantly lower than confidence ratings in the RR-Conf experiment, t(270.01) = 5.26, p < .001 (see 
Supplementary Figure A4
). We therefore conclude that the lower mean confidence levels found in the UR-Conf and RW-Conf experiments are attributable primarily to the presence of the unreliable and reliably wrong sources, rather than the contents of their predictions.
Together, these results suggest that the differences in confidence between experiments were primarily due to a decreased sensitivity to evidence strength in the presence of unreliable and reliably wrong information. In other words, more objective evidence for the winning colour did not increase confidence the same amount in the presence of an unreliable or reliably wrong information source as it did when all sources were reliably correct. Posterior predictive checks indicated a close fit to the data (Supplementary 
Figure A5)
.
In summary, the presence of an unreliable or reliably wrong information source decreased decision confidence, which fits the poorer choice behaviour. This decrease in confidence was manifested through a lowered sensitivity to increasing evidence strength. However, despite being less confident participants in the Unreliable and Reliably Wrong information-seeking experiments forwent the option to obtain additional information. These results are incompatible with the idea of artificially inflated confidence when deciphering misinformation. They rather support the hypothesis that the presence of unreliable information and misinformation reduced trust in the entire information environment, leading to a reduced reliance of available information for decision-making as well as a reduced willingness to sample from this environment.


Discussion
In this study, we investigated the effects of information explicitly labelled as unreliable and misleading on belief formation and information-seeking behaviour. Our novel approach allowed us to discard effects due to complex factors that are always present in real-world scenarios, such as attitudes, prior knowledge, source identity and many others, whose impacts are difficult to assess. Instead, we investigated in isolation how the mere presence of misinformation Across all models, the µ β Dif f parameter corresponds to the hyperparameter governing the mean of the effect of win-probability on the outcome variable. The β Cond parameter corresponds to the effect of the experimental condition, while the β DC parameter indicates the interaction between the difference in win-probability and the experimental condition. Finally, in the models of confidence ratings, the µ β Cons parameter corresponds to the hyperparameter governing the mean constant or baseline confidence rating across participants. Intervals below each density plot indicate the 95% highest density interval. affects basic decision processes. We showed participants a sequence of information sources making predictions about the winning choice of a binary lottery. Crucially, each source was associated with a reliability level, expressed as a percentage corresponding to the probability of predicting the correct choice. Within this stream of information, the information sources could either all be reliable (above 50% chance of being correct; Reliably Right) or include either a fully unreliable information source (50% chance of being correct; Unreliable), or a misleading information source (less than 50% chance of being correct; Reliably Wrong). Critically, the information content within each experimental condition was formally identical, as unreliable information added to the sequence did not change the balance of information, and misleading information was still equally informative about the likely correct response. Therefore, the experimental conditions differed only in how the information was presented to participants, and whether unreliable information that should be ignored, or misleading information which had to be inverted, was present. We then gauged participants' interest in receiving information from an additional source (which was always reliable) using a BDM auction procedure, before they chose which lottery outcome they believed would occur. In three follow-up experiments, we assessed participants' confidence in this choice in each of the experimental conditions.
First, across each of the first three experiments, we found that participants were most interested in receiving more information when the information they had was least deterministic, consistent with previous findings 
[22]
[23]
[24]
[25]
. In contradiction with our predictions however, we found that participants were less likely to pay to view an additional piece of information (that was always helpful) when they were presented with one piece of reliably wrong information before. Choosing to ignore optional information could be linked to an increase in confidence, suggesting that, in some situations, labelling misinformation is sufficient to reduce the subjective sense of uncertainty it invokes 
[29,
30]
. However, if this were true, one would expect participants' choices to be at least equally accurate, which is not what we observed. Instead, participants made less accurate choices when presented with reliably wrong information, consistent with our hypothesis that the reliably wrong source would have a harmful impact on decision-making. Importantly, when replacing the option to seek more information with a confidence assessment, we demonstrated that participants did indeed have lower confidence when presented information that was reliably wrong. This fits with their choice behaviour, given that accuracy and confidence are typically positively correlated 
[31,
32]
. It can therefore be concluded that the decrease in information-seeking did not occur because of an increase of confidence in the existing information, but despite a reduced confidence in this information. This paradoxical combination of lower confidence and decreased information-seeking highlights a deviation from both normative and typical behaviour, by which information-seeking is upregulated in states of increased uncertainty 
[22]
[23]
[24]
[25]
33]
. It suggests that there is a substantial effect of the presence of misinformation on choice behavior that goes beyond the information content communicated, leading participants to rely less on accumulated information (i.e., make less accurate decisions) and also rely less on potentially available information (i.e., bid less for additional information).


Specific effects of unreliable information and misinformation
The results for the Unreliable source experiment were near identical to the Reliably Wrong source experiment, as participants were less likely to sample additional information in both when compared with the Reliably Right experiment. We also showed that participants made more decision errors when presented with unreliable information, even though this information was only added on top of the sequence and therefore did not formally affect the overall certainty about the correct response. Importantly, and maybe even more surprisingly, this decrease in accuracy was not related to whether the unreliable information supported or challenged the overall consensus about the correct response. Therefore, it challenges the interpretation that the unreliable information simply biased the decision, as would be predicted from "mere exposure" effects 
[11]
[12]
[13]
. If this explanation were true for our findings, one would have expected to observe an increase in choices for the colour that the final, unreliable source predicted. A potential alternative explanation is that the act of ignoring irrelevant information may increase cognitive load, interfering with the process of belief formation and reducing accuracy 
[34,
35]
. However, similar to what was found in the Reliably Wrong experiment, we also found the same combination of low accuracy and decreased desire for additional information in the presence of unreliable information sources, which did not require much cognitive effort to detect or ignore since they were always presented at the end of each sequence. Also consistent with the findings of the Reliably Wrong experiment, we showed that the presence of an unreliable information source decreased participants' confidence in their ability to choose the winning colour, which is not easily explained by the cognitive effort hypothesis. Together, these findings highlight an overlooked and persistent effect of misinformation on belief formation. Even when unreliable and misleading information was explicitly identified, it inhibited participants' ability to form accurate beliefs. This occurred regardless of the contents of those information signals, suggesting that their mere presence in the information environment was sufficient to reduce the accuracy of the agents' beliefs.
It is plausible that the process of integrating unreliable and reliably wrong information was noisier than integrating the reliably correct information, leading to an increase in choice uncertainty. This is supported by our follow-up experiments, which revealed a smaller increase in decision confidence with increasing evidence strength in the Reliably Wrong condition relative to the Reliably Right condition, suggesting increased difficulty in integrating the reliably wrong information. This may be due to the relative infrequency of the unreliable and misleading information sources relative to the reliably correct sources, as infrequent stimuli require more time and effort to process 
[36]
[37]
[38]
. Alternatively, additional noise could enter the decision process if unreliable and misleading information require additional mental operations to be performed prior to integration. For example, information that is below 50% reliable may be subject to a process of inversion to translate it into more familiar, reliably correct terms -just as novice second language speakers tend to translate sentences back into their first language 
[39,
40]
. Whether these effects are attributable to heterogeneity of source reliability or are unique to unreliable and misleading information remains an important question for future research to address.
Critically, reductions in belief accuracy were not counterbalanced by an upregulation of information-seeking behaviour. Instead, the presence of unreliable and misleading information reduced the desire for further information, despite decreases in decision accuracy and confidence. Both accuracy and confidence are negatively associated with uncertainty 
[41,
42]
, which is understood to be a key driver of information-seeking 
[21,
33]
. However, the role of uncertainty in information-seeking is often studied in contexts where the processes of inducing and resolving uncertainty are separated, and full, deterministic information is available 
[22,
24,
43]
. In many applied settings, however, subjective uncertainty levels are instead determined through the act of information sampling. In these circumstances, the perceived quality of the information being sampled is directly tied to the agent's subjective sense of uncertainty; if an environment consists of low-reliability information, an agent sampling from that environment will have high uncertainty 
[44]
. Within a normative framework, information is valued not only by the agent's current state of uncertainty, but also by the extent to which the information is expected to reduce uncertainty 
[33]
. The observed decrease in information-seeking in environments containing unreliable and misleading information may therefore be attributable to the false perception that any additional information sampled from the same environment is likely to be less reliable and therefore less informative. While reducing information-seeking in environments in which information is less reliable may be adaptive, the information environments in the present study were equally informative. Therefore, as a consequence of this perception, participants may have opted to forgo the opportunity to improve their understanding, retaining less accurate beliefs and earning lower rewards. Importantly, our results suggest that this mechanism is more anchored in an intuitive, possibly emotional response to the information environment. Indeed, given that the additional information was always helpful (i.e., of positive reliability), a rational observer should have been always sampling more information, not less.


Conclusions
In the present study, investigated the effects of clearly-identified misleading and unreliable information to isolate their key effects on information-seeking behaviour and belief formation. We demonstrated that environments containing low-reliability information led to decreased accuracy of beliefs, as might be expected. Crucially, misleading information also led to decreased interest in acquiring additional information, suggesting a general decrease in participants' reliance on -and trust in -the received information as well as the still available information from the same environment. Follow-up experiments showed that the observed down-regulation of information-seeking was not due to an inflated sense of confidence in one's beliefs but accurately reflect the decrease in confidence that would result from a drop in trust in the available information. These findings suggest that, while an effective method for reducing false beliefs, merely labelling misinformation does not entirely prevent its harmful effects. Specifically, even when it is labelled as such, exposure to misleading information may decrease the probability an agent will continue to pursue additional reliable information, leading them to prematurely terminate their information search and settle on potentially inaccurate beliefs. Having isolated these effects in a highly controlled laboratory paradigm in which other potentially confounding factors were absent, future research can now investigate whether these effects help explain behaviour in realistic settings, such as online media or social networks.  
25)
. Participants that made the same response (chose the same bid size) on a minimum of 95% of trials were excluded, as they were deemed not to have engaged with the task. This left final samples of 166 for the RR experiment (71 female, 92 male, 3 other, M = 32.95, SD = 11.70), 162 for the RW experiment (52 female, 108 male, 2 other, M = 30.64, SD = 11.08) and 165 for the UR experiment (73 female, 91 male, 1 other, M = 31.72, SD = 11.00). For their participation, participants were instructed that they would receive either AUD $7.50 or their winnings from the experiment, whichever was highest. To ensure equity, participants were all paid AUD $8, equivalent to a success rate of 67% on the task, which is higher than is realistically expected. In all conditions, informed consent was provided by participants, and research was conducted in accordance with the Declaration of Helsinki. All study protocols were approved by the University of Melbourne Human Research Ethics Committee (ID 23253).


Methods


Procedure
All stimuli were presented using the jsPsych 6.3.1 library 
[46]
. Before commencing the experiment, all participants were provided with comprehensive written instructions for the task. Participants were instructed that, in each trial, they would be required to guess which of two colours would win a binary lottery. To help them decide, they would first be shown predictions of the outcome made by a series of information sources. Each source was comprised of a rectangle (200 pixels wide by 160 pixels tall) coloured according to the prediction of that source (either pink or orange) with the percentage reliability of the source displayed in the centre of the rectangle in 24-point font. Each source was displayed on screen for 1250 ms, followed by a fixation cross for 400 ms before the next source appeared. To ensure that the meaning of the reliability scores was clear, participants were explicitly instructed that a source with 0% reliability will always predict the wrong colour, 100% reliability will always predict the right colour, and 50% reliability will be guessing at random. After viewing the predictions of six sources (or seven sources, in the UR experiment), participants were given the opportunity to bid small amounts of money (between 0 cents and 5 cents) in order to view the prediction of one additional information source. This bid took the form of a Becker-DeGroot-Marschak auction (BDM auction 
[27]
), in which the participant's bid would be compared to a randomly generated price (between 1 cent and 5 cents). If the participant's bid exceeded the price, the price would be deducted from the participant's winnings, and they would be shown the prediction of the additional information source. If their bid was lower than the price, no money would be deducted, and they would be shown a blank prediction for the equivalent length of time. The position of the slider was initialized at 0 for each trial, and participants were required to adjust the slider before a response could be submitted. No time limit was placed on bid responses. Following this, participants were asked to press a key corresponding to which colour they believed would win the lottery on that trial ('F' key for orange, 'J' key for pink). They were instructed that successful choices would be rewarded with 15 cents, while unsuccessful guesses would win 0 cents. As with the bid responses, the choice options remained on screen until a response was made, with no time limit. After a choice was made, a fixation cross was shown for 1500 ms before the next trial commenced. No immediate feedback was presented to avoid creating additional biases (e.g., participants might start to mistrust sources when sometimes the guess could be wrong, even following the prediction of the more reliable sources).


Trial structure
In the RR experiment, each trial was comprised of six information sources. Three of the sources predicted orange to be the winning colour, while three predicted pink would win. The percentage reliability of each source varied between 51% and 55%. Trials were predetermined such that the cumulative difference in the reliability of sources supporting each colour varied between four levels: 0, 2, 4, or 6%. This corresponded to win probabilities of 50, 52, 54, or 56% respectively for the winning colour. The probability that the "winning" colour would win was therefore 0, 4, 8, or 12% higher than the probability that the "losing" colour would win. These values were calculated from the conditional probabilities of each outcome being true given the observed sequence of predictions. For example, given a sequence featuring three sources predicting pink with reliabilities of 52%, 53%, and 54%, and three predicting orange with reliabilities of 51%, 52%, and 52% we can assert that the probability that the final outcome will be pink is 54% -8% higher than the probability that it will be orange (see equation 1). All possible combinations of predictions that fit these criteria were generated but combinations in which the percentage reliability of all three predictors supporting either colour was the same were excluded. Six of the remaining combinations were randomly selected for each cumulative difference level. The exact same combinations were presented to all participants. The order in which the trials were presented, as well as the order in which each information source within each trial was presented, were randomised across all participants. Each trial with a 0% win-probability difference was presented twice, while trials with 4, 8, or 12% difference were presented four times each. Within these repetitions, on half of the trials, the sources predicting pink had the higher cumulative reliability (i.e., pink was the "winning" colour), while on the other half, those predicting orange had the higher reliability (i.e., orange was the "winning" colour). This produced a total of 84 trials per participant, which were divided into four equal blocks, each taking approximately 7 minutes to complete. See 
Supplementary Table  A13
 for the exact sequences used. (1)
In the RW experiment, we used the exact same trials as were used in the RR experiment, with one key change implemented: On each trial, a pseudorandomly selected information source was "inverted" by swapping the colour of its prediction and changing its reliability to be below 50%, though remaining equidistant from this point of indifference (e.g., a source predicting orange that was 53% reliable would be swapped to one predicting pink with 47% reliability). The inverted sources were selected such that each trial with 4, 8, or 12% win-probability difference was presented four times, once each for the possible combinations of winning colour (orange or pink) and colour of the inverted source (orange or pink). As trials with a 0% win-probability difference have no dominant colour, they were each presented twice, once each with the inverted source colour being orange and pink. The trial structure therefore mimicked that of the RR experiment, with the only change being the inversion of one source on each trial. See 
Supplementary Table A14
 for the exact sequences used.
Finally, in the UR experiment, the exact same trials were again copied from the RR experiment. For all trials, an extra "unreliable" source (a source with 50% reliability) was added. This source was always the final source shown to participants on each trial. Similar to the RW experiment, the colour predicted by the unreliable source was chosen such that each trial with a 4, 8, or 12% win-probability difference was presented four times, once each for the possible combinations of winning colour (orange or pink) and unreliable source prediction colour (orange or pink). Again, trials with a 0% reliability difference were each presented twice, once each with the unreliable source predicting orange and pink. See 
Supplementary Table A13
 for the exact sequences used.


Computational modelling
All computational modelling analyses were conducted using a hierarchical Bayesian modelling approach. Models were fit using Hamiltonian Monte Carlo sampling, as implemented in Stan 
[47]
. Models were fit from four parallel chains with 1500 warm-up samples, followed by 5000 samples drawn from converged chains.
Participants' bid responses were modelled using a hierarchical Bayesian estimation of a cumulative probit model, equivalent to ordinal regression 
[28]
. The model was run twice, once each to compare data from the Reliably Right experiment with each of the other two experiments. The absolute value of the difference in win-probability between colours was modelled as a random factor (β Dif fj ), varying between participants. As it only contained two levels (Reliably Right vs. Reliably Wrong/Unreliable), experimental condition was treated as a fixed factor (β Cond ), as was the interaction between experimental condition and win-probability difference (β DC ). Response thresholds for each bid response were also treated as fixed effects. To prevent overfitting, dispersion was fixed at a value of 1 for all participants. Weakly informative priors were used for all variables. Priors for the response thresholds were specified such that each response option contained equal probability mass within the standard normal distribution. See full model specification in equation 2. (2) Participants' choice behaviour was modelled using hierarchical Bayesian estimation of a Bernoulli logit model, equivalent to logistic regression. As with the model of bid size, this model was fit twice, once each to compare the Reliably Right experiment to the two other experiments. The win-probability difference between colours (including the extra information participants could bid for, if it was received) was modelled as a random factor, varying between participants. Both experimental condition and the interaction between condition and win-probability difference were modelled as fixed effects. All prior distributions were weakly informative. As experimental condition was a multiplicative factor, its prior was distributed according to a logarithmic scale to ensure symmetrical effects of equal increases or decreases. See full model specification in equation 3.
Choice ij ∼ e Vij 1 + e Vij V ij = ((β Dif fj + (β DC Cond ij ))Dif f ij )
• (1 + (β Cond − 1) • Cond ij ) log(β Cond ), β DC ∼ Normal(0, 1) β Dif fj ∼ Normal(µ β Dif f , σ β Dif f ) µ β Dif f ∼ Normal(0, 1) σ β Dif f ∼ HalfCauchy(0, 2)
(3)
4.2 Experiments 4-6


Participants
We used the online recruitment platform Prolific 
[45]
  For their participation, participants were instructed that they would receive either AUD $7.50 or their winnings from the experiment, whichever was highest. To ensure equity, participants were all paid AUD $10, equivalent to a success rate of 80% on the task, which is higher than is realistically expected. In all conditions, informed consent was provided by participants, and research was conducted in accordance with the Declaration of Helsinki. All study protocols were approved by the University of Melbourne Human Research Ethics Committee (ID 23253).


Procedure
The experimental procedure in the each of the confidence experiments was the same as in the first three experiments, with two key differences. Following the presentation of the six information sources, instead of bidding to view an additional information source, participants were asked to immediately choose which colour they believed would win the lottery. Following this, they were asked to provide a confidence rating corresponding to how likely they believed their chosen colour was to win. This rating was made on a continuous scale ranging from 0 to 100. The scale was marked "Not confident at all" at the lower end, "Somewhat confident" in the centre, and "Extremely confident" at the top of the scale. The starting position of the slider was randomised, and participants were required to adjust the slider position before they could submit a response. No time limit was given for responding.


Trial structure
All trials from the RR, UR, and RW experiments were repeated in the RR-Conf, UR-Conf, and RW-Conf experiments, respectively.


Computational modelling
Participants' choice behaviour was modelled exactly as for experiments 1-3 (see equation 3). Confidence ratings were modelled as a normally distributed variable, equivalent to linear regression. As with the model of bid size, the absolute value of the difference in win-probability between colours was modelled as a random factor, while both experimental condition and the interaction between experimental condition and win-probability difference were modelled as fixed effects. A random intercept was also included in the model (β Consj ) to allow for subject-specific variability in baseline responses. See equation 4 for full model specification. 
Figure 2 :
2
Information-Seeking Experiment Results. A) Mean choice accuracy by experimental condition and win-probability difference. B) Mean bid sizing by condition and win-probability difference. C) Mean choice accuracy in the Unreliable experiment by whether the unreliable information source predicted the winning or losing colour. D) Mean choice accuracy in the Reliably Wrong experiment by whether a source predicting the winning or losing colour was inverted. E) Mean bid size in the Unreliable experiment by whether the unreliable information source predicted the winning or losing colour. F) Mean bid size in the Reliably Wrong experiment by whether a source predicting the winning or losing colour was inverted. All error bars denote standard error of the mean.


Figure 3 :
3
Posterior distributions of model parameters from information-seeking experiments. All models involved the comparison of the Reliably Right experiment with either the Unreliable or Reliably Wrong experiment.


[F (2, 296) = 122.49, p < .001, η 2 = .112], UR-Conf [F (2, 298) = 97.26, p < .001, η 2 = .111], and RW-Conf [F (2, 298) = 62.85, p < .001, η 2 = .089] experiments (see


Figure 4 :
4
Choice behaviour and confidence ratings from confidence experiments. A) Mean choice accuracy by condition and win-probability difference. B) Mean confidence ratings by condition and win-probability difference. C) Mean choice accuracy in the Unreliable experiment by whether the unreliable information source predicted the winning or losing colour. D) Mean choice accuracy in the Reliably Wrong experiment by whether a source predicting the winning or losing colour was inverted. E) Mean confidence rating in the Unreliable experiment by whether the unreliable information source predicted the winning or losing colour. F) Mean confidence in the Reliably Wrong experiment by whether a source predicting the winning or losing colour was inverted. All error bars denote standard error of the mean. Note. * indicates statistically significant difference, p < .05


Figure 5 :
5
Posterior distributions of model parameters from confidence experiments. All models involved the comparison of the Reliably Right Confidence experiment with either the Unreliable Confidence or Reliably Wrong Confidence experiment. The top panels show parameter values obtained from the modelling of choice behaviour, while the bottom panels show parameters obtained from modelling confidence ratings.


4. 1
1
Experiments 1-3 4.1.1 Participants Using the online recruitment platform Prolific [45], we recruited independent samples of 200 participants for each of our three information-seeking experiments: the Reliably Right (RR) experiment (81 female, 116 male, 3 other, M = 33.13, SD = 11.46), Reliably Wrong (RW) experiment (85 female, 113 male, 2 other, M = 30.33, SD = 11.03), and Unreliable (UR) Information experiment (56 female, 141 male, 3 other, M = 32.63, SD = 11.


P
(preds|pink) = .52 • .53 • .54 • .49 • .48 • .48 = .0168 P (preds|orange) = .48 • .47 • .46 • .51 • .52 • .52 = .0143 P (pink|preds) = P (preds|pink) P (preds|pink) + P (preds|orange) ≈ .54


p(bid = k|τ k , µ ij ) = Φ(τ k+1 − µ ij ) − Φ(τ k − µ ij ) µ ij = β Dif fj |Dif f ij | + β Cond Cond ij + β DC |Dif f ij |Cond ij τ 1 ∼ Normal(−0.97, 1) τ 2 ∼ Normal(−0.43, 1) τ 3 ∼ Normal(0, 1) τ 4 ∼ Normal(0.43, 1) τ 5 ∼ Normal(0.97, 1) β Cond , β DC ∼ Normal(0, 1) β Dif fj ∼ Normal(µ β Dif f , σ β Dif f ) µ β Dif f ∼ Normal(0, 1) σ β Dif f ∼ HalfCauchy(0, 2)


to recruit independent samples of 150 participants for all three experiments: Reliably Right Confidence (RR-Conf) experiment (60 female, 86 male, 4 other, M = 30.25, SD = 10.56), Unreliable Confidence (UR-Conf) experiment (50 female, 96 male, 4 other, M = 31.59, SD = 10.79), and Reliably Wrong Confidence (RW-Conf) experiment (71 female, 77 male, 2 other, M = 34.76, SD = 12.66). One participant in the RR-Conf experiment was excluded for giving the same confidence rating on every trial, leaving a final sample of 149 (60 female, 85 male, 4 other, M = 30.27, SD = 10.59).


Conf idence∼ Normal(V ij , σ Confj ) V ij = (β Consj + β Cond Cond ij + (β Dif fj + (β DC Cond ij )) • Dif f ij ) β Cond , β DC ∼ Normal(0, 2) β Consj ∼ Normal(µ β Cons , σ β Cons ) β Dif fj ∼ Normal(µ β Dif f , σ β Dif f ) µ β Cons ∼ Normal(50, 10) µ β Dif f ∼ Normal(5, 5) σ β Cons , σ β Dif f , σ Conf ∼ HalfCauchy(0, 10)


RR-Conf [F (3,444) = 193.97, p < .001, η 2 = .213], UR-Conf [F (3,447) = 66.73, p < .001, η 2 = .027], and RW-Conf conditions [F (3,447) = 26.808, p < .001, η 2
= .007] (see








Data availability
Raw data is available from the open science foundation website (https://osf. io/kjbsy/).


Declarations
The authors declare no competing interests.
 










Political Misinformation. Annual Review of Political Science




J
Jerit






Y
Zhao




10.1146/ANNUREV-POLISCI-050718-032814








23














Correction format has a limited role when debunking misinformation. Cognitive Research: Principles and Implications




B
Swire-Thompson






J
Cook






L
H
Butler






J
A
Sanderson






S
Lewandowsky






Ukh
Ecker




10.1186/S41235-021-00346-6/TABLES/6








6














Proceedings of the National Academy of Sciences




J
D
West






C
T
Bergstrom




10.1073/PNAS.1912444117








118


1912444117






Misinformation in and about science








Online engagement with 2020 election misinformation and turnout in the 2021 Georgia runoff election




J
Green






W
Hobbs






S
Mccabe






D
Lazer




10.1073/PNAS.2115900119/SUPPL_FILE/PNAS.2115900119.SAPP.PDF








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






119


2115900119












Online misinformation about climate change. WIREs Climate Change




'i
Treen






K
M
Williams






Htp






O
Neill






S
J




10.1002/wcc.665








11












Online misinformation and vaccine hesitancy




R
Garett






S
D
Young




10.1093/tbm/ibab128








Translational Behavioral Medicine




11


12
















The similarity-updating model of probability judgment and belief revision




R
Albrecht






M
A
Jenny






H
Nilsson






J
Rieskamp




10.1037/REV0000299








Psychological Review




128


6


1088














Do humans make good decisions? Trends in Cognitive Sciences




C
Summerfield






K
Tsetsos




10.1016/J.TICS.2014.11.005








19














How do readers handle incorrect information during reading?




D
N
Rapp




10.3758/MC.36.3.688








Memory & Cognition




36


3
















Can't We Just Disregard Fake News? The Consequences of Exposure to Inaccurate Information




D
N
Rapp






N
A
Salovich




10.1177/2372732218785193








5








Policy Insights from the Behavioral and Brain Sciences








Heuristic Decision Making. Annual Review of Psychology




G
Gigerenzer






W
Gaissmaier




10.1146/annurev-psych-120709-145346








62














Mere Exposure Effect Is Sometimes Insensitive to Mood Inductions. Experimental Psychology




M
Molet






P
Craddock






A
J
Osroff






P
Li






T
L
Livingston






R
R
Miller




10.1027/1618-3169/A000514








68














Mere Exposure: A Gateway to the Subliminal. Current directions in psychological science




R
B
Zajonc




10.1111/1467-8721.00154








10














Creating illusions of knowledge: Learning errors that contradict prior knowledge




L
K
Fazio






S
J
Barber






S
Rajaram






P
A
Ornstein






E
J
Marsh




10.1037/A0028649








Journal of Experimental Psychology: General




142


1
















How to protect eyewitness memory against the misinformation effect: A meta-analysis of post-warning studies




H
Blank






C
Launay




10.1016/J.JARMAC.2014.03.005








Journal of Applied Research in Memory and Cognition




3


2
















Cleaning Up Social Media: The Effect of Warning Labels on Likelihood of Sharing False News on Facebook




P
Mena




10.1002/POI3.214








Policy & Internet




12


2
















Creating News Literacy Messages to Enhance Expert Corrections of Misinformation on Twitter. Communication Research




E
K
Vraga






L
Bode






M
Tully




10.1177/0093650219898094








49














Warning weakens retrievalenhanced suggestibility only when it is given shortly after misinformation: The critical importance of timing




Jck
Chan






R
O'donnell






K
D
Manley




10.1037/XAP0000394








Journal of Experimental Psychology: Applied
















Processing inaccurate information: Theoretical and applied perspectives from cognitive science and the educational sciences




P
Kendeou






E
O'brien






Rapp DN, Braasch JLG






The MIT Press








The Knowledge Revision Components (KReC) framework: Processes and mechanisms








Explicit warnings reduce but do not eliminate the continued influence of misinformation




Ukh
Ecker






S
Lewandowsky






Dtw
Tang




10.3758/MC.38.8.1087








Memory & Cognition




38


8
















Towards a neuroscience of active sampling and curiosity




J
Gottlieb






P
Y
Oudeyer




10.1038/s41583-018-0078-0








Nature Reviews Neuroscience




19


12
















Valuation of knowledge and ignorance in mesolimbic reward circuitry




C
J
Charpentier






E
S
Bromberg-Martin






T
Sharot




10.1073/pnas.1800547115








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






115














Neurocomputational mechanisms underlying the subjective value of information




Axa
Goh






D
Bennett






S
Bode






Ttj
Chong




10.1038/s42003-021-02850-3








Communications Biology




4


1
















Choosing increases the value of non-instrumental information




M
Jiwa






P
S
Cooper






Ttj
Chong






S
Bode




10.1038/s41598-021-88031-y








Scientific Reports




11


1
















Diverse motives for human curiosity




K
Kobayashi






S
Ravaioli






A
Baranès






M
Woodford






J
Gottlieb




10.1038/s41562-019-0589-3








Nature Human Behaviour




3


6
















The Psychology of Fake News. Trends in Cognitive Sciences




G
Pennycook






D
G
Rand




10.1016/J.TICS.2021.02.007








25














Measuring utility by a singleresponse sequential method




G
M
Becker






M
H
Degroot






J
Marschak




10.1002/bs.3830090304








Behavioral Science




9


3
















Doing Bayesian Data Analysis : A Tutorial Introduction with R




J
Kruschke








Elsevier Science & Technology


Boston












Monckton of Brenchley C. Climate Consensus and 'Misinformation': A Rejoinder to Agnotology, Scientific Consensus, and the Teaching and Learning of Climate Change. Science & Education




D
R
Legates






W
Soon






W
M
Briggs




10.1007/s11191-013-9647-9








24














Linguistic characteristics and the dissemination of misinformation in social media: The moderating effect of information richness. Information Processing & Management




C
Zhou






K
Li






Y
Lu




10.1016/J.IPM.2021.102679








58


102679












I know I know it, I know I saw it": The stability of the confidence-accuracy relationship across domains




B
H
Bornstein






D
J
Zickafoose




10.1037/1076-898X.5.1.76








Journal of Experimental Psychology: Applied




5


1
















Choosing, confidence, and accuracy: A meta-analysis of the confidence-accuracy relation in eyewitness identification studies




S
L
Sporer






S
Penrod






D
Read






B
Cutler




10.1037/0033-2909.118.3.315








Psychological Bulletin




118


3
















How people decide what they want to know




T
Sharot






C
R
Sunstein




10.1038/s41562-019-0793-1








Nature Human Behaviour




4
















The effect of illustrations in arithmetic problem-solving: Effects of increased cognitive load. Learning and Instruction




I
E
Berends






Ecdm
Van Lieshout




10.1016/J.LEARNINSTRUC.2008.06.012








19














A comparison of four methods for cognitive load measurement




S
Chen






J
Epps






F
Chen




10.1145/2071536.2071547








Proceedings of the 23rd Australian Computer-Human Interaction Conference


the 23rd Australian Computer-Human Interaction Conference
OzCHI


















Theta brain rhythms index perceptual narrowing in infant speech perception




A
N
Bosseler






S
Taulu






E
Pihko






J
P
Mäkelä






T
Imada






A
Ahonen




10.3389/fpsyg.2013.00690








Frontiers in Psychology




4














Prefrontal Activation Evoked by Infrequent Target and Novel Stimuli in a Visual Target Detection Task: An Event-Related Functional Magnetic Resonance Imaging Study




E
Kirino






A
Belger






P
Goldman-Rakic






G
Mccarthy




10.1523/JNEUROSCI.20-17-06612.2000








The Journal of Neuroscience




20


17
















The effect of repetition of infrequent familiar and unfamiliar visual patterns on components of the event-related brain potential




A
Kok






Jong
De






Hl




10.1016/0301-0511(80)90013-7








Biological Psychology




10


3
















The Role of Mental Translation in Second Language Reading. Studies in Second Language Acquisition




R
G
Kern




10.1017/S0272263100013450








16














Roles of mental translation in first and foreign language reading




Yau
Jlc




10.1177/1367006910380038








International Journal of Bilingualism




15


4
















Evaluation of Objective Uncertainty in the Visual System




S
Barthelmé






P
Mamassian




10.1371/JOURNAL.PCBI.1000504








PLOS Computational Biology




5


9














Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence




R
N
Denison






W
T
Adler






M
Carrasco






W
J
Ma




10.1073/PNAS.1717720115/SUPPL_FILE/PNAS.1717720115.SAPP.PDF








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






115














Induction and Relief of Curiosity Elicit Parietal and Frontal Activity




Llf
Van Lieshout






Are
Vandenbroucke






Ncj
Müller






R
Cools






X
Floris






De
Lange






P




10.1523/JNEUROSCI.2816-17.2018








The Journal of Neuroscience




38


10
















Elicitation from Large, Heterogeneous Expert Panels: Using Multiple Uncertainty Measures to Characterize Information Quality for Decision Analysis. Decision Analysis




S
Hoffmann






P
Fischbeck






A
Krupnick






M
Mcwilliams




10.1287/deca.1070.0090








4














Prolific.ac-A subject pool for online experiments




S
Palan






C
Schitter




10.1016/J.JBEF.2017.12.004








Journal of Behavioral and Experimental Finance




17
















jsPsych: A JavaScript library for creating behavioral experiments in a Web browser




J
R
De Leeuw




10.3758/S13428-014-0458-Y








Behavior Research Methods




47


1
















A probabilistic programming language




B
Carpenter






A
Gelman






M
D
Hoffman






D
Lee






B
Goodrich






M
Betancourt




10.18637/jss.v076.i01








Journal of Statistical Software

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]