You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Who's to say who's an expert?


~ Paul Newman
As societies evolve, so do systems of societal decision-making. In democratic societies the aggregates of individual votes afford and sustain political decision making 
(Fiorina & Noll, 1979)
.
Beyond political elections, there are myriad contexts for crowdsourcing judgments and decisions, which may serve as testing grounds for innovative methods that integrate available information in many different ways (e.g., 
Dredze et al., 2008;
Hertwig, 2012;
Wang et al., 2012)
. Of these methods, the majority rule (e.g., 
Hastie & Kameda, 2005)
, confidence weighing (e.g., 
Bahrami et al., 2010;
Koriat, 2012ab)
 and meta-prediction models (e.g., 
Martinie et al., 2020;
Palley & Satopää, 2020;
Prelec et al., 2017)
 have received much research attention. How can these methods be used for further progress towards a deeper understanding of expert judgment? 
Prelec and colleagues (2017, p. 532)
 observed that some of the most prominent contemporary models "are biased for shallow, lowest common denominator information, at the expense of novel or specialized knowledge that is not widely shared." This concern raises the question of how experts may be characterized so that their unique and valuable knowledge can be used for the common good. We present a novel approach to identify the psychological properties of experts.


A new way to identify expertise
There exist several theoretical ideas concerning the nature of expertise. Most of these models focus on a person's own decision and one other psychological indicator (see 
Table 1
 below for three prominent examples). Going beyond these models, we combine two critical indicators of expertise in order to capture expert judgment with greater psychological nuance. Specifically, we focus on a participant's degree of confidence in a particular judgment and their level of social projection, that is, their prediction of others' judgments. These two conceptually independent metajudgments can open a unique window into the expert's mind. of Confidence. To identify decision makers with expertise, many theories rely on the decision maker's level of confidence (e.g., 
Bahrami et al., 2010;
Koriat, 2012ab)
. Indeed, confidence is diagnostic for expertise in kind decision-making environments 
(Hertwig, 2012)
. Kind environments offer a close match between the context of information acquisition (learning) and the context of information application 
(prediction;
Hogarth, 2001)
. In a kind environment, feedback is swift, accurate, and cheap. For crowdsourcing this means that the level of confidence in a prediction is positively correlated with its degree of accuracy. High confidence is a valid and valuable cue for a decision maker's expertise in such environments. Conversely, wicked environments 
(Hogarth, 2001)
 provide noisy feedback, which might also be delayed or hard to obtain. In this environment, confidence is not diagnostic of accuracy, and hence it does not point to expertise.
We take task difficulty as a suitable proxy for the type of environment at hand, and assume that experts know this to be so. We expect confidence to be lower overall for difficult than for easy tasks and to be higher overall among experts than among nonexperts. Whereas the experts' high confidence on difficult tasks is justified, high confidence among the less informed is not. The latter, if confident, commit errors of overconfidence 
(Moore, 2020)
. As shown in 
Table 1
, 
Zhang (2021)
 offers an advanced perspective on confidence to increase its diagnosticity for expertise. But this method is costly and rarely applicable. We address the low diagnosticity of confidence in wicked environments by assessing a second prominent indicator of expertise: social projection.


Social projection.
The term projection has enjoyed (or suffered) many a usage 
(Allport, 1924;
Freud, 1894;
Heck & Krueger, 2020;
Krueger, 2007;
Newman et al., 1997)
. A simple and robust way of looking at projection is to see it as an instance of inductive reasoning based on a sample of one. As a person has access to their own prediction, they can use this information to meta-predict the predictions of others. A person who predicts 'high' will (and should!) predict a greater proportion of 'high' predictions than a person who predicts 'low' to the of extent that no other information about others' predictions is available (see 
Dawes, 1989;
Krueger, 1998
, for the Bayesian logic of this claim; , for an applied scenario).
In general, and in the most uncertain environments, greater projection leads to higher accuracy.
Empirically, however, it appears that people project too little 
(Hoch, 1987)
. Little is known about who or what type of person in what kind of context shows the highest degree of calibration in their level of social projection. This is where the issue of expertise provides traction. As shown in 
Table   1
, some theories of expertise assume that the most skilled individuals project most accurately 
(Martinie et al., 2020;
Palley and Satopää, 2020)
. That is, experts might show adequate levels of projection where the less skilled tend to overproject. However, projection is also comparatively low in people who have little or no relevant knowledge in the field of interest and who know so. If projection were the only indicator of expertise, these individuals would be confused with experts.
Like confidence, projection is a fallible indicator of expertise when considered in isolation. If high confidence and low projection can be found among nonexperts, these two measures are not sufficient to indicate expertise on their own. But when put together they might be. 


Projection
The difference of own decision and own projection in relation to the sum of all differences of others' decisions and projections.


Martinie et al. (2020)


Projection accuracy 2
Projection Projection accuracy as the difference between projection and the average estimation.
A more comprehensive approach to characterizing experts. In light of the unique limitations of confidence and social projection to characterize experts, we propose a joint consideration of both to find new opportunities to capture expertise. We assume that only experts solve difficult tasks with confidence while knowing that few others can. Specifically, when prediction tasks are difficult, the combination of high confidence and low projection signals expertise, while none of the three alternative combinations do. When prediction tasks are easy, expertise is not defined, or, if it does exist, it cannot be detected.
Using data collected by Prelec et al. 
2017
, we evaluate the following hypotheses: Experts have high confidence in their choice (H1.a) and they project less than the average decision-maker (H1.b). Confidence and projection are least correlated among experts (H2). Experts have a greater ability than others to tell easy from difficult questions. Hence, experts more accurately adapt their predictions of other people answering correctly to the question's difficulty (H3).


Methods
We analyzed three samples of data collected by 
Prelec et al. (2017)
. The samples differ in terms of 1 item topics (i.e., "state capitals of the U.S.", "general knowledge", and "dermatological assessment of lesions") and the sampled respondents. We briefly summarize the methods here. For details, see presented with 50 different items in the topical realm of "state capitals of the U.S.". For each item, they read the statement "X is the capital of Y", where X was a city and Y a U.S. state.


General knowledge.
A total of 39 participants were recruited from Amazon Mechanical Turk (MTurk). Participants were presented with 80 different items within their topics ranging from
We thank Drazen Prelec for sharing the data with us.
history to geography. For each item, respondents read a general knowledge statement (e.g., "A muon has negative electric charge.").
Dermatological lesions. 25 dermatologists were recruited by referral. From 80 pictures of of lesions, each participant was presented with a sample of 60 pictures.
For every statement in all three topic domains, participants indicated (1) their assessment of a statement (i.e., true or false) or picture (i.e., benign or malignant), (2) their confidence in their assessment (from 50% to 100%), and (3) their estimate of the percentage of other respondents believing the statement to be true or the picture to show a benign lesion (from 0% to 100%). All three data sets were combined into one data set for analysis. The resulting data set comprised 97 participants making 6,270 decisions on 210 different items.


Results
Setting the stage. Prelec and colleagues (2017) presented participants with two choice alternatives for each item: a written claim was to be judged as either true or false (i.e., questions regarding state capitals of the U.S. or general knowledge), or a lesion depicted visually was to be judged as either benign or malignant (i.e., dermatological lesions). For every item a crossing of the statistical status of indicating the majority or the minority response with the response's correctness or incorrectness yields four distinctive groups of responses. These four groups do not comprise individuals but individuals' choices on items. For example, on the statement "Berlin is the capital of Germany," (item 1) a respondent might correctly indicate "yes," while also falsely indicating "yes" in response to the statement "Marseille is the capital of France."(item 2). A respondent who chose "yes" for item 1 would find him-or herself in the correct majority if most others also responded by choosing "yes." A person saying "yes" to item 2, in contrast, would be in the correct minority if most others responded correctly by indicating "no." The incorrect majority or the correct minority were categorized in an analogous way.
of After removing cases with missing values, individuals' answers to a question that were correct and were chosen by the minority of people were categorized as the correct minority (N = 862). Answers that were correct and were given by the majority of people were categorized as the correct majority (N = 2,783). All incorrect answers to a question chosen by individuals were, too, separated into two groups depending on the answer's popularity among all people (N incorrect minority = 1,017; N incorrect majority = 1,595).
As we assume that, by definition, experts respond correctly, we can expect expert choices to cluster in the correct majority and the correct minority. We can also expect that the relative proportion of experts will be higher in the correct minority than in the correct majority. Hence, expert choices in the available sample seem to be best -though not exclusively -captured by the correct minority choices. For the study of what characterizes experts, a focus on the correct minority relative to the other three groups is thus necessary.
Differences in confidence and projection. We tested our first hypothesis by comparing the average confidence ratings (H1.a) and projection rates (H1.b) among the four groups. As the average perceived item difficulty is expected to be different in majority and minority groups, a comparison of the average confidence between the correct minority and the two majority groups is uninformative. Therefore, we compared the mean judgments of the two minority groups with each other and separately compared the two majority groups with each other. The means of the four 2 groups are displayed in 
Figure 1
. As expected, the confidence level displayed in the correct minority (M = .71) was significantly higher than confidence in the incorrect minority (M = .68),
with ∆M = .03, t(1877) = -4.12, p < .001, showing a small effect, d = -.19. There was also a significant difference in confidence between the incorrect (M = .73) and the correct majorities (M = .79), with a ∆M = .06, t(4376) = -10.67, p < .001, and a small effect, d = -.34. This is also consistent
Tests for normality revealed significant skews. As the differences between the means and the medians were small, we focus on the with our hypothesis. Experts -who reside in the correct groups -have higher confidence than other decision makers. However, we focus our analysis on the correct minority because that group carries a higher proportion of experts.


Figure 1
Judgmental confidence: Means and standard errors for the four groups' confidence levels
As shown in 
Figure 2
 and as expected, projection was weaker in the correct minority (M = .52) than in the incorrect minority (M = .55), ∆M = .03, t(1877) = 3.51, p = .003, with a small effect,
d = .
16. There was no significant difference, t < 1, p = .480, d = .02, between the projection rates in the correct majority (M = .60) and the incorrect majority ((M = .60). Again, the comparison of the correct minority with the majority groups provides little information, as the projection rate is expectedly influenced by the perceived difficulty of an item which differs between minority and majority. Importantly, the critical pattern was obtained.


Figure 2
Social projection: Means and standard errors for the four groups' projection rates of In summary and as expected (H1), judgments by the correct minority were characterized by comparatively high confidence and low social projection. This pattern, we submit, is the footprint of expertise. As shown in 
Table A1
 in the Appendix, nonparametric Mann-Whitney U tests were consistent with these analyses.
Correlational analyses. We expected a lower correlation between confidence and projection within the correct minority than within the other three groups because the correct minority comprises the highest proportion of experts (H2). As shown in 
Figure A2
 in the Appendix, the correlation in the correct minority was small, r(860) = .10, but significant, p = .003, across item topic groups. After Fisher's Z transformations, we found that the association between projection and confidence in the correct minority was significantly lower than in the incorrect minority, r(1015) =
.345, Z = -5.59, p < .001, the incorrect majority, r(1593) = .326, Z = -5.62, p < .001, and the correct majority, r(2781) = .302, Z = -5.52, p < .001, respectively. For one of the three domains of questions, namely, the questions about state capitals, the correlation of confidence and projection in the correct minority was near zero and nonsignificant, r = -.035, p = .579, while the relationship remained highly significant in the other three groups.
Sensitivity to item difficulty. We predicted that experts' projections are more sensitive to item difficulty than others' (H3). We therefore compared the differences in projection rates among the four groups, contrasting easy with difficult items. The percentage of respondents choosing the of correct option for an item represented the item's difficulty. Difficulty is greatest when no one chooses the correct option (0%), and lowest when everyone chooses the correct option (100%). By definition, the correct minority and the incorrect majority only contain items that were answered correctly by the minority (< 50%). The incorrect minority and the correct majority only contain items that were answered correctly by the majority (> 50%). To test for projection sensitivity to 3 item difficulty in the former two groups, we compared choices for items that less than 11% of the participants answered correctly (highest difficulty in the two groups) and for items which were answered correctly by 39 to 49 percent (lowest difficulty). For the latter two groups, we compared choices for items which were answered correctly by 51 and 61 percent of participants (high difficulty) and for items which more than 89 percent of the participants answered correctly (low difficulty). For the very difficult items the correct minority only contained a few cases (N = 14). To be able to judge if the data are informative enough to evaluate our hypothesis in such cases we used Bayesian hypothesis testing.
We found strong evidence, BF10 = 14.60, indicating that the average rate of projection in the correct minority adapted to difficult (M = .35, N = 14) and easy items (M = .51, N = 446; see 
Figure   3
, a). That is, expert respondents (i.e., the correct minority) were sensitive to the item's difficulty when predicting other respondents' judgments. For the incorrect majority there was extremely strong evidence, BF10 = 179.83, in the opposite direction 
(Figure 3, b)
. Projection was stronger for high (M = .67, N = 141) than for low (M = .59, N = 540) item difficulty. There was moderate evidence, BF10 = .20, for the interpretation that there is no difference in projection between high (M = .56, N = 329) and low (M = .58, N = 41) item difficulty in the incorrect minority 
(Figure 3, c)
.
That is, projection in the incorrect minority -that is among nonexperts -was not sensitive to item difficulty. In contrast, there was extreme evidence, BF10 = 2357 e+39 , for the claim that projection is weaker for difficult (M = .50, N = 421) than for easy items (M = .72, N = 845) in the correct Note that no question resulted in 50% of people answering correctly and incorrectly.
sensitive to the difficulty of an item.
In summary, there was clear evidence for projection sensitivity to item difficulty in both groups that included experts, the correct minority and the correct majority. There was no evidence for such sensitivity in the two groups that lack experts, the incorrect minority and the incorrect majority.


Figure 3
Projection on difficult and easy items difficulties across the four groups Note. Item difficulty in correct minority and incorrect majority: high (under 11% of people answered correctly) vs. low (39% to 49% answered correctly). Item difficulties in incorrect minority and correct majority: high (51% to 61% answered correctly) vs. low (over 89% answered correctly).
Additionally, 
Figure A3
 in the Appendix shows the confidence ratings of high and low item difficulty in the four different groups. Only in the expert group (i.e., correct minority) did the item difficulty not change the participants' confidence level significantly.


Discussion
The aim of this study was to test assumptions generated by an integrative psychological approach to expertise. Using data collected by Prelec and colleagues (2017) we find support for these assumptions. Expert choices are associated with high confidence and a comparatively low of social projection rate (H1). Moreover, confidence and projection are conceptually and statistically discriminable. The two indicators are less positively correlated among experts than among nonexperts (H2). Experts are most sensitive to an item difficulty, and thus project their own responses to others to a lesser degree for difficult items (H3). Together, these findings encourage a more nuanced view of expertise as they point to the relevance of two variables, confidence and projection, that are commonly used in studies of expertise but whose joint effects have thus far not been explored.
We wish to remind the reader that we analyzed expert choices (i.e., Further, the present analyses address the description of expertise and do not include its prediction. A next crucial step is to build and test a model that formalizes confidence, social projection, and their relation mathematically to predict expertise in decision-makers.
Recognizing the roles of kind and wicked environments is critical when asking questions of expertise. We addressed this issue with the analysis of items of different difficulties. However, there are many ways to proceed to ascertain what this theory can tell us in kind versus wicked environments and what it cannot tell us. In this regard, we can learn from the history of research on decision making that shows that testing a theory mostly in kind or in wicked environments leads to a myopic view on this theory . A better test of any assumption addresses both environments. We call for such testing for the present theory.
Relatedly, future research might explore how to best use identified experts for decisionmaking. Drawing on the literature on the wisdom of the crowd 
(Krueger & Chen, 2014;
Surowiecki, 2004)
, we assume this choice to be most accurate that is generated by a crowd of experts (see e.g., of 
Mannes et al., 2014
; for applied example, 
Kattan et al., 2015)
. Averaging multiple expert opinions instead of listening to a single expert voice presents the advantage that individual cognitive biaseswhich afflict experts, too -are partly cancelled out by the crowd. Following the tradeoff between expertise and diversity advised by 
Olsson and Loveday (2015)
, the wisdom of the expert crowd might advance decision-making by integrating important insights from expertise research into the common wisdom of the crowd-approach.
As societies evolve, so do systems of societal decision-making. Experts have always had a position of influence in this process, and rightly so. However, with the accelerating speed in which reality grows more complex, it gets increasingly complicated to identify experts for certain domains, especially for domains that have just been born. We in the present paper attempt to make a first step towards a more complex theory of multiple indicators of expertise. This approach blends in the recent understanding that identifying expertise is a complex endeavor that calls for a wellfledged and detailed psychological theory, and welcomes efforts to refine indicators with more psychological detail (e.g., 
Zhang, 2021;
Sziklai, 2018)
. We suggest that accuracy can be gained by relating multiple indicators of expertise to each other in a formalized manner. Our combination of indicators considers confidence and social projection as expertise markers. Experts are not just confident in their decision but also more sensible in projecting what others might decide. Their confidence has less influence on their projective behavior and this behavior is sensitive to the environment's wickedness (i.e., difficulty of items).
of


Figure A2
Visualization of correlations between projection (y-axis) and confidence (x-axis) across all four groups (correct minority, incorrect minority, incorrect majority, and correct majority); and tests for significant differences between the correlation in the correct minority and the other three groups' correlations 


Figure A3
Visualization of confidence differences between item difficulties across the four groups Note. Item difficulties in correct minority and incorrect majority: high (under 11% of people answered correctly) vs.
low (39% to 49% answered correctly). Item difficulties in incorrect minority and correct majority: high (51% to 61% answered correctly) vs. low (over 89% answered correctly).
of
correct minority) not experts per se. Respondents answered choice problems and they indicated their confidence in their own response as well as their (projective) predictions of the responses of others. The results thus do not represent characteristics of individuals. Future research will need to extend the present analyses by treating individual respondents as units of analysis.


Table 1 Overview of models to identify expert individuals from a crowd of tested decision makers
1
Variable used
Modelling of Expertise
Reference
Revealed
Confidence
Robust beliefs in light of new information. Zhang (2021)
confidence
Projection
accuracy 1


Palley & Satopää (2020)
 of


results of parametric tests, noting the nonparametric tests did not change any of our conclusions. of








Acknowledgements
We thank Drazen Prelec for sharing the data with us.


ORCID iDs
David J. Grüning https://orcid.org/0000-0002-9274-5477 Joachim I. Krueger https://orcid.org/0000-0001-9607-1695
The authors declare that they conducted the research reported in this article in accordance with the Ethical Principles of the Journal of Expertise.
of






Contributions


Contributed to conception: DJG, JIK
Contributed to analysis of data: DJG Contributed to interpretation of data: DJG, JIK Drafted and/or revised the article: DJG, JIK Approved the submitted version for publication: DJG, JIK


Authors' Declarations
The authors declare that there are no personal or financial conflicts of interest regarding the research in this article. 


Appendix
 










Social psychology




F
H
Allport








Riverside












Optimally interacting minds




B
Bahrami






K
Olsen






P
E
Latham






A
Roepstorff






G
Rees






C
D
Frith




10.1126/science.1185718








Science




329


5995
















Statistical criteria for establishing a truly false consensus effect




R
M
Dawes




10.1016/0022-1031(89)90036-X








Journal of Experimental Social Psychology




25


1
















Confidence-weighted linear classification




M
Dredze






K
Crammer






F
Pereira




10.1145/1390156.1390190








Proceedings of the 25th international conference on Machine learning


the 25th international conference on Machine learning


















Majority rule models and legislative elections




M
P
Fiorina






R
G
Noll




10.2307/2129734








The Journal of Politics




41


4
















The neuro-psychoses of defence




S
Freud








Hogarth Press












Strategic Thinking: A Random Walk Into the Rabbit Hole




D
J
Grüning






J
I
Krueger




10.1525/collabra.24921








Collabra: Psychology
















The Experience Heuristic




D
J
Grüning






J
I
Krueger








Journal of Expertise




4


3














The Robust Beauty of Majority Rules in Group Decisions




R
Hastie






T
Kameda




10.1037/0033-295X.112.2.494








Psychological Review




112


2
















Self-enhancement error motivates social projection




P
R
Heck






J
I
Krueger




10.1521/soco.2020.38.5.489








Social Cognition




38


5
















Tapping into the wisdom of the crowd-with confidence




R
Hertwig




10.1126/science.1221403








Science




336


6079
















Perceived consensus and predictive accuracy: The pros and cons of projection




S
J
Hoch




1037/0022-3514.53.2.221








of Hogarth, R. M




University of Chicago Press




53








Educating intuition








The wisdom of crowds of doctors: Their average predictions outperform their individual ones




M
W
Kattan






C
O'rourke






C
Yu






K
Chagin




10.1177/0272989X15581615








Medical Decision Making




36


4
















The self-consistency model of subjective confidence




A
Koriat




10.1037/a0025648








Psychological Review




119


1
















When are two heads better than one and why?




A
Koriat




10.1126/science.1216549








Science




336


6079
















Advances in experimental social psychology




J
Krueger




10.1016/S0065-2601(08








30








On the perception of social consensus








From social projection to social behaviour




J
I
Krueger




10.1080/10463280701284645






European Review of Social Psychology




18
















The first cut is the deepest: Effects of social projection and dialectical bootstrapping on judgmental accuracy




J
I
Krueger






L
J
Chen




10.1521/soco.2014.32.4.315








Social Cognition




32


4
















Psychological perversities and populism




J
I
Krueger






D
J
Grüning








The social psychology of populism


J. P. Forgas & K. Fiedler










The 22nd Sydney Symposium on Social Psychology












&
Taylor






Francis














The wisdom of select crowds




A
E
Mannes






J
B
Soll






R
P
Larrick




10.1037/a0036677








Journal of Personality and Social Psychology




107


2
















Using meta-predictions to identify experts in the crowd when past performance is unknown




M
Martinie






T
Wilkening






P
D
Howe




10.1371/journal.pone.0232058








PloS one




15


4














Perfectly confident: How to calibrate your decisions wisely




D
A
Moore








Harper Collins






of Vox Peritorum








A new look at defensive projection: Thought suppression, accessibility, and biased person perception




L
S
Newman






K
J
Duff






R
F
Baumeister




10.1037/0022-3514.72.5.980








Journal of Personality and Social Psychology




72


5
















A Comparison of Small Crowd Selection Methods




H
Olsson






J
Loveday




D. C.


















R
Noelle






A
S
Dale






J
Warlaumont






T
Yoshimi






C
D
Matlock






Jennings








Proceedings of the 37th Annual Meeting of the Cognitive Science Society


& P. P. Maglio


the 37th Annual Meeting of the Cognitive Science Society












Boosting the wisdom of crowds within a single judgment problem: Selective averaging based on peer predictions




A
Palley






V
Satopää




10.2139/ssrn.3504286


















A solution to the single-question crowd wisdom problem




D
Prelec






H
Seung






J
Mccoy




10.1038/nature21054








Nature




541
















The wisdom of crowds




J
Surowiecki












Random House








How to identify experts in a community?




B
Sziklai




10.1007/s00182-017-0582-x








Int J Game Theory




47
















Exact Soft Confidence-Weighted Learning




J
Wang






P
Zhao






S
C
Hoi




10.1145/2932193








Proceedings of the 29th International Conference on Machine Learning


the 29th International Conference on Machine Learning






8














Identify Experts through Revealed Confidence: Application to Wisdom of Crowds




Y
Zhang




10.2139/ssrn.3739192of



















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]