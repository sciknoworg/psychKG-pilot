You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Making decisions about complex datasets can be difficult, even for the savvy statistician. Consequently, data is typically presented to people in the form of figures or data dashboards to allow for quick and (ideally) principled decision-making in a manner that's consistent with the data at hand. Although figures may provide the illusion of easy decision-making, the choice of data visualization is a more complicated art than many realize. In the emerging field of the psychology of data visualization, it's been observed that certain sorts of figures can, in fact, systematically mislead people. For example, only plotting a model rather than how it fits the underlying data can lead people to think the model fits the data better than it does. Likewise, manipulating the axes data is presented on can exaggerate the size of effects which are actually small. These points are perhaps obvious, but even certain types of data visualizations (e.g., bar graphs, violin plots, and so forth) may lead to sub-optimal decision making. For example, Kay and colleagues 
(2018)
 observe that while participants struggle to properly interpret random forest plots or fail to grasp uncertainty altogether when only point estimates are provided about the likely time a bus will arrive, displaying the uncertainty in an estimate using a quantile dotplot improves decision making. One might think then that ever more advanced data visualization methods (that is, methods seemingly more elaborate than providing participants with things like a bar chart with error bars) will improve participants understanding. In fact, some more advanced methods for displaying uncertainty can similarly confuse non-expert users. For example, Ruginiski (2016) observed that the typical visualization used by The National Hurricane Center, a forecast cone, is often misinterpreted. Whereas a wide forecast cone means that there is a high-level of uncertainty about the path of a hurricane, participants interpreted the size of the cone as an indicator of how large and severe the hurricane itself is. Even more advanced, alternative visualizations of hurricane forecasts (such as those that include spaghetti plots), can be misinterpreted by users as indicating a hurricane might specifically strike certain cities that happen to fall along the line of one random posterior draw of a forecast.
These results suggest care needs to be exercised in visualization choice for non-experts. But to what extent do these results apply to expert users of data dashboards.
Although the evidence for this is less direct, there are several indicators that commonly used data dashboard visualizations, like forest plots where significant effects are color-coded green and non-significant effects are color-coded gray, may well also be incorrectly interpretted by more savvy users. Simonsohn and colleagues observed that in academic research by psychologists including confidence intervals in the report of estimates had no impact on people's interpretation of the data. Instead, Simonsohn and colleagues found that was relevant was whether an effect was significant or not -researchers altogether ignored the uncertainty of their point estimates 2014. Indeed, 
Yu and colleagues (2014)
 found that when researchers focused exclusively on statistical significance when deciding whether to collect more data, they increased both the Type 1 and Type 2 error rates 
(Yu et al., 2014)
. These set of results suggest that even savvy users of data, like academic psychologists with PhD level training in statistics, may exhibit some biases in their decision making which lead them to misunderstand data dashboards, and in turn, adopt a decision-strategy which will perform poorly in the long run.
In the present experiments, we sought to address two questions: First, do people actually understand commonly used figures on experimentation platforms? For example, do their judgments accurately reflect the precision of estimated effects? Second, what can the pattern of inferences people make using these data summaries tell us about their decision strategies, including the consequences of those strategies. For instance, does their decision strategy reflect null hypothesis significance testing (NHST) or some cost-benefit analysis compatible with expected utility theory. Three experiments, one with participants with an undergraduate education which included sustained statistics training, one with participants from tech, finance, and similar fields, and one simulation experiment tested these questions.


Study 1a


Methods
Participants. In Study 1a, we recruited 33 participants enrolled in a course covering linear and logistic regression through the University of Edinburgh Psychology Subject Pool (48.5% women, 48.5% men, and 3% who preferred not to indicate their gender M age = 23 years old). Participants received course credit for participating in the experiment. This sample was used to pilot the materials and measures used in the primary experiment with industry professionals.


Procedure
Study 1a consisted of three parts. In the first part of the study, participants were presented with a brief text description about a researcher who conducted a study and asked them to make a set of decisions on the basis of the results of a report about this study. In the second part of the experiment, participants were provided with similar brief text descriptions about a researcher conducting a study, but also shown a figure accompanying this text description. In the third part of the study, participants completed a short demographics questionnaire which included asking them about their confidence interpreting the results of statistical analyses. Part 1. In the first part of the experiment, we aimed to examine how confident participants are in the results of an experiment when they are only provided with information about whether a statistical test was significant or not. Specifically, participants read about a researcher who evaluated the efficacy of a treatment relative to a control condition. Participants were told:
Imagine a researcher runs a preregistered study (N = [A random number between 5, 000 and 5, 999]) to investigate the efficacy of a treatment relative to a control condition. They observe the treatment was [was not] statistically significantly better than the control condition. Based on this information alone, please answer the following questions.
Participants then answered a series of Likert-scale and multiple choice questions based on this prompt. Specifically, they were asked to indicate how confident they were that the treatment was effective, how uncertain the effect of the treatment was, and how precise the effect of the treatment was.
We then asked them to imagine they had to make a decision based on this experiment: "Imagine you need to make a decision based on these results alone: What would you decide?". The response options were "Do not implement the treatment", "Implement the treatment", "Not enough information to say", and "Continue collecting data to learn more". Participants completed this task for both a significant and non-significant prompt (i.e., two trials).
After completing Part 1, participants advanced to the second part of the study where they were provided with a similar set of short descriptions but were also provided with a figure summarizing the results. We describe the details of this second part below. Part 2. After completing the first part of the experiment, we presented participants with a series of brief descriptions about a researcher who conducted experiments to test the efficacy of two treatments relative to a control condition. Participants were then shown the outcome of the experiment using a forest plot located immediately below the text prompt.
Every text description included three parts: A specification of the sample size, which was randomized to be between 5, 000 and 5, 999, text stating that the experiment was preregistered (which was defined prior to proceeding to the main task), and then an explanation of the color-coding scheme of the figure they were provided with. The prompt stated that "Treatments which are statistically significant relative to the control condition (p ≤ .05) are shown in green. Treatments which are not statistically significant relative to the control condition (p > .05) are shown in gray."
After reading this text, participants were shown a color-coded plot summarizing the results of the experiment. This plot was meant to mirror commonly used plots in data dashboards. A complete example trial is shown below. 
Figure 1
 . This is an example stimulus from the study showing a chart that is very similar to visualizations common in experimentation platforms. The control is represented by the line at 0%, i.e., an average treatment effect (ATE) of 0. And the treatments represented by the 95% confidence intervals of the ATE and a line representing the mean.
The key manipulation across trials in Study 1a, was the results presented in the plot.
We manipulated whether a given treatment was either gray or green (that is, significant or not) and the length of the confidence interval for each treatment. Thus, the design was a 2 (green or gray) × 2 (one treatment or both treatments) × 2 (short or long) within-subjects design. Participants were randomly assigned to only receive six of the eight possible scenarios to prevent participant fatigue. We presented trials to participants in counterbalanced and randomized order.
After each trial, participants completed a series of Likert and multiple choice questions about the treatments. As before, they rated their confidence in, the precision of, and uncertainty of both treatments 1 and 2. They then were asked questions comparing both treatments. As before, we said:
Imagine you need to make a decision based on these results alone: What would you decide?
The response options were: "Do not implement Treatment 1 or Treatment 2", "Implement Treatment 1", "Implement Treatment 2", "Continue collecting data to learn more". We then asked participants to think about treatments 1 and 2: "Think about Treatments 1 and 2. Which treatment is more effective?". The response options were "Treatment 1", Treatment 2, Both treatments are equally effective.
We now want to emphasize some key design decisions in this experiment. First, the two treatments were not statistically significantly different from each other, as indicated by the large overlap in the confidence intervals of both treatments (see 
Figure 1)
. Rather, as we stated in the instructions, if a treatment was colored green that only indicated that the treatment was significantly different from the control condition. Nonetheless, we were interested in whether people would make a common mistake of inferring two things are significantly different from each other because one was significantly different from the control condition whilst the other was not.
Second, as noted above, we never indicated what was being tested so participants could not rely on their priors of the likely effectiveness of the treatment in interpreting the data. Although we think people ought to rely on their priors in interpreting data, under the NHST framework, one's priors should not bear on the statistical interpretation of the data.
Because the aim of our investigation was on how people understand common data visualizations within the NHST framework, it was necessary to eliminate confounding caused by the impact of priors on their responses.
Third, in each trial, we randomly generated the number of samples in the experiment that participants were tasked with learning about. Specifically, the sample size of the study was always between 5, 000 and 5, 999, a sample size we chose based on the fact that in formal power analyses of between-subjects two-sample t-test, 5000+ samples is sufficient to detect any meaningful condition effect (i.e., Cohen's d > .08). Randomizing this number allowed us to prevent the possibility that a specific sample size was responsible for participants' inferences. 


Results
We preregistered the data collection plan and predictions for both experiments at https://aspredicted.org/977_B83.
We predicted that measures of uncertainty, precision, and confidence (e.g., the length of the confidence interval) will be unrelated to the width of green or gray bars. Rather, we predicted that whether an effect was green or gray would primarily determine people's confidence in a given treatment effect and even their conceptions of the precision of this effect. We also sought to explore how people chose among treatments in which one was significant or not, or both were significant but one had a nominally higher mean. To test the first question, we fit a Bayesian cumulative regression model regressing Response (e.g., Likert scale response selection) on color, interval width, question, and their interactions.
The results of these analyses and the raw data plotted relative to the model fit are shown in 
Figure 2
. As indicated by the figure, participants judged that treatments in green were less uncertain, more precise, and they had more confidence in, than those in gray. Whether an error bar was long or short did not materially impact participants' responses, a result consistent with prior literature suggesting that people ignore the width of an interval instead focusing exclusively on statistical significance. 
Figure 2
 . The results of Study 1a showing that participants used statistical significance as a reason to judge treatments as being more precise and less uncertain, and for having more confidence in their measurements. Gray dots represent responses for treatments that were not statistically significant, and green dots for responses for treatments that were statistically significant. The gray and green error bars represent the respective credible intervals.
Next, we examined how participants decided between treatments under various conditions. Here, we will focus on a few key examples of stimuli to illustrate the key findings of these analyses. The results of this series of case studies are contained in 
Figure   ?
.
Consider the stimuli in 
Figure 3
, which indicates that treatment 1 is significant, treatment 2 is significant, but both have longer confidence intervals and are clearly not significantly different from each other. Under this condition, we observed that many participants choose the treatment with the nominally higher mean effect, although there is no null hypothesis testing-based justification for this decision. Further, many participants in this case seem to think that treatment 1 is more effective than treatment 2 based on the central tendency of the two estimates (again, which is not generally accepted under a NHST framework). Likewise, when treatment 1 is significant and treatment 2 is not 
(Figure 4)
, participants reliably prefer the significant treatment over the non-significant treatment, although, again, under a NHST framework you could not reject the hypothesis that they are identical. In contrast, when both treatments are non significant, but the mean of treatment 1 is higher than that of treatment 2, participants seem to think that the treatments are now equally effective, and they do not prefer one treatment over the other. Altogether, these results suggest that participants are operating under a mixed 
Figure 4
 . Top: The stimulus shown to participants where one treatments is statistically significant from control and has a wide confidence interval, and the other is not statistically significant from either the control or the first treatment and has a narrower confidence interval. Bottom: Participants' responses to questions about the experiment represented by the chart.
decision-making strategy, one which is not supported by a null hypothesis testing framework. Namely, if a treatment is significant, participants choose that, and if two treatments are significant, they choose treatments based on the nominally higher mean.
But when neither treatment is significant, they ignore the mean of the intervals, instead realizing that from a NHST perspective, they have no reason to think one treatment is more effective than the other.
In Study 1b, we sought to replicate and extend these findings by considering whether industry professionals with some experience interpreting the results from a data dashboard exhibited the same effects as college students currently enrolled in statistics courses.


Study 1b
Participants. Based on the large effects we observed in the pilot experiment, along with the large effects previously observed in the literature on data visualization, we recruited 100 participants through Connect by Cloud Research. Connect is a curated sample of participants whom reliably pass questions checking their attention and have been rated by other researchers as being high-quality participants. Connect panels allowed us to selectively recruit participants with specific industry experience to take part in our study.
Namely, we required participants to have a Bachelors degree or above, and required that they work in an industry which involved the interpretation of data based on figures.
Specifically, participants were industry professionals working in one of the following fields:
Business Administration and Management, Finance, Government and Public Administration, Information Technology, Marketing and Sales, Science Technology Engineering and Mathematics, and Social Sciences. Participants were paid $30 per hour for completing a study which took approximately 10 minutes.


Procedure
Study 1b was structured identically as Study 1a, but we made a few minor changes based on post-study feedback on the pilot study. First, we removed the question which asked about uncertainty, instead only asking participants to provide Likert ratings about confidence and precision. This question was removed because several participants indicated that the question confused them. Second, we made the study slightly longer by having them complete additional trials. Third, we added demographic questions asking participants to type in a text box the field they worked in, and their experience with A/B testing in industry contexts. We added these questions to ensure that we in fact recruited participants with the industry experience that we sought to recruit. 
Figure 5
 . The results of Study 1b showing that participants used statistical significance as a reason to judge treatments as being more precise, and for having more confidence in their measurements.


Results
We directly replicated all of the results we observed in the pilot experiment. The results of the key analyses are shown in 
Figures 5 and ?
. We again observed that participants were insensitive to the length of the confidence interval when indicating their confidence and their interpretation of the precision of the estimate. Rather, confidence and precision was entirely driven by whether the effect was significant or not. Likewise, we again observed that participants operated with a mixed decision-making strategy, one which is only weakly supported by a frequentist framework in a narrow set of contexts.
These strategies we observed we sought to investigate further in Study 2.


Study 2: Simulating different decision-making strategies


Methods
In the studies above we observed some study participants implement a "mixed-strategy" for deciding which treatment should be selected as the winner.
Specifically, participants used statistical significance as a filter after which they selected the treatment with the highest expected value. We are able to use the data to see this mixed-strategy in individuals since we are using a within-subject experimental design -the same participants relied on a significance testing framework for some decisions whilst choosing the highest expected value under other conditions. As a point of clarification, all of the results where the participants selected the treatment with the highest expected value, that treatment also had the highest upper confidence bound. Consequently, we can't know which feature of the data was salient for the decision. Nonetheless, there is no principled significance testing-based justification for this strategy. For example, whereas one primary feature of NHST is error control, this strategy does not permit error control for the multiple hypotheses being tested.
To see how the strategies differ consider 
Figure 6
 and 
Figure 7
. Intuitively the mixed strategy is worse than only using statistical significance or only using expected value since the experimenter does not get the guarantees of statistical significance nor the value maximization of expected value. To demonstrate this we simulate experiments using three strategies for decision making: statistical significance, expected value, and a mixed-strategy.
We do need to make an assumption under the statistical significance strategy. Null hypothesis significance testing does not provide a clear verdict for situations in which two treatments are statistically significant against a control condition, but are not significantly different from each other -which treatment do we "choose" as the winner in this situation?
To address this, we will assume that the experimenter conducts a runoff experiment 
Figure 6
 . Using statistical significance as the decision criterion, assuming: (1) a runoff against the treatments when both have distributions that are statistically significant from control, (2) treatment 1 is the control in the runoff, and (3) the runoff results in no positive, statistically significant difference (thus treatment 1 is chosen) between the two treatments, where one treatment, for the simulations we choose the treatment with the lower expected value, is treated as the control condition. Notice that this does incur more cost for this strategy than for the other two since the experimenter needs to run another experiment on more subjects.
We ran a set of simulations creating three distributions, one for the control and one for each of the two treatments. Each distribution's mean is randomly drawn 
from N (10, 1)
 or in the case where we're simulating an effect of 0.5%, N (10.5, 1). You could imagine the mean is average revenue per user of an online platform, i.e., $10. The simulator then runs 
Figure 7
 . Using mixed strategy for choosing.
10,000 experiments where 50% of the experiments have no true difference between the control and treatments. In each experiment there are 333 draws from each treatment's distribution with a standard deviation of 5, e.g., the samples would be drawn from N (10, 5) representing a $10 average revenue per user, with moderate variance if 10 was the value sampled from N (10, 1). Next, each of the three strategies is applied to the results to see what decision someone operating under a given strategy would make. The outcome of the decision is measured against the other two strategies. For example, if the strategy evaluated is statistical significance, then it's compared against the outcome of the mixed and expected value strategies. In addition, the value captured is recorded for each strategy.
That is, by making the decision, how much additional revenue, if any, would the business expect.


Results
The results of these 10,000 experiments are displayed in 
Figures 8, 9
, and 10 below. 
Figure 8
 . When statistical significance is the standard for making decisions, using either expected value or a mixed strategy for decisions result in worse outcomes. 
Figure 9
 . When the mixed strategy is the standard for making decisions, using either expected value or a statistical significance for decisions result in worse outcomes. 
Figure 10
 . When expected value is the standard for making decisions, using either a mixed strategy or statistical significance for decisions result in worse outcomes.
First, note that all of the strategies performed optimally when measured against their own success criteria. For instance, when the NHST strategy is used, of course it satisfies its own criteria every time. This is represented by the dashed line in the graphs. Second, and importantly, notice that the mixed strategy does worse than the statistical significance strategy when an experimenter's preferred decision framework is significance testing.
Likewise, the mixed strategy does worse than an expected value strategy when operating within an expected value framework. This is intuitive, but the simulations highlight the extent to which this is problematic. In particular, the mixed strategy performs about 92% as well as the statistical significance strategy and 84% as well as the expected value strategy when comparing against their criteria respectively. That means that if the experimenter wanted the error guarantees of NHST they wouldn't be getting them by using a mixed strategy and likewise if they wanted the value maximization of an expected value approach they would not be getting that either.
To make these results more concrete, we assume that the value of the variable being observed is revenue per user so that we can show what the business value of each strategy is. Again, the mean is $10 which represents an average of $10 per user in revenue.
Assuming this we can look at the relative cost of each strategy by looking at the accumulated value over all experiments.  The results which show that using the expected value strategy is best are not surprising givent that much of the current research on A/B Testing is coming down decidedly against hypothesis testing 
(Manski (2019)
, 
Sudijono, Ejdemyr, Lal, and Tingley (2024)
). Also, recall that in our simulation NHST has an unfair advantage in that we collect more data when both treatments are statistically significant against the control yet it still does worse than the expected value strategy. With more data we expect that the other strategies would perform better and show a greater difference from NHST.


Discussion
In the studies above we observed two significant behaviors of users working with data with typical data visualizations. First, users consistently reported levels of precision in the data that was not warranted by the visualizations. This is consistent with prior research in reporting uncertainty to consumers of data. Second, some study participants implemented a "mixed-strategy" for deciding which treatment should be selected as the winner.
Specifically, participants used statistical significance as a filter after which they selected the treatment with the highest expected value.
Both of these affect how business make decisions. When users of experimentation dashboards misunderstand the precision or confidence they have in measurements they will report those to their stakeholders incorrectly and potentially make business decisions not warranted by the data. Likewise, we showed in Study 2 that using inconsistent criteria for decision making, as we observed subjects in Studies 1a and 1b do, is worse than using only NHST or only expected value.
As experimentation practitioners in industry we suspect participants were not intentionally following the mixed-strategy. That is, we believe they likely considered themselves to be making decisions in accordance with a strict statistical significance framework. Notice that most experimentation tools in industry support running multiple treatments against a control condition, but only present statistical significance against the control rather than making explicit whether all pairwise comparisons are statistically significant. In fact, this is the type of results visualization we used in the studies above and is the reason we used them, viz. to represent how most experimentation tools in industry visualize experimentation results.
An interesting note is that the mixed strategy results is not just worse when compared to the other strategies, but has markedly inconsistent decision making outcomes.
Imagine a product manager at Company X who has an engineering team developing various alternative features to experiment with. Suppose they have two new features.
There are two possible scenarios. First, both features might be ready to experiment on at the same time. Second, one might be ready to experiment on earlier. Suppose that both treatments would be statistically significant relative to a control treatment, but are are not statistically significantly different from each other.
In scenario 1, if they ran both treatments at the same time and used the mixed strategy, they could just pick the treatment with the highest expected value. However, in scenario 2, where they ran two experiments sequentially, in the first experiment they would pick the treatment as the winner. Then when the second version of the feature was ready for testing, they would run that treatment against the previous winner, now the new control. But because they don't achieve statistical significance between these two variations of the feature, they would select the control (because they failed to reject the null). This is a clearly an incoherent strategy for deciding which treatments to implement, and in turn, likely to be a source of needless waste at many companies relying on similar sorts of data dashboards. In contrast, neither a strict statistical significance approach where a runoff as described earlier is performed nor the expected value approach have this difficulty, viz. neither produce strictly incoherent results.
Clearly there is more work to be done here to determine what visualization does result in the best decisions for a company. We find papers like those mentioned above by Kay and others to be compelling alternatives though they don't exactly address the A/B testing question. While there are some A/B testing specific thoughts on visualization such as 
Cunningham (2023)
 which has an interesting suggestion for visualizing priors that is worth testing, we believe this to be a fruitful area for research especially given the high potential impact it can have on businesses.
Part 3 .
3
After completing the first two parts of the experiment, participants answered a series of demographic questions, including answering questions about their comfort with and familiarity with interpreting data and figures commonly used in academic papers.


Figure 3 .
3
Top: The stimulus shown to participants where both treatments are statistically significant from control and have wide confidence intervals. Bottom: Participants' responses to questions about the experiment represented by the chart.


Table 1
1
shows how much business value is added


Table 1 :
1
Effect multiplied in bottom 2 rows to show the magnitude of switching strategies for a business.














Experiment interpretation and extrapolation




T
Cunningham






















Uncertainty displays using quantile dotplots or cdfs improve transit decision-making




M
Fernandes






L
Walls






S
Munson






J
Hullman






M
Kay








Proceedings of the 2018 chi conference on human factors in computing systems


the 2018 chi conference on human factors in computing systems


















Treatment choice with trial data: Statistical decision theory should supplant hypothesis testing




C
F
Manski








The American Statistician




73


S1




















I
T
Ruginski






A
P
Boone






L
M
Padilla






L
Liu






N
Heydari






H
S
Kramer














Non-expert interpretations of hurricane forecast uncertainty visualizations




S
H
Creem-Regehr








Spatial Cognition & Computation




16


2
















The impact of adding confidence intervals to research reports




U
Simonsohn






L
Nelson






J
Simmons






















Optimizing returns from experimentation programs




T
Sudijono






S
Ejdemyr






A
Lal






M
Tingley












Forthcoming








When decision heuristics and science collide




E
C
Yu






A
M
Sprenger






R
P
Thomas






M
R
Dougherty








Psychonomic bulletin & review




21

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]