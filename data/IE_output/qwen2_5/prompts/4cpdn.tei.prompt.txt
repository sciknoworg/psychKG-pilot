You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
As algorithmic decision making is increasingly used in every walk of life (e.g., hiring, lending, online advertising, online learning, criminal justice), many researchers have raised fairnessrelated concerns with such algorithms 
(Corbett-Davies & Goel, 2018)
. A significant concern surrounding automated decision-making algorithms is that they may produce unconscious bias in decision making against vulnerable subgroups 
(Mehrabi et al., 2021)
. That is, an algorithm may give an unfair advantage to one subgroup, such as whites, over another subgroup, such as blacks. Detecting and resolving such fairness-related biases in machine learning algorithms have motivated an area of research now called algorithmic fairness 
(Mitchell et al., 2021)
. Broadly speaking, this rapidly emerging literature provides formal, quantifiable measures of fairness, such as statistical parity and separation, which can be used either to diagnose existing algorithms or to inform designs of new algorithms 
(Barocas et al., 2019;
Corbett-Davies et al., 2017;
Dwork et al., 2012;
Feldman et al., 2015;
Mitchell et al., 2021;
Pessach & Shmueli, 2022)
; see details in Section 2. The overarching goal of this paper is to propose a new theoretical and practical framework for evaluating algorithmic fairness based on differential item functioning (DIF), which has been commonly used to measure item bias in test development and psychometrics.
Although discussions on algorithmic fairness are recent, the concept of test fairness was formulated in the 1960s and has evolved over the past six decades. Psychometricians have developed DIF frameworks and relevant methods to measure the fairness and validity of tests at the item level 
(Angoff & Sharon, 1974;
Cleary & Hilton, 1968;
Crocker & Algina, 1986;
Pine, 1977)
. Briefly, an item is considered to exhibit DIF if the item behaves or functions differently across groups of examinees (often, focal versus reference groups) after accounting for examinees' ability; see equation 
5
for a formal definition. Group categories considered in DIF often include gender, race, or ethnicity in the social equality context, and common methods to detect the presence of DIF include the Mantel-Haenszel test 
(Shealy & Stout, 1993)
, logistic regression 
(Swaminathan & Rogers, 1990)
, and item-response-theory-based methods, e.g., area measures 
(Kim & Cohen, 1991;
Raju, 1988)
 and residual-based DIF 
(Lim et al., 2022)
. When an item is detected to have DIF, the item is typically reviewed by content experts for revision or removal from the item pool. DIF analysis is an essential component of standardized tests developed in the United States, such as the SAT, ACT, Graduate Record Examinations (GRE), and Graduate Management Admissions Test (GMAT). But, to the best of our knowledge, there is no research on harnessing the concept of DIF and its detection methods to uncover potential fairness-related harms in modern, automated algorithms.
In this paper, we propose a DIF-based approach to assess algorithmic fairness in modern, machine learning algorithms. In a nutshell, our approach, which we call differential algorithmic functioning (DAF), expands existing DIF to encompass algorithmic fairness based on three pieces of information typically available from a modern algorithm: (i) a decision variable, (ii) a "fair" variable, and (iii) a protected variable such as race, ethnicity, or gender. With these pieces of information, an algorithm can exhibit what we call uniform DAF, nonuniform DAF, or neither (i.e., non-DAF, fair). We also modify existing DIF detection methods, notably the Mantel-Haenszel test, logistic regression, and residual-based DIF, to assess the presence of DAF in algorithms; see Section 4 for details of the proposed DAF framework.
Throughout the manuscript, we use an example concerning student grade retention where an automated algorithm assists teachers' decisions on whether a student is retained or promoted. Typically, grade retention is recommended if students make inadequate progress in academic achievement or show developmental immaturity 
(Greene & Winters, 2006;
Jackson, 1975)
. We measure the fairness of such decision-making algorithms using DAF and compare DAF to other notions of algorithmic fairness, notably statistical parity.
The remainder of this paper is organized as follows. Sections 2 and 3 briefly review algorithmic fairness and DIF, respectively. Section 4 discusses our DAF framework. Section 5 shows the empirical results about designing a new, fair algorithm that assists teachers' decisions to retain a student or not. Discussion and conclusions are in Section 6.
2 Review: Algorithmic Fairness


Notation
Suppose we have a classification algorithm that is trained on data from N study units, indexed by i = 1, 2, . . . , N . Each study unit's data consists of features/covariates V i ∈ V and a binary outcome Y i . For evaluating algorithmic fairness, the covariates are partitioned into protected variables G i and unprotected variables X i , i.e., V i = (G i , X i ). We define a decision rule δ : V → {0, 1} which takes on two possible actions based on
V i , i.e., D i = δ(V i ). If a correct decision is made, Y i = D i .
The goal for a classification algorithm is to find a decision rule that makes correct decisions.
For example, in the case of grade retention, Y would be a student's retention status where 0 indicates that he/she/they were promoted and 1 indicates that he/she/they were retained. V would be a student's characteristics in the kindergarten year, and D would be the algorithm's decision to retain or promote a student based on V where 0 corresponds to promoting him/her/them to the next grade and 1 corresponds to retaining him/her/them in the same grade.


Notions of Algorithmic Fairness
We review four common measures of algorithmic fairness: (i) statistical parity, (ii) conditional statistical parity, (iii) separation, and (iv) sufficiency 
(Barocas et al., 2019;
Corbett-Davies et al., 2017;
Feldman et al., 2015;
Mitchell et al., 2021)
. Statistical parity requires that an algorithm's decision be independent of protected group membership, and in the case of binary classification, it is defined as:
P r(D = 1|G = g) = P r(D = 1|G = g ′ ).
(1)
In our retention example, statistical parity means that retention decision rates are equal across sub-populations such as gender or racial groups. Statistical parity is often referred to as demographic parity, disparate impact, or independence 
(Barocas et al., 2019;
Corbett-Davies et al., 2017;
Mitchell et al., 2021)
. Statistical parity pursues equality of outcomes/results and does not account for intrinsic characteristics of each individual, which may ultimately decrease the overall prediction performance (e.g., accuracy, recall) of the algorithm for all groups 
(Xu et al., 2022)
. For example, among racial groups, if black students were more likely to be retained than white students during kindergarten for some reasons, it would be reasonable to consider the actual racial differences for the retention predictions in the kindergarten year. But statistical parity may prevent an algorithm from reflecting this intrinsic difference. Conditional statistical parity requires that an algorithm's decision be independent of protected group membership after controlling for a set of "legitimate" risk factors L = l(X) 
(Corbett-Davies et al., 2017)
. Formally, it is defined as:
P r(D = 1|L, G = g) = P r(D = 1|L, G = g ′ )
(2)
This notion aims to treat people who are similar in their legitimate risk factors similarly regardless of group membership. For example, among students who had the same developmental immaturity, black and white students are retained at equal rates. It should be noted that achieving conditional statistical parity does not always guarantee statistical parity (and vice versa), in particular if legitimate risk factors are correlated with protected variables. Separation requires that an algorithm's decision should be independent of protected group membership conditional on the outcome. Formally, it is defined as:
P r(D = 1|Y = y, G = g) = P r(D = 1|Y = y, G = g ′ ), y ∈ {0, 1}.
(3)
Here, P r(D = 1|Y = 1, G = g) represents the true positive rate among group g and P r(D = 1|Y = 0, G = g) represents the false positive rate among group g. Separation is also called error rate balance or equalized odds 
(Chouldechova, 2017;
Hardt et al., 2016)
, and there are relaxed versions of separation, say satisfying equation 
3
only with y = 1 (or y = 0) ( 
Barocas et al., 2019)
. Separation would treat people with the same outcomes similarly regardless of group membership. In our retention example, separation is satisfied when black students and white students have the same false positive rates.
Sufficiency requires that the outcome be independent of the group conditional on the algorithmic decision:
P r(Y = 1|D = d, G = g) = P r(Y = 1|D = d, G = g ′ ), d ∈ {0, 1}
(4)
Here, P r(Y = 1|D = 1, G = g) represents the positive predictive value among group g and P r(Y = 1|D = 0, G = g) represents the false discovery rate among group g. For example, a retention algorithm will satisfy sufficiency when black and white students who are recommended retention are actually retained at the same rate. We also make some general remarks about existing fairness criteria in the literature. First, it is impossible to satisfy all the fairness notions simultaneously because some are inherently in conflict; see 
Chouldechova (2017)
, 
Berk et al. (2021)
, and 
Kleinberg et al. (2017)
. Second, there is no consensus as to what notion of fairness should be used in each context. Instead, researchers need to select fairness notion(s) that are the most appropriate in their own context 
(Xu et al., 2022)
.
3 Review: Differential Item Functioning DIF has been widely used to detect items that exhibit discriminatory bias in assessments 
(Lim et al., 2022)
. DIF refers to different functioning of items across different groups of examinees 
(Holland & Wainer, 1993)
, and it typically means the difference in the probabilities of endorsing an item between groups conditional on ability 
(Magis et al., 2010;
Pine, 1977)
, i.e.,
P r(Y = 1|θ, G = g) ̸ = P r(Y = 1|θ, G = g ′ ),
(5)
Here, Y represents whether an examinee's response to the test item is correct (i.e., Y = 1) or incorrect (i.e., Y = 0); θ represents ability scores, and G represents group membership that consists of a focal group (i.e., G = g) and a reference group (i.e., G = g ′ ). In the DIF literature, the focal group represents the particular group of interest who is expected to be disadvantaged by the test, whereas the reference group represents the group who is expected to have an advantage 
(Holland & Wainer, 1993)
. Typically, test developers investigate the presence of DIF to create a test where the performance of examinees is only affected by their abilities and not by other factors like examinees' demographics 
(Ackerman, 1992)
. They assume that if there exists DIF, the item discriminates the examinees mainly (or partially) based on their group membership 
(Holland & Wainer, 1993)
. There are two common types of DIF: uniform DIF and nonuniform DIF 
(Mellenbergh, 1982;
Swaminathan & Rogers, 1990)
. A test item exhibits uniform DIF when the item is always more advantageous to one group (e.g., whites) than another group (e.g., blacks), showing a higher probability of correctly answering the item at any ability level. In contrast, a test item exhibits nonuniform DIF when the advantage in the item depends on ability level, and it often results in an interaction between ability and group membership.
A wide array of statistical methods have been developed to evaluate the presence and impact of DIF. They can be categorized into two streams depending on whether the methods rely on item response theory (IRT) or not 
(Magis et al., 2010)
. Non-IRT-based methods match examinees based on their test scores, and some of the most popular methods include the Mantel-Haenszel test 
(Holland & Thayer, 1986)
, logistic regression 
(Swaminathan & Rogers, 1990)
, and simultaneous item bias test 
(Shealy & Stout, 1993)
. In contrast, IRT-based methods estimate examinees' latent ability, and some of the most popular methods include Lord's χ 2 
(Lord, 1980)
, area measures 
(Kim & Cohen, 1991;
Raju, 1988)
, and most recently, residual-based DIF method 
(Lim et al., 2022)
.
To better illustrate our procedures in later sections, we review three DIF detection methods: the Mantel-Haenszel test, logistic regression, and residual-based DIF. First,the Mantel-Haenszel test is based on a contingency table where the rows of the table correspond to group membership (focal group G = g versus reference group G = g ′ ) and the columns correspond to correct (Y = 1) or incorrect (Y = 0) responses. After discretizating ability scores into K non-overlapping strata (k = 1, 2, . . . , K), the Mantel-Haenszel test computes the differences in the responses between the two groups at each k-th stratum of ability. Under the null hypothesis that the item is non-DIF, the Mantel-Haenszel test has a chi-squared null distribution with one degree of freedom, and if the test statistic exceeds a critical value based on the null distribution, an item exhibits DIF 
(Holland & Thayer, 1986;
Magis et al., 2010)
.
Second, the detection method based on logistic regression regresses the item response Y i on ability/test scores θ i , group membership G i , and their interaction (i.e., θ i G i ) 
(Swaminathan & Rogers, 1990
) as:
logit(π i ) = β 0 + β 1 θ i + β 2 G i + β 3 θ i G i (6)
where π i is the probability of getting the studied item correct. The term β 2 represents the main effect coefficient of G i , and the term β 3 represents the coefficient of the interaction effect between G i and θ i . The null hypothesis that the item is non-DIF (i.e., β 2 = 0 and β 3 = 0) is rejected if either β 2 or β 3 is significant through a likelihood ratio test. Unlike the Mantel-Haenszel test, the logistic regression can differentiate uniform DIF and nonuniform DIF by individually testing β 2 and β 3 via Wald or likelihood ratio tests. The item is uniform DIF if β 2 ̸ = 0 and β 3 = 0 and is nonuniform DIF if β 3 ̸ = 0 regardless of the value of β 2 . Finally, Lim et al. 
2022
propose a residual-based DIF (RDIF) procedure to detect the presence of DIF by using an IRT model. Specifically, first, the residual-based DIF procedure fits an IRT model. 1 Second, it obtains examinee i's residuals from the estimated IRT model r i = Y i − Y i where Y i is the prediction from the fitted IRT model. Third, the residuals are used to compute three different statistics: RDIF R , RDIF S , and RDIF RS . The first DIF statistic RDIF R is the difference of mean raw residuals between the focal group and the reference group, and it follows asymptotically a normal distribution. RDIF R has (statistical) power to detect uniform DIF as 
Lim et al. (2022)
 showed via the simulations. The second DIF statistic RDIF S is the difference of mean squared residuals between the two groups and also follows asymptotically a normal distribution. RDIF S has power to detect nonuniform DIF. The third DIF statistic RDIF RS is a weighted combination of the two test statistics RDIF R and RDIF S , and it follows asymptotically a chi-squared distribution with two degrees of freedom. RDIF RS is designed to detect any type of DIF 
(Lim et al., 2022)
; see 
Lim et al. (2022)
 for more details. 4 Our Proposal: Differential Algorithmic Functioning


Definitions
We propose a DAF framework to assess the fairness of algorithmic decision making by modifying the notion of DIF and its detection methods. Under the DAF framework, a fair algorithm should not make discriminatory decisions based on protected variables (e.g., gender, race/ethnicity) after accounting for fair attribute W , where W is some function of X, i.e. W = h(X), h : R px → R pw . The fair attribute W means a set of justifiable variables that are important and valid in decision making processes, and it can be continuous or discrete.
We define DAF as conditional dependence of algorithmic decision D and group membership G given fair attribute W . Following the DIF literature, we refer to the focal group (G = g) as the group anticipated to be disadvantaged by the algorithm and the reference group (G = g ′ ) as the group who is anticipated to have an advantage, though the designation does not affect a DAF analysis. Formally, DAF and non-DAF are written as:
DAF : P r(D = 1|W, G = g) ̸ = P r(D = 1|W, G = g ′ ),
(7)
Non-DAF :
P r(D = 1|W, G = g) = P r(D = 1|W, G = g ′ ).
(8)
In words, an algorithm exhibits DAF if the probability of receiving the treatment decision is different across groups after accounting for the fair attribute; otherwise, the algorithm is non-DAF. Our DAF notion does not pursue fairness of outcomes/results (i.e., the objective of statistical parity). Rather, it aims to highlight the fairness of the process with respect to decision allocations by treating individuals with the same fair attribute similarly. Also, the DAF notion is related to the negation of an existing fairness notion known as conditional statistical parity 
(Corbett-Davies et al., 2017)
. Specifically, DAF and conditional statistical parity are identical if legitimate risk factors for conditional statistical parity are the same as the fair attributes identified in DAF analysis. However, DAF provides a more detailed description of disparity patterns by defining different types of DAF; see 
Table 1
. Borrowing from the DIF literature (e.g., 
Hanson, 1998)
, we define two types of DAF, uniform DAF and nonuniform DAF. Uniform DAF exists when the statistical relationship between D and G is constant for all levels of W . For example, a statistical relationship between D and G can be expressed using odd ratios: ∆(W ) := P r(D=1|W,G=g)(1−P r(D=1|W,G=g ′ ))
(1−P r(D=1|W,G=g))P r(D=1|W,G=g ′ ) . In such a case, uniform DAF is defined to exist if ∆(W ) = c ̸ = 1 for every value of W where c is a constant but not equal to one; note that ∆(W ) = 1 achieves the conditional independence of D and G given W , i.e., non-DAF. In contrast, nonuniform DAF is DAF that is not uniform DAF; see 
Figure  1
 for illustrations based on simulated data. When an algorithm is uniform DAF, the algorithm is consistently more advantageous to one group than the other group by recommending more 
Figure 1
: Illustrations of decision characteristic curves for the reference group (whites) and focal group (blacks) with different types of differential algorithmic functioning (DAF) favorable decisions to one group across the entire range of the fair attribute; that is, it shows static disparity with respect to decision allocations. In contrast, an algorithm is nonuniform DAF when the advantage for one group in the algorithm depends on the fair attribute. That is, a decision may favor one group within a certain range of the fair attribute, but may favor the other group within another range of the attribute. As shown in 
Figure 1
, nonuniform DAF would have different steepness between the decision curves of the two groups and result in dynamic disparity in decision allocations. 2


Methods
An important advantage of DAF is that it can be easily tested by borrowing existing test statistics in the DIF literature. To detect the presence of DAF in algorithms, we adopt the aforementioned methods for DIF: the Mantel-Haenszel test, logistic regression, and residualbased DIF. First, we use the Mantel-Haenszel test by discretizing the fair attribute into K strata (k = 1, 2, . . . , K) and creating a contingency table where the rows of the table correspond to group membership and the columns correspond to treatment decision (D = 1) or control decision (D = 0) for each k-th stratum of the fair attribute: 
D = 1) Control Decision (D = 0) Focal (G = g) N g1k N g0k Reference (G = g ′ ) N g ′ 1k N g ′ 0k
Here, N (••)k denotes observed cell frequencies within the k-th stratum of the fair attribute. For example, N g ′ 1k denotes the number of study units who have D = 1 in the reference group at the k-th stratum. Then, the Mantel-Haenszel test computes the differences in the decisions between the focal and reference groups at each k-th stratum of the fair attribute. Under the null hypothesis that the algorithm is non-DAF, the Mantel-Haenszel test for detecting DAF is:
χ 2 M H = | K k=1 N g ′ 1k − K k=1 E(N g ′ 1k )| − .5 2 K k=1 V (N g ′ 1k ) .
(9)
Here, E(N g ′ 1k ) and V (N g ′ 1k ) represents the expectation and variance of N g ′ 1k . 3 If the test statistic exceeds a critical value based on the chi-squared null distribution (e.g., 3.84 for α = 0.05 and one degree of freedom), the algorithm has DAF. Second, the logistic regression procedure requires fitting the following model:
logit(e i ) = α 0 + α 1 W i + α 2 G i + α 3 W i G i ,
(10)
where e i is the conditional probability of unit i's receiving the treatment decision given W and G. The term α 2 represents the effect of the group membership on the decision and α 3 represents the interaction effect between the fair attribute and group membership. We can detect the presence of DAF in a decision-making algorithm by testing the null hypothesis that the algorithm is non-DAF (i.e., α 2 = α 3 = 0) via a likelihood ratio test. We can also detect uniform DAF by testing the null hypothesis α 2 = 0, α 3 = 0 versus the alternative α 2 ̸ = 0, α 3 = 0, with a Wald 2 We note that the slight differences observed in the far left plot of non-DAF are due to sampling variability.
3
E(N g ′ 1k ) = (N g ′ 1k +N g ′ 0k )(N g ′ 1k +N g1k ) N k ; V (N g ′ 1k ) = (N g ′ 1k +N g ′ 0k )(N g ′ 1k +N g1k )(N g1k +N g0k )(N g ′ 0k +N g0k ) N 2 k (N k −1)
. test or a likelihood ratio test. Furthermore, we can detect nonuniform DAF by testing the null hypothesis α 3 = 0 versus the alternative α 3 ̸ = 0. Third, we revise the existing residual-based DIF method 
(Lim et al., 2022)
 to detect DAF. We replace the first step in residual-based DIF based on an IRT model with a more flexible, ensemble learning algorithm from machine learning and use the residuals from the ensemble learning algorithm in the subsequent steps. Note that an IRT model is not suitable in our setting because our outcomes of interest are not item responses. Algorithm 1 summarizes the steps of the residual-based DAF method. A bit more formally, in the first step, we estimate E[D|W ] via machine learning and in particular, the SuperLearner algorithm 
(van der Laan et al., 2007)
 that combines predictions from different supervised learning models. We use a super learning algorithm because an ensemble estimator of functionals like E[D|W ] will perform at least as well as the best individual estimator in terms of the cross-validated error, thereby increasing the prediction performance 
(Porter et al., 2011;
Suk & Kang, 2022;
van der Laan et al., 2007)
. Then, the proposed residual-based DAF method computes three statistics, RDAF R , RDAF S , and RDAF RS , which, similar to their RDIF counterparts in 
Lim et al. (2022)
, are able to detect different types of DAF. The three test statistics are written as:
RDAF R = N i=1 r i I(G i = g) N i=1 I(G i = g) − N i=1 r i I(G i = g ′ ) N i=1 I(G i = g ′ ) (11) RDAF S = N i=1 r 2 i I(G i = g) N i=1 I(G i = g) − N i=1 r 2 i I(G i = g ′ ) N i=1 I(G i = g ′ )
(12)
RDAF RS = Q ′ Σ −1 Q (13) Here, r i represents an individual i's residual, i.e., r i = D i − D i .
Q represents a 2 × 1 matrix that contains differences between the first two test statistics and their respective population means, i.e., Q =
RDAF R − µ R
RDAF S − µ S , and Σ represents a 2 × 2 covariance matrix of RDAF R and RDAF S , i.e., Σ =
σ 2 R σ R,S σ R,S σ 2 S .
Likewise, the first test statistic RDAF R follows asymptotically a normal distribution and is designed to detect uniform DAF. The second test statistic RDAF S follows asymptotically a normal distribution and is designed to detect nonuniform DAF. The third test statistic RDAF RS follows asymptotically a chi-squared distribution with two degrees of freedom and is designed to detect any type of DAF. The residual-based DAF method using software R (R Core Team, 2021) is available in the first author's GitHub repository. 4


Algorithm 1 Residual-based differential algorithmic functioning (RDAF)
Input: Decision D i , fair attribute W i , and group membership G i 1: Fit a super learning algorithm 5 that regresses decision D i on fair attribute W i , and compute its prediction We summarize strengths and limitations of three methods to detect DAF; see also the results of our simulation study that investigated the Type-1 error and power rates of each method in Appendix A. Overall, the Mantel-Haenszel test is a non-parametric test that does not depend on a model and hence, has valid Type-1 error control irrespective of the potentially complex relationship between D, W , and G 
(Sireci & Rios, 2013)
; note that the Type-1 error control does not depend on how the strata are defined so long as they are non-overlapping. But the use of the Mantel-Haenszel test requires discretization of the fair attribute, and it has low power to detect nonuniform DAF. In contrast, the tests based on logistic regression or residual-based DAF have power to detect different types of DAF (i.e., uniform and nonuniform DAF). However, for the tests based on logistic regression, the asymptotic distributions of these tests rely on the correctness of the logistic regression model, which if mis-specified, can lead to Type-1 error inflation. The residual-based DAF method can alleviate concerns for model mis-specification by using ensemble, super learning algorithms. But it is certainly not as simple as the tests based on logistic regression.
D i (i.e., D i = E[D i |W i ]) 2: Compute the residuals, r i = D i − D i . 3
Lastly, we make a few remarks about DAF analysis. First, researchers must use subject matter knowledge to determine which variables are fair attributes. If, however, there is limited subject matter knowledge, researchers may resort to more data-driven measures to choose fair attributes, say those based on changes in R-squared, Gini index, or classification accuracy. Second, DAF, by definition, allows multi-dimensional fair attributes, and the DAF methods above can easily accommodate multiple fair attributes. For example, the Mantel-Haenszel test just needs to create strata (k = 1, 2, ..., K) that are non-overlapping based on the multiple fair attributes. The logistic regression and residual-based DAF approaches need to add multiple fair attributes as predictors in the models. Third, researchers can reduce the dimension of fair attributes using dimensionality reduction tools (e.g., factor analysis, principle component analysis) in particular when multiple fair attributes are highly correlated. As we will see below in Section 5, we use kindergarten year's test scores from the Early Childhood Longitudinal Study-Kindergarten cohort (ECLS-K) as fair attributes. Specifically, tests about math, reading, and general knowledge were conducted in the fall and spring of their kindergarten year, thus producing six test scores during the kindergarten year. To account for multicollinearity among the observed fair attributes, we used factor analysis 
(Gorsuch, 1983;
Lawley & Maxwell, 1962)
 in our empirical example; see the next section. Fourth, our DAF detection methods can be applied to detect other notions of group fairness, such as separation (i.e., equalized odds). Specifically, by replacing the use of W as a conditioning variable with Y , the DAF methods can be used to evaluate whether an algorithm achieves separation or not.


Empirical Example: Retention in ECLS-K


Data and Methods
Decisions to retain students in grade have historically been based on teachers' assessment or test-based assessment 
(Huddleston, 2014)
. But recent algorithmic decision making can be used to assist teachers' decision making processes. Grade retention is typically considered a last-resort option and recommended to students who make inadequate progress in academic achievement or show developmental immaturity 
(Cannon & Lipscomb, 2011;
Greene & Winters, 2006;
Jackson, 1975)
. Prior research found that there are disparities in grade retention where retention has been skewed towards male, ethnic minority, or low-income students 
(Huddleston, 2014;
Xia & Kirby, 2009)
. Given these existing disparities in retention, it is of paramount importance to consider a fairness constraint in a new algorithm for grade retention. We consider DAF as our fairness notion of interest to ensure fairness of the process in algorithmic decision making.
Specifically, we used the ECLS-K data for a retention decision-making algorithm. ECLS-K, sponsored by the National Center for Education Statistics, is a national longitudinal study to examine the school achievement and student experiences from kindergarten to middle school. ECLS-K selected a nationally representative sample of kindergarteners in the fall of 1998 and followed them until the spring of 2007 
(Walston & McCarroll, 2010)
. For data analysis, we used the data collected in the fall and the spring of the kindergarten year (i.e., the fall in 1998 and the spring in 1999) to obtain covariates. We also used the data collected in the spring of 2000 when most of the students were in the first grade to find whether a student was actually retained or not, which is our outcome of interest. Our analytic sample included 11,532 students that allowed for kindergarten retention.
We selected 60 covariates (i.e., V ) that are expected to affect whether a student is retained or promoted to design a decision-making algorithm for retention based on prior works 
(Cannon & Lipscomb, 2011;
Greene & Winters, 2006;
Hong & Raudenbush, 2006;
Jackson, 1975)
; see Appendix B for a list of covariates used in our data analysis. Among them, protected variables included gender (GENDER), race (RACE), ethnicity (WKHISP), and poverty (W1POVRTY), and fair attributes included prior achievement scores in math, reading, and general knowledge, all collected in the kindergarten year (C1RSCALE, C1MSCALE, C1GSCALE, C2RSCALE, C2MSCALE, and C2GSCALE). We summarized the fairness attributes into one variable by using factor analysis to account for multicollinearity and make it easier to demonstrate our DAF framework. This was also done as grade retention should be based on a student's general ability, rather than their performance in a specific subject. Specifically, we conducted the factor analysis using maximum likelihood estimation. The factor scores were used in their original form for the logistic regression and residual-based DAF methods, whereas for the Mantel-Haenszel test, they were categorized into 20 non-overlapping categories based on quantiles. Note that we imputed any missing values in the covariates with predictive mean matching 
(White et al., 2011)
.
To develop a retention algorithm, we fitted random forests 
(Breiman, 2001)
 with 60 covariates as predictors and actual retention status Y i as the outcome of interest. Then, we used unit i's prediction P i to make a decision, i.e., whether to give retention or not. To account for the small number of retained students reported in prior works (e.g., 
Hong & Raudenbush, 2006;
Zill et al., 1997)
 and in our sample, we considered a wide range of threshold values below 0.5 in our analysis to assess which threshold value would exhibit DAF in algorithmic decision making. In particular, we used a set of threshold values, ranging from 0.25 to 0.50 with an increment of 0.05. For example, based on a threshold value of 0.25, our decision is made as: D i = I(P i ≥ 0.25). Then, we assessed the presence of DAF in each working algorithm using three DAF detection methods (i.e., Mantel-Haenszel test, logistic regression, and residual-based DAF) regarding the four protected variables. Our three DAF methods have total seven test statistics: one from the Mantel-Haenszel test (denoted as MH), three from logistic regression, and three from residual-based DAF. The logistic regression procedure in (10) include the following three statistics: Wald statistic for α 2 (denoted as LR α 2 ), Wald statistic for α 3 (denoted as LR α 3 ), and likelihood ratio test statistic for α 2 and α 3 (denoted as LR α 2 ,α 3 ). The residual-based DAF method has three RDAF statistics: RDAF R , RDAF S , and RDAF RS . As a comparison, we considered the statistical parity notion to assess a marginal difference in decision proportions where a two proportion Z-test is conducted (denoted as Z-test).
As for software, we used the R package randomForest 
(Liaw & Wiener, 2002)
 for developing a random-forests-based algorithm, the R package psych 
(Revelle, 2021)
 for factor analysis, and the R package mice (van Buuren & Groothuis-Oudshoorn, 2011) for predictive mean matching.


Results
We include the results of DAF analysis with four protected variables: gender, race, ethnicity, and poverty. 
Figure 2
 summarizes the p-values of three detection methods for DAF and one detection method (i.e., a two proportion Z-test) for statistical parity, with different threshold values. Each row of 
Figure 2
 represents protected variables of interest, and the x-axis within each subplot varies the threshold values used in algorithmic decision making. The red dashed line indicates the p-value of 0.05. If the p-value is below 0.05, there is sufficient evidence of DAF for DAF detection methods (or the marginal difference in decision proportions for statistical parity). As seen from 
Figure 2
, the statistical parity notion that seeks fairness of overall outcomes/results is not satisfied for all combinations of the threshold values and different protected variables. This means that there is the marginal difference in the proportions of retention decisions between the focal group and the reference group within each protected variable. But satisfying statistical parity is not of much interest in designing this retention algorithm, and as mentioned before, we aim to make the working algorithm DAF-free.
DAF results depend on the threshold values, protected variables, and DAF detection methods. All the DAF detection methods detect DAF if the threshold value is less than or equal to 0.40 except for the Mantel-Haenszel test and logistic regression with the poverty variable. More specifically, logistic regression and residual-based DAF methods detect the presence of nonuniform DAF with respect to gender, race, and ethnicity if the threshold value is below or at 0.4; see the results of LR α 3 and RDAF RS . Regarding the poverty level, the residual-based DAF detects the presence of nonuniform DAF if the threshold value is below 0.4, whereas logistic regression does not detect any type of DAF across different threshold values. This difference may be partly due to different power rates of detecting nonuniform DAF between two methods.
Furthermore, we investigated the decision characteristic curves from logistic regression to better understand the presence of DAF between the threshold value of 0.25 (Decision 0.25 ) and 0.45 (Decision 0.45 ). 
Figure 3
 visualizes the decision characteristic curves from logistic regression. Note that omitting the fair attribute of above 2.3 permitted clearer comparison of the logistic regression curves. Regarding gender, race, and ethnicity, the line for the reference group (in gray) does not agree with the line for the focal group (in black) at the threshold value of 0.25. That is, DAF exists, and in particular, nonuniform DAF is clearly shown for the gender variable. But in 
Figure 3
 there seems no DAF in the poverty level at the threshold value of 0.25, which is confirmed from 
Figure 2
 with the logistic regression procedure. At the threshold value of 0.45, the two curves in each subplot generally agree with each other (i.e., show non-DAF), though we observe a somewhat departure at the low extreme for race and poverty. Based on all our findings, we conclude that the working algorithm exhibits nonuniform DAF at some threshold values, and using a threshold of above 0.4 can make it DAF-free.


Discussion and Conclusions
This paper presents a novel framework for evaluating fairness in algorithmic decision making, referred to as DAF. The framework is based on DIF and places emphasis on the fairness of the decision-making process rather than the fairness of the outcomes or results by considering a fair attribute. We define DAF as conditional dependence of algorithmic decision D and group membership G given fair attribute W . Compared to other fairness notions, one of the key innovations of this framework is the ability to distinguish between two subtypes of DAF: uniform DAF and nonuniform DAF. This distinction is made by examining disparities in decision allocations, where uniform DAF exhibits static disparity and nonuniform DAF shows dynamic disparity. This differentiation is crucial as it helps in understanding the underlying disparity mechanisms of algorithms over fair attributes, the impact of unfair bias, and the connection to other fairness concepts such as statistical parity. Moreover, to detect the presence of DAF, we provide three different DAF detection methods: Mantel-Haenszel test, logistic regression, and residual-based DAF. Unlike the Mantel-Haenszel test, logistic regression and residual-based DAF are capable of distinguishing between uniform DAF and nonuniform DAF. The effectiveness of the DAF framework is demonstrated through an application to assess the presence of DAF in an algorithm for grade retention.
Tradeoffs between different fairness notions may be inherent because they fulfill different objectives and are often in conflict. As mentioned above, when comparing statistical parity with DAF, the statistical parity notion pursues the long-term goal of fairness in outcomes or results from algorithms, whereas the DAF notion underscores the fairness in the process of algorithmic decision making by incorporating the fair attribute. Obviously, satisfying the outcome equity via the statistical parity notion does not guarantee satisfying process fairness via the DAF notion (as can be inferred from our simulation study in Appendix A). Therefore, researchers should prioritize a fairness notion that is of most importance depending on their contexts.
While satisfying a particular notion of fairness restricts a set of decision rules in algorithmic decision making, multiple rules may satisfy the given fairness notion. Thus, researchers have to determine which rule is optimal among those satisfying the fairness constraint. In general, one seeks to maximize a certain notion of the prediction performance (e.g., accuracy, area under curve, F1 score) or utility (i.e., a function of the benefit and cost) in designing an algorithm and thus, they can choose an optimal decision rule by accounting for metrics on both fairness and prediction performance (or utility). Also, researchers should avoid designing a fair but ineffective algorithm which is of little use in practice. A working algorithm, despite achieving the fairness, can still give poor results when evaluated against prediction performance or utility metrics.
In choosing a decision rule, a single threshold value might not be a solution to ensure that an algorithm is fair. In this case, researchers may consider setting different thresholds for different groups as in prior works (e.g., 
Corbett-Davies et al., 2017;
Lee & Kizilcec, 2020)
. However, under the DAF framework, using group-specific thresholds may not be justifiable if using group-specific thresholds decreases the perceived fairness of the decision-making process. That is, if individuals who receive decisions view the algorithmic decision-making process unfavorably due to the use of group-specific thresholds, it would go against the objective of DAF notion. Also, it should be noted that using group-specific threshold values is at odds with other fairness criteria such as anti-classification (Corbett-Davies & Goel, 2018) because the decision rule depends on protected group membership.
Based on all the findings of this paper, we provide some suggestions for future research concerning our proposed DAF framework. First, we did not consider intersectionality, i.e., systematic disadvantages along intersecting dimensions, which contain not only gender, but also race, ethnicity, or disability status 
(Foulds et al., 2020)
. Further research will investigate how to consider intersectionality in DAF analysis. Second, while we briefly discussed an optimal decision rule above, we did not formalize how to choose the optimal decision. Future research will examine how to choose an optimal decision rule by considering tradeoffs between DAF and prediction performance or utility metrics. Third, among many other DIF methods, we utilize three DIF methods for DAF analysis. Future research would examine how to modify other DIF methods like simultaneous item bias test 
(Shealy & Stout, 1993)
 for DAF analysis and evaluate their Type-1 error and power. Fourth, we only used the conditional independence tests within the DIF literature and did not consider other statistical methods proposed elsewhere (e.g., 
Azadkia & Chatterjee, 2021;
Neykov et al., 2021)
. Future work could consider these alternative methods. Fifth, DIF is a necessary but not sufficient condition for bias and fairness 
(Angoff, 1993)
. Likewise, algorithms flagged as DAF only have the potential to be unfair. A holistic approach spanning technical and non-technical solutions would be required to scrutinize fairness-related biases inside algorithms, such as 
Mulligan et al. (2019)
's fairness analytic and 
Madaio et al. (2020)
's co-designed checklist for AI fairness. Sixth, we applied an imputation technique to handle missing values in the covariates in the nationally representative ECLS-K data. This imputation may have introduced unwanted bias if the imputed values reinforced any unfair dependencies between the protected attributes, fair attributes, and decisions. Thus, future studies would examine solutions to handle missing data when evaluating fairness in algorithmic decision making. Lastly, in the literature from industrial and organizational (I/O) psychology, ethical decision making is one of the most important research areas (e.g., 
Jarrahi, 2018;
Jones, 1991;
Lefkowitz, 2017)
. Future research would enrich our findings on algorithmic fairness by identifying biases in decision making from the I/O psychology literature and borrowing associated methodology.
While no one-size-fits-all definition suits all systems and contexts, our DAF framework highlights the fairness of decision allocations and provides insights on different patterns of decision disparities from the lens of psychometric testing. We believe that our DAF framework will serve as a useful tool to assess fairness in algorithmic decision making and can be a meaningful starting point that connects the concept of test fairness and algorithmic fairness.


A Simulation Study


A.1 Designs and Evaluation
We conduct simulation studies to assess the performance of three DAF methods with total seven test statistics: one from the Mantel-Haenszel test (denoted as MH), three from logistic regression, and three from residual-based DAF. For the logistic regression procedure in (10), we use the following three statistics: Wald statistic for α 2 (denoted as LR α 2 ), Wald statistic for α 3 (denoted as LR α 3 ), and likelihood ratio test statistic for α 2 and α 3 (denoted as LR α 2 ,α 3 ). For the residual-based DAF method, we use three RDAF statistics: RDAF R , RDAF S , and RDAF RS . As a comparison, we include the statistical parity metric where a test statistic is based on a two proportion Z-test that compares two independent population proportions (denoted as Z-test).
Our simulation study is categorized into four designs; see 
Figure 4
 for illustrations of our simulation designs. Design 1 assumes a non-DAF case where α 2 = 0 and α 3 = 0 in equation (10). Design 2 assumes a uniform DAF case where α 2 ̸ = 0 and α 3 = 0. Design 3 assumes a "balanced" nonuniform DAF case where the group advantage is balanced across the fair attribute, i.e., α 2 = 0 and α 3 ̸ = 0. Design 4 assumes a "unbalanced" nonuniform DAF case where the group advantage is not balanced across the fair attribute i.e., α 2 ̸ = 0 and α 3 ̸ = 0. Specifically, we set α 2 and α 3 to be 0.4 if they are not equal to zero. For each design, we varied the measurement scale of the fair attribute with three different levels: a continuous scale, a discrete scale with 15 unique values, and a discrete scale with 5 unique values. We did this to consider that a fair attribute may not be measured on the continuous scale. Also, for the three scale levels, we formed 20, 9, and 5 intervals in the Mantel-Haenszel procedure, respectively. As for the sample size, we used the fixed value of 2,000 that consists of 1,000 in the focal group and 1,000 in the reference group.


Figure 4: Simulation Designs
For all the designs, we examined the performance of DAF methods by repeating the simulation 1,000 times, and we evaluated the performance of each method by measuring average detection rates ( I(P-value ≤ 0.05)/1, 000). A DAF test statistic that is significant at the 5% alpha level indicates evidence of DAF, and a test statistic for statistical parity is significant at the 5% alpha level indicates evidence of the marginal difference in decisions. Specifically, the average detection rates indicate the power rates if a test statistic is designed for detecting a particular type of DAF (or marginal difference for statistical parity); otherwise, they indicate Type-1 error rates. 
Table 3
 summarizes the average detection rates under Designs 1, 2, 3, and 4. If the average detection rates can be interpreted as Type-1 error, we highlight them in gray in the table. Under Design 1 that assumes non-DAF, the performances of all DAF detection methods-the Mantel-Haenszel test, logistic regression, and residual-based DAF-are similar across different measurement scales of the fair attribute, and they show very low detection rates of DAF, i.e., well controlled Type 1 error rates (.¡07). The statistical parity test (i.e., Z test), which focuses on a marginal difference in decision between groups, also shows very low detection rates under this design.


A.2 Results
Design 2 assumes uniform DAF, and in this design, all the DAF methods generally perform as expected. DAF test statistics that specialize in detecting uniform DAF or any type of DAF show high detection rates (i.e., ¿.9 power rates) across measurement scales of the fair attribute. When we focus on logistic regression and residual-based DAF methods, test statistics for detecting uniform DAF (i.e., LR α 2 and RDAF R ) show slightly higher power rates than test statistics for detecting any type of DAF (i.e., LR α 2 ,α 3 and RDAF RS ). In contrast, test statistics for detecting nonuniform DAF (i.e., LR α 3 and RDAF S ) show low detection rates, which are desirable, but RDAF S shows somewhat over-estimated Type-1 error rates. The statistical parity test shows high detection rates of the marginal difference, but its power rates are smaller than those from any DAF methods.
For Design 3 with balanced unnonuniform DAF, logistic regression and residual-based DAF methods perform well; the test statistics that specialize in detecting nonuniform DAF (i.e., LR α 3 and RDAF S ) show large power rates (¿.8) regardless of the measurement scales, and have higher power rates than the test statistics for detecting any type of DAF (i.e., LR α 2 ,α 3 and RDAF RS ). Also, the test statistics for detecting uniform DAF (i.e., LR α 2 and RDAF R ) show well-controlled Type-1 error rates. However, the Mantel-Haenszel statistics fail to detect nonuniform DAF. This is because the Mantel-Haenszel procedure is not able to detect DAF in particular when the group advantage is cancelled out across the levels of the fair attribute. Also, the statistical parity test shows very low retention rates because no marginal difference is expected under the balanced nonuniform design. Our last design of Design 4 assumes unbalanced nonuniform DAF, and under this design, all the average detection rates are high, ranging from 88.1% to 99.5%. We also find that RDAF S shows high power rates than LR α 3 , and DAF methods generally higher power rates compared to the statistical parity test.
Overall, the logistic regression and residual-based DAF methods perform well by detecting both uniform DAF and nonuniform DAF. Using the Mantel-Haenszel procedure may not be desirable when balanced nonuniform DAF exhibits where the group advantage is cancelled out over the levels of the fair attribute. We also find that satisfying statistical parity does not guarantee achieving DAF-free; specifically, the statistical parity is met even when DAF (in particular, balanced uniform DAF) is present. Note. MH = Mantel-Haenszel, LR = logistic regression, and RDAF = residual-based differential algorithmic functioning (DAF) statistics. The average detection rates are interpreted as power rates when test statistics are designed for detecting a particular type of DAF (or a marginal difference); otherwise, they are interpreted as Type-1 error (highlighted in gray in the table). The true effect size for α on odds ratio scale is 1.49.
56 Spring, K principal report of school being successful in providing help to low achievers S2SUCC7 57 Spring, K principal report of raising performance level of low-achieving students influencing evaluation of principal performance S2PRFLVL 58 Spring, K principal report of teacher and staff support influencing evaluation of principal performance S2STFSPP 59 Spring, K school safety rating K2Q3 60 Spring, K school with decorated hallways K2Q6 A
Figure 2 :
2
P-values of fairness measures about differential algorithmic functioning (DAF) and statistical parity with four protected variables: gender, race, ethnicity, and poverty. For DAF detection methods, p-values below 0.05 provide sufficient evidence of DAF, whereas p-values of 0.05 or above do not provide sufficient evidence.


Figure 3 :
3
Decision characteristic curves from logistic regression between the threshold value of 0.25 (Decision 0.25 ) and of 0.45 (Decision 0.45 ) with four protected variables: gender, race, ethnicity, and poverty.


Table 1 :
1
Types of Differential Algorithmic Functioning (DAF)
Type
Definition
Allocation Pattern
Uniform DAF
The statistical relationship (e.g., odd ratios)
Static disparity
between D and G is constant for all levels of W .
Nonuniform DAF if it is not uniform DAF, but is still DAF
Dynamic disparity


Table 2 :
2
A Contingency Table by Group Membership G and Algorithmic Decision D Within the k-th Stratum of Fair Attribute W
Treatment Decision (


: Compute the three test statistics RDAF R , RDAF S , and RDAF RS and their p-values. Output: RDAF R , RDAF S , RDAF RS , and p-values.


Table 3 :
3
Average detection rates under Designs 1, 2, 3, and 4
Continuous
Discrete: 15
Discrete: 5


An IRT model for the residual-based DIF procedure is a three-parameter model and formalized as:P (Y i = 1; θ i ) = c + 1−c 1+exp(−a(θi−b)). The parameter θ i represents examinee i's ability parameter. The item parameters a, b, and c represent the item discrimination, difficulty/location, and pseudo guessing parameters, respectively.


https://github.com/youmisuk/DAF 5 We include generalized linear models, random forests, and neural network, as default individual estimators for our residual-based DAF approach. If we only select a generalized linear model as an individual estimator insides the super learning method, the predictions are made based solely on the parametric model.








Acknowledgements
The authors thank Dan Bolt for his valuable feedback on the manuscript. This research was partly funded by the National Science Foundation under Grant No. 2225321. The opinions, findings, conclusions, or recommendations expressed in this work are solely those of the authors and do not necessarily reflect the views of the National Science Foundation.






B A list of Covariates
 










A didactic explanation of item bias, item impact, and item validity from a multidimensional perspective




T
A
Ackerman




10.1111/j.1745-3984.1992.tb00368.x








Journal of Educational Measurement




29


1
















Perspectives on differential item functioning methodology




W
H
Angoff








Differential item functioning


P. W. Holland & H. Wainer












Routledge








The evaluation of differences in test performance of two or more groups




W
H
Angoff






A
T
Sharon




10.1177/001316447403400408








Educational and Psychological Measurement




34


4
















A simple measure of conditional dependence




M
Azadkia






S
Chatterjee




10.1214/21-aos2073








The Annals of Statistics




49


6
















Fairness and machine learning




S
Barocas






M
Hardt






A
Narayanan




















Fairness in criminal justice risk assessments: The state of the art




R
Berk






H
Heidari






S
Jabbari






M
Kearns






A
Roth




10.1177/0049124118782533








Sociological Methods & Research




50


1
















Random forests




L
Breiman




10.1023/a:1010933404324








Machine Learning






45














Early grade retention and student success: Evidence from Los Angeles




J
S
Cannon






S
Lipscomb














Public Policy Institute of California








Fair prediction with disparate impact: A study of bias in recidivism prediction instruments




A
Chouldechova




10.1089/big.2016.0047








Big Data




5


2
















An investigation of item bias. Educational and Psychological Measurement




T
A
Cleary






T
L
Hilton




10.1177/001316446802800106








28














The measure and mismeasure of fairness: A critical review of fair machine learning




S
Corbett-Davies






S
Goel




10.48550/ARXIV.1808.00023


















Algorithmic decision making and the cost of fairness




S
Corbett-Davies






E
Pierson






A
Feller






S
Goel






A
Huq




10.1145/3097983.3098095








Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining


the 23rd ACM SIGKDD international conference on knowledge discovery and data mining


















Introduction to classical and modern test theory




L
Crocker






J
Algina








ERIC












Fairness through awareness




C
Dwork






M
Hardt






T
Pitassi






O
Reingold






R
Zemel




10.1145/2090236.2090255








Proceedings of the 3rd innovations in theoretical computer science conference


the 3rd innovations in theoretical computer science conference


















Certifying and removing disparate impact




M
Feldman






S
A
Friedler






J
Moeller






C
Scheidegger






S
Venkatasubramanian




10.1145/2783258.2783311








Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining


the 21th ACM SIGKDD international conference on knowledge discovery and data mining


















An intersectional definition of fairness




J
R
Foulds






R
Islam






K
N
Keya






S
Pan




10.1109/icde48307.2020.00203








IEEE 36th International Conference on Data Engineering (ICDE)


















Factor analysis




R
L
Gorsuch








Lawrence Erlbaum Associates












Getting ahead by staying behind: An evaluation of florida's program to end social promotion




J
P
Greene






M
A
Winters








Education Next




6


2
















Uniform DIF and DIF defined by differences in item response functions




B
A
Hanson




10.2307/1165247








Journal of Educational and Behavioral Statistics




23


3
















Equality of opportunity in supervised learning




M
Hardt






E
Price






E
Price






N
Srebro










Advances in neural information processing systems


D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R. Garnett




Curran Associates, Inc














Differential item functioning




P
W
Holland




10.4324/9780203357811




Wainer, H.






Lawrence Erlbaum Associates












Differential item functioning and the mantel-haenszel procedure




P
W
Holland






D
T
Thayer




10.1002/j.2330-8516.1986.tb00186.x


i-24








ETS Research Report Series




2














Evaluating kindergarten retention policy: A case study of causal inference for multilevel observational data




G
Hong






S
W
Raudenbush




10.1198/016214506000000447








Journal of the American Statistical Association




101


475
















Achievement at whose expense? a literature review of test-based grade retention policies in US schools. Education Policy Analysis Archives




A
P
Huddleston




10.14507/epaa.v22n18.2014








22














The research evidence on the effects of grade retention




G
B
Jackson




10.3102/00346543045004613








Review of Educational Research




45


4
















Artificial intelligence and the future of work: Human-ai symbiosis in organizational decision making




M
H
Jarrahi




10.1016/j.bushor.2018.03.007








Business Horizons




61


4
















Ethical decision making by individuals in organizations: An issue-contingent model




T
M
Jones




10.5465/amr.1991.4278958








Academy of Management Review




16


2
















A comparison of two area measures for detecting differential item functioning




S.-H
Kim






A
S
Cohen




10.1177/014662169101500307








Applied Psychological Measurement




15


3
















Inherent trade-offs in the fair determination of risk scores




J
Kleinberg






S
Mullainathan






M
Raghavan




10.4230/LIPIcs.ITCS.2017.43








8th innovations in theoretical computer science conference


C. H. Papadimitriou






43








:23). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik








Factor analysis as a statistical method




D
N
Lawley






A
E
Maxwell




10.2307/2986915








Journal of the Royal Statistical Society. Series D (The Statistician)




12


3
















Evaluation of fairness trade-offs in predicting student success




H
Lee






R
F
Kizilcec




10.48550/ARXIV.2007.00088


















Ethics and values in industrial-organizational psychology




J
Lefkowitz




10.4324/9781315628721


















Classification and regression by randomForest




A
Liaw






M
Wiener










R News




2


3
















A residual-based differential item functioning detection framework in item response theory




H
Lim






E
M
Choe






K
T
Han




10.1111/jedm.12313








Journal of Educational Measurement




59


1
















Applications of item response theory to practical testing problems




F
M
Lord








Lawrence Erlbaum Associates












Co-designing checklists to understand organizational challenges and opportunities around fairness in ai




M
A
Madaio






L
Stark






J
Wortman Vaughan






H
Wallach










Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems


the 2020 CHI Conference on Human Factors in Computing Systems


















A general framework and an r package for the detection of dichotomous differential item functioning




D
Magis






S
Béland






F
Tuerlinckx






P
Boeck




10.3758/brm.42.3.847








Behavior Research Methods




42


3
















A survey on bias and fairness in machine learning




N
Mehrabi






F
Morstatter






N
Saxena






K
Lerman






A
Galstyan




10.1145/3457607








ACM Computing Surveys




54


6
















Contingency table models for assessing item bias




G
J
Mellenbergh




10.2307/1164960








Journal of Educational Statistics




7


2
















Algorithmic fairness: Choices, assumptions, and definitions




S
Mitchell






E
Potash






S
Barocas






A
D'amour






K
Lum




10.1146/annurev-statistics-042720-125902








Annual Review of Statistics and Its Application




8
















This thing called fairness: Disciplinary confusion realizing a value in technology




D
K
Mulligan






J
A
Kroll






N
Kohli






R
Y
Wong




10.1145/3359221








Proceedings of the ACM on Human-Computer Interaction




3






CSCW












Minimax optimal conditional independence testing




M
Neykov






S
Balakrishnan






L
Wasserman




10.1214/20-aos2030








The Annals of Statistics




49


4
















A review on fairness in machine learning




D
Pessach






E
Shmueli




10.1145/3494672








ACM Computing Surveys (CSUR)




55


3
















Applications of item characteristic curve theory to the problem of test bias




S
M
Pine








Applications of computerized adaptive testing: Proceedings of a symposium presented at the 18th annual convention of military testing association


D. J. Weiss




Psychometric Methods Program










University of Minnesota, Department of Psychology












The relative performance of targeted maximum likelihood estimators




K
E
Porter






S
Gruber






M
J
Van Der Laan






J
S
Sekhon




10.2202/1557-4679.1308








The International Journal of Biostatistics




7


1


31














R: A language and environment for statistical computing. R Foundation for Statistical Computing




R Core Team










Vienna, Austria












The area between two item characteristic curves




N
S
Raju




10.1007/bf02294403








Psychometrika




53


4
















Psych: Procedures for psychological, psychometric, and personality research




W
Revelle










Evanston, Illinois






Northwestern University






R package version 2.1.9








A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF




R
Shealy






W
Stout




10.1007/bf02294572








Psychometrika




58


2
















Decisions that make a difference in detecting differential item functioning




S
G
Sireci






J
A
Rios




10.1080/13803611.2013.767621








Educational Research and Evaluation




19


2-3
















Robust machine learning for treatment effects in multilevel observational studies under cluster-level unmeasured confounding




Y
Suk






H
Kang




10.1007/s11336-021-09805-x








Psychometrika




87


1
















Detecting differential item functioning using logistic regression procedures




H
Swaminathan






H
J
Rogers




10.18637/jss.v045.i03








Journal of Educational Measurement


Buuren, S., & Groothuis-Oudshoorn, K.




27


4










Journal of Statistical Software








Super learner. Statistical Applications in




M
J
Van Der Laan






E
C
Polley






A
E
Hubbard




10.2202/1544-6115.1309








Genetics and Molecular Biology




6


1














Eighth-grade algebra: Findings from the eighth-grade round of the early childhood longitudinal study, kindergarten class of 1998-99 (ECLS-K). statistics in brief. NCES 2010-016




J
Walston






J
C
Mccarroll












National Center for Education Statistics












Multiple imputation using chained equations: Issues and guidance for practice




I
R
White






P
Royston






A
M
Wood




10.1002/sim.4067








Statistics in Medicine




30


4
















Retaining students in grade: A literature review of the effects of retention on students' academic and nonacademic outcomes




N
Xia






S
N
Kirby
























J
Xu






Y
Xiao






W
H
Wang






Y
Ning






E
A
Shenkman






J
Bian






F
Wang


















Algorithmic fairness in computational medicine


10.1101/2022.01.16.21267299














The elementary school perfor mance and adjustment of children who enter kindergarten late or repeat kindergarten: Findings from national surveys (statistical analysis report NCES 98-097)




N
Zill






L
S
Loomis






J
West





















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]