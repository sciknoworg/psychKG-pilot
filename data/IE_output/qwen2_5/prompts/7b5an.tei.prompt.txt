You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Email phishing is a type of cyber social engineering attack in which seemingly legitimate emails attempt to lure the receiver into performing an action with negative consequences (e.g., opening a malicious attachment that installs malware on the victims' device). While the popular conception of phishing is a message from the infamous "Nigerian Prince," modern phishing emails can be hard to distinguish from real emails, with large-scale studies suggesting click rates as high as 20% for the most effective phishing emails 
(PhishMe, 2016;
Williams, Hinds, & Joinson, 2018)
. In part because of this high click rate, phishing is estimated to cost tens of billions of dollars each year 
(Smart People Easier, 2014)
 and is now also recognized as a major public health problem associated with negative health outcomes that include depression and suicide 
(Button, Lewis, & Tapley, 2014;
Fraud Advisory Panel, 2015)
.
While technological solutions, such as filters and blacklists ("Google Safe Browsing," n.d.), massively reduce the number of phishing emails reaching people's inboxes, purely technical defense solutions can never be perfect. This is in part because of the sheer volume of phishing attempts, which are estimated to account for 1 in every 392 emails ("Overview of fraud and computer misuse statistics for England and Wales -Office for National Statistics," n.d.), and because of the sophistication of many phishing emails, which are carefully crafted to avoid filters. Further, phishing messages and corresponding malicious landing pages change constantly (Google, n.d.), adding challenges to machine learning methods that try to filter unwanted messages. Indeed there are even websites (such as ("Test and Optimize your Emails for the Inbox," n.d.)) where attackers can test whether a particular email will be flagged or not by several leading email providers. Thus, human decision-making is the last line of defense against phishing and there is much interest in understanding the cognitive and neural processes underlying phishing detection in order to identify those individuals who are most susceptible 
(Ebner et al., 2018;
Lin et al., 2019;
Oliveira et al., 2017)
 as well as to evaluate the outcome of training programs designed to reduce phishing victimization 
(Norris, Brookes, & Dowell, 2019)
.
Perhaps the Gold Standard for measuring an individual's susceptibility to phishing deception is to actually try to phish them in the home or office setting -that is, to send them a phishing email and measure whether they engage with it or not 
(Caputo, Pfleeger, Freeman, & Eric Johnson, 2014;
Kumaraguru et al., 2009;
Luo, Zhang, Burd, & Seazzu, 2013;
Oliveira et al., 2017;
Vishwanath, 2015;
Williams et al., 2018)
. This direct approach has high ecological validity because the only difference from a real phishing attack is that the participants are not harmed. On the flip side, however, because the experiment occurs outside of the lab, these "field" experiments are less controlled than a lab study and any "in-the-moment" correlates of phishing susceptibility (such as physiologiological and neural measures) are almost impossible to obtain. Another challenge is that the number of emails sent to participants in field experiments is often small because of the need to simulate real-world phishing, where emails might be spaced out over several days to avoid arousing suspicion. This results in limited sampling of each person's susceptibility to attack, and this small amount of data per person makes it difficult to test cognitive models of how individual phishing emails are evaluated and detected.
Another approach is to try to measure phishing susceptibility in the lab. In the most direct version of this approach, the experimenter tries to phish people in the lab, for example by having them browse real and phishing websites to see whether or not they divulge sensitive information 
(Gavett et al., 2017)
. Other lab-based experiments involve roleplaying another person checking their emails 
(Downs, Holbrook, & Cranor, 2007;
Sheng, Holbrook, Kumaraguru, Cranor, & Downs, 2010)
 or rating a series of emails according to how suspicious they appear or how likely the person would be to respond to such an email 
(Alsharnouby, Alaca, & Chiasson, 2015;
Dhamija, Tygar, & Hearst, 2006;
Jones, Towse, Race, & Harrison, 2019;
Kelley & Bertenthal, 2016;
Rajivan & Gonzalez, 2018;
Wood, Liu, Hanoch, Xi, & Klapatch, 2018;
Yan & Gozu, 2012)
. Despite the increased experimental control offered by these lab-based tasks, the extent to which these measures are ecologically valid remains unknown. Given the urgency and magnitude of this growing crisis from phishing on health and well-being, there is a need to document and quantify the relationship between lab and real-world behavior.
In this paper we provide initial evidence that lab and real-world phishing susceptibility are related. Using a new task, the Phishing Email Suspicion Test (PEST), we asked participants to rate the degree to which a series of 160 emails appeared suspicious. By design, 84 of these emails had been previously used in a field-experimental phishing study using an independent sample of participants 
(Oliveira et al., 2017)
. This allowed us to determine whether emails that were effective in real life (i.e., people had fallen for them in the field experiment), were also effective in the lab (i.e., were rated as less suspicious in PEST).
In addition to testing the ecological validity of phishing detection in the lab, PEST allowed us to probe the cognitive mechanisms underlying phishing detection using a computational model. This model was inspired by the similarity between PEST, in which participants evaluate a series of emails according to their level of suspicion, and more traditional tasks from psychophysics, in which participants evaluate a series of physical stimuli (e.g., loudness of sounds or heaviness of weights) according to their perceived properties. Such psychophysical tasks have been extensively studied for over a hundred years, and there are well-established models as to how people make the transformation from stimulus to rating.
One model in particular 
(Ward & Lockhead, 1970)
 suggests that people assign a rating to stimuli by comparing it with previous stimuli. For example, in the case of a loudness rating task, participants compare the loudness of the current stimulus to that of the previous stimulus. If they judge the current stimulus to be louder, they give it a higher rating; if they judge the current stimulus to be quieter, they give it a lower rating. Such a comparison process reveals itself in the form of sequential effects in the ratings, whereby the current rating is biased by an "assimilative" effect of the past rating (i.e., the current rating is biased towards the past rating), and a "contrastive" effect of the past stimulus (i.e., the current rating is biased away from the past stimuli) 
(Ward & Lockhead, 1970)
.
We reasoned that email evaluation may involve a similar comparison process, such that people judge how suspicious an email is by comparing it to previous emails. To test this idea, we used a linear regression model to quantify sequential effects in PEST. In particular, we asked whether there was a positive (assimilative) effect of past rating and a negative (contrastive) effect of past stimulus. More generally, this formal modeling approach allowed us to quantify individual differences in phishing detection in terms of the regression parameters, a potentially more robust measure than "accuracy" because it does not depend on the exact emails participants had seen during the lab task.


Methods


Participants
Ninety seven undergraduates (36 male, 61 female, ages 18-27, mean 18.9, standard deviation 1.39) were recruited from the University of Arizona Psychology subject pool. An additional three participants completed the task as part of their enrollment in the subject pool but were excluded from the analysis because they were under 18 (as required by our subject pool-IRB agreement). The large sample size was designed to give us high statistical power while still being feasible to run within one semester. Participants were not paid for participation, but received course credit for completing the experiment. The study was approved by the Institutional Review Board at the University of Arizona and all participants provided written informed consent.


The Phishing Email Suspicion Test (PEST)
Participants were presented with a series of 160 emails and instructed to classify on a four-point scale from 1, "definitely safe," to 4, "definitely suspicious" via keyboard press (see 
Figure 1
 for schematic display and sample emails). Participants were told to maximize their classification accuracy in order to maximize their "score," although they received no monetary or other reward for their performance. The order of emails seen in the task was randomized for each participant. To minimize learning during the task, participants did not receive feedback about their performance until the end of the experiment. The task was coded using Psychtoolbox for Matlab 
(Brainard, 1997)
. Key press as well as response times were recorded.
Emails varied in their source (real emails vs simulated emails created by us) and legitimacy (safe vs phishing) such that there were four types of email (real-safe, simulated-safe, real-phish, and simulated-phish). Each participant saw 40 examples of each type of email, but because we had more than 40 examples of some emails types, they did not all see the same emails. Overall the 160 emails seen by participants were sampled from 348 emails made up of 140 real phishing emails, 84 simulated phishing emails, 84 simulated real emails and 40 real emails.
Real phishing emails were sampled from a set of one hundred and forty genuine phishing emails obtained from four websites that collect phishing examples ("Information Security at UVA," n.d., "Phishing and Scam Emails: A Realtime Database of Phishing Emails," n.d.; Information Security University of Arizona, n.d.; UCLA Information Security Office, n.d.). These emails were edited to improve relevance for University of Arizona students only in cases where they mentioned specific institutions or locations (e.g., University of Virginia was changed to University of Arizona). The real phishing emails were selected such that they spanned a range of effectiveness, from obvious phishing to more convincing attacks (based on subjective judgments of two of the authors, ZMH and KL). Simulated phishing emails were taken from the eighty four emails used in the PHishing Internet Task (PHIT) 
(Lin et al., 2019;
Oliveira et al., 2017)
, see below for details.
Safe emails also comprised real and simulated messages. Forty real safe emails were taken from our inboxes and included messages from banks and paypal, as well as password resets or general account management. Eighty-four simulated safe emails were adapted versions of the original PHIT emails that were altered to make them seem less suspicious, but matched on other features such as word count, purported identity of the sender and topic. In particular we corrected for poor syntax, grammatical errors, informal language, and aggressive tone in the original PHIT emails.


Figure 1 -The Phishing Email Suspicion Test (PEST).
Participants were presented with a series of 160 emails from one of four categories (real-safe, simulated-safe, real-phish, simulatedphish, 40 emails per category) in randomized order and instructed to rate them on a four-point scale from "definitely safe" to "definitely suspicious." (Top) Schematic of the display seen by participants when evaluating an email. In this case the email is a real phishing email. (Bottom) Examples of the subject line and text from the other types of email: a simulated phishing email, a real safe email, and a simulated safe email.


Real-world phishing efficacy for a subset of emails
Phishing efficacy of the 84 simulated phishing emails had previously been assessed in a field experiment by our group 
(Lin et al., 2019;
Oliveira et al., 2017)
. In this Phishing Internet Task (PHIT), 158 participants were sent emails to the email address they had registered with the study, and a browser plug in recorded whether participants clicked on the link embedded in each email. This data allowed us to construct a measure of phishing efficacy for each email had it occurred in real life, as the proportion of times an email was successful in eliciting a click. In particular, each participant received 21 emails each during the course of 21 days (one per day). Emails had been created based on a large set of authentic spam emails from an independent sample of Internet users 
(Oliveira et al., 2019)
. On average each email was sent to just under 40 participants (range 37-42).


Results
Participants were more suspicious of phishing than safe emails Overall, participants performed above chance in PEST, achieving a mean accuracy (defined as the safe and phishing emails were correctly classified as possibly or definitely safe or phishing) of 62%. Regarding mean suspicion scores for each of the four categories of emails (real-safe, simulated-safe, real-phish, simulated-phish), we conducted a repeated measures ANOVA with email legitimacy (safe/phish) and source (real/simulated) as within-subject factors (Supplementary 
Table 1
). This analysis revealed a main effect of legitimacy (F(1, 288) = 455.1, p < 0.001, partial η 2 = 0.61), a main effect of source (F(1, 288) = 56.3, p < 0.001, partial η 2 = 0.16), and an interaction effect between legitimacy and source (F(1, 285) = 66.5, p < 0.001, partial η 2 = 0.19). Additional analysis further showed that these effects were due to participants rating phishing emails as more suspicious than safe emails (t(96) = 20.3, p < 0.001), simulatedsafe emails as more suspicious than real-safe emails (t(96) = 11.3, p < 0.001), and simulatedphishing emails the same as real-phishing emails (t(96) = 0.48, p = 0.63) 
(Figure 2A
). Despite this difference in overall score between real and simulated safe emails, ratings for both types of simulated emails were highly correlated with ratings of real emails ( 
Figure 2B
, for safe emails r(95) = 0.56, p < 0.001; 
Figure 2C
 for phishing emails r(95) = 0.45, p < 0.001).


Item analysis reveals a wide range of suspicion scores across emails
Averaging across participants allowed us to compute a suspicion score for each email. As shown in 
Figure 3
, there was a wide range of suspicion scores for all four types of email. While safe emails were generally rated less suspicious than phishing emails, there was considerable overlap in the ratings for the four types of emails. For example, the least suspicious phishing email (mean score = 1.46) was ranked almost as safe as the least suspicious safe email (mean score = 1.38). Likewise the most suspicious safe email (mean score = 3.26) was rated more suspicious than the average phishing email (mean score = 2.79). This overlap in suspicion score between phishing and safe emails illustrates how convincing some phishing emails can be. Each dot represents the mean suspicion score given to each type of email by one participant. (A) Mean suspicion score for each of the four different email types (real-safe, simulated-safe, real-phish, simulated-phish). Phishing emails were rated as more suspicious than safe emails and simulated safe emails were rated as more suspicious than real safe emails. Notched box plots represent median, confidence intervals for the median, upper and lower quartiles of suspicion scores and range. * indicates p < 0.001. (B, C) Correlation between suspicion scores for real and simulated emails. Each participant corresponds to a single dot. The correlations between simulated and real safe (B) and simulated and real phishing (C) emails suggest that performance on the simulated emails was positively associated with performance on real emails, for both safe and phishing emails. As a test of the ecological validity of our newly developed PEST, we compared the average suspicion score of each email on PEST to the efficacy of the email as previously assessed in the original PHIT field experimental 
(Lin et al., 2019;
Oliveira et al., 2017)
. By design, 84 emails (the simulated phishing emails) for which we had real-world measures of efficacy were included in PEST. As shown in 
Figure 4A
, we found a negative correlation between PEST and real-world behavior (Spearman's rho(82) = -0.22, p = 0.048) such that emails with a low suspicion score in the lab had been more effective at phishing participants in the real world field experiment.
As a complement to the correlation analysis, we ran a separate analysis in which we split the emails into two groups depending on whether they had been clicked on at all ("clicked", n = 44 emails) or not ("not-clicked", n = 40 emails) in PHIT. Consistent with the correlation analysis, clicked emails were rated as less suspicious than not-clicked emails (t(82) = 2.09, p = 0.04) ( 
Figure 4B
). Together these findings provide initial evidence that PEST may be an ecologically valid measure of phishing behavior in the real world. Correlation between PEST behavior and real-world efficacy of each email in the PHIT field experiment, operationalized as the fraction of times the link in the email was clicked. Emails rated as more suspicious in PEST had been more effective at phishing people in the real world. (B) Alternate analysis separating emails into two groups based on whether they had been successful at phishing at least one Internet user in the PHIT field experiment (i.e., at least one person clicked on the link embedded in the email; "clicked" email) or not (no person had clicked on the link embedded in the email; "not-clicked" email). In line with the correlation analysis, emails that were not clicked in the original PHIT were also rated as more suspicious in PEST. * indicates p < 0.05. 


Regression model of PEST behavior
To further analyze PEST behavior, we built a linear regression model similar to that used in 
(Wilson, 2018)
). This model assumes that participants' ratings for each email are computed according to
! = !"#$ + ! ! + !"#$% !!! + !"#$% !!! (Equation 1)
where ! is the rating on trial , ! is the current stimulus "strength," computed as the average suspicion score across participants for this stimulus, !!! is the previous stimulus, and !!! is the previous choice. The regression weights are the free parameters of the model and denote the overall phish bias ( !"#! ), the effect of the current stimulus ( ! ), the effect of the last stimulus ( !"#$% ), and the effect of the last rating ( !"#$% ). For the purposes of the regression (and to keep the notation in line with our previous work; 
(Wilson, 2018)
), the ratings and suspicion scores were normalized to range between -1 and +1.
Regression weights were estimated separately for each participant using a standard least-squares method (implemented via the glmfit function in Matlab). Results are shown in 
Figure 5
. The largest effect was for the current stimulus with a median regression weight close to 1, consistent with participants' score being correlated with the average score of other people, but with considerable variability across people, consistent with individual differences in phishing susceptibility. This effect of the current stimulus was positive, assimilative, for all participants and statistically significant (t(96) = 27.28, p < 0.001). There was also a significant phish bias. That is, all else being equal, participants were more likely to say that an email was suspicious than safe (t(96) = 4.34, p < 0.001). More subtly, we also saw evidence for sequential effects in the form of a negative, contrastive, effect of the last stimulus (t(96) = -3.53, p < 0.001) and a positive, assimilative, effect of the last rating (t(96) = 4.40, p < 0.001). These effects are consistent with previous work in sequential judgment tasks with no feedback and suggest a comparison process for email evaluation in which the current email is evaluated relative to the last 
(Holland & Lockhead, 1968
).
Figure 5 -Regression model of PEST behavior. Regression weights for the overall phish bias ( !"#$ ), the effect of the current stimulus ( ! ), the effect of the past stimulus ( !"#$% ), and the effect of the past rating ( !"#$% ) are plotted for each participant . Ratings are most strongly influenced by the average suspicion score of the current email (blue), but also show an overall phish bias (red). In addition we see evidence of sequential effects in the form of a contrastive effect of past stimulus (yellow) and an assimilative effect of past rating (green). * indicates p < 0.001. 


Discussion
In this paper, we introduced PEST as a tool for assessing email phishing susceptibility in the lab with ecological validity. In this task participants evaluated a series of 160 emails regarding their level of suspicion. While participants were generally more suspicious of phishing than safe emails, their performance was far from perfect, with participants only classifying emails correctly about 62% of the time. Such a relatively low accuracy is consistent with previous reports, such as 
(Jones et al., 2019)
 who found an accuracy of 68% in a similar task. This low accuracy is indicative of how convincing phishing emails can be. Indeed, when we looked at the average suspicion score given to each email 
(Figure 3
), there was considerable overlap between the suspicion score given to safe vs. phishing emails, with one of the real phishing emails being among the least suspicious of all.
A unique feature of PEST is that the real-world efficacy of 84 simulated-phishing emails had been previously assessed in a field experiment 
(Lin et al., 2019;
Oliveira et al., 2017)
. Thus, by comparing real-world and lab-based efficacy, we were able to test, for the first time, whether email phishing susceptibility in the lab is related to email phishing susceptibility in the real world. In particular, we found that emails that were more effective at phishing people in the real world were rated as less suspicious in the lab, suggesting that PEST can capture at least some of the processes underlying phishing susceptibility in real life. This finding of possibe ecological validity, in turn, lends weight to the idea of using PEST in combination with neuroimaging and other physiological measures (e.g., eye tracking and galvanic skin response) in the lab, to capture the cognitive and neural processes of real-life phishing decision making.
One limitation of our current approach is that the lab-based and real-world measures of email phishing susceptibility were acquired from different participants. This limits the scope of our conclusions such that, while we can say that PEST is effective in assessing the efficacy of individual emails, we cannot say that PEST is effective at determining the phishing susceptibility of individual people. To determine this, we will need a within-subjects design to examine PEST in parallel with a direct phishing measure. These data would allow us to assess the extent to which PEST behavior captures real-world phishing behavior, and which components of PEST (e.g., regression coefficients in Equation 1) correlate with real-world phishing decision-making across participants.
In addition to assessing the ecological validity of PEST, the large number of trials in our experiment allowed us to model trial-by-trial behavior with linear regression. We built a model in which participant ratings were determined by a combination of the current email, a bias towards saying an email is suspicious, and sequential effects from the last email and its rating. Of particular interest were the two sequential effects: a contrastive effect for past stimulus and an assimilative effect for past rating. While sequential effects have not previously been reported in phishing tasks, they are known to occur in other sequential judgment tasks without performance feedback 
(Garner, 1953;
Holland & Lockhead, 1968;
Parducci & Marshall, 1962;
Pegors, Mattar, Bryan, & Epstein, 2015;
Ward & Lockhead, 1970;
Wedell, Parducci, & Edward Geiselman, 1987)
, see 
(Kiyonaga, Scimeca, Bliss, & Whitney, 2017)
 for review.
Guided by this previous literature, the presence of these sequential effects in PEST suggests a model for real-world phishing decision making. In particular, one way in which contrastive and assimilative sequential effects can simultaneously occur if the judgment is made via a comparison process with recent stimuli 
(Holland & Lockhead, 1968
). In the simplest form of this model, which considers only the stimulus and rating on the last trial, the rating on the present trial is computed by adjusting the rating from the last trial in proportion to the difference between the present stimulus and the last stimulus. Written mathematically this implies that ! = !!! + ! + !!! 
(Equation 2)
 hence the past rating, !!! , has a positive, i.e. assimilative, effect on the current rating, while the past stimulus, !!! , has a negative, i.e. contrastive, effect. More generally, if the comparison process is not just with one but the average of several past stimuli, then the rating takes the form 
)
where ! describes the weighting given to the stimulus at time − .
! = ! + ! ( !!! − !!! ) ! (
Equation 3
In line with this equation, including more terms in the regression analysis suggests that the assimilative effect of past rating and the contrastive effect of past stimuli extend multiple trials into the past ( 
Supplementary Figure 2)
. Could real-world phishing decisions made in a similar manner? That is, are phishing decisions made by comparing the current email with exemplars of safe and phishing emails we have seen before? Such a process is certainly possible, but would need more extensive field experiments to test, for example by sending emails in pairs such that the effect of neighboring emails in the inbox on susceptibility to a phishing email could be assessed.
Clearly more work is needed to understand the complex set of processes that underlie one's decision to engage with a phishing email or not. This study has taken initial steps toward the development of a tool for in-lab assessment of email phishing susceptibility, which is efficient in administration and allows data collection to inform the cognitive mechanisms of phishing detection.
Figure 2 -
2
Suspicion scores in PEST. Safe emails are depicted in blue and phishing emails are depicted in red.


Figure 3 -
3
Average suspicion scores for each email type. Each email corresponds to a different dot. Emails were grouped by type (real-safe, simulated-safe, real-phish, simulated phish). Safe emails are depicted in blue and phishing emails in red. While, on average, safe emails were ranked as less suspicious, there was considerable overlap in the mean suspicion score for each type of email.


Figure 4 -
4
Ecological validity of PEST. (A)








Acknowledgements
This work was supported by a pilot grant from the McKnight Brain Research Foundation, NSF grant SBE-1450624, and NIH grant 1R01AG057764-01A1.






Author contributions
RCW, MDG, NCE, DSO, SJG, and BEL conceived the study. ZMH built the experiment with supervision from RCW and MDG. ZMH and KL designed the simulated-safe emails. NCE, DSO, and TL provided the original simulated phishing emails and the PHIT data. ZMH ran data collection with supervision from RCW and MDG. ZMH and RCW analyzed the data. RCW wrote the paper with input from MDG, NCE, DSO, SJG, BEL, ZMH, and VL.
 










Why phishing still works: User strategies for combating phishing attacks




M
Alsharnouby






F
Alaca






S
Chiasson




10.1016/j.ijhcs.2015.05.005








International Journal of Human-Computer Studies




82
















The Psychophysics Toolbox




D
H
Brainard




10.1163/156856897x00357








Spatial Vision




10
















Not a victimless crime: The impact of fraud on individual victims and their families




M
Button






C
Lewis






J
Tapley




10.1057/sj.2012.11








Security Journal




27
















Going Spear Phishing: Exploring Embedded Training and Awareness




D
D
Caputo






S
L
Pfleeger






J
D
Freeman






M
Johnson




10.1109/msp.2013.106








IEEE Security & Privacy




12
















Sex Differences in Emergent Literacy and Reading Behaviour in Junior Kindergarten




S
Deasley






M
A
Evans






S
Nowak






D
Willoughby




10.1177/0829573516645773








Canadian Journal of School Psychology




33
















Why phishing works




R
Dhamija






J
D
Tygar






M
Hearst




10.1145/1124772.1124861








Proceedings of the SIGCHI Conference on Human Factors in Computing Systems -CHI '06


the SIGCHI Conference on Human Factors in Computing Systems -CHI '06
















Behavioral response to phishing risk




J
S
Downs






M
Holbrook






L
F
Cranor




10.1145/1299015.1299019








Proceedings of the Anti-Phishing Working Groups 2nd Annual eCrime Researchers Summit on -eCrime '07


the Anti-Phishing Working Groups 2nd Annual eCrime Researchers Summit on -eCrime '07
















Uncovering Susceptibility Risk to Online Deception in Aging




N
C
Ebner






D
M
Ellis






T
Lin






H
A
Rocha






H
Yang






S
Dommaraju






D
S
Oliveira




10.1093/geronb/gby036








The Journals of Gerontology. Series B, Psychological Sciences and Social Sciences
















Supporting the victims of fraud: The year in review








Fraud Advisory Panel
















An informational analysis of absolute judgments of loudness




W
R
Garner








Journal of Experimental Psychology




46


5
















Phishing suspiciousness in older and younger adults: The role of executive functioning




B
E
Gavett






R
Zhao






S
E
John






C
A
Bussell






J
R
Roberts






C
Yue




e0171620. Google








PloS One




12


2








Disclosing vulnerabilities to protect users across platforms. from Google Online Security Blog website. html Google Safe Browsing. (n.d.








Sequential effects in absolute judgments of loudness




M
K
Holland






G
R
Lockhead




10.3758/bf03205747








Perception & Psychophysics




3






















from Information Security Alerts & Warnings website










Security University of Arizona.






Information Security at UVA. (n.d.. Retrieved








Email fraud: The search for psychological predictors of susceptibility




H
S
Jones






J
N
Towse






N
Race






T
Harrison








PloS One




14


1


209684














Attention and past behavior, not security knowledge, modulate users' decisions to login to insecure websites




T
Kelley






B
I
Bertenthal




10.1108/ics-01-2016-0002








Information and Computer Security




24
















Serial Dependence across Perception, Attention, and Memory




A
Kiyonaga






J
M
Scimeca






D
P
Bliss






D
Whitney








Trends in Cognitive Sciences




21


7
















School of phish




P
Kumaraguru






J
Cranshaw






A
Acquisti






L
Cranor






J
Hong






M
A
Blair






T
Pham




10.1145/1572532.1572536








Proceedings of the 5th Symposium on Usable Privacy and Security -SOUPS '09


the 5th Symposium on Usable Privacy and Security -SOUPS '09
















Susceptibility to Spear-Phishing Emails




T
Lin






D
E
Capecci






D
M
Ellis






H
A
Rocha






S
Dommaraju






D
S
Oliveira






N
C
Ebner




10.1145/3336141








ACM Transactions on Computer-Human Interaction




26
















Investigating phishing victimization with the Heuristic-Systematic Model: A theoretical framework and an exploration




X
Luo






W
Zhang






S
Burd






A
Seazzu




10.1016/j.cose.2012.12.003








Computers & Security




38
















Gender differences in reading motivation: does sex or gender identity provide a better account




S
Mcgeown






H
Goodwin






N
Henderson






P
Wright




10.1111/j.1467-9817.2010.01481.x








Journal of Research in Reading




35
















The Psychology of Internet Fraud Victimisation: a Systematic Review




G
Norris






A
Brookes






D
Dowell




10.1007/s11896-019-09334-5








Journal of Police and Criminal Psychology
















Dissecting Spear Phishing Emails for Older vs Young Adults




D
Oliveira






N
Ebner






H
Rocha






H
Yang






D
Ellis






S
Dommaraju






T
Lin




10.1145/3025453.3025831








Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems -CHI '17


the 2017 CHI Conference on Human Factors in Computing Systems -CHI '17
















Assimilation vs. contrast in the anchoring of perceptual judgements of weight




A
Parducci






L
M
Marshall








Journal of Experimental Psychology




63
















Simultaneous perceptual and response biases on sequential face attractiveness judgments




T
K
Pegors






M
G
Mattar






P
B
Bryan






R
A
Epstein








Journal of Experimental Psychology. General




144


3
















A Realtime Database of Phishing Emails. (n.d.)




Scam
Phishing






Emails










from Phishing and Scam Emails: A Realtime Database of Phishing Emails website
















Enterprise Phishing Susceptibility Report




Phishme




















Gender Differences in Reading Impairment and in the Identification of Impaired Readers: Results From a Large-Scale Study of At-Risk Readers




J
M
Quinn






R
K
Wagner








Journal of Learning Disabilities




48


4
















Creative Persuasion: A Study on Adversarial Behaviors and Strategies in Phishing Attacks




P
Rajivan






C
Gonzalez




10.3389/fpsyg.2018.00135








Frontiers in Psychology




9














Who falls for phish?




S
Sheng






M
Holbrook






P
Kumaraguru






L
F
Cranor






J
Downs




10.1145/1753326.1753383








Proceedings of the 28th International Conference on Human Factors in Computing Systems -CHI '10


the 28th International Conference on Human Factors in Computing Systems -CHI '10
















-NOT-FINAL-1.pdf Test and Optimize your Emails for the Inbox












Smart People Easier






Smart people easier to scam








Empathy matters: ERP evidence for inter-individual differences in social language processing




D
Brink






J
J A
Van Berkum






M
C M
Bastiaansen






C
M J Y
Tesink






M
Kos






J
K
Buitelaar






P
Hagoort










PHISH BOWL/PHISHING SCAMS






7








UCLA Information Security Office












Examining the Distinct Antecedents of E-Mail Habits and its Influence on the Outcomes of a Phishing Attack




A
Vishwanath




10.1111/jcc4.12126








Journal of Computer-Mediated Communication




20
















Sequential effects and memory in category judgments




L
M
Ward






G
R
Lockhead




10.1037/h0028949








Journal of Experimental Psychology




84
















A formal analysis of ratings of physical attractiveness: Successive contrast and simultaneous assimilation




D
H
Wedell






A
Parducci






R
Edward Geiselman




10.1016/0022-1031








Journal of Experimental Social Psychology




23
















Exploring susceptibility to phishing in the workplace




E
J
Williams






J
Hinds






A
N
Joinson




10.1016/j.ijhcs.2018.06.004








International Journal of Human-Computer Studies




120
















Sequential choice effects predict prevalence-induced concept change




R
C
Wilson




10.31234/osf.io/75bpy


















Call to claim your prize: Perceived benefits and risk drive intention to comply in a mass marketing scam




S
Wood






P.-J
Liu






Y
Hanoch






P
M
Xi






L
Klapatch




10.1037/xap0000167








Journal of Experimental Psychology: Applied




24
















Online Decision-Making in Receiving Spam Emails Among College Students




Z
Yan






H
Y
Gozu




10.4018/ijcbpl.2012010101








International Journal of Cyber Behavior




2










Psychology and Learning









"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]