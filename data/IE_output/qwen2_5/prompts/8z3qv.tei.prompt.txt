You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
When humans make choices, they must consider how they affect both current and future outcomes in a given context. This balance between immediate and future rewards is a key feature of foraging decisions, during which agents decide whether to engage with (or harvest) a current reward option or leave in search of a better one. This is a common problem when humans make decisions about, for example, what or where to eat, maintaining relationships, or searching for a job. A key feature of foraging decisions is that the longer one continues to engage with a current option, the more the utility of its rewards diminish. This makes it necessary to consider when to switch to a new resource. In the real world, people typically consider the structure of the environment when searching for a new option. However, most previous foraging research assumes a relative lack of structure. In this study, we show that people account for structure during foraging, and that doing so changes their decision making.
Optimal foraging theory 
(Charnov, 1976;
Stephens & Krebs, 1986)
 has become a popular tool for studying decision making in simple but naturalistic environments. This theory addresses a key subclass of 'patch-leaving' foraging problems, in which agents encounter 'patches' in a given environment at a fixed rate. The optimal policy in this scenario can be implemented using one simple variable: the average experienced reward rate. Specifically, the Marginal Value Theorem (MVT; 
Charnov, 1976)
 states that an optimal agent should leave a current option for a better one when the expected reward of their next harvest falls below a threshold defined by their experienced average reward rate. In most experimental patch-leaving scenarios, all patches share statistical regularities, so that a single stable leaving threshold is effective across them. Indeed, previous work has shown that humans and animals use a threshold policy consistent with the MVT in foraging environments 
(Constantino & Daw, 2015;
Hayden et al., 2011;
Kolling et al., 2012;
Le Heron et al., 2020)
.
Even though the MVT provides a parsimonious decision rule for simple decisionmaking contexts, it does not capture the complexity of many real-world foraging tasks. Most pertinently, the MVT assumes that agents do not have control over what options they encounter when leaving a current option. Instead, it assumes they will encounter a randomly selected new option with the same reward regularities as previous patches (i.e., the same patch type). However, the world is inherently structured, which allows agents to plan where to forage next (K. J. 
Miller & Venditto, 2021)
. Rideshare drivers, for example, can maximize their earnings by considering patterns of customer demand across time and location. Rather than accepting rides at any given location, drivers can learn which areas offer higher fares-like airports or stadiums-and weigh whether it's worth relocating. Because travel takes time, they must balance the potential gain from switching locations against the consistent fares that can be obtained in their current area. This raises the question of how people forage in more complex, structured foraging environments, where they can rely on their knowledge of the task structure to decide when to leave, and plan which option to visit next.
An extensive body of research on reinforcement learning (RL) describes how people are motivated to learn about, and take into account, an environment's structure when making decisions. Here, planning (or goal-directed control) is formalized as model-based RL 
(Daw et al., 2005;
Drummond & Niv, 2020;
Sutton & Barto, 2018)
.
Model-based learners select optimal actions by simulating their consequences in an internal causal model of their environment 
(Daw et al., 2011;
Doll et al., 2015;
Karagoz et al., 2024;
Kool et al., 2016
Kool et al., , 2017
Pouncy et al., 2021)
. This strategy stands in contrast to model-free RL, which simply chooses actions that previously produced rewards 
(Thorndike, 1898)
. Foraging, like other RL problems, requires agents to learn the value of actions in an environment to decide whether to exploit the current option or explore alternatives 
(Alejandro & Holroyd, 2024;
Morimoto, 2019)
. The optimal choice rule proposed by the MVT can be thought of as model-free, because the threshold is a simple running tally of experienced rewards and does not require prospective simulation of future consequences 
(Constantino & Daw, 2015;
Harhen & Bornstein, 2023;
Kolling & Akam, 2017)
. It has been suggested that model-based simulations can be used to predict future reward rates from the current option in foraging scenarios 
(Frankenhuis et al., 2019;
Kolling & Akam, 2017)
. Such an algorithm would explain the encoding of future reward rate predictions in the anterior cingulate cortex observed by 
Wittmann and colleagues (2016)
. However, it does not address model-based planning towards alternative options in foraging contexts.
The MVT is powerful because it provides an efficient learning rule that achieves optimal foraging behavior. At the same time, the more complicated nature of real-world foraging problems demands a more comprehensive approach to understanding patchleaving behavior. Rewards may become predictably more or less volatile 
(Behrens et al., 2007;
Stephens & Krebs, 1986)
, options may stop being available 
(Navarro et al., 2018)
, or environmental conditions can affect the possibility of gaining reward 
(Stephens, 2008)
. Here, we approach this set of questions by asking how a foraging task that allows planning towards subsequent options changes participants' behavioral policy of making stay/leave decisions. This scenario is not captured by the MVT, because it does not use forward simulation to consider how rewarding future options will be.
Two recent studies have investigated foraging in structured environments. 
Hall-McMaster and colleagues (2021)
 found that foraging decisions change when humans can control which patches they visit. Their task included three distinct patches, each of which had its rewards replenished at a different rate. When given the option to revisit patches, participants sought out the ones that replenished the fastest. This suggests that people relied on information about alternative reward options along with the average reward rate during foraging decisions. More recently, 
Harhen & Bornstein (2023)
 demonstrated that people can use model-based state inference during foraging. In their task, participants used reward outcomes to learn the reward structure of a multimodal environment, which allowed them to infer the quality of the current patch. Critically, they found that overharvesting in this task could be explained by the uncertainty that arises in this learning process. Together, these studies demonstrate that people can learn and use a model of their environment to make foraging decisions, and that they are sensitive to explicit representations of alternative rewards outside of the current option.
At the same time, they leave unanswered the question of how the opportunity to plan in structured environments changes foraging behavior. In the study by 
Harhen and Bornstein (2023)
, participants mostly used model-based latent-cause inference to identify the current state. However, their ability to plan was limited because they were not given control over where to travel after leaving a patch. In fact, the transitions in this task were set so that participants typically returned to the same patch type after leaving. In the study by 
Hall-McMaster and colleagues (2021)
, participants directly chose their next option, which prevented a direct assessment of planning behavior.
Here, we build on these studies to explore how model-based planning and foraging interact in structured environments. We developed a novel paradigm that allowed us to simultaneously investigate patch-leaving decisions, planning, and reward learning throughout a limited foraging period. In our task, which combines features of current foraging tasks 
(Constantino & Daw, 2015;
Hall-McMaster et al., 2021)
 and model-based RL tasks 
(Daw et al., 2011;
Kool et al., 2016)
, participants navigate a tropical pirate-themed world consisting of three foraging patches with dynamically and independently changing initial rewards. At each of these patches, represented as visually distinct islands with treasure chests, participants decided between earning reward from a depleting treasure chest or leaving for a new island. When participants decided to leave the current island, they chose between two boats that 'traveled' to the other two islands according to a probabilistic transition structure (i.e., participants could not return to their current island). Participants learned that each boat was connected to the other islands according to a probabilistic transition structure. Specifically, each boat was more likely to travel to one island (through a common transition) than the other (rare transition). This structure allowed us to measure the degree to which participants used model-based planning when making travel decisions.
We found that participants deviated from the strategy proposed by the MVT by including alternative reward options in their patch-leaving decisions. Specifically, if expected rewards at the other patches were high (low), participants left their current patch earlier (later). This behavioral pattern was especially pronounced for participants who used more model-based control during travel decisions. Thus, model-based considerations of alternative rewards drive deviations from the MVT. Finally, we use formal computational modeling to demonstrate that these deviations from the MVT are optimal, but only to a limited degree dependent on choice stochasticity. This study introduces a novel method for studying how planning, exploration, and motivation interact, and provides novel insights into how humans make foraging decisions in more complex environments.


Methods


Subjects
We recruited a sample of 150 younger adults (19-34, mean = 28.1 ± 4.2 years, 59 female, 85 male, and 6 non-binary) for this study using Prolific, an online research crowdsourcing platform. Participants were required to be between 18-35 years old, fluent in English, and not colorblind. There were 23 participants (15.3%) who were excluded from analysis for a variety of reasons. We excluded participants who admitted in the post-task survey that they wrote down the task structure that they were asked to memorize (10), participants who missed the response deadline on 15% or more travel trials (9), participants whose mean reaction time for the travel trial response deadline was at least 2SD below or above the group mean (4), participants who missed the response deadline on 15% or more harvests (7) and participants whose final score fell below 2SD below the group mean (7; reflecting low engagement in the task).
All participants were compensated at a mean rate of $10.38/hour for the task. They were also given a performance bonus of 1 cent for every 30 points earned in the task (mean = $1.38, SD = 0.22, range = $0.59-$1.75).
All participants gave informed consent, and procedures were approved by the Washington University in St. Louis Institutional Review Board. A replication of this study can be found in the Supplemental Materials.


Design
We developed a novel task to investigate the interaction between model-based planning and foraging decisions (using the jsPsych library; de Leeuw, 2015). This task combined features of classic foraging tasks 
(Bustamante et al., 2023;
Constantino & Daw, 2015)
 with those of tasks that measure the deployment of model-based RL.
In classic patch foraging tasks, participants harvest from 'patches' to gain as many rewards as possible within a given time period. In each patch, the rewards returned from each harvest decrease exponentially over time. Therefore, the participant can, at any time, choose to travel to a new patch with replenished rewards. While each forage decision forces participants to incur a typically small time cost for harvesting rewards, choosing to travel to a new patch forces the participant to incur a greater travel time cost. This introduces a tradeoff between harvesting rewards from a current patch and incurring the greater time cost of traveling to a new, replenished patch. The Marginal Value Theorem 
(Charnov, 1976)
 dictates that the optimal strategy in this task is to leave a patch when the expected reward rate on the following harvest falls below a threshold based on the average reward rate of the environment. Most foraging tasks do not provide the option to identify and revisit individual patches because they lack the inherent structure that exists in real-world foraging settings (cf. 
Hall-McMaster et al., 2021;
Harhen & Bornstein, 2023)
. Therefore, they do not allow participants to plan towards future patches.
Our task was partially adapted from the patch-foraging task developed by 
Constantino and Daw (2015)
, as well as from aspects of the foraging tasks from 
Bustamante et al. (2023)
 and 
Hall-McMaster and colleagues (2021)
. Participants were given 30 minutes in a tropical pirate-themed foraging environment to collect as many coins as possible from treasure chests on three visually distinct islands. On each of these islands, participants experienced a 'harvesting phase' in which they decided whether to collect diminishing rewards from that island's treasure chest, or to leave. Importantly, once participants decided to leave the current island, they experienced a 'traveling phase' in which they chose between two boats that allowed them to travel to the two other islands. Participants alternated between foraging and travel phases, harvesting rewards and traveling between islands, for the entire task duration.


Foraging Phase
At the start of each foraging phase, participants saw an animation of their avatar arriving on an island. Then, they were required to press the 'F' key to earn coins on an initial harvest. After this first forced harvest, participants were given the choice to continue harvesting by pressing the 'F' key again, or to leave the island by pressing the 'J' key.
Each harvest lasted exactly 3 seconds. This included a 2-second response window during which participants could make their choice (harvest or leave), and the duration of an animation displaying the harvest action and its results ( 
Figure 1A
). If participants failed to respond within the 2-second deadline, they were shown a reminder to respond faster.
The initial rewards associated with each island changed slowly and independently over time according to a Gaussian random walk (mean = 15, σ = 1, bounds = [5, 25]) pre-generated for every second of the task ( 
Figure 1C
). This allowed participants to track the current value of each island throughout the experiment and to identify which one would yield the greatest return at any given time 
(Daw et al., 2011)
. After the first harvest of each foraging phase, the rewards from each following harvest would exponentially decay. Specifically, each harvest would produce a reward equal to the product of the previous harvest reward and a depletion rate. This depletion rate was randomly sampled from a Beta distribution with a mean value of 0.88 ( = 14.909, = 2.033) on each harvest. Participants were told that each island's treasure chest would always be fully replenished once a participant chose to leave the current island. This way, only the first reward they observed at each island needed to be used to determine its current value. After participants arrived on an island, they chose between collecting reward and leaving (after an initial forced harvest on each island). If participants chose to earn reward, they received feedback about the number of coins they harvested. Each instance of collecting reward lasted 3s. Participants could continue to earn rewards until they chose to leave the island. Then, they chose between two boats, which then took them one of the other islands. Each instance of traveling lasted 10s. B. Probabilistic transition structure. The task consisted of three distinct islands (purple, red, and green) that were connected with boats. Transitions between islands by boat ... were probabilistic, so that 80% of the time, each boat would consistently sail between the same two islands ("common transition"), but the other 20% of the time, a boat could be diverted to the third island ("rare" transition). C. Drifting initial reward dynamics. Three Gaussian random walks (bounds = 
[5,
25]
, σ = 1) determined the initial reward for each island and each second of the task (dotted lines). Each successive harvest would return a reward that was decaying from that initial reward (solid lines). Participants did not receive any reward while traveling between patches (grey sections). D. Example task sequence demonstrating different predictions of model-based vs. model-free behavior. After a rare transition that leads to a high-rewarding island, a model-based agent becomes less likely to repeat the same boat choice when returning to the original island. This is because their knowledge of the transition structure allows them to infer that selecting the other boat is more likely to lead them back to the previously rewarding island through a common transition. A model-free agent, however, becomes more likely to repeat the same boat choice because it previously yielded a positive prediction error.


High reward!


Travel Phase
Each travel phase began when participants chose to leave their current island. Then, they saw an animation of their avatar leaving the current island and arriving on a shoreline. There, they were given 3 seconds to choose from one of the two boats, presented side-by-side, by pressing the 'F' key (for the left boat) or the 'J' key (for the right boat). The boats' positions were randomized on each travel phase. After selecting a boat, participants saw an animation of their avatar moving to the selected boat, leaving the shoreline, sailing across the ocean, and arriving on the destination island ( 
Figure 1A
). Each travel phase lasted exactly 10 seconds (from the decision to leave to the onset of the stay/leave decision on the new island). If participants failed to select a boat within the 3-second deadline, a boat would automatically be selected at random. Each pair of islands was connected by a single boat, identified by the shape of its sail (triangle, oval, rectangular). Thus, when arriving at the shoreline, participants would see the two boats that connected the current island to the other two islands. To capture model-based planning, these connections were stochastic ( 
Figure 1B)
. Specifically, during each travel phase, there was an 80% probability that the chosen boat would travel to the connected island (a "common" transition) and a 20% probability that "rough seas" would divert it to the third island (a "rare" transition). In order to ensure that participants had encoded a correct internal representation of the task structure (Feher da Silva & Hare, 2020), they were extensively trained on traveling between the islands (see Procedure) and required to pass multiple comprehension tests before starting the main experiment.


Task rationale
We used our task to explore the interaction between model-based planning and foraging decisions in structured environments. To accomplish this, we drew inspiration from previous two-stage RL tasks 
(Daw et al., 2011;
Kool et al., 2016)
 by employing a probabilistic transition structure, allowing us to measure participants' use of modelbased planning during travel decisions. We reasoned that after a rare transition and a high reward on the subsequent island, model-based, but not model-free, participants would be less likely to repeat that same boat choice on the original island ( 
Figure 1D)
. This is because model-based agents use the learned transition structure to associate boats with islands and their expected values, making travel decisions based on which option will most likely bring them to the island with the highest expected reward. Thus, after a rare transition and a high reward on the subsequent island, their knowledge of the transition structure would lead them to realize that the alternative boat option would be more likely to bring them to the previously rewarding island (through a common transition).
Model-free agents, on the other hand, maintain reward expectations for each action based only on reward prediction errors. These agents increase the value of selecting actions (boats) that previously led them to more rewarding islands, regardless of whether these rewards were experienced after a common or rare transition. Thus, after a rare transition and a high reward, model-free agents increase the value of the action that produced it (the chosen boat), leading them to become more likely to choose it in the future.
These diverging predictions follow a similar logic to that of the two-step task, in which choices after rare transitions indicate their degree of planning 
(Daw et al., 2011)
. We rely and expand on this rationale in our following analysis of choice behavior.


Procedure
Participants were instructed that they would play the role of an explorer traveling between three distinct tropical islands to collect as much treasure as possible in a limited amount of time (see Supplemental Materials). Participants completed five phases of instructions and a set of practice trials before starting the main task. These phases introduced each component of the task in a step-by-step fashion.
First, participants learned how to collect coins from a treasure chest. They were informed about the slowly changing initial rewards on each island, the decaying nature of the treasure chests, and that the chests would become fully replenished after leaving the island. Second, they learned how to leave their current island. Third, participants were introduced to the boats, and practiced arriving on an island's shoreline and selecting a boat. Fourth, they were taught the transition structure, and completed a series of tests to ensure they had fully encoded it. In this phase, participants practiced traveling to each island separately. Then, they practiced traveling to all three islands in a randomly determined order. To proceed to the next test, participants were required to choose the correct boats several times in a row. Participants were told that it was important for them to remember how all islands were connected, and were asked not to use any external tools (e.g., a piece of paper) to help them. Finally, participants were introduced to the probabilistic nature of the transition structure, and experienced rare transitions in a set of practice trials. As they were introduced to the rare transitions, they were encouraged to continue picking boats that would most likely take them to their intended destination island. Then, the main task started. Participants were truthfully told that they were given 30 minutes to engage with the task.
After completing the task, participants were asked demographic questions, probed about whether they had used external tools to help them remember the task structure, and debriefed.


Analyses
We approached our analyses by separately evaluating data from the foraging and travel phases of the task, using RL to model travel decisions and generalized linear models to assess participants' patch-leaving decisions.


Dual-system RL model
In order to capture the degree of model-based planning during the travel phase, we fit a dual-system RL model to behavior 
(Daw et al., 2011;
Doll et al., 2015;
Kool et al., 2016
Kool et al., , 2017
. This model explains behavior as a mixture of a model-free and a model-based RL strategy. These strategies involve using predicted reward values and reward prediction errors, from sampling reward states in the task, to track and plan across each aspect of the task environment.
Each system consists of a function ( , ) that associates each combination of state and action with estimates of expected future reward. These pairs are maintained for both island-boat pairings encountered during the travel phase and for island-chest pairings encountered during the harvest phase (mirroring the two stages in the two-step task; 
Daw et al., 2011)
. The islands' indices during the travel phase are represented as !,# , and as $,# during the foraging phase, with t representing the trial number.
Model-free system. The model-free component updates reward expectations based on reward prediction errors using the SARSA temporal difference learning rule 
(Sutton & Barto, 2018)
. Here, the prediction error following a boat choice is calculated as the difference between the expected value given by the agent's boat choice and the initial reward expected from the treasure chest of the current island:
!,# = %& !"#$% , $,# -− %& &'(% , !,# , !,# -.
This prediction error is then used to update the value of the boat/island pairing following:
%& &'(% , !,# , !,# -= %& &'(% , !,# , !,# -+ ⋅ !,# ,
where is a learning rate used to scale the prediction error following the boat choice. A second reward prediction error occurs after receiving the initial reward on the new island:
$,# = # − %& !"#$% , $,# -
with # representing the initial reward and %& !"#$% , $,# -the expected value of the treasure chest on the current island. The second prediction error is also used to update the value of the island/chest pairing:
%& !"#$% , $,# -= %& !"#$% , $,# -+ ⋅ $,#
It is also used to update the Q-value for the boat/island pair that was previously selected:
%& &'(% , !,# , !,# -= %& &'(% , !,# , !,# -+ ⋅ ⋅ $,#
where represents an eligibility trace decay parameter representing how much a prediction error is used to update previous actions.
Model-based system. The model-based component calculates reward expectations for each boat by combining the probabilistic transition structure with the model-free estimates of the island/chest pairs. Thus, for each available boat, the model-based value is defined in terms of its expected value of the next island:
%' , !,# , !,# -= 4 ( $ ′| !,# , !,# ) %& !"#$% , $,# ′- ( ),% )
where , $ ) 8 !,# , !,# -is the probability of transitioning to state $ after choosing action ! in state ! . The combination of transition probabilities and reward expectations gives us an estimate for model-based reward expectations on the next stage.
Choice rule. To connect these values to choices, the Q-values of both systems are mixed according to a model-based weighting parameter :
*+# ( ! , ! ) = ⋅ %' ( ! , ! ) + (1 − ) ⋅ %& ( ! , ! ).
The model-based weighting parameter is used in RL models to describe the degree of model-based control employed by the participant, where a value closer to 1 represents a high level of model-based control and a value closer to 0 represents a lower level of model-based control, or more model-free behavior. We then used the softmax function to convert these reward expectations to choice probabilities:
( !,# = ! | !,# ) = ,⋅. +#% (0 ,,% ,1 , ) ∑ ,⋅. +#% (0 ,,% ,1 -) 1 -
where is the inverse temperature parameter which controls the exploitationexploration trade-off between two choice options given their difference in value. This parameter can change the function from describing pure exploration ( closer to 0, insensitive to the value of actions) to pure exploitation (higher value never exploring lower value options).
We used maximum a-posteriori (MAP) estimation to find best fitting values for each of the four free parameters ( , , , ) for each participant separately using the optim function in R. To avoid local maxima, we repeated this process ten times with random starting points, selecting the iteration with the highest log-likelihood.


Return intentions
In order to provide a more intuitive description and statistical test of travel choice behavior in this task, we analyzed choice behavior during the travel phase as a function of previous reward history and transition type. As introduced above (see Task rationale), a model-based agent should change their willingness to return to an island only based on the prediction error they observed there, regardless of whether the prediction error was preceded by a rare or common transition. A model-free agent however, will increase the likelihood of choosing a particular boat after choosing it results in a positive prediction error, even after a rare transition 
(Daw et al., 2011;
Doll et al., 2015)
.
To make this more concrete, we frame this analysis in terms of "return intentions." We define a return intention as a boat choice that is most likely to bring the participant back to the island they visited before their current island. Thus, if participants first visited the red island, and then the green island, then upon leaving the green island, choosing the boat that most likely returned to the red island would be taken as a return intention.
We reasoned that model-based agents would be more likely to return to an island where they previously experienced a positive prediction error, regardless of the type of transition they experienced 
(Figure 2A
). We reasoned that while a model-free agent would be likely to return to an island where they previously experienced a positive prediction error after a common transition, this would not be the case if the positive prediction error occurred after a rare transition. This is because a model-free agent increases the value of the boat that brought them to the rewarding island, leading them to choose this boat again on a future trial instead of selecting the alternative boat that is more likely to bring them back to the rewarding island (see 
Figure 2B
).
For these analyses, we used each participant's best-fit learning rate from the dual-system RL model to generate prediction errors for each trial.


Hierarchical mixed effects models
We modeled participants' stay/leave decisions with a series of hierarchical mixed effects logistic regressions (following 
Hall-McMaster et al., 2021)
. These models explained participants' stay/leave decisions as a combination of expected reward rate, estimated average reward rate, and available alternative rewards, as well as an interaction between available alternative rewards and participants' degree of model-based behavior.
The goal of this set of analyses was to better understand how participants' knowledge of the task structure impacted their foraging decisions. To do this, we fit three models. The first, baseline, model explained stay/leave decisions as a combination of expected reward rate and average reward rate. The second model additionally included the available rewards at the alternative islands. The third model included all previous regressors, but added the interaction between available alternative rewards and participants' best-fit degree of model-based behavior (see Dual-system RL model above). We fit these models with hierarchical logistic regression using the glmer function from the lme4 package in R.
For these models, the average reward rate was calculated for each participant separately, using a threshold updating rule outlined in 
Constantino and Daw (2015)
. After every trial, we calculated a prediction error:
Δ = # # −
where # is the reward gained from the previous action, is the average reward rate, and # is the length of the previous action. This was then used to update the average reward rate as:
= + (1 − (1 − 3 ) 4 % ) • Δ
where 3 represents each participant's learning rate. We also calculated an estimate of participants' expected reward rate for each harvest decision:
= #5! ⋅ 6
where is the average depletion rate of a patch after each harvest, set to the mean value of 0.88 of the Beta distribution, and 6 is the time cost of a harvest (3 seconds). For each of the three models, we tested three different methods of calculating the available alternative rewards using the initial richness values from participants' most recent experience on each island. One of these represented the alternative rewards as the mean of the two values for the other islands, the second represented them as the maximum of these values, and the final one represented them as a combination weighted by the transition probabilities (assuming people would prefer to visit the most rewarding state).
Because this approach requires fitting individual learning rates to generate the reward rate predictors, we first fit these for each participant separately using individual logistic regressions (without the model-based control parameter). Specifically, for each participant, we used the nloptr function in R to find the learning rate and a starting value for the average reward rate that maximized the loglikelihood of the regression model. To avoid local maxima, we repeated this process for 35 iterations with random starting points and extracted the solution with the highest loglikelihood. We did this for each of the three versions of alternative reward (mean, max, weighted), which results in three sets of reward-related regressors for each participant.
Next, we used these to run the hierarchical models described above. We then compared the AIC values from each model to determine which best explained performance.


Results
Participants performed a foraging task that was embedded in a task structure that allowed planning towards goals. The task alternated between a foraging phase, in which participants decided whether to continue harvesting a depleting patch (treasure chest) from their current island, and a travel phase, in which they chose between boats that took them to another island.
Here, we first analyze choice behavior from the travel phase to demonstrate that people used model-based control to plan towards island selection. Then, we show that people's inclination to plan (their use of model-based control) predicts how much they let alternative reward information (in addition to average reward rate) influence their foraging decisions.


Travel decisions


Dual-system RL model
To measure whether participants used model-based control to plan towards islands, we fit a dual-system RL model to their travel choices. This is possible because this task dissociates model-free from model-based control by exploiting low-probability pairings between behavior and reward. Model-free agents are sensitive to such reward, because they rely on the direct experience of action-reward pairings. Model-based agents, however, discount these experiences using an explicit causal model of the task structure (see Dual-system RL model and Task rationale).
The model-based weighting parameter w in our dual-system RL model reflects the degree to which participants used the task's transition structure to plan towards an island. As is often observed in the two-stage RL tasks that inspired the current design, participants' behavior reflected a mixture of model-based and model-free strategies as indicated by a mean w of 0.52 (SD = 0.18). 
Table 1
 reports the descriptive statistics for all estimated parameters. 
Table 1
. Travel choice RL model parameter fits. A dual-system RL model was individually fit to each participant's travel choices. The model included four parameters (learning rate, inverse temperature, eligibility trace decay, and model-based behavior weighting parameter), each of which was fit for each participant. Mean and SD of all participants' parameter fits are shown here.


Return intentions
To provide a more intuitive description and statistical test of our data, we also directly analyzed choice probabilities as a function of the reward earned and the transition type experienced on a previous trial. We do this in terms of "return intentions", the tendency to revisit an island on which the participants previously experienced a reward outcome.
The rationale for this analysis is that after a boat choice and a common transition, the sign of the prediction error should influence model-free and model-based agents in the same way when they encounter the same boat again. Following common transitions, both types of agents become more likely to repeat their previous choice after a positive prediction error (registering as an intent to return) and less likely to repeat their previous choice after a negative prediction error (see 
Figure 2AB
). For the case of a rare transition, however, positive prediction errors drive model-free agents to repeat their choice, and model-based agents to switch (a return intention). To visualize these behavioral patterns, we plot return intentions as a function of previous prediction error sign (positive vs. negative) and previous transition type (common vs. rare) in 
Figure 2
. First, we simulated behavior for fully model-free (w = 0) and fully model-based (w = 1) agents ( 
Figure 2AB
). Next, we used the fitted learning rates from our RL model to calculate participants' prediction errors at the start of each foraging phase. 
Figure 2C
 shows the results of this analysis. We found that participants were more likely to return to an island after receiving a positive reward prediction error compared to a negative reward prediction error, but only after common transitions (t = 4.69, p < 0.001, Cohen's d = 0.42) and not after rare transitions (t = -0.11, p = 0.91, d = 0.01). Importantly, the effect of previous reward on the intention to return was significantly different between transition types (t = 2.55, p = 0.012, d = 0.23). These results, which are consistent with the computational model fits above, indicate that participants were not entirely model-based when making travel choices. Indeed, the pattern of participants' return intentions reflects a mixture of those for the model-free and model-based agents, whose effect of prediction error sign is consistent following common transitions, but opposite following rare transitions. 


Parameter


Stay/leave decisions
Having confirmed that people use a mixture of model-free and model-based control in our task, we next investigated whether their propensity towards these systems influenced their foraging decisions.
Based on the MVT, we predicted that participants would be less likely to stay on their current island if (a) the average reward rate was higher and (b) the expected reward for the next harvest decision was lower. Based on prior work 
(Hall-McMaster et al., 2021;
Harhen & Bornstein, 2023)
, we also predicted that participants would be less likely to stay with increasing 'alternative' rewards. That is, if the rewards on the other two islands are higher, participants may consider leaving their current island sooner. Finally, we investigated whether this tendency was modulated by the degree of participants' model-basedness, reasoning that goal-directed participants may be more inclined to let task structure guide their foraging decisions.
Before modeling participants' data in a hierarchical regression, we fit individual learning rates and starting values for the average reward rate 
(Hall-McMaster et al., 2021)
. We did this to reduce the complexity of our full model and for computational tractability. These models explained stay/leave decisions as a function of expected reward, average reward rate, and available alternative rewards.
Next, we used these values to calculate individual regressors representing average reward rates and alternative reward values for each stay/leave decision. We used these to run three different, increasingly complex, hierarchical mixed effects models. The baseline model used only the average reward rate across all islands and the expected reward rate of the next harvest to explain foraging decisions. Consistent with the MVT, we found that people were more likely to continue harvesting rewards on an island with an increasing expected reward rate for the next forage ( = 5.62, SE = 0.33, p < 2e-16) and when the average reward rate decreased ( = -2.11, SE = 0.296, p = 1.06e-12).
We then added the alternative rewards as a regressor to model how they influenced stay/leave decisions. Here, we again saw main effects of expected reward ( = 5.70, SE = 0.34, p < 2e-16) and average reward rate ( = -2.05, SE = 0.30, p = 1.6e-11). Most importantly, we found a main effect of average 1 available alternative rewards ( = -0.15, SE = 0.03, p = 5.78e-8). This replicates the finding of Hall-McMaster and colleagues (2021) in a task with a different structure.
This finding also allowed us to test whether participants with a stronger reliance on alternative rewards during stay/leave decisions also used a more model-based strategy during travel decisions. Therefore, our final model also included participants' degree of model-based behavior, as measured by the RL weighting parameter w (see Dual-system RL model), as regressors. As before, we again found main effects of expected reward ( = 5.71, SE = 0.34, p < 2e-16), average reward rate ( = -2.05, SE = 0.30, p = 8.94e-12), and mean available alternative rewards ( = -0.15, SE = 0.03, p = 2.66e-8). However, this latter effect was qualified by an interaction effect with modelbasedness ( = -0.08, SE = 0.03, p = 0.002). These findings were replicated in our second set of participants (see Supplemental Materials).
We compared the three models using AIC scores (see 
Table 2
), and found that the third model, that modeled stay/leave decisions as a function of (i) expected reward rate, (ii) average reward rate, and (iii) an interaction between the mean of available alternative rewards and participants' degree of model-based behavior was the best fitting model for our participants' data.
These findings indicate that participants who used the task structure to make travel decisions also used it to make foraging decisions. Specifically, more model-based participants used the task structure more strongly when deciding whether to continue foraging. It should be noted that we estimated the degree of model-based planning from the travel phase and applied it to our analysis of the foraging phase here. This independence strongly suggests that goal-directed control played a key role during decision making in our foraging task. 
Table 2
. Hierarchical mixed-effects model comparison. Column 1: Predictors; Lists each regressor used in the models to predict stay/leave decisions. Columns 2-3: MVT; Lists beta coefficients and p-values for each regressor in our baseline model, which uses expected reward rate of the next harvest and average reward rate across all islands to explain foraging decisions. Columns 4-5: MVT + Alt rewards; Lists beta coefficients and p-values for each regressor in our second model, which predicts foraging decisions using a combination of expected reward rate, average reward rate, and average available alternative rewards. Columns 6-7: MVT + Alt rewards*w; Lists beta coefficients and p-values for each regressor in our final model, which predicts foraging decisions using a combination of expected reward rate, average reward rate, average available alternative rewards, and an interaction between alternative rewards and w (degree of model-based behavior). The AIC scores for each model are denoted in the final row of the table.


Simulations
Our findings indicate that knowledge about task structure not only influenced decisions about where to travel next but also about whether to continue harvesting a current patch. They reveal a setting in which people systematically deviate from the leaving rule dictated by the MVT. However, our task violates, by design, at least one core assumption of this framework: agents have some control over which patches they visit. Therefore, given the increased control over which islands are visited, it may be optimal to let alternative rewards influence foraging decisions.
The optimal threshold suggested by the MVT does not consider the possibility of revisiting and tracking the value of patches. Thus, we needed to determine the optimal decision rule for our task. We did this using a comprehensive generative model of the task, combining our existing models to simulate harvesting behavior, tracking average reward rates to make stay/leave decisions, and travel decisions using dual-system RL. This model differed from conventional foraging models in two ways. First, we allowed stay/leave decisions to be influenced by not just the average reward rate across all choices, but also the estimates of the reward rates accrued at individual islands. After each reward, the model updates average reward rate as described above, but it now also updates an island-specific reward rate:
0 = 0 + (1 − (1 − 3 ) 4 % ) •
where 0 is the island-specific reward rate.
Based on our results, we investigated how task performance changes as a function of both model-basedness (w) and the extent to which alternative reward information is incorporated into stay/leave decisions. The latter was implemented by an additional weighting parameter, z, that governed the extent to which the agent relies on their knowledge of available alternative rewards in making stay/leave decisions. Specifically, the agent calculates a "mixed" reward rate as:
789+: = (1 − ) ⋅ + ⋅ ( 1;#+<*1#8=+ )
where 1;#+<*1#8=+ is the average reward rate experienced at the other islands. They then use this to decide whether to stay in the current patch by comparing the expected rewards against this mixed reward rate using the softmax rule:
0#1> = 1 G1 + ,− 0 ⋅ ( #5! ⋅ − 789+: )-I
with an inverse temperature 0 specific to stay/leave decisions. Note that for values of z close to 0, the agent implements the decision rule given by the MVT, only comparing the expected reward to the average reward rate. For values of z closer to 1, however, the agent strongly relies on the rewards experienced on the islands that it is currently not visiting. During the travel phase, the agent uses the algorithm from the dual-system RL model to make travel decisions. We used this model to simulate behavior on this task using various combinations of the w and z parameters to better understand how average reward rate in the task changed as a function of these two parameters. Additionally, we varied the inverse temperature parameter, 0 , which governs choice stochasticity or the tradeoff between exploration and exploitation 
(Addicott et al., 2017;
Cohen et al., 2007)
. The inverse temperature parameter for travel decisions and the learning rates for this model were fixed to values consistent with our prior modeling ( = 0.36, ?@ = 0.5, 3 = 0.1).
We discovered that contrary to the MVT threshold rule, agents can increase their average reward by relying on information about the task structure during stay/leave decisions. First, following prior work 
(Kool et al., 2016)
, we found that model-based agents gain more reward on average compared to more model-free agents. This effect is not surprising because model-based agents make more accurate decisions about which islands to visit. This way, they can choose islands that carry higher initial reward values. The difference in average reward rate between more model-free agents and more model-based agents is distinctly small, but consistent (see 
Figure 3B
). Second, we found that reliance on alternative rewards during stay/leave decisions also increased the average reward. However, the degree to which the agent should rely on alternative rewards depends on their stay/leave choice stochasticity ( 0 ). We see that no matter how explorative a foraging agent's behavior is, it is always optimal to incorporate alternative rewards to a certain extent. 
Figure 3A
 shows the result of this analysis. When agents are very exploratory (lower values of 0 ), they accrue maximal reward when fully abandoning the average reward rate and only using the rewards at the alternative islands. However, the benefit of incorporating the alternative rewards tapers off when agents become less exploratory. This is highlighted by the plateau in optimal z around 0.3, indicating that agents should weigh the alternative rewards and the average reward rate in a 3:7 ratio when deciding whether to continue harvesting.
We provide a more detailed look into the relationship between z, w, 0 , and optimal reward in 
Figure 3C
. Reliance on model-based control yields a small but reliable increase in reward. However, the extent to which the agent incorporates alternative rewards during stay/leave decisions has a more dramatic effect on total reward. For agents with low to moderate degrees of exploitation, non-zero values of z only lead to increases in average reward rate. For more exploitative agents, however, intermediate reliance on alternative rewards improves task performance, but strong reliance is detrimental. These analyses highlight that our task demands a delicate balance: agents should incorporate alternative reward information into their decisions, but not let them dictate choice behavior entirely. We interpret these results in the Discussion. Relationship between and optimal z. The simulations used to generate these values used a w value of 0.5, but the relationship between and optimal z remains qualitatively consistent across w values. Simulated data was generated for 1020 combinations of possible and z values. We performed 1000 simulations of each combination of and z for 100 different sets of generated rewards. Each point in this plot indicates the z value that led to the highest average reward rate for each value (indicated by the x-axis). B. Relationship between w and reward rate. The simulations used to generate these values used a z value of 0.5 and a of 0.5, though the average reward rate of a fully model-based agent (w = 1) is always somewhat greater than that of a fully modelfree agent (w = 0) regardless of . Each point represents the average of 1000 simulations of the task using that w value (indicated by the x-axis) for 100 different sets of generated rewards. C. Relationship between z, reward rate, and . In each of the three plots, each point represents the average of 1000 simulations using that z value (indicated by the x-axis) and (indicated on the plot) for 100 different sets of generated rewards.


Discussion
In naturalistic foraging environments, we must plan over the structure of the environment to maximize reward. However, most extant foraging tasks 
(Constantino & Daw, 2015;
Hayden et al., 2011;
Kane et al., 2017
Kane et al., , 2019
Kolling et al., 2012)
 and computational frameworks 
(Charnov, 1976;
Stephens & Krebs, 1986)
 do not consider task structure nor the role of planning during foraging 
(Hall-McMaster & Luyckx, 2019)
. Here, we addressed this gap using a structured foraging task in which participants alternated between deciding whether to continue harvesting on the current island or to visit another one.


A B C
We found that participants not only consulted the average reward rate when making stay/leave decisions, but also explicitly incorporated the reward available on the alternative options into their decision threshold. This result, which replicates that of 
Hall-McMaster and colleagues (2021)
, suggests that people use the task's structure to retrieve the identities and values of the next possible islands, which they then use to decide whether or not to leave the current one. Moreover, because our task included a stochastic transition structure, we were able to measure the degree to which people used model-based control to plan their island visits 
(Daw et al., 2011;
Doll et al., 2015;
Kool et al., 2016;
Pouncy et al., 2021)
.
This revealed two key influences of environmental structure on foraging behavior. First, participants displayed a mixture of model-based and model-free control in their travel decisions. Participants were more likely to return to a patch after receiving a positive reward prediction error there, but only after common transitions. This aligns with the finding from 
Hall-McMaster et al. (2021)
 that participants were more likely to return to patches with faster reward-replenishment rates. After rare transitions, however, there was no group-level effect of prediction error sign. Together, these findings suggest that participants aimed to maximize reward rate, although they use multiple (sometimes conflicting) strategies to do so.
Second, we found that the degree to which people made model-based travel decisions predicted their reliance on alternative rewards when deciding whether to stay on or leave the current island. Thus, by introducing structure into this task, we saw that participants deviated from the strategy suggested by the MVT. The literature on foraging is replete with explanations of why people may deviate from the MVT. For example, it has been suggested that people are prone to overstay their current option because they are sensitive to sunk costs 
(Wikenheiser et al., 2013)
, discount future rewards 
(Blanchard & Hayden, 2014)
, prefer short-term rewards 
(Kane et al., 2019)
, or are riskaverse 
(Eisenreich et al., 2019)
. In our case, however, participants' tendency not to solely rely on the average reward rate seems to be beneficial rather than irrational. Specifically, our computational model simulations demonstrated that including the reward information from alternative options during stay/leave decisions increased performance. This finding indicates that even though the MVT is a simple and powerful decision rule for foraging situations, it is not well-equipped to guide behavior in more structured environments.
Recent work by 
Harhen and Bornstein (2023)
 has similarly demonstrated that a structured task environment leads to optimal deviations from the MVT. In their task, participants were not shown explicit cues associated with each patch (c.f., our islandspecific colors). Instead, they needed to use reward information to infer in which of three possible patch types (low, medium or high reward) they were currently located. Participants in this task displayed substantial overharvesting, the tendency to stay with an option too long in terms of the MVT's threshold policy. The authors, however, argued that this behavioral pattern was the result of uncertainty about the environment. We designed our task to avoid such uncertainty, as participants completed a thorough training procedure to ensure that they encoded the task's structure and its transitions 
(Feher da Silva et al., 2023;
Feher da Silva & Hare, 2020)
. This suggests that participants' deviation from the MVT was driven by their knowledge of the task structure, and not despite it. At the same time, it would be interesting to investigate whether the main effects in this paper are dependent on the resolution of participants' internal representation of the task structure (which can be captured from behavioral assessments; 
Karagoz et al., 2024)
.
We found that participants' patch-leaving decisions were impacted by their knowledge of alternative reward options. This result, which suggests that participants considered the task structure when deciding whether to stick with their current patch, replicates the key findings first reported by Hall-McMaster and colleagues (2021). In their task, participants deterministically chose which patch to visit after each leave decision. In our task, this choice was less direct. Participants chose between boats that then transitioned them to the next island according to a stochastic transition structure. This allowed us to determine that people used a mixture of model-free and model-based control in this task. Most importantly, this measure predicted the degree to which participants used alternative rewards to decide when to leave. Because the study design from Hall-McMaster and colleagues (2021) does not assess goal-directed planning, it remains ambiguous whether our effects are directly related. However, they invite fascinating follow-up research to test this hypothesis. For example, one could investigate whether alternative rewards affect patch-leaving decisions differently between more direct 'island-selection' and less direct 'boat-selection' blocks of trials (i.e., varying the stochasticity of travel decisions).
Our computational model simulates patch-leaving and travel decisions in the same framework 
(Hall-McMaster et al., 2021;
Kolling & Akam, 2017)
. These simulations revealed that it is always beneficial to incorporate alternative rewards to some extent when making patch-leaving decisions. However, they also showed that the degree to which this increases reward depends on the stochasticity of those foraging decisions. That is, particularly exploratory (i.e., random) agents benefit from mostly relying on alternative rewards, but more exploitative (i.e., greedy) agents should mix these values with average reward rate roughly in a 3:7 ratio. These results suggest an important normative deviation from optimal foraging theory: when structure is introduced, it becomes beneficial to consider alternative reward options in one's environment when making reward-maximizing patch-leaving decisions, rather than only contrasting expected rewards against overall average rewards. One possible explanation for this deviation from MVT is that participants in our task are unable to immediately revisit the option from which they are currently harvesting rewards. However, because those rewards strongly affect the time-discounted average reward rate, this measure is not an accurate reflection of the reward rate that is to be expected in the near future (at one of the other two options). This is corrected for by incorporating an average of the alternative rewards into the decision threshold. This line of reasoning also explains why it is optimal for very exploratory agents to mostly rely on alternative rewards. In rewardrich environments, exploratory agents are more likely to leave than exploitative agents. Letting that decision be primarily dictated by alternative rewards compensates them for using a suboptimal explore/exploit tradeoff (i.e., the exploitative agents in 
Figure 3C
 have higher optimal reward rates).
Our results add nuance to the role of explore/exploit tradeoffs in foraging 
(Addicott et al., 2017;
Lloyd et al., 2021;
van Dooren et al., 2021)
. They also invite us to consider how people determine their behavioral policy in terms of how much exploration and consideration of alternative rewards they should deploy. One possibility is that one of these parameters is relatively stable (e.g., a trait-level characteristic; 
Fridhandler, 1986)
, while the other is more flexible. Alternatively, both parameters may be flexibly adjustable. Can people learn optimal control policies using RL 
(Bustamante et al., 2021;
Lieder et al., 2018)
 or through exhaustive evaluation of parameter settings 
(Shenhav et al., 2013)
? Or does their inclination to follow the MVT 
(Constantino & Daw, 2015;
Turrin et al., 2017;
Zhang et al., 2017
) hurt them in a task where the optimal decision rule requires the consideration of task structure? Implementing successful control strategies necessitates successful 'metacontrol' between them 
(Boureau et al., 2015)
, and our results suggest this is a pertinent topic for foraging as well.
A body of research suggests that the exertion of model-based planning requires some form of top-down cognitive control 
(Gershman et al., 2014;
Otto et al., 2013
Otto et al., , 2015
Schad et al., 2014;
Shenhav et al., 2017;
Smittenaar et al., 2013)
. This set of cognitive functions, dependent on computations in the frontal cortex, allows humans to execute novel and effortful tasks by reconfiguring information processing (E. K. 
Miller & Cohen, 2001)
. We predict that similar computations underlie the use of model-based planning in our foraging task, either through prospective simulation of future consequences 
(Doll et al., 2015)
, or retrospective credit assignment using the task structure 
(Gershman et al., 2014;
Sharp & Eldar, 2024)
. Research on strategy arbitration in human RL provides several ways in which people's reliance on model-based control can be altered. When people are under cognitive load 
(Otto et al., 2013)
 or time pressure 
(Keramati et al., 2016)
, they use less model-based control. When potential rewards are amplified, they use more model-based control 
(Kool et al., 2017;
Patzelt et al., 2019a)
. To assess the nature of model-based control in the current task, it would be interesting to see whether these factors also modulate people's reliance on task structure during foraging.
Our paradigm also provides a new method for investigating how various mental health phenotypes affect planning and choice behavior in structured foraging contexts. This allows us to transdiagnostically explore the interaction between model-based planning and patch-leaving decisions within the same framework. It may be that some transdiagnostic individual differences could predict participants' tendency to use the task structure (e.g., schizotypy), whereas others may modulate the perceived reward rate in the MVT (e.g., depressed mood). For example, if the exertion of model-based control carries an intrinsic effort cost 
(Kool & Botvinick, 2018)
, then this may lead to a reduced reliance on task structure during both travel and stay/leave decisions in individuals experiencing reduced motivation (i.e., apathy, anhedonia), or they may guide the adaptation of other choice strategies. Similarly, individuals with depression, which is associated with differences in motivational processes 
(Bustamante et al., 2024;
Grahek et al., 2019;
Treadway et al., 2012;
Yang et al., 2014)
, may also exhibit this behavior. Additionally, it has been argued that the process of forming internal representations of the environment, or cognitive maps, is different in individuals with and without schizophrenia 
(Karagoz et al., 2025;
Musa et al., 2022;
Nour et al., 2025)
. In the context of our current task, this would make the intriguing prediction that people with schizophrenia would more closely follow the MVT rather than rely on an internal representation of task structure. Persons with schizophrenia also display differences in reward learning, anticipation/prediction, and translating reward information into action plans 
(Barch et al., 2016;
Barch & Treadway, 2014;
Culbreth et al., 2020;
Green et al., 2015;
Moran et al., 2023)
. We predict that these differences could be reflected in our task through decreased reward outcomes, differences in return intentions, and a reduced reliance on the task structure. Recent research on foraging in individuals with attention-deficit/hyperactivity disorder reflects an increased tendency to explore while foraging 
(Barack et al., 2024)
. Based on our simulations of the task, this could translate to a stronger reliance on alternative reward information when making stay/leave decisions in these individuals. Generally, examining these behaviors through a transdiagnostic lens 
(Gillan et al., 2016;
Patzelt et al., 2019a
Patzelt et al., , 2019b
 could enable enhancing the specificity of interventions for individuals with differences in planning or information processing behaviors.
In summary, we have shown that foraging behavior does not follow a goldstandard decision rule in a structured environment, but that this deviation is beneficial. Though conventional foraging tasks already appeal to notions of ecological validity, we believe that our task goes further by incorporating structure. Of course, not all real-world foraging environments should be treated as structured (either because they are not or because people do not know they are), but the world is inherently structured. If agents in those environments try to maximize reward, then our analyses suggest they should be sensitive to this.


Data, Materials, and Software Availability
Data, task, and analysis code are publicly available at the Open Science Framework repository https://osf.io/xumy6/.


Supplemental Materials


Subjects
To replicate our initial results, we recruited a sample of 44 undergraduate participants (18-22, mean = 19.4 ± 1.3 years, 31 female, 12 male, and 1 non-binary) from Washington University in St. Louis's undergraduate participant pool. Participants were required to be at least 18 years old, fluent in English, and not colorblind. There were 8 participants (18.2%) who were excluded from analysis for a variety of reasons. We excluded participants who admitted to writing down the task structure that they were asked to memorize in the post-task survey (5), participants who missed the response deadline on 15% or more travel trials (3), participants whose mean reaction time for the travel trial response deadline was at least 2SD below or above the group mean (2), participants who missed the response deadline on 15% or more harvests (3) and participants whose final score fell below 2SD below the group mean (4).
The undergraduate participants were compensated with 1 psychology course credit hour. The participants with the five highest scores were given an additional monetary bonus in the form of a $15 Amazon gift card.
All participants gave informed consent, and procedures were approved by the Washington University in St. Louis Institutional Review Board.


Results


Travel decisions
Participants performed the same structured foraging task as presented in the main analyses.


Dual-system RL model
To measure whether participants used model-based control to plan towards islands, we fit a dual-system RL model to their travel choices. Consistent with the main experiment, participants' behavior reflected a mixture of model-based and model-free strategies as indicated by a mean w of 0.58 (SD = 0.19). 
Table S1
 reports the descriptive statistics for all estimated parameters. 
Table S1
. Travel choice RL model parameter fits. A dual-system RL model was individually fit to each participant's travel choices, and returned four parameter fits (learning rate, inverse temperature, eligibility trace decay, and model-based behavior weighting parameter) for each participant. Mean and SD of all participants' parameter fits are shown here.


Return intentions
As with our initial dataset, we directly analyzed choice probabilities as a function of the reward earned and the transition type experienced on a previous trial. We do this in terms of "return intentions," the tendency to revisit an island on which the participants previously experienced a reward outcome. 
Figure S1
 shows the results of this analysis on our second set of participants. We found that participants were more likely to return to an island after receiving a positive reward prediction error compared to a negative reward prediction error, but only after common transitions (t = 2.16, p = 0.04, d = 0.36) and not after rare transitions (t = 1.45, p = 0.16, d = 0.25). The effect of previous reward on the intention to return was not significantly different between transition types (t = -0.07, p = 0.94, d = 0.01). These results, which are largely consistent with our initial results and the computational model fits above, indicate that participants were not entirely model-based when making travel choices.  
Figure S1
. Average return intentions by transition type and reward prediction error. The leftmost plot displays simulated behavior for a fully model-based agent, and the center plot displays simulated behavior for a fully model-free agent. Participant behavior is shown in the rightmost plot. Error bars indicate within-subject standard error of the mean.


Parameter


Hierarchical mixed effects models
Finally, we investigated whether participants' use of model-based control in the task influenced their foraging decisions.
As with our previously mentioned hierarchical mixed effects models, we fit learning rates and starting values for the average reward rate for each participant. We used these values to calculate individual regressors representing expected and average reward rates, and alternative reward values for each stay/leave decision.
We again ran three different, increasingly complex, hierarchical mixed effects models. The baseline model explained stay/leave decisions as a function of expected and average reward rate. Consistent with our previous results and the MVT, we found that participants were more likely to continue harvesting rewards on an island with an increasing expected reward for the next forage ( = 4.53, SE = 0.50, p < 2e-16) and decreased average reward rate ( = -0.94, SE = 0.31, p = 0.002).
The second model added the alternative rewards as a regressor to investigate how they influenced stay/leave decisions. Again, we saw main effects of expected reward ( = 4.66, SE = 0.50, p < 2e-16) and average reward rate ( = -0.86, SE = 0.31, p = 0.006). We also found a main effect of average available alternative rewards ( = -0.22, SE = 0.05, p = 7.3e-5). This replicates our initial findings, as well as that of Hall-McMaster and colleagues (2021).
Our final model included participants' degree of model-based behavior, as measured by the RL weighting parameter w (see Dual-system RL model), as regressors. Consistent with our main findings, we found main effects of expected reward ( = 4.68, SE = 0.50, p < 2e-16), average reward rate ( = -0.88, SE = 0.30, p = 0.004), and mean available alternative rewards ( = -0.21, SE = 0.05, p = 3.9e-5). Importantly, this latter effect was qualified by an interaction effect with model-basedness ( = -0.10, SE = 0.05, p = 0.05). Once again, we found that the third model, modeling stay/leave decisions as a function of expected reward rate, average reward rate, and an interaction between the mean of available alternative rewards and participants' degree of model-based behavior was the best fitting model for our participants' data (see 
Table  S2
). These results replicate the findings from our main analysis, indicating that Negative participants who used the task's structure to make travel decisions also used it to make foraging decisions. 
Table S2
. Hierarchical mixed-effects model comparison. Column 1: Predictors; Lists each regressor used in the models to predict stay/leave decisions. Columns 2-3: MVT; Lists beta coefficients and p-values for each regressor in our baseline model, which uses expected reward rate of the next harvest and average reward rate across all islands to explain foraging decisions. Columns 4-5: MVT + Alt rewards; Lists beta coefficients and p-values for each regressor in our second model, which predicts foraging decisions using a combination of expected reward rate, average reward rate, and average available alternative rewards. Columns 6-7: MVT + Alt rewards*w; Lists beta coefficients and p-values for each regressor in our final model, which predicts foraging decisions using a combination of expected reward rate, average reward rate, average available alternative rewards, and an interaction between alternative rewards and w (degree of model-based behavior). The AIC scores for each model are denoted in the final row of the table.
Task instructions "Welcome to the experiment! Please read the following instructions very carefully." "In this game, you will be an explorer traveling between tropical islands to collect as much treasure as possible in a limited amount of time." "On each island you will see a treasure chest, which will look something like this:" [Here, participant is shown an example image of an island with a treasure chest and the explorer's avatar on that island.] "When you are on an island, you can collect treasure from the treasure chest by pressing the 'F' key. Each time you press 'F' to collect coins, you will see how much treasure you have just earned, which will look like this:" [Here, participant is shown the same example island image as on the previous page, but with a display of how much treasure was earned from an example harvest.] "You will need to wait until the coin icon disappears before you can collect more coins. To make this clear, your avatar will 'glow' whenever you are allowed to make a choice." "Let's practice collecting coins from an island's treasure chest a few times. Press 'next' to try this out." [Participant must press 'Next' to practice collecting coins from a treasure chest 5x.]
"As you may have noticed, each time you choose to collect coins from a chest, the number of coins you earn decreases. However, when you leave the island, its treasure chest will be replenished before you return. Therefore, you will have to decide when to leave the current island and sail to another one (more about this later)."
"Different islands will give you a different number of coins at the start. Also, the starting amount of each island will slowly change throughout the game. For example, an island that starts with a lot of coins now might start with fewer coins later on. This means that you can increase your score by keeping track of which island has the best treasure chest at each point in time. Even though the starting amounts change, they change slowly. So, an island that has a lot of treasure upon your arrival will likely have a lot of treasure to start with the next time you visit that island." "Here are a few more tips about the treasure chests: 1. The starting amount of an island is not related to the way it looks. 2. Just because one island has a low starting amount does not mean another island has a high starting amount. 3. Finally, there are no funny patterns in how much an island pays off (such as alternating between high and low on every visit)." "Let's learn about visiting different islands! Whenever you are done collecting treasure on your current island, you can press 'J' to leave. However, you have to collect coins at least once on each island, before you can decide to leave." "Now, you will briefly practice collecting some treasure from an island, using the 'F' key, and then pressing 'J' to leave. You will do this three times in a row. Press 'next' to begin." [Participant must press 'Next' to practice collecting coins from a treasure chest and leaving their current island 3x.]
"Great! Now that you understand how to collect treasure and leave an island, you can learn how sailing between islands works. In this game, there are three islands where you can collect treasure. Each island has a unique identifying color, shown here:" [Here, participants are shown example images of the three different islands (purple, red, and green) with treasure chests.] "After you decide to leave an island, you will be asked to select a boat to take you to a different island. This will look something like this:" [Here, participants are shown an example image of the explorer avatar on the shoreline of an island with two boat options.] "To select the boat on the left, press the 'F' key. To select the boat on the right, press the 'J' key. Once you select a boat, there will be a brief delay as you sail to your next island." "Let's practice this a couple of times. You will start by choosing a boat, which will take you to a new island. You will then continue practicing as you were a moment ago, by collecting coins, and leaving the island, and then selecting a new boat. You will do this three times in a row. Press 'next' to begin." [Participant must press 'Next' to practice selecting a boat, observe the boat traveling to a new island, and arriving on a new island where they practice collecting coins from the treasure chest, and leaving the new island 3x.] "Great job! As you just experienced, when you decide to leave an island, you can decide where you want to go next. Each boat you see will sail to one of the other two islands. (You can't return to your current island because the treasure chest needs to be replenished.) Note that the location of the boats on the shoreline will always be random and will have no relation to their destination."
"Each pair of islands is connected by a specific boat. So, the boat that takes you from the red island to the purple island will also take you from the purple island back to the red island. This holds for all islands: For each two islands, there is a unique boat that connects them. That means there are three different boats, each with their own unique flag shape, that can take you to and from the other islands, pictured here:" [Here, participants are shown an image of the three unique boats that will allow them to travel between the islands.]
"It is very important that you understand how the islands are connected by the boats. To help you learn, we will take you through some short testing phases. In the next few trials, you will start on the purple island, and go straight to the shoreline. Before you see the boats, we will tell you which island to go to next. Then, you have to choose the boat to go there, and you will see whether your choice was right. In order to move on to the next phase, you have to choose the correct boat 6 times in a row."
"It is important that you learn how all locations are connected without relying on anything else but your mind. For example, please do not draw it out on a piece of paper. The main task requires you to be quick, so learning the structure by heart will be useful to you. Press 'next' to begin your practice phase." [Participant must press 'Next' to practice traveling from the purple island to a prompted target island by selecting the correct boat from the purple island's shoreline 6x in a row.]
"Great job! Now, you will do the same thing again, starting from the red island." [Participant must press 'Next' to practice traveling from the red island to a prompted target island by selecting the correct boat from the red island's shoreline 6x in a row.] "Great! Now, you will start from the green island." [Participant must press 'Next' to practice traveling from the green island to a prompted target island by selecting the correct boat from the green island's shoreline 6x in a row.] "Great work! You are almost ready to start the experiment! Now, you will do one more set of testing trials. We will drop you on a random island. Then, we will tell you which island to visit next. You will need to sail to the correct island ten times in a row to continue. Press 'next' to begin." [Participant must press 'Next' to practice traveling to all three islands in a randomly determined order. They must travel from a randomly determined starting island to a prompted target island by selecting the correct boat from that starting island's shoreline 10x in a row.] "Great job! So far, each boat has traveled between the same two islands every time. In the main game, however, sometimes the seas will be rough, and therefore the boat will be forced to travel to a different island. For example, the boat that travels between the purple and red islands may occasionally travel from the red island to the green island." "Regardless, if you want to travel to a specific island, you should pick the boat that is most likely to take you there. You just have to remember that sometimes you might end up on a different island than you expected." "To see this, you will now briefly practice traveling one more time. From now on, you will not be asked to travel to particular islands again; it is up to you to decide where to go. Almost everything will be the same as before. The only difference is that even though each boat will still sail to the same island most of the time, sometimes a boat will be redirected to a different island than expected. You will do this five times in a row. Press 'next' to begin." [Participant must press 'Next' to complete 5 travel trials in which they select a boat and sail to a new island of their choosing. Some of these trials will include rare transitions.]
"Great! There is one more thing you need to learn before you can begin. You will get 30 minutes to play the total game. Each choice you make will cost some time, so you have to decide how to spend it. On each island, you can decide to stay and collect more coins, or travel to another island with a replenished chest. Remember that you have to collect coins at least once on each island before you can leave." "In the game, you will only have 2 seconds for each choice on an island (2 seconds to collect coins or leave the island), and 3 seconds to choose a boat on the shoreline. If you take too long to make a choice on an island, you will see a reminder to respond faster. The more trials you miss, the less time you have to collect coins! If you take too long to select a boat when leaving an island, a boat will automatically be selected for you at random."
"Collecting coins will take 3 seconds, and increase your score. Traveling takes longer, 10 seconds, but it brings you to an island with a replenished chest. Importantly, each action will always take the same amount of time, so you will not be able to finish the experiment faster by making your decisions more quickly. Everyone will complete the experiment in exactly the same amount of time, so use the time you have for each decision to your advantage!" "In the game, you'll get entirely new treasure chests, so it is not relevant how good they were during these practice phases. Everything else will be the same. Let's review what you've learned and then begin playing. Remember, you want to increase your score as much as you can in the time you have by collecting coins from each island and traveling to find islands with the greatest amount of treasure. The starting amounts of treasure on each island change slowly over time, so you need to concentrate and be flexible to keep track of which islands are good right now." "Once you begin, you will have 30 minutes to collect as much treasure as possible. The timer will not stop, so there will be no break." For Prolific participants: "As a bonus, you will earn 1 cent for every 30 points you earn in the game." For SONA participants: "The top five scorers will win a $15 Amazon gift card as a bonus."
"The experiment will start after you press 'next', so make sure you have your fingers on the 'F' and 'J' keys to begin. Good luck!" 
[Task begins.]
 
Figure 1 .
1
Foraging Task. A. Example task sequence.


Figure 2 .
2
Average return intentions by transition type and reward prediction error. The leftmost plot displays simulated behavior for a fully model-based agent, and the center plot displays simulated behavior for a fully model-free agent. Participant behavior is shown in the rightmost plot. Error bars indicate within-subject standard error of the mean.


Figure 3 .
3
Simulations. A.


As outlined in the Methods section, we tested three different methods of representing alternative rewards; the mean of the two alternative island reward values, the maximum of the two values, and a combination weighted by the transition probabilities. Here, we report the results of the model that uses the mean of the alternative reward values, as the results of these models do not qualitatively change between the three different methods of representing alternative rewards.














A Primer on Foraging and the Explore/Exploit Trade-Off for Psychiatry Research




M
A
Addicott






J
M
Pearson






M
M
Sweitzer






D
L
Barack






M
L
Platt




















10.1038/npp.2017.108








Neuropsychopharmacology




42


10














Hierarchical control over foraging behavior by anterior cingulate cortex




Alejandro






R
J
Holroyd






C
B




10.1016/j.neubiorev.2024.105623








Neuroscience & Biobehavioral Reviews




160


105623














Attention deficits linked with proclivity to explore while foraging




D
L
Barack






V
U
Ludwig






F
Parodi






N
Ahmed






E
M
Brannon






A
Ramakrishnan






M
L
Platt




10.1098/rspb.2022.2584








Proceedings of the Royal Society B: Biological Sciences




291














Mechanisms Underlying Motivational Deficits in Psychopathology: Similarities and Differences in Depression and Schizophrenia




D
M
Barch






D
Pagliaccio






K
Luking


















10.1007/7854_2015_376




Behavioral Neuroscience of Motivation


H. Simpson & P. D. Balsam




Springer International Publishing














Effort, anhedonia, and function in schizophrenia: Reduced effort allocation predicts amotivation and functional impairment




D
M
Barch






M
T
Treadway




10.1037/a0036299








Journal of Abnormal Psychology




123


2
















Learning the value of information in an uncertain world




T
E J
Behrens






M
W
Woolrich






M
E
Walton






M
F S
Rushworth




10.1038/nn1954








Nature Neuroscience




10


9
















Neurons in Dorsal Anterior Cingulate Cortex Signal Postdecisional Variables in a Foraging Task




T
C
Blanchard






B
Y
Hayden








Journal of Neuroscience




34


2


















10.1523/JNEUROSCI.3151-13.2014














Deciding How To Decide: Self-Control and Meta-Decision Making




Y.-L
Boureau






P
Sokol-Hessner






N
D
Daw








Trends in Cognitive Sciences




19


11


















10.1016/j.tics.2015.08.013














Major depression symptom severity associations with willingness to exert effort and patch foraging strategy. medRxiv: The Preprint Server for Health Sciences




L
A
Bustamante






D
M
Barch






J
Solis






T
Oshinowo






I
Grahek






A
B
Konova






N
D
Daw






J
D
Cohen




10.1101/2024.02.18.24302985


2024.02.18.24302985


















Learning to Overexert Cognitive Control in a Stroop Task




L
A
Bustamante






F
Lieder






S
Musslick






A
Shenhav






J
Cohen




10.3758/s13415-020-00845-x








Cognitive, Affective, & Behavioral Neuroscience




21


3
















Effort Foraging Task reveals positive correlation between individual differences in the cost of cognitive and physical effort in humans




L
A
Bustamante






T
Oshinowo






J
R
Lee






E
Tong






A
R
Burton






A
Shenhav






J
D
Cohen






N
D
Daw




10.1073/pnas.2221510120








Proceedings of the National Academy of Sciences




120


50














Optimal foraging, the marginal value theorem




E
L
Charnov




10.1016/0040-5809(76)90040-X








Theoretical Population Biology




9


2
















Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration




J
D
Cohen






S
M
Mcclure






A
J
Yu








Philosophical Transactions of the Royal Society B: Biological Sciences




362


















10.1098/rstb.2007.2098














Learning the opportunity cost of time in a patchforaging task




S
M
Constantino






N
D
Daw








Cognitive, Affective, & Behavioral Neuroscience




15


4


















10.3758/s13415-015-0350-y














Effort, Avolition, and Motivational Experience in Schizophrenia: Analysis of Behavioral and Neuroimaging Data With Relationships to Daily Motivational Experience




A
J
Culbreth






E
K
Moran






S
Kandala






A
Westbrook






D
M
Barch




10.1177/2167702620901558








Clinical Psychological Science




8


3
















Model-Based Influences on Humans' Choices and Striatal Prediction Errors




N
D
Daw






S
J
Gershman






B
Seymour






P
Dayan






R
J
Dolan




10.1016/j.neuron.2011.02.027








Neuron




69


6
















Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control




N
D
Daw






Y
Niv






P
Dayan




10.1038/nn1560








Nature Neuroscience




8


12
















jsPsych: A JavaScript library for creating behavioral experiments in a Web browser




J
R
De Leeuw




10.3758/s13428-014-0458-y








Behavior Research Methods




47


1
















Model-based choices involve prospective neural activity




B
B
Doll






K
D
Duncan






D
A
Simon






D
Shohamy






N
D
Daw




10.1038/nn.3981








Nature Neuroscience




18


5
















Model-based decision making and model-free learning




N
Drummond






Y
Niv




10.1016/j.cub.2020.06.051


R860-R865








Current Biology




30


15














Macaques are risk-averse in a freely moving foraging task




B
R
Eisenreich






B
Y
Hayden






J
Zimmermann




10.1038/s41598-019-51442-z








Scientific Reports




9


1


15091














Humans primarily use model-based inference in the two-stage task




C
Feher Da Silva






T
A
Hare








Nature Human Behaviour




4


10


















10.1038/s41562-020-0905-y














Rethinking model-based and model-free influences on mental effort and striatal prediction errors




C
Feher Da Silva






G
Lombardi






M
Edelson






T
A
Hare




10.1038/s41562-023-01573-1








Nature Human Behaviour




7


6
















Enriching behavioral ecology with reinforcement learning methods




W
E
Frankenhuis






K
Panchanathan






A
G
Barto








Behavioural Processes




161


















10.1016/j.beproc.2018.01.008














Conceptual Note on State, Trait, and the State-Trait Distinction




B
M
Fridhandler








Journal of Personality and Social Psychology




50


1
















Retrospective revaluation in sequential decision making: A tale of two systems




S
J
Gershman






A
B
Markman






A
R
Otto




10.1037/a0030844








Journal of Experimental Psychology: General




143


1
















Characterizing a psychiatric symptom dimension related to deficits in goal-directed control. eLife, 5, e11305




C
M
Gillan






M
Kosinski






R
Whelan






E
A
Phelps






N
D
Daw




10.7554/eLife.11305


















Motivation and cognitive control in depression




I
Grahek






A
Shenhav






S
Musslick






R
M
Krebs






E
H W
Koster








Neuroscience & Biobehavioral Reviews




102


















10.1016/j.neubiorev.2019.04.011














Effort-Based Decision Making: A Novel Approach for Assessing Motivation in Schizophrenia




M
F
Green






W
P
Horan






D
M
Barch






J
M
Gold




10.1093/schbul/sbv071








Schizophrenia Bulletin




41


5
















Control over patch encounters changes foraging behavior. iScience




S
Hall-Mcmaster






P
Dayan






N
W
Schuck




10.1016/j.isci.2021.103005








24


103005












Revisiting foraging approaches in neuroscience




S
Hall-Mcmaster






F
Luyckx
























Cognitive, Affective, & Behavioral Neuroscience




19


2
















10.3758/s13415-018-00682-z














Overharvesting in human patch foraging reflects rational structure learning and adaptive planning




N
C
Harhen






A
M
Bornstein




10.1073/pnas.2216524120








Proceedings of the National Academy of Sciences




120


13














Neuronal basis of sequential foraging decisions in a patchy environment




B
Y
Hayden






J
M
Pearson






M
L
Platt








Nature Neuroscience




14


7


















10.1038/nn.2856














Rats exhibit similar biases in foraging and intertemporal choice tasks. eLife, 8, e48429




G
A
Kane






A
M
Bornstein






A
Shenhav






R
C
Wilson






N
D
Daw






J
D
Cohen




10.7554/eLife.48429


















Increased locus coeruleus tonic activity causes disengagement from a patch-foraging task




G
A
Kane






E
M
Vazey






R
C
Wilson






A
Shenhav






N
D
Daw






G
Aston-Jones






J
D
Cohen








Cognitive, Affective & Behavioral Neuroscience




17


6


















10.3758/s13415-017-0531-y














Evidence for shallow cognitive maps in Schizophrenia




A
B
Karagoz






E
K
Moran






D
M
Barch






W
Kool






Z
M
Reagh




10.3758/s13415-025-01283-3








Cognitive, Affective, & Behavioral Neuroscience
















The construction and use of cognitive maps in model-based control




A
B
Karagoz






Z
M
Reagh






W
Kool




10.1037/xge0001491








Journal of Experimental Psychology: General




153


2
















Adaptive integration of habits into depth-limited planning defines a habitual-goal-directed spectrum




M
Keramati






P
Smittenaar






R
J
Dolan






P
Dayan








Proceedings of the National Academy of Sciences




113


45


















10.1073/pnas.1609094113
















N
Kolling






T
Akam




10.1016/j.conb.2017.08.008








Reinforcement?) Learning to forage optimally






46














Neural Mechanisms of Foraging




N
Kolling






T
E J
Behrens






R
B
Mars






M
F S
Rushworth




10.1126/science.1216930








Science




336


6077
















Mental labour




W
Kool






M
Botvinick




10.1038/s41562-018-0401-9








Nature Human Behaviour




2


12
















When Does Model-Based Control Pay Off?




W
Kool






F
A
Cushman






S
J
Gershman




10.1371/journal.pcbi.1005090








PLOS Computational Biology




12


8














Chapter 7-Competition and Cooperation Between Multiple Reinforcement Learning Systems




W
Kool






F
A
Cushman






S
J
Gershman




10.1016/B978-0-12-812098-9.00007-3








Goal-Directed Decision Making


R. Morris, A. Bornstein, & A. Shenhav




Academic Press
















Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems




W
Kool






S
J
Gershman






F
A
Cushman








Psychological Science




28


9


















10.1177/0956797617708288














Planning Complexity Registers as a Cost in Metacontrol




W
Kool






S
J
Gershman






F
A
Cushman








Journal of Cognitive Neuroscience




30


10


















10.1162/jocn_a_01263














Dopamine Modulates Dynamic Decision-Making during Foraging




C
Le Heron






N
Kolling






O
Plant






A
Kienast






R
Janska






Y.-S
Ang






S
Fallon






M
Husain






M
A J
Apps








The Journal of Neuroscience




40


27


















10.1523/JNEUROSCI.2586-19.2020














Rational metareasoning and the plasticity of cognitive control




F
Lieder






A
Shenhav






S
Musslick






T
L
Griffiths




10.1371/journal.pcbi.1006043








PLOS Computational Biology




14


4














Are adolescents more optimal decision-makers in novel environments? Examining the benefits of heightened exploration in a patch foraging paradigm




A
Lloyd






R
Mckay






C
L
Sebastian






J
H
Balsters




10.1111/desc.13075








Developmental Science




24


4














An Integrative Theory of Prefrontal Cortex Function




E
K
Miller






J
D
Cohen








Annual Review of Neuroscience




24


















10.1146/annurev.neuro.24.1.167














Multi-step planning in the brain




K
J
Miller






S
J C
Venditto




10.1016/j.cobeha.2020.07.003








Current Opinion in Behavioral Sciences






38














Effort-cost decision-making in psychotic and mood disorders




E
K
Moran






C
Prevost






A
J
Culbreth






D
M
Barch




10.1037/abn0000822








Journal of Psychopathology and Clinical Science
















Foraging decisions as multi-armed bandit problems: Applying reinforcement learning algorithms to foraging data




J
Morimoto




10.1016/j.jtbi.2019.02.002








Journal of Theoretical Biology




467
















The shallow cognitive map hypothesis: A hippocampal framework for thought disorder in schizophrenia




A
Musa






S
Khan






M
Mujahid






M
El-Gaby




10.1038/s41537-022-00247-7








8














Aversion to Option Loss in a Restless Bandit Task




D
J
Navarro






P
Tran






N
Baz




10.1007/s42113-018-0010-8








Computational Brain & Behavior




1


2
















Cognitive maps and schizophrenia




M
M
Nour






Y
Liu






M
El-Gaby






R
A
Mccutcheon






R
J
Dolan








Trends in Cognitive Sciences




29


2


















10.1016/j.tics.2024.09.011














The Curse of Planning: Dissecting Multiple Reinforcement-Learning Systems by Taxing the Central Executive




A
R
Otto






S
J
Gershman






A
B
Markman






N
D
Daw




10.1177/0956797612463080








Psychological Science




24


5
















Cognitive Control Predicts Use of Model-based Reinforcement Learning




A
R
Otto






A
Skatova






S
Madlon-Kay






N
D
Daw




10.1162/jocn_a_00709








Journal of Cognitive Neuroscience




27


2
















Incentives Boost Model-Based Control Across a Range of Severity on Several Psychiatric Constructs




E
H
Patzelt






W
Kool






A
J
Millner






S
J
Gershman




10.1016/j.biopsych.2018.06.018








Biological Psychiatry




85


5
















The transdiagnostic structure of mental effort avoidance




E
H
Patzelt






W
Kool






A
J
Millner






S
J
Gershman




10.1038/s41598-018-37802-1








Scientific Reports




9


1


1689














What Is the Model in Model-Based Planning?




T
Pouncy






P
Tsividis






S
J
Gershman




10.1111/cogs.12928








Cognitive Science




45


1














Processing speed enhances model-based over model-free reinforcement learning in the presence of high working memory functioning




D
J
Schad






E
Jünger






M
Sebold






M
Garbusow






N
Bernhardt






A.-H
Javadi






U
S
Zimmermann






M
N
Smolka






A
Heinz






M
A
Rapp






Q
J M
Huys




10.3389/fpsyg.2014.01450








Frontiers in Psychology
















Humans adaptively deploy forward and backward prediction




P
B
Sharp






E
Eldar




10.1038/s41562-024-01930-8








Nature Human Behaviour




8


9
















The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function




A
Shenhav






M
M
Botvinick






J
D
Cohen








Neuron




79


2


















10.1016/j.neuron.2013.07.007














The relationship between intertemporal choice and following the path of least resistance across choices, preferences, and beliefs




A
Shenhav






D
G
Rand






J
D
Greene








Judgment and Decision Making




12


1


















10.1017/S1930297500005209














Disruption of Dorsolateral Prefrontal Cortex Decreases Model-Based in Favor of Model-free Control in Humans




P
Smittenaar






T
H B
Fitzgerald






V
Romei






N
D
Wright






R
J
Dolan




10.1016/j.neuron.2013.08.009








Neuron




80


4
















Decision ecology: Foraging and the ecology of animal decision making




D
W
Stephens
























Cognitive, Affective, & Behavioral Neuroscience




8


4
















10.3758/CABN.8.4.475














Foraging Theory




D
W
Stephens






J
R
Krebs








Princeton University Press














R
S
Sutton






A
G
Barto




Reinforcement Learning




An Introduction. MIT Press








second edition








Animal intelligence: An experimental study of the associative processes in animals




E
L
Thorndike








The Psychological Review: Monograph Supplements




2


4


109
















10.1037/h0092987














Effort-based decisionmaking in major depressive disorder: A translational model of motivational anhedonia




M
T
Treadway






N
A
Bossaller






R
C
Shelton






D
H
Zald




10.1037/a0028813








Journal of Abnormal Psychology




121


3
















Social resource foraging is guided by the principles of the Marginal Value Theorem




C
Turrin






N
A
Fagan






O
Dal Monte






S
W C
Chang




10.1038/s41598-017-11763-3








Scientific Reports




7


1


11274














The exploration-exploitation trade-off in a foraging task is affected by mood-related arousal and valence




R
Van Dooren






R
De Kleijn






B
Hommel






Z
Sjoerds




10.3758/s13415-021-00917-6








Cognitive, Affective, & Behavioral Neuroscience




21


3
















Subjective costs drive overly patient foraging strategies in rats on an intertemporal foraging task




A
M
Wikenheiser






D
W
Stephens






A
D
Redish








Proceedings of the National Academy of Sciences


the National Academy of Sciences






110
















10.1073/pnas.1220738110














Predictive decision making driven by multiple time-linked reward representations in the anterior cingulate cortex




M
K
Wittmann






N
Kolling






R
Akaishi






B
K H
Chau






J
W
Brown






N
Nelissen






M
F S
Rushworth




10.1038/ncomms12327








Nature Communications




7


1


12327














Motivational deficits in effort-based decision making in individuals with subsyndromal depression, first-episode and remitted depression patients




X.-H
Yang






J
Huang






C.-Y
Zhu






Y.-F
Wang






E
F C
Cheung






R
C K
Chan






G.-R
Xie




10.1016/j.psychres.2014.08.056








Psychiatry Research




220


3
















How humans react to changing rewards during visual foraging




J
Zhang






X
Gong






D
Fougnie






J
M
Wolfe








Perception, & Psychophysics




79


8










Attention










10.3758/s13414-017-1411-9















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]