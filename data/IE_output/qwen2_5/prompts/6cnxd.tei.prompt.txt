You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



consistency and hence, results in more valid predictions than holistic prediction 
(Yu & Kuncel, 2020)
, and decision-makers' autonomy is retained through control over the predictor weights 
(Nolan & Highhouse, 2014)
. Alternatively, decision makers could choose different predictor weights for each applicant to be judged. This allows decision makers to account for suspected exceptions to the linear rule, such as interactions. For example, decision makers may weight an applicant's test score less heavily when their high school grade was excellent (for a similar example see MacDonald, 2020). However, this approach still requires decision makers to choose explicit weights, which could increase consistency compared to holistic prediction if decision makers would be consistent in a majority of cases (where they do not expect rule exceptions), while holistic predictions for these applicants may still be inconsistent due to unintentional factors such as fatigue and inattention. This approach contains components of mechanical and holistic prediction; the combination of information does not happen 'in the mind', as in holistic prediction, but is based on explicit weights.
However, because weights are not consistent across individuals, this approach does not meet the definition of mechanical prediction.


Adjusting the Result of a Mechanical Rule
Autonomy is also retained when decision makers can holistically adjust the result after applying a mechanical rule. This procedure, which is also called clinical synthesis 
(Kuncel, 2018;
Sawyer, 1966)
 allows decision makers to account for suspected rule exceptions more directly. Although not explicitly based on self-determination theory, Dietvorst et al. 
2018
conducted a series of experiments in which participants' autonomy in using a mechanical rule to make standardized test performance predictions was varied. The results showed that participants who could adjust the rule's predictions were more confident in and more satisfied with their predictions, chose to use the rule more often, and made more accurate predictions as a result, compared to participants who could not adjust the rule's predictions.
Although predictive validity decreases when decision makers holistically adjust mechanical rule predictions 
(Guay & Parent, 2018;
Hanson & Morton-Bourgon, 2009;
Hoffman et al., 2017;
Neumann et al., 2021)
, there is some evidence that adjusting rule predictions still results in higher predictive validity than pure holistic prediction 
(Dietvorst et al., 2018)
. This is because a rule prediction based on appropriate weights provides a valid "anchor" 
(Dietvorst et al., 2018;
Tversky & Kahneman, 1974)
.
In sum, the results of 
Nolan and Highhouse (2014)
 and 
Dietvorst et al. (2018)
 suggest that the use of mechanical rules can be increased by granting decision makers autonomy over predictor weights or over the rule prediction. However, it remains unclear which of these procedures yields the most favorable results in terms of user perceptions, intentions, and predictive validity.


Aim and Contribution
The aim of the current studies was to investigate the effects of different kinds of autonomy-enhancing features in using a mechanical rule on (1) user perceptions and intentions, and (2) predictive validity, to see if a satisfactory compromise between optimizing autonomy and validity can be established. The present studies extend the studies by 
Nolan and Highhouse (2014)
 and 
Dietvorst et al. (2018)
 in three ways. First, 
Nolan and Highhouse (2014)
 investigated whether autonomy in determining predictor weights affected use intentions for mechanical rules based on vignettes describing different procedures. We instead asked our participants to make actual predictions based on real data. Second, we also investigated the effect of autonomy-enhancement on predictive validity: enhancing decisionmakers' autonomy should not result in a large reduction in predictive validity to be of any practical value. Third, to our knowledge, no studies have jointly investigated autonomy in designing the rule and autonomy through adjusting rule predictions. In the current studies, we included both prediction procedures, to investigate whether attitudes and validity depend on the stage in the decision-making process in which decision makers have autonomy 
(Burton et al., 2020)
. In sum, we respond to the calls for investigations of how decision-makers' acceptance of mechanical prediction procedures can be increased without substantially reducing predictive validity 
(Burton et al., 2020;
Kuncel, 2018;
Kuncel et al., 2013;
Neumann et al., 2020;
Ryan et al., 2015;
Sackett & Lievens, 2008)
.


Study 1
We employed a within-subjects design to ensure sufficient statistical power and to allow for a joint evaluation of the different prediction procedures. This approach yields the most representative measure of use intentions, because in practice, decisions to use certain prediction procedures are made by comparing multiple procedures jointly rather than separately 
(Highhouse et al., 2017;
Hsee & Zhang, 2004;
Nolan et al., 2020)
. Furthermore,
given that many decision makers in practice are unaware of the distinction between holisticand mechanical prediction, a within-subjects design should improve the evaluability of an attribute that is difficult to evaluate due to a lack of knowledge 
(Hsee & Zhang, 2010)
.
In the first four conditions, each participant predicted the first-year GPA and the chance of dropout of five applicants to a psychology undergraduate program, based on three predictors: high school GPA, an admission test score, and a personal statement. In the first condition, participants made holistic predictions. This condition was included so the other conditions could be compared to the prediction procedure typically used in practice. In the second and third condition, participants could design a mechanical rule by choosing general predictor weights (general weights condition) or by choosing applicant-specific predictor weights (individual weights condition), respectively. Including these conditions allowed us to test whether participants would prefer designing rules tailored to the unique individual (individual weights) over designing one consistent rule (general weights). This preference was expected because people often think information should be interpreted in the context of all other information 
(Grove & Meehl, 1996)
, and because decontextualization seems to be a driver of algorithm aversion 
(Newman et al., 2020)
. In the fourth condition -the adjustment condition -participants could adjust the prediction of a statistical model (see 
Dietvorst et al., 2018)
, in a similar way as in typical advice-taking studies 
(Önkal et al., 2009)
. Including this condition allowed us to investigate whether perceptions and use intentions, and predictive validity differ depending on whether participants design the prediction process (choosing general weights) or have the final say at the end (adjusting optimal model predictions), which are two qualitatively different types of autonomy-enhancing features. Lastly, in the fifth condition, participants did not make predictions themselves (no autonomy) but had to rely on optimal regression model predictions which they could not adjust (optimal model condition).
This condition was included to compare the previous four conditions with this optimal but least utilized procedure in practice. Based on the findings of Nolan and Highhouse 
(2014) and
Dietvorst et al. 
2018
, we formulated the following expectations:
Hypothesis 1: Perceived autonomy will decrease from the holistic condition (most autonomy), individual weights condition, general weights condition to the optimal model condition (least autonomy).
We had no specific hypothesis on how the adjustment condition would compare to the two weighting conditions, because participants could adjust optimal model predictions and hence had complete autonomy. Yet, seeing the model prediction may reduce autonomy perceptions compared to making holistic predictions. Therefore, we only expected that perceived autonomy will be higher compared to the optimal model condition, and lower compared to the holistic condition. For brevity, hypothesis 1 is only formulated for perceived autonomy. We expected the same results for use intentions (H2), confidence (H3), and satisfaction (H4).
Hypothesis 5: Predictive validity will increase from the holistic condition (lowest predictive validity), individual weights condition, general weights condition to the optimal model condition (highest predictive validity).
We had no specific hypothesis on how the adjustment condition would compare to the two weighting conditions, but expected lower predictive validity than in the optimal model condition and higher predictive validity than in the holistic condition.


Method Participants
A power analysis 1 for the omnibus F test of a repeated measures ANOVA (hypotheses 1-4) resulted in a required sample size of N = 40 (assuming 1-ß > .8, α = .05, number of measurements = 5, nonsphericity correction ε = 1, and a moderate effect size, f = 0.25, consistent with previous research, Nolan 
& Highhouse, 2014)
. Furthermore, since we had no a priori estimate of the correlation among repeated measures, we set this correlation equal to zero, which results in the most conservative sample size estimation). To be conservative and counteract inflated effect sizes in the published literature 
(Schäfer & Schwarz, 2019)
, we aimed to collect minimally 50 participants, and maximally as many as signed up before the planned closing-date of data collection (see the pre-registration protocol, https://osf.io/fjbvw).
1 Although we used Bayesian parameter estimation to analyze our results, we reported the results from our preregistered frequentist power analyses so that readers who are mostly familiar with frequentist testing can understand that our studies had sufficient power to detect significant differences if we had tested our hypotheses using frequentist null hypothesis significance tests.
Many more participants than minimally required signed up before the planned closingdate of data collection, which resulted in a total of N = 153 first-year psychology students who consented to take part in this study for course credit. Three participants failed one or both of two attention checks (see Appendix A) and were excluded. The final dataset consisted of N = 150 2 participants (17% male, 83% female, < 1% did not want to disclose) who ranged in age from 18 to 39 years (M = 19.85, SD = 2.52). Most participants had the Dutch nationality (97%). The other four participants had German, Brazilian, Russian, and Syrian nationalities.
This study was approved by the ethics committee of the [blinded for review] for psychological research (code: 18255 -S).


Materials
Participants were presented with anonymized archival data from applicants to the psychology undergraduate program of the University of Groningen. Application data of 192 Dutch applicants were used as stimulus material 
(Niessen et al., 2018)
. From this pool, five applicants were randomly allocated (without replacement within participants) to the four conditions in which participants made predictions (i.e., all conditions except the optimal model condition). Applicants were evenly sampled so that every applicant was judged. So, applicants were assigned to participants until all 192 applicants were sampled, after which a new sampling round started. Participants were presented with each applicant's high school GPA, an admission test score, and a personal statement. We used these predictors because they are commonly used in college admission procedures 
(Davis et al., 2018;
Niessen et al., 2018
).
2 We also conducted a sensitivity power analysis for a one-tailed dependent t test, given that pairwise comparisons were most important for answering our hypotheses. Given N = 150, α = .05, and 1-β = .80, the smallest effect size we could detect with sufficient power was dz = 0.20. Furthermore, for predictive validity, we also calculated the required effect size to detect a significant difference between dependent correlations, assuming 1-β > .8; α = .05, and N = 750. The N resulted from 150 participants who each made five judgments in a condition. Given a correlation coefficient of rab = .4, the alternative correlation coefficient must at least be rac = .45, assuming rbc = 0.8.


High School GPA
High school GPA was the mean of all final grades obtained at the end of secondary education (vwo), and was measured on the Dutch ten-point grading scale (1 is lowest, 10 is highest). High school GPA ranged from 5.90 to 8.55 (M = 6.61, SD = 0.46) and was a good predictor of first-year GPA (r = .45), and a moderate predictor of dropout (r = -.23) in this sample.


Admission Test
Applicants had to study two chapters from an introductory psychology book and then took an admission test of 40 multiple-choice items that covered the study material. 'Number of items correct' was transformed to the Dutch ten-point grading scale and ranged from 2.2 to 9.7 (M = 6.42, SD = 1.77). In this sample, the admission test was a good predictor of first-year GPA (r = .40), and a moderate predictor of dropout (r = -.27).


Personal Statement
Before making predictions, participants also rated on a 7-point scale (1 = very unmotivated, 7 = very motivated) applicants' personal statements, in which applicants expressed their motivation to study psychology at the university in a maximum of 250 words.
The mean rating was M = 5.47 (SD = 1.08). For the calculation of predicted scores within conditions, personal statement ratings were transformed to the Dutch 10-point grading scale.
Research suggests that inter-rater reliability of personal statements is low 
(GlenMaye & Oakes, 2002;
Murphy et al., 2009)
. Indeed, inter-rater reliability was also low in this study (ICC = .24). The personal statement rating was a poor predictor of first-year GPA across conditions (!̅ = .05) and of dropout (!̅ = .02).


Procedure and Conditions
Participants completed the study in a lab space at the university in groups of up to 16 participants simultaneously. The experimenter informed participants about the opportunity to earn a reward (up to €5), depending on their prediction accuracy. The reward served to simulate accountability and to encourage effort, as is common in this area of research (e.g., 
Dietvorst et al., 2018)
. All other instructions were presented on a computer screen. The participants were informed that they would first rate personal statements and then predict applicants' first-year GPA and chance of dropout in the first year of the psychology program.
Participants received predictor validity information. They were informed that high school GPA and the admission test were "good predictors", while the personal statement was a "poor predictor" of the outcomes. Participants also received summary statistics (mean, minimum, and maximum) for the predictors high school GPA and the admission test score, and the criteria first-year GPA and dropout as reference.
In the holistic condition, participants saw the applicants' data (i.e., high school GPA, admission test score, personal statement in qualitative form, and their quantitative rating) and holistically predicted applicants' first-year GPA and the chance of dropout.
In the individual weights condition, participants saw the applicants' data and assigned percentage weights to the predictors for each of the five applicants individually. So, participants could assign different predictor weights to different applicants. To calculate the resulting predictive score, each predictor score was multiplied by the individual weight that a participant assigned to this predictor.
In the general weights condition, participants assigned percentage weights to the predictors once, which applied to all of the five applicants to be judged in this condition. Each predictor score was multiplied by the weight that a participant assigned to this predictor.
Additionally, we also wanted to explore whether thinking about predictor importance (by choosing a set of general weights) would by itself result in more consistent and hence more valid holistic predictions that immediately follow after choosing weights, compared to predictions in the holistic condition. Therefore, after participants chose general weights and responded to the attitudinal measures, they again judged five applicants holistically, in the same manner as in the holistic condition. We present the additional details (Section S1) and the results (Section S2) in the supplementary material.
In the adjustment condition, participants saw the applicants' data and a regression model prediction of applicants' first-year GPA and chance of drop out. Participants were told that the model was based on an applicant's high school GPA and admission test score 3 .
Furthermore, they were informed that the model was quite accurate in predicting the criteria, and that it most likely gave the best prediction, better than their own, holistic prediction.
Participants could holistically adjust the model prediction. If they did not want to adjust it, they simply reproduced the model prediction.
In the optimal model condition, participants did not make predictions themselves but imagined that they had to use the regression model predictions based on the same model as in the adjustment condition, without the possibility to adjust it in any way.
Each participant completed each condition. The order of the conditions was counterbalanced 4 , except for the optimal model condition. This condition was always presented last because the adjustment condition always had to be presented before the optimal model condition. The instructions to participants per condition are shown in Appendix B. At the end of each condition, participants answered questions regarding their intentions to use, their confidence in-, their satisfaction with-, and their perceived autonomy in the prediction approach.
3 The model did not include the personal statement because ratings were not available before data collection. However, personal statements have very low predictive validity 
(Murphy et al., 2009)
. Therefore, we expected that personal statement ratings would not significantly improve a model that contained high school GPA and admission test scores, as indeed was the case (first-year GPA, ΔR 2 < .001, F(1,746) = .18, p = .671; dropout, Nagelkerke's pseudo R 2 , ΔR 2 p = .004, c 2 (1) = 1.85, p = .173).
Finally, participants imagined that their task was to select applicants that would achieve a high first-year GPA and would be unlikely to drop out for the upcoming academic year. Participants ranked the different prediction procedures according to which procedure they were most likely to use (rank 1 = most likely, rank 5 = least likely). Participants were informed that their reward they had earned up to this point would be increased by 20% if they chose the procedure that yielded the most accurate predictions (we assumed this to be the optimal model condition, as indeed turned out to be the case). The descriptions of the five procedures were displayed in random order. The median time it took to complete the study was 39 minutes.


Attitudinal Measures
Perceived autonomy was measured with a translated version of the six-item scale (α = .91) from 
Nolan and Highhouse (2014)
. We adapted the three-item scale from 
Nolan and Highhouse (2014)
 to assess the extent to which participants intend to use a certain approach for future admission decisions (α = .83). Confidence and satisfaction were assessed with oneitem measures based on Dietvorst et al. (2018). Participants responded to these measures on 5-point Likert scales. More details about the measures can be found on https://osf.io/86jfb/.


Measures Used for Predictive Validity Hypotheses


First-year GPA
Participants indicated the predicted first-year GPA on a continuous scale (1 = lowest grade; 10 = highest grade), up to one decimal. The predictive validity was operationalized as the correlation between the predicted and the observed first-year GPA.


Chance of Dropout
Participants indicated the predicted chance that an applicant dropped out of the program on a continuous scale from 0% to 50%. This scale was chosen because the regression model did not return a chance of drop out above 50% for any of the applicants.


Analytical Approach
Dichotomous decision making based on p-values has received much criticism 
(Cohen, 1994;
Wasserstein et al., 2019)
. Bayesian approaches have been recommended as a suitable alternative because they have several advantages over null hypothesis significance testing 
(NHST, see Cohen, 1994;
Kruschke, 2015;
Kruschke et al., 2012;
Wagenmakers, 2007)
. For example, multiple comparisons are not problematic in Bayesian approaches because prior knowledge is incorporated into the model 
(Berry & Hochberg, 1999;
Gelman, 2016;
Gelman et al., 2012;
Kruschke, 2015)
. For these reasons, we used a Bayesian approach to analyze our data. We only conducted frequentist tests to analyze the ranks that participants assigned to the prediction procedures because, to our knowledge, there are no Bayesian alternatives for these tests. Moreover, we used Bayesian parameter estimation instead of hypothesis testing 
(Kruschke & Lidell, 2018)
 because we were primarily interested in the magnitude and uncertainty of the effects. Furthermore, the null hypothesis is unlikely to be exactly true in most contexts, so testing it against an alternative is much less informative than estimating effect sizes 
(Kruschke & Liddell, 2018)
.
The Bayesian approach to parameter estimation allows estimating a distribution that displays the credibility of each possible parameter value 
(Kruschke, 2011
(Kruschke, , 2015
. This distribution is commonly summarized by a 95% highest density interval (HDI), which is constructed in such a way that it includes the true parameter with probability .95, conditional on the model and observed data. The advantage of HDIs over the commonly misinterpreted frequentist confidence intervals 
(Morey et al., 2016)
 is that they can be used to make the intuitive statement that there is a 95% probability that the parameter falls within the boundaries of the HDI, conditional on the model and data at hand. Therefore, we reported parameter estimates and the corresponding 95% HDIs.


Results
All analyses were run in R (4.1.1). We had no permission to share the archival application data due to privacy reasons. Therefore, our publicly available (https://osf.io/86jfb/) R scripts and data allow re-running all reported analyses except for the results on predictive validity.


Attitudinal Measures
We ran two-factor mixed-effects Bayesian ANOVAs with the condition as a withinsubject factor (fixed effect) and subjects as a random effect, for each of the four dependent measures. The mean differences and the HDIs for the contrasts as stated in hypotheses 1-4 are reported. 
Figure 1
 displays the observed means per attitudinal measure for each condition.
Furthermore, 
Table 1
 shows the means and standard deviations for each attitudinal measure in each condition, and 
Table 2
 shows correlations between the attitudinal measures. As 
Table 1
 and 2 show, mean patterns and correlations for the confidence-and satisfaction measure were very similar to the mean patterns and correlations of the use intentions measure. Therefore, we report the results for confidence and satisfaction, and more details of the Bayesian analysis in Section S2 of the supplementary material.
-Insert 
Figure 1
 about here --Insert 
Table 1
 about here --Insert 
Table 2
 about here -


Perceived Autonomy
Figure 1 suggests that perceived autonomy was very similar across prediction procedures, except for using optimal model predictions, where perceived autonomy was much lower compared to all other prediction procedures. As expected, autonomy was highest for making holistic predictions and lowest for using optimal model predictions. Participants experienced less autonomy when choosing individual weights than when making holistic predictions, but this difference was negligible in size 
(
 between the holistic-and the optimal model condition. So, hypothesis 1 was mostly unsupported. Although we observed the hypothesized ordering in perceived autonomy, differences between conditions were negligible, except for the optimal model condition, in which participants experienced much less autonomy compared to all other conditions (Cohen's ds > 1.00).


Use Intentions
We found a number of unexpected results. Use intentions were not highest when making holistic predictions (see 
Figure 1
). Use intentions were noticeably higher when As expected, use intentions were slightly lower for using optimal model predictions than for holistic predictions (Mholistic -Moptimal = 0.22, 95% HDI [0.05, 0.39], d = 0.26), and slightly lower for using individual weights than holistic predictions (Mholistic -Mindividual = 0.17, 95% HDI [0.01, 0.34], d = 0.23). Also, use intentions were markedly lower when using optimal model predictions than when choosing general weights (Mgeneral -Moptimal = 0.46, 95% HDI [0.30, 0.63], d = 0.54). In sum, hypothesis 2 was also mostly unsupported, as we did not observe the hypothesized decrease in use intentions from the holistic-to the optimal condition. However, our results suggest that decision makers welcome prediction procedures in which they can choose general weights or adjust optimal model predictions.


Procedure Ranking
Participants also ranked the prediction procedures according to which procedure they were most likely to use. The resulting mean preference ranks and rank frequencies per procedure are displayed in 
Table 3
. The majority of participants (56%) preferred using the adjustment procedure for future admission decisions the most. Of interest was that only 10% preferred making holistic predictions the most and few participants preferred choosing predictor weights the most (individual weights 10%; general weights 11%). Also, few participants (13%) preferred optimal model predictions the most, although they were informed that these predictions would likely be most valid. A Friedman test revealed significant differences in mean ranks between the procedures (χ 2 (4) = 83.781, p < .001). To determine whether participants significantly preferred the adjustment procedure, we compared it to the general weights procedure (second smallest rank) using a Wilcoxon signed rank test.
This difference in mean ranks was moderate to large (z = -5.77, p < .001, r = -.47, 95% CI 
[-.59, -.33]
). So, when participants had to choose a procedure for making valid future admission decisions, they clearly preferred adjusting optimal model predictions.
-Insert 
Table 3
 about here -


Use of Autonomy-enhancing Features
The results in the supplementary material (Section S2) show that almost all participants used the autonomy-enhancing features. Furthermore, we investigated how participants weighted the predictors, by regressing their predictions on the predictors and subsequently calculating relative weights 
(Tonidandel & LeBreton, 2015)
. 
Figure S1
 in the supplementary material shows relative weights for the holistic-, adjustment-, and optimal model condition, and the averaged weights in the general weights condition. Compared to the optimal model, participants weighted the personal statement more heavily in all conditions, and the difference was larger when predicting chance of dropout.


Predictive Validity
For each condition, we computed the correlation coefficient between the predicted scores and the two outcomes (first-year GPA and drop out). We also computed a multiple correlation coefficient based on an optimal regression model for each condition. Because the applicants that were judged differed per condition due to random allocation of applicants to participants, the multiple R based on an optimal regression model differed slightly between conditions. Therefore, we always compared the validity of condition-specific participants' predictions with the validity of a condition-specific optimal regression model 5 . We compared correlations by computing posterior distributions of differences between correlations using the BayesFactor package (0.9.12 -4.2), and we reported the 95% highest density intervals 6 .


First-year GPA
The correlation coefficients are presented in 
Figure 2
. The predictive validity of participants' predictions was similar across conditions. However, as expected, the optimal model resulted in the highest predictive validity in each condition (!̅ = .50) 7 . Contrary to 5 Cross-validated multiple Rs of the optimal regression models are reported in the supplementary material because they barely deviated from the original multiple Rs. 
6
 We had to deviate from our pre-registration here due to an oversight and misunderstanding among the authors with regard to the pre-registered analysis plan. 7 Correlational differences and 95% HDIs between the validity of participants' predictions and optimal model predictions per condition for first-year GPA: Holistic -optimal = -.10, 95% HDI [-.18, -.03]; Individual -expectations, the holistic predictions did not result in the lowest predictive validity (r = .43).
The holistic predictions had higher validity than predictions made based on individual weights (r = .36), and similar validity to predictions made based on general weights (r = .41) and holistically adjusted model predictions (r = .39). Only the optimal model (r = .54) resulted in higher predictive validity than holistic predictions. Other conditions than the optimal condition resulted in slightly lower validity than the holistic condition. These differences were in the opposite direction of our hypothesis.
-Insert 
Figure 2
 about here -


Dropout
The results for predicting dropout mostly aligned with those for predicting first-year GPA (see 
Figure 3
). The holistic predictions did not result in the lowest predictive validity (r = .21), but in validity similar to those based on individual weights (r = .18) and general weights (r = .19). Again, the optimal model had by far the largest predictive validity (!̅ = .39) 8 . Furthermore, adjusted optimal model predictions (r = .30) also resulted in higher validity than holistic predictions (rdiff = -.11, 95% HDI [-.20, -.03]). So, of all autonomyenhancing procedures, only holistically adjusted optimal model predictions were more valid than holistic predictions. 


Exploratory Analyses
In the general weights condition, weights were only consistent for the five applicants that were judged by a single participant. However, each participant chose different weights, which reduced consistency within the entire sample of predictions. We also averaged all participants' weights (see 
Figure S1
 in the supplementary material) to check whether full consistency would increase predictive validity. Averaging weights barely increased the validity for first-year GPA (from r = .41 to r = .42) and dropout (from r = .19 to r = .22). As 
Figure S1
 also shows, there was relatively little variance in the weights that participants chose, which explains why averaged weights did not increase predictive validity much.


Discussion
The results of Study 1 showed that, contrary to expectations, differences in perceived autonomy between all autonomy-enhancing prediction procedures were negligible, as were differences between making holistic predictions and all autonomy-enhancing prediction
procedures. Yet, as predicted, decision makers experienced much more autonomy when making holistic predictions and when using autonomy-enhancing prediction procedures than when using prescribed optimal model predictions. This suggests that participants were relatively insensitive to how autonomy was enhanced (by choosing individual/general weights or by adjusting optimal model predictions). So, hypothesis 1 was mostly unsupported. The results were different for use intentions. Although holistic prediction is predominantly used in practice, participants unexpectedly preferred choosing general predictor weights and adjusting optimal model predictions over all other prediction procedures. So, hypothesis 2 was also mostly unsupported. One explanation may be that these two autonomy-enhancing prediction procedures require less effort than making holistic predictions 
(Nolan & Highhouse, 2014)
, which may also explain why participants disliked choosing weights for each specific applicant. Yet, the optimal model condition, which required the least effort, was also disliked the most.
Choosing one set of predictor weights and holistically adjusting optimal model predictions resulted in high use intentions. However, contrary to expectations, predictive validity barely increased compared to holistic predictions. For predicting first-year GPA, choosing predictor weights and adjusting optimal model predictions resulted in very similar predictive validity compared to holistic predictions. For predicting dropout, only adjusting optimal model predictions resulted in slightly higher predictive validity than holistic predictions. Furthermore, our finding that letting participants choose one set of predictor weights did not result in higher validity than holistic predictions cannot be explained by a lack of consistency between participants 
(Yu & Kuncel, 2020)
, as averaging weights across participants barely increased predictive validity. An alternative explanation may be that participants weighted the personal statement more heavily when choosing general weights than when making holistic predictions (as shown in 
Figure S1
 in the supplementary material).
In general, our findings are only partially in line with results presented by Dietvorst et al.
(2018), who found that holistically adjusting optimal model predictions resulted in higher validity than fully holistic predictions.


Study 2
The aim of Study 2 was to answer a number of questions that were raised by Study 1.
In Study 1, holistic predictions were surprisingly valid. This may have been a result of providing participants with predictor validity information, which generally increases validity 
(Balzer et al., 1989)
, likely due to increased consistency. However, in practice, predictor validity information may be unavailable, or less salient than it was for the participants in Study 1. Therefore, we randomly varied between subjects whether participants received predictor validity information (presented in the same way as in Study 1) or not, expecting that participants would make more valid predictions when validity information is available. 
Dietvorst et al. (2018)
 found that decision makers used mechanical rule predictions as long as they could adjust them holistically, regardless of the extent of allowed adjustments. Therefore, to improve predictive validity in the adjustment condition, we restricted the extent to which participants could adjust optimal model predictions. Participants could adjust model predictions of first-year GPA by 0.5 grade points (upward or downward), and model predictions of the chance of dropout by 5%. Restricting participants' adjustment behavior allowed us, together with the results from Study 1, to formulate more specific hypotheses. We expected that participants would perceive less autonomy when restrictedly adjusting optimal model predictions compared to choosing predictor weights (cf. hypothesis 6). Yet, based on the results presented by Dietvorst et al. 
2018
, we expected that use intentions would be similar to Study 1, where adjustment was unrestricted. Furthermore, we expected that restrictedly adjusting optimal model predictions would result in more valid predictions than freely choosing consistent predictor weights without valid reference points (cf. hypothesis 11).
A couple of changes were made to investigate attitudes towards prediction procedures more elaborately. At the end of the general weights condition, participants completed the attitudinal measures twice, assuming that (1) their own weights would be used and (2) their weights would be averaged with the weights from other participants. This was done to explore how decision makers would perceive a procedure in which solely their own weights are used, compared to a procedure where their weights are averaged with weights of other decision makers. Participants could also choose this "averaged weights procedure" when rank-ordering the prediction procedures according to which procedure they would most likely use. We also asked participants to report how valid they thought each predictor was. This was done because decision-makers' beliefs in predictor validities do not always align with empirical validities 
(Rynes et al., 2002)
. However, such differences between beliefs and empirical validities are important to investigate because beliefs may partly determine how decision makers weight predictors 
(Kuncel, 2018)
.
In contrast to Study 1, Study 2 was conducted online, and each condition included ten predictions and a prediction trial to familiarize participants with the task before their prediction was rewarded (as in Study 1, participants could earn a reward up to €5).
Furthermore, to reduce the duration of the study, we dropped the holistic predictions at the end of the general weights condition as well as the individual weights condition, which was disliked by participants and resulted in low validity. Moreover, in the optimal model condition, we actually showed participants the regression model predictions that they could not adjust. This allowed us to counterbalance all conditions 9 . The median time it took to complete the study was 64 minutes.
Hypothesis 6: Perceived autonomy will decrease from the holistic condition (most autonomy), general weights condition, adjustment condition, to the optimal model condition (least autonomy).
Hypothesis 7: Participants will report higher use intentions in the general weights condition and the adjustment condition than in the optimal model condition, and higher use intentions in the holistic condition than in the optimal model condition.
We had the same hypotheses for confidence (H8) and satisfaction (H9).
9 As in Study 1, the order of conditions had a negligible effect on participants' behavior and differences in predictive validity were in the opposite direction of expectations. We present the results in the supplementary material (Section S3).
Hypothesis 10: Participants who receive predictor validity information will make predictions that result in higher predictive validity in each condition (except for the optimal model condition), than participants who do not receive predictor validity information.
Hypothesis 11: Predictive validity will increase from the holistic condition (lowest predictive validity), general weights condition, adjustment condition to the optimal model condition.


Method Participants
We conducted power analyses for the repeated measures ANOVAs (hypotheses 6-9) and for the difference tests between two independent correlations (hypothesis 10). The latter power analysis required the highest sample size (N = 192), assuming 1-ß > .8, α = .05, allocation ratio N2/N1 = 1, predictions per condition = 10, and an effect size of q = 0.11 based on the results from Study 1. We recruited participants in two ways. Participants recruited via the research platform of the University of Groningen received €10 compensation for their participation. Participants from the first-year psychology student pool took part for course credit. In total, 269 participants completed the experiment. No participants from Study 1 participated in Study 2. We excluded participants who failed at least one of three attention checks (see Appendix A) or completed the study in less than 20 minutes (n = 77). The mean age of the final sample (N = 192) was M = 21.69 (SD = 6.04, range 16-64) and the majority of participants was female (77%) and Dutch (93%). All other participants had another European nationality (7%). Furthermore, 88% were enrolled as a student, 11% were employed and 1% was unemployed. This study was approved by the ethics committee of the [blinded for review] for psychological research (code: PSY-1920-S-0120).


Measures and Stimulus Material
We used the exact same attitudinal measures as in Study 1. The scales for use intentions and autonomy showed good reliability (α = .87 and α = .92). Furthermore, we used a one-item measure to assess how effective participants considered each predictor to be (1 = not effective; 5 = very effective). As in Study 1, the inter-rater reliability for the personal statement ratings was low (ICC = .11). The personal statement rating was a poor predictor of first-year GPA (!̅ = .06) and dropout (!̅ = -.02).


Results


Attitudinal Measures
We ran two-factor mixed-effects Bayesian ANOVAs with the condition as a withinsubject factor (fixed effect) and subjects as a random effect, for each of the four dependent measures. The between-subjects factor was not included in the ANOVAs 10 because we only hypothesized differences between the two levels of the between-subjects factor for predictive validity. 
Figure 4
 displays the observed means per attitudinal measure for each condition.
Means and standard deviations for each attitudinal measure in each condition are reported in 
Table 4
 and correlations between attitudinal measures in 
Table 5
. As in Study 1, mean patterns and correlations for the confidence-and satisfaction measures were very similar to the mean patterns and correlations of the use intentions measure. Therefore, we report the results for confidence and satisfaction in the supplementary material (Section S3).
-Insert 
Figure 4
 about here --Insert 
Table 4
 about here --Insert 
Table 5
 about here -


Perceived Autonomy
In general, we observed the decreasing trend in perceived autonomy in line with hypothesis 6. Although perceived autonomy was very similar for making holistic predictions and for choosing predictor weights (Mholistic -Mgeneral = 0.002, 95% HDI 
[-0.12, 0.12]
, d = 0.004), it was higher for choosing predictor weights than for adjusting optimal model predictions (Mgeneral -Madjust = 0.25, 95% HDI [0.12, 0.37], d = 0.41). This was expected (cf. hypothesis 6) because we restricted the range in which participants could adjust optimal model predictions, but not the range of predictor weights they could choose. Furthermore, as expected, participants perceived much more autonomy when they could adjust optimal model predictions than when they had to use optimal model predictions (Madjust -Moptimal = 0.79, 95% HDI [0.67, 0.92], d = 1.03). So, hypothesis 6 was mostly supported.


Use intentions
As expected, participants' use intentions were much higher for choosing predictor weights than for using optimal model predictions (Mgeneral -Moptimal = 0.65, 95% HDI [0.53, 0.79], d = 0.81), and higher for adjusting optimal model predictions than for using optimal model predictions (Madjust -Moptimal = 0.49, 95% HDI [0.36, 0.62], d = 0.58). Furthermore, participants showed higher intentions to use holistic predictions compared to optimal model predictions (Mholistic -Moptimal = 0.25, 95% HDI [0.12, 0.38], d = 0.29). Therefore, hypothesis 7 was fully supported.


Procedure Ranking
The mean ranks and rank frequencies per procedure are displayed in 
Table 3
. The adjustment procedure was most often (44%) ranked as most preferred for making future admission decisions. In contrast, only 14% of participants preferred to use optimal model predictions the most, although, as in Study 1, they were informed that this procedure would likely be most valid. Also, only a minority (11%) preferred the holistic procedure the most.
Interestingly, participants ranked the "averaged weights procedure" first more often than the procedure where solely their own weights were used, although this difference was small 
(19% vs. 13%)
. A Friedman test revealed significant differences in mean ranks between the procedures, χ 2 (4) = 112.83, p < .001. A follow-up Wilcoxon signed rank test showed that participants moderately preferred to adjust optimal model predictions compared to using averaged weights (second smallest rank, z = -3.70, p < .001, r = -.27, 95% CI 
[-.40, -.13]
).


Validity Beliefs
To investigate whether participants actually believed in the presented predictor validity information, we compared beliefs between participants who did and those who did  
11
 We had pre-registered that we would also provide Bayes Factors for differences in validity beliefs. For simplicity and consistency reasons, we decided to stick to Bayesian parameter estimation throughout the paper and therefore reported mean differences and 95% highest density intervals.


Use of Autonomy-enhancing Features
The results presented in the supplementary material (Section S3) show that, as in Study 1, almost all participants made use of the autonomy-enhancing features. Furthermore, the relative weights analysis showed that, compared to the optimal model, participants weighted the personal statement more heavily in all conditions, and the difference was larger when predicting chance of dropout (see 
Figure S2)
. Moreover, participants who did not receive validity information weighted the personal statement slightly more heavily than participants who received validity information.


Predictive Validity
First-year GPA 
Figure 5
 displays the validity of participants' predictions for those who did and did not receive validity information separately, and the optimal model validity. In the holistic-and adjustment condition, predictive validity was very similar between participants who received validity information and participants who did not receive validity information (holistic: r = .31 and r = .32; rdiff = -.01, 95% HDI [-.09, .07] and adjustment: r = .34 and r = .36; rdiff = -.02, 95% HDI [-.10, .05], respectively). However, in the general weights condition, participants who received validity information made slightly more accurate predictions than participants who did not receive validity information (r = .41 and r = .34; rdiff = .07, 95% HDI 
[-.01, .15]
). So, presenting participants with validity information only increased predictive validity in the general weights condition, albeit only slightly (partial support for hypothesis 10).
Hypothesis 11 stated that predictive validity increases from the holistic condition to the general weights condition to the adjustment condition to the optimal model condition. For participants who did not receive predictor validity information, predictive validity slightly increased from the holistic condition (r = .32) to the general weights condition (r = .34) to the adjustment condition (r = .36) to the optimal model condition (!̅ = .41). For participants who received predictor validity information, predictive validity increased markedly from the holistic condition (r = .31) to the general weights condition (r = .41). However, predictive validity in the adjustment condition (r = .34) was not higher than predictive validity in the general weights condition. Furthermore, optimal model predictions were as valid (!̅ = .41) as predictions in the general weights condition. So, we found some evidence for the predicted increase in predictive validity from the holistic condition to the optimal model condition, although the general weights condition resulted in higher predictive validity than the adjustment condition when participants had predictor validity information (partial support for H11).
-Insert 
Figure 5
 about here -Dropout 
Figure 6
 displays the validity of participants' predictions and the optimal model validity for predicting dropout. These results are quite different from the results obtained for predicting first-year GPA. Unexpectedly, in the holistic condition, participants who had validity information made less accurate predictions than participants without validity information (r = .18 and r = .25; rdiff = -.07, 95% HDI 
[-.16, .01]
). In the general weights-and adjustment condition, participants with validity information made similarly accurate predictions as participants without validity information (general: r = .23 and r = .20; rdiff = .03, 95% HDI 
[-.06, .11]
 and adjustment: r = .35 and r = .37; rdiff = -.03, 95% HDI 
[-.11, .05
], respectively).
For participants who did not receive predictor validity information, predictive validity decreased from the holistic condition (r = .25) to the general weights condition (r = .20).
However, the adjustment condition (r = .37) resulted in more accurate predictions than the general weights condition. Furthermore, optimal model predictions (!̅ = .36) were as valid as predictions in the adjustment condition. For participants who received predictor validity information, predictive validity increased from the holistic condition (r = .17) to the general weights condition (r = .23) to the adjustment condition (r = .35). So, support for hypothesis 11 differed depending on whether validity information was provided. We found support for hypothesis 11 when validity information was provided, but inconsistent results when validity information was not provided.
-Insert 
Figure 6
 about here -


Exploratory Analysis


Averaged General Weights Condition
If decision makers determine predictor weights in practice, it is likely that multiple decision makers are involved. To resemble this scenario, we also asked participants to report their attitudes towards using weights that are averaged across participants. In contrast to the results from the ranking procedure, these results showed that participants liked the idea to use averaged weights less than using their own weights. Compared to using their own weights, participants experienced less autonomy in an approach where weights would be averaged using averaged weights did not increase predictive validity compared to using participants' own weights when validity information was present (r = .41 and r = .41 for first-year GPA and r = .23 and r = .23 for dropout). When validity information was absent, averaging weights only slightly increased predictive validity (from r = .34 to r = .39 for first-year GPA and from r = .20 to r = .24 for dropout). In sum, using averaged weights resulted in more negative attitudes, but in very similar predictive validity compared to using participants' own weights.


Discussion
In line with the results from Study 1 and our expectations, participants experienced much more autonomy when making holistic predictions and when using autonomy-enhancing prediction procedures than when using optimal model predictions. Furthermore, as expected, participants perceived less autonomy when restrictedly adjusting optimal model predictions than when freely choosing predictor weights. Like in Study 1, participants were more likely to use autonomy-enhancing prediction procedures compared to using holistic-and optimal model predictions. In terms of validity, presenting participants with validity information only increased validity when choosing predictor weights for predicting first-year GPA, albeit only slightly. So, the unexpectedly high validity in the holistic condition does not seem to be fully explained by the availability of validity information. Furthermore, the expectation that validity would increase from the holistic condition, general weights condition, adjustment condition to the optimal condition (hypothesis 11) was mostly unsupported; we found this pattern for dropout predictions, but only when validity information was available.
While we did not have an a priori hypothesis, our results suggest that it may be somewhat easier for decision makers to translate validity information into explicit predictor weights than to use this information holistically, although earlier research suggests that validity information also helps when making holistic predictions 
(Balzer et al., 1989)
. Our results also showed that predictor validity beliefs were better aligned with actual predictor validities when participants received validity information, which was also reflected in the predictor weights that participants chose (see 
Figure S2
). Interestingly though, even when validity information was present, participants considered the personal statement to be more important for dropout predictions than for first-year GPA predictions. This underscores that, besides criterion-related validity, face validity can play an important role in decision making (also see 
Kuncel, 2018, p. 475
).


General Discussion
In two studies, we investigated the effect of several autonomy-enhancing features that participants had when making predictions on perceptions and predictive validity. In contrast to expectations, most of our hypotheses were not supported. Yet, in both studies, perceived autonomy was much higher in all autonomy-enhancing prediction procedures than when using optimal model predictions. In line with earlier findings by 
Nolan and Highhouse (2014)
 and Dietvorst et al. 
2018
, we found clear evidence that participants preferred choosing their own, consistent (but not applicant-specific) predictor weights and adjusting optimal model predictions over holistic-and optimal model predictions. So, although perceived autonomy was similar for holistic predictions and autonomy-enhancing prediction procedures, use intentions differed between these conditions, suggesting that other factors besides autonomy affect use intentions.
We found mixed evidence that autonomy-enhancing prediction procedures resulted in higher validity than holistic predictions. In Study 1, only adjusted optimal model predictions of dropout were more valid than holistic predictions. In Study 2, restrictedly adjusting optimal model predictions resulted in slightly higher validity than holistic predictions, as did choosing predictor weights, but only for predicting first-year GPA and only when predictor validity information was available. So, our results tentatively suggest that the tradeoff between autonomy need satisfaction and validity is optimized when decision makers can restrictedly adjust optimal model predictions or choose consistent predictor weights when validity information is available to decision makers.
In earlier research, choosing predictor weights 
(Nolan & Highhouse, 2014
) and adjusting optimal model predictions (Dietvorst et al., 2018) as a means to overcome algorithm aversion have been investigated in separate studies. We investigated both prediction procedures to allow for a comparison in attitudes and validity. Extending earlier research, we showed that use intentions for these two autonomy-enhancing prediction procedures were similar, although ranking the different prediction procedures resulted in a preference for adjusting optimal model predictions. One reason why these results were different may be that we explicitly asked and incentivized participants to rank the prediction procedures according to predictive validity, while the use intentions scale items were generic and did not explicitly ask participants to evaluate the procedures with regard to predictive validity. Aside from predictive validity, decision makers also consider other aspects of selection procedures, such as costs, effort, and fairness 
(König et al., 2010;
Neumann et al., 2020;
Pyburn et al., 2008)
. It may be that decision makers prefer adjusting optimal model predictions when predictive validity is the most important aspect, because they are confident that they can find valid rule exceptions 
(Dietvorst et al., 2018)
. When also considering other aspects, choosing predictor weights may be preferred because this procedure does not require judging each applicant individually and treats each applicant equally 
(Nolan et al., 2016)
.
Relatedly, results based on the use intentions scale suggested that participants preferred solely using their own weights over the averaged weights procedure. In contrast, results based on the ranking procedure showed that participants had a small preference for the averaged weights procedure over solely using their own weights. This suggests that decision makers may trust 'the wisdom of the crowd' when predictive validity is most important.
Another explanation for these findings may be that the joint evaluation mode in the ranking task allowed participants to differentiate more easily between prediction procedures than the separate evaluation mode of the use intentions measure 
(Hsee & Zhang, 2010)
.


Limitations and Future Research
Compared to our student samples, experienced professionals may evaluate prediction procedures such as choosing predictor weights and adjusting optimal model predictions more negatively, as they may feel less need for prediction support 
(Arkes et al., 1986;
Dawes, 1976
). Therefore, a replication with experienced professionals such admission officers, HR professionals, or clinical psychologists would be very valuable.
Our choice to use a within-subjects design had advantages and disadvantages. Besides increasing power, it allowed for a joint evaluation of prediction procedures 
(Nolan et al., 2020)
. This is most representative of decision making in practice when deciding on what procedure to adopt. Yet, it is plausible that smaller effect sizes would have been observed for our attitudinal measures in a between-subjects design (i.e., in separate evaluation) because prediction procedures could not be easily compared. Furthermore, the within-subjects design required us to vary the stimuli (applicants) between conditions, which resulted in small variations in the optimal model validity. Therefore, validity differences between prediction procedures could be partly due to sampling error variance. Another consequence of our within-subjects design may have been that participants applied the prediction procedure they experienced first also in later conditions, although our results in the supplementary material suggest that the condition order had a negligible effect on participants' behavior. In any case, it should be noted that earlier studies utilized a between-subjects design, which may explain why our results were only partially in line with the results found by 
Dietvorst et al. (2018)
.
For these reasons, future studies could use a between-subjects design and the same applicants across conditions. Lastly, in the adjustment condition, participants knew that the regression model predictions did not include the personal statement. Although adding the personal statement ratings as a predictor would have barely changed the model predictions, participants may have adjusted model predictions less if this predictor was included.
In our studies, we labelled predictors as "good" or "bad" for ease of communication.
These descriptions were rather unspecific and may explain why decision makers still assigned a non-trivial weight to the personal statement. Therefore, validity information may be presented more specifically and in different formats, such as in the form of suggested percentage predictor weights or even in a narrative manner. Alternatively, in future studies, decision-makers' choice in predictor weights may be restricted to avoid that poor predictors receive a substantial weight. However, restricting the choice of weights or the adjustment of rule predictions requires someone to set these restrictions and to control whether decision makers adhere to it, which may be difficult in practice.
Researchers are also encouraged to investigate predictive validity of-and decisionmakers' attitudes towards a prediction procedure that combines autonomy in the design of a mechanical rule and in the adjustment of the rule's result. Based on cognitive dissonance theory 
(Festinger, 1957)
, it may be expected that decision makers are less likely to adjust a rule's prediction when they have designed the rule themselves.
Moreover, the nature of predictors may influence the effort that is required to process information, which could affect intentions to use certain prediction procedures and how decision makers weight information when making predictions. Therefore, the nature of predictors could be varied in future studies. For example, predictors could be presented quantitatively (e.g., test scores or interview ratings) or qualitatively (e.g., recordings of an interview, written personal statements). Also, researchers may investigate other selection contexts than admission to higher education, and vary the manner in which the criteria are predicted. For example, in personnel selection, decision makers may predict applicants' job performance by classifying them into more general categories (e.g., red, yellow, green; see 
Hoffman et al., 2017)
. When decision makers are restricted in adjusting optimal model predictions, they may experience restrictions on shifts between categories as more consequential than on adjustments to continuous performance predictions.
Lastly, researchers may investigate how alternative prediction procedures satisfy decision-makers' competence-and relatedness needs. There exists some evidence that competence needs are less satisfied in mechanical prediction, compared to holistic prediction 
(Nolan, 2013)
 because decision makers cannot demonstrate their assumed expertise in combining information 
(Nolan et al., 2016)
. Relatedness needs may be reduced in mechanical prediction because holistic prediction often takes place in group settings 
(Bolander & Sandberg, 2013)
. In future research, it could be investigated how relatedness needs are satisfied when decision makers design a mechanical rule in a group setting rather than individually as in our studies. Relatedly, it should be investigated whether stakeholders give more credit to decision makers who use a self-designed, rather than an existing mechanical rule 
(Nolan et al., 2016
(Nolan et al., , 2020
.


Practical Recommendations
Our results suggest that, compared to holistic prediction, slightly more valid predictions were made when decision makers could restrictedly adjust optimal model predictions. This approach requires that an optimal rule is available, which is not always the case in practice. Alternatively, decision makers could choose a consistent set of predictor weights themselves. Yet, when predictors substantially differ in predictive validity, which is often the case in practice, care should be taken that decision makers choose appropriate weights. Our results suggest that one way to support decision makers in designing their own mechanical rules effectively is to provide them with predictor validity information.


Conclusion
One reason for the underutilization of mechanical prediction is that it restricts decision-makers' autonomy in the decision-making process. We contribute to the literature on interventions to promote the use of mechanical prediction in practice 
(Dietvorst et al., 2018;
Neumann et al., 2020)
 by showing that decision makers are much more likely to use mechanical prediction procedures when their autonomy is enhanced by the option to choose their own, consistent predictor weights or to adjust mechanical rule predictions. However, contrary to expectations, we found inconclusive evidence that these autonomy-enhancing mechanical prediction procedures resulted in higher predictive validity than holistic predictions, and the increase in predictive validity was rather small. To close the sciencepractice gap in decision making, more research is needed to identify ways to increase the validity of autonomy-enhancing mechanical prediction procedures.       Note. Error bars represent 95% confidence intervals. Some jittering was added to the plot to improve readability. Validity for the Prediction of Dropout in Study 2
Note. Error bars represent 95% confidence intervals.


Attention Checks


Study 1
Participants answered two attention checks. After reading the study instructions, the first attention check required participants to indicate the maximum reward they could earn in the study. The second attention check required participants to indicate for each predictor (high school GPA, admission test score, and personal statement) whether it is a good or bad predictor of study success.


Study 2
Participants answered three attention checks. After reading the study instructions, the first attention check required participants to indicate the maximum reward they could earn in the study. For participants who received validity information, the second attention check was the same as in Study 1. Participants who did not receive validity information had to choose which predictor they would not use for making predictions (answer options: mean high school GPA, admission test score, personal statement, sex). The third attention check was administered before participants ranked the prediction procedures and required them to answer by how much their reward earned up to that point would increase if they chose the most valid procedure (correct answer: 20%).


Instructions to Participants


Table B1
Instructions given to Participants in Study 1 per Condition (translated from Dutch to English)
Condition Instructions


Holistic
In this condition, we first ask you to rate a student's personal statement. After you have given your rating, you will see a student's score on the admission test, high school GPA, and your personal statement rating.
Individual weights In this condition, we first ask you to rate a student's personal statement. After you have given your rating, you will see a student's admission test score, high school GPA, and your personal statement rating. Based on this information, you indicate how you would like to weight the admission test score, high school GPA, and personal statement rating to predict a student's study success. The higher the weight that you assign, the more the information determines your prediction. If you, for example, assign a weight of 33.3% to the admission test score, high school GPA, and the personal statement rating, then your prediction of a student's first-year GPA will be determined with 33.3% by the admission test score, the high school GPA and the personal statement rating, respectively.
In this condition, you assign weights to the information for each of the five students separately.


General weights
In this condition, we first ask you to indicate how you would like to weight the information for making predictions. We would like you to indicate how you want to weight the admission test score, high school GPA, and personal statement rating to predict a student's study success. The higher the weight that you assign, the more the information determines your prediction. If you, for example, assign a weight of 33.3% to the admission test score, high school GPA, and the personal statement rating, then your prediction of a student's first-year GPA will be determined with 33.3% by the admission test score, the high school GPA and the personal statement rating, respectively.
In this condition, you assign weights to the information, which will apply to all five students that you evaluate in this condition.
After you have done this, you will answer some questions about your thoughts and opinion with regard to this prediction procedure.
Afterwards, you will rate a student's personal statement, after which you will make predictions for each of the five students separately.


Adjustment
In this condition, we first ask you to rate a student's personal statement. After you have given your rating, you will see a prediction of the student's study success based on a statistical model. The statistical model is based on the student's high school GPA and admission test score, and applies weights to this information such that the prediction based on this information is as optimal as possible. The personal statement is not taken into account.
The statistical model is quite accurate in predicting a student's firstyear GPA and chance of dropout in the psychology bachelor
program. The statistical model most likely gives the best prediction possible, better than a human prediction. The prediction based on this model is not perfect though. You can adjust the model prediction in case you want to do so. If you do not want to adjust the model prediction, you simply indicate the model prediction.


Optimal model
Imagine that your predictions would be solely determined by the statistical model, and that you could not adjust the model predictions.
Mholistic -Mindividual = 0.03, 95% HDI [-0.10, 0.16], d = 0.05). Similarly, the difference in perceived autonomy between choosing general-and individual weights was negligible (Mindividual -Mgeneral = 0.05, 95% HDI [-0.09, 0.18], d = 0.07). However, participants experienced much less autonomy when using optimal model predictions compared to choosing general weights (Mgeneral -Moptimal = 0.96, 95% HDI [0.84, 1.09], d = 1.18). Furthermore, perceived autonomy in the adjustment condition fell


choosing general-rather than individual weights (Mindividual -Mgeneral = -0.42, 95% HDI [-0.58, -0.25], d = -0.54), and also slightly higher for adjusting optimal model predictions than for making holistic predictions (Mholistic -Madjust = -0.19, 95% HDI [-0.35, -0.03], d = -0.24). Although, as expected, using optimal model predictions resulted in the lowest use intentions, the difference was practically indistinguishable (d = 0.05) from the intentions to use individual weights.


-
Insert Figure 3 about hereoptimal = -.09, 95% HDI [-.18, -.01]; General -optimal = -.13, 95% HDI [-.21, -.05]; Adjust -optimal = -.11, 95% HDI [-.20, -.04].8 Correlational differences and 95% HDIs between the validity of participants' predictions and optimal model predictions per condition for dropout: Holistic -optimal = -.20, 95% HDI[-.28, -.10]; Individual -optimal = -.12, 95% HDI[-.22, -.03]; General -optimal = -.12, 95% HDI[-.22, -.03]; Adjust -optimal = -.09, 95% HDI[- .18, -.01].


not receive predictor validity information 11 . Participants who received validity information considered the predictor high school grade to be more effective (M = 3.97, SD = 0.68) than participants who did not receive validity information (M = 3.67, SD = 0.74, Mdiff = 0.29, 95% HDI [0.09, 0.49], d = 0.41). Validity beliefs regarding the effectiveness of the admission test score were similar for participants who received validity information (M = 4.16, SD = 0.64) and those who did not receive validity information (M = 4.21, SD = 0.76, Mdiff = -0.05, 95% HDI [-0.25, 0.14], d = -0.08). Lastly, participants who received validity information considered the personal statement to be much less effective (M = 2.45, SD = 0.92) than participants who did not receive validity information (M = 3.39, SD = 1.05, Mdiff = -0.92, 95% HDI [-1.21, -0.63], d = -0.95). This shows that predictor validity beliefs were more aligned with actual predictor validities when participants received validity information.


(
Mdiff = 0.29, 95% HDI [0.19, 0.39], d = 0.45). We obtained similar results for use intentions (Mdiff = 0.20, 95% HDI [0.09, 0.31], d = 0.29), confidence (Mdiff = 0.18, 95% HDI [0.06, 0.29], d = 0.25), and satisfaction (Mdiff = 0.22, 95% HDI [0.11, 0.33], d = 0.32). Furthermore,


Figure 1
1
Observed Means (Descriptives) for all Conditions per Attitude Measure in Study 1 Note. Error bars represent 95% confidence intervals. Some jittering was added to the plot to improve readability.


Figure 2 Figure 3
23
Validity for the Prediction of First-year GPA in Study 1 Note. Error bars represent 95% confidence intervals. DECISION-MAKING IN SELECTION 57 Validity for the Prediction of Dropout in Study 1 Note. Error bars represent 95% confidence intervals.


Figure 4
4
Observed Means (Descriptives) for all Conditions per Attitude Measure in Study 2


Figure 5
5
Validity for the Prediction of First-year GPA in Study 2Note. Error bars represent 95% confidence intervals.


Table 1
1
Means and Standard Deviations per Condition and Attitudinal Measure in Study 1
Perceived autonomy Use intentions
Confidence
Satisfaction
Condition
M
SD
M
SD
M
SD
M
SD
Holistic
3.43
0.57
2.98
0.76
3.11
0.67
3.30
0.73
Individual
3.40
0.63
2.80
0.78
3.01
0.75
3.13
0.81
General
3.36
0.65
3.20
0.76
3.33
0.69
3.45
0.68
Adjust
3.21
0.64
3.17
0.80
3.37
0.69
3.41
0.73
Optimal
2.39
0.95
2.76
0.94
3.13
0.90
2.99
1.00


Table 2
2
Correlations between Attitudinal Measures Autonomy, Use Intentions, Confidence, and
Satisfaction in Study 1
Measure
Autonomy
Use intentions
Confidence
Satisfaction
Autonomy
-
Use intentions
.33*
-
Confidence
.27*
.63*
-
Satisfaction
.40*
.70*
.66*
-
Note. Correlations were averaged across conditions using Fisher's z transformation.
Correlations between attitudinal measures per condition are reported in the supplementary
material in Table S1. * p < .05.


Table 3
3
Mean Rank per Condition and Rank Frequencies in Study 1 and Study 2
Study
Condition
Mean rank
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Study 1
Holistic
3.53
15
24
27
34
50
Individual
3.13
15
34
37
44
20
General
3.11
16
31
47
32
24
Adjust
1.99
84
21
15
23
7
Optimal
3.23
20
40
24
17
49
Study 2
Holistic
3.75
21
16
35
38
82
General
3.14
24
38
43
62
25
General averaged
2.70
36
50
59
29
18
Adjust
2.15
84
45
25
27
11
Optimal
3.27
27
43
30
36


Table 4
4
Means and Standard Deviations per Condition and Attitudinal Measure in Study 2 Correlations between Attitudinal Measures Autonomy, Use Intentions, Confidence, and Satisfaction in Study 2Note. Correlations were averaged across conditions using Fisher's z transformation.Correlations between attitudinal measures per condition are reported in the supplementary material inTable S3. * p < .05.
Perceived autonomy Use intentions
Confidence
Satisfaction
Condition
M
SD
M
SD
M
SD
M
SD
Holistic
3.65
0.70
2.97
0.80
3.15
0.78
3.29
0.82
General
3.65
0.61
3.37
0.73
3.51
0.73
3.56
0.71
Adjust
3.40
0.61
3.20
0.81
3.47
0.77
3.52
0.79
Optimal
2.61
0.90
2.72
0.87
3.08
0.79
2.96
0.97


We checked for order effects for predictive validity. The order of conditions had a negligible effect on participants' behavior and differences in predictive validity were in the opposite direction of expectations. We present the results in the supplementary material (Section S2).


We also ran the ANOVAs with the between-subjects factor included. The main effect for the between-subjects factor and the interaction effect showed negligible effect sizes (η 2 ps ≤ .014).








Appendix A
Appendix B
 










The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction




S
Aegisdóttir






M
J
White






P
M
Spengler






A
S
Maugherman






L
A
Anderson






R
S
Cook






C
N
Nichols






G
K
Lampropoulos






B
S
Walker






G
Cohen






J
D
Rush




10.1177/0011000005285875








The Counseling Psychologist




34


3
















Being an advocate for linear models of judgment is not an easy life




H
R
Arkes


















Rationality and social responsibility: Essays in honor of Robyn Mason Dawes


I. Krueger




Psychology Press














Factors influencing the use of a decision rule in a probabilistic task




H
R
Arkes






R
M
Dawes






C
Christensen




10.1016/0749-5978








Organizational Behavior and Human Decision Processes






37














Effects of cognitive feedback on performance




W
K
Balzer






M
E
Doherty






R
J
Connor




10.1037/0033-2909.106.3.410








Psychological Bulletin




106


3
















Bayesian perspectives on multiple comparisons




D
A
Berry






Y
Hochberg








Journal of Statistical Planning and Inference




82


1-2


















10.1016/S0378-3758(99)00044-0














The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and metaanalysis




P
Bobko






P
L
Roth






M
A
Buster








Organizational Research Methods




10


4


















10.1177/1094428106294734














How employee selection decisions are made in practice




P
Bolander






J
Sandberg




10.1177/0170840612464757








Organization Studies




34


3
















A systematic review of algorithm aversion in augmented decision making




J
W
Burton






M
K
Stein






T
B
Jensen








Clinical-Statistical Controversy. Psychology, Public Policy, and Law




2


2










Journal of Behavioral Decision Making










10.1037/1076-8971.2.2.293














Clinical versus mechanical prediction: A meta-analysis




W
M
Grove






D
H
Zald






B
S
Lebow






B
E
Snitz






C
Nelson








Psychological Assessment




12


1


















10.1037/1040-3590.12.1.19














Broken legs, clinical overrides, and recidivism risk: An analysis of decisions to adjust risk levels with the LS/CMI




J
P
Guay






G
Parent




10.1177/0093854817719482








Criminal Justice and Behavior




45


1
















The accuracy of recidivism risk assessments for sexual offenders: A meta-analysis of 118 prediction studies




R
K
Hanson






K
E
Morton-Bourgon




10.1037/a0014421








Psychological Assessment




21


1
















Stubborn reliance on intuition and subjectivity in employee selection




S
Highhouse




10.1111/j.1754-9434.2008.00058.x








Industrial and Organizational Psychology: Perspectives on Science and Practice




1


3
















Is a .51 validity coefficient good? Value sensitivity for interview validity




S
Highhouse






M
E
Brooks






S
Nesnidol






S
Sim




10.1111/ijsa.12192








International Journal of Selection and Assessment




25


4
















Discretion in hiring




M
Hoffman






L
B
Kahn






D
Li




10.1093/qje/qjx042








The Quarterly Journal of Economics




133


2
















Distinction bias: Misprediction and mischoice due to joint evaluation




C
K
Hsee






J
Zhang








Journal of Personality and Social Psychology




86


5


















10.1037/0022-3514.86.5.680














General evaluability theory




C
K
Hsee






J
Zhang




10.1177/1745691610374586








Perspectives on Psychological Science




5


4
















The effects of predictive ability information, locus of control, and decision maker involvement on decision aid reliance




S
E
Kaplan






J
H
Reneau






S
Whitecotton








Journal of Behavioral Decision Making




14


1


















10.1002/1099-0771


1<35::AID-BDM364>3.0.CO;2-D








14












Determinants of linear judgment: A meta-analysis of lens model studies




N
Karelaia






R
M
Hogarth








Psychological Bulletin




134


3


















10.1037/0033-2909.134.3.404














Overconfidence in personnel selection: When and why unstructured interview information can hurt hiring decisions




E
E
Kausel






S
S
Culbertson






H
P
Madrid








Organizational Behavior and Human Decision Processes




137


















10.1016/j.obhdp.2016.07.005














Transforming admissions: The gateway to medicine




D
G
Kirch








JAMA: Journal of the American Medical Association




308


21


















10.1001/jama.2012.74126














Reasons for being selective when choosing personnel selection procedures




C
J
König






U
Klehe






M
Berchtold






M
Kleinmann




10.1111/j.1468-2389.2010.00485.x








International Journal of Selection and Assessment




18


1
















Bayesian assessment of null values via parameter estimation and model comparison




J
K
Kruschke








Perspectives on Psychological Science




6


3


















10.1177/1745691611406925














Doing Bayesian data analysis: A tutorial with




J
K
Kruschke






Jags
; R






Stan




10.1016/B978-0-12-405888-0.09999-2








Academic Press






2nd ed.








The time has come: Bayesian methods for data analysis in the organizational sciences




J
K
Kruschke






H
Aguinis






H
Joo




10.1177/1094428112457829








Organizational Research Methods




15


4
















The Bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective




J
K
Kruschke






T
M
Liddell




















10.3758/s13423-016-1221-4








Psychonomic Bulletin and Review




25


1














Judgment and decision making in staffing research and practice




N
R
Kuncel


















The sage handbook of industrial, work and organizational psychology


10.4135/9781473914940




S. Ones, N. Anderson, C. Viswesvaran, & H. K. Sinangil




SAGE Publications Ltd








2nd ed.








Mechanical versus clinical data combination in selection and admissions decisions: A meta-analysis




N
R
Kuncel






D
M
Klieger






B
S
Connelly






D
S
Ones




10.1037/a0034156








Journal of Applied Psychology




98


6
















Resistance to medical artificial intelligence




C
Longoni






A
Bonezzi






C
K
Morewedge








Journal of Consumer Research




46


4
















OPINION: Tests are not the problem; how they are used can be




W
Macdonald




















Empirical comparisons of clinical and actuarial prediction




P
E
Meehl




P. E
















Clinical versus statistical prediction: A theoretical analysis and a review of the evidence




Meehl






University of Minnesota Press
















10.1037/11281-008














The special powers of the clinician. In Clinical versus statistical prediction: A theoretical analysis and a review of the evidence




P
E
Meehl




10.1037/11281-004








University of Minnesota Press














Continued misinterpretation of confidence intervals: Response to Miller and Ulrich




R
D
Morey






R
Hoekstra






J
N
Rouder






E.-J
Wagenmakers




10.3758/s13423-015-0955-8








Psychonomic Bulletin & Review




23
















Subject matter expert judgments regarding the relative importance of competencies are not useful for choosing the test batteries that best predict performance




K
R
Murphy






P
J
Deckert






T
B
Kinney






M.-C
C
Kung




10.1111/ijsa.12051








International Journal of Selection and Assessment




21


4
















The predictive power of personal statements in admissions: A meta-analysis and cautionary tale




S
C
Murphy






D
M
Klieger






M
J
Borneman






N
R
Kuncel








84








College and University
















M
Neumann






M
Hengeveld






A
S M
Niessen






J
N
Tendeiro






R
R
Meijer


















Education increases decision-rule use: An investigation of education and incentives to improve decision making


10.1037/xap0000372








Journal of Experimental Psychology: Applied. Advance Online Publication












Implementing evidence-based assessment and selection in organizations: A review and an agenda for future research




M
Neumann






A
S M
Niessen






R
R
Meijer








Organizational Psychology Review






Advance Online Publication














10.1177/2041386620983419














When eliminating bias isn't fair: Algorithmic reductionism and procedural justice in human resource decisions




D
T
Newman






N
J
Fast






D
J
Harmon








Organizational Behavior and Human Decision Processes




160


















10.1016/j.obhdp.2020.03.008














Admission testing for higher education: A multi-cohort study on the validity of high-fidelity curriculum-sampling tests




A
S M
Niessen






R
R
Meijer






J
N
Tendeiro




10.1371/journal.pone.0198746








Article e0198746






13












Basic psychological need fulfillment and user resistance to objective and analytical decision-making practices in employee selection




K
P
Nolan












Doctoral dissertation








Threat of technological unemployment: Are hiring managers discounted for using standardized employee selection practices?




K
P
Nolan






N
T
Carter






D
K
Dalal








Personnel Assessment and Decisions




2


1


















10.25035/pad.2016.004














Threat of technological unemployment, use intentions, and the promotion of structured interviews in personnel selection




K
P
Nolan






D
K
Dalal






N
Carter








Personnel Assessment and Decisions




6


2


















10.25035/pad.2020.02.006














Need for autonomy and resistance to standardized employee selection practices




K
P
Nolan






S
Highhouse








Human Performance




27


4


















10.1080/08959285.2014.929691














The relative influence of advice from human experts and statistical methods on forecast adjustments




D
Önkal






P
Goodwin






M
Thomson






S
Gönül






A
Pollock








Journal of Behavioral Decision Making




22


4


















10.1002/bdm.637














The diversity-validity dilemma: Overview and legal context




K
M J
Pyburn






R
E
Ployhart






D
A
Kravitz








Personnel Psychology




1


















10.1111/j.1744-6570.2008.00108.x














Trends in testing: Highlights of a global survey




A
M
Ryan






I
Inceoglu






D
Bartram






J
Golubovich






J
Grand






M
Reeder






E
Derous






I
Nikolaou






X
Yao












I








Employee recruitment, selection, and assessment: Contemporary issues for theory and practice


Nikolaou & J. Oostrom
















A survey of individual assessment practices by I/O psychologists




A
M
Ryan






P
R
Sackett








Personnel Psychology




40


3


















10.1111/j.1744-6570.1987.tb00610.x














The research-practice gap in I/O psychology and related fields: Challenges and potential solutions




S
L
Rynes








The Oxford handbook of organizational psychology


S. W. J. Kozlowski




Oxford University Press




1














HR professionals' beliefs about effective human resource practices: Correspondence between research and practice




S
L
Rynes






A
E
Colbert






K
G
Brown




10.1002/hrm.10029








Human Resource Management




41


2
















Personnel selection




P
R
Sackett






F
Lievens




10.1146/annurev.psych.59.103006.093716








Annual Review of Psychology




59
















A contribution to the study of actuarial and individual methods of prediction




T
R
Sarbin








American Journal of Sociology




48


5


















10.1086/219248














Measurement and prediction, clinical and statistical




J
Sawyer




10.1037/h0023624








Psychological Bulletin




66


3
















The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases




T
Schäfer






M
A
Schwarz




10.3389/fpsyg.2019.00813








Frontiers in Psychology




10














The recalcitrance of overconfidence and its contribution to decision aid neglect




W
R
Sieck






H
R
Arkes




10.1002/bdm.486








Journal of Behavioral Decision Making




18


1
















RWA web: A free, comprehensive, web-based, and user-friendly tool for relative weight analyses




S
Tonidandel






J
M
Lebreton




10.1007/s10869-014-9351-z








Journal of Business and Psychology




30
















Judgment under uncertainty: Heuristics and biases




A
Tversky






D
Kahneman




10.1126/science.185.4157.1124








Science




185


4157
















Survey on the use of clinical and mechanical prediction methods in clinical psychology




S
I
Vrieze






W
M
Grove




10.1037/a0014693








Professional Psychology: Research and Practice




40


5
















A practical solution to the pervasive problems of p values




E.-J
Wagenmakers




10.3758/BF03194105








Psychonomic Bulletin & Review




14


5
















Moving to a world beyond




R
L
Wasserstein






A
L
Schirm






N
A
Lazar




p < 0.05






American Statistician




73


sup1


















10.1080/00031305.2019.1583913














Pushing the limits for judgmental consistency: Comparing random weighting schemes with expert judgments




M
C
Yu






N
R
Kuncel








Personnel Assessment and Decisions




6


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]