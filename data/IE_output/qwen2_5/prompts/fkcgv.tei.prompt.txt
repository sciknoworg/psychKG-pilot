You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Validate Mouse-Tracking: How Design Factors Influence Action Dynamics in Intertemporal


Decision Making
Decision science has experienced a paradigmatic shift evolving its focus, methods, and approaches from an outcome based perspective towards a more process orientated paradigm 
(Oppenheimer & Kelso, 2015)
. This process paradigm acknowledges the temporal nature of basic mental processes, and hence, builds theories of choice incorporating perceptual, attentional, memory, and decisional processes. In order to test these process explanations, process-tracing methods are required. In the last 60 years, decision scientists introduced a variety of processtracing methods to the field, e.g., verbal protocols (e.g., 
Ericson & Simon, 1984)
, eye tracking (e.g., 
Russo & Rosen, 1975)
 and, most recently, mouse-tracking (e.g., 
Dale, Kehoe, & Spivey, 2007;
Spivey, Grosjean, & Knoblich, 2005
) (for an overview, please see 
Schulte-Mecklenbeck, Johnson, et al., 2017)
.
Whenever scientists apply such process-tracing methods, they rely on specific prerequisites and core concepts in order to conduct the reverse inference 
(Poldrack, 2006)
:
Reverse inference describes the reasoning by which the presence of a particular cognitive process is inferred from a pattern of neuroimaging or behavioral data (cf. 
Heit, 2015)
. One prerequisite of reverse inference stipulates a close mapping between the covert mental process and the overt observable behavior. Given this mapping, reverse inference is used to test process explanations and produce meaningful interpretation of the results. However, this direct interpretation is hampered by the fact that the mapping between mental processes and behavior might be mediated by the (process-tracing) method itself. Validation of the assumed mapping is hence essential. In this study, we concentrate on the validation of mouse-tracking as a processtracing method by evaluating the interaction between the mapping and different implementations of the method. Eventually, we derive recommendations for the implementation of the method which we hope will reinforce extensive discussions about standards (i.e., best practices) in mouse-tracking research.


Mouse-tracking as a process-tracing method
Mouse-tracking is a relatively new and convenient process-tracing method 
(Freeman, 2018;
Koop & Johnson, 2011)
. It offers five advantages: the temporal resolution is high, the risk of distortion tolerable, the technical equipment is cheap and runs in almost every lab facility, most participants are highly familiar with moving a computer mouse, and mouse-tracking algorithms can be implemented in most experimental software with only a few technical skills 
(Schulte-Mecklenbeck, Johnson, et al., 2017)
. Additionally, ready-to-use experimental builder and analysis software has been developed that allows researchers to create mouse-tracking experiments without programming and to process, analyze and visualize the resulting data with little technical effort 
(Freeman & Ambady, 2010;
Kieslich & Henninger, 2017;
Kieslich, Henninger, Wulff, Haslbeck, & Schulte-Mecklenbeck, in press
).
Hence, in the last ten years, mouse-tracking flourished in many fields of psychological research (for a review, see 
Erb, 2018;
Freeman, 2018;
Freeman, Dale, & Farmer, 2011)
, finding applications in studies of phonological and semantic processing 
(Dale et al., 2007;
Dshemuchadse, Grage, & Scherbaum, 2015;
Spivey et al., 2005)
, cognitive control 
(Dignath, Pfister, Eder, Kiesel, & Kunde, 2014;
Scherbaum, Dshemuchadse, Fischer, & Goschke, 2010;
Yamamoto, Incera, & McLennan, 2016)
, selective attention 
(Frisch, Dshemuchadse, Görner, Goschke, & Scherbaum, 2015)
, numerical cognition 
(Szaszi, Palfi, Szollosi, Kieslich, & Aczel, 2018)
, perceptual choices 
(Quinton, Volpi, Barca, & Pezzulo, 2014)
, moral decisions 
(Koop, 2013)
, preferential choices 
(Koop & Johnson, 2013;
O'Hora, Dale, Piiroinen, & Connolly, 2013)
, lexical decisions 
(Barca & Pezzulo, 2012
, 2015
, and value-based decisions 
(Calluso, Committeri, Pezzulo, Lepora, & Tosoni, 2015;
Dshemuchadse et al., 2013;
Kieslich & Hilbig, 2014;
Koop & Johnson, 2011;
O'Hora, Carey, Kervick, Crowley, & Dabrowski, 2016;
Scherbaum et al., 2016;
Scherbaum, Dshemuchadse, Leiberg, & Goschke, 2013;
Scherbaum, Frisch, & Dshemuchadse, 2018b
, 2018a
van Rooij, Favela, Malone, & Richardson, 2013)
.
By measuring mouse movements, all those studies advanced their field into a process oriented paradigm by revealing participants' cognitive processing over the time course (see 
Figure 1
). The core concept underlying mouse-tracking is that cognitive processing is continuously leaking into motor (e.g., hand or computer mouse) movements 
(Spivey, 2007;
Spivey & Dale, 2006)
. Ideally, there would be a one-to-one correspondence 
(i.e., mapping)
 between overt behavior and covert cognitive processes. However, this mapping might be crucially dependent on the applied mouse-tracking paradigm in such a way that it must generally support the continuous leaking of cognitive processing into the mouse movements. This continuous leakage in a mouse-tracking paradigm is not trivial: 
Scherbaum and Kieslich (2018)
 recently showed that different procedures of starting a trial in a mouse-tracking paradigm resulted in different mouse movements (in the following this premise is referred to as P1). This finding questions whether the mapping between overt behavior and covert cognitive processes holds equally for different implementation of a starting procedure. The results of the Scherbaum and Kieslich study give rise to the supposition that a mouse-tracking paradigm must be carefully designed and that specific variations can strongly influence the mapping, and hence challenge the validity of the reverse inference. Consequently, this questions the validity, generalizability, and the replicability of mouse-tracking results.


Varieties in the implementation of mouse-tracking
As mouse-tracking is relatively new, there exists nothing like a gold standard for mousetracking paradigms which is reflected in the diversity in methodological setups of mousetracking studies all over psychological research. Besides the starting procedure, mouse-tracking paradigms differ-for instance-with respect to the procedure of final response indication and the position of the stimuli-both of which can influence the mapping between cognitive processing and mouse movements in substantial way, as described in the following.
Concerning the response procedure, in most mouse-tracking paradigms, participants indicate their responses by clicking with the mouse cursor into specific areas (i.e., response boxes) on the screen (e.g., 
O'Hora et al., 2016)
. In contrast to this click-procedure, in other paradigms, participants indicate their responses by just moving the mouse cursor into those areas (e.g., 
Scherbaum et al., 2010)
, which we will call a hover-procedure. Comparing the two procedures, it is rather obvious that the click-procedure, allows for second thoughts, and hence possibly (discrete) changes of mind 
(Barca & Pezzulo, 2015;
Resulaj, Kiani, Wolpert, & Shadlen, 2009)
, since the response process is not terminated when reaching the response box, but only after clicking into it (P2).
Concerning the position of stimuli, there are paradigms in which stimuli are presented very close to the center of the screen (e.g., 
Calluso et al., 2015)
, whereas in other paradigms the stimuli are presented close to or within the response boxes which are usually positioned at the upper left and right edge of the screen (e.g., 
Faulkenberry, Cruise, Lavro, & Shaki, 2016)
.
Comparing the two implementations of the stimuli position, it is rather obvious that a paradigm in which stimuli are positioned at the upper left and right edge of the screen demands for many eye movements. Those eye movements might interfere with cognitive processing (see 
Orquin & Mueller Loose, 2013)
 and, in turn, mouse movements (P3). In sum, it appears that at least some of the aforementioned design factors might jeopardize the basis of mouse-tracking and thus its validity as a process-tracing method.
In this study, we systematically investigate how different design factors influence mousetracking data and results. In this regard, we build on the previous work by 
Scherbaum and Kieslich (2018)
 but extend the focus, first by extending the application to value-based decision making, and second by studying two additional design factors. We will contrast different implementations of the starting procedure, the response procedure, and the stimuli position. For each of these design factors we compare two of the most common approaches. As for the starting procedure, we compare (1) a "dynamic starting procedure" in which participants have to initiate a mouse movement first to trigger stimulus presentation, and (2) a "static starting procedure" in which a stimulus is presented after a fixed time interval and participants can freely decide when to initiate their mouse movement. As for the response procedure, we compare (1) a "hover response procedure" in which participants merely have to enter the response box with the mouse cursor to indicate a response, and (2) a "click response procedure" in which participants additionally have to click into the response box. As for the stimuli position, we compare (1) a "centered stimuli position" in which stimuli are presented close to the horizontal and vertical center of the screen, and a "edged stimuli position" in which stimuli are presented within or close to the response boxes that are located at the upper edges of the screen.
To study the influences of the different design factors, we used a standard intertemporal choice task, a value-based decision-making task that is well established in decision science (e.g., 
Cheng & González-Vallejo, 2017;
Dai & Busemeyer, 2014;
Franco-Watkins, Mattson, & Jackson, 2015)
. In this task, participants have to choose between two monetary rewards that are available at different time points, constituting a soon-small and a late-large option. The classic effect that can be observed in this task is temporal discounting (for an overview, see 
Frederick, Loewenstein, & O'Donoghue, 2002)
, the devaluation of the monetary reward with increasing temporal delay. A well-established modulation of temporal discounting is represented by the date-delay effect 
(DeHart & Odum, 2015;
Read, Frederick, Orsel, & Rahman, 2005
) which refers to decreased temporal discounting when temporal delays are presented as calendar dates rather than intervals of days. In an earlier study, these effects and their dynamics were investigated in a specific mouse-tracking setup, using a dynamic starting procedure, a hover response procedure, and a centered stimuli position 
(Dshemuchadse et al., 2013)
. This study revealed that a lack of reflection could account for temporal discounting, as indicated by more direct mouse movements when choosing the soon-small option in contrast to more indirect mouse movements when choosing the late-large option. Furthermore, an analysis of movement dynamics extracted the relative weight of the different sources of information in this paradigm and suggested that a change in the weighting of the monetary reward could be a source of the date-delay effect.
In the current study, we will use the original study as a starting point to investigate in how far the different design factors influence the consistency of mouse movements within and across participants. We will hence study, how these design factors affect different mousetracking measures, and eventually the original effects. So far, most mouse-tracking studies rely on discrete measures on the trial level, for instance, calculating initiation times, movement times, movement deviation, or the number of changes in movement direction for statistical analysis 
(Freeman & Ambady, 2010)
. Discrete measures integrate information over the course of the whole trial and thus should be more robust against changes in the specific study design  1 . In contrast, dynamic measures focus on the within-trial continuous mouse movement over time 
(Dshemuchadse et al., 2013;
Scherbaum et al., 2010
Scherbaum et al., , 2013
Scherbaum et al., , 2018a
Sullivan, Hutcherson, Harris, & Rangel, 2015)
. Hence, they should be more prone to changes in the setup or procedure of the mouse-tracking paradigm ).
In sum, in a mouse-tracking version of a standard intertemporal choice task we will investigate to what extent specific design factors influence the consistency of mouse movement data. Specifically, we will investigate three factors: First, we will investigate to what degree a static starting procedure decrease data quality compared to a dynamic starting procedure. We will investigate this factor by comparing data from the original study 
(Dshemuchadse et al., 2013)
, which applied a dynamic start procedure, with data from a new sample of participants who performed the identical task with a static starting procedure. Second, we investigate how the response procedure (hover vs. click) influences data quality by varying this procedure within our new study. Third, we will investigate how the stimuli position (centered vs. edged) influences data quality by varying this factor within our new study.


Hypotheses
For the comparison between studies (original vs. new), we expected (H1) that cognitive 2 effects on discrete movement measures should only be slightly influenced by differences in the starting procedure (see P1), whereas (H2) cognitive effects on within-trial continuous movement measures should be larger and more reliable when using the dynamic starting procedure (see P1).
Furthermore, we expected (H3) that the consistency of mouse movements within trials, across trials, and across participants is higher using the dynamic rather than the static starting procedure (see P1).
For the comparison of design factors within the new study, we expected (H4) that the cognitive effects based on discrete mouse movement measures should only be slightly influenced by the variation of the response procedure and the stimuli position (see P1), whereas (H5) cognitive effects on within-trial continuous movement measures should be larger and more reliable when using the hover response procedure and the centered stimuli position, respectively (see P1, P2, and P3). Moreover, we expected (H6) that the consistency of mouse movements within trials, across trials, and across participants would be higher when using the hover response procedure (compared to the click response procedure) as well as when using the centered stimuli position (compared to the edged stimuli position, see P1, P2, and P3).


Method


Ethics statement
The study was performed in accordance with the guidelines of the Declaration of Helsinki and of the German Psychological Society. An ethical approval was not required since the study did not involve any risk or discomfort for the participants. All participants were informed about the purpose and the procedure of the study and gave written informed consent prior to the experiment. All data were analyzed anonymously.


Participants
Forty participants (60% female, mean age = 23.28, SD = 4.73) completed the new experiment conducted at the Technische Universität Dresden, Germany. The experiment lasted 75 minutes. Participants were uniformly recruited through the department's data-base system which is based on ORSEE 
(Greiner, 2004)
. Three participants were left-handed; another four participants did not indicate their handedness. Similar to the original study, five participants were excluded because they did not exhibit any discounting (k parameter of the fitted hyperbolic function < 0.01) in the different conditions (delay or date) and thus did not execute a sufficient number of soon-small choices. An additional analysis including all participants did not qualitatively change the pattern of results.
In the original experiment, forty-two right-handed students (62% female, mean age = 22.95 years, SD = 2.72) had participated of whom six participants had been excluded from data analysis, see above.
All participants had normal or corrected-to-normal vision. They received either class credit or 6€ (5€, original study) payment.


Apparatus and Stimuli
The apparatus in the new experiment was identical to the apparatus in the original experiment using the same lab facility. Stimuli were presented in white on a black background on a 17-inch screen running at a resolution of 1,280 × 1,024 pixels (75-Hz refresh rate). We used Psychophysics Toolbox 3 
(Brainard, 1997;
Pelli, 1997)
   starting procedure (i.e., original experiment; top panel), participants had to move the cursor upwards, in order to start stimulus presentation-only after reaching a movement threshold, the stimulus was presented. In the static starting procedure (i.e., new experiment, bottom panel), the stimulus was presented 200 ms after clicking into the red box. Stimuli were either presented close to the vertical and horizontal center of the screen (i.e., centered stimuli position) or within the response boxes at the top left and right edges of the screen (i.e., edged stimuli position). To respond, participants had to move the mouse cursor to the left or the right response box, either just moving into the box (i.e., hover response procedure; top panel) or clicking into the box (i.e., click response procedure).


Procedure
As in the original experiment, participants were asked to decide on each trial which of two options they preferred: the left (soon-small, SS) or the right (late-large, LL) option.
Participants were instructed to respond to the hypothetical choices as if they were real choices.
Similar to the original study, trials were grouped into miniblocks of 14 trials (see 
Figure 2
).
Within each miniblock, the two monetary values remained constant and only the temporal delay of the two options was varied. At the start of each miniblock, the monetary values were presented for 5 s, allowing participants to encode them in advance (see 
Figure 2
).
Each trial consisted of three stages: The alignment stage, the start stage, and the response stage. In the alignment stage, participants had to click on a red box at the bottom of the screen within a deadline of 1.5 s. This served to align the starting area for each trial. After clicking this box, the start stage began and two response boxes in the right and left upper corner of the screen were presented. The procedure of the start stage differed between the new experiment in which we implemented a static starting procedure, and the original experiment in which we had implemented a dynamic starting procedure. In the static starting procedure, the start stage simply lasted 200 ms (this was as long as the start stage in the Scherbaum and Kieslich paradigm and in this case slightly longer than the average duration of the start stage in the original experiment that used the dynamic starting procedure: 184 ms) and participants simply had to wait for the start of the response stage. In contrast, in the dynamic starting procedure, participants were required to start the mouse movement upwards within a deadline of 1.5 s. Specifically, the response stage only started after participants moved the mouse upwards for at least four pixels in each of two consecutive time steps. Usually, this procedure is applied to force participants to be already moving when entering the decision process to assure that they do not decide first and then only execute the final movement 
(Scherbaum et al., 2010)
. In the response stage, the remaining stimuli (i.e., the temporal delays) were presented beneath the monetary values, either as a delay in days (e.g., "3 Tage") or as a German calendar date (e.g., "6. Mai"). The procedure of the response stage also differed between the new experiment in which we implemented both a click and a hover response procedure and the original experiment in which we had implemented a hover response procedure only. Participants in the hover response procedure solely had to move the cursor into one of the responses boxes, whereas participants in the clicking response condition additionally had to click in the respective response box. Usually, the hover response procedure is applied to force participants to finish the evaluation of their preference before reaching response boxes 
(Scherbaum et al., 2010)
. In both conditions, participants were instructed to respond as quickly and informed as possible and to move the mouse continuously upwards once they had initialized their movement.
The trial ended after the indication of a response within a deadline of 2 s. If subjects missed the respective deadline in one of the three stages, the next trial started with the presentation of the red start box. Response times were measured as the duration of the response stage, reflecting the interval between the onset of the stimuli and the indication of the response.
Inter trial intervals were measured as the duration of the alignment stage, reflecting the interval between a response in a previous trial and the click on the red box at the bottom of the screen in the current trial.
After onscreen instructions and demonstration by the experimenter, participants practiced 28 trials (14 trials with feedback and no deadline for any stage of a trial, and 14 trials with feedback and deadline). Furthermore, preceding each block in which changes of the experimental setup occurred (e.g., stimuli position, date/delay condition), participants practiced ten additional trials in order to get used to the alteration.


Design
In the new experiment, we used the same set of trials as in the original experiment. We orthogonally varied the temporal interval between the options (1, 2, 3, 5, 7, 10, and 14 days), the monetary value of the SS option as a percentage of the monetary value of the LL option (20%, 50%, 70%, 80%, 88%, 93%, 97%, and 99%), and the framing of the time (delay vs. date). The framing of the time was varied between blocks; order balanced between participants; the percentage of the monetary value of the option was varied between miniblocks; the temporal interval between the options was varied across trials. Additionally, we orthogonally varied the temporal delay of the SS option (0 and 7 days) and the monetary value of the LL option (19.68€ and 20.32€). Both variations were introduced to control for specific effects (e.g., delay duration effect and magnitude effect; 
Dai & Busemeyer, 2014;
Green, Myerson, & McFadden, 1997)
 and
to collect more data without repeating identical trials, which could have led to memory effects.
As in the original experiment and due to the focus of the new experiment these factors were ignored in following analyses.
In sum, both the new and the original experiment consisted of two blocks (date and delay), order balanced across participants, and contained 224 trials (16 miniblocks) per block in randomized order. In the new experiment, we additionally varied the response procedure between participants (randomly assigned) and the stimuli position within participants. Hence, for the stimuli position, the two blocks (date and delay) were divided into two sub-blocks, respectively, order balanced across participants.


Data Preprocessing and statistical analyses
We excluded erroneous trails (original: 3.6 %; new: 2.3 %), that is, trails in which participants missed any of the introduced deadlines (see Procedure). Additionally, we also excluded trials with response times less than 300 ms (original study: 5.4 %; new study: 3.6 %).
Mouse trajectories were (1) truncated after the first data sample being in the response box of the ultimately chosen option (to enhance comparison of trajectories between hover and click response procedures), (2) mirrored, so that all trajectories would end in the left response box, 
3
horizontally aligned for a common starting position (horizontal middle position of the screen corresponds to 0 pixels, and values increase towards the right), and (4) time-normalized into 100 equal time steps (following the original study).
In order to investigate the unique influence of each design factor's variation, we computed two analyses for each dependent variable: first, we compared data between experiments (original vs. new); second, we compared data within the new experiment between the different design factor conditions. Through the combination of both analyses, we evaluated the contribution of each design factor to the pattern of results. This logic of analyses also justified the sample size for the new experiment. Additional a priori power analyses (power = .95) for the replication of the cognitive effects on discrete mouse measures (see Results, g = 0.7)
revealed that a sample size of N = 24 would have been sufficient 
(Faul, Erdfelder, Buchner, & Lang, 2009)
. Thus, with the sample size in the new experiment (N = 35), even small to medium effects-such as the date-delay effect-were appropriately powered (power = .8).
The data of the original experiment had been used in a previous publication 
(Dshemuchadse et al., 2013, JEP:G)
. The data were reanalyzed completely for the current article.
Due to improvements of the algorithms for the analysis of mouse movement data in recent years, results of the original experiment might exhibit minor variations in comparison to the results reported in the original publication.
Data preprocessing and aggregation was performed in Matlab 2015a (the Mathworks Inc.). Statistical analyses were performed in Matlab and JASP 0.8.6. (JASP Team, 2018).
Primary data of both experiments and analyses scripts are publicly available at the Open Science Framework (osf.io/3w6cr).


Results


Comparison of groups
Since our analysis builds on two independent groups of participants from different experiments (original vs. new) we first checked for differences between groups other than the starting procedure, response procedure, and stimuli position. Groups of participants showed neither significant differences in age, t(69) = -0.46, p = .65, g = -0.11, 95% bootstrapped CI of g All other descriptive variables also showed no significant differences (all p >0.120, see Appendix A), with the exception that participants in the new experiment reported to feel more rested and less tired, 95% CIs [-0.98, -0.02], [-0.01, 0.93], respectively. Following Scherbaum and Kieslich (2018), we analyzed the inter trial interval as a general measure for task speed, which is defined by the time interval between reaching the response box of the current trial and clicking on the start box of the next trial. We found a significant difference between groups, t(69) = 2.33, p < .05, g = 0.55, 95% bCIg [0.12, 1.00], indicating that participants in the original experiment were generally slower (M = 0.92 s, SD = 0.26 s) than participants in the new experiment (M = 0.85 s, SD = 0.18 s). Contrasting the difference on the inter trial interval as a general measure of task speed, we found the reverse pattern for response times, t(69) = -3.23, p < .05, g = -0.76, 95% bCIg [-1.32, -0.28], indicating that participants in the original experiment responded generally faster (M = 0.88, SD = 0.27) than participants in the new experiment (M = 1.00, SD = 0.31).
In combination, these results corroborated our general subjective intuition that mousetracking paradigms are procedurally very demanding for the participants, and hence they have to take some time to relax during the task-either during the inter trial interval or after starting the trial. This motivated a further analysis of the sum of inter trial interval and the response time as a general measure for trial speed. We found no significant difference between groups, t(69) = -1.16, p = .25, g = -0.27, 95% bCIg 
[-0.80, 0.17]
. Taken together, the results suggested that participants in both experiments completed trials similarly fast, but participants in the original experiment took more time between trials, whereas participants in the new experiment took more time within trials.


Cognitive Effects
In the next step, we wanted to find out whether our variations of the design factors lead to differences in the observed cognitive effects. In this regard, as a manipulation check, we expected that choice effects (i.e., date-delay effect) remain unaffected by variations of the design factors. Furthermore, we expected that differences in the starting procedure should only slightly influence discrete cognitive measures (H1) but should have greater impact on continuous measures, resulting in larger and more reliable effects for the dynamic compared to the static starting procedure (H2). Likewise, the variation of the response procedure or stimuli position within the new experiment should only slightly alter discrete measures (H4), but should show larger and more reliable effects for the hover response procedure and centered stimuli position (H5).


Choice effects
We examined choice effects in the new experiment, expecting to find larger temporal discounting in the delay compared to the date condition, replicating the results of the original experiment.
To follow the approach of the original experiment with respect to the exclusion of participants (i.e., applying the exact same exclusion criterion, see Participants), we calculated indifference points for the two conditions (delay vs. date), which is the estimated value ratio for each interval at which a participant would be indifferent about choosing the SS or the LL option 3 , and consecutively fitted a hyperbolic function to these indifference points over time intervals 
4
 . From this fitted function, we retrieved the k parameter as an overall measure of discounting 
(Dshemuchadse et al., 2013)
.
However, to investigate the date-delay effect we followed a different approach since k parameters come with several caveats with respect to distribution and sensitivity that might distort comparisons within and between experiments 
(Myerson, Green, & Warusawitharana, 2001
; please see online supplement). First, we calculated the proportion of LL choices for each interval and derived the areas under the curve (AUC) as summary measure of temporal discounting. For reasons of simplicity, we only calculate and report the results of the difference 
3
 The calculation of the indifference points was obtained by fitting a logistic regression model to participants' choices for each interval. The fitting of the logistic regression model was performed using the StixBox mathematical toolbox by Anders Holtsberg (http://www.maths.lth.se/matstat/stixbox/). The fit was based on the model log � 1− � = , where p is the probability that the choice is 1 (SS) and not 0 (LL), X represents value ratio (SS/LL), and b represents the point estimates for the logistic function. Please see our custom Matlab function findIndifferencePoints.m available at osf.io/3w6cr for further details and the specific implementation. 
4
 The fitting of the hyperbolic function was performed by applying Matlab's multidimensional unconstrained nonlinear minimization function to the hyperbolic function 1/(1+kX) = Y, with X denoting time interval, Y denoting the indifference point, and k denoting the discounting parameter. Please see our custom Matlab function fitCurve.m available at osf.io/3w6cr for further details and the specific implementation. of the AUC between the two conditions (delay vs. date) as simple main effects for each experiment (original vs. new).
In the original dataset, we revealed a significantly smaller AUC in the delay condition (M = 0.36, SD = 0.16) compared to the date condition (M = 0.43, SD = 0.16), t(35) = -3.68, p < .001, g = -0.41, 95% bCIg 
[-0.67, -0.20]
. This was also the case for the new dataset in which we found a significantly smaller AUC in the delay condition (M = 0.40, SD = 0.13) compared to the date condition (M = 0.47, SD = 0.14), t(34) = -4.78, p < .001, g = -50, 95% bCIg 
[-0.74, -0.20]
. Thus, participants in the new experiment showed again more temporal discounting in the delay than in the date condition replicating the effect from the original experiment (see 
Figure 3
). 


Discrete mouse movement effects
We inspected discrete movement measures by calculating the directness of mouse trajectories towards the response boxes. We expected less direct mouse movements for LL choices, indicating greater reflection on these decisions. Directness was defined as the average deviation (AD) of trajectories from a notional line between starting and ending point of the movement. We computed AD for each trial, z-scored and averaged it per participant according to experiments and/or conditions. For reasons of simplicity, we only calculate and report the results of the difference of AD between SS and LL choices as simple main effects for each experiment (original vs. new) as well as within the new experiment separately for each condition (see 
Figure   4
), but please see online supplement for a visual inspection of further, more fine-grained discrete mouse movement effects that had also been reported in the original study. In the original dataset, we reproduced significantly less direct mouse movements for LL compared to SS choices, t(35) = 2.39, p = .022, g = 0.75, 95% bCIg [0.15, 1.41]. In the new dataset, the same effect neither occurred over all conditions, t(36) = 0.81, p = .42, g = 0.26, 95%
bCIg 
[-0.38, 0
.90], nor within conditions, all ts < 0.98, ps > 0.33, 0.03 < g < 0.40.
In sum, our analysis of AD of mouse trajectories did not show the expected robustness of discrete movement effects (controverting H1 and H4). As expected, we reproduced the effect in the reanalysis of the original experiment. In contrast, in the new experiment the effect vanished.
A closer examination of the simple main effects (see 
Figure 4)
 as well as the comparison of the raw deviations between the experiments (original vs. new; see 
Figure 5
) indicated that the static starting procedure accounts for our finding: 
Figure 5
 illustrates a severe increase of more direct mouse movements when a static starting procedure was used. However, we will elaborate on this mechanism more specifically when turning to the continuous measures of mouse movements in section 0. 


Continuous mouse movement effects
For the statistical analysis of mouse movement dynamics, we performed time continuous multiple linear regression (TCMR; Scherbaum & Dshemuchadse, submitted) on movement angle on the x/y plane. In order to dissect the influences of the independent variables on mouse movements within a trial, we applied a three step procedure: (1) We defined the variables value ratio (eight discrete steps, ranging from 0.2 to 0.99) and time interval (seven discrete steps, ranging from 1 to 14) as predictors for each trial and each participant (see Design). The value ratio is given by the monetary value of the SS option as a percentage of the monetary value of the LL option. Time interval is given by the difference of the temporal delays between options. These predictors were normalized to a range of -1 to 1 in order to extract comparable beta weights. (2) We performed multiple regression with these predictors on trajectory angle (also normalized to a range of -1 to 1) for each time step. This resulted in 100 multiple regressions (from 100 time steps), returning two beta weights for each of the 100 time steps. (3) These beta weights were separately averaged for each time step and displayed in a time-varying curve of influence strength. We compared curves of influence strength between the date and delay condition by contrasting beta weights for each time step and predictor. Furthermore, we computed one-sided Student's t-tests for each beta weight contrast for each time step. Significant temporal segments were identified with at least ten consecutive significant time steps in order to compensate for multiple comparisons 
(Dshemuchadse et al., 2013;
Scherbaum et al., 2010)
.
Curves and contrasts for both experiments are displayed in 
Figure 6
 (original) and 
Figure 7
 (new).
With regard to the curves of the beta weights, we found that the trajectory angle is significantly predicted by value ratio and time interval in both experiments (see 
Figure 6A
/B and 
Figure 7A
/B). However, those curves also illustrate a higher predictive power of value ratio and time interval in the original experiment as compared to the new experiment, suggesting that mouse movements contain more stimuli information when the dynamic starting procedure is used.
With regard to the beta weight contrast date-delay, we found a significant temporal segment in the original dataset, which is comparable to the one that has already been reported in the original paper 
(Dshemuchadse et al., 2013
)(see 
Figure 6C
). Beta weights for the predictor value ratio were significantly higher in the date condition in time steps 49 to 82 (mean times from 429 ms to 718 ms) than in delay condition. In the new dataset, we found no such temporal segments for the beta weight contrast matching our criterion of significance ( 
Figure 7C
). Furthermore, no temporal segments were found for contrasts in the different conditions within the new experiment (please see online supplement).
In sum, the variation of the starting procedure leads to a substantial change in date-delay differences when performing a continuous analysis of mouse movements (supporting H2): While the value ratios in the date condition in the original experiment had a significantly greater impact on predicting mouse movements on the x/y plane, this difference was not significant in the new experiment. However, the variation of the response procedure and the stimuli position made no additional change to the date-delay difference (controverting H5).  


Mouse movement consistency
In addition to cognitive effects on the discrete and continuous movement measures, we analyzed the influence of design factors on the consistency of mouse movements within trials, across trials and across participants. We concentrated on the quantitative evaluation of the mouse movement consistency, but will augment the results with some qualitative interpretations which were derived from visual inspection of pooled or mean mouse movements (see also Appendix B).
Consistency within trials was evaluated by calculating the continuous movement index and inspecting mean profiles of movements on the y-axis and velocity. Consistency across trials was evaluated by computing the bimodality index and inspecting visual heatmaps of pooled movements on the x/y plane. Consistency across participants was evaluated by comparing the distributions of movement initiation times and positions after the stimuli were presented. We expected higher consistency of mouse movements for the dynamic than for the static starting procedure (H3). Within the new experiment, we anticipated the same pattern for the hover response procedure and the centered stimuli position (H6).


Continuous movement index
In order to examine the consistency within trials, we calculated the continuous movement index. The continuous movement index was computed for each trial as the correlation of the actual position on the y-axis and a hypothetical position on the y-axis for a constant and straight mouse movement from the starting point to the response box. A strong correlation indicates a smooth and constant movement . The continuous movement index was averaged for each participant and first compared between experiments (original vs. new), revealing a significant difference, t(69) = 5.18, p < .001, g = 1.22, 95% bCIg [0.83, 1.69]: Mouse movements were smoother and more constant in the original (M = .95, SD = 0.047) than in the new experiment (M = .86, SD = 0.097, see 
Figure 8A
). Furthermore, we compared the continuous movement index within the new experiment. An ANOVA with the independent variables stimuli position and response procedure showed significant main effects for stimuli position, F(1, 33) = 14, p < .001, η 2 = .27, and response procedure, F(1, 33) = 8.9, p = .005, η 2 = .21. Mouse movements were smoother and more constant for the edged stimuli position than for the centered as well as for the click than for the hover response procedure (see 
Figure 8B
). The interaction Stimuli position × Response procedure was significant as well, F(1, 33) = 5.11, p = .031, η 2 = .10, reflecting a stronger effect of the stimuli position for the hover response procedure (see 
Figure 8B
). In sum, our analysis of movement consistency within trials via the continuous movement index provided mixed results with regard to our hypotheses. Mouse movements in the static starting procedure were less smooth and less constant than in the dynamic starting procedure (supporting H3). Furthermore, the analyses revealed that a click response procedure and an edged stimuli position can partly compensate for the former effect (controverting H6). Our quantitative results were also supported by visual inspection of mean profiles of movements on the y-axis (see Appendix B, 
Figure B4
) and velocity (see 
Figure 10C
-D).


Bimodality index
In order to examine the consistency across trials, we assessed bimodality for distributions of AD per participant. Therefore, we calculated the bimodality index that uses kurtosis and skewness to estimate bimodality of a distribution 
(Freeman & Dale, 2013)
. The bimodality index ranges from 0 to 1, an index above .555 suggesting bimodality, and hence, in our terms, lower consistency across trials. A comparison between experiments (original vs. new) did not show significant differences, t(69) = -1.46, p = .15, g = -0.34, 95% bCIg 
[-0.82, 0.12]
, indicating a comparable bimodality index between the original (M = .38, SD = 0.13) and the new experiment (M = .43, SD = 0.14, see 
Figure 9A
). Within the new experiment, an ANOVA with the independent variables stimuli position and response procedure revealed significant main effects for response procedure, F(1, 33) = 4.25, p = .047, η 2 = .11, and stimuli position, F(1, 33) = 5.02, p = .032, η 2 = .13. The bimodality index was higher for the click than for the hover response procedure, as well as for the edged than for the centered stimuli position (see 
Figure 9B
). The interaction was not significant, F(1, 33) = 0.12, p = .73, η 2 < .01. In sum, our analysis of movement consistency across trials via the bimodality index provided mixed results with respect to our hypotheses. In contrast to our expectations, distributions of AD were estimated to be similar and distributed rather unimodally and hence equally consistent in both starting procedures (controverting H3). Nevertheless, within the new experiment consistency decreases for the click response procedure and edged stimuli position (supporting H6), though in no condition the bimodality index exceeds the threshold of .555 (.37 < M < .50, 0.12 < SD < .15). However, our quantitative results were controverted by visual inspection of pooled profiles of movements on the x/y plane (see Appendix B, 
Figure B1
 and 
Figure B2
) and the x-axis (see 
Figure 10A
-B). The visual inspection suggests that a static starting procedure introduces two sets of sharp and qualitatively different movement profiles: A high rate of mouse movements went straight (i.e., without curvature) towards the chosen option whereas some mouse movements went straight towards the unchosen option followed by another straight movement towards the chosen option (i.e., change of mind; see e.g., 
Resulaj, Kiani, Wolpert, & Shadlen, 2009)
. In contrast, movement profiles of the dynamic starting procedure yielded the full continuum of mouse movements ranging from straight profiles to changes of mind with a high frequency of smoothly curved profiles in between. Furthermore, within the new experiment, the inspection suggests, that a hover response procedure yielded mainly straight movements towards the chosen option, whereas a click response procedure enhanced discrete changes of mind. 


Movement initiation strategies
In order to examine the consistency across participants, we examined both movement initiation times and positions. Analogue to the original experiment, the movement initiation time was given by the time between stimuli presentation and moving at least four pixels in two consecutive time steps. The movement initiation position was given by the mouse position on the y-axis where participants initiated their movement after stimulus presentation. Since in the original experiment with the dynamic starting procedure the movement initiation strategies were highly controlled, here, we concentrate on the differences of initiation strategies within the new experiment. However, as for the comparison between experiments (original vs. new), we will check whether the higher controlled dynamic starting procedure indeed yielded less variance compared to the lower controlled static starting procedure.


Movement initiation time
Due to the differences in the starting procedure (dynamic vs. static), a comparison of movement initiation time between experiments (original vs. new) is hardly feasible, since in the dynamic starting procedure, the stimuli presentation is triggered by movements and hence movements were already initiated with stimuli presentation; in the static starting procedure, stimuli are presented after fixed 200 ms, and hence movements were not necessarily already initiated with stimulus presentation (please see Appendix B, 
Figure B5
). Therefore, we focused on the variation in the movement initiation time in the new experiment only. We found a significant initiation times in the new (M = 0.43, SD = 0.21, see 
Figure 11A
, t(34) = 11.73, p < .001, d = 1.98, 95% bCId [1.54, 2.82]. Additionally, we conducted an ANOVA with the independent variables stimuli position and response procedure. The ANOVA revealed a significant main effect for stimuli position, F(1, 33) = 20.96, p < .001, η 2 = .37, indicating higher initiation times in the centered stimuli position than in the edged stimuli position (see 
Figure   11B
); neither main effect for response procedure, F(1, 33) = 0.43, p = .52, nor the interaction, F(1, 33) = 2.38, p = .13, were significant. 


Movement initiation position
In order to evaluate what participants did during the 200 ms between clicking on the start box and stimuli presentation, we examined the movement initiation position. Here, the results revealed no different variance on the initiation position comparing the new (M = 923 px, SD = 26 px) and the original experiment (M = 896 px, SD = 15 px, see 
Figure 12A
), Levene's test F(1, 69) = 3.54, p = .06, but please see Appendix B, 
Figure B6
. However, as suggested by the means and also supported statistically, t(69) = -5.26, p < .001, g = -1.24, 95% bCIg [-1.97, -0.69], on average, participants in the static starting procedure supposedly moved the mouse only slightly after clicking the start box and hence stayed closer to the start box before stimuli presentation as compared to participants in the dynamic starting procedure who had to move for stimulus presentation (see also 
Figure 10C
-D). To explain the variance in the new experiment, we conducted an ANOVA with the independent variables stimuli position and response procedure.
The ANOVA revealed a significant main effect for stimuli position, F(1, 33) = 4.96, p = .05, η 2 = .11, indicating a more advanced initiation position for the edged stimuli position than for the centered stimuli position (see 
Figure 12B)
; neither main effect for response procedure, F(1, 33) = 1.80, p = .19, nor the interaction, F(1, 33) = 0.08, p = .78, were significant. A lower value on the y axis/coordinate corresponds to a more advanced movement initiation position.
In sum, our analysis of movement consistency across participants via the movement initiation time and position confirmed our hypotheses. Participants in the lower controlled static starting procedure exploited their freedom with regard to initiation times (supporting H3), and stuck close to the start box with regard to their average initiation position, though with high variability (see Appendix B, 
Figure B5
 and 
Figure B6
). The greater variety of initiation strategies in the static starting procedure was partly explained by the stimuli position but could not compensate for the general effect. Thus, with an edged stimuli position, participants initiated their movements earlier and on a more advanced position in trials, as compared to the centered stimuli position (controverting H6).


Discussion
In this study, we investigated how different implementations (i.e., design factors) of the mouse-tracking paradigm influence the mapping between (covert) cognitive processing and (overt) mouse movement. We expected that different design factors would influence the consistency of mouse movements (within trials, between trials and between participants) and hence the theoretically expected cognitive effects as given by discrete and continuous mousetracking measures while choice effects, that is, decision outcomes should remain unaffected.
Overall, we found that design factors significantly influence the cognitive effects and the consistency of mouse movements, though not always in the direction we expected.
Specifically, we investigated the influence of three factors, the starting procedure, the response procedure, and the stimuli position in a mouse-tracking version of a standard intertemporal choice task. To this end, we compared data from two studies: On the one hand, a previously published study 
(Dshemuchadse et al., 2013)
, which used a dynamic starting procedure in combination with a hover response procedure and stimuli that were presented at the center of the screen. On the other hand, a new study, which used a static starting procedure in combination with varying response procedures (hover vs. click) and stimuli positions (centered vs. edged).
With regard to the influence of the design factors on theoretically predicted effects, we split our analysis examining effects on choice behavior as well as discrete and continuous movement measures. Examining choice behavior, we found the expected effects such as temporal discounting and the date-delay effect. Interestingly, we found generally stronger temporal discounting in the original study compared to the new study. Though this is a common finding in the temporal discounting literature (cf. 
Lempert & Phelps, 2016)
, it could potentially be attributed to group differences concerning fatigue since participants in the new study reported to be less tired and more rested than participants in the original study (see Appendix A; 
Berkman, Hutcherson, Livingston, Kahn, & Inzlicht, 2017;
Blain, Hollard, & Pessiglione, 2016)
.
The date-delay effect, however, was present for both groups with comparable effect sizes. The occurrence of temporal discounting and the date-delay effect served as a general manipulation check for the current intertemporal choice paradigm and were the prerequisite for the investigation of the cognitive effects found in both discrete and continuous movement measures.
Examining discrete movement measures (i.e., average deviation), we found that the anticipated cognitive effects were sensitive to the influences of the starting procedure: The originally found effect showing more direct mouse movements for late-large compared to soon-small choices was not significant when using a static starting procedure; variations of the response procedure and the stimuli position did not introduce further variance. While we had not expected a strong effect of the starting procedure on the discrete measures, the finding is in line with previous discussions that especially for more subtle cognitive effects (as is often the case in value-based decision making) design factors might play a more important role . When examining continuous measures in the time continuous multiple regression analysis, we found that the anticipated movement effects were highly sensitive to influences of the starting procedure: The originally found effect showing reliably higher beta weights for the predictor value ratio in the date condition compared to the delay condition vanished when applying a static starting procedure; again, variations of the response procedure and the stimuli position could not compensate for this influence. This finding supported our expectations and substantiates previous results examining the influence of the starting procedure on cognitive effects as given by continuous movement measures .
With regard to the influence of the design factors on the consistency of mouse movements, we found that the static starting procedure yielded less consistent movements than the dynamic starting procedure supporting our expectations and previous research . This pattern was present for the consistency of mouse movements both within trials and across participants. Only for the consistency across trials we did not find the expected differences between starting procedures, though trends pointed in the expected direction. We also found that the additional influence of the response procedure and the stimuli position was less distinct and largely in contrast to our expectations: We did not find the expected evidence that a hover response procedure and a centered stimuli position yielded more consistent mouse movements than a click response procedure and an edged stimuli position. Instead, we found that a click response procedure and an edged stimuli position yielded more consistent mouse movements. Only for the consistency across trials we found the expected pattern in favor of the hover response procedure and the centered stimuli position.
Our results suggest that different design factors in a mouse-tracking paradigm-here specifically the starting procedure as well as the response procedure and stimuli positionindeed influence the consistency of mouse movements and thereby the theoretically important effects investigated in such studies. We showed that dynamic effects in value-based decision making-here specifically intertemporal choice-are highly sensitive to the setup of the mousetracking paradigm. Hence, we must assume that each experimenter's decision about the design of a mouse-tracking paradigm might influence the results that she might find and report.
Furthermore, we must assume that a comparison of effects over several studies (e.g., in meta analyses) should take into account differences in the methodological setup.
In order to exemplify this conclusion in the case of intertemporal choice, we refer to the literature that also investigated the action dynamics of intertemporal decision making. To our knowledge there exist only four studies including our original work 
(Calluso et al., 2015;
Dshemuchadse et al., 2013;
O'Hora et al., 2016;
Scherbaum et al., 2018a)
. The most basic results of those studies focused on the comparison of mouse movements between choices of the soonsmall and the late-large option. The original article by 
Dshemuchadse and colleagues (2013)
 found that soon-small choices were overall associated with more direct mouse movements than late-large choices and that this effect positively correlated with the difficulty of the choice task. 
Scherbaum and colleagues (2018a)
 as well as 
Calluso and colleagues (2015)
 reported the opposite effect concerning choices but did not evaluate its connection to difficulty. O'Hora and colleagues (2016) reported no overall (main) effect for the choice but found that the directness of the mouse movements depends on the subjective evaluation of the choice task, and hence its difficulty. Our results suggest that those results cannot be compared directly since in each of these studies another setup of the mouse-tracking procedure was applied: The latter two studies applied a static starting procedure and a click response procedure, whereas our original study applied a dynamic starting procedure and a hover response procedure.
However, besides the fact that we found evidence that a comparison between studies should take methodological differences into account, our results also suggest that the mapping between cognitive processing and mouse movements might vary with differing design factors.
Thus, it is a plausible assumption that the validity of the reverse inference from mouse movements to cognitive processing also varies with differing design factors, which makes a comparison between studied even more problematic. In order to produce (theoretically) valid, reproducible, and comparable results, the mapping should be optimized and held constant.


Towards an evidence-based gold standard for mouse-tracking paradigms
There already is an emerging debate about boundary conditions and standards for mousetracking paradigms which is gradually evolving from discussion at conferences and meetings to articles in the literature 
(Faulkenberry & Rey, 2014;
Fischer & Hartmann, 2014;
Scherbaum & Kieslich, 2017;
Wulff, Haslbeck, & Schulte-Mecklenbeck, in prep)
. The discussion has been focused on two main issues: (1) How must a mouse-tracking paradigm be designed in order to continuously capture the ongoing decision process, that is, ensure a direct mapping of the cognitive processing onto motor movements? (2) How must such trajectories be analyzed with the aim to make valid inferences about the underlying cognitive processes? The latter issue is motivated by the notion that effects can easily be driven by only a small subset of trajectories 
(Wulff et al., in prep)
. In consequence, the consistency of mouse movements within trials, across trials, and across participants is of superordinate importance for the diagnostic value of trajectories when measuring cognitive processes.
As the first issue is more analytic (ad hoc) and the second issue is more synthetic (post hoc), we should take a new perspective by tackling both issues experimentally. This approach consists of an experimental manipulation of design factors in mouse-tracking paradigms and their influence on the consistency of the mouse movements, which has recently been investigated for the first time )(but please see 
Burk, Ingram, Franklin, Shadlen, & Wolpert, 2014
 demonstrating a decrease of changes of mind with decreasing vertical distance between choice options). Together with two further studies 
(Grage, Schoemann, Kieslich, & Scherbaum, in prep; Kieslich, Schoemann, Grage, & Scherbaum, submitted)
, our study incorporated this approach and the recent results, and eventually provides a strong argument in favor of evidence-based standards for mouse-tracking paradigms. Based on the current evidence, we would recommend using a dynamic starting and a hover procedure in combination with a centered stimulus position. As such recommendations might depend on the relevant task, and hence the measured cognitive process, there might be instances in which the methodological implementation must be adjusted (cf. . Accordingly, we would recommend using an edged stimulus presentation when a static starting procedure is required, to facilitate an earlier horizontal mouse movement. Certainly, our recommendations constitute only a starting point for future discussions about standards for mouse-tracking paradigms. Hence, we would like to encourage further work in the same direction challenging our recommendations.
However, the validation of mouse-tracking as a process-tracing method using the current approach can only provide standards for designing mouse-tracking studies so that one can produce the most reliable and comparable data. Our approach comes with two caveats: First, we cannot measure the mapping between cognitive processing and mouse movements without interference from design factors. Second, we cannot validate the (theoretical) basis that justifies mouse-tracking as being a process-tracing method. This assumption states that the continuous cognitive processing leaks into continuous motor movements 
(Spivey, 2007;
Spivey & Dale, 2006
). Whenever we conduct mouse-tracking studies and analyze mouse movement trajectories, we must accept this assumption in order to conduct the reverse inference.
It has been pointed out that this reasoning is deductively invalid because different cognitive processes can be responsible for the same observable pattern 
(Poldrack, 2006, p. 59
).
To overcome this caveat, forward inference 
(Heit, 2015;
Henson, 2006)
 has been emphasized:
Forward inference turns the direction of inference upside down by making cognitive processing explicit through instruction which has recently been studied for eye-tracking paradigms 
(Schoemann, Schulte-Mecklenbeck, Renkewitz, & Scherbaum, submitted;
Schulte-Mecklenbeck, Kühberger, Gagl, & Hutzler, 2017)
. The forward inference route from cognitive processing to behavioral pattern might also be beneficial to approach the raised problems in mouse-tracking.
By taking the forward inference route, one might be able to evaluate to what degree the continuous processing leaks into continuous (mouse) movements and what design factors of a mouse-tracking paradigm produce the best fit between processing and movements. Here, an appropriate experimental paradigm is still to be designed but might be the next necessary step in order to complement recent efforts on the development of an evidence-based gold standard for mouse-tracking paradigms as well as the validation of mouse-tracking as a process-tracing method.


Conclusion
We studied the impact of three commonly occurring methodological variations on the quality of mouse-tracking measures, and hence, the reported cognitive effects. We found varying effects with varying methodological setups of the mouse-tracking paradigm. In sum, our studyaugmented with previous  evidence-suggests that the methodological setup of mouse-tracking studies needs to be taken into account when interpreting mouse-tracking data. The problems concern both the validity of mouse-tracking as a processtracing method and the principles of reproducible science (i.e., detailed documentation of setup and procedure; cf., 
Munafò et al., 2017;
Nosek et al., 2015)
. Both problems can be tackled by understanding how different design choices affect the resulting data and hence consequently solved by compiling an evidence-based gold standard for mouse-tracking. 


B. Raw and mean mouse movements
Mouse movements on x/y plane 
Figure B1
. Heatmaps of log transformed probability for pooled mouse movements along the x/y plane for the original (A) and new (B) experiment. Brighter colors indicate higher probability. White curves represent mean mouse movements aggregated over all trials and participants.    
Figure 1 .
1
A simplified illustration of mouse-tracking as process-tracing method. Cognitive processing (on the left) is depicted as the activation difference between two options as a function of time. The corresponding continuous mouse movement (on the right) is depicted as the recorded mouse cursor position (on the x/y-plane) in a standard mouse tracking paradigm in which participants have to choose between two options represented as response areas on a computer screen. Through a reverse inference (the lower arrow from right to left), this mouse movement is taken as an indicator of the relative activation of response options over the course of the decision-making process, assuming that the more an option is activated, the more the mouse trajectory deviates towards it (the upper arrow from left to right).


in Matlab 2010b (MathWorks Inc., Natick, MA) as presentation software, running on a Windows XP SP2 personal computer. Participants performed their responses with a standard computer mouse (Logitech Wheel Mouse USB). The mouse speed was reduced to ¼ in the systems settings and nonlinear acceleration was switched off. Mouse movement trajectories were sampled with a frequency of 92 Hz and recorded from presentation of the complete choice options (including temporal delays and monetary values) until participants indicated their response and the trial ended. As targets for mouse movements, response boxes were presented at the top left and top right of the screen. Due to the variation of mouse-tracking design factors, the setup of the current study deviated from the original study with respect to the starting and response procedure (see Procedure) and stimuli position. Concerning the stimuli position, the choice options were presented either in the vertical center of the left and the right half of the screen (centered stimuli position, analogue to the original study, see Figure 2 top), or within the response boxes at the top left and top right edge of the screen (edged stimuli position, see Figure 2 bottom).


Figure 2 .
2
Setup of the experiment: Participants had to click with the mouse cursor on a red box at the bottom of the screen. After clicking, response boxes appeared at the upper edge of the screen. In the dynamic


(bCIg)[-0.55, 0.39], nor in sex (original: 21 female and 15 male; new: 20 female and 15 male).


Figure 3 .
3
Proportion of choices of the LL option as a function of the interval between the LL and the SS option for the original study with dynamic start condition (dyn) and the new study with static start condition (sta).


Figure 4 .
4
The difference of average deviation (z-scored AD) between SS and LL choices as simple main effects for each experiment (dyn vs. sta) as well as within the new experiment separately for each condition (h/chover/centered; h/e -hover/edged; c/c -click/centered; c/e -click/edged). Note: Error bars depict standard deviation; * depicts a significant t-test with p < .05; n.s. depicts a non-significant t-test with p ≥ .05.


Figure 5 .
5
Average x-coordinate per normalized time step depending on choice (SS vs. LL) for the original (A) and new (B) experiment. Coordinates were first averaged within and then across participants. Note: Confidence bands depict standard error.


Figure 6 .
6
Aggregated time continuous beta weights and beta weight contrast for predictors value ratio and time interval (original experiment). Curves indicate influence strength on predicting trajectory angle of mouse movements. Beta weights are shown for the date condition (A) and delay condition (B). Date-delay beta weight


Figure 7 .
7
Aggregated time continuous beta weights and beta weight contrast for predictors value ratio and time interval (new experiment). Curves indicate influence strength on predicting trajectory angle of mouse movements. Beta weights are shown for the date condition (A) and delay condition (B). Date-delay beta weight contrasts are depicted in C. Note: Lines above graph show temporal significantly segments greater than zero (with at least 10 significant time steps in a row); confidence bands depict standard error.


Figure 8 .
8
Mean continuous movement index for the original and new experiment over all conditions (A), as well as within the new experiment for each condition (B). Note: Error bars depict standard error.


Figure 9 .
9
Mean bimodality index for the original and new experiment over all conditions (A) as well as, within the new experiment for each condition (B). Note: Error bars depict standard error.


Figure 10 .
10
Heatmaps of log transformed probability for pooled mouse movements along the x-axis over normalized time steps for the original (A) and new (B) experiment over all conditions. Brighter colors indicate higher probability that trajectories crossed a bin at a specific time step. White curves represent mean mouse movements aggregated over all trials and participants. Average velocity of mouse movements per normalized time step between experiments (C) and within the new experiments (D) for each condition (h/c -hover/centered; h/ehover/edged; c/c -click/centered; c/e -click/edged). Note: Confidence bands depict standard error.


Figure 11 .
11
Mean movement initiation time for the original and new experiment over all conditions (A) as well as, within the new experiment for each condition (B). Note: Error bars depict standard error; For the original experiment the movement initiation time is 0 per definition.


Figure 12 .
12
Mean movement initiation position (y coordinate) for the original and new experiment over all conditions (A) as well as, within the new experiment for each condition (B). Note: Error bars depict standard error.


Figure B2 .
B2
Heatmaps of log transformed probability for pooled mouse movements along the x/y plane within the new experiment. Movements were separated according to hover (A and B; n = 16) and click (C and D; n = 19) response procedure as well as centered (A and C) and edged (B and D) stimuli position. Brighter colors indicate higher probability. White curves represent mean mouse movements aggregated over all trials and participants.Mouse movements on x coordinateFigure B3. Average x coordinate of mouse movements per normalized time step between experiments (A) and within the new experiments (B) for each condition (h/c -hover/centered; h/e -hover/edged; c/cclick/centered; c/e -click/edged). Note: Confidence bands depict standard error.


Figure B4 .
B4
Panel A and B: Heatmaps of log transformed probability for pooled mouse movements along the y-axis over normalized time steps for the original (A) and new (B) experiment. Brighter colors indicate higher probability that trajectories crossed a bin at a specific time step. White curves represent mean mouse movements aggregated over all trials and participants. Panel C and D: Average y coordinate of mouse movements per normalized time step between experiments (C) and within the new experiments (D) for each condition (h/chover/centered; h/e -hover/edged; c/c -click/centered; c/e -click/edged). Note: Confidence bands depict standard error.


Figure B5 .
B5
Probability distribution of movement initiation time (in seconds) after stimulus onset for the original experiment (A) and the new experiment overall conditions (B), as well as, for each condition within the new experiment (C-F; hover/centered -C; hover/edged -D; click/centered -E; click/edged -F).


Figure B6 .
B6
Probability distribution of movement initiation position (y coordinate in px) after stimulus onset for the original experiment (A) and the new experiment overall conditions (B), as well as, for each condition within


Table A2 .
A2
Results of independent samples t-tests comparing different sample statistics between the original and the new experiment. Note. Student's t-test were performed. ᵃ Levene's test was significant (p < .05), suggesting a violation of the equal variance assumption. Applying Welch's t-test yielded comparable results
95% CI for Cohen's d
Variabel
t
df
p
Cohen's d
Lower
Upper
Sex (f = 0, m = 1)
-0.100
69.000 0.921
-0.024
-0.489
0.442
Age
-0.456
69.000 0.650 ᵃ
-0.108
-0.574
0.358
ITI
2.334
69.000 0.023 ᵃ
0.554
0.078
1.026
RT
-3.226
69.000 0.002 ᵃ
-0.766
-1.246
-0.281
Time (h)
1.564
69.000 0.122
0.371
-0.099
0.839
Sleep (h)
0.405
69.000 0.686
0.096
-0.370
0.561
Rested
-2.092
69.000 0.040
-0.497
-0.967
-0.022
Motivated
-1.225
69.000 0.225 ᵃ
-0.291
-0.757
0.178
Energy
-0.974
69.000 0.334
-0.231
-0.697
0.237
Bored
-0.110
69.000 0.913
-0.026
-0.491
0.439
Stressed
0.690
69.000 0.493
0.164
-0.303
0.629
Interested
-0.502
69.000 0.617
-0.119
-0.584
0.347
Nervous
-0.120
69.000 0.905
-0.029
-0.494
0.437
Aroused
-1.006
69.000 0.318
-0.239
-0.705
0.229
Tired
1.935
69.000 0.057 ᵃ
0.459
-0.014
0.929


In this article, the term discrete mouse-tracking measure refers to any measure that summarizes (i.e., over time) dynamic characteristics of a continuous mouse movement in one single value.


In this article, the term cognitive effect is used rather conventionally, that is, the effect of a paradigm, a manipulation, or stimuli on cognitive processing. This effect on cognitive processing should be expressed in mouse movements; in turn, effects in mouse movements should reflect cognitive effects (reverse inference, seeFigure 1).


contrasts are depicted in C. Note: Lines above graph show temporal segments significantly greater than zero (with at least 10 significant time steps in a row); confidence bands depict standard error.








Acknowledgements
We thank Marie Gotthardt for her support in data collection. This research was partly supported by the German Research Council (DFG grant SFB 940/2 to Stefan Scherbaum). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.






Appendix


A. Descriptive Sample Statistics
 
Table A1
. Descriptive sample statistics for the original (Group = 1) and new (Group = 2) experiment. 


Variable


Group
 










Unfolding visual lexical decision in time




L
Barca






G
Pezzulo








PLoS ONE




7


4
















10.1371/journal.pone.0035932














Tracking second thoughts: Continuous and discrete revision processes during visual lexical decision




L
Barca






G
Pezzulo








PLoS ONE




10


2


















10.1371/journal.pone.0116193














Self-Control as Value-Based Choice




E
T
Berkman






C
A
Hutcherson






J
L
Livingston






L
E
Kahn






M
Inzlicht




10.1177/0963721417704394








Current Directions in Psychological Science
















Neural mechanisms underlying the impact of daylong cognitive work on economic decisions




B
Blain






G
Hollard






M
Pessiglione




10.1073/pnas.1520527113








Proceedings of the National Academy of Sciences




113


25
















The Psychophysics Toolbox




D
H
Brainard




10.1163/156856897X00357








Spatial Vision




10
















Motor effort alters changes of mind in sensorimotor decision making




D
Burk






J
N
Ingram






D
W
Franklin






M
N
Shadlen






D
M
Wolpert








PLoS ONE




3


9
















10.1371/journal.pone.0092681














Analysis of hand kinematics reveals inter-individual differences in intertemporal decision dynamics




C
Calluso






G
Committeri






G
Pezzulo






N
F
Lepora






A
Tosoni




















10.1007/s00221-015-4427-1








Experimental Brain Research




233


12














Action Dynamics in Intertemporal Choice Reveal Different Facets of Decision Process




J
Cheng






C
González-Vallejo




10.1002/bdm.1923








Journal of Behavioral Decision Making




30


1
















A probabilistic, dynamic, and attribute-wise model of intertemporal choice




J
Dai






J
R
Busemeyer




10.1037/a0035976








Journal of Experimental Psychology: General




143


4
















Graded motor responses in the time course of categorizing atypical exemplars




R
Dale






C
Kehoe






M
J
Spivey








Memory & Cognition




35


1


















10.3758/BF03195938














The effects of the framing of time on delay discounting




W
B
Dehart






A
L
Odum








Journal of the Experimental Analysis of Behavior




103


1


















10.1002/jeab.125














Something in the way she moves--movement trajectories reveal dynamics of self-control




D
Dignath






R
Pfister






A
B
Eder






A
Kiesel






W
Kunde




10.3758/s13423-013-0517-x








Psychonomic Bulletin & Review




21
















Action dynamics reveal two types of cognitive flexibility in a homonym relatedness judgment task




M
Dshemuchadse






T
Grage






S
Scherbaum




10.3389/fpsyg.2015.01244








Frontiers in Psychology




6














How decisions emerge: Action dynamics in intertemporal decision making




M
Dshemuchadse






S
Scherbaum






T
Goschke




10.1037/a0028499








Journal of Experimental Psychology: General




142


1
















The developing mind in action: measuring manual dynamics in childhood




C
D
Erb








Journal of Cognition and Development




19


3


















10.1080/15248372.2018.1454449














Protocol analysis: Verbal reports as data




K
A
Ericson






H
A
Simon








MIT Press


Cambridge












Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses




F
Faul






E
Erdfelder






A
Buchner






A.-G
Lang




10.3758/BRM.41.4.1149








Behavior Research Methods




41


4
















Response trajectories capture the continuous dynamics of the size congruity effect




T
J
Faulkenberry






A
Cruise






D
Lavro






S
Shaki








Acta Psychologica




163


















10.1016/j.actpsy.2015.11.010














Extending the reach of mousetracking in numerical cognition




T
J
Faulkenberry






A
E
Rey




10.1038/35006062








Frontiers in Psychology




5














Pushing forward in embodied cognition: May we mouse the mathematical mind?




M
H
Fischer






M
Hartmann








Frontiers in Psychology




5


















10.3389/fpsyg.2014.01315














Now or Later? Attentional Processing and Intertemporal Choice




A
M
Franco-Watkins






R
E
Mattson






M
D
Jackson




10.1002/bdm.1895








Journal of Behavioral Decision Making




29
















Time Discounting and Time Preference: A Critical Review




S
Frederick






G
Loewenstein






T
Donoghue








Journal of Economic Literature




40


2
















Doing Psychological Science by Hand




J
B
Freeman




10.1177/0963721417746793








Current Directions in Psychological Science


















MouseTracker: Software for studying real-time mental processing using a computer mouse-tracking method




J
B
Freeman






N
Ambady




10.3758/BRM.42.1.226








Behavior Research Methods




42


1
















Assessing bimodality to detect the presence of a dual cognitive process




J
B
Freeman






R
Dale








Behavior Research Methods




45


1


















10.3758/s13428-012-0225-x














Hand in motion reveals mind in motion




J
B
Freeman






R
Dale






T
A
Farmer




10.3389/fpsyg.2011.00059








Frontiers in Psychology




35


1
















Unraveling the sub-processes of selective attention: insights from dynamic modeling and continuous behavior




S
Frisch






M
Dshemuchadse






M
Görner






T
Goschke






S
Scherbaum




10.1007/s10339-015-0666-0








Cognitive Processing




16


4
















Lost to translation: How design factors of the mouse-tracking procedure impact the inference from action to cognition




T
Grage






M
Schoemann






S
Scherbaum












Manuscript submitted for publication








Rate of temporal discounting decreases with amount of reward




L
Green






J
Myerson






E
Mcfadden








Memory and Cognition




25


5


















10.3758/BF03211314














The Online Recruitment System ORSEE 2.0 -A Guide for the Organization of Experiments in Economics




B
Greiner










Working Paper Series in Economics


















Brain Imaging, Forward Inference, and Theories of Reasoning




E
Heit




10.3389/fnhum.2014.01056








Frontiers in Human Neuroscience




8
















Forward inference using functional neuroimaging: Dissociations versus associations




R
Henson








Trends in Cognitive Sciences




10


2


















10.1016/j.tics.2005.12.005














Mouse tracking reveals that bilinguals behave like experts




S
Incera






C
T
Mclennan








Bilingualism: Language and Cognition




19


03


















10.1017/S1366728915000218






















JASP Team
















Mousetrap: An integrated, open-source mouse-tracking package




P
J
Kieslich






F
Henninger




10.3758/s13428-017-0900-z


















Mouse-tracking: A practical guide to implementation and analysis




P
J
Kieslich






F
Henninger






D
U
Wulff






J
M B
Haslbeck






M
Schulte-Mecklenbeck




10.31234/osf.io/zuvqa








A Handbook of Process Tracing Methods


M. Schulte-Mecklenbeck, A. Kühberger, & J. G. Johnson


New York






in press








Cognitive conflict in social dilemmas: An analysis of response dynamics




P
J
Kieslich






B
E
Hilbig








Judgment and Decision Making




9


6
















Design factors in mouse-tracking: What makes a difference?




P
J
Kieslich






M
Schoemann






T
Grage






J
Hepp






S
Scherbaum












Manuscript submitted for publication








An assessment of the temporal dynamics of moral decisions




G
J
Koop








Judgment and Decision Making




8


5
















Response dynamics: A new window on the decision process




G
J
Koop






J
G
Johnson








Judgment & Decision Making




6


8
















The response dynamics of preferential choice




G
J
Koop






J
G
Johnson




10.1016/j.cogpsych.2013.09.001








Cognitive Psychology




67


4
















The Malleability of Intertemporal Choice




K
M
Lempert






E
A
Phelps




10.1016/j.tics.2015.09.005








Trends in Cognitive Sciences




20


1
















Embodied Choice: How Action Influences Perceptual Decision Making




N
F
Lepora






G
Pezzulo








PLoS Computational Biology




11


4


















10.1371/journal.pcbi.1004110














A manifesto for reproducible science




M
R
Munafò






B
A
Nosek






D
V M
Bishop






K
S
Button






C
D
Chambers






N
Percie Du Sert






J
P A
Ioannidis




10.1038/s41562-016-0021








Nature Human Behaviour




1


1
















Area under the curve as a measure of discounting




J
Myerson






L
Green






M
Warusawitharana








Journal of the Experimental Analysis of Behavior




76


2


















10.1901/jeab.2001.76-235














Promoting an open research culture




B
A
Nosek






G
Alter






G
C
Banks






D
Borsboom






S
D
Bowman






S
J
Breckler






Yarkoni








Nature




348


6242


















10.1126/science.aab3847














Decisions in Motion: Decision Dynamics during Intertemporal Choice reflect Subjective Evaluation of Delayed Rewards




D
O'hora






R
Carey






A
Kervick






D
Crowley






M
Dabrowski




10.1038/srep20740








Scientific Reports




6
















Local dynamics in decision making: The evolution of preference within and across decisions




D
O'hora






R
Dale






P
T
Piiroinen






F
Connolly




10.1038/srep02210








Scientific Reports
















Information Processing as a Paradigm for Decision Making




D
M
Oppenheimer






E
Kelso




10.1146/annurev-psych-010814-015148








Annual Review of Psychology




66


1
















Attention and choice: A review on eye movements in decision making




J
L
Orquin






S
Mueller Loose








Acta Psychologica




144


1


















10.1016/j.actpsy.2013.06.003














The VideoToolbox software for visual psychophysics: transforming numbers into movies




D
G
Pelli




10.1163/156856897X00366








Spatial Vision




10


4
















Can cognitive processes be inferred from neuroimaging data?




R
A
Poldrack




10.1016/j.tics.2005.12.004








Trends in Cognitive Sciences




10


2
















The Cat is on the Mat. or is it a Dog? Dynamic competition in perceptual decision making




J
C
Quinton






N
C
Volpi






L
Barca






G
Pezzulo








IEEE Transactions on Systems, Man, and Cybernetics: Systems




44


5


















10.1109/TSMC.2013.2279664














Four score and seven years from now: The date/delay effect in temporal discounting




D
Read






S
Frederick






B
Orsel






J
Rahman








Management Science




51


9


















10.1287/mnsc.1050.0412














Changes of mind in decisionmaking




A
Resulaj






R
Kiani






D
M
Wolpert






M
N
Shadlen




10.1038/nature08275








Nature




7261
















An eye fixation analysis of multialternative choice




J
E
Russo






L
D
Rosen




10.3758/BF03212910








Memory & Cognition




3


3
















Psychometrics of the continuous mind: Time continuous multiple regression as a method to exploit the dynamics of computer mouse movements




S
Scherbaum






M
Dshemuchadse












Manuscript submitted for publication








How decisions evolve: The temporal dynamics of action selection




S
Scherbaum






M
Dshemuchadse






R
Fischer






T
Goschke








Cognition




115


3


















10.1016/j.cognition.2010.02.004














Harder than Expected: Increased Conflict in Clearly Disadvantageous Delayed Choices in a Computer Game




S
Scherbaum






M
Dshemuchadse






S
Leiberg






T
Goschke




10.1371/journal.pone.0079310








PLoS ONE




8


11














A Bird in the Hand Isn ' t Good for Long: Action Dynamics Reveal Short-Term Choice Impulses in Intertemporal Choices




S
Scherbaum






S
Frisch






M
Dshemuchadse








Experimental Psychology




65


1
















Step by step: Harvesting the dynamics of delay discounting decisions




S
Scherbaum






S
Frisch






M
Dshemuchadse




10.1080/17470218.2017.1307863








The Quarterly Journal of Experimental Psychology




71


4
















Process dynamics in delay discounting decisions : An attractor dynamics approach




S
Scherbaum






S
Frisch






S
Leiberg






S
J
Lade






T
Goschke






M
Dshemuchadse








Judgement and Decision Making




11


5
















Stuck at the starting line: How the starting procedure influences mouse-tracking data




S
Scherbaum






P
J
Kieslich








Behavior Research Methods




50


5


















10.3758/s13428-017-0977-4














Forward Inference in Risky Choice: Mapping between Gaze and Decision Processes




M
Schoemann






M
Schulte-Mecklenbeck






F
Renkewitz






S
Scherbaum












Manuscript submitted for publication








Process-Tracing Methods in Decision Making: On Growing Up in the 70s




M
Schulte-Mecklenbeck






J
G
Johnson






U
Böckenholt






D
G
Goldstein






J
E
Russo






N
J
Sullivan






M
C
Willemsen








Current Directions in Psychological Science




26


5


















10.1177/0963721417708229
















M
Schulte-Mecklenbeck






A
Kühberger






B
Gagl






F
Hutzler




10.1002/bdm.2007








Inducing Cognitive Processes: Bringing Process Measures and Cognitive Processes Closer Together
















The Continuity of Mind




M
J
Spivey








Oxford University Press


Oxford












Continuous Dynamics in Real-Time Cognition




M
J
Spivey






R
Dale








Current Directions in Psychological Science




15


5
















From The Cover: Continuous attraction toward phonological competitors




M
J
Spivey






M
Grosjean






G
Knoblich




10.1073/pnas.0503903102








Proceedings of the National Academy of Sciences


the National Academy of Sciences






102














Dietary Self-Control Is Related to the Speed With Which Attributes of Healthfulness and Tastiness Are Processed




N
J
Sullivan






C
A
Hutcherson






A
Harris






A
Rangel




10.1177/0956797614559543








Psychological Science




26


2
















Thinking dynamics and individual differences: Mouse-tracking analysis of the denominator neglect task




B
Szaszi






B
Palfi






A
Szollosi






P
J
Kieslich






B
Aczel










Judgment and Decision Making




13


1
















Modeling the Dynamics of Risky Choice




M
M J W
Van Rooij






L
H
Favela






M
Malone






M
J
Richardson








Ecological Psychology




25


3


















10.1080/10407413.2013.810502














Measuring the (dis-)continuous mind: What movement trajectories reveal about cognition




D
U
Wulff






J
M B
Haslbeck






M
Schulte-Mecklenbeck












Manuscript in preparation








A reverse stroop task with mouse tracking




N
Yamamoto






S
Incera






C
T
Mclennan








Frontiers in Psychology




7


















10.3389/fpsyg.2016.00670















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]