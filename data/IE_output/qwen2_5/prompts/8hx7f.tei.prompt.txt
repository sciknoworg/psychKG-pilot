You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
In the last few decades, a number of papers, both empirical and conceptual, advocated for broadening of the study of cognitive abilities by including concepts and constructs from the domain of decisionmaking 
(Baron, 1985;
Stankov, 2017;
Stanovich, 2009a
Stanovich, , 2009b
Stanovich, , 2012
Stanovich & West, 1998
, 2000
. There have been some indications that tasks that measure different cognitive biases (CBs) capture something other than fluid intelligence, a construct that is labelled as rationality (e.g. Stanovich 
& West, 1998
& West, , 2000
Stanovich, West & Toplak, 2016)
 or decision-making competence (DMC, e.g., Bruine de Bruin, 
Parker & Fischoff, 2007;
Parker & Fischoff, 2005)
, and as such could enrich our understanding of individual differences in cognitive processing.
In line with this, the first goal of our study was to investigate a factorial structure of a set of CB tasks in a search for existence of a rationality factor. Many of the previous studies showed that the correlations between different CBs are generally very low and that rationality assessed using CB tasks has a complex factorial structure with a minimum of two to three factors needed to sufficiently account for the common variance among the tasks (e.g. 
Aczel, Bago, Szollosi, Foldes & Lukacs, 2015;
Berthet, 2021;
Berthet & de Gardelle, 2021;
Ceschi, Constantini, Sartoti, Weller & Di Fabio, 2019;
Slugoski, Shields & Dawson, 1993;
Teovanović, Knežević & Stankov, 2015;
Weaver & Stewart, 2012)
. Given these results, it was reasonable to expect that a multifactorial solution will be needed to appropriately account for the relations between individual CBs included in the study. Alternatively, if a single-factor solution turns out to be the most appropriate, which is possible given that previous studies failed to find any systematicity regarding CBs factorial structure, this factor would probably be weak and account for a modest amount of variance among the individual tasks.
Our second goal was to investigate the relationships between individual CBs and rationality factor(s) with different cognitive abilities such as fluid intelligence and numerical ability. Although  showed that some CB tasks are independent from cognitive abilities, the majority of studies demonstrated that the correlations between CBs and cognitive abilities are low to moderate (e.g. 
Bruin et al., 2007;
Erceg, Galić & Bubić, 2019;
Parker & Fischoff, 2005;
Teovanović et al., 2015;
Toplak, West & Stanovich, 2011)
. Still, a recent study by 
Blacksmith, Behrend, Dalal & Hayes (2019)
 even found a correlation between decision-making competence (which is a combination of CB tasks and other tasks that are not generally considered to assess cognitive biases) and general mental ability as high as to declare them to be empirically redundant. The conflicting findings pointed to a need for additional studies using different measures and additional research contexts.


Bruine de
Finally, our third goal was to validate extracted rationality factor(s) by correlating them with variables from their nomological network 
(Cronbach & Meehl, 1955)
, such as superstitious and conspiracy beliefs, thinking dispositions and personality traits (convergent validity), as well as with potential reallife consequences of decisions (criterion validity). Previous studies showed that better performances on CB tasks are related to lower susceptibility towards epistemically suspect beliefs (superstitious/ paranormal/conspiracy beliefs; Čavojova, 
Šrol & Jurkovič, 2020;
Erceg et al., 2019;
Pennycook, Cheyne, Barr, Koehler & Fugelsang, 2015;
Šrol, 2020
Šrol, , Toplak et al., 2017
 but greater orientation towards actively open-minded thinking 
(Stanovich & West, 1997
, 1998
Sá, West, & Stanovich, 1999;
Toplak, West & Stanovich, 2014a)
. A few studies looking at the relationship between personality traits and CB performance found that the ability to resist framing errors was positively related with emotional stability, agreeableness and conscientiousness 
(Soane & Chmiel, 2005)
 and that a composite score on different CB tasks was positively correlated with conscientiousness, openness and honesty/humility 
(Weller, Ceschi, Hirsch, Sartori, & Costantini, 2018)
. Finally, performance on CBs tasks seems to be predictive of more positive real-life outcomes 
(Bruine de Bruin et al., 2007;
Toplak et al., 2017)
.


Previous work on the dimensionality of cognitive bias tasks
There were several empirical attempts so far to establish the structure and dimensionality of CBs. In short, the results of these attempts mainly do not align particularly well with theoretical taxonomies of cognitive biases (e.g. 
Oreg & Bayazit, 2009)
 and also fail to show a great level of consistency among themselves. In one of the earlier attempts at analyzing the structure of CBs, 
Weaver and Stewart (2012)
 concluded that a two-factor solution best describes the relationships among nine different tasks from judgment and decision-making domain. The tasks that are traditionally used in the heuristics and bias research loaded on the first, coherence factor (e.g. probability combination tasks, conjunction fallacy task, framing task, base-rate task and four-card selection task).
The other factor, correspondence factor, accounted for the variance in tasks such as judging the prices of cars and apartments, or the quality of teams based on different features. 
Teovanović et al. (2015)
 conducted a factor analysis with oblimin rotation on seven CB tasks (anchoring, belief bias, overconfidence bias, hindsight bias, base-rate neglect, sunk cost and outcome bias) and also found that the two-factor solution was the most appropriate for their data. They labeled the first factor that was mostly defined by the belief bias and outcome bias as the "normative rationality" factor, as higher score on this factor indicated higher rates of predictably irrational responses. The other factor was defined by positive loadings on anchoring and hindsight bias and negative loadings on overconfidence. They called this factor the "ecological rationality" factor as the biases that defined this factor indicated responsiveness to feedback and well calibrated confidence judgments, the characteristics of an ecologically rational agents.
Unlike the previous two studies, 
Aczel et al. (2015)
 showed that the four-factor solution was the best in both of their studies for the eight CBs that they investigated (gambler's fallacy, sunk cost, base-rate neglect, Monty Hall problem, insensitivity to sample size, relativity bias, outcome bias, anchoring).
However, although the four-factor solution was the most appropriate in both of their studies, these factors were not consistent and the factor structures greatly varied. For example, outcome bias task formed a separate factor in one study, but loaded on the same factor with gambler's fallacy, sunk cost and relativity bias in the other study. However, it has to be noted that in this study each of the biases was measured with only one item.
Recently, additional indication of multidimensionality of CB tasks came from research by 
Ceschi et al. (2019)
. This study investigated the greatest number of CB tasks thus far 
(17)
. A three-component solution fitted the data best. The first dimension was defined by biases that indicate reliance on both availability heuristic (availability bias, imaginability bias) and representativeness heuristic (base-rate neglect, conjunction fallacy, gambler's fallacy). The second dimension was defined by biases that indicate overvaluation of costs and overestimation of losses (endowment effect, sunk cost) as well as those reflecting an overly optimistic view of the world (optimism bias). Finally, the third dimension was defined by biases that depend on the reference point (anchoring and regression towards the mean).
Finally, two most recent studies 
(Berthet, 2021;
Berthet & de Gardelle, 2021)
 showed that the correlations among eight and six CBs respectively were very low, to the degree that the datasets were not even suited for a factor analysis.
All of these studies indicate that CB tasks are very heterogeneous and share very little common variance among themselves. Here, we must take a small detour and mention somewhat related body of research on the so-called "decision-making competence". Motivated by 
West (1998, 2000)
 observation about existence of positive manifold among CB tasks, 
Parker and Fischoff (2005)
 factor-analyzed seven different behavioral decision tasks using principal components analysis (consistency in risk perceptions, recognizing social norms, resistance to sunk cost, resistance to framing, applying decision rules, path independence and overconfidence) and showed that, although a three-factor solution fitted data the best, one factor was able to explain 25.1% of the variance in these tasks. They concluded that judgmental biases are not just random errors and that the DMC construct can explain why some people are better and others worse at solving these types of tasks. Bruine de 
Bruin et al. (2007)
 tried to replicate and extend these findings. This time they found that the same tasks were best described by two factors, but that one factor was again able to explain a substantial portion of variance among tasks (30.1%). However, it must be noted that only five of the seven DMC tasks can be viewed as classical CB tasks (i.e. applying decision rules and recognizing social norms are not typical CB tasks). Relatively high correlations that these two tasks exhibit with other DMC tasks could be the reason why 
Parker and Fischoff (2005)
 and 
Bruine de Bruin et al. (2007)
 found enough communality among their tasks, while the authors examining exclusively CB tasks generally do not find these levels of communality.
The difference between rationality and intelligence There are both empirical and theoretical reasons to treat rationality as a distinct construct from the fluid intelligence. In their book, 
Stanovich et al. (2016)
 systematized a large body of their own research on the validity of their rationality measure -Comprehensive Assessment of Rational Thinking (CART).
The CART is, basically, a composite measure of large number of different CBs, numeracy and some thinking dispositions such as disposition towards superstitious or conspiracy thinking. The authors showed that the correlation between CART and fluid intelligence scores are moderate. Similar results were obtained when correlating DMC and fluid intelligence. In their recent work, Bruine de Bruin, Parker and Fischoff (2020) described a lot of DMC validation studies and concluded that the correlation between DMC and fluid intelligence seems to be generally moderate and positive. Building on these non-perfect correlations between rationality and fluid intelligence and additional theoretical work, 
Stanovich et al. (2016)
 claim that rationality assessed with CB tasks is broader and conceptually distinct from fluid intelligence.
Theoretically, the conceptual difference between fluid intelligence and rationality follows from the tripartite theory of mind 
(Stanovich, 2009a
(Stanovich, , 2012
. This theory differentiates between autonomous, algorithmic and reflective parts of the mind. According to it, in order to successfully solve the majority of CBs tasks, a person first has to overcome initial incorrect response generated by the autonomous mind. In other words, a person has to reflect on his/her response and recognize the need to engage in more deliberate processing (reflective mind's task) and also possess adequate ability and computational power to calculate or come up with a correct response (algorithmic mind's task). Conversely, success on classical intelligence tests do not depend so much on the reflective, but only on the algorithmic mind, constituting this as the crucial difference between the two. In this framework, the reflective mind refers to different thinking dispositions that are in principle malleable and to a degree teachable (e.g. reflection/impulsivity (R/I), the disposition to be careful at the expense of speed when solving tasks 
[Baron, 2018]
) and that help a person effectively solve a task, while algorithmic mind refers more to cognitive capacities or abilities in the narrower sense that are less prone to change (e.g. fluid intelligence; 
Stanovich, 2012)
. In this conceptualization of intelligence (i.e. what the usual intelligence tests measure), intelligence is practically not dependent on dispositions but mostly on capacities. From this, it follows that rationality captured by CB tasks is a broader construct than intelligence, as it is more dependent on thinking dispositions, and therefore it makes sense to conceptually differentiate between the two (e.g. 
Stanovich, 2009b
Stanovich, , 2012
. 
Baron (1985)
 also holds that rational thinking is mainly about thinking dispositions. Specifically, it refers to the way people form beliefs based on which they make decisions, what rules they follow and what methods they use in the process. In other words, rationality can be seen as a disposition to adequately search for goals, possibilities, and evidence, trying to find even those that are against our current ones and giving them a fair treatment. However, as opposed to Stanovich who defines intelligence in narrower terms, 
Baron (1985)
 holds that thinking dispositions are integral part of intelligence. Hence, rational thinking and behavior is integral part of intelligent thinking/behavior. However, apart from rational thinking that is mostly about "how" people go about forming beliefs (i.e. dispositions), intelligence also includes additional properties such as cognitive capacities and knowledge. From this argument follows that intelligence is a broader concept than the rationality, where rationality represents a part of intelligence that is more malleable and teachable. This is also the main reason why it could be useful to make some sort of distinction between the two -as rationality is all about "how" to think, we can teach people to be rational (and therefore more intelligent) by teaching them better, or more rational ways of thinking.


Convergent and predictive validity of rationality
Bruine de Bruin et al. (2020) showed that DMC factor predict real-life outcomes after statistically adjusting for individual differences in fluid intelligence. For example, DMC predicted an index of different life outcomes that could have come about due to person's bad decisions, over and above cognitive ability. 
Weller, Moholy, Bossard and Levin (2015)
 similarly showed that lower DMC obtained at ages 10-11 predicted greater psycho-social difficulties two years later, even after statistically accounting for the effects of numeracy and inhibitory control. Finally, DMC was shown to be negatively correlated with childhood delinquency and the number of sexual partners after statistically adjusting for cognitive ability 
(Parker, Bruine de Bruin, Fischoff & Weller, 2018)
.
However, as we previously noted, DMC is a composite of both CB and non-CB tasks, so it would be instructive to see the predictive validity of rationality captured solely by CB tasks. In their study, 
Toplak et al. (2017)
 arrived at similar conclusion as DMC researchers: a composite score based on five CB tasks (ratio bias, belief bias in syllogistic reasoning, cognitive reflection, probabilistic and statistical reasoning, and rational temporal discounting) predicted a composite score of real-world outcomes across several different domains (electronic media use, secure computing, substance use, driving behavior, financial behavior and gambling) even when the effects of education and gender were taken into account.
Taken together, it seems that there is a general consensus among researchers that rationality taps into additional constructs besides fluid intelligence. In their review, Bruine de Bruin et al. (2020) cite studies that show that DMC, in addition to fluid intelligence, correlates with motivation to think (i.e. need for cognition), experience (i.e., crystallized intelligence), executive cognitive functioning (i.e., inhibition, monitoring and shifting; 
Del Missier, Mäntylä, & De Bruin, 2012)
 and numeracy, and conclude that "decision-making competence may reflect a combination of intellectual, motivational, emotional, and experience-based skills" (p. 188). This is similar to one of two possible interpretations of CBs put forward by 
Stankov (2017)
. According to this interpretation, CBs could lie on the crosssection of personality and abilities, being an amalgam of cognitive and non-cognitive processes (the other possibility is that CBs are domain specific and capture very specific processes). In addition to the previously mentioned non-cognitive variables relevant for DMC/rationality, additional thinking disposition of actively open-minded thinking (AOT) seems to be of particular importance. AOT is a disposition to be open to and actively search for new information and evidence that counteract current beliefs as well as the willingness to revise beliefs if new evidence deems it necessary 
(Baron, 2019;
Baron, Scott, Fincher & Metz, 2015)
. Again, drawing from the tripartite theory 
(Stanovich, 2012)
, this disposition could be related to the reflective mind -the ability and/or disposition to reflect on one's current beliefs/position and correct them. The relationship between the CBs and AOT has also been demonstrated empirically. For example, in 
Stanovich et al. (2016)
 work on validation of CART, AOT has consistently come up as one of the strongest correlates of the rationality score with the majority of the CART subtests showing moderate to high correlations with it.
In sum, the aim of our research was to explore the dimensionality of a relatively large number of CB tasks and to investigate the validity of rationality factor(s) by correlating it/them with different variables from its nomological network (i.e. fluid intelligence, numeracy, cognitive reflection, AOT, superstitious and conspiracy thinking and personality traits) as well as several real-life outcomes (DOI, life and career satisfaction). In comparison to previous studies that investigated the validity of rationality measures, ours has several advantages. For example, in comparison to 
Aczel et al. (2015)
, we use several tasks per bias and thus have more reliable CB measures. Next, we tested more CB tasks than 
Teovanović et al. (2015)
 and Bruine de Bruin (2007) and did it over two different samples (students and community sample). Although 
Ceschi et al. (2019)
 measured greater number of tasks, they did not include variables that could be used for validating their decision-making factors which was accomplished in this work. Therefore, our study extends previous in several ways: we investigate a great number of CB tasks, have two different samples and a large set of variables useful for investigating convergent and predictive validity of rationality measure.


Study 1 Participants
A total of 253 undergraduate University of Zagreb students participated in this study (214 from humanities and social sciences -mostly psychology students, 34 from other disciplines and five undeclared). There were 187 females, 62 males and four participants refused to report their gender. The mean participants' age was 21.47 (SD = 1.89; Min = 18, Max = 29).


Procedure
Participants solved our focal tasks as a part of larger battery of tasks not all of which are reported in this study. Relevant for the current study, the participants solved 10 different CB tasks (each CB was measured by several items), fluid intelligence test, numeracy test, cognitive reflection test, three different questionnaires, actively open-minded thinking, superstitious thinking and conspiracy thinking questionnaire and, finally, the decision-outcome inventory (DOI). Some of the tasks were pseudorandomized (meaning that there were three different sequences of tasks randomly given to participants), while the others were solved in fixed order. The students filled-in the tests and questionnaires on computers, in groups of 20 to 25 participants under the supervision of the investigators. The whole testing lasted up to two hours and was organized in two parts divided by a 15 minute break. In the first part, participants first solved a group of tasks that were pseudo-randomized and that consisted of cognitive reflection, base-rate neglect, causal base-rate, outcome bias, sunk cost, gambler's fallacy, attribute and risk framing tasks. This was followed by the belief-bias syllogism tasks, numeracy and DOI that were presented in fixed order. After this, there was a 15 minute break followed by a second part of testing. In the second part, all of the tasks were presented in fixed order.
Participants first solved the fluid intelligence test, followed by the conspiracy thinking questionnaire, actively open-minded thinking questionnaire, second part of CBs that have to be measured with two related questions in order to determine whether the bias is present (Type-B tasks in 
Aczel et al. [2015]
 taxonomy of CB tasks, such as framing effects and outcome bias), four-card selection tasks, availability bias tasks and, finally, the superstitious thinking questionnaire.


Instruments
Here we describe all of the measures used in the Study 1 and for each one provide one item/task as an example. We provide all the items we used in the Supplement file.


a) Cognitive biases tasks
Belief-bias syllogisms. Belief-bias syllogisms tasks pit the believability of a conclusion against its logical validity. An example task goes as follows: "Premise 1: All flowers have petals. Premise 2:
Roses have petals. Conclusion: Roses are flowers." 
(Markovits & Nantel, 1989
). This conclusion is logically incorrect because it does not follow from these premises that only flowers have petals, so roses might as well be something other than flowers (e.g. children collage art). However, because the conclusion that roses are flowers conforms with our empirical reality, it is quite believable and many people accept it as valid. Thus, the false intuitive response is the product of believability of the conclusion, while strong conformity with logical principles is needed to come up with the right, logically valid response. In addition to this example, we used three additional syllogisms whose conclusions were believable, but logically incorrect and four more syllogisms whose conclusions were unbelievable, but logically correct, having eight items in total. The responses where participants identified believable and illogical conclusions as logically incorrect or unbelievable and logical conclusions as logically correct were scored as correct. We calculated the total score as a proportion of correct responses. Four-card selection task. We had four different tasks based on the ones introduced by 
Wason (1966
Wason ( , 1968
) that all had the same structure. A rule was explicitly stated for each of the items and the participants were informed that the rule may or may not be correct. Their task was to check the accuracy of the rule by turning two cards of their choice. For example, one of the items was: "Rule: If a card shows "5" on one side, the word "Excellent" is on the opposite side. Which two cards would you choose to turn to check the accuracy of this rule?". Participants were then showed four cards that had numbers 5 and 3 and words "Excellent" and "Good" written on the front side. The correct answer here would be to turn the cards containing number 5 and the word "Good" because turning only these would allow one to conclude whether the rule is correct or false. However, often, the participants are lured to turn the card containing the word "Excellent" instead of the card "Good", although for the rule to be correct it does not matter what is behind the "Excellent" and "3" cards 
(Nickerson, 1998)
. Besides the two non-deontic tasks, such as the one described, we also had two deontic tasks whose content was related to a socially relevant, not just arbitrary, rule (e.g. If a person drinks beer, he/she must be over 18 years old). Picking the two accurate cards to turn was scored as 1 while all other combinations were scored as 0. The total score was calculated as the average of responses on four tasks.
Base-rate neglect. The participants were presented with four different problems where the description of a person was contrasted to the base-rate information. These problems were modeled after 
Kahneman and Tversky (1973)
 items and the one we used four our study were from De Neys and Glumicic 
2008
study. Specifically, there were two possible answers, a stereotypical one (based on the description of a person) and one that was consistent with the base-rate. For example, one of the items was: "Among the 1000 people that participated in the study, there were 50 16-year-olds and 950 50-year-olds. Helen is randomly chosen participant in this research. Helen listens to hip hop and rap music. She likes to wear tight T-shirts and jeans. She loves to dance and has a small nose piercing. Which is more likely? a)
Helen is 16 years old; or b) Helen is 50 years old."
Here, the description of Helen was stereotypical for a teenager. Thus, a person who heavily relies on this information would respond with an "a". However, base-rate information indicated a much greater probability that a randomly chosen participant would be 50 years old. Thus, response "b" was coded as a correct one. However, it has to be noted that technically this does not have to be a correct response and that this depends on the diagnosticity of the information in the task (e.g. the information could be that Helen is a minor which would render a base-rate based response incorrect). Nevertheless, as the stereotypical response is the intuitive one on these tasks and the participants need to engage in correcting this response in order to accompany base rate information into a judgment 
(Barbey & Sloman, 2007;
Pennycook, Fugelsang, & Koehler, 2012)
, we always coded a response based on baserates as a correct one. The correct responses were scored as 1 and total score was the average of four responses.
Causal base-rate. Here, participants are provided with two conflicting pieces of information: one is statistical and favors one decision while another is based on personal, case-based experience and favors another decision. These items were based on the classic Volvo vs. Saab items from 
Fong, Krantz and Nisbett (1986)
 and were used in previous studies (e.g. 
Toplak et al., 2011;
Stanovich et al., 2016)
. An example item would be: Gambler's fallacy. Gambler's fallacy refers to the tendency for people to see links between events in the past and events in the future when the two are really independent 
(Tversky & Kahneman, 1974)
.
We used four items that were either taken from 
Stanovich et al. (2016)
 or obtained through personal communication with Predrag Teovanović. Consider the following problem which is one of the four we used:
"Imagine you are throwing a fair coin (there is a 50% chance of it falling on either side) and it happened to fall on the tail side 5 times in a row. What do you think is more likely to happen in the sixth throw? a) A head is more likely in the sixth throw b) A tail is more likely in the sixth throw c) Both head and tail are equally likely in the sixth throw"
Here the correct answer is "c". However, people prone to gambler's fallacy would reason that, since there were five tails in a row, head would be more probably in the sixth throw. This does not make sense as a coin does not "remember" previous outcomes and always has a 50% probability of falling on each side. We measured gambler's fallacy with four items. Correct responses were scored as 1 and the total score was the average of responses.
Availability bias. Availability heuristic refers to assessing the frequency of a class or the probability of an event by the ease with which instances or occurrences can be brought to mind . Availability or the ease of retrieval certain instances of events is often influenced by the vividness or media exposure and does not necessarily correspond to the true frequency of such instances. For example, people might think that homicide is a much more common cause of death than the diabetes (it is the other way round; this was one of the questions we asked our participants) because homicides are often covered in media while diabetes complications and deaths are rarely discussed publicly. In this study, we followed a paradigm introduced by 
Lichtenstein, Slovic, Fischhoff, Layman and Combs (1978)
 and asked the participants which of the four pairs of lethal events is more common.
Choosing causes of death that are less common but more vivid and more covered in media is a sign of over-reliance on easily available and retrievable information, 
(Pachur, Hertwig, & Steinmann, 2012;
Stanovich et al., 2016)
. Thus, we refer to responses that follow from the availability heuristic even in situations when this does not correspond to reality as the availability bias. We scored the correct responses as 1 and incorrect (based on the availability heuristic) as 0. Again, we calculated the total score as the average of responses. Sunk cost. Sunk cost effect refers to the tendency to "continue an endeavor once an investment of money, effort, or time has been made" 
(Arkes & Blumer, 1985, p. 124)
. We measured the susceptibility to sunk cost using four different scenarios describing situations where a person had to choose between an option reflecting unrecoverable past expenditure (sunk-cost) and a more beneficial option in a given situation (normative option). The items were taken from Aczel et al. (2015), Bruine de Bruin et al. 
2007
and 
Teovanović et al. (2015)
 studies. For example, one of the tasks were "You paid a fair amount of money to rent a club for a birthday party. Three days before the party, you learn that you can move the party completely free of charge to a much better place, a villa with a pool. All additional costs (food, drinks, cleaning) are the same in both locations. It is too late to cancel the rented space, and the money paid cannot be refunded. What would you do?" Participants were instructed to make a choice between two options by using a rating scale that ranged from 1 (most likely to choose sunk-cost option, in this case to have a party in a club) to 6 (most likely to choose the normatively correct option, in this case to throw a party in a villa with a pool). We averaged the score on four individual items to calculate the total score of susceptibility to sunk cost.
Attribute framing. Framing problems assess the degree to which a judgment is affected by irrelevant variations in problem descriptions. Therefore, similar to other studies that measure susceptibility to framing (some of our items were taken from Bruine de Bruin et al., 2007, and some were obtained through personal communication with Predrag Teovanović), for each framing problem a participant reads two scenarios. Scenarios are constructed so that they are normatively identical -the only difference is the way the information is presented (i.e., framed). For example, one scenario our participants saw was this one: "Imagine going to work by public transport every day and in 80% of cases waiting longer than three minutes for the bus to arrive. How satisfied would you be with public transportation services?" Participants were then instructed to evaluate their satisfaction with public transport on a 6-point scale (1 -completely unsatisfied, 6 -completely satisfied). The other, normatively identical scenario to the previous one, was: "Imagine going to work by public transport every day and in 20% of cases waiting less than three minutes for the bus to arrive. How satisfied would you be with public transportation services?" Again, participants rated their satisfaction on a 6point scale. In order to minimize memory effects, two scenarios were presented at two different time points, one at the beginning of the first part of survey and the other at the end of the second part.
Generally, this means that at least an hour passed between the two scenarios during which the participants solved a number of different tasks that further interfered with their memory. We scored susceptibility to framing by subtracting the evaluation in the "worse" scenario (i.e. waiting longer than 3 minutes) from the "better" scenario (i.e. waiting less than 3 minutes). The majority of framing scores when assessed in this way was positive, although there were also negative scores (people who would judge "worse" scenario as being better than a "good" one). As the higher result meant higher framing bias in the expected direction, before calculating the total score on attribute framing, we transformed individual item scores by subtracting them from 5, theoretically highest score indicating maximal susceptibility to bias. In this way, similarly to our other CB tasks, higher scores indicated lower susceptibility to bias, i.e. higher rationality. We then averaged the score on the four attribute framing items to get the total score. Risk framing. Risk framing 
(Tversky & Kahneman, 1981)
, similar to attribute framing, measures the degree to which a judgment is affected by irrelevant variations in problem descriptions. However, this time both scenarios face participants with a choice between a "sure thing" and a risky option. The difference is again in the wording of scenarios. In a so-called loss-frame, the outcome is framed in terms of losses while in a gain-frame it is framed in terms of gains. We measured risk framing with four pairs of tasks. For example, we faced our participants with the following options (the items were taken from Bruine de 
Bruin et al., 2007 and Predrag Teovanović [personal communication])
: "Imagine that a recent research showed that a certain pesticide could kill 1,200 endangered animals. Two response options to this pesticide threat have been proposed: a) If you go with option A, 600 animals would be saved for sure. b) If you go with option B, there is a 75% chance that 800 animals would be saved and a 25% chance that no animals would be saved. Which option would you recommend?" The response scale again had six point (1 -certainly option A, 6 -certainly option B). This was a gain-frame as the animals would be saved, as opposed to the following loss-frame where the animals would die:
"Imagine that a recent research showed that a certain pesticide could kill 1,200 endangered animals.
Two response options to this pesticide threat have been proposed: a) If you go with option A, 600 animals would die for sure. b) If you go with option B, there is a 75% chance that 400 animals would die and a 25% chance that all 1200 animals would die. Which option would you recommend?" We subtracted a gain-frame response from a loss-frame response. Therefore, a higher score indicated a greater susceptibility to framing effects, so before calculating the total score, we did the same transformation as with the attribute framing (subtraction from five) in order for higher scores to indicate higher rationality.
Outcome bias. Our final CB task was the outcome bias task 
(Baron & Hershey, 1988
) that was, similarly to framing problems, composed of two parallel and relatively equivalent scenarios, one with a positive and the other with a negative outcome (we obtained our items from 
Aczel et al., 2015
, Baron & Hershey, 1988
and Teovanović, 2013
. An example of a positive outcome scenario we showed our participants would be the following: "A 54-year-old had heart problems. He had to stop working because of chest pain but he loved his job and wanted to continue working. Pain also affected other things, such as travel and recreation. Successful heart bypass surgery would ease his pain and increase his life expectancy from 65 to 70 years. However, 8% of people who decide to have this operation die because of the operation itself. His doctor decided to go on with a surgery and the surgery was successful. Please rate the doctor's decision to have surgery performed." Participants again rated the quality of the decision on a six-point scale (1 -Very bad decision, 6 -Very good decision). In another scenario, the outcome of the decision was the opposite, in this case a negative one: "A 58-year-old had degenerative hip disease. He was confined to a wheelchair and forced to retire early last year. Due to immobility, he gained weight and was depressed because he could not work or engage in any recreational activities. He loved his job and recreation and did not want to stop with it. He consulted a doctor who told him that successful degenerative hip surgery would ease his pain and prolong his life expectancy by 10 years or more because he could exercise. However, because the surgery is complicated and the man had a milder heart disease, there is a 2% chance of dying from the surgery itself. Unfortunately, there were complications on the operating table and the man died of heart failure.
Please rate the doctor's decision to have surgery performed." We used four pairs of tasks to measure outcome bias. We scored the tasks by subtracting the negative outcome rating from the positive outcome rating with the greater score indicating greater susceptibility to outcome bias. Therefore, we again subtracted the scores from five before calculating the total score as the average score on four outcome bias item pairs. of three items, each with a distinctive characteristic of pitting an intuitive but incorrect responses against a correct one. Probably the best-known item is a bat-and-ball item: "A bat and a ball cost $1.10 in total. The bat costs $1 more than the ball. How much does the ball cost?" The immediate response that comes to mind is 10 cents which is, on a further reflection, incorrect and a correct response I 5 cents. Following the publication of an original three-item test, several studies were published that extended this short form test with additional items (e.g. 
Primi, Morsanyi, Chiesi, Donati, & Hamilton, 2016;
Thomson & Oppenheimer, 2016;
Toplak, West & Stanovich, 2014b)
. In this study, we used six items, each cuing strong but incorrect intuitive response. The proportion of correct responses represented a total score. d) Numeracy. We used the Berlin numeracy test 
(Cokely, Galesic, Schulz, Ghazal, & Garcia-Retamero, 2012
) as a measure of numeracy. The BNT is a four-question test for assessing numeracy and risk literacy. An example of a question is "Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many times would this five-sided die show an odd number (1, 3 or 5)?". The questions are designed in a way that they gradually become harder and a total score is calculated by averaging the responses on the four questions.
e) Actively open-minded thinking. We used a 15-item AOT scale. The items were taken from 
Stanovich and West (2007)
 41-item scale and these specific 15 items were first used in 
Campitelli and Gerrans (2014)
. AOT scale is a scale that assesses individual beliefs about proper standards of thinking by asking participants to indicate their level of agreement (1 -strongly disagree to 6 -strongly agree) with items such as "It is OK to ignore evidence against your established beliefs". This particular item would be reverse coded as it goes against the actively open-minded principles of thinking. The total score on this scale was calculated as a mean level of agreement with the items (after recoding the responses so that the higher score indicates higher AOT) and can be anything between 1 and 6. f) Superstitious thinking. Superstitious thinking was assessed using items from 
Toplak et al. (2011)
.
The scale consisted of 13 items in total, and the participants rated their level of agreement with each of the items on a six-point scale. The total score was calculated as a mean score of responses on all of the items. The items tapped into four different concepts: superstitious beliefs (e.g. "When something good happens to me, I believe it is likely to be balanced by something bad."), luck (e.g. "I have personal possessions that bring me luck at times."), paranormal beliefs ("Astrology can be useful in making personality judgments."), and extrasensory perception ("Dreams can provide information about the future.") g) Conspiracy beliefs. We measured conspiracy beliefs with 12 items taken from the Generic conspiracist beliefs scale developed by 
Brotherton, French, and Pickering (2013)
. For example, one of the items in the scale was "A small, secret group of people is responsible for making all major world decisions, such as going to war." Participants rated their level of agreement with the claims such as this one on a six-point scale and the total score was the average of these 12 ratings. It presents a list of negative outcomes, ranging from mild (e.g. throwing out food or groceries you had bought or destroying clothes because you did not follow the washing instructions on the label) to more serious ones (e.g. participation in a fight or experience of breaking a bone because you fell, slipped, or misstepped). We used 33 different outcomes and our outcomes somewhat differed from the original inventory. Namely, as our Study 1 participants were students, some of the original outcomes were extremely unlikely or totally impossible to have happened to them and were thus removed. For example, we removed some of the most serious negative outcomes for this reason (e.g. been in a jail cell overnight for any reason or got divorced). We also added some outcomes that seems appropriate for college students such as "Overslept classes multiple times" or "Had to do an additional assignment because you missed too many lectures." The total score was calculated in a following way: first, for some outcomes the participants were asked whether they had the opportunity to experience them (for example, someone who does not have a driving license could not have had it seized from him/hertherefore, for these kinds of outcomes, we first asked the participants whether they could or could not experience it). Next, to account for the severity of outcomes, possible outcomes were weighted by the proportion of participants who reported not experiencing them (thus, more severe outcomes or the ones that were experienced by less people were weighted more). Finally, a total score was calculated by averaging these weighted scores.


Results
In order to answer our research questions, we did two things. First, we factor-analyzed our 10 CB measures to investigate the dimensionality of our tasks. Second, we correlated our measures in order to validate our rationality factor(s). Prior to reporting the results of our main analyses, in 
Table 1
 we report the descriptive statistics of raw scores and the reliabilities of all the measures we used in the study. In addition, we also report the effect sizes of biased responding for each of the CB tasks, i.e. the size of the difference between the mean of the responses and the normative value for each of the tasks. Cohen's ds for cognitive bias tasks were calculated by the formula d = (M − μ) / SD, where M is the mean CB score and μ is normative value. 
Table 1
 shows that the participants found CB tasks to be of varying difficulties. Specifically, the highest scores, i.e. the highest number of correct/normative responses, were obtained on the gambler's fallacy tasks and belief-bias tasks, indicating that our participants were the least susceptible to these biases.
Conversely, the four-cards selection task was by far the most difficult revealing strong susceptibility of our participants to confirmation bias as assessed by this task. Overall, and in line with college students being on average relatively smart and rational, our participants on average correctly solved two thirds of fluid intelligence tasks, more than half of the cognitive reflection tasks and just under half numeracy tasks (this numeracy test is generally considered to be very hard; for example, college students scored 40% correct in 
Cokely et al., 2012, study)
, at the same time mostly disagreeing with the items measuring conspiracy and superstitious beliefs.
The correlations among our focal variables are presented in the 
Table 2
. The raw correlations are presented above the diagonal, while the correlations corrected for attenuation are presented below the diagonal. It can be seen that the correlations among our CB tasks are generally positive, albeit low, replicating previous findings 
(Stanovich & West, 1998
, 2000
Parker & Fischoff, 2005;
Teovanović et al., 2015)
. Most of our CB tasks are significantly correlated with three to six other tasks. However, there are three tasks that are more problematic in this way. Specifically, sunk cost is correlated only with two other tasks, risk framing with one, while gambler's fallacy failed to correlate with any of the other CB tasks. Perhaps the main problem with gambler's fallacy was ceiling effect in its scores, as majority of students solved these tasks correctly, while the risk framing tasks had serious issues with reliability that could have substantially diminished its correlations with other tasks. Interestingly, only four of our CB tasks (belief-bias syllogisms, base-rate neglect, four-card selection task and sunk cost) correlated significantly with fluid intelligence, but these correlations were all relatively small. This suggests that the performance on CB tasks was mostly independent of fluid intelligence. However, it was not as independent from numeracy, as both numeracy and cognitive reflection test that mostly captures numeracy construct 
(Attali & Bar Hillel, 2020)
, each correlated with six of the CB tasks.
Moreover, the thinking disposition of actively open-minded thinking was significantly positively correlated with all but one of the CB tasks, indicating that this thinking disposition plays an important role in success on rationality tasks. It is interesting to note that AOT was more important for CB tasks than for success on the fluid intelligence measure, judging from a very low correlation between the two. Finally, it seems that our CB tasks were largely irrelevant for conspiracy thinking and real-life decision outcomes (DOI), but somewhat relevant for superstitious thinking, with four of them being negatively related with this measure.  Before conducting a factor analysis on our CB tasks, we checked whether our data was adequate for performing such analysis. Therefore, we calculated the Kaiser-Meyer-Olkin factor (KMO) and
Bartlett's test of sphericity. KMO reached an acceptable level of KMO = .66 and Bartlett's test indicated that the correlation matrix is significantly different from an identity matrix (χ2(45) = 143.01, p < .001). To identify the most appropriate structure for our data, we conducted a parallel analysis using a fa.parallel function from the R -package "psych" 
(Revelle, 2021)
 and inspected the scree plot. Both the parallel analysis and the scree plot indicated a one-factor solution as the most appropriate one (the outputs from this analysis are presented in Supplemental materials). We then conducted a factor analysis using a maximum likelihood extraction method, extracting one factor with good fit indices (χ2(35) = 35.68, p = .44) that was able to explain 12% of the variance among CB tasks. This is substantially lower amount of explained variance than in the case of the decision-making competence factors obtained in earlier studies (e.g. one factor explained 25% of the variance in 
Parker & Fischoff [2005]
 and 30% of the variance in Bruine de Bruin et al. 
[2007]
 2 ). We called this factor a rationality factor. This factor was most saturated with base-rate neglect, causal base-rate neglect and belief bias scores and least saturated with sunk cost, risk framing and gambler's fallacy scores. Although the general factor of CB tasks was quite weak, we still wanted to check if it has a meaningful pattern of relationships with the other variables from our study. In 
Table 2
 we show all the correlations among our variables. We calculated rationality factor score for each of the participants (i.e., the score reflecting an individual's standing on the factor) using the regression method. Looking at the 
Table 2,
 several things are notable. First, the fact that Rationality factor exhibits relatively strong correlations with numeracy and cognitive reflection and somewhat lower with fluid intelligence confirms that rationality is closely tied to these well-known abilities. As these correlations, even the disattenuated ones, are not perfect, this means that rationality factor captured somewhat different constructs than these other abilities. Second insight from the correlation table is thus related to one of these potential additional constructs. Namely, the rationality factor was strongly related to actively open-minded thinking, and the magnitude of this correlation is greater than any of the correlations between actively open-minded thinking and other cognitive abilities.
In an additional analysis, we wanted to see how much of the variance in rationality factor is explained by numeracy (assessed with CRT), fluid intelligence (ICAR) and actively open-minded thinking, the strongest correlates of rationality factor. Therefore, we regressed the rationality factor on CRT, ICAR and actively open-minded thinking factors using SEM. Specifically, we defined our four latent variables by their corresponding manifest variables (rationality latent variable was defined as a second order factor of ten cognitive bias factors that were each defined by their corresponding manifest variables, while CRT, ICAR and AOT latent variables were defined as first order factors by their respective manifest variables, i.e. six CRT items, 16 ICAR items and 15 AOT items) and did calculations using a "sem" function with maximum likelihood estimation method from the "lavaan" Rpackage 
(Rosseel, 2012
 
(Azen & Budescu 2003;
Budescu, 1993)
 by calculating the general dominance coefficient for each of the predictors. In short, this is done by averaging the added variance explained by the predictor in all possible subsets of regression models. In our case, there are only two possible models for our two predictors -one where the predictor is alone in the model and the other where both of the predictors are included. The dominance coefficient showed that, of the 61% variance explained in the rationality factor, numeracy was responsible for 47% and actively open-minded thinking for 14% of the variance. Finally, neither rationality factor nor individual CB factors were correlated with the decision-outcome inventory score as a potential real-life indicator of bad decisions. This means that either rationality, as measured with these CB tasks, is not important for the chosen real-life decisions or that this version of the decision-outcome inventory is not a particularly good measure of real-life decision quality. However, the rationality factor negatively correlated with superstitious thinking. Given that the actively open-minded thinking correlates with superstitious thinking but neither of the cognitive abilities does, rationality might be related to these epistemically suspect beliefs because of its correlation with actively open-minded thinking. In sum, Study 1 findings show existence of a weak rationality factor that has meaningful relationship with other individual differences in cognitive variables and that mostly reflects numeracy and AOT.


Study 2
The goal of Study 2 was to replicate and expand on findings from the Study 1. In comparison with In sum, in Study 2 we measured the following variables: seven CBs (belief-bias, attribute framing, outcome bias, causal base-rate, base-rate neglect, sunk cost and four-card selection task), cognitive
reflection, actively open-minded thinking scale and SJT , the "Big Five" personality traits (extraversion, emotional stability, agreeableness, openness and conscientiousness) and four potential real-life decision-making outcomes (decision-outcome inventory, job satisfaction, career satisfaction and peer-rated decision-making quality).


Participants
In total, 210 participants participated in our study. As our study was divided in two parts separated by a
week time, there was some loss of participants. Specifically, 183 participants also participated in and completed second part of the study. Therefore, our sample size is somewhat different depending on the analysis. There were 79 male and 103 female participants (28 unknown). The average participants' age was M = 34.31 
(SD = 10.63,
Min = 19,
Max = 63)
. Regarding the education, we had one participant with primary school only, 38 with high school, 41 with college education, 87 with higher education and 16 with PhDs.


Procedure
Similarly as Study 1, Study 2 was conducted in two parts, only this time with a longer time distance between them. In the first part of the study, the participants solved the cognitive reflection test, seven CB tasks and actively open-minded thinking self-report questionnaire and several other measures not reported in this study. In the second part, they completed actively open-minded thinking SJT, personality questionnaire, decision-outcome inventory, job and career satisfaction scale and the second part of the two-part CB tasks (attribute framing and outcome bias). Apart from solving these two parts of our study, we asked our participants to forward an additional link containing several other-report measures to their peers who were asked to rate them on these measures. Of relevance for this study is a peer-rated overall decision-making quality. In total, 192 of our participants received peer ratings. They were mostly rated by two peers (N = 144), some by only one peer (N = 39) and several were rated by three peers (N = 9).


Instruments
In this part, we will only describe those instruments that were either not used or were somewhat changed from Study 1. If we used an instrument that is not listed here, it is the same as was used in Study 1. All of the items from described instruments are presented in the Supplement to this manuscript.
Belief-bias syllogisms. Unlike the first study, in the second study we had only four BBS items and they were all of the same type, namely with unbelievable, but logically correct conclusions. We decided to cut on some of the biases in this study, including some of the BBS task, as we believed that too much of the tasks would have discouraged participants from participating in our study or from finishing it completely, leaving us with much smaller sample size in the end.
Actively open-minded thinking SJT. We developed a three-item construct-driven SJT 
(Lievens, 2017)
 for measuring actively open-minded thinking in realistic work situations. Specifically, each task consisted of a description of a realistic situation that could be encountered in real life or work and four potential responses to this scenario. Participant's task was to choose, out of these four options, what he/she would do in that situation. The response options were developed in a way that they increasingly reflect actively open-minded thinking, starting from a first option that reflected almost complete lack of this type of thinking and was scored as 1 (i.e. deciding now without looking for additional information or competing arguments) and ending with a final option that reflected this type of thinking in a high degree and was scored as 4 (i.e. not only taking more time or looking for additional information before deciding, but specifically looking for those information and arguments that counteracts person's current opinion). Here is a sample item:
"You were recently promoted to the position of HR Manager of a large company. Management expects you to make some changes to motivate employees. An older colleague, a long-term employee of the human resources department, believes based on his practice and experience that rewarding employees according to performance is the best way to motivate them. This sounds like a good idea to you too -it makes sense to you that people will work harder if they are paid according to the work they did and you don't see any objective disadvantage of this method. What will you do? a) You can't see any major drawbacks to this approach, so you'll be introducing a performance-based reward system as soon as possible. This will increase employee motivation and at the same time meet the requirements of management.
b) You will talk to an older colleague who advocates this system and knows more about it than you do.
If his reasons for introducing this approach are reasonable and good, you will implement it immediately.
c) You will engage and try to find on the internet what other experts think about why such a system should be introduced. d) Although it seems that this approach is generally supported, you will do your best to find key arguments against it or identify possible problems with the introduction of this human resource management practice."
By looking at previous example, it is possible to identify the key features of response options. In response a), an actor relies exclusively on his/her current arguments and knowledge for making a decision and makes immediate decision. In response b), an actor seeks out for additional information, but only for the information that is confirming his/her current views. In option c), an actor seeks out a wider range of available information, although still not specifically looking for one that will disconfirm his/her position. know about the important things in life" were omitted and items such as "People should take into consideration evidence that goes against conclusions they favor" and "It is important to be loyal to your beliefs even when evidence is brought to bear against them" were included in this version of questionnaire. In total, questionnaire had 13 items that were scored in the same way as the ones in the Study 1.
Mini International Personality Item Pool questionnaire (Mini IPIP). Mini IPIP 
(Donnellan, Oswald, Baird, & Lucas, 2006
) is a 20-item personality measure, measuring the Big 5 traits each with four items. Participants were instructed to rate the accuracy of the description (e.g. "Am the life of the party.", "Sympathize with others' feelings.", "Get chores done right away.", "Have frequent mood swings.", "Have a vivid imagination.") on a five-point scale (1 = Completely incorrect 5 = Completely correct) and the total score is the average of the ratings on the four items per trait, after the items were recoded so that a higher score indicates a higher degree of a trait.
Decision-outcome inventory. In comparison with Study 1, Decision-outcome inventory was significantly shorter, consisting of 18 items. We dropped number of outcomes that were appropriate for student population, but inappropriate for adults. We also added several finance-related items such as "Took out an unfavorable short-term loan." and "Spent more money in a month than you could afford."
The items were again weighted by the percentage of participants who did not experience given outcome, thus giving more weight to more serious negative outcomes. The total score is average of item scores.
Job satisfaction. We measured job satisfaction with one item: "Think about your current job. Weigh all its advantages and disadvantages and then assess how satisfied you are, on the whole, with your job."
Participants were instructed to rate their satisfaction on a five-point scale (1 = Very dissatisfied, 5 = Very satisfied).
Career satisfaction. We measured career satisfaction with a five item Career satisfaction scale 
(Greenhouse, 1990)
. Participants were instructed to indicate to what extent they agreed or disagreed (1 = Completely disagree, 5 = Completely agree) with statements such as "I am satisfied with the success I have achieved in my career." or "I am satisfied with the progress I have made towards meeting my overall career goals." (the rest of the statements are in the Appendix). The total score is calculated as a mean of the scores on these five statements and higher score indicates higher career satisfaction.
Peer-rated general decision-making quality. This scale was developed by 
Wood (2012)
 and consists of four items (e.g. "The decisions my friend makes are quality ones", "The decisions my friend makes end up working out well"). Participants' peers were instructed to rate their agreement with the statements describing their colleague/friend participants on a five-point scale (1 = Completely disagree, 5 = Completely agree). Each participant was rated by up to three different peers. Therefore, before calculating the final scores we averaged peers' ratings on these four items. The final score is calculated as an average of ratings and its higher values indicate peers' more positive perception of decisionmaking abilities of their colleague.


Results
In Study 2, we used the same analyses as in the Study 1. This means that we factor-analyzed our CB tasks in order to investigate their dimensionality and then correlated the factor score(s) with different measures in order to validate them. Before showing results from these analyses, we present descriptive statistics and reliabilities of our measures in 
Table 4
.  Not unexpectedly, our community sample scored somewhat lower on basically every comparable task than college students. Looking at the correlations among the CB tasks in 
Table 5
, it can be seen that they are again mostly low or moderate and positive. The exception is attribute framing that was uncorrelated with any of the other tasks. Possible reason for this is that this measure exhibited very low reliability, lower than in the Study 1. Again, numerical abilities and actively open-minded thinking proved to be relevant for the performance on our CB tasks. Specifically, cognitive reflection test score, as a measure of numeracy, was significantly correlated with each of the seven CB tasks, while actively open-minded thinking was related to five out of seven CB tasks, or six out of seven when measured with a situational judgment test. Conversely, personality traits seemed to matter less, with emotional stability (significantly correlated with five of the CB tasks) and openness (significantly correlated with four of the tasks) being the most important of the personality traits. Finally, looking at the correlations between CB tasks and potential outcomes of decisions (DOI, job and career satisfaction and peer-rated decision-making quality), there is not much to see. Only few of the correlations were significant, and even those that were significant were relatively small (all of the correlations were lower than r = .20, disattenuated lower than r = .30). 
Table 5
. Correlations among the Study 2 variables. Raw correlations are above the diagonal while the disattenuated correlations are below the diagonal.  Before proceeding with factor-analyzing our CB measures, we checked if the data was adequate for performing such analysis. KMO (KMO = .75) was again acceptable and the Bartlett's test showed that the correlation matrix was not an identity matrix (χ2(21) = 131.27, p < .001) meaning that our data was appropriate for conducting factor analysis. We again conducted parallel analysis and scree plot inspection in order to decide on the most appropriate number of factors. Again, both the parallel analysis and scree plot indicated that a one-factor solution was the most appropriate one (output of this analysis is shown in the Supplement). A one-factor solution obtained using a maximum likelihood extraction method is presented in 
Table 6
. This solution showed an excellent fit to the data (χ2(14) = 11.54, p = .64) and one factor was able to explain 22% of the variance in our CB tasks. Although a single factor managed to account for substantially larger part of the CB tasks variance compared to the Study 1, this is still relatively modest amount of common variance shared by CB tasks. However, it seems that the co-variation in our CB tasks scores was not random but at least to some degree under the influence of factor that could be labeled as the rationality factor. As in Study 1, this factor was mainly saturated with base-rate neglect, causal base-rate and belief bias scores, with outcome bias, four-card selection and sunk cost scores showing greater loadings than in the Study 1. Conversely, attribute framing exhibited somewhat lower loading compared to Study 1, being the only score whose loading did not exceed the value of .30. Again, to investigate the nature of our rationality factor and test its convergent and predictive validity, we correlated it with number of different variables and outcomes. We again calculated the rationality score using a regression method based on factor loadings. We report these correlations in 
Table 5
. Raw correlations are presented above the diagonal while the disattenuated ones are below the diagonal. 
Table 5
 offers several interesting insights. First, the rationality factor again correlated highly with the numeracy measure (cognitive reflection), as well as with two actively open-minded thinking measures.
Indeed, these three were the highest correlations between the rationality factor and any of the variables, confirming that cognitive abilities such as numeracy, as well as actively open-minded thinking, lie at the core of the rationality factor that we extracted.
We again conducted a SEM regression analysis to investigate whether both numeracy and actively open-minded thinking independently predict rationality factor and to identify the portion of the variance that they account for. Similarly as in the Study 1, rationality latent variable was defined as a second-order factor of seven CBs that were each defined by their corresponding manifest variables. Finally, correlations between the rationality factor and real-life outcomes (DOI, job and career satisfaction and peer's perception of decision quality) were generally low. However, it is interesting that those that scored higher on rationality measures were somewhat more satisfied with their jobs (r = .18; p < .05; disattenuated r = .27) and careers (r = .19, p < .01, disattenuated r = .26).


Discussion
Across the two studies presented in this manuscript, we aimed to investigate a) the dimensionality of relatively large set of CB tasks, b) the correlations between the uncovered rationality factor(s) and other cognitive abilities as well as other measures from its/their nomological network (i.e. actively openminded thinking, epistemically suspect beliefs and personality traits), and c) the correlations between rationality factor(s) and some real-life outcomes that could depend on good decision making.
Regarding the first research questions, our results were comparable to some of the similar previous studies. The correlations were mostly positive, but small in size in both of our studies (the highest correlations in the Study 1 were between base-rate neglect and belief bias/causal base-rate, [both correlations were r = .27], and between base-rate neglect and causal base-rate in Study 2, r = .37).
Factor-analyzing these tasks, we found that a single factor solution was the best one in both studies. In Study 1, one factor was able to explain 12% of the variance among our variables, while in Study 2 it was able to account for and 22% percent of the variance. Although a single-factor solution turned out to be the most appropriate for our data in both studies, the extracted factors in both studies were quite weak. This leads to the conclusion that is in line with majority of previous studies that investigated the dimensionality of CB tasks: these tasks are quite heterogenous and idiosyncratic which reflects in low levels of shared variance among them (e.g. 
Aczel et al., 2015;
Blacksmith et al., 2019;
Ceschi et al., 2019;
Berthet, 2021;
Teovanović et al., 2015)
. Therefore, although our results indicate that there exists some common core that accounts for a success on many different CB tasks, it seems that this core is small and not robust enough to be replicated across the studies. However, although small, this common core, showed meaningful relationship with other variables in our studies.
There are several things apparent from the 
Table 2 and Table 5
 that are relevant for our discussion about the validity of the rationality factor that we extracted. First, our rationality score was positively and moderately correlated with fluid intelligence in Study 1. The fact that superior decision making on normative tasks is positively, but modestly, related with fluid intelligence confirms some of the previous findings (e.g. 
Teovanović et al., 2015;
Sobkow, Olszewska & Traczyk, 2020)
, although some other studies obtained somewhat higher correlations between these types of tasks (e.g. 
Blacksmith et al., 2019)
. Therefore, it seems to make sense to separate the construct of rationality from fluid intelligence, as some researchers advocate (e.g. Bruine de 
Bruin et al., 2020;
Stanovich, 2012;
Stanovich et al., 2016)
. While fluid intelligence certainly underpins quality decision making, it is not a synonym for it, resulting in, among other, many cases of "dysrationalia" 
(Stanovich, 2002
(Stanovich, , 2009b
Erceg et al., 2019)
.
Second, the highest correlations that the rationality score showed with any of the variables was with measures of numeracy and cognitive reflection (that seems to be indistinguishable from numeracy; 
Attali & Bar-Hillel, 2020;
Erceg et al., 2020)
. This is also a common finding as many previous studies have shown that numeracy is the strongest predictor of good decision-making both in real life and on CB tasks (e.g. 
Allan, 2018;
Cokely, Feltz, Ghazal, Allan, Petrova & Garcia-Retamero, 2018;
Garcia-Retamero, Sobkow, Petrova, Garrido & Traczyk, 2019)
. In fact, strong disattenuated correlations between the rationality score and numeracy/cognitive reflection show that these two constructs overlap to a large extent. 
Cokely et al. (2018)
, as part of their skilled decision theory, propose that numeracy and decision-making skills share number of common processes, including metacognitive, heuristic, intuitive, affective, subjective, gist-based, and number-sense processes. They conclude that statistical numeracy, a type of numeracy measured by the Berlin Numeracy Test, predicts decisions because statistical numeracy tests are relatively representative judgment and decision-making tasks whose solving requires the same kinds of reasoning and metacognitive skills essential for good decision making. These metacognitive skills that are shared among numeracy and rationality task could be related with the disposition to be more careful, thorough and elaborate in solving problems. For example, numeracy and cognitive reflection predicted a higher number of verbalized considerations on risk decision-making tasks which was positively related both to the number of normative correct responses and to the response times 
(Cokely & Kelley, 2009)
. Similarly, it has been shown that participants that scored higher on statistical numeracy performed better on various tasks (lotteries, intertemporal choice, denominator neglect, and confidence judgments) because they deliberated more during decision making and, in that way, more accurately evaluated their judgments 
(Ghazal, Cokely & Garcia-Retamero, 2014)
. Therefore, apart from fluid intelligence and quantitative reasoning, thinking dispositions that predispose a person towards more careful and elaborate cognition seem to also be important for different cognitive biases and, thus, lie at the core of our rationality factor.
This brings us to the third notable conclusion following from our correlation tables which suggests that the most important disposition that underpins rationality and makes a person more careful, thorough and elaborate in solving problems and making decisions is actively open-minded thinking. In fact, in both of our samples the correlations between the rationality score and actively open-minded thinking were comparable to those of rationality and numeracy. In this regard, our results replicate previous findings on the importance of actively open-minded thinking for success on heuristics and biases tasks (e.g. 
Stanovich et al., 2016;
. Given that rationality tasks are perspective, we can conclude that they align nicely with the so-called tripartite theories that extend the popular dual-process theories (e.g. 
Stanovich, 2009a
Stanovich, , 2012
Evans, 2019
). Stanovich's tripartite theory distinguishes between "three different minds", an autonomous mind (that is called Type 1 processing in dual-processes theory), an algorithmic and a reflective mind (these two are the extensions of Type 2 processes). According to this theory, unlike for the classical fluid intelligence tasks, when solving tasks that assess rationality, a person needs to first recognize the need to suppress and override responses generated by the autonomous mind (reflective mind) and only then to have sufficient computational power to replace this initial response by calculating a new, correct one (algorithmic mind). Therefore, in order to be successful on rationality tasks, a person needs to possess adequate dispositions by which he/she will be able to recognize the need to suppress and correct an initial, autonomously generated response, as well as adequate intelligence that will allow him/her to come up with a correct response. In his recent work, Evans (2019) introduced the so-called Type 3 processes that predispose a person to check one's intuition, therefore being conceptually similar to Stanovich's reflective mind, with both of these concepts clearly relating to the previously discussed dispositions of reflection and actively openminded thinking.
Our results are remarkably in line with the ideas put forward by 
Baron (1985)
  be a more fruitful direction, one which we tried pursuing by developing a pilot version of situational judgment test of this disposition. This is something that definitely deserves more work in the future.
Second, unlike fluid intelligence and many other types of cognitive abilities that predominately depend on individual's capacities that are relatively stable and hard to change, actively open-minded thinking is a thinking disposition that could be teachable and changeable. In other words, it is probably possible to teach a person to be more rational through teaching him/her how to apply the principles of actively open-minded thinking in real-life settings, while it would be quite hard to substantially raise someone's score on fluid intelligence tests through instruction.
In discussing the broader perspectives, another question that our findings could inform is the one related to the position of rationality and/or decision-making skills among other cognitive abilities, especially within probably the broadest and most known model of human intelligence, the Cattell-Horn-Carroll (CHC) model. This model represents human cognitive abilities along the three stratums differing in the specificity/generality of cognitive abilities. The most specific abilities constitute the first stratum. These specific abilities then group together to form broad abilities that constitute the second stratum and are perhaps most commonly discussed in the literature (e.g. fluid intelligence, crystalized intelligence, short and long-term memory, speed of processing). Finally, the most general factor, the g-factor constitutes the third stratum and represents the shared variance of second stratum abilities. As we and many other researchers have shown that rationality and/or decision-making competence represents a somewhat distinct construct from the fluid intelligence, it appears that it is not adequately represented in the most complete and broadest taxonomy of cognitive abilities. Others have already took notice of this and suggested the ways in which rationality could be accommodated in this taxonomy (e.g. 
Alan, 2018;
Cokely et al, 2018)
. These suggestions also align nicely with our results, ending up with the proposition that rationality/decision-making skills should be represented as a separate broad ability in the second stratum of CHC taxonomy, one that is more underpinned by thinking dispositions than cognitive capacities and one that could be defined and measured with different CB tasks, statistical numeracy that seems to be highly dependent on thinking dispositions, and different measures of thinking dispositions, especially actively open-minded thinking.
The final point of our discussion related to our main problems represents the relationship between the rationality score and different outcomes in which two things become apparent. First, rationality was related to holding more correct beliefs about the world, as seen from its negative correlation to superstitious thinking in Study 1 (although, it did not correlate with conspiracy thinking). As holding more correct beliefs about the world is the definition of epistemic rationality 
(Stanovich et al., 2016)
, it seems that rationality as measured with ability to suppress cognitive biases is related to epistemic rationality. Second, rationality exhibited quite low and non-significant correlations with real life decision-making outcomes (DOI). The question is why, and we believe that the part of the answer can be deduced from the relationships between different personality traits and these outcomes. As is evident from the 
Table 5
, traits of conscientiousness and emotional stability were the most predictive of DOI.
Therefore, it seems that these long-term outcomes are more affected by stable personality characteristics than the quality of reasoning. This means that, no matter how good and rational someone reasoning is, in affecting real life outcomes this will probably be overcome by whether one is careful or self-disciplined (i.e. conscientious) or anxious and prone to excessive emotional reactions (i.e. emotional stable). A glimpse of the relevance of the rationality perhaps comes from its low, but positive correlation with job and career satisfaction. However, these low correlations between rationality and real-life outcomes need to be viewed from a broader perspective, namely the fact that practically neither of the cognitive ability variables in our two studies (i.e. fluid intelligence, numeracy, cognitive reflection) were meaningfully related with these real life outcomes. This probably reflects the fact that multiple determinants, including luck, work together and sometimes probably in opposite direction to produce these outcomes, diminishing the effects of single individual determinants.
Finally, we will briefly comment on what we see as the probably the biggest downside of our study, namely the low reliability of some of our measures. This in particular refers to two framing measures whose score is calculated as the difference of the scores on two different question versions. These types of scores, the difference scores, are long known to suffer from the reliability problems (e.g. 
Peter, Churchill Jr & Brown, 1993)
. Although the discussion about the reasons for and remedies of this problem is beyond the scope of this article, we can offer a brief speculation about what caused low reliability in our difference scores measures and what could perhaps be done about it. In his article, 
Trafimow (2015)
 writes that the reliability of difference score depends on the reliabilities of each of the two forms and the correlation between them. If we take our attribute framing task from the Study 1, the reliabilities of each of the forms were very low (α = .15 for the first form, α = .29 for the second form).
This reflects our items assessing preferences in completely different domains, diminishing the relationships between these preferences. For example, if a person A prefers a consulting firm (attribute framing item 1) more than a person B, this certainly does not mean that he/she will be more satisfied with the quality of public transport (attribute framing item 4) than person B. Therefore, in order to increase the difference score reliability, it will be necessary to raise the reliability of individual forms.
Unlike our approach here, where we tried to sample over a large range of domains, it would probably be better to devise domain-specific questions where the reliability would not suffer and the framing effects could still be observed. The other CB tasks had a satisfactory reliability given that they were measured with only few items, with the exception of very low reliability of sunk cost in Study 2, although the items were the same as in the Study 1. Granted, these "satisfactory" reliabilities were lower than it is generally deemed acceptable. This was expected given that each of the biases was assessed with relatively few items. However, this could have lowered the relationships between or CBs. Given this, it is possible that, had the CBs been measured more reliably, perhaps with more items, the positive manifold of CB tasks would perhaps be greater than the one we found. However, we do not think that this would fundamentally change our conclusions in terms of the nature of rationality factorit would probably still explain quite variance in CB tasks and exhibit similar correlations with other variables that we measured.


Conclusion
Across two studies, we investigated a validity of the rationality factor(s) as assessed by different CB tasks. Our findings can be summed up in following way: a) one factor, a rationality factor, could be extracted from responses on different CB tasks although it accounted for relatively small amount of variance among the tasks; b) this factor of rationality is separate from fluid intelligence, but closely related to numeracy and dispositions of reflection and actively open-minded thinking; and c) consequently, our results add credence to the view that rationality should find its place in the taxonomy of cognitive abilities, at the same level as some other broad abilities such as fluid and crystallized intelligence.


Literature
"
Professor Kellan, the director of a teacher preparation program, was designing a new course in human development and needed to select a textbook for the new course. She had narrowed her decision down to one of two textbooks: one published by Pearson and the other published by McGraw. Professor Kellan belonged to several professional organizations that provided Web-based forums for its members to share information about curricular issues. Each of the forums had a textbook evaluation section, and the websites unanimously rated the McGraw textbook as the better choice in every category rated. Categories evaluated included quality of the writing, among others. Just before Professor Kellan was about to place the order for the McGraw book, however, she asked an experienced colleague for her opinion about the textbooks. Her colleague reported that she preferred the Pearson book. What do you think Professor Kellan should do? a. She should definitely use the Pearson textbook (1 point) b. She should probably use the Pearson textbook (2 points) c. She should probably use the McGraw textbook (3 points) d. She should definitely use the McGraw textbook" (4 points) Here preference for the McGraw textbook indicates a tendency to rely on the large-sample information in spite of a salient personal testimony. A preference for the Pearson textbook indicates reliance on the personal testimony over the large-sample information. Therefore, a larger preference for McGraw book was assigned with more points. A total score was calculated as an average of responses to all three items.


b) Fluid intelligence. We measured fluid intelligence with a 16 items version of the International Cognitive Ability Resource (ICAR; for details see icar-project.com and Condon and Revelle, 2014). ICAR is a broad cognitive ability assessment tool consisting of four different types of tasks: letters and numbers series, matrix reasoning items, verbal reasoning items and three-dimensional rotation items. The validation of this measure is reported in Condon and Revelle (2014). We formed the total score as an average of responses to these 16 items. c) Cognitive reflection test. Cognitive reflection test is test that originally (Frederick, 2005) consisted


h) Decision-outcome inventory. The Decision-Outcome Inventory was developed by Bruine de Bruin et al. (2007) as a measure of decision-making success in terms of avoiding negative decision outcomes.


Note. * p < .05; ** p < .01 RAT = Rationality factor; BBS = Belief bias syllogisms; BRN = Base-rate neglect; CBR = Causal base-rate; FCS = Four-card selection task; AVB = Availability bias; ATF = Attribute framing; OB = Outcome bias; SC = Sunk cost; GF = Gambler's fallacy; RIF = Risk framing; ICAR = Fluid intelligence; NU = Numeracy; CRT = Cognitive reflection test; AOT = Actively open-minded thinking; CON = Conspiracy thinking; SUP = Superstitious thinking; DOI = Decision outcome inventory.


Finally, in response d), an actor actively tries to find key counterarguments to his/her own views in order to balance the direction of arguments before deciding and potentially coming up with more objective view of the situation. The total score on this measure was the average of the scores on individual items and could thus be between one and four. Actively open-minded thinking scale. Actively open-minded thinking scale was somewhat different that the one in Study 1. Specifically, we removed some of the items and replaced them with the items from the currently recommended measure of actively open-minded thinking (http://www.sjdm.org/dmidi/Actively_Open-Minded_Thinking_Beliefs.html). For example, items "No one can discourage me from something I know is right" and "In general, I know everything I need to


= Actively open-minded thinking; AOT SJT = Actively open-minded thinking situational judgment test; DOI = Decision-Outcome Inventory; CWB = Counterproductive work behavior; PRDMQ -Peer-rated decision-making quality. Cohen's ds for cognitive bias tasks were calculated by the formula d = (M − μ) / SD, where M is the mean CB score and μ is normative value.


Note. * p < .05; ** p < .01 RAT = Rationality factor; BBS = Belief bias syllogisms; BRN = Base-rate neglect; CBR = Causal base-rate; FCS = Four-card selection task; ATF = Attribute framing; OB = Outcome bias; SC = Sunk cost; CRT = Cognitive reflection test; AOT = Actively open-minded thinking; SJT = Actively open-minded thinking situational judgment test; EXT = Extraversion; AGR = Agreeableness; CON = Conscientiousness; STA = Emotional stability; OPE = Openness; DOI = Decision outcome inventory; JS = Job satisfaction; CS = Career satisfaction; DMQ = Peer rated decision-making quality.


Numeracy (again captured with the Cognitive Reflection Test) and actively open-minded thinking were first order factors defined by their six and 13 manifest variables respectively (measurement model fits and outcomes of the regression analysis are described in the Supplement). Both numeracy and actively open-minded thinking were significant predictors of the rationality factor and together they explained 75% of the variance in the rationality factor. Dominance analysis showed that numeracy contributed by explaining 56% and actively open-minded thinking with additional 19% of the variance in Rationality factor. Other notable correlations of the rationality factor are with two of the personality traits, namely emotional stability and openness. Both of the traits are positively and moderately correlated with the rationality factor. We investigated whether any of these traits were able to account for additional variance in rationality beyond cognitive reflection and actively open-minded thinking by conducting two additional regression analysis, each with cognitive reflection, actively open-minded thinking and one of the personality traits as predictors. However, neither of the personality factors was significant predictor in the regression equations and the proportion of explained variance practically did not change after including these two traits as predictors. One plausible explanation is that emotional stability and openness are related with rationality because people higher on these two traits are better at actively open-minded thinking and perhaps more reflective, therefore more rational. This replicates the findings of Study 1 that cognitive capacities, especially numerical ability, as well as dispositions (actively open-minded thinking, reflection) make up rational thinking.


performance-based measures while actively open-minded thinking was measured with self-report scales, correlations this high are quite remarkable. Even a newly developed and short situational judgment measure of actively open-minded thinking exhibited a quite high correlation with the rationality score, especially when looking at disattenuated correlations. In fact, constructs of numeracy and actively open-minded thinking explained a majority of variance in the rationality factor (61% in Study 1 and 75% in Study 2), with numeracy apparently being somewhat more important predictor, as indicated by the dominance analyses. In addition to fluid intelligence, numeracy and actively openminded thinking, personality traits of emotional stability and openness were also relatively highly correlated with rationality. However, neither of these traits managed to explain additional portion of variance in the rationality score above what was explained by numeracy and actively open-minded thinking. Therefore, our findings paint a picture of rationality as a complex construct reflecting cognitive abilities, such as fluid intelligence and quantitative reasoning, as well as thinking dispositions, such as dispositions to be reflective (as opposed to being impulsive; Baron [2018] calls this disposition reflection/impulsivity or R/I) and actively open-minded. Putting these findings into a broader


Table 1 .
1
Descriptive statistics and reliabilities of the Study 1 measures with the effect sizes of biased responding in comparison with normative responding on cognitive bias tasks
Measure
M
SD
Min
Max Cronbach
ωh Normative
Cohen's
α
value
d
Belief-bias
0.74
0.21
0
1
.79
.77
1
1.20
Base-rate neglect
0.58
0.39
0
1
.93
.91
1
1.08
Causal base-rate
2.96
0.51
1.33
4
.55
.56
4
2.05
Four-card selection
0.34
0.33
0
1
.86
.80
1
2.00
Attribute framing
4.73
0.65
2.25
7.50
.35
.39
5
0.42
Outcome bias
4.34
0.94
0.25
6.75
.65
.59
5
0.70
Sunk cost
4.41
0.95
2
6
.56
.51
6
1.67
Availability bias
0.68
0.30
0
1
.77
.64
1
1.08
Gambler's fallacy
0.84
0.21
0
1
.76
.69
1
0.75
Risk framing
4.86
0.77
2.25
7.50
.24
.14
5
0.19
ICAR
0.67
0.18
0.1
1
.83
.61
-
-
Numeracy
0.42
0.27
0
1
.64
.61
-
-


Table 2 .
2
Correlations among the Study 1 variables. Raw correlations are above the diagonal while the disattenuated 1 correlations are below the diagonal.
RAT BBS BRN CBR FCS ATF OB
SC
AVB
GF
RIF
ICAR NU
CRT AOT CON SUP DOI
RAT
.62* .66*
.62* .47*
.31** .44** .25** .47** -.04
.15* .30** .38** .44** .40** -.10
-.23** -.09
BBS /
.27** .19** .23** .12
.20** .14* .11
-.06
.01
.26** .37** .41** .24** -.05
-.15* -.03
BRN /
.32
.27** .18** .14*
.17** .01
.19** .08
-.01
.19** .24** .28** .24** -.11
-.15* -.10
CBR /
.29
.38
.12
.13*
.15* .14* .23** -.09
.10
.12
.19** .24** .19** -.09
-.19** -.04
FCS /
.28
.20
.17
.04
.14* .05
.13*
-.11
.13* .19** .12
.20** .21** -.07
.05
-.10
ATF /
.23
.25
.30
.07
.05
.06
.08
.02
.12
.09
.13* .10
.13*
.03
-.07
.03
OB
/
.28
.22
.25
.19
.11
.06
.12
.09
.00
.09
.13* .15* .22** .01
-.04
-.01
SC
/
.21
.01
.25
.07
.14
.10
.10
.06
.05
.15*
.11
.21** .15*
-.01
-.07
.08
AVB /
.14
.22
.35
.16
.15
.17
.15
-.04
.04
.09
.14* .12
.22*
-.03
-.23** -.06
GF
/
-.08
.10
-.14
-.14
.04
.13
.09
-.05
.02
.00
-.07
-.04
.11
-.02
-.09
.05
RIF
/
.02
-.02
.27
.28
.41
.00
.14
.09
.05
.01
-.03
-.05
.13*
-.04
-.01
-.01
ICAR .46
.32
.22
.18
.22
.17
.12
.22
.11
.00
.02
.40** .44** .16*
.01
-.04
.03
NU
.67
.52
.31
.32
.16
.28
.20
.17
.20
-.10
-.05
.55
.46** .18** .01
-.12
.04
CRT .67
.51
.32
.36
.24
.19
.20
.31
.15
-.05
-.11
.53
.63
.17** -.03
-.09
.01
AOT .61
.29
.27
.28
.25
.22
.30
.22
.27
.14
.29
.19
.25
.20
-.26** -.22** -.10
CON -.15
-.06
-.12
-.13
-.08
.05
.01
-.01 -.04
-.02
-.09
.01
.01
-.04
-.30
.36** .23**
SUP -.35
-.19
-.17
-.28
.05
-.13
-.05
-.10 -.29
-.11
-.02
-.05
-.17
-.11
-.26
.43
.11
DOI -.15
-.04
-.13
-.07
-.13
.06
-.02
.13
-.08
.07
-.02
.04
.06
.01
-.13
.30
.13


Table 3 .
3
One factor model of CB tasks
Cognitive bias task
Loadings
Base-rate neglect
.51
Causal base-rate
.48
Belief bias
.48
Four-card selection
.36
Availability bias
.36
Outcome bias
.34
Attribute framing
.24
Sunk cost
.19
Risk framing
.12
Gambler's fallacy
-.03


). Only numeracy and actively open-minded thinking were significant predictors of rationality, explaining 61% of its variance, with unique effect of ICAR being virtually non-existent (description of model fits and beta ponders are shown in Supplement). This points to the conclusion that the constructs assessed by rationality measures are largely cognitive capacities and dispositions captured by the cognitive reflection test and actively open-minded thinking. To quantify the relative importance of numeracy and actively open-minded thinking for rationality obtained in the regression analysis, we conducted a dominance analysis


Study 1, Study 2 had several benefits. Probably the most important one relates to the sampleparticipants in our Study 2 were not college students but a community sample consisting of participants of different age and education. Second, we expanded the variety of variables for testing convergent and predictive validity of rationality factor(s). Specifically, we measured personality traits, as well as joband career satisfaction as potential additional real-life outcomes of good decisions. Third, we wanted to see whether greater rationality would be reflected in person's quality of decision-making as evaluated by his/her peers. Fourth, to additionally test the role of actively open-minded thinking in rationality, we developed and tested a new, more direct measure of actively open-minded thinking. Specifically, we developed a short construct-driven situational judgment test (SJT; Guenole, Chernyshenko, & Weekly, 2017; Lievens, 2017) of actively open-minded thinking. Finally, to further minimize the potential effects of memory on results in cases where the bias is measured by two parallel items (such as attribute framing and outcome bias), participants solved two parts of the questionnaire one week apart.However, Study 2 also had several weaknesses. One weakness of Study 2 is that, due to time constraints, we captured seven instead of ten CBs and we did not measure conspiracy and superstitious thinking. Furthermore, our research set did not include measures fluid intelligence or numeracy, but only the cognitive reflection test as the only cognitive ability measure. The reason for this was that these three measures were very highly correlated in our first study, as well as the findings from some
recent studies (e.g. Attali & Bar Hillel, 2020; Erceg, Galić & Ružojčić, 2020) showing that cognitive reflection test actually represents a fairly good numeracy test (and little beyond that).


Table 4 .
4
Descriptive statistics and reliabilities of the Study 2 measures with the effect sizes of biased responding in comparison with normative responding on cognitive bias tasks
Measure
M
SD
Min
Max Cronbach
ωh Normative
Cohen's
α
value
d
Belief-bias
0.43
0.35
0
1
.82
.64
1
1.64
Base-rate neglect
0.37
0.40
0
1
.95
.93
1
1.57
Causal base-rate
2.75
0.62
1
4
.48
.49
4
2.00
Four-card selection
0.16
0.24
0
1
.80
.77
1
3.56
Attribute framing
4.67
0.71
2.25
6.75
.17
.23
5
0.47
Outcome bias
3.78
1.11
1.25
7.25
.68
.66
5
1.09
Sunk cost
4.31
0.97
1.25
6
.39
.27
6
1.74
Cognitive reflection
0.51
0.34
0
1
.88
.82
-
-
AOT
4.58
0.88
1
6
.85
.75
-
-
AOT SJT
2.87
0.65
1
4
.23
.48
-
-
Extraversion
3.50
0.76
1.25
5
.76
.73
-
-
Agreeableness
3.92
0.64
2
5
.71
.56
-
-
Conscientiousness
3.73
0.73
1.75
5
.74
.70
-
-
Emotional stability
3.48
0.71
1.5
5
.74
.73
-
-
Openness
3.77
0.74
1.25
5
.75
.64
-
-
DOI
19.02
9.49
0.76
47.50
.62
.24
-
-
Job satisfaction
3.94
0.90
1
5
/
/
-
-
Career satisfaction
3.57
0.80
1.40
5
.82
.79
-
-


Table 6
6
. One-factor model of Study 2 CB tasks
Cognitive bias task
Loadings
Base-rate neglect
.62
Causal base-rate
.61
Belief bias
.52
Outcome bias
.44
Four-card selection
.43
Sunk cost
.38
Attribute framing
.13


who claimed that biased responding, i.e. departures from normative responding, is most often in one direction: searching too little for possibilities, evidence and goals and ignoring and discounting evidence against our favorite position. In order to combat these biases, he proposed the prescriptive model of actively open-minded thinking, defining the way people should form beliefs and make judgments and decisions so that their responses approximate normative ones as much as possible given cognitive and environmental limitations. Practically, in this framework, actively open-minded thinking is rational thinking. We believe that our results corroborate this idea that has at least two important implications. First, measuring rationality using CB tasks is not the only way and probably not even the best way of capturing rationality in thinking and decision making as the tasks are highly idiosyncratic and heterogeneous. Developing good indicators of real world actively open-minded thinking would perhaps


We disattenuated correlations based on the coefficient alpha calculated using the omega () function from psych R package(Revelle, 2021). This function allows for estimation of reliability based on polychoric correlations among the items, leading to less overcorrection.


However, it seems that these authors conducted a principal components analysis that inflates the communalities and the proportion of variance explained in comparison to factor analysis.














Measuring individual differences in decision biases: Methodological considerations




B
Aczel






B
Bago






A
Szollosi






A
Foldes






B
Lukacs








Frontiers in psychology




6


1770














Numeracy vs. intelligence: A model of the relationship between cognitive abilities and decision making




J
N
Allan










Norman, USA






University of Oklahoma






Master's thesis








The psychology of sunk cost




H
R
Arkes






C
Blumer








Organizational Behavior and Human Decision Processes




35


1
















Cognitive biases and decision support systems development: a design science approach




D
Arnott








Information Systems Journal




16


1
















The false allure of fast lures




Y
Attali






M
Bar-Hillel








Judgment & Decision Making




15


1
















The dominance analysis approach for comparing predictors in multiple regression




R
Azen






D
V
Budescu








Psychological Methods




8


2
















Base-rate respect: From ecological rationality to dual processes




A
K
Barbey






S
A
Sloman








Behavioral and Brain Sciences




30


3
















Rationality and intelligence




J
Baron








Cambridge University Press












Individual Mental Abilities vs. the World's Problems




J
Baron








Journal of Intelligence




6


2


23














Actively open-minded thinking in politics




J
Baron








Cognition




188
















Outcome bias in decision evaluation




J
Baron






J
C
Hershey








Journal of personality and social psychology




54


4
















The Measurement of Individual Differences in Cognitive Biases: A Review and Improvement




V
Berthet








Frontiers in psychology




12


419
















V
Berthet






V
De Gardelle




10.31219/osf.io/7wfvb




Measuring individual differences in cognitive biases: The Cognitive Bias Inventory
















General mental ability and decision-making competence: Theoretically distinct but empirically redundant




N
Blacksmith






T
S
Behrend






R
S
Dalal






T
L
Hayes








Personality and Individual Differences




138
















Measuring belief in conspiracy theories: The generic conspiracist beliefs scale




R
Brotherton






C
C
French






A
D
Pickering








Frontiers in psychology




4


279














Individual differences in adult decisionmaking competence




W
Bruine De Bruin






A
M
Parker






B
Fischhoff








Journal of personality and social psychology




92


5


938














Decision-making competence: More than intelligence?




W
Bruine De Bruin






A
M
Parker






B
Fischhoff








Current Directions in Psychological Science




29


2
















Dominance analysis: A new approach to the problem of relative importance of predictors in multiple regression




D
V
Budescu








Psychological Bulletin




114


3
















Does the cognitive reflection test measure cognitive reflection? A mathematical modeling approach




G
Campitelli






P
Gerrans








Memory & cognition




42


3
















Dimensions of decisionmaking: an evidence-based classification of heuristics and biases




A
Ceschi






A
Costantini






R
Sartori






J
Weller






A
Di Fabio








Personality and Individual Differences




146
















Decision making skill: From intelligence to numeracy and expertise




E
T
Cokely






A
Feltz






S
Ghazal






J
N
Allan






D
Petrova






R
Garcia-Retamero




K. A. Ericsson, R. R. Hoffman, A
















Cambridge handbook of expertise and expert performance


Kozbelt, & A. M. Williams




Cambridge University Press


New York, NY






2nd ed.








Measuring risk literacy: The Berlin Numeracy Test




E
T
Cokely






M
Galesic






E
Schulz






S
Ghazal






R
Garcia-Retamero








Judgment and Decision Making




7
















The International Cognitive Ability Resource: Development and initial validation of a public-domain measure




D
M
Condon






W
Revelle








Intelligence




43
















Construct validity in psychological tests




L
J
Cronbach






P
E
Meehl








Psychological bulletin




52


4
















Why should we try to think like scientists? Scientific reasoning and susceptibility to epistemically suspect beliefs and cognitive biases




V
Čavojová






J
Šrol






M
Jurkovič








Applied Cognitive Psychology




34


1
















Conflict monitoring in dual process theories of thinking




W
De Neys






T
Glumicic
























Cognition




106


3














Decision-making competence, executive functioning, and general cognitive abilities




Del
Missier






F
Mäntylä






T
De Bruin






W
B








Journal of Behavioral Decision Making




25


4
















The mini-IPIP scales: tiny-yeteffective measures of the Big Five factors of personality




M
B
Donnellan






F
L
Oswald






B
M
Baird






R
E
Lucas








Psychological assessment




18


2


192














Dysrationalia" Among University Students: The Role of Cognitive Abilities, Different Aspects of Rational Thought and Self-Control in Explaining Epistemically Suspect Beliefs




N
Erceg






Z
Galić






A
Bubić








Europe's journal of psychology




15


1


159














A reflection on cognitive reflection-testing convergent/divergent validity of two measures of cognitive reflection




N
Erceg






Z
Galić






M
Ružojčić








Judgment and Decision Making




15


5


741














Reflections on reflection: the nature and function of type 2 processes in dualprocess theories of reasoning




J
S B
Evans








Thinking & Reasoning




25


4
















The effects of statistical training on thinking about everyday problems




G
T
Fong






D
H
Krantz






R
E
Nisbett








Cognitive psychology




18


3
















Numeracy and risk literacy: What have we learned so far?




R
Garcia-Retamero






A
Sobkow






D
Petrova






D
Garrido






J
Traczyk




22.e10.Doi:10.1017/sjp.2019.16






The Spanish Journal of Psychology
















Predicting biases in very highly educated samples: Numeracy and metacognition




S
Ghazal






E
T
Cokely






R
Garcia-Retamero








Judgment and Decision Making




9


1
















Effects of race on organizational experiences, job performance evaluations, and career outcomes




J
H
Greenhaus






S
Parasuraman






W
M
Wormley








Academy of Management Journal




33
















On designing construct driven situational judgment tests: Some preliminary recommendations




N
Guenole






O
S
Chernyshenko






J
Weekly








International Journal of Testing




17


3
















On the psychology of prediction




D
Kahneman






A
Tversky








Psychological Review




80
















Judged frequency of lethal events




S
Lichtenstein






P
Slovic






B
Fischhoff






M
Layman






B
Combs








Journal of experimental psychology: Human learning and memory




4


6


551














Construct-driven SJTs: Toward an agenda for future research




F
Lievens








International Journal of Testing




17


3
















Confirmation bias: A ubiquitous phenomenon in many guises




R
S
Nickerson








Review of general psychology




2


2
















Prone to bias: Development of a bias taxonomy from an individual differences perspective




S
Oreg






M
Bayazit








Review of General Psychology




13


3
















How do people judge risks: availability heuristic, affect heuristic, or both?




T
Pachur






R
Hertwig






F
Steinmann








Journal of Experimental Psychology: Applied




18


3


314














Robustness of decision-making competence: Evidence from two measures and an 11-year longitudinal study




A
M
Parker






W
Bruine De Bruin






B
Fischhoff






J
Weller








Journal of behavioral decision making




31


3
















Decision-making competence: External validation through an individual-differences approach




A
M
Parker






B
Fischhoff








Journal of Behavioral Decision Making




18


1
















On the reception and detection of pseudo-profound bullshit




G
Pennycook






J
A
Cheyne






N
Barr






D
J
Koehler






J
A
Fugelsang








Judgment and Decision making




10


6
















Are we good at detecting conflict during reasoning?




G
Pennycook






J
A
Fugelsang






D
J
Koehler








Cognition




124


1
















Caution in the use of difference scores in consumer research




J
P
Peter






G
A
Churchill
Jr






T
J
Brown








Journal of consumer research




19


4
















The development and testing of a new version of the cognitive reflection test applying item response theory (IRT)




C
Primi






K
Morsanyi






F
Chiesi






M
A
Donati






J
Hamilton








Journal of Behavioral Decision Making




29


5
















psych: Procedures for Psychological, Psychometric, and Personality Research




W
Revelle










Evanston, Illinois






Northwestern University






R package version 2.0.8








lavaan: An R Package for Structural Equation Modeling




Y
Rosseel










Journal of Statistical Software




48


2
















Relation of conditional reasoning to heuristic processing




B
R
Slugoski






H
A
Shields






K
A
Dawson








Personality and Social Psychology Bulletin




19


2
















Are risk preferences consistent?: The influence of decision domain and personality




E
Soane






N
Chmiel








Personality and Individual Differences




38


8
















Multiple numeric competencies predict decision outcomes beyond fluid intelligence and cognitive reflection




A
Sobkow






A
Olszewska






J
Traczyk








Intelligence




80


101452


















L
Stankov








Journal of Intelligence




5


4


33














Rationality, intelligence, and levels of analysis in cognitive science: Is dysrationalia possible? In




K
E
Stanovich








What intelligence tests miss: The psychology of rational thought


J. St. B. T. Evans & K. Frankish


New Haven, CT; Oxford, England




Yale University Press










In two minds: Dual processes and beyond








On the distinction between rationality and intelligence: implications for understanding individual differences in reasoning




K
E
Stanovich








The Oxford handbook of thinking and reasoning


K. J. Holyoak & R. G. Morrison


New York




Oxford University Press
















Reasoning independently of prior belief and individual differences in actively open-minded thinking




K
E
Stanovich






R
F
West








Journal of experimental psychology: general


Stanovich, K. E., & West, R. F.




89


2


161








Journal of Educational Psychology








Individual differences in reasoning: Implications for the rationality debate




K
E
Stanovich






R
F
West








Behavioral and brain sciences






23














Natural myside bias is independent of cognitive ability. Thinking and Reasoning




K
E
Stanovich






R
F
West








13














On the relative independence of thinking biases and cognitive ability




K
E
Stanovich






R
F
West








Journal of personality and social psychology




94


4


672














The development of rational thought: A taxonomy of heuristics and biases




K
E
Stanovich






M
E
Toplak






R
F
West








Advances in child development and behavior




JAI




36














The rationality quotient: Toward a test of rational thinking




K
E
Stanovich






R
F
West






M
E
Toplak








MIT press












The domain specificity and generality of belief bias: Searching for a generalizable critical thinking skill




W
C
Sá






R
F
West






K
E
Stanovich








Journal of educational psychology




91


3


497














Individual differences in epistemically suspect beliefs: The role of susceptibility to cognitive biases




J
Šrol




10.31234/osf.io/4jcf7




















P
R
Teovanović




Sklonost kognitivnim pristrasnostima. Универзитет у Београду
















Individual differences in cognitive biases: Evidence against one-factor theory of rationality




P
Teovanović






G
Knežević






L
Stankov








Intelligence




50
















Investigating an alternate form of the cognitive reflection test




K
S
Thomson






D
M
Oppenheimer








Judgment and Decision making




11


1


99














The Cognitive Reflection Test as a predictor of performance on heuristics-and-biases tasks




M
E
Toplak






R
F
West






K
E
Stanovich








Memory & cognition




39


7


1275














Rational thinking and cognitive sophistication: Development, cognitive abilities, and thinking dispositions




M
E
Toplak






R
F
West






K
E
Stanovich








Developmental psychology




50


4


1037














Assessing miserly information processing: An expansion of the Cognitive Reflection Test




M
E
Toplak






R
F
West






K
E
Stanovich








Thinking & Reasoning




20


2
















Real-world correlates of performance on heuristics and biases tasks in a community sample




M
E
Toplak






R
F
West






K
E
Stanovich








Journal of Behavioral Decision Making




30


2
















A defense against the alleged unreliability of difference scores




D
Trafimow








Cogent Mathematics




2


1


1064626














Availability: A heuristic for judging frequency and probability




A
Tversky






D
Kahneman








Cognitive psychology




5


2
















Judgment under uncertainty: Heuristics and biases




A
Tversky






D
Kahneman








Science




185


4157
















The framing of decisions and the psychology of choice




A
Tversky






D
Kahneman








Science




211


4481
















Reasoning




P
C
Wason








New Horizons in psychology 1. Harmondsworth: Penguin


B. M. Foss,
















Reasoning about the rule




P
C
Wason








Quarterly Journal of Experimental Psychology




20
















Dimensions of judgment: Factor analysis of individual differences




E
A
Weaver






T
R
Stewart








Journal of Behavioral Decision Making




25


4
















Accounting for individual differences in decision-making competence: Personality and gender differences




J
Weller






A
Ceschi






L
Hirsch






R
Sartori






A
Costantini








Frontiers in psychology




9


2258














Preadolescent decision-making competence predicts interpersonal strengths and difficulties: A 2-year prospective study




J
A
Weller






M
Moholy






E
Bossard






I
P
Levin








Journal of Behavioral Decision Making




28


1
















Heuristics and biases as measures of critical thinking: Associations with cognitive ability and thinking dispositions




R
F
West






M
E
Toplak






K
E
Stanovich








Journal of educational psychology




100


4


930














Job satisfaction and organizational commitment as predictors of organizational citizenship and in-role behaviors




L
J
Williams






S
E
Anderson








Journal of management




17


3
















Individual differences in decision-making styles as predictors of good decision making (Doctoral dissertation




N
L
Wood












Bowling Green State University













"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]