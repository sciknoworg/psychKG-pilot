You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
We often use visual information to guide our behavioral decisions. The color 'red', for example, is used to decide whether to stop or go when driving an automobile, or similarly to select a ripe versus unripe tomato. These kinds of perceptual decisions require that we make a judgment about the identity of a stimulus based on the external sensory information that is available to us. All theories of perceptual decision-making postulate that this external sensory information is transformed into the internal evidence that is used to guide behavior. The hallmark of many theories -including signal detection theory (D. M. 
Green & Swets, 1966)
, the drift-diffusion model 
(Ratcliff, 1978)
, and ideal-observer models (e.g., 
Ma et al., 2006)
 -is the general notion that increasing the magnitude of the external feature leads to an increase in the strength of the internal evidence signal. However, the exact nature of this external-to-internal transformation can take many forms and remains unclear ( 
Figure 1
). 
Figure 1
. Hypothetical external-to-internal transformations. Theories of perceptual decisionmaking postulate that external sensory information is transformed into the internal evidence that is used to guide behavior, yet the nature of this external-to-internal transformation remains unknown. The linear model (red line) predicts a one-to-one relationship between the internal evidence signal and the external feature magnitude. The exponential model (green line) predicts that the strength of the internal evidence signal changes at an increasing rate as the external feature is changed. The quadratic (orange line) model predicts that the internal signal strength increases before peaking and then decreasing (or increasing). The logarithmic (purple line) model predicts that the internal signal strength increases by incrementally smaller amounts (and potentially decreases) as the external signal is increased.
Most psychophysics studies do not assume a priori the function that describes how external sensory features are transformed into internal evidence. Instead, the relationship between the manipulation of any given sensory feature and performance is characterized post hoc. Although no single function is likely to universally describe the external-to-internal transformation for all perceptual domains, there have been many attempts to identify one. For example, Fechner 
(1860)
 proposed that subjective perceptual experiences were logarithmic functions of the external stimulation 
(Figure 1
, purple line). Almost exactly one century later 
Stevens (1961)
 proposed that power functions ( 
Figure 1
, green and orange lines), rather than a logarithmic function, better describe the external-to-internal transformations of sensory systems (see also 
Naka & Rushton, 1966)
. Power functions can capture both linear relationships as well as saturation effects allowing this class of functions to flexibly fit a range of relationships between manipulations of sensory features and performance 
(Adler & Ma, 2018)
. Similarly, negative power functions do well at capturing how psychological similarity decreases exponentially as the external feature distance is increased 
(Shepard, 1987;
Sims, 2018)
. Measuring the similarity function for a given feature has been useful for improving models of many cognitive processes 
(Nosofsky, 1992)
, including recent advances in visual working memory performance 
(Schurgin et al., 2020)
. However, although it seems reasonable that any given function for one feature will not describe the external-to-internal transformation for another 
(Augustin, 2008)
, it seems equally reasonable to assume that a given transformation function would hold within that feature class. Still some effects, like the oblique effect in orientation 
(Appelle, 1972)
 and motion perception 
(Gros et al., 1998)
, are examples in which the effect of some sensory features vary nonlinearly and nonmonotonically across the sensory space 
(Li et al., 2003)
. This kind of variability in how a given sensory feature affects performance leaves it unclear whether a particular external-to-internal mapping will be universal across different task contexts even for highly similar decisions within the same feature class.
Models of perceptual decision making assume different external-to-internal transformations. A straightforward model assumption is that the decision variable is a function of measurement alone, for example the strongest sensory neuron response (i.e., winner-take-all, 
Lee et al., 1999)
. This class of models assumes that sensory features have a fixed mapping with the internal decision variable, but they are agnostic to sensory uncertainty 
(Adler & Ma, 2018)
.
Population coding models capture this sensory uncertainty by considering noise in sensory measurements across a population of neurons, yet the nature of the external-to-internal transformation depends on how the decision variable is computed from the sensory measurements. For example, a conventional population coding approach is to sum the logarithms of these sensory measurements to generate the log-likelihood function and then apply some algorithm to compute the decision variable. Taking the argmax of the likelihood function, a common approach which is akin to winner-take-all but in the likelihood space 
(Webb et al., 2007
(Webb et al., , 2010
, has been demonstrated to be equivalent to a linear transformation of sensory evidence into the decision variable 
(Jazayeri & Movshon, 2006)
. Other algorithms can be applied to the likelihood function to compute the decision variable, and any linear function applied to the decision variable would simply scale or shift it in the decision space, but any nonlinear function could also be applied, for example a quadratic function. Both the linear and quadratic external-to-internal transformations can approximate a Bayesian transformation despite not being based on the Bayesian decision variable 
(Adler & Ma, 2018)
. This Bayesian class of models has dominated recent theory development for characterizing how particular sensory features are transformed into decisions 
(Wohrer et al., 2013)
. Despite the success of many of these modeling approaches, the external-to-internal transformation is rarely established empirically beyond the scope of the model development. A general method for discovering such transformations is therefore needed.
Artificial neural networks (ANNs) are a powerful way to model human behavior 
(Doerig et al., 2023;
Kriegeskorte, 2015;
Ma & Peters, 2020)
. Modern deep-learning models share similarities with the connectionist networks of the 1980's and 1990's 
(Feldman & Ballard, 1982)
. However, deep-learning models differ in that they often have many layers in between the input and output layers, each of which applies a sequence of linear or nonlinear operations on the output from prior layers making them adept at approximating most functions. An example of deeplearning ANN that is commonly used to analyze visual stimuli is convolutional neural networks which utilize convolutional layers as feature detectors to integrate information across spatially localized regions of an image 
(Kheradpisheh et al., 2016)
. Although the ANN architectures vary (e.g., VGG, ResNet, CORnet), they are the leading class of models of the mechanisms underlying primate vision 
(Kubilius et al., 2019)
. In practice, ANNs are trained to classify large sets of visual stimuli until they can accurately predict a novel visual stimulus. This learning process makes them particularly useful for discovering how sensory features map onto internal evidence because the external-to-internal mapping is emergent.
Here we empirically test how a particular stimulus feature -orientation -is transformed into internal decision evidence, and further test whether this transformation holds for similar decisions with highly different task demands. Experiment 1 demonstrated that in a highcontrast fine-discrimination task 
(Figure 2A
), orientation is linearly transformed into internal evidence such that stimulus sensitivity is proportionate to the magnitude of the stimulus' tilt away from the decision boundary. However, Experiment 2 showed that a low-contrast coarsediscrimination task ( 
Figure 2B
) produces a very different relationship with orientation having little influence on the internal evidence. Critically, we validated that artificial neural networks (ANNs), trained on orientation categorization, mirrored the empirical results -fine-scale increments in tilt were linearly transformed into internal evidence, but coarse-scale increments in tilt had little influence on internal evidence -providing a framework for examining how the sensory stimuli map onto internal decisional evidence. The hidden-layer activations of the ANNs revealed that fine-scale increments in the magnitude of tilt results in greater discriminability between internal signal and noise distributions, but the degree of discriminability does not increase further after the magnitude of stimulus tilt becomes sufficiently large. These results suggest that external sensory information is transformed into internal decisional evidence in complex ways and suggest that ANNs could serve as a platform for understanding the mechanism underlying this critical transformation. 
Figure 2
. Tasks for Experiments 1 and 2. (A) Experiment 1. Subjects judged whether highcontrast Gabor patches were tilted clockwise or counterclockwise from a 45-degree cue. The Gabor patches varied in fine-scale increments from .4 to 2.4 degrees. (B) Experiment 2. Subjects judged whether noisy, low-contrast Gabor patches were tilted clockwise or counterclockwise from vertical. The Gabor patches varied in coarse-scale increments from 7 to 42 degrees. 


Methods


Participants
A set of 13 subjects were recruited for Experiment 1 and a separate set of subjects were recruited for Experiment 2 (25 subjects total). A power analysis was not conducted prior to data collection, and the data sample for each experiment was obtained out of convenience. Post hoc power analyses showed that sufficient power (>.9) was obtained in both experiments given the variance explained by the preferred models ( 2 > .76) and the number of subjects included in analyses (see Supplementary for details). All subjects had normal or corrected to normal vision.
One subject in Experiment 1 and one subject in Experiment 2 were excluded from analyses due to chance performance (50% accuracy across all conditions). All subjects provided informed consent and were compensated for their participation. All research methods and protocols were approved by the Georgia Institute of Technology Institutional Review Board and were performed in accordance with relevant guidelines and regulations.


Task and stimuli
In Experiment 1, subjects performed a fine-scale orientation categorization task in which a Gabor stimulus was tilted counterclockwise or clockwise of 45⁰ ( 
Figure 2A
). On each trial, subjects fixated on a small white dot presented at the display center. A visual cue consisting of a circle (diameter = 4.5⁰ visual angle) and a line (length = 4.5⁰ visual angle) oriented at 45⁰
was presented for 1000 ms. Following fixation and cueing, a Gabor patch (diameter = 4⁰ visual angle) with a frequency of 3 cycles per degree visual angle was presented for 100 ms at full contrast. Immediately after the stimulus presentation subjects provided their response by pressing "1" to respond "counterclockwise" or "2" to respond "clockwise". After a response was made the subject was given accuracy feedback for 500 ms.
In Experiment 2, subjects performed a coarse-scale orientation categorization task in which a Gabor stimulus was tilted left (counterclockwise) or right (clockwise) of vertical ( 
Figure 2B
). On each trial, subjects fixated on a small white dot presented at the display center for 500 ms.
Following fixation, a Gabor patch (diameter = 4⁰ visual angle) with a frequency of 3 cycles per degree visual angle was presented for 100 ms at 9% contrast embedded in random pixel noise at 90% contrast. Noise was included to prevent ceiling performance, and the level of noise was selected to keep performance in a range that is between chance level (50%) and perfect performance (100%). Immediately after the stimulus presentation, subjects provided their response by pressing "1" to respond "left" or "2" to respond "right". After a response was made the subject was given accuracy feedback for 500 ms. Responses were not speeded, and participants were given unlimited time to respond in both experiments. Participants were instructed to make accurate decisions in both experiments.


Procedure
In Experiment 1, Gabor stimuli were tilted away from 45⁰ and the magnitude of tilt was manipulated across six levels (.4⁰, .8⁰, 1.2⁰, 1.6⁰, 2⁰, and 2.4⁰) that were randomly chosen on each trial and counterbalanced across counterclockwise and clockwise directions. Prior to beginning the experimental runs, subjects received task instructions and then completed practice trials consisting of 20 easy trials with a 10⁰ tilt, 40 moderately difficult trials (10 each at 5⁰, 3⁰, 2⁰, and .5⁰), and finally trials with a 1.4⁰ tilt. Subjects completed four experimental runs each consisting of six 45-trial blocks for a total of 1,080 trials such that each tilt condition had an equal number of 180 trials.
In Experiment 2, Gabor stimuli were tilted away from vertical, and the magnitude of stimulus tilt was manipulated across five levels (7⁰, 14⁰, 21⁰, 28⁰, 35⁰, and 42⁰) which were randomly chosen on each trial and counterbalanced across left and right directions. Prior to beginning experimental runs, subjects received task instructions and then performed practice trials consisting of 20 easy trials with a 42⁰ tilt at 30% contrast, 40 moderately difficult trials at decreasing tilts 35⁰, 28⁰, 14⁰, and 7⁰ (10 trials each) and at 35%, 28%, 10% and .9% contrast respectively, and finally 30 trials with a 21⁰ tilt at decreasing contrasts (12%, 10%, and 9%, 10 trials each). Subjects completed four experimental runs each consisting of six 45-trial blocks for a total of 1,080 trials such that each tilt condition had an equal number of 180 trials.


Apparatus
Both Experiments 1 and 2 were designed in the MATLAB environment using Psychtoolbox 3 
(Brainard, 1997)
. Stimuli were presented on a 21.5-inch iMac monitor (1920 × 1080 pixel resolution, 60 Hz refresh rate) in a dark room. Subjects were seated 60 cm away from the display and provided their responses using a standard computer keyboard.


Analysis
To compare subjects' sensitivity to orientation across conditions, we computed d' using the standard formula (D. M. 
Green & Swets, 1966)
 by treating clockwise tilt trials as the target and calculating the hit rate (HR) and false alarm rate (FAR):
′ = Φ −1 ( ) − Φ −1 ( )
where Φ −1 is the inverse of the cumulative standard normal distribution that transforms the HR and FAR into z-scores. Sensitivity (d') was computed separately for each individual and each condition. This same approach was used to compute sensitivity (d´) from the HR and FAR produced by the ANN model.
We fit a series of random effects regression models to estimate the function that best describes how external sensory signal strength -the magnitude of stimulus tilt -maps onto internal evidence as measured with sensitivity (d') for each individual. A random effects model allows each individual to have their own unique regression relationship by estimating both the random slope and random intercept. A model which describes a linear relationship between the magnitude of stimulus tilt and sensitivity is given as:
′ = + +
where is the intercept, is the weight of the linear slope, and is random error.
Neither Experiment 1 nor 2 included a condition in which the stimulus was not tilted at all since such a condition would not have a correct decision. However, we assumed a hypothetical sensitivity (d') of zero by constraining the intercept to the origin. By fixing the intercept to be zero, the intercept term can be dropped, and the linear model can be reduced to:
′ = +
To account for potential nonlinear external-to-internal mappings, we fit several additional models. First, we fit a quadratic regression model whereby the effect of manipulating the magnitude of stimulus tilt on sensitivity (d') is given as:
′ = 1 + 2 2 +
We then fit a polynomial regression model by including an additional exponent term of the third degree whereby the effect of manipulating tilt magnitude on sensitivity is given as:
′ = 1 + 2 2 + 3 3 +
Finally, we fit a logarithmic model whereby the effect of manipulating tilt magnitude on sensitivity is given as:
′ = 1 + 2 +
To assess which model best describes the transformation of external sensory signals into internal evidence, we used Bayesian Information Criterion (BIC; 
Schwarz, 1978)
 to compare the relative fits of these regression models to each other. BIC penalizes model complexity by taking the product of the number of parameters and the natural log of the number of data points.
Similar results are found if models are compared using Akaike's information criterion (AIC; 
Akaike, 1973)
 which is more lenient on model complexity than BIC because the penalty term is a constant factor of two.


Artificial neural network (ANN) models
We built three simple feedforward ANN models ( 
Figure 3
) using the TensorFlow toolbox 
(Abadi et al., 2016)
. The models take as input a 100×100 image and output one of two category labels corresponding to clockwise or counterclockwise tilt. The models consist of one convolutional layer, one pooling layer, and between one and three fully connected hidden layers. The convolutional layer consists of a linear filter with equally sized receptive fields (3×3 pixels) and with equally spaced intervals followed by rectified linear unit (ReLU) activation function. This processing operation results in a 98×98 activation map encoding the response of the filter at each spatially localized region of an image. The output of the convolutional layer is pooled to reduce the size of the input volume by taking the maximum activation value of 2×2 spatially localized units resulting in a 49×49 output volume. This pooled result was then flattened and passed to the fully connected layers followed by a decision unit. The 5-layer model which, in total, consists of a convolutional layer (plus pooling layer), 3 hidden layers with 256, 128, and 64 units across descending layers, and 1 output layer, has a total of 656,139 trainable weights. In addition to this 5-layer model, a 4-layer and a 3-layer model were created by removing either the last fully connected layer or the last two fully connected layers for the purpose of examining the effect of model depth on performance. 
Figure 3
. Schematic of the ANN models. We tested a 3-layer, a 4-layer, and a 5-layer ANNs. All ANN models take as input a 100×100 image and outputs one of two category labels corresponding to counterclockwise (left) or clockwise (right) orientation. The models consist of one convolutional layer, one pooling layer, and between one and three fully connected layers. The convolutional layer consists of a set of one linear filter with equally sized receptive fields (e.g., 3×3 pixels) with equally spaced intervals followed by rectified linear unit (ReLu) activation. The output of the convolutional layer is pooled to reduce the size of the input volume by taking the maximum activation value of 2×2 spatially localized units. This pooled result was then flattened and passed to the fully connected layers followed by a decision unit. The graph depicts the 5-layer ANN.
The models were trained to categorize Gabor stimuli as tilted counterclockwise or clockwise of vertical. The total training set consisted of 10,000 stimuli. During training, the degree of tilt, contrast, and noise of each Gabor stimulus was randomly drawn from uniform distributions. The sampling distribution for tilt magnitude ranged from 0 to 60 degrees, for contrast from 1% to 90% (i.e., amplitude from .01 to .9) and for noise from 1% to 100%. was defined as 0 with black being -1 and white being 1. When combining pixel values for the Gabor and noise, it is possible that some pixels will exceed the intended range and result in a stimulus contrast greater than 100%, particularly when both contrast and noise are high. To avoid this issue, pixel values less than -1 were set equal to -1, and pixels greater than 1 were set equal to 1, ensuring that the total stimulus contrast was within the range of 0 to 100%. Learning was evaluated by testing the model on 1,000 novel Gabor stimuli generated from the same sampling distributions for tilt, contrast, and noise. To reduce the chance of idiosyncratic model behavior due to the random starting weights, we trained 30 model initializations of each model and averaged their results. The models were trained for 10 epochs.
The trained ANN was then tested on fine and coarse-scale orientation categorization tasks. For the fine-scale orientation categorization task, orientation offset was varied across 29 equally spaced levels ranging from 0 to 2.8 degrees. For the coarse-scale orientation categorization task, orientation offset was varied across 29 equally spaced levels ranging from 0 to 49 degrees. For both the fine-and coarse-scale tests of the ANN, the Gabor stimuli varied in contrast across five levels ranging from 2.5% to 8.5%. Each test stimulus was presented at 100% visual noise. The model was tested on 1,000 stimuli at each combination of tilt and stimulus category (clockwise or counterclockwise tilt), resulting in a total of 58,000 simulated trials for each task. To reduce the chance of idiosyncratic model behavior due to the random starting weights, we trained 30 model initializations of the model and averaged their results. We then analyzed the models' output layer activation in response to the Gabor stimuli for each orientation category and for each tilt magnitude condition 
(Shekhar & Rahnev, 2024)
. These activation distributions represent the internal evidence for each stimulus category (clockwise and counterclockwise) and provide a way to evaluate how the internal evidence of the ANN is affected by increasing the magnitude of tilt. We then quantified the discriminability of the two stimulus categories as the distance between the means of the internal evidence distributions, and we quantified their noisiness as the standard deviation of the two evidence distributions. The means and standard deviations of the evidence distributions were computed separately for each of the 30 instances of each model.


Data and code
The data and analyses code are available at https://osf.io/v6q3d/.


Results
Our goal was to examine how external sensory information is transformed into internal decisional evidence, validate ANN models against data from human subjects, and examine the ANNs' internal representations to assess the mechanism underlying this critical transformation.
In two behavioral experiments, we empirically tested how a particular stimulus featureorientation -is transformed into internal evidence. Subjects judged whether a Gabor stimulus was tilted clockwise or counterclockwise from a criterion. Across the two experiments we tested how orientation is transformed into internal evidence for similar decisions with different task contexts.


Behavioral results
In Experiment 1, subjects judged the orientation of a high-contrast Gabor that was tilted in finescale increments from .4 to 2.4 degrees away from a 45-degree cue 
(Figure 2A
). We found that sensitivity (d') appears to increase linearly as the magnitude of tilt offset was increased ( 
Figure    4A
). To examine the linearity of the function, we fit random effects regression models to characterize how sensitivity (d') varied as a function of tilt magnitude. We found that a slopeonly model with the intercept constrained to zero provides the best fit ( ′ = .72 ,  intercept constrained to the origin (red line) was the best performing model, suggesting a linear external-to-internal transformation. (C) Experiment 2 results. Sufficiently large increases in tilt magnitude had little effect on sensitivity. (D) Model comparison for Experiment 2. BIC suggested that the logarithmic model (purple line) was the best performing model, suggesting a highly non-linear transformation. Error bars in panels A and C show 95% confidence intervals; lines show best fitting functions.
Critically, we found a strikingly different pattern of results for Experiment 2 where subjects judged the orientation of a noisy, low-contrast Gabor in coarse-scale increments from 7 to 42 degrees away from vertical ( 
Figure 2B
). Instead of sensitivity (d') linearly increasing with the magnitude of tilt, we found that above 14 degrees tilt sensitivity no longer increased and appeared to slightly decrease ( 
Figure 4C
). This effect was not due to either floor or ceiling effects as the accuracy in the different conditions was in a range (69 − 79%) that is far from both chance level (50%) or perfect performance (100%).
In stark contrast with the results of Experiment 1, in Experiment 2, the linear model with the slope constrained to zero provided for a subjectively poor fit to the data ( 
Figure 4C
, red line).
Despite this poor fit to the data, the linear slope parameter of the constrained linear model is significant ( (55) = 7.62, < .001; model: ′ = .02 ). But including the intercept in the linear model ( ' = 1.37 + .007 , (54) = 8.64, < .001) provides for a better fit relative than the constrained linear model (∆ = 55.90, ∆ = 62.33) and demonstrates that the linear slope parameter is a poor predictor of the outcome ( (54) = 1.31, = .19). However, it is crucial to focus on a model in which the intercept is constrained to zero because, in theory,  
Figure    4D
) features a steep initial increase followed by a mostly flat but slightly decreasing portion ( 
Figure 4C
, purple line) because the linear term was estimated to be negative. However, a purely logarithmic model would not predict this slight decrease because the logarithm, for all bases, is a monotonically increasing function. A follow-up comparison revealed that the model which includes both the linear and logarithmic terms was preferred over a purely logarithmic model (∆ = 2.37, ∆ = 8.79; model: ' = .19 log( )) confirming that sensitivity begins to slightly decrease as the magnitude of tilt becomes sufficiently large. Taken together, these results reveal that, under the task context of Experiment 2, orientation is strongly nonlinearly transformed into internal decision evidence.


ANN results
The results from Experiments 1 and 2 show that intuitively similar external stimulus features can have different mappings to internal evidence depending on the task context and the range of stimulus features tested. While these results are likely to be explainable within different modeling frameworks, it is not clear whether any model frameworks would have predicted them a priori without additional assumptions about perceptual similarity functions. For example, an off-the-shelf probabilistic population coding model did not predict these results (see 
Supplementary)
, though it is of course likely that a better fit can be obtained if additional assumptions were included in the model. Instead of attempting to make such additional assumptions about the nature of the external-to-internal mapping, we use ANN models to further investigate the transformation of external sensory stimuli to internal evidence.
We first evaluated whether ANN models trained on orientation categorization would naturally reproduce the observed external-to-internal transformations found in Experiments 1 and 2 without any additional assumptions or training. We trained 3-, 4-, and 5-layer ANN models on discriminating between Gabor patches tilted clockwise or counterclockwise from vertical. To provide an unbiased training set, we trained the ANNs on a wide range of tilts (0 to 60 degrees) and contrasts (1 to 90%). We then tested the trained ANNs on Gabor stimuli at contrasts ranging from 2.5 to 8.5 and at tilt magnitudes that mimic Experiment 1 (fine-grained tilts up to 2.8 degrees) and Experiment 2 (coarse-grained tilts up to 49 degrees). We found that all three ANN models reproduced both the linear relationship between sensitivity and orientation for finegrained tilts that we observed in Experiment 1 ( 
Figure 5A
,C,E), and the non-linear, flat relationship between sensitivity and orientation for coarse-grained tilts that we observed in Experiment 2 ( 
Figure 5B,D,F)
. The depth of the model had no effect on the overall pattern as each model performed nearly equivalently even though a different set of stimuli were used to test each model and model instance. Nevertheless, unlike the human data in Experiment 2 where d' peaks around 14 degrees, the ANN models have a maximum d' around 7 degrees (see Discussion). Overall, these results suggest that the ANN model shows very similar emergent behavior to what we see in humans for the transformation from external sensory features to internal decision evidence, and that this general pattern of emergent behavior is robust to manipulations of stimulus contrast. transformed into internal evidence 
(B,D,F)
. The performance of the 3-layer (A,B), 4-layer (C,D) and 5-layer (E,F) models was nearly identical. For both tasks, increasing the stimulus contrast had a scaling-up effect on sensitivity but had little effect on the general trend in the relationship between sensitivity and tilt magnitude. The shaded regions show 95% confidence intervals. This pattern of results suggest that the ANN models show very similar emergent behavior to what we see in humans for the transformation from external sensory features to internal decision evidence. To understand the ANNs behavior, we examined the internal activations of the models. Here we focus our analysis on the internal behavior of the 3-layer ANN models because performance was unaffected by network depth and because it is the simplest of the three ANN models. Specifically, we analyzed the models' output layer activation in response to the Gabor stimuli for each orientation category and for each tilt magnitude condition, providing a way to evaluate how the internal evidence of the ANNs is affected by increasing the magnitude of tilt.
We generated the internal evidence distributions for each orientation category for each tilt magnitude condition. We found that fine scale increases in tilt magnitude resulted in systematic differences between the evidence distributions for clockwise and counterclockwise stimulus categories. In particular, larger tilt offsets shifted the means of the distributions away from zero and away from each other but did not change the spread of evidence within each distribution ( 
Figure 6A
). Indeed, tilt magnitude had a large effect on the means of the distributions ( 
Figure    6B
, (28, 812) = 1795, < .001), but no effect on the standard deviation of the distributions for either stimulus category ( 
Figure 6C
, ≤ 1.48, ′ ≥ .05). This systematic increase in the distance between the two evidence distributions with no change in standard deviation provides a causal explanation for why the ANN's sensitivity (d') increased linearly with increases in tilt magnitude. 
Figure 6
. ANN activations from small increases in tilt magnitude. A) Small, fine-scale increments in tilt magnitude result in systematic differences between the distributions of ANN activations in response to stimuli from each stimulus category (counterclockwise and clockwise). B) The distribution means increase linearly. C) However, the standard deviations of these distributions were entirely unaffected by tilt magnitude. The shaded regions in B and C show 95% confidence intervals.
We conducted the same analyses using tilt offset across a larger range, thus mimicking the design of Experiment 2. We found that the coarse-scale manipulation of tilt magnitude had a significant but more complex effect on the means of the evidence distributions ( (28,812) = 577.6, < .001; 
Figure 7A
), but again had no influence on the standard deviation of the distributions ( 
Figure 7C
, ≤ 1.48, ′ > .05). In particular, at smaller tilt magnitudes between A B C 0 and degrees, the difference between the means of the evidence distributions systematically increased before peaking around 6.5 degrees, decreasing, and then plateauing between and 49 degrees ( 
Figure 6B
). Taken together, these results suggest that the highly nonlinear relationship between orientation and sensitivity (d') is driven by complex changes in the means of the internal evidence distributions but not the standard deviation of these distributions. 
Figure 7
. ANN activations from large increases in tilt magnitude. A) Large, coarse-scale increments in tilt magnitude result in systematic nonlinear differences between the distributions of ANN activations in response to stimuli from each stimulus category (counterclockwise and clockwise). C) The distribution means initially increase systematically before decreasing, plateauing, and then decreasing again. D) The standard deviations of these distributions were unaffected by tilt magnitude. The shaded regions in B and C show 95% confidence intervals.


A B C


Discussion
We examined how external sensory information is transformed into internal decisional evidence across different task contexts. In Experiment 1, increasing the tilt of a high-contrast Gabor in fine-scale increments away from 45 degrees resulted in a linear increase in sensitivity (d'),
suggesting a linear transformation from orientation to internal evidence strength. In contrast, in Experiment 2, increasing the tilt of a noisy, low-contrast Gabor in coarse-scale increments away from vertical resulted in a fast initial gain in sensitivity followed by a plateau with a slight decrease, suggesting a highly non-linear relationship between orientation and internal evidence strength. These results suggest that external sensory information is transformed into internal decisional evidence in complex ways that may not be clear from a snapshot of results obtained from a given experimental task. Critically, several artificial neural networks (ANNs) trained on orientation categorization naturally reproduced this general pattern of results. Mechanistically, we discovered that when the stimulus tilt was increased in fine-scale increments, the ANNs'
internal distributions of evidence for each stimulus condition became more discriminable, but the degree of discriminability stopped increasing after a sufficiently large and coarse increases in tilt magnitude. Taken together, these results begin to reveal how external sensory information is transformed into the internal evidence that is used to make decisions and suggest that ANNs could serve as a platform for understanding the mechanism underlying this critical transformation.


Dissociation between tasks using fine-and coarse-scale stimuli
The general intuition of most theories of perceptual decision-making is that increasing the strength of a given external signal leads to a graded increase in the strength of the internal signal (e.g., 
Green & Swets, 1966;
Ma et al., 2006;
Ratcliff, 1978)
. In contrast with this intuition, we show that in the task with coarse-scale stimuli (Experiment 2) a large increase of the external signal does not translate into a monotonic one-to-one increase in the internal signal.
Crucially, this pattern does not appear to reflect a ceiling or a floor effect because performance saturates in a range between 69 and 79% accuracy. What explains this pattern of results? There appear to be at least three different explanations.
First, it could be that the results are explained by the fact that the optimal readout mechanisms differ between the fine-and coarse-scale discrimination tasks. Fine-scale tasks require a subject
to discriminate between signals that are nearby in the stimulus space, whereas coarse-scale tasks require a subject to discriminate between far apart signals. Performance differences across fine-and coarse-scale tasks have been found to be a function of the activity of featureselective neurons 
(Britten et al., 1992;
Celebrini & Newsome, 1995;
Salzman et al., 1990
Salzman et al., , 1992
.
Optimal performance depends on the readout mechanisms from these neurons -for fine-scale tasks, similar stimulus features activate roughly the same population of feature selective neurons and so the most informative neurons are those tuned slightly away from the feature to be discriminated 
(Jazayeri & Movshon, 2007;
Scolari & Serences, 2010;
Verghese et al., 2012)
 and may involve high-precision selection of the optimal neurons 
(Purushothaman & Bradley, 2005)
. For coarse-scale tasks, the most informative neurons are those tuned to the to-bediscriminated feature because the neurons which are selective for the competing stimulus category are less likely to be active, and as such all of the active neurons can be pooled for behavior 
(Shadlen & Newsome, 1996)
. It could be that similar mechanisms are involved in fine and coarse orientation categorization tasks whereby sufficient increases in the magnitude of tilt changes the informative value of responding neurons and optimal performance requires transitioning from a fine-scale scheme to a coarse-scale scheme. However, whereas these different decision mechanisms are likely responsible for some aspects of the differing patterns of performance across the fine-and coarse-scale tasks, they do not directly explain why performance plateaus, and even slightly decreases, in the coarse-scale task.
Second, it could be that the coarse-scale task involves threshold mechanisms which are not present in the fine-scale tasks. Specifically, it may be that in the coarse-scale task, which involves noisy Gabor patches, there is some intensity of the external feature needed to perceive the stimulus 
(Rouder & Morey, 2009)
. Crucially, in the coarse-scale task, although orientation sensitivity was expected to increase as the tilt magnitude was increased across conditions with a fixed level of noise, akin to increasing the signal-to-noise ratio (SNR), we found that a sufficiently large increase in tilt had no additional effect on sensitivity. In other words, this flat performance suggests that the high amount of visual noise (90%) used in the coarse discrimination task may have functioned as a threshold on identifying the orientation of the Gabor whereby the observer either perceives the orientation signal and can easily categorize whether it is tilted left or right, or the observer fails to perceive the orientation signal and cannot identify the orientation at all regardless of the magnitude of the tilt. This explanation is in line with recent work demonstrating that different mechanisms can result in either graded or all-or-none perception even for highly similar visual stimuli (M. L. 
Green & Pratte, 2022)
.
Third, rather than stemming from different decisional or threshold mechanisms, it may simply be that any external feature is transformed into internal evidence in complex ways that depend on a host of factors and are difficult to intuit. Many studies suggest that sensitivity to sensory stimuli follows a logarithmic or power law function 
(Augustin, 2008;
Naka & Rushton, 1966;
Stevens, 1961)
, not only for orientation but for various feature magnitudes 
(Dotan & Dehaene, 2016
). Although we see a very different pattern of results across Experiments 1 and 2, it could be that the pattern across these two experiments is explained by an underlying relationship between perceptual similarity and feature distance which may follow the Weber-Fechner law. A small increase in the difference between the feature and the decision boundary (e.g., vertical) may initially result in an increase in perceptual sensitivity for detecting that difference, but with little-to-no increase in sensitivity for detecting sufficiently large feature distances. Given that any of the stimulus magnitudes sampled within the fine-grained task ( 
Figure 4A
) could be sampled within the coarse-grained task ( 
Figure 4C
), it is plausible that the observed sensitivity would fall in the approximately linear part of the best-fitting model in Experiment 2. Instead of attempting to intuit the relationship between the external feature and internal evidence, our goal here was to train an ANN on the orientation task, validate the network's emergent behavioral performance against the empirical data, and directly evaluate the network's internal evidence. The empirical data here showed a downward trend in sensitivity in the coarse scale task, which is not predicted by the purely logarithmic Weber-Fechner function. However, the ANN results also showed this downward trend and suggests that the external-to-internal mapping is indeed more complex than a purely logarithmic transformation.


Using ANNs as hypothesis-generation platforms
One of the big promises of ANN models is that they can function as increasingly more appropriate models of the human visual system 
(Doerig et al., 2023;
Kriegeskorte, 2015)
. It is clear that current versions of these models differ from human visual perception in many ways 
(Bowers et al., 2022)
, which is not surprising given the vast differences between brains and ANNs in both architecture and training. Nevertheless, despite these vast differences, many similarities between ANNs and brains have also been reported 
(Kheradpisheh et al., 2016;
Kubilius et al., 2019)
. The existence of these similarities suggests that ANNs may sometimes be useful for generating hypotheses about patterns of behavior across stimulus and task conditions even without aligning their architecture or training with that of human brains. The reasoning here is that some tasks may involve built-in constraints, such that most systems that learn to complete the task, regardless of their details, will also exhibit the same dependencies. We believe that this may be why the simple ANN used here was able to reproduce, out of the box, the complex qualitative pattern in human data despite this ANN being so different from human brains. If so, many ANNs may already be useful as hypothesis-generation platforms, at least in the specific cases where they are trained and tested on the same specific task performed by human subjects. On the other hand, when ANNs are trained on one task/dimension and tested on a different task/dimension, there is little reason to believe that they will behave similarly to humans.
Critically, we investigated whether an ANN could reproduce our external-to-internal mapping results. We found that an ANN trained on orientation discrimination mirrored the observed pattern of results -fine-scale increments in the magnitude of stimulus tilt were linearly transformed into internal evidence, but coarse-scale increments in tilt had little influence on internal evidence. These results show that a given sensory feature may not have a monotonic one-to-one mapping with the internal representation of evidence across different tasks.
Critically, our findings suggest that ANNs could serve as a platform for evaluating this critical external-to-internal transformation, and possibly generate new hypotheses such that one can examine their behavior in detail, generate novel hypotheses, and then test them in human subjects.


Using ANNs as platforms for evaluating the external-to-internal mapping
A fundamental goal in the study of perceptual decision-making is to understand how external sensory information is transformed into the internal evidence that is used to guide behavior.
However, the nature of this external-to-internal transformation is generally unknown. In practice, this transformation is inferred by characterizing the relationship between external sensory features and behavioral decisions. Here we tested how a very simple visual feature, orientation, maps onto internal evidence in the context of a single class of stimuli (Gabor patches). The simplicity of this task allowed us to build relatively small and shallow ANNs. In fact, the simplicity of the task makes it superfluous to employ a deep network, such as the ones in most contemporary deep-learning models (e.g., VGG or ResNet). Indeed, we found that the 3-, 4-, and 5-layer ANNs equally mirrored human performance without additional assumptions or special training. However, more complex features, such as ones that allow for image classification or person recognition, will certainly require deeper and more complex networks.
Thus, the network chosen as a platform for studying the external-to-internal transformation should have complexity commensurate to the task at hand.


Conclusion
Whereas previous work has shown that the external-to-internal mapping often varies from one visual domain to another, here we show that the mapping varies drastically across tasks within a visual domain. We further demonstrated that shallow ANNs, trained on the orientation discrimination task, mirrored the pattern of results observed in human subjects without any additional assumptions or training. Taken together, these results begin to reveal how external sensory information is mapped onto internal decisional evidence. Critically, our findings suggest that artificial neural networks could serve as a powerful tool for the arduous work of building a theory of this critical external-to-internal transformation.
= 7.05, < .001). A linear model that includes the intercept did not meaningfully improve the model fit (∆ = 16.35, ∆ = 9.28; model: ' = −.03 + .74 ) because, although the linear slope parameter was significant ( (64) = 6.73, < .001), the intercept was not ( (64) = −.47, = .64). Similarly, neither the inclusion of a quadratic term (∆ = 16.31, = 9.25, (64) = −.29, = .77; Figure 4B), third-degree polynomial term (∆ = 25.07, ∆ = 8.57, (63) = −1.86, = .07), nor logarithmic term (∆ = 15.98, ∆ =8.91, (64) = .70, = .48) outperformed the constrained linear model. Although each of the models can reasonably capture the data, suggesting that these models are not well differentiated by the fine-scale task, the linear model with the intercept constrained to zero provides the most parsimonious fit to the data and including additional nonlinear parameters did not improve the model. Altogether, this pattern of results suggests that orientation is linearly transformed into internal decision evidence in a fine-scale discrimination task.


Figure 4 .
4
ehavioral results. (A) Experiment 1 results. Increasing the magnitude of tilt in finescale increments results in a linear increase in sensitivity (d'). (B) Model comparison for Experiment 1. A model comparison using BIC demonstrates that the linear model with the


sensitivity (d') should approach zero as the external signal becomes sufficiently weak. The full model comparison analysis revealed that the logarithmic model ( ' = −.03 + .77 log( )) outperformed the slope-only linear model (Both the linear term (t(54) = −4.62, < .001) and logarithmic term (t(54) = 9.25, < .001) of the logarithmic model were significant predictors of the outcome variable. The preferred logarithmic model (


Figure 5 .
5
Artificial neural network (ANN) results. The ANNs results (lines) reproduced the empirical results from Experiment 1 and Experiment 2 -fine-scale increments in the tilt magnitude from 0 to 2.8 degrees were linearly transformed into internal evidence (A,C,E), but coarse-scale increments in the tilt magnitude up to 49 degrees were nonlinearly
















M
Abadi






A
Agarwal






P
Barham






E
Brevdo






Z
Chen






C
Citro






G
S
Corrado






A
Davis






J
Dean






M
Devin






S
Ghemawat






I
Goodfellow






A
Harp






G
Irving






M
Isard






Y
Jia






R
Jozefowicz






L
Kaiser






M
Kudlur






X
Zheng




arXiv:1603.04467


TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems










arXiv Preprint








Comparing Bayesian and non-Bayesian accounts of human confidence reports




W
T
Adler






W
J
Ma




10.1371/journal.pcbi.1006572








PLOS Computational Biology




14


11














Information theory and an extension of the maximum likelihood principle




H
Akaike








Second International Symposium on Information Technology


















Perception and discrimination as a function of stimulus orientation: The "oblique effect" in man and animals




S
Appelle








Psychological Bulletin




78


4


















10.1037/h0033117














Stevens' power law and the problem of meaningfulness




T
Augustin




10.1016/j.actpsy.2007.12.005








Acta Psychologica




128


1




















J
S
Bowers






G
Malhotra






M
Dujmović






M
L
Montero






C
Tsvetkov






V
Biscione






G
Puebla






F
Adolfi






J
E
Hummel






R
F
Heaton






B
D
Evans






J
Mitchell






R
Blything


















Deep Problems with Neural Network Models of Human Vision


10.1017/S0140525X22002813








Behavioral and Brain Sciences
















The Psychophysics Toolbox




D
Brainard








Spatial Vision




10


4
















The analysis of visual motion: A comparison of neuronal and psychophysical performance




K
Britten






M
Shadlen






W
Newsome






J
Movshon




10.1523/JNEUROSCI.12-12-04745.1992








The Journal of Neuroscience




12


12
















Microstimulation of extrastriate area MST influences performance on a direction discrimination task




S
Celebrini






W
T
Newsome




10.1152/jn.1995.73.2.437








Journal of Neurophysiology




73


2
















The neuroconnectionist research programme




A
Doerig






R
P
Sommers






K
Seeliger






B
Richards






J
Ismael






G
W
Lindsay






K
P
Kording






T
Konkle






M
A J
Van Gerven






N
Kriegeskorte






T
C
Kietzmann








Nature Reviews Neuroscience




24


7


















10.1038/s41583-023-00705-w














On the origins of logarithmic number-to-position mapping




D
Dotan






S
Dehaene




10.1037/rev0000038








Psychological Review




123


6
















Elemente der Psychophysik (Reissued 1964 by Bonset




G
T
Fechner








Leipsiz: Breitkopf & Hartel










Amsterdam ed.








Connectionist Models and Their Properties




J
A
Feldman






D
H
Ballard




10.1207/s15516709cog0603_1








Cognitive Science




6


3


















D
M
Green






J
A
Swets




Signal detection theory and psychophysics




Wiley




1












Local motion pooling is continuous, global motion perception is discrete




M
L
Green






M
S
Pratte




10.1037/xhp0000971








Journal of Experimental Psychology: Human Perception and Performance




48


1
















Anisotropies in visual motion perception: A fresh look




B
L
Gros






R
Blake






E
Hiris




10.1364/JOSAA.15.002003








Journal of the Optical Society of America A




15


8














Optimal representation of sensory information by neural populations




M
Jazayeri






J
A
Movshon




10.1038/nn1691








Nature Neuroscience




9


5
















A new perceptual illusion reveals mechanisms of sensory decoding




M
Jazayeri






J
A
Movshon




10.1038/nature05739








Nature




446


7138
















Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition




S
R
Kheradpisheh






M
Ghodrati






M
Ganjtabesh






T
Masquelier




10.1038/srep32672








Scientific Reports




6


1


32672














Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing




N
Kriegeskorte




10.1146/annurev-vision-082114-035447








Annual Review of Vision Science




1


1




















J
Kubilius






M
Schrimpf






K
Kar






R
Rajalingham






H
Hong






N
Majaj






E
Issa






P
Bashivan






J
Prescott-Roy






K
Schmidt






A
Nayebi






D
Bear






D
L
Yamins






J
J
Dicarlo


















Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs






Advances in Neural Information Processing Systems




32












Attention activates winner-take-all competition among visual filters




D
K
Lee






L
Itti






C
Koch






J
Braun




10.1038/7286








Nature Neuroscience




2


4
















Oblique Effect: A Neural Basis in the Visual Cortex




B
Li






M
R
Peterson






R
D
Freeman








Journal of Neurophysiology




90


1


















10.1152/jn.00954.2002














Bayesian inference with probabilistic population codes




W
J
Ma






J
M
Beck






P
E
Latham






A
Pouget








Nature Neuroscience




9


11


















10.1038/nn1790














A neural network walks into a lab




W
J
Ma






B
Peters




arXiv:2005.02181








Towards using deep nets as models for human behavior
















S-potentials from luminosity units in the retina of fish (Cyprinidae)




K
I
Naka






W
A H
Rushton








The Journal of Physiology




185


3


















10.1113/jphysiol.1966.sp008003














Similarity Scaling and Cognitive Process Models




R
M
Nosofsky








Annual Review of Psychology




43
















Biological constraints on neural network models of cognitive function




F
Pulvermüller






R
Tomasello






M
R
Henningsen-Schomers






T
Wennekers




10.1038/s41583-021-00473-5








Nature Reviews Neuroscience




22


8
















Neural population code for fine perceptual decisions in area MT




G
Purushothaman






D
C
Bradley








Nature Neuroscience




8


1


















10.1038/nn1373














A Theory of Memory Retrieval




R
Ratcliff




10.1037/0033-295X.85.2.59








Psychological Review




85


2
















The nature of psychological thresholds




J
N
Rouder






R
D
Morey




10.1037/a0016413








Psychological Review




116


3
















Cortical microstimulation influences perceptual judgements of motion direction




C
Salzman






K
H
Britten






W
T
Newsome








Nature




346


6280


















10.1038/346174a0














Microstimulation in visual area MT: Effects on direction discrimination performance




C
Salzman






C
Murasugi






K
Britten






W
Newsome




10.1523/JNEUROSCI.12-06-02331.1992








The Journal of Neuroscience




12


6
















No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit




R
Schaeffer






M
Khona






I
R
Fiete




10.1101/2022.08.07.503109


















Psychophysical scaling reveals a unified theory of visual memory strength




M
W
Schurgin






J
T
Wixted






T
F
Brady








Nature Human Behaviour




4


11


















10.1038/s41562-020-00938-0














Estimating the dimension of a model




G
Schwarz








The Annals of Statistics




6


2
















Basing Perceptual Decisions on the Most Informative Sensory Neurons




M
Scolari






J
T
Serences








Journal of Neurophysiology




104


4


















10.1152/jn.00273.2010














Motion perception: Seeing and deciding




M
N
Shadlen






W
T
Newsome








Proceedings of the National Academy of Sciences




93


2


















10.1073/pnas.93.2.628














Human-like dissociations between confidence and accuracy in convolutional neural networks




M
Shekhar






D
Rahnev




10.1101/2024.02.01.578187


















Toward a Universal Law of Generalization for Psychological Science




R
N
Shepard




10.1126/science.3629243








Science




237


4820
















Efficient coding explains the universal law of generalization in human perception




C
R
Sims




10.1126/science.aaq1118








Science




360


6389
















To Honor Fechner and Repeal His Law: A power function, not a log function, describes the operating characteristic of a sensory system




S
S
Stevens




10.1126/science.133.3446.80








Science




133


3446
















Attention Selects Informative Neural Populations in Human V1




P
Verghese






Y.-J
Kim






A
R
Wade








The Journal of Neuroscience




32


46


















10.1523/JNEUROSCI.1174-12.2012














Cortical pooling algorithms for judging global motion direction




B
S
Webb






T
Ledgeway






P
V
Mcgraw




10.1073/pnas.0611288104








Proceedings of the National Academy of Sciences




104


9
















Relating spatial and temporal orientation pooling to population decoding solutions in human vision




B
S
Webb






T
Ledgeway






P
V
Mcgraw




10.1016/j.visres.2010.04.019








Vision Research




50


22
















Population-wide distributions of neural activity during perceptual decision-making




A
Wohrer






M
D
Humphries






C
K
Machens








Progress in Neurobiology




103


















10.1016/j.pneurobio.2012.09.004















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]