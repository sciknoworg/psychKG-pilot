You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



 
Isager, van 't Veer, and Lakens (2021)
 
[IVL21] put forth a proposal to quantify replication value. Replication value, denoted as RVCN, is calculated as the product of the average yearly citation count of a given article in which the original effect was reported (value) and the sample size used to investigate the original study 
(uncertainty)
. The authors further suggest that replication targets should be identified using a four-step selection procedure including: (1) curation of an initial set of candidate studies, (2) calculation of RVCN for the candidate studies, (3) in-depth evaluation of the subset of studies with the highest RVCN, and (4) selection of the most suitable candidate based on RVCN and the in-depth evaluation.
We agree with the notion that replication value is best thought of as a function of value and uncertainty. However, we view the proposal of IVL21 as a starting point for a much-needed expert discussion about the conceptualization and potential quantification of replication value as well as other assessment strategies for replication targets. In this commentary, we identify what we believe to be some issues with RVCN before comparing it to our own selection procedures 
(Field et al., 2019;
Pittelkow et al., 2021
Pittelkow et al., , 2023
.


RVCN fails to capture the complexity of replication target selection in practice
Our primary point of critique is that the conceptualization of what makes up value and uncertainty, operationalized by just one measure each, oversimplifies the multifaceted approach typically employed in practice for choosing replication targets. Results from surveying researchers who have conducted, or plan to conduct a replication suggest that they typically consider additional aspects such as feasibility and methodology when deciding what to replicate 
(Pittelkow et al., 2023)
. Feasibility, while not related to the concepts of value and uncertainty, is considered very important by replicating authors because replication is only possible when the necessary resources (e.g., money, time, staff, expertise) are available. (In-)appropriate methodology informs both value and uncertainty. Although IVL21 refer to additional qualitative assessments for studies with the largest RVCN, we argue that it is premature to speak of the formula as assessing replication value without including these important additional concepts.


It is unclear whether RVCN is prescriptive or descriptive
An overarching question central to the utility of measures such as RVCN is whether it is meant to be prescriptive (what ought to be selected) or descriptive (what is selected in practice). The first half of the article by IVL21 suggests RVCN is prescriptive, as the aim of the article is to develop a measurement model of a study selection metric aiming to maximize the expected utility gain of a single replication. From the section "Preliminary validation […] studies" on, the distinction between RVCN as a prescriptive and descriptive measure becomes murkier. In this section, the authors propose validating the RVCN metric by correlating actual study selection behaviour with what RVCN suggests researchers should have selected. In our assessment, this correlation only makes sense when evaluating RVCN as a descriptive measure. If researchers already select studies with the largest utility gain for replication, there would be no need for a measure like RVCN. Our own work suggests that replication study selection in practice is often content-driven (e.g., based on interest) and not utility-based 
(Pittelkow et al., 2023)
. Thus, we argue that the preliminary validation strategy of correlating RVCN to actual replication behaviour is unlikely to be particularly informative.


Indicators depend on their specification
Citation count is an imperfect indicator of scientific value. As the authors mention in 
Figure 1
, there are many reasons why articles might get cited that have little to do with the scientific value of an article, such as studies which have had a lot of social media attention for being novel or controversial (and not necessarily reliable or valid). Examples include scandals surrounding the authors, methodological flaws, or citation practices (self-citations by authors or by journals, and citation bias). Citation counts also vary considerably between scientific subfields 
(Patience et al., 2017)
 making RVCN unsuitable for assessments or comparisons across fields.
Every metric has faults, but accepting the premise that citation count might possess some utility as a metric for scientific value, we turn to the specific version the authors use in their preliminary validation, which closely follows the predicted citation count of articles as described in 
Figure 2A
 of IVL21 (i.e., a uniform distribution). The authors acknowledge that the actual trajectory of citations through time might be better approximated by 
Figure 2B
 of IVL21 (i.e., a gamma distribution). We agree with this observation and to assess robustness of the chosen distribution for predicting citation count, we have redone the preliminary validation of IVL21, using their parametrization of the gamma distribution instead 1 . To this end, we adjusted the citation counts by assuming x is gamma distributed with a shape parameter of 2 and a rate parameter of 0.8. Parameter x is defined as years since publication + 1 divided by 5 (see IVL21). This distribution produces a corrected citation count accounting for the expected increase in citations over time thus normalizing the citation count in relation to the age of the publication (see 
Figure 1B)
. This impacted the values of RVCN leading to less overlap between RVCN values for replicated and non-replicated studies (see 
Figure 1
 panel D). This demonstrates the dependency of RVCN on the distributional assumptions for citation count and underscores the need for follow-up work on the best way to quantify 'value'.  
Figure 2
.


The suitability of the validation data set is unclear.
Good validation is key to establish that RVCN is a useful measure of replication value. However, we contend that the authors' validation strategy falls short of demonstrating their indicator's effectiveness in measuring the target feature. While doing manual checks for our re-analysis of the results we realised that the Psychological Bulletin dataset relied on data reported in meta-analysis. These effects encompass not only primary outcomes from original studies but also secondary outcomes and results from additional analyses based on internally shared data. We argue that a study's replication value should be determined primarily by the effects considered in the original study. We believe it typically makes sense to focus on the main effect because it reflects the most accessible and transparent information for potential replication authors, as well as usually being the most central claim of a study. While a comprehensive manual check of the validation dataset to determine the proportion of non-primary effects is beyond the scope of this commentary, we question the dataset's suitability for validating replication values.


Comparison of RVCN to previous suggestions
In this section, we compare RVCN to replication target selection procedures outlined in our earlier publications 
(Field et al., 2019;
Pittelkow et al., 2021)
. A full description of these procedures is beyond the scope of this commentary. Briefly, we proposed that studies with uncertain evidence are in greater need of corroboration than studies with strong evidence either for or against a particular effect. To quantify this uncertainty, we advocated for the use of Bayes Factors (BFs), which assess the relative strength of evidence supporting two competing hypotheses, thereby providing a quantitative and continuous measure of uncertainty. Studies identified as having ambiguous evidence were subsequently assessed based on qualitative criteria, as illustrated in 
Table 1
 and recommendations for possible replication targets were made. Following these projects, we conducted a large-scale initiative involving a survey and a Delphi consensus method to develop a comprehensive set of qualitative criteria (see 
Table  1
) for replication target selection in practice 
(Pittelkow et al., 2023)
.
Our assessment of the proposal by IVL21 is that RVCN acts as a filter to make a preselection based on a set of candidate studies. As such it resembles the function of the Bayes Factor (BF) 2 as described in our earlier publications 
(Field et al., 2019;
Pittelkow et al., 2021)
.We therefore decided to compare the performance of RVCN to the BF and the results of 
Field et al. (2019)
 and 
Pittelkow et al. (2023)
. 
Field et al. (2019)
 extracted data for 57 results from 30 Psychological Science articles published between 2015-2016, that reported significant statistical tests (one-sample, paired, and independent t-tests), associated with primary research questions. 
Pittelkow et al. (2021)
 extracted data for 97 results from 78 articles published in the Journal of Consulting and Clinical Psychology published between 2012 and 2016, which supported their primary outcome by a statistically significant t-test. We had previously extracted the sample size. To calculate RVCN we collected additional data on citation count from Crossref on January 16 th 2025 following the method proposed by IVL21. Importantly, our data includes information from additional qualitative analyses assessing the value of the replication targets. If RVCN captures 'replication value', the studies identified by qualitative assessment should also present relatively larger RVCN values. 
Figure 2
 presents the results of our re-analysis.  There was no clear relationship between RVCN and BFs (rField = -0.09, rPittelkow = -0.12), suggesting that the two measures capture distinct aspects of replication value. Furthermore, RVCN scores did not differ between studies identified as replication targets through qualitative assessment and those that did not (see 
Table 2
). For the dataset from 
Field et al. (2019)
, RVCN values were even lower for studies suggested as replication targets compared to those not identified as replication targets. This implies that potentially important replication targets might be prematurely excluded from consideration based solely on RVCN scores. However, this does not mean that these studies are unsuitable replication targets. Every selection procedure has its strengths and limitations, but the minimal overlap between our qualitative suggestions and RVCN -based selections, particularly in the 
Field et al. (2019)
 dataset, begs the question of why two different selection procedures come to such different conclusions. 
Figure 1 .
1
Distribution of overall citation count and RVCN in the comparison sample of psychological findings (red) and the sample of replicated findings in psychology (blue).Following IVL21, the scale in all plots has been transformed by taking the cube root of the true values, which preserves the overall shape of the distribution but compresses the scale towards 1.(A) Overall citation count as reported in IVL21. (B) Overall citation count as estimated based on the gamma distribution. (C) RVCN as reported by IVL21 (D) RVCN based on total predicted citation count based on the gamma parameterization as specified by IVL21 in


Figure 2 .
2
Scatterplots plotting replication value (RVcn) against Bayes Factors (BF).BFs are presented on a reversed log 10 scale. The dotted lines indicate BF = 1 and BF = 3. We excluded very largeBFs and RVcns (BF> 10^10; RVcn > 10).


Table 1 .
1
Qualitative criteria proposed in previous work. Please note that this is not a row-by-row comparison but a listing of the different criteria; OS = original study equivalent to original claim.
Field et al. 2019
Pittelkow et al. 2021
Pittelkow et al. (2023)
relevance
relevance clinical
interventional study
interest
relevance of the OS for your current line of
relevance
research or the field you work in
clinical sample
doubt
current strength of evidence in favour of the
OS
severity of the condition
the (un)clarity and (un)replicability of the
original protocol
insufficient
scientific
evidence base
impact
the importance of the OS for research
investigation
relevance
theoretical
quality
theory
scientific background
the theoretical relevance of the OS
importance
sound
clear rational
implications of the OS (e.g. for practice,
policy or clinical work).
methodology
methodology
CONSORT criteria
methodology sample size
statistical method
flaws of the OS
appropriate
interpretation
interpretation and
operationalization of the OS's measures
conclusion follow logically
generalizability
concerns about questionable research
practices
robustness
generalizability of the OS
feasibility
the resources available for replicating the OS
the replicating team's presence or absence
of previous experience or expertise on the
OS
Note:


Table 2 .
2
Average RVCN scores for studies suggested and not suggested as replication targets based on previous analyses.


While this exact parametrization is to some extent arbitrary, we agree with the authors that "Figure 2Bdisplays a more realistic 50-year citation trajectory (Parolo et al., 2015)".


In contrast to RVCN , the BF is used as a function of uncertainty only.








Replication


Concluding remarks
We hope that this commentary can provide some nuance and multidimensionality to the perspective of IVL21. While their approach to assessing replication values facilitates the evaluation of large sets of potential targets, it risks oversimplifying a complex reality. However, we believe the IVL21 approach provides a systematic starting point that is preferable to having no structured methodology at all. Simplification by quantitative approximation does carry risks, but these need to be weighed against the practical benefits of transparent and systematic approximations.
Still, we argue that failing to emphasize the complexity of replication target selection may compromise the utility of their method. Although we have previously proposed partially quantitative methods ourselves, our recent work has shifted towards more qualitative and open approaches for describing replication value, exemplified by the checklist for transparent replication target selection (for more details please review 
Pittelkow et al. 2023)
. We contend that such flexible methodologies offer greater practical utility in the nuanced landscape of replication research. At the same time, we caution against definitive conclusions about the superiority of any one method in capturing replication value. By continuing to assess these approaches critically, we can refine strategies for replication target selection.
Data availability: The data and code that support the findings of this study are openly available on the Open Science Framework (OSF) at [https://osf.io/hnsry/; DOI 10.17605/OSF.IO/HNSRY]


Conflict of Interest:
The authors declare no conflict of interests.
Funding: This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. 
 










When and Why to Replicate: As Easy as 1, 2, 3? Collabra: Psychology




S
M
Field






R
Hoekstra






L
Bringmann






D
Van Ravenzwaaij




10.1525/collabra.218








5












Deciding what to replicate: A decision model for replication study selection under resource and knowledge constraints




P
M
Isager






R
C M
Van Aert






Š
Bahník






M
J
Brandt






K
A
Desoto






R
Giner-Sorolla






J
I
Krueger






M
Perugini






I
Ropovik






A
E
Veer






M
Vranka






D
Lakens




10.1037/met0000438








Psychological Methods




28


2
















Replication value as a function of citation impact and sample size




P
M
Isager






A
E
Veer






D
Lakens




10.31222/osf.io/knjea


















Citation analysis of scientific categories




G
S
Patience






C
A
Patience






B
Blais






F
Bertrand




10.1016/j.heliyon.2017.e00300








Heliyon




3


5














The process of replication target selection in psychology: What to consider




M.-M
Pittelkow






S
M
Field






P
M
Isager






A
E
Veer






T
Anderson






S
N
Cole






T
Dominik






R
Giner-Sorolla






S
Gok






T
Heyman






M
Jekel






T
J
Luke






D
B
Mitchell






R
Peels






R
Pendrous






S
Sarrazin






J
M
Schauer






E
Specker






U
S
Tran






D
Van Ravenzwaaij




10.1098/rsos.210586








Royal Society Open Science




10


2


210586














Replication target selection in clinical psychology: A Bayesian and qualitative reevaluation




M.-M
Pittelkow






R
Hoekstra






J
Karsten






D
Van Ravenzwaaij




10.1037/CPS0000013








Clinical Psychology: Science and Practice




28


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]