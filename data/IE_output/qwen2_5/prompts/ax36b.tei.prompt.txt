You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
Computational modeling has been used to study the properties of decision making for over years. Given the difficulty in both perturbing and observing the brain, models are vital for formally encoding and testing mechanistic hypotheses about decision making processes. For example, how do people process information over time, how do they modulate their levels of caution under different circumstances, or how do they extract information from complex choice sets with multiple alternatives and attributes. These types of questions are difficult to study through direct observation or statistical analysis alone. Models provide a rigorous way to study these types of questions indirectly by comparing the expected patterns of data predicted by the models to experimental observation.
One of the dominant classes of models in this area are Evidence Accumulation Models (EAMs). These models predict the outcome of decisions by modeling the process by which that decision is made. For example, the popular Drift Diffusion Model (DDM)  hypothesizes that people stochastically sample information over time, additively accumulate evidence based on that information, and make a decision when a critical threshold of evidence has been achieved. This and similar models are mathematically encoded in stochastic differential equations (SDE). A useful feature of this family of models is that they make predictions about both the choices made and the time it takes them to make choices, so-called Choice-Response Time (choice-RT) data. Importantly, the time required to complete the decision provides insight into the underlying decision process. For example, one might expect a strong preference for one alternative to yield a fast decision.
EAMs are a highly successful modeling framework in cognitive psychology. They both qualitatively and quantitatively capture a range of choice and response time benchmarks, including (1) speed-accuracy trade-off, (2) the positive skew of human response time distributions, (3) the relation between the mean and variance in response times, and (4) differences in fast and slow errors 
(Brown & Heathcote, 2008;
Ratcliff, 1978;
Ratcliff & Rouder, 1998;
Ratcliff, Zandt, & McKoon, 1999;
Usher & McClelland, 2001
). These models have also been applied to a wide range of behaviors including learning 
(Evans, Brown, Mewhort, & Heathcote, 2018;
Fontanesi, gluth, Spektor, & Rieskamp, 2019)
, categorization 
(Nosofsky, Little, Donkin, & Fific, 2011;
Nosofsky & Palmeri, 1997)
, memory 
(Osth & Farrell, 2019;
Ratcliff, 1978)
, language processing 
(Lerche, Christmann, & Voss, 2018;
Wagenmakers, Ratcliff, Gomez, & McKoon, 2008)
, and consumer choice 
(Busemeyer, Gluth, Rieskamp, & Turner, 2019;
Evans, Holmes, & Trueblood, 2019)
. More recently, researchers have started combining EAMs with psychophysiological data, such as neural recordings 
(Turner et al., 2013;
Turner, Rodriguez, Norcia, McClure, & Steyvers, 2016;
Turner, van Maanen, & Forstmann, 2015)
, motor recordings 
(Servant, White, Montagnini, & Burle, 2016)
, and eye movements 
(Krajbich, Armel, & Rangel, 2010)
. Despite their popularity and success, most applications use only the simplest forms of EAMs, such as the DDM. Tools for quantitatively fitting general EAMs are still lacking.
All current theoretical modeling approaches in this area make some type of significant sacrifice that limits their use. The typical modeling process in this area involves three essential elements: a theoretical model, experimental data to test it, and efficient and accurate methods for challenging the model with data. Existing methods sacrifice at least one of these. Accurate and efficient methods exist for working with complex experimental designs, but only with simple models. Alternatively, there are approaches for testing complex models, but with either documented accuracy issues, poor efficiency that limits experimental design (i.e. too many experimental conditions = too much compute time), or both (more details in the Background). These limitations restrict the questions researchers can ask and the studies they design.
The main challenge of working with EAMs and choice-RT data is to determine, for a particular model and parameter set, how well the model matches the empirical data 
(Dutilh et al., 2019)
. The likelihood function is a natural way to do this.
Unfortunately, due to the difficulty in working with SDEs, only the simplest models in this family have sufficiently analytic likelihoods to be tractable. We say "sufficiently" because the likelihoods of even the simplest models (e.g. the DDM) involve infinite series or intractable integrals. For example, Smith 
(Smith, 2000)
 developed an analytic approach to solve for the likelihood function of a range of SDE models. Unfortunately it tends to be slow to compute since it requires numerical calculation of many integrals.
Further, numerical integration produces resolution issues similar to that of numerical solutions. For these reasons, most methods utilize some way of approximating the quantitative agreement between model and data and make some sacrifice in the process.
Since much of the work on choice-RT model fitting applies to binary choice models, we will limit this brief discussion to binary decision modeling approaches (see 
Table 1
).
Until recently, the majority of quantitative fitting approaches were based on summary statistics 
(Turner & Van Zandt, 2018)
. One common approach is the quantile maximization approach 
(Heathcote, Brown, & Mewhort, 2002;
Ratcliff & Tuerlinckx, 2002)
. For a stochastic model, the user simulates a large number of outcomes, finds the quantiles of the RT distribution, compares those to the quantiles of the experimental RT distribution, and optimizes parameters for agreement. CHaRTr 
(Chandrasekaran & Hawkins, 2019)
, a recently developed R modeling package for general binary EAMs, takes this approach. It is well known however that this compression of the model and data into summary statistics leads to significant errors in parameter estimates for EAMs 
(Turner & Sederberg, 2014)
.
A second approach is the Probability Density Approximation 
(Holmes, 2015;
Lin, Heathcote, & Holmes, 2019;
Turner & Sederberg, 2014)
 method. This method starts much the same way by simulating a large number of stochastic outcomes. Those samples are then used to construct a kernel density estimate approximation of the actual likelihood function. This avoids the problems with summary statistics and is easily generalizable to more complex models 
(Evans, Holmes, Dasari, & Trueblood, 2021;
Evans, Trueblood, & Holmes, 2020;
Holmes, O'Daniels, & Trueblood, 2020;
Holmes, Trueblood, & Heathcote, 2016;
Trueblood, Heathcote, Evans, & Holmes, 2021;
Trueblood et al., 2018;
Turner & Sederberg, 2014)
. However, the simulated likelihood
F o r R e v i e w O n l y PYBEAM 6
function becomes a stochastic entity, which introduces significant problems for Bayesian inference . Specifically, a favorable estimate of the likelihood is easily accepted but difficult to reject in favor of a new parameter set, leading to MCMC chain stagnation, high MCMC rejection rates, and generally poor posterior approximations. Additionally, the large numbers of simulations needed are extremely computationally intensive.
Finally PyDDM 
(Shinn, Lam, & Murray, 2020)
 takes the approach of converting SDE models into Fokker-Planck models. This approach essentially reformulates the SDE description of the model as a probabilistic model described by a Partial Differential Equation (PDE). Solving this PDE then provides an approximation for the likelihood. Their approach is similar in idea to what we propose below, but has a number of drawbacks. First, relative to the needs of complex EAMs, it is inefficient due to inadequate numerical simulation approaches (which also introduces problematic accuracy issues), taking up to 5 hours to perform a maximum likelihood fit (according to their own benchmarks). Since Bayesian methods require sampling far more parameter sets than maximum likelihood (10-100x more), it would be infeasible to directly extend this. Second, their approach is not Bayesian. This is problematic because many models in this field have parameter indeterminacy issues that Bayesian methods naturally diagnose. Finally, their approach is limited to binary choice models. This is only a sub-sample of methods (see 
Table 1
) and there are a number of variations on them. However, they illustrate that there are currently no methods for  
Table 1
 List of existing software for fitting two-threshold, binary choice models to choice-RT data with the proposed characteristics of our approach PyBEAM in the final column.


Methods
In this section, we describe the broad methodological details of our algorithm for performing Bayesian parameter estimation for two-threshold models of binary evidence accumulation. Before continuing, we make a few notes. First, we mainly discuss the general idea behind this method. There are numerous implementation details "under the hood" that are required to make this work; however, we leave this discussion for the Supplementary Information. Second, we do not describe the Pythonic implementation of this algorithm in this article. Instead, we will provide multiple Jupyter notebooks 
(Kluyver et al., 2016)
 demonstrating the use of this method for both parameter recovery and application to data. These notebooks are more than just documented code. They are descriptive in nature and should be more effective than writing pseudo-code. These are publicly available and can be found at https://github.com/murrowma/pybeam. Finally, while reading these methods will likely help a reader understand how this method works at a high level, understanding these methods are not required to use the provided Python implementation.
F o r R e v i e w O n l y PYBEAM 8
The general two-threshold binary accumulation model
In this article, we develop an approach for calculating choice-RT likelihoods for general two threshold models of binary decision making and integrate them with Bayesian parameter estimation. For this type of model 
(Figure 1
), the evidence accumulation process begins at point z, corresponding to an initial bias prior to stimulus presentation. Upon stimulus presentation, evidence is noisily accumulated, described by the stochastic differential equation (SDE) 
(Smith, 2000)
,
dx(t) = v(x, t)dt + D(x, t)dB(t),
(1)
where x(t) is the total evidence accumulated at time t and v(x, t) is the rate of evidence accumulation (referred to as the drift rate). The drift is determined by the quality of information in the presented stimulus, where decisions with clear stimulus information produce larger drift rates. The function D(x, t) is the diffusion rate, and though in some models it can depend on (x, t), it is most commonly constant and fixed for scaling purposes (commonly set to either D(x, t) = 1, 0.1). Lastly, B(t) is a Gaussian noise term.
The evidence accumulation process described by Equation (1) continues until one of the two opposing decision thresholds (c 1 (t) or c 2 (t)) is reached, triggering a response ( 
Figure 1
). The separation s(t) between thresholds indicates the degree of time dependent caution exhibited by the decision maker. If thresholds are far apart (close together), the participant makes slower (faster), more (less) cautious decisions. Though in many evidence accumulation models the degree of caution is fixed, time changing caution and accordingly thresholds (as shown in 
Figure 1
) is increasingly being investigated as a mechanism to optimize reward rates 
(Drugowitsch, Moreno-Bote, Churchland, Shadlen, & Pouget, 2012;
Tajima, Drugowitsch, & Pouget, 2016)
 or the speed accuracy trade-off 
(Frazier & Yu, 2007)
. An additional parameter referred to as the non-decision time, t nd , is also generally included in EAMs. This term describes the time it takes for a participant to encode the stimulus information and the motor processes involved in selecting one of the available choices. Mathematically, this process is described by the noted stochastic differential equation,
F o r R e v i e w O n l y dx(t) = v(x,t)dt + D(x,t)dB(t) Bias (z) Evidence (v) Choice 1 Probability f 1 (t) Time Choice (c ) Choice 1 (c 1 ) Caution (s)
where x(t) is the total accumulated evidence (or preference), v(x, t) is the rate of evidence accumulation, D(x, t) is the diffusion rate, and B(t) is the noise term. This process results in the choice probability distribution f i (x, t), which indicates how likely it is for an accumulator to cross the decision threshold at that time. t nd indicates the time it takes for non-decision processes, which is added to the decision time from the response time. So called choice-RT data is common in this field. In order to determine how well a particular set of parameters for a particular model describe data of this form, it is useful to calculate the "likelihood" function which describes the probability of making the observed choice at the observed time. Though the stochastic form of binary choice evidence accumulation models is simple to formulate and interpret, it does not immediately provide us with this quantity of interest. This can be circumvented in a number of ways. Smith 
(Smith, 2000)
 devised an analytic integral based solution for this problem; however, it is computationally intensive and limited in its application.
Further, since it requires numerical calculation of integrals, it only can approximate the actual solution. Stochastic simulation based methods 
(Chandrasekaran & Hawkins, 2019;
Holmes, 2015)
 are more general, but are computationally inefficient largely due to the necessity of generating immense quantities of random numbers.
Fortunately, this stochastic process can instead be written as the Fokker-Planck equation, which describes the probability that the accumulator has precisely state (x) at time (t) 
(Ã–ttinger, 1996)
. We in particular use the forwards Fokker-Planck (FP) equation,
âˆ‚p(x, t) âˆ‚t = âˆ’ âˆ‚ [v(x, t)p(x, t)] âˆ‚x + 1 2 âˆ‚ [D(x, t) 2 p(x, t)] âˆ‚x 2 ,
(2)
where p(x, t) is the probability of accumulated evidence x at time t, and v(x, t) and
D(x, t) are the drift and diffusion rates introduced earlier. The EAM response thresholds are modeled as the PDE's boundary conditions. Specifically, we use absorbing boundary conditions where p(c 1 , t) = p(c 2 , t) = 0, encoding the fact that when the preference state reaches the threshold, it is removed or absorbed by that threshold.
While this is a probability, it is still not quite the quantity we need. This describes the accumulator state rather than the probability of a choice. We require the probability of having first crossed either of the two thresholds at time t. This is often referred to as the first passage time problem. Fortunately, this first passage time probability can be directly calculated from p(x, t) by calculating the flux of probability 
J(x, t) = v(x, t)p(x, t) âˆ’ âˆ‚ [D(x, t) 2 p(x, t)] âˆ‚x ,
(3)
is the probability flux at point (x,t) and the first passage time density at threshold i can be calculated as
f i (t) = J(c i (t), t).
As a synopsis, here are the steps necessary to calculate the likelihood function. 1)
Transform the model from a SDE formalism to the FP formalism. 2) Solve the the FP equation in the relevant (x,t) domain determined by the model thresholds. 3) Use that solution to calculate probability fluxes at the threshold, which ultimately is the likelihood function.
Step 1 is strait-forward, any SDE of the form discussed here can be directly transformed into a FP.
Step 3 is also straightforward once p(x, t) is calculated.
Step 2, simulating the FP equation, is the most complex and requires the most care.


Numerical Solution
Here we describe the main ideas of the process for numerically simulating the accumulator state probability p(x, t) from the FP equation. We have added numerous bells and whistles to this scheme for numerical stability, generality, and numerical speed. These are further discussed in the Supplementary Information and we focus on the main ideas only here.
There are three sources of complexity in this model: the state dependent drift v(x, t), the state dependent diffusion D(x, t), and the time dependent bounds c i (t). It turns out the bounds are the most challenging to deal with. We will be using finite differences to solve this problem. This causes issues like thresholds crossing between discretized grid points and points being inside the threshold at one time and outside at the next, introducing significant error into the solution. Though these can be overcome, there is a better way to address this complexity.
In its current form, the FP model is described by a relatively simple PDE with a time changing bound. Since this time changing bound is complicated to deal with numerically, we make a change of coordinates that flattens the thresholds 
(Crank, 1984)
, at the expense of making the PDE more complicated. Note that since this is an
F o r R e v i e w O n l y
exact transformation, no information about the model is lost. We introduce the following change of coordinates,
Îµ(x) = x âˆ’ c 1 (t) c (t) âˆ’ c 2 (t) .
(4)
In this new coordinate (Îµ(x)), the thresholds are fixed at values zero and one.
Substituting into Equation 
2
yields,
âˆ‚p(Îµ, t) âˆ‚t = 1 s Îµ ds dt + dc 2 dt âˆ‚p(Îµ, t) âˆ‚Îµ âˆ’ 1 s âˆ‚ [v(Îµ, t)p(Îµ, t)] âˆ‚Îµ + 1 2s 2 âˆ‚ 2 [d(Îµ, t) 2 p(Îµ, t)] âˆ‚Îµ 2 ,
(5)
where p(Îµ, t) is the probability of accumulated evidence in the new coordinate frame Îµ, s = c 1 âˆ’ c 2 is the separation between thresholds, and v(Îµ, t) and D(Îµ, t) are the drift and diffusion rates in the new coordinate frame. Though Equation 
5
is more complex, it only needs to be solved on a rectangular (Îµ,t) domain. This transforms the space domain to (0,1), dramatically simplifying implementation.
This transformed problem now amounts to solving a PDE on a rectangular domain. To solve this, we use the second order Crank-Nicolson finite difference scheme . This is a highly robust, unconditionally stable numerical scheme used in a wide range of PDE simulation applications. We do not go into detail here, but we have extensively tested the numerical discretization used for this method and introduced adaptive stepping in coordinate t to improve it. While we do not focus on these implementation complexities here, we note that they are absolutely critical to the practical application of this method. First, they speed the numerical solution by a factor of 10x or more, which is important when integrating this into a Bayesian framework. Second, these optimizations allow the user to apply this method to a wide array of problems without having to substantially "tune" the algorithm. With this method, we can now robustly and efficiently calculate p(x, t), which can then be used to compute first passage dime probabilities and construct the desired likelihood function.
We do make a brief note about one computational complexity. As noted by Shin et al. 
(Shinn et al., 2020)
, the Crank Nicholson scheme can introduce a numerical instability arising from the initial condition to the FP equation (i.e. the start point distribution), which can be particularly problematic with time varying thresholds. This
F o r R e v i e w O n l y PYBEAM 13
is a well known issue 
(Ã˜sterby, 2003)
. We ameliorate this by starting the PDE simulation with multiple small time steps to essentially smooth the transition from the initial condition to the remaining time domain. This adjustment removes this issue without any intervention by the user. With this and other similar optimizations, we have a highly efficient and stable algorithm for calculating choice-RT likelihoods.


Parameter Inference
To perform Bayesian parameter estimation, we integrate this method of likelihood construction into the python package PyMC3 
(Salvatier & andC. Fonnesbeck, 2016)
.
PyMC3 is a highly robust, well supported python package designed specifically to perform Markov chain Monte Carlo. Using this Bayseian platform comes with a number of benefits. First, it has been tested by a wide array of researchers over a number of years. Second, it has a number of different MCMC samplers integrated into it, which is important for this application. Third, it simplifies posterior analysis since PyMC3 has a number of functionalities built in (e.g. trace plotting and posterior summary statistics)
and supporting packages such as ArviZ 
(Kumar, Carroll, Hartikainen, & Martin, 2019)
 are available.


Though PyMC3 is known principally for implementing gradient based Monte
Carlo methods like NUTS 
(Hoffman & Gelman, 2014)
, we find that this algorithm is sub-optimal for this type of problem. Since these methods require the calculation of model derivatives with respect to every parameter in each step of the inference processsomething that is very slow when performed numerically -they work best when fast, analytic solutions are available. We instead utilize and recommend two different MCMC samplers which do not require gradients. The first and primary algorithm used in this work is history based Differential Evolution Markov Chain (DE-MCz) 
(Braak & Vrugt, 2008
), a variant of the popular Differential Evolution Markov Chain (DE-MC)
algorithm 
(Braak, 2006)
. In brief, DE-MC runs many chains in parallel and uses information shared between chains to auto-sense the structure of the posterior and intelligently construct parameter proposals. The second algorithm, Slice 
(Neal, 2003)
, is the alternative option implemented in PyBEAM. Though Slice converges in many fewer samples than DE-MCz, this algorithm is not recommended for general use since it is about 2-5x slower than DE-MCz. Slice is only recommended for use in models with very slow convergence or models with more than one posterior mode. In these uncommon cases, DE-MCz may require many more samples than normal to reach a final solution, making Slice a useful alternative.
As demonstrated in the following results, coupling this approach to constructing model likelihoods with the capabilities of PyMC3 yields a robust and efficient tool for performing Bayesian inference with general binary choice-RT models.


EAMs Implemented in PyBEAM
To facilitate use of this approach while allowing maximum flexibility, we have implemented two sections of the python package implementing these methods. First, we have constructed pre-coded, default versions of a number of common models. This facilitates their rapid use with minimal interaction using the PyBEAM framework.
Second, we implement a general model class that allows users to go beyond the common models. This requires more interaction with the modeling framework, but provides increased flexibility. We address both approaches in the remainder of this section. replace z with w = z/(2a), referred to as the relative start point, which sets the accumulator start as a ratio of the threshold separation (ranging from 0 to 1). In PyBEAM, we refer to this as the "base" EAM. This EAM is sometimes referred to in the literature as the "simple DDM" since it is a DDM without inter-trial variability parameters 
(Evans, Hawkins, & Brown, 2020)
.
The next two pre-coded EAMs add additional model assumptions to the base EAM to account for other psychological factors. The first, referred to as the "leakage" model in PyBEAM, adds leaky integration to the base EAM. This changes the drift rate from a constant to v(x, t) = Âµ âˆ’ lx, where l is the strength of leaky integration and
x is the total accumulated evidence. The second, referred to as "moving thresholds,"
adds time varying decision thresholds to the base EAM. They are free to either collapse towards zeros or move outwards, though collapse is the more common behavior in the literature. By default, three time varying decisions thresholds are implemented in PyBEAM. The first, linear, defines the decision thresholds as,
c 1 (t) = âˆ’c 2 (t) = a 0 âˆ’ mt,
(6)
where a 0 indicates the threshold location at time zero and m is the thresholds' slope.
The second, exponential, has thresholds defined as
c 1 (t) = âˆ’c 2 (t) = a 0 exp(âˆ’t/Ï„ ),
(7)
where a 0 , as before, is the threshold location at time zero and Ï„ describes how quickly the threshold collapses. The last, weibull, uses a weibull distribution function for the decision threshold, given by,
c 1 (t) = âˆ’c 2 (t) = a 0 âˆ’ a 0 (1 âˆ’ c) 2 ï£® ï£° 1 âˆ’ exp ï£« ï£­ âˆ’ t Î» Îº ï£¶ ï£¸ ï£¹ ï£» .
(8)
Here, a 0 has the same meaning as in the linear and exponential models. The next parameter, Î», is the scale parameter and approximately sets the time at which threshold collapse or expansion occurs. Parameter Îº is referred to as the shape parameter and indicates whether the threshold behavior is logistic (Îº > 1) or exponential (Îº < 1).
Lastly, c is the collapse ratio and sets the amount the threshold collapses or expands.
F o r R e v i e w O n l y
The weibull distribution is a particularly good choice for a collapsing threshold due to its flexibility in behavior 
(Hawkins, Forstmann, Wagenmakers, Ratcliff, & Brown, 2015)
.
Depending upon the choice of parameters, it is able to model early collapse, late collapse, and no collapse, while simultaneously replicating logistic and exponential threshold behavior.
Though the weibull decision threshold still maintains the same functional form, we modify it slightly when running PyBEAM's MCMC sampler. Instead of sampling the shape Î» and scale Îº parameters directly, we instead sample from the base ten logarithm of these parameters. Since the weibull threshold can produce both logistic and exponential behavior, Î» and Îº have large functional parameter ranges. This makes it difficult for the MCMC algorithm to evenly sample from all parts of parameter space.
For example, common Îº values for an exponential model are between 0.5 and 1, while Îº values for a logistic model generally range from 1 to 10. This disparity in reasonable parameter range makes its difficult to sample from accurately. Thus, by defualt, PyBEAM samples from and reports the base ten logarithm for parameters Î» and Îº.
The last EAM pre-coded in PyBEAM is the Urgency Gating Model, referred to as the "UGM." 
(Cisek, Puskas, & El-Murr, 2009)
. As in Trueblood et al. , it starts with the base EAM discussed above, then adds an urgency signal and leakage to both the drift and diffusion rates. The drift rate is defined as,
v(x, t) = Âµ(1 + kt) + ï£« ï£­ k 1 + kt âˆ’ l ï£¶ ï£¸ x.
(9)
Here, Âµ and l are the drift and leakage rates from the base and leakage EAMs described above. The additional parameter, k, is referred to as the urgency ratio parameter and describes the strength of urgency in the model (see 
Trueblood et al. (Trueblood et al., 2021
) for further discussion). If k = 0, the model encodes no urgency and is instead the leakage EAM. If k â†’ âˆž, urgency dominates the decision process. In addition to altering the drift rate, the UGM modifies the EAM diffusion rate, D(x, t). It is given by,
D(x, t) = Ïƒ(1 + kt),
(10)
where k is still the urgency ratio and Ïƒ is the same as the noise parameter from the Custom Models in PyBEAM. In addition to providing users with pre-coded models, PyBEAM also allows for the creation of user-defined custom scripts.
Instructions for creating a script can be found on https://github.com/murrowma/pybeam, but we provide a useful example here which we test in the results section of this paper.
Though we provide the UGM by default in PyBEAM (see Section ), it is known that parameter recovery for it can be challenging. The urgency parameter k is correlated with a and l, causing multiple combinations of each parameter to generate similar likelihood functions. To address this problem and improve parameter recovery, 
Trueblood et al. (Trueblood et al., 2021)
 proposed an experiment with time changing stimulus information. Specifically, using a grid of pixels flashing one of two colors (blue / orange), they altered the fraction of each color on the screen at a given time while a decision was being made. So, early in the decision process, the grid may be 55 percent blue while later it might be 55 percent orange..
We implement a similar model as a custom script in PyBEAM. In the UGM, stimulus strength is encoded in the parameter Âµ. Thus, we modify the UGM drift rate from Equation 9 to account for the time changing stimulus information, giving
v(x, t) = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ Âµ(1 + kt) + k 1+kt âˆ’ l x, if t < t 1 âˆ’Âµ(1 + kt) + k 1+kt âˆ’ l x, if t â‰¥ t 1
(11)
where t 1 is the time when stimulus information changes. For example, at time t < t 1 , the grid may be dominated by blue which corresponds to positive Âµ. Then, at time t 1 , they flip to orange dominated, modeled by making Âµ negative.


Results
In We first validate PyBEAM's likelihood construction. We do this for the three of the models discussed in Section : the base EAM, the base EAM with leaky integration, and the base EAM with weibull decision thresholds. For each model, we choose thirty parameter sets, then simulate 250 data points using Equation 1. For the base and weibull cases, we choose parameters from human data sets. Described fully in Section , we used PyBEAM to fit the base and weibull EAMs to a human data set collected by Evans et al. 
(Evans, Hawkins, & Brown, 2020)
. Thirty random parameter sets were chosen from this data for validation in this section. For the leaky integration EAM, we did not have data to draw upon, so we instead randomly chose parameters within reasonable ranges. The parameter sets from which these are drawn are located at https://github.com/murrowma/pybeam. Using the data generated from each model, we calculated the total loglikelihood of the data sets using both the Smith analytic solution discussed in Section and the PyBEAM numerical approach. The Smith analytic solution was pulled from Drugowitsch's lab github, https://github.com/DrugowitschLab/dm. The results of this are shown in 
Figure 2
.
In panel A, we display the analytic loglikelihood versus the PyBEAM numerical loglikelihood for all parameter sets. The solid black line indicates the graphical location where the analytic and PyBEAM solutions are identical. In panel B, we report the error in the loglikelihood for each of the EAMs parameter sets. The error is calculated by taking the absolute difference between the analytic and numerical solutions. These results show that PyBEAM's numerical approach to calculating likelihood functions is highly accurate in all cases. Notably, the PyBEAM numerical solution is 5-10x faster than the Smith analytic solution with only minimal losses in precision.


Parameter Recovery
We next assess PyBEAM's ability to perform model and parameter recovery for EAMs of the type discussed in Section . To do this, we first simulate data from the model. Then, we use PyBEAM to fit the model to the simulated data set. Lastly, we compare both the posterior distributions output by PyBEAM to the generating parameters and the best fit choice-RT distribution to the true distribution.
We make a brief note that there are multiple ways to assess the efficacy of parameter recovery. The first and most straight forward is to determine whether the posterior distribution found matches the generating parameters. However, models, particularly more complex ones, often present parameter indeterminacy issues where a subspace of parameters can produce nearly identical model outputs. In this case, the estimation procedure may fail to recover the exact generating parameters but still fit the data well. This is a consequence of the model structure and is not a failing of the parameter estimation procedure. For this reason we visualize both posterior parameter distributions as well as the quality of fit to the data itself. Base EAM. We first demonstrate parameter recovery for the base EAM described in Section . Following the procedure outlined in , we first simulate five-hundred data points from the model using Equation 1. Then, we use PyBEAM to fit the model to this synthetic data (results in 
Figure 3)
.  plotting the likelihood function generated by each parameter set (red). The posterior predictive displays the range of likelihood distributions which fit the data, providing an approximation of the variance in fit. In this case, the posterior predictive describes the data set well, with little variation between the predicted likelihood function and data.
âˆ’2 0 2 Time (s) A) Likelihood Val. PyBEAM Analytic âˆ’2 0 2 Time (s) B) Posterior Pred. PyBEAM Data 0.2 0.3 Non-decision Time (t nd ) C1) Posteriors True Posterior 0.4 0.6 Relative Start (w) C2) 0 1 2 Drift (Î¼) C3) 0.6 0.8 1.0 Decision Threshold (a) C4)
The remaining panels C1-C4 display the model parameter posteriors generated by PyBEAM. The vertical blue lines indicate the true parameter values of the simulated data sets (displayed in 
Figure 3
), while the grey histograms are the posteriors. These results show parameter recovery is excellent for the base EAM. In addition, PyBEAM is able to recover parameters quickly, with an approximate run time for this experiment of less than a minute on a 2020 Macbook Pro.
Base EAM with Leaky Integration. The second model we recover parameters from is the base EAM with leaky integration discussed in Section .
Recovering the leakage parameter from a single condition EAM can be challenging, so we instead simulate a slightly more complicated data set with two conditions: low decision threshold a 1 (condition 1) and high decision threshold a 2 (condition 2). This mimics a speed/accuracy trade-off scenario. The remaining parameters are shared between the two conditions and only the threshold parameter a differs between the two simulated conditions.
To validate PyBEAM's parameter recovery for the leakage EAM, we follow the process outlined at the beginning of the results section. We simulate 250 data points for each condition. Then, we use PyBEAM to find the model parameters which best describe this data. We report the results of parameter recovery in 
Figure 4
.
In panel groups A and B of 
Figure 4
, we report the likelihood validation and posterior predictive plots for condition 1 (low a) and condition 2 (high a), respectively.
As with the previous example, likelihood validation compares the PyBEAM likelihood to the analytic (using the Smith 
(Smith, 2000)
 approach in this case) for the known   Panel group C displays the posteriors for both fits. We leave out the non-decision time and relative start posteriors since they are easily fit (see Supplementary Information for these fits). We find that the shared parameters, drift and leakage, have posteriors (grey) which closely match the true values (blue lines). The individual caution parameters a 1 and a 2 also match expectation closely. Parameter recovery is once again highly efficient, taking approximately a minute on a 2020 MacBook Pro.
Base EAM with Moving Decision Thresholds. We next use PyBEAM to recover parameters from the base EAM with moving decisions thresholds. Specifically, we use the model with weibull decision thresholds described in Section . To validate PyBEAM's parameter recovery for this model, we again follow the methodology described at the beginning of the results. First, we choose our parameters. Since the weibull threshold is capable of producing both logistic and exponential thresholds, we choose two parameter sets to replicate these behaviors: one with k > 1 and one with k < 1. The weibull threshold is also able to produce varying amounts of collapse via the c parameter. For convenience, for both parameter sets, we set this to -1.0, indicating that the thresholds will collapse to zero. As with the base EAM, we simulate 500 data points for each parameter set. We then use PyBEAM to recover parameters from the simulated data. We show the parameter recovery results in 
Figure 5
.
Panel groups A and B display the likelihood validation and posterior predictive (described in Section )for the k > 1 and k < 1 parameter sets, respectively. In both cases, the PyBEAM numerical solution produces the same result as the analytic solution, and the posterior predictive matches the data set closely. Panel group C displays the posterior predictive for the threshold itself in each case. The blue line corresponds to the true threshold, while the red lines are generated by drawing 100 random random parameter sets from the posterior and plotting their threshold shape.  In both the k > 1 and k < 1 cases, the posterior produces time varying thresholds that closely match that used to generate the data.
Panel groups D and E display the posteriors for the k > 1 and k < 1 parameter sets, respectively. We only display the posteriors related to the decision thresholds (recovery for other parameters can be found in the Supplementary Information). As discussed in Section , for the scale Î» and shape k parameters, we sampled from and report here the log of these parameter values. Both have large reasonable parameter regimes, so sampling in log space guarantees we get good coverage of all possible parameter space.
For the k > 1 case, we see good agreement between the true parameter values (blue) and the PyBEAM prediction (grey histogram). However, for the k < 1 case, the posteriors predictive for the threshold is slightly different than that of the simulated values, even though the likelihood and threshold recovery from panel groups B and C are very good. This is the result of parameter sloppiness inherent to the weibull threshold model, where multiple parameter combinations can produce similar likelihood functions. Though this is a common problem with the weibull model, particularly in the k < 1 parameter regime, it doesn't significantly affect the interpretation of the results.
Different parameter sets produce qualitatively similar decision thresholds.
Urgency Gating Model with Changing Information. The final model we recover parameters from is the UGM with time changing stimulus information described in Section . In this model, the drift rate takes the form of Equation 11, where stimulus information changes from positive to negative at time t 0 . Note that since this is a more complex synthetic experimental design (time changing stimuli), fits here are implemented with the custom component of PyBEAM. We fit the model for two data conditions: low drift (condition 1) and high drift (condition 2), corresponding to low and high quality stimulus information. The parameters used are displayed in 2.
To validate PyBEAM's parameter recovery for the UGM with changing information, we simulate 500 data points for each condition using the parameters in 
Table 2
. As before, we then use PyBEAM to find the model parameters which best  
Table 2
 Parameters used for the UGM with changing information example. Parameters are t nd , the non-decision time; w, the relative start point; Âµ, the drift rate; l, the leakage rate; k, the urgency parameter; a, the decision threshold location; and t 1 , the time when stimulus information flips. describe this data. We report the results of parameter recovery in 
Figure 6
.
In Panels A1 and B1, we display the likelihood validation for data conditions and 2, respectively. Since the UGM has no analytic solution, we instead simulate 25,000 data points from the model for each condition (black histograms) to compare our numerical result to (red). In both cases, the PyBEAM likelihood is once again highly accurate. In Panels A2 and B2, we report the posterior predictive for conditions 1 and 2, respectively. PyBEAM once again performs well at fitting the data.
In panel group C we report the posteriors output by PyBEAM. As before, we omit the non-decision time and threshold locations results here (reported in the 
Supplementary Information)
. In panel C1, we report the drift rate posteriors for both condition 1 (left histogram) and condition 2 (right histogram). The remaining three posteriors in panels C2-4 are for the leakage, urgency, and decision threshold, and are shared between the two conditions. For all cases, the posteriors (grey) match the true data values (blue) well, indicating effective parameter recovery.  Since real data is always messier than simulated data, we last demonstrate the use of PyBEAM to perform parameter inference using choice-RT experimental data collected by Evans et al. 
(Evans, Hawkins, & Brown, 2020)
. This data set consists of three different experiments. In each, participants made decisions about direction of dot motion in a random dot kinematogram presented at different coherence's: 0%, 5%, 10%, and 40%. In the first experiment, sixty-three participants were instructed to maximize reward rate; in the second experiment, seventy-one participants were given a decision deadline; and in the third experiment, one hundred and fifty-four participants were instructed to emphasize speed. The goal of these experiments was to examine if and when participants might utilize collapsing thresholds. To answer this questions, they used a hierarchical Bayesian approach to fit three evidence accumulation models to their data: the base EAM discussed in Section ; a base EAM with parameter inter-trial variability; and a base EAM with weibull decision thresholds.
Our goal here is to fit models with and without changing thresholds to each of these data sets and compare results to those obtained by Evans et al 
(Evans, Hawkins, & Brown, 2020)
. The purpose here is not to study the psychological question motivating their work. Rather, the purpose is to assess the efficacy of this method and compare its conclusions to theirs. Their modeling approach would be considered the "gold standard" for this type of modeling. Thus we use it as a point of comparison while noting that: 1) their implementation utilized fully custom code compared to PyBEAM's more user friendly package approach and 2) PyBEAM is likely faster than their Smith solution approach 
(Smith, 2000)
. Though we cannot directly compare the computational speed of PyBEAM to the Evan's approach, the numerical integration required by it scales quadratically with changes in resolution 
(Buonocore, Nobile, & Ricciardi, 1987)
, while Crank-Nicolson's speed scales linearly with resolution. In our experience, this results in a 5-10x slowdown when compared to versus the PyBEAM approach.
We do make a few notable changes in our fitting methodology compared to that in Evans et al. First, we do not use a hierarchical approach and instead fit the model to but this is left for future development. Second, we do not model inter-trial variability since 1) two of the models fit by Evans did not include it and 2) these parameters are usually not recoverable "Estimating across-trial variability parameters of the Diffusion Decision Model: Expert advice and recommendations" (2018). As discussed in the discussion, inter-trial variability in the start point and the non-decision distribution could be readily incorporated into PyBEAM, though inter-trial variability in all other parameters would incur significant extra computational cost.
We first examine the Choice Proportion (CP) and choice-RT predictions of the model in 
Figure 7
. Panel groups A1, B1, and C1 compare the CP predicted by the model to that of the data for Experiments 1, 2, and 3, respectively. For all three Experiments, we find good model fits, with correlation coefficients R 2 far exceeding 0.9.
Additionally, for all three models, the correlation between model and data is very similar for both the flat and weibull decision thresholds.
Panel groups A2, B2, and C3 compare the average choice-RT of the model to that of the data for Experiments 1, 2, and 3, respectively. For Experiments 1 and 3, both models describe the data well, with R 2 values for both exceeding 0.9. However, in Experiment 2, we see a distinct preference for the weibull model. This matches closely the conclusions of Evans, who noted that only Experiment 2 displays clear evidence for collapsing decision thresholds.
In addition to reporting CP and choice-RT data comparison, we also report DIC values for each model in 
Figure 8
. Specifically, we plot the absolute difference between base EAM's DIC (DIC f ) and the base EAM weibull decision thresholds' DIC (DIC w ).
If this value is positive, the moving thresholds model is preferred; if it is negative, the base EAM is preferred.
We see that, for Experiment 1, the vast majority of participants favored the moving thresholds model. The participants which preferred the base EAM over the moving thresholds version did so with small DIC differences. For Experiment 3, There is roughly an even split between those which favored the base and moving threshold   For Experiment 2, we see that all participants favor the moving thresholds model, with most strongly favoring (DIC difference greater than 10) that model.
Our DIC results also match closely those determined by Evans. In general, the base EAM was only rarely preferred for Experiment 1 and Experiment 2 in their work, while commonly preferred in Experiment 3. We see a similar pattern in our results. We last report the decision thresholds predicted by both the flat and weibull models in 
Figure 9
. In the first row, we display the threshold which produces the max loglikelihood for each participant. In row two, we display the average threshold, calculated by averaging together all of the participants decision thresholds. We find that, for Experiments 1, little collapse occurs for most participants, producing a threshold similar to that of the base EAM. This mathches well with the CP and choice-RT analyses which indicated that there was little preference between the two models. Notably, the shape of the average threshold is comparable to that of Evan's results (shown in 
Figure 2
 of their publication 
(Evans, Hawkins, & Brown, 2020)
).
In Experiment 2, we see that all participants exhibit thresholds which collapse substantially, suggesting that a collapsing threshold best describes the data. This matches the results of the CP, choice-RT, and DIC results which indicated that Lastly, in Experiment 3, we observe decision thresholds which vary substantially from one participant to the next. For this data set, some participants had mean choice-RTs near a second, while others had mean choice-RTs upwards of three and four seconds. This difference in response time produces dramatically different thresholds for each individual which make describing the average behavior challenging. In spite of this, the average threshold we calculate is very similar to that determined by Evans (shown in 
Figure 2
 of their publication 
(Evans, Hawkins, & Brown, 2020)
). Further, it also supports the conclusions of the CP, choice-RT, and DIC results which suggested that Experiment 3 had no clear model preference.   In this work, we introduced the Python package PyBEAM to provide an easy to use tool for Bayesian inference of complex EAMs. Unlike previous methods which have either generated likelihood functions using simulated SDEs or slow analytic solutions, PyBEAM instead uses the Fokker-Planck equation, dramatically improving both accuracy and speed. In addition to being fast, it increases flexibility by allowing custom inputs for the drift rate, diffusion rate, and decision thresholds. PyBEAM then pairs this likelihood construction method with the Python package PyMC3 
(Salvatier & andC. Fonnesbeck, 2016)
 for rapid Bayesian inference of model parameter sets. Using PyBEAM, more complex models can be fit to data with both high precision and high speed. Further, to make this type of modeling accessible to others, we provide a Python based package which allows exploration of complex models with a relatively low cost of entry.
Looking forward, we plan to incorporate a number of additional features into the PyBEAM framework. First, we plan to incorporate hierarchical capability. Second, we plan to incorporate the ability to include inter-trial variability in the threshold and non-decision time parameters. These additions can be made without substantial changes to the main numerical engine within PyBEAM. We have opted to leave these for a v2 development as our focus here was producing the most user friendly framework possible. Finally, this approach can likely be extended to model multi-alternative choice models. This will however require significantly more numerical development and is beyond the scope of this initial article.  Supplementary Information for PyBEAM: A Bayesian approach to parameter inference for a wide class of binary evidence accumulation models.


Solving the forwards Fokker-Planck equation using the Crank-Nicolson method
To numerically solve the Fokker-Planck equation discussed in the main body of the publication, PyBEAM uses the Crank-Nicolson method . This is a finite difference method of second order accuracy which, for the Fokker-Planck equation, requires discretization in two dimensions: space and time. To simplify the spatial discretization, we introduced the following change of coordinates,
Îµ(x) = x âˆ’ c 1 (t) c 1 (t) âˆ’ c 2 (t) .
(1)
where x is the accumulator state, c 1 is the upper decision threshold, and c 2 is the lower decision threshold. In this new coordinate, the Fokker-Planck equation becomes,
âˆ‚p(Îµ, t) âˆ‚t = 1 s Îµ ds dt + dc 2 dt âˆ‚p(Îµ, t) âˆ‚Îµ âˆ’ 1 s âˆ‚ [v(Îµ, t)p(Îµ, t)] âˆ‚Îµ + 1 2s 2 âˆ‚ 2 [d(Îµ, t) 2 p(Îµ, t)] âˆ‚Îµ 2 ,
(2)
where p(Îµ, t) is the probability of accumulated evidence in the new coordinate frame Îµ, s = c 1 âˆ’ c 2 is the separation between thresholds, and v(Îµ, t) and d 
(Îµ, t)
 are the drift and diffusion rates in the new coordinate frame.
This change of coordinates transforms the space domain to (0,1). Thus, the spatial discretization is independent of the decision threshold location. In PyBEAM, the default spatial step size is âˆ†Îµ = 0.01, which provides sufficient accuracy for nearly all models. It can be modified if needed, with documentation for how to do this located at https://github.com/murrowma/pybeam.
The time discretization required to guarantee high precision is more complicated.
Since PyBEAM uses computationally taxing Bayesian methods, choosing the correct time step âˆ†t is critical for maintaining high accuracy without sacrificing speed. To approximate an appropriate time step, PyBEAM simulates the model ten times using Equation (1) from the main publication. Then, we determine the average first passage time and multiply it by 0.025 to set the time step. This is the default time resolution in PyBEAM, and can be modified per the instructions at https://github.com/murrowma/pybeam.
In addition to approximating an appropriate time step, two additional issues occur due to the Crank-Nicolson method. The first is due to the discontinuity located at the initial condition. At time t = 0, the initial condition for p(Îµ, t) is
f (x) = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ p(Îµ, 0) = Î´(Îµ) if Îµ = w, 0 otherwise,
(3)
where w is the relative start point and Î´(Îµ) is the Dirac delta function. The delta function initial condition is discontinuous, causing large, damped oscillations in the numerical solution for p(Îµ, t) around the spatial region of the discontinuity. The damping time scale of the oscillations are proportional to the ratio âˆ†t/âˆ†Îµ 2 . If this ratio is large, then the oscillations persist for longer times. If it is small, the oscillations damp more quickly. Fortunately, the effect of the oscillations are localized to the spatial region surrounding the initial condition. In cases where decision thresholds are not moving, the effect of the oscillations near the thresholds is insignificant. However, if you have a moving threshold which crosses the spatial coordinate of the initial condition (i.e. at some time t, threshold c(t) = z), the oscillations near the initial condition propagate into the first passage time distribution. This causes large inaccuracies in the likelihood function which are unacceptable for parameter estimation.
There are a number of ways to manage this problem, several of which are summarized by Ã˜sterby 
(Ã˜sterby, 2003)
. A more recent approach taken by Boehm et al. 
(Boehm, Cox, Gantner, & Stevenson, 2021
) simplifies the backwards Fokker-Planck equation by using the known series solution  for the flat threshold, constant drift version of the equation. Doing so eliminates the discontinuity, avoiding the problem entirely.
In PyBEAM, we take an approach discussed in Ã˜sterby 
(Ã˜sterby, 2003)
. Since the oscillations about the initial condition are dependent upon the âˆ†t/âˆ†x 2 ratio, if âˆ†t is small, the oscillations go away quickly. However, âˆ†t need not remain small always, just at the beginning of the numerical solution. So, in PyBEAM, we take twenty-five time steps âˆ†t i that are one-hundred times smaller than the approximated time step, eliminating the oscillations from the solution.
The second time discretization problem occurs when thresholds change location rapidly. Though we have transformed the spatial domain to (0,1), rapid changes in threshold location still produce large inaccuracies in the first passage time distribution.
This can cause otherwise appropriate time steps to fail in specific, localized regions of the likelihood function. The easiest way to handle this is to make the time step small so that your solution always remains precise; however, as discussed above, since Bayesian inference is computationally taxing, you ideally want the largest time step possible to increase speed.
To circumvent this speed-accuracy trade off, PyBEAM implements two changes to the time discretization. First, after the initial twenty-five time steps âˆ†t i used to reduce solution oscillations are completed, the time step is linearly increased over one-hundred time steps from the initial time step size âˆ†t i to the estimated time step size âˆ†t. This prevents thresholds which collapse early in the decision process (such as exponential thresholds) from reducing solution precision by guaranteeing that we have small time steps for small time t.
Second, PyBEAM monitors the distance between decision thresholds and, if the distance changes quickly, decreases the time step size to an acceptable level. The algorithm is roughly as follows. At time t, the decision threshold separation s n is calculated. The decision threshold separation is then calculated at the previous and next time steps, s nâˆ’1 and s n+1 , respectively. Time step n âˆ’ 1 occurs at t âˆ’ âˆ†t, while time step n + 1 is at t + âˆ†t. We then calculate the ratio of collapse s r for time steps n âˆ’ 1 and n + 1. For example, for n + 1, this ratio is
s r,n+1 = abs(s n âˆ’ s n+1 )/s n .
(4)
If this ratio or that for time step n âˆ’ 1 is more than 2%, the time step is halved. The process is repeated until a time can be found which limits the collapse ratio to 2% or less. and 6, we excluded posteriors which were easy to recover. The figures in this section add to those figures the excluded posteriors.
In 
Figure 1
, we include the non-decision time and relative start posteriors excluded in 
Figure 4
 of the main publication. In 
Figure 2
, we include the non-decision time, relative start, and drift rate posteriors excluded in Figure of the main publication. And, in 
Figure 3
, we include the non-decision time and relative start posteriors excluded in 
Figure 6
 of the main publication. Jupyter notebooks 
(Kluyver et al., 2016)
 which produce these figures and the main publication versions can be found on https://github.com/murrowma/pybeam.    
Figure 1 .
1
Model Schematic. Evidence is accumulated (blue line) starting from an initial bias (black dot) until one of the two choice thresholds, c 1 or c 2 , is reached (red dot), triggering a decision. The distance between thresholds indicates an individuals degree of caution which can change or remain constant as a function of time.


Default EAMs .
.
PyBEAM contains pre-coded versions of EAMs commonly used in the literature. The basis for all pre-coded models is the EAM described in Section , which is then modified to match the specific model needs. The simplest of these is an EAM with constant non-decision time t nd ; constant drift rate v(x, t) = Âµ; constant diffusion D(x, t) = Ïƒ, usually set to either Ïƒ = 0.1, 1.0; accumulation start point z; and flat, symmetric decision thresholds c 1 (t) = âˆ’c 2 (t) = a. For convenience, we


Figure 2 .
2
Validation of PyBEAM likelihood construction. A) Analytic loglikelihood versus PyBEAM's numerical loglikelihood of simulated data. Thirty parameter sets were chosen for each of the listed EAM types, being the base EAM (blue dot), and the base EAM with leaky integration (red upwards triangle), and the base EAM with weibull collapsing thresholds (orange downwards triangle). The black line indicates where the analytic and numerical are equal. B) Error in loglikelihood calculation for each parameter set. Calculated as the difference between the analytic and numerical loglikelihood for each parameter set. Marker shapes and colors are the same as in panel A.


Figure 3 .
3
Results from the base EAM parameter recovery example. A) Likelihood function validation for the base EAM. The red line is the PyBEAM numerical solution, while the blue dashed line is the analytic solution (Navarro & Fuss, 2009). B) PyBEAM posterior predictive. Red lines are the PyBEAM solutions, while the grey bars are the simulated data. C) PyBEAM posteriors for each of the base EAM parameters. Grey bars are the PyBEAM posteriors, while the blue lines are the true values used to generate the data. C1 displays the non-decision time posterior, with a true value of t nd = 0.25. C2 displays the relative start point posterior, with a true value of w = 0.5. C3 displays the drift rate posterior, with a true value of Âµ = 1.0. C4 displays the decision threshold posterior, with a true value of a = 0.75. Panel A of Figure 3 compares the analytic (dashed blue) and PyBEAM (red)likelihood functions using the known generating parameters. The results exactly overlap, demonstrating once again that the PyBEAM approach produces highly accurate RT


B displays the simulated data and the posterior predictive fit given by PyBEAM. In this example and all going forwards, we generate the posterior predictive by drawing 100 random parameter sets from the PyBEAM posteriors and


Figure 4 .
4
Results from the base EAM with leaky integration example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low caution. In A1, we plot the PyBEAM solution in red and the analytic solution in blue. In A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high caution. Colors have same meaning as in A. C) Posteriors for both conditions. Vertical blue lines indicate the true parameter values, while the grey bars are the PyBEAM posteriors. The drift (C1) and leakage (C2) posteriors are shared between conditions with true values of Âµ = 1.0 and l = 3.0, respectively. The threshold locations for the low caution a 1 and high caution a 2 conditions are displayed in C3 and C4, with true values of a 1 = 0.5 and a 2 = 0.75, respectively.


the posterior predictive panels compare the data to the RT distributions resulting from the PyBEAM model fits. Results demonstrate that once again, PyBEAM produces highly accurate likelihood functions and fits the generating data well.


Figure 5 .
5
Results from the collapsing thresholds example. A) Likelihood validation and posterior predictive for the k > 1 parameter set. In A1, we plot the PyBEAM likelihood function in red and the analytic likelihood function in blue. In A2, the PyBEAM likelihood predictions are in red while the simulated data is grey. B) Likelihood validation and posterior predictive for the k < 1 parameter set. Colors have same meaning as in A. C) Predicted thresholds for both the k > 1 and k < 1 parameter sets. Red lines show the PyBEAM predicted thresholds, while the blue line is the true threshold. D) Posteriors for the k > 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. D1 contains the threshold start posterior, with true value a 0 = 1.0. D2 contains the log of the shape parameter, with true value lambda = 1.0. D3 contains the log of the scale parameter, with true value k = 3.0. E) Posteriors for the k < 1 parameter set. Colors have same meaning as in D. E1 contains the threshold start posterior, with true value a 0 = 1.0. E2 contains the log of the shape parameter, with true value lambda = 5.0. E3 contains the log of the scale parameter, with true value k = 0.67.


Figure 6 .
6
Results from the UGM with changing information example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low drift rate. In panel A1, we plot the PyBEAM solution in red and the analytic solution in blue. In panel A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high drift rate. Colors have same meaning as in A. C) Posteriors for all conditions. Blue indicates the true parameter value, while grey are the PyBEAM posteriors. Panel C1 contains the posteriors for both the low (left histogram) and high drift rate (right histogram)conditions, with drift rates of Âµ = 0.5 and Âµ = 2.0, respectively. Panels C2-C4 contains the leakage, urgency, and decision threshold posteriors which are shared between both conditions. These have true values of l = 3.0, k = 1.0 and a = 1.0, respectively.


It is in principle possible to extend PyBEAM to fit hierarchical models,


Figure 7 .
7
Choice proportion (CP) and average choice-RT results from PyBEAM. Panels plot the model predictions on the vertical axis and the data values on the horizontal axis. All three experiments share the same legend displayed in the plot's first row. The text on the figure indicates which model is used, either the base EAM (Base) or the weibull moving thresholds EAM (Weib.), and the correlation coefficient R 2 of the model and data. A) CP (A1) and choice-RT (A2) results for Experiment 1. B) CP (B1) and choice-RT (B2) results for Experiment 2. C) CP (C1) and choice-RT (C2) results for Experiment 3.


Figure 8 .
8
Absolute difference between the base EAM's DIC (DIC f ) and the weibull moving thresholds' DIC (DIC w ) for Experiment 1 (A), Experiment 2 (B), and Experiment 3 (C). Participants are ordered by how the large this difference is, with smallest on the left and largest on the right. If the absolute difference is positive, the moving thresholds EAM is favored. If negative, the base EAM is favored.


the preferred model. This also matches the results of Evans, which indicated that Experiment 2 had the clearest support for collapsing decision thresholds.


Figure 9 .
9
Upper decision thresholds predicted by the model for Experiment 1 (A), Experiment 2 (B), and Experiment 3 (C) versus time. The upper and lower thresholds are symmetric, so the upper is only displayed here. Black lines are for the base EAM, while the red lines are for the weibull moving thresholds EAM. The first row displays the decisions thresholds for each participant, while the second row averages all decision thresholds together.


are one of the most dominant classes of computational models used to study decision making behavior. However, to date, few tools are available which allow researchers to study complex EAMs. Further, Bayesian methods have generally been too slow for practical use, restricting to model fitting to techniques like max loglikelihood or quantile maximization.


F


In the main body of the publication, we provided figures demonstrating that PyBEAM can recover parameters from a number of common models. InFigures 4, 5,


Figure 1 .
1
Results from the base EAM with leaky integration example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low caution. In A1, we plot the PyBEAM solution in red and the analytic solution in blue. In A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high caution. Colors have same meaning as in A. C) Posteriors for both conditions. Vertical blue lines indicate the true parameter values, while the grey bars are the PyBEAM posteriors. The drift (C1) and leakage (C2) posteriors are shared between conditions with true values of Âµ = 1.0 and l = 3.0, respectively. The threshold locations for the low caution a 1 and high caution a 2conditions are displayed in C3 and C4, with true values of a 1 = 0.5 and a 2 = 0.75, respectively. Panels C5 and C6 contain the non-decision time and relative start parameters, respectively, with true values of t nd = 0.25 and w = 0.5.


Figure 2 .
2
Results from the collapsing thresholds example. A) Likelihood validation and posterior predictive for the k > 1 parameter set. In the first panel, we plot the PyBEAM likelihood function in red and the analytic likelihood function in blue. In the second panel, the PyBEAM likelihood predictions are in red while the simulated data is grey. B) Likelihood validation and posterior predictive for the k < 1 parameter set. Colors have same meaning as in A. C) Predicted thresholds for both the k > 1 and k < 1 parameter sets. Red lines show the PyBEAM predicted thresholds, while the blue line is the true threshold. D) Posteriors for the k > 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. D1 contains the threshold start posterior, with true value a 0 = 1.0. D2 contains the log of the shape parameter, with true value lambda = 1.0. D3 contains the log of the scale parameter, with true value k = 3.0. D4, D5, and D6 contain the non-decision time, relative start, and drift rate posteriors, respectively. These have true values of t nd = 0.25, w = 0.5, and mu = 1.0. E) Posteriors for the k < 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. E1 contains the threshold start posterior, with true value a 0 = 1.0. E2 contains the log of the shape parameter, with true value lambda = 5.0. E3 contains the log of the scale parameter, with true value k = 0.67. E4, E5, and E6 contain the non-decision time, relative start, and drift rate posteriors, respectively. These have true values of t nd = 0.25, w = 0.5, and mu = 1.0.


Figure 3 .
3
Results from the UGM with changing information example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low drift rate. In panel A1, we plot the PyBEAM solution in red and the analytic solution in blue. In panel A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high drift rate. Colors have same meaning as in A. C) Posteriors for all conditions. Blue indicates the true parameter value, while grey are the PyBEAM posteriors. Panel C1 contains the posteriors for both the low (left histogram) and high drift rate (right histogram) conditions, with drift rates of Âµ = 0.5 and Âµ = 2.0, respectively. Panels C2-C4 contains the leakage, urgency, and decision threshold posteriors which are shared between both conditions. These have true values of l = 3.0, k = 1.0 and a = 1.0, respectively. Panels C5 and C6 contain the non-decision time and relative start posteriors, with true values of t nd = 0.25 and w = 0.5, respectively.


similarly except that each chain only looks at its own history, not other chains. Based on PyMC3's analysis of this algorithm, DE-MCz is able to achieve comparable results to DE-MC with many fewer chains (DEMetropolis(Z): Population vs. History efficiency comparison, 2022), making it an attractive algorithm for use on a personal computer with a limited number of cores.
F o
r
R e v
i e w
O n
l
y
The history based version, DE-MCz, works








 










A markov chain monte carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces




C
J F
Braak




10.1007/s11222-006-8769-1






Statistics and Computing


















Differential evolution markov chain with snooker updater and fewer chains




C
J F T
Braak






J
A
Vrugt




10.1007/s11222-008-9104-9






Statistics and Computing




18
















The simplest complete model of choice response time: Linear ballistic accumulation




S
D
Brown






A
Heathcote




10.1016/j.cogpsych.2007.12.002






Cognitive Psychology




57


3
















A new integral equation for the evaluation of first-passage-time probability densities




A
Buonocore






A
G
Nobile






L
M
Ricciardi










Advances in Applied Probability




19


4
















Cognitive and neural bases of multi-attribute, multi-alternative, value-based decisions




J
R
Busemeyer






S
Gluth






J
Rieskamp






B
M
Turner








Trends Cogn Sci




23


3
















Chartr: An r toolbox for modeling choice and response times in decision-making tasks




C
Chandrasekaran






G
E
Hawkins




10.1016/j.jneumeth.2019.108432






Journal of Neuroscience Methods




328














Decisions in changing conditions: The urgency-gating model




P
Cisek






G
A
Puskas






S
El-Murr




10.1523/JNEUROSCI.1844-09.2009






Psychological Review




29


37
















Free and moving boundary problems




J
Crank








Oxford University Press












A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type




J
Crank






P
Nicolson




10.1017/S0305004100023197






Cambridge University Press














Population vs. history efficiency comparison




Demetropolis


















The cost of accumulating evidence in perceptual decision making


10.1523/JNEUROSCI.4010-11.2012








The Journal of Neruoscience




32


11
















The quality of response time data inference: A blinded, collaborative assessment of the validity of cognitive models




G
Dutilh






J
Annis






S
D
Brown






P
Cassey






N
J
Evans






R
P
Grasman




10.3758/s13423-017-1417-2






Psychonomic bulletin & review




26


4
















Estimating across-trial variability parameters of the diffusion decision model: Expert advice and recommendations


10.1016/j.jmp.2018.09.004






Journal of Mathematical Psychology




87
















Refining the law of practice




N
J
Evans






S
D
Brown






D
J K
Mewhort






A
Heathcote




10.1037/rev0000105






Psychological review




125


4
















The role of passing time in decision-making




N
J
Evans






G
E
Hawkins






S
D
Brown




10.1037/xlm0000725






Journal of Experimental Psychology: Learning, Memory, and Cognition




46


2
















The impact of presentation order on attraction and repulsion effects in decision-making




N
J
Evans






W
R
Holmes






A
Dasari






J
S
Trueblood




















10.1037/dec0000144






Decision




8


1


36












Response-time data provide critical constraints on dynamic models of multi-alternative




N
J
Evans






W
R
Holmes






J
S
Trueblood












multi-attribute choice










10.3758/s13423-018-1557-z






Psychon Bull Rev




26


3














A parameter recovery assessment of time-variant models of decision-making. Behavior research methods




N
J
Evans






J
S
Trueblood






W
R
Holmes




10.3758/s13428-019-01218-0






52














A reinforcement learning diffusion decision model for value-based decisions




L
Fontanesi






S
Gluth






M
Spektor






J
Rieskamp




10.3758/s13423-018-1554-2






Psychon Bull Rev




26


















P
I
Frazier






A
J
Yu






Sequential hypothesis testing under stochastic deadlines. NIPS


















Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making




G
E
Hawkins






B
U
Forstmann






E
J
Wagenmakers






R
Ratcliff






S
D
Brown




10.1523/JNEUROSCI.2410-14.2015






The Journal of Neuroscience




35


6
















Quantile maximum likelihood estimation of response time distributions




A
Heathcote






S
Brown






D
J K
Mewhort




10.3758/bf03196299






Psychon Bull Rev




9


2
















The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo




M
D
Hoffman






A
Gelman










Journal of Machine Learning Research




15
















A practical guide to the probability density approximation (pda) with improved implementation and error characterization




W
R
Holmes




10.1016/j.jmp.2015.08.006






Journal of Mathematical Psychology




68


69
















A joint deep neural network and evidence accumulation modeling approach to human decision-making with naturalistic images




W
R
Holmes






P
O'daniels






J
S
Trueblood




10.1007/s42113-019-00042-1






Computational Brain & Behavior




3


1
















Bayesian analysis of the piecewise diffusion decision model




W
R
Holmes






J
S
Trueblood




10.3758/s13428-017-0901-y






Behavior Research Methods




50


2
















A new framework for modeling decisions about changing information: The piecewise linear ballistic accumulator model




W
R
Holmes






J
S
Trueblood






A
Heathcote




10.1016/j.cogpsych.2015.11.002






Cognitive psychology




85




















T
Kluyver






B
Ragan-Kelley






F
PÃ©rez






B
Granger






M
Bussonnier






J
Frederic














Jupyter notebooks -a publishing format for reproducible computational workflows




C
Willing




10.3233/978-1-61499-649-1-87






Positioning and power in academic publishing: Players, agents and agendas


F. Loizides & B. Schmidt


















Visual fixations and the computation and comparison of value in simple choice




I
Krajbich






C
Armel






A
Rangel




10.1038/nn.2635






Nature Neuroscience




13


10
















Arviz a unified library for exploratory analysis of bayesian models in python




R
Kumar






C
Carroll






A
Hartikainen






O
Martin




10.21105/joss.01143


doi: 10.21105/joss.01143








Journal of Open Source Software




4


33


1143














Impact of context information on metaphor elaboration a diffusion model study




V
Lerche






U
Christmann






A
Voss




10.1027/1618-3169/a000422






Experimental Psychology




65


6


















Y.-S
Lin






A
Heathcote






W
R
Holmes




10.3758/s13428-018-1153-1


Parallel probability density approximation. Behavior research methods






51














Fast and accurate calculations for first-passage times in wiener diffusion models




D
J
Navarro






I
G
Fuss




10.1016/j.jmp.2009.02.003






Journal of Mathematical Psychology




53


4
















Slice sampling




R
M
Neal




10.1214/aos/1056562461






The Annals of Statistics




31


3
















Short-term memory scanning viewed as exemplar-based categorization




R
M
Nosofsky






D
R
Little






D
Donkin






M
Fific




10.1037/a0022494






Psychological review




118


2
















An exemplar-based random walk model of speeded classification




R
M
Nosofsky






T
J
Palmeri




10.1037/0033-295x.104.2.266






Psychological review




104


2
















Five ways of reducing the crank-nicolson oscillations




O
Ã˜sterby




10.1023/B:BITN.0000009942.00540.94






BIT Numerical Mathematics




43
















Using response time distributions and race models to characterize primacy and recency effects in free recall initiation




A
F
Osth






S
Farrell




10.1037/rev0000149






Psychological review




126


4
















Stochastic processes in polymeric fluids




H
C
Ã–ttinger




10.1007/978-3-642-58290-5








Springer-Verlag


Berlin Heidelberg












A theory of memory retrieval




R
Ratcliff




10.1037/0033-295X.85.2.59






Psychological review




85


2
















The diffusion decision model: Theory and data for two-choice decision tasks




R
Ratcliff






G
Mckoon




10.1162/neco.2008.12-06-420






Neural Computation




20


4
















Modeling response times for two-choice decisions. psychological science




R
Ratcliff






J
N
Rouder




10.1111/1467-9280.00067






Psychological Science




9


5
















Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability




R
Ratcliff






F
Tuerlinckx




















10.3758/BF03196302






Psychon Bull Rev




9


3














Connectionist and diffusion models of reaction time




R
Ratcliff






T
V
Zandt






G
Mckoon




10.1037/0033-295x.106.2.261






Psychological Review




106


2
















Probabilistic programming in python using pymc3




J
Salvatier






T
V W
Fonnesbeck




10.7717/peerj-cs.55






PeerJ Computer Science




2


55














Linking theoretical decision-making mechanisms in the simon task with electrophysiological data: A model-based neuroscience study in human




M
Servant






C
White






A
Montagnini






B
Burle




10.1162/jocn_a_00989






Journal of Cognitive Neuroscience




28


10
















A flexible framework for simulating and fitting generalized drift-diffusion models. eLife, 9




M
Shinn






N
H
Lam






J
D
Murray




10.7554/eLife.56938
















Stochastic dynamic models of response time and accuracy: A foundational primer




P
L
Smith




10.1006/jmps.1999.1260






The Journal of Mathematical Psychology




44


2
















Optimal policy for value-based decision-making




S
Tajima






J
Drugowitsch






A
Pouget




10.1038/ncomms12400






Nature Communications




7














Urgency, leakage, and the relative nature of information processing in decision-making




J
S
Trueblood






A
Heathcote






N
J
Evans






W
R
Holmes




10.1037/rev0000255






Psychological Review




128


1
















The impact of speed and bias on the cognitive processes of experts and novices in medical image decision-making




J
S
Trueblood






W
R
Holmes






A
C
Seegmiller






J
Douds






M
Compton






E
Szentirmai






.
.
Eichbaum






Q


















10.1186/s41235-018-0119-2


Cognitive Research: Principles and Implications
















A bayesian framework for simultaneously modeling neural and behavioral data




B
M
Turner






B
U
Forstmann






E
J
Wagenmakers






S
D
Brown






P
B
Sederberg






M
Steyvers




10.1016/j.neuroimage.2013.01.048






Neuroimage




72
















Why more is better: Simultaneous modeling of eeg, fmri, and behavioral data




B
M
Turner






C
A
Rodriguez






T
M
Norcia






S
M
Mcclure






M
Steyvers




















10.1016/j.neuroimage.2015.12.030






Neuroimage




128














A generalized, likelihood-free method for posterior estimation




B
M
Turner






P
B
Sederberg




10.3758/s13423-013-0530-0






Psychon Bull Rev




21


2
















Informing cognitive abstractions through neuroimaging: The neural drift diffusion model. psychological review




B
M
Turner






L
Van Maanen






B
U
Forstmann




10.1016/j.tics.2018.12.003






Psychological Review




122


2
















Approximating bayesian inference through model simulation




B
M
Turner






T
Van Zandt




10.1016/j.tics.2018.06.003






Trends in cognitive sciences




22


9
















The time course of perceptual choice: The leaky, competing accumulator model




M
Usher






J
L
Mcclelland




10.1037/0033-295x.108.3.550






Psychological Review




108


3
















A diffusion model account of criterion shifts in the lexical decision task




E
J
Wagenmakers






R
Ratcliff






P
Gomez






G
Mckoon




10.1016/j.jml.2007.04.006






Journal of Memory and Language




58


1
















Fast solutions for the first-passage distribution of diffusion models with space-time-dependent drift functions and time-dependent boundaries




References
Boehm






U
Cox






S
Gantner






G
Stevenson






R




10.1016/j.jmp.2021.102613






Journal of Mathematical Psychology




105


102613














A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type




J
Crank






P
Nicolson




10.1017/S0305004100023197






Cambridge University Press


















T
Kluyver






B
Ragan-Kelley






F
PÃ©rez






B
Granger






M
Bussonnier






J
Frederic














Jupyter notebooks -a publishing format for reproducible computational workflows




C
Willing




10.3233/978-1-61499-649-1-87






Positioning and power in academic publishing: Players, agents and agendas


F. Loizides & B. Schmidt


















Fast and accurate calculations for first-passage times in wiener diffusion models




D
J
Navarro






I
G
Fuss




10.1016/j.jmp.2009.02.003






Journal of Mathematical Psychology




53


4
















Five ways of reducing the crank-nicolson oscillations




O
Ã˜sterby




10.1023/B:BITN.0000009942.00540.94






BIT Numerical Mathematics




43

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]