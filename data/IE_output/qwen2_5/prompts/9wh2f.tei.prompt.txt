You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
"Incorrect. I am not an AI. My code name is Project 2501. I am a living, thinking entity that was created in the sea of information." -Puppet Master (Ghost in the Shell)
The Fourth Industrial Revolution is characterised by the ubiquity of information and digital technologies. This revolution is epitomised by Artificial Intelligence 
(AI)
 and Machine Learning (ML), and at the heart of AI/ML are algorithms. Institutions, organisations and governments are using algorithms to cope with the vast amounts of information in these social sectors and to speed up and optimise decision-making processes 
[1]
. For example, the widespread use of algorithms in society was particularly demonstrated by the research undertaken to understand the global impact of COVID-19. During this crisis, algorithms played crucial roles across multiple domains: statistical algorithms were deployed to model virus fatality curves and study intervention effectiveness 
[2]
, while machine learning techniques supported molecular, medical, and epidemiological applications 
[3]
. The successful deployment of algorithms in such high-stakes scenarios underscores both their growing importance in societal decisionmaking and the critical need to understand the factors that influence public trust in algorithmic systems. This evolution of algorithmic applications extends beyond public health emergencies to numerous other domains where decisions can significantly impact human lives and society. From surveillance systems monitoring public spaces
To whom correspondence should be addressed; E-mail: fernando.marmolejoramos@flinders.edu.au to algorithms managing financial markets and predicting economic trends, these tools increasingly mediate high-stakes decisions across various sectors. The growing reliance on algorithmic decision-making in such consequential contexts necessitates a deeper understanding of their societal implications and reliability.
Algorithms help people to make decisions that have wider social implications; algorithms have transformative social power 
[4]
 when they are used to integrate complex data, such as the risk factors of homeless people 
[5]
 or identifying the people with the greatest need in relation to different diseases 
[6]
. The use of algorithms to aid decision making implies that there should be some confidence in their reliability. This raises a number of important questions. First, how much trust do people place in algorithms?
More specifically, does trust depend on the context in which the algorithm is used? Is trust determined by knowing how the algorithm works? And is trust affected by an individual's cognitive abilities?
This study examines how trust in algorithms is affected by the societal relevance of the algorithm, the declared reliability of the algorithm, and the level of data literacy of the cogniser. First, the three key concepts of AI/ML, data and algorithms are defined.
Second, it provides examples of the nature and use of algorithms in society. Third, the issue of explainable algorithms and trust is considered. Finally, the nature of the current study and the working hypotheses are outlined.


AI/ML, data, and algorithms
Broadly speaking, artificial intelligence (AI) is any type of technology that automates processes to solve problems that are usually associated with human intellectual capabilities 
[7]
. More specifically, AI aims to solve problems and achieve goals with limited or no human supervision. A closely related term is machine learning (ML). Originally coined by Samuel 
[8]
, ML can be defined as a collection of algorithms (mainly statistical and mathematical) to build computers capable of learning through experience (see 
[9]
). While the terms AI and ML are often used interchangeably, ML may be considered a more appropriate term than AI. Stereotypically, AI tends to be associated with rather unrealistic narratives depicting agents capable of human behaviour (see 
[10]
), and such examples are not yet feasible (also known as general AI). ML refers to algorithms designed to perform specific tasks in an automated way (also known as narrow AI) 
[11]
.
ML relies on data and algorithms (see 
[12]
), which together permeate many sectors of society (e.g. Schwab Intelligent Portfolios, 
[13]
). While algorithms can be defined as step-by-step procedures for solving a problem, data can be defined as numerical and categorical information about objects, events, processes and people that is digitally encoded (see 
[12]
). For example, the following steps represent a solution algorithm for estimating the central tendency in a vector of numbers: i) sum all the numbers, and ii) divide the result of the sum by the number of elements in the vector. This algorithm is known as the arithmetic mean (or average). The caveat of this algorithm is that it will be biased if the data does not follow a Gaussian shape. In other words, the output of this algorithm is only reliable if the data can be confidently shown to have a normal shape (e.g. via normality tests). In the context of AI-related technologies, algorithms are procedures designed to perform automated tasks using data sets to support human reasoning and decision making. In other words, data is used to feed algorithms, and algorithms in turn are used to drive AI agents 
[14]
. Thus, algorithms are the "ghost in the shell" behind any AI agent. The figure 1 illustrates this relationship between algorithms, data and AI (here ADA) 
[12]
. The place of algorithms in society
Algorithms influence our daily lives. Whether it is defining our interests through our browser history 
[15]
, determining what music we should listen to 
[16]
, or where we should go for dinner 
[17]
. On a massive scale, algorithms are being used to extract information from so-called "big data" and support decision making in areas as diverse as surveillance 
[18]
, traffic management 
[19]
, and financial markets 
[13]
. More recently, a new field of human-algorithm interaction mediated by natural language generation (NLG) systems has emerged, such as the Generative Pre-trained Transformer 3 model (better known as GPT-3) 
[20]
. GPT-3 produces human-like texts that are difficult to distinguish from texts written by humans 
[21]
, and this has begun to raise concerns about its use in various contexts, such as academic plagiarism 
[22]
 or computer programming 
[23]
. While algorithms are increasingly embedded in our digital experiences, it is important to distinguish between their varying levels of impact on human lives.
As such, the majority of algorithms are used in a context that does not significantly affect our lives. We refer to these instances of algorithmic use as low-stakes scenarios.
More recently, however, AI and ML algorithms have been used in scenarios that could have a significant impact. For example, algorithms are being used in hiring and promotion decisions 
[24]
, the criminal justice system 
[25]
, and self-driving cars 
[26]
, to name a few. We call the latter a high-stakes scenario. That is, the above situations represent two types of scenarios in which algorithms could affect our daily lives: one with little involvement and almost no consequences (low-stakes scenario), and the other with great involvement and consequences (high-stakes scenario).
However, our interactions with algorithms are not limited to low-stakes and highstakes scenarios and often involve preconceptions related to fear and distrust 
[27]
.
The literature suggests several explanations for why people do not trust algorithms, including a cost-benefit oriented logic where people tend to distrust algorithms even when presented with evidence of their superior performance, as they weigh potential risks more heavily than potential benefits 
[28]
. Many see algorithms as an "enigmatic technology" because they are difficult to understand 
[4]
 or in some cases, because people believe that algorithms are not capable of learning from their mistakes 
[29]
, but at the same time they also believe that they could be replaced by computers 
[30,
31]
. Algorithmic bias can also affect trust (see examples in medicine 
[32,
33]
. For a recent comprehensive report on trust in AI, see 
[34]
).
'Technophobia', a term coined by 
Rosen
  and African American participants 
[36,
37]
. These findings suggest that technophobia's relationship with demographic factors is more complex than previously assumed, transcending traditional socio-demographic boundaries and affecting individuals across various social, professional, and cultural groups.
Similar existential fears dominate the public debate around concerns such as autonomous weapons 
[38,
39]
. One of these sociological fears is the fear of autonomous robots. This is a widespread fear in different countries 
[40,
41]
, even though most people have not had contact with this type of robot. These fears could be the result of exposure to the way robots are portrayed in science fiction or social constructs related to robots, such as the possibility of being replaced by a robot at work 
[40,
41]
.
This polarisation against robots and AI is fuelled by attention-grabbing events such as the recent confirmation by Blake Lemoine, a Google engineer, that the chatbox LaMDA has the ability to express thoughts and feelings like a human child 
[42]
 or the concerns about text generated by GPT-3 
[43]
. These examples further distract the public from the most legitimate and worrying problems of these systems, such as "data colonialism" or the disturbing parallels between AI development and European colonialism 
[44]
. These parallels manifest in several ways: the extraction and exploitation of data from marginalized populations, mirroring colonial resource extraction; the use of Global South populations as testing grounds for AI systems developed in the Global North, reminiscent of colonial medical experimentation; and the imposition of Western conceptual frameworks of intelligence and ethics onto diverse cultural contexts. The field's emphasis on "ethics" often serves, paradoxically, as a form of technocratic rationalization similar to how ethical arguments were used to justify colonial expansion 
[44]
. Additionally concerning is that algorithms may reinforce preconceived stereotypes 
[45]
 and mishandle our personal data or who our data is shared with 
[46]
, perpetuating historical patterns of discrimination and surveillance that characterized colonial governance. In addition, how the data given to algorithms is annotated has a direct impact on algorithmic performance 
[47]
, raising questions about whose worldview and categories are being encoded into these systems.
The media plays a significant role in shaping public perception of AI by cover-ing two main sources of concern: autonomous technology and computer technology 
[48]
. Autonomous technology refers to intelligent machines capable of making decisions independently, while computer technology encompasses software that supports communication and computation. The media tends to distinguish between these two categories and also differentiates between fear and criticism when discussing AI. This dichotomous approach to presenting the issues surrounding AI introduces a bias in how we perceive the risks associated with the technology. Consequently, this bias influences the level of trust we place in AI systems. The way the media frames the discussion about AI has a substantial impact on public opinion and can lead to a distorted understanding of the actual risks and benefits of the technology.
Developing a better understanding of how algorithms work and how to modify them can help reduce distrust in these systems, as suggested by several authors 
[4,
28,
49]
.
When people have knowledge about how algorithms work, they can use this information to empower themselves as users. For example, music fans have acted collectively to boost the rankings of certain bands by engaging in massive streaming or downloading 
[50]
. Another example is Linkedln Brazil, which changed its algorithms to allow job ads targeted at Afro-Brazilians following social pressure 
[51]
. These cases show that understanding how an algorithm works can both minimise suspicion and empower users. It is not necessary to understand all the technical details of how an algorithm works, but rather to understand that algorithms use statistical methods to classify, sort, rank and order information. This understanding of statistical concepts is called statistical literacy 
[52]
.


Explainable algorithms
The knowledge required to understand and critically evaluate statistical results in order to make decisions based on them is defined as statistical literacy (SL) 
[52]
. Since its inception, the concept of SL has evolved 
[53]
 to include elements related to the context in which statistical reasoning can be applied 
[54]
. SL plays a crucial role in society 
[55]
 and the communication of statistical information is now more important than ever 
[56]
. More recently, SL is leading individuals to recognise the importance of mathematics in the world 
[57]
.
Due to the statistical nature of algorithms, some level of SL is crucial to understanding what algorithms are capable of, but this understanding will also depend on the level of transparency or explainability of the algorithms 
[58]
. Explainability refers to the interpretability, comprehensibility or readability of the algorithm. Most of the latest algorithms are based on complex multi-layer networks, the basis of deep learning, which use an internal logic that experts cannot fully understand 
[59]
. These systems are called 'black box' algorithms and various efforts have been made to promote their transparency 
[60]
. Black box algorithms are less trusted than transparent models because they cannot be explained 
[61]
.
Several approaches have been proposed to increase the transparency of AI models and reduce systematic errors that affect their performance. One such approach is based on the concept of "model cards for model reporting" (see 
Figure 1
 from 
[62]
).
This approach suggests that a comprehensive list of information should accompany the description of how the model was trained. This information should include details of the technician who developed the model, the intended use of the model, and the demographic or phenotypic groups on which the model has been tested. In addition, the model card should list the decisions made to optimise the model's performance and the various analyses carried out during the training process. Similar efforts to provide a framework for identifying biases associated with the data used to build or train AI models include the REVISE (REvealing VIsual biaSEs) 
[63]
 and The Spotlight 
[64]
 projects. These initiatives aim to increase transparency by systematically documenting and disclosing potential biases, enabling more informed use and interpretation of AI models.
Another more complex concern, also related to explainability, is the principle of explicability, a concept that combines intelligibility and accountability as the basis of an interpretable AI model 
[65]
. The latter concept points to the importance of trans- algorithms are used (half related to low-stake situations and the other half to highstake situations). Each scenario was followed by two questions (see below), which were answered on a VAS rating scale from 0 (not at all likely) to 5 (very likely), using up to two decimal places. The results of expert judgement of these items are provided in the supplementary material. All phases of the study were programmed and distributed using Qualtrics â„¢. . SL = 12 scenarios list 1 and 2 (list 1 = six low-stake scenarios with explainability and six high-stake scenarios without explainability, scenario list 2 = six low-stake scenarios without explainability and six high-stake scenarios with explainability). PtT = six-item propensity to trust scale. BLIS = 14-item BLIS scale. f = Items were presented in a fixed order. r = items presented in random order. Note that PtT always followed one of the two scenario lists.
to make restaurant recommendations, (2) to select stories for online news, 


Procedure
The experiment is a 2 Ã— 2 factorial design: the importance of the situation in which an algorithm is used (low and high stake situation) and the explainability of the algorithm (with and without). These factors were implemented in the 12 scenarios via two lists; list 1 = six low-stake scenarios with explainability and six high-stake scenarios without explainability, and scenario list 2 = six low-stake scenarios without explainability and six high-stake scenarios with explainability. The four sets of questions were counterbalanced across participants, resulting in four experimental conditions (see 
Figure
 3). Qualtrics ensured that participants were randomly assigned to each condition and that a balanced number of responses were collected for each condition. While the median time to complete the task was 24 minutes, there was some variation, with an interquartile range of 27 minutes (i.e., half of the participants completed the task within a 27-minute time span).


Statistical analyses
Data analysis was conducted using multilevel linear models implemented in the R packages lmerTest and lme4 
[71,
72]
 A stepwise backward model/variable selection algorithm was applied to this model to produce a significant and parsimonious model. The initial and final models were evaluated using metrics such as AIC and AICc weights 
[73]
, R 2 (coefficient of determination) for conditional (both fixed and random effects) and marginal (fixed effects) models, and performance score. These metrics were estimated using the performance R package 
[74]
.
Once a parsimonious model was found, the marginal and conditional R 2 values were estimated using the r2 nakagawa command from the performance R package 
[74]
, then, the variance components of the random factors were estimated using the gstudy command from the gtheory R package 
[75]
.
For access to all materials and analysis codes, including a machine learning approach, visit the following link: https://figshare.com/projects/Trust_in_ algorithms_An_experimental_approach_-_Data_repository/156212


Results
The stepwise backward evaluation suggested the same model as the initial model (see section 'Statistical Analyses'). 
Tables 1 and 2 provide a summary of the models, while   table 3
 provides an ANOVA-like table for the model. An evaluation of the assumptions of the linear model using the R package gvlma showed that these assumptions were not met 
[76]
 (although, a QQ plot of the residuals showed no significant deviation from normality). As a result, a robust linear mixed model 
[77]
 was fitted using the robustlmm R package, and the estimates obtained were similar to those of the linear mixed model. These results are not unexpected, as previous research has shown that linear mixed models are robust to violations of distributional assumptions 
[78]
. Further details of the statistical models can be found in the supplementary material.
The intercept of the resulting mixed linear model was 1.46 (see 
table 1
), suggesting that on a scale of 0 to 5, the probability of trusting, recommending, or using algorithms in explainable and high-stake scenarios, as rated by young women with lower BLIS and ADA scores, was 29.32% (  
Figure 5
). Regarding the interactions between predictors, the likelihood of trusting, recommending, or using algorithms significantly increased for low-stake scenarios combined with higher BLIS scores (53.8%) and significantly decreased for scenarios without explainability combined with low-stake and higher BLIS scores (21.4%), always compared to the intercept (see 
Figure 6
).
In terms of main effects, the results suggest a positive association between the likelihood of trusting/recommending/using algorithms and statistical literacy and familiarity with ADA, and a negative association between the likelihood of trusting/recommending/using algorithms and age. That is, the higher the level of statistical literacy, the higher the likelihood of trusting algorithms, and the higher the familiarity with ADA, the higher the likelihood of trusting algorithms. Also, the older a person is, the less likely they are to trust algorithms (although focused analyses indicated a slightly negative association between age and BLIS, such an association must be treated with caution as the number of observations decreases with increasing age). In terms of gender, it was found that participants who identified their gender as male were less likely to trust, recommend or use algorithms than those who identified their gender as female or other (this situation may be related to the fact that men have statistically significantly higher average levels of BLIS than women or 'other'; see supplementary materials for details). Finally, only three countries showed a trend towards less reliance on algorithms, all of them highly industrialised countries (see 
Fig. 5
).  of these investigations employed fictional scenarios grounded in real-world contexts 
[81,
82,
84]
, and one study utilized a comparable sample size of approximately 2,000
participants 
[84]
. Notably, none of these studies employed multicultural samples or examined the relationship between algorithm trust and statistical literacy. This gap was also identified in a systematic review by Mahmud et al. 
[85]
, which encompassed  This study is the first to examine the relationship between statistical literacy and trust in algorithms, revealing a nuanced relationship that depends on context. Our findings demonstrate that statistical literacy has opposite effects in different scenarios: it increases trust in algorithmic decisions for low-stakes situations while decreasing trust for high-stakes decisions. This differential effect suggests that statistical literacy enables a more sophisticated understanding of algorithmic capabilities and limitations.
In low-stakes scenarios (such as restaurant recommendations or music suggestions), individuals with higher statistical literacy appear to recognize that algorithmic predictions based on pattern recognition and large datasets can be effective and reliable.
However, in high-stakes contexts (such as employment or criminal justice decisions), this same statistical knowledge leads to greater skepticism -not because the algorithms are necessarily less accurate, but because statistically literate individuals better understand the potential consequences of algorithmic biases and limitations. Those with statistical literacy are better equipped to understand that while statistical models may achieve high average accuracy, they can still fail in critical individual cases or perpetuate systemic biases present in training data. This cautious approach to high-stakes algorithmic decisions reflects not just critical thinking, but a deeper understanding of how statistical methods work and where they may fall short.
Paradoxically, explainability only affected people's trust in algorithms when it was absent, the stakes were low, and statistical literacy was high. This contradicts previous findings in the literature, which have shown that interventions focused on explaining the decision-making processes of algorithms can increase the use of and trust in algorithms, for example in healthcare 
[86]
, journalism 
[87]
 and military settings 
[88,
89]
.
One possible reason for this inconsistency could be due to the way we operationalised "explainability" in our study, where the explanations included technical jargon that may have exceeded the expected level of familiarity among participants. However, this may also mean that the information related to the explainability of the algorithm is not related to trust or distrust in the algorithm. Rather than focusing on how an algorithm works, our results suggest that statistically literate individuals primarily consider what the algorithm is being used for -its purpose and potential impactwhen deciding whether to trust it. This finding challenges the common assumption that greater algorithmic transparency necessarily leads to more appropriate trust calibration.
Over time, the concept of statistical literacy has evolved from the understanding and application of statistical techniques to a broader understanding explicitly related to trust in algorithms. Algorithms now consist of thousands of lines of formulae and are increasingly used to make decisions that may be difficult for humans to understand (known as the black box effect). Consequently, statistical literacy now encompasses not only the ability to understand statistical output, but also the skills needed to critically interpret and evaluate statistical information and reasoning, which requires a higher degree of critical thinking. Therefore, the promotion of statistical literacy is essential to ensure that individuals have the necessary skills to understand and interpret statistical information and algorithms and to become critical users of ADA.  Beanplots showing the tendency to trust/recommend/use algorithms as a function of explainability (with or without) and situation stake (high stake = HS or low stake = LS). This figure shows the main effect of the stake factor (S) and the non-significant effect of explainability (e) (recall that this variable was not significant but used for illustrative purposes). The dotted horizontal line represents the grand mean and the four solid horizontal lines represent the groups' means.
with the skills they need to navigate an increasingly data-driven world and make informed decisions based on statistical information and algorithms (but see section 'implications and limitations' below).
Our results showed that older people and men were less likely to trust algorithms than younger people and women. Previous research has shown that certain demographic groups are more likely to trust algorithms than others. However, previous studies have shown that older people tend to trust ADA more than younger people, while gender has been shown to have inconsistent effects (see for example 
[90,
91]
).
These differences may be due to particular characteristics of the study participants, possibly influenced by a bias towards certain aspects of the topic at hand.
In our cross-country analysis, we observed variations in trust in algorithms, with industrialised countries such as Japan, the US, and the UK exhibiting lower levels of trust in AI. This finding aligns with a recent study on trust in AI by 
Gillespie et
 al. 
[34]
, which reported that Japan had one of the lowest levels of trust in AI, while  
1
 and 
Figure 5
), although not statistically significant. This suggests that different methodologies may yield varying perceptions of trust levels across countries.


Implications and limitations
Various machine learning techniques require data work or human intervention in the form of data generation, annotation and algorithmic verification 
[47]
. This labourintensive process is often distributed to teams in business process outsourcing companies (BPOs) or to individuals through labour platforms, reducing production costs 
[92]
.
Miceli and Posada 
[93]
 studied one BPO in Argentina and three platforms operating in Venezuela and found that the discourses and social relations that structured data work were aimed at controlling workers (through managerial approaches in the BPO and algorithms in the platforms) to increase productivity and reduce worker "bias".
The problem is that feedback from workers was discouraged and, by taking clients' decisions as "ground truth", the data production process reproduced clients' biases, which were carried out by algorithms trained on that data. Their research concluded that the quality of the data depended on the voice and engagement of workers, which in turn required decent working conditions and recognition. Even if the data used in the algorithm is well annotated and leads to good algorithmic performance, there is the question of the human ability to interpret these results, as human judgments are modulated by social-emotional processes 
[21,
[94]
[95]
[96]
. Future work should consider the 
Figure 6
. Scatterplot showing the correlation between BLIS scores, explainability and the tendency to trust/recommend/use algorithms as a function of stake level. This figure illustrates the interaction between stake (high stake = HS or low stake = LS) and statistical literacy (BLIS) according to the level of explainability of algorithms (e). The observations on the x-axis are jittered for visualisation purposes.
human and social aspects of data production and make the work visible in documentation efforts 
[97]
. This transparency of the social aspects of datasets will contribute to trust in the operation of algorithms.
While the current findings are indeed informative, it is important to recognize certain limitations that may constrain the generalizability of these results and claims 
[98]
.
We argued that statistical literacy influences trust in both low-and high-stakes scenarios; however, it could be part of a broader understanding of technology, algorithms, and data. Indeed, statistical literacy could be considered a sub-skill of AI literacy if AI literacy is understood as the ability to recognize, understand, use, and critically evaluate AI technologies and their societal impacts, supported by foundational knowledge in statistics and computing. Therefore, policymakers should consider promoting AI literacy to address some of the complexities associated with trust in algorithms.
Our study utilized self-reported measures via rating scales, which are efficient and cost-effective for capturing data on thoughts, feelings, and subjective experiences. How-ever, these measures can be influenced by social desirability, response bias, misinterpretation, or lack of self-awareness. For instance, physiological research has shown that self-reported measures of physical activity can both overestimate and underestimate actual levels of physical activity 
[99]
. Therefore, future extensions of this work should consider a more robust approach, such as triangulating the data with direct observations of user interactions with algorithms or physiological measures to assess trust more accurately.
High-stakes and low-stakes situations exhibit significant variability across individuals and cultures, existing on a context-dependent continuum rather than as discrete categories. For example, choosing a restaurant for dinner with friends may carry different stakes across cultural contexts, socioeconomic backgrounds, and individual preferences. Our study's primary limitation lies in not systematically investigating how participants from different backgrounds interpreted and classified these scenarios.
Additionally, while our sample included participants from 20 countries, certain geographical regions like Central Europe were underrepresented, potentially limiting the generalizability of our findings across different cultural contexts. Although we aimed to move beyond WEIRD (Western, Educated, Industrialized, Rich, and Democratic) sampling biases, more comprehensive geographic and cultural representation, along with larger sample sizes from each region, would be necessary to make broader generalizations about algorithmic trust across diverse populations 
[100,
101]
. Future research should incorporate scenario validation across different cultural contexts and expand sampling to include currently underrepresented regions and demographic groups.


Conclusion
This study investigated the personal and algorithmic factors that affect individuals' trust in algorithms. Our findings revealed that when the stakes are low, statistical literacy is positively correlated with the likelihood of trusting an algorithm. However, when the stakes are high, our results indicated a negative correlation between statistical literacy and the likelihood of trusting an algorithm. Therefore, we conclude that having statistical literacy enables individuals to critically evaluate the decisions made by ADA and consider them alongside other factors before making significant life decisions. This ensures that individuals are not solely relying on algorithms that may not fully capture the complexity and nuances of human behaviour and decision-making.


Disclosure statement
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.


Data availability statement
The data that support the findings of this study as well as the analysis scripts in R are openly available at https://figshare.com/projects/Trust_in_algorithms_ An_experimental_approach_-_Data_repository/156212
Figure 3 .
3
Scenarios relating to algorithms used Two scenarios were created to illustrate different situations in which people interact with algorithms. Half of them represented low-stake situations, i.e. (1) algorithms Illustration of the four experimental conditions to which participants were randomly assigned. D = demographic questions (age, gender, education level, open-ended question about what algorithms are, and VAS rating of participants' level of familiarity with ADA)


Figures 4 and 6
6
show the main results in terms of the main effect of S and the two-way interactions between stake level (S) and statistical literacy (BLIS).


Figure 4
4
shows that the likelihood to trust/recommend/use algorithms is higher in low-stakes than in high-stakes scenarios, regardless of whether the scenarios have some explainability information or not.Figure 6suggests that the likelihood to trust/recommend/use algorithms in low-stakes scenarios increases as the level of statistical literacy increases; however, in high-stakes scenarios, the likelihood to trust decreases as the level of statistical literacy increases.DiscussionThe aim of this study was to investigate the personal characteristics (i.e. statistical literacy and demographics) and algorithmic characteristics (i.e. explainability and levels of stakes of algorithms) that influence people's trust in algorithms. The results showed a negative relationship between statistical literacy and trust in algorithms in high-stakes situations and a positive relationship in low-stakes scenarios. Explainability alone did not influence people's trust in algorithms. These results and their implications are discussed, as well as the limitations of the study.Existing research has explored various factors influencing trust in AI. For instance, Lee et al.
[81]
 highlighted the importance of perceived fairness of algorithms and users' perceptions of algorithm agency and intentionality. Arauju et al.
[82]
 investigated the roles of potential usefulness, fairness, and risk perceptions in users' engagement with algorithms. Cabiddu et al.
[83]
 examined factors such as users' inherent trust propensity and the drivers of information technology acceptance. Aysolmaz et al.
[84]
 focused on algorithm fairness, accountability, and privacy. Similar to the present study, some


341 Signif. codes: *** [0, 0.001], ** (0.001, 0.01], * (0.01, 0.05], . (0.05, 0.1] over 80 empirical studies, none of which included statistical literacy as a factor influencing trust in AI.


Furthermore, our findings
have important implications for policymakers and educators, who should consider incorporating statistical literacy training into school curricula and professional development programs. This can help ensure that individuals are equipped


Figure 4 .
4
Figure 4. Beanplots showing the tendency to trust/recommend/use algorithms as a function of explainability (with or without) and situation stake (high stake = HS or low stake = LS). This figure shows the main effect of the stake factor (S) and the non-significant effect of explainability (e) (recall that this variable was not significant but used for illustrative purposes). The dotted horizontal line represents the grand mean and the four solid horizontal lines represent the groups' means.


Figure 5 .
5
Plot showing the variability in the tendency to trust/recommend/use algorithms across countries. Countries are labelled with Turkey: TR, Colombia: CO, Brazil: BR, India: IN, Spain: ES, Armenia: AM, Italy: IT, Bulgaria: BG, Philippines: PH, Thailand: TH, Nigeria: NG, Taiwan: TW, Indonesia: ID, USA: US, Australia: AU, Cameroon: CM, Czech Republic: CZ, Vietnam: VN, UK: UK, and Japan: JP. The most important predictors for all models in each country were S followed by ADA and BLIS. Error bars represent 95% confidence intervals around the mean. The horizontal line indicates the overall mean. Although the substantial overlap of the confidence intervals suggests no significant statistical pairwise differences, the focus is on ranking countries based on their average tendency to trust algorithms.the US and the UK had intermediate levels. Interestingly, countries such as India andBrazil, which demonstrated high levels of trust in the Gillespie et al. study (seeFigure 2in their report), appear in our linear mixed model with positive estimates (see table


parency, in the sense that all procedures and details used to build, train and test the AI model should be available during its development and use. This principle is part of the four principles endorsed by the OECD
[66]
 and the European Commission's age = 26.03 Â± 9.88 SD; 59.5% women, 38.2% men, 1.8% other).Each participating laboratory obtained ethical approval from its local ethics committee, and data collection began only after ethical approval (the ethics approval for the leading research group in Australia was granted by the University of South Australia, with the approval number 203238. This approval was then used by the other participating laboratories to obtain their own respective ethics approvals). All participants voluntarily accessed the internet link for this study and agreed to participate after reading the information page and agreeing to take part. They were recruited via social media using convenience sampling.MaterialsThis online survey consisted of four sets of questions: (1) a demographic questionnaire in which participants were asked about their first language, country of residence, age, gender, level of education, level of familiarity with ADA (their level of familiarity with ADA was assessed using a visual analogue rating scale (VAS) ranging from 0[not very familiar] to 5 [very familiar] and using up to two decimal places); (2) a VAS rating scale version of the six-item 'propensity to trust scale items' from
[69]
, with a range of responses from 0 (strongly disagree) to 5 (strongly agree), using up to two decimal places; (3) a selection of 14 items (questions
2,
4,
9,
10,
12,
14,
18,
19,
27,
31,
[34]
[35]
[36]
[37]
 from the 37-item Basic Literacy In Statistics (BLIS) scale
[70]
. The 14 items from the BLIS were chosen to cover different statistical concepts equally, i.e. items 2 and 4 relate to data production, items 9 and 10 to graphs, items 12 and 14 to descriptive statistics, items 18 and 19 to sampling distributions, items 27 and 31 to hypothesis testing, items 34 and 35 to the scope of conclusions, and items 36 and 37 to regression and correlation (these items are available in the supplementary material via the Qualtrics files). Finally, (4) 12 scenarios related to situations in which
High Level Expert Group on Artificial Intelligence (HLEG) to guide the development of 'trustworthy' AI: respect for human autonomy, prevention of harm, fairness and accountability [67]. Despite consensus on these four principles, we are still far from creating a legal framework that guarantees accountability mechanisms in AI develop- ment [68]. In this context, our work presents an experimental study that looks at factors that might explain why people trust algorithms, such as: SL, explainability, stake levels, demographics, among others.Methods Participants Data from 3,260 participants were available from 20 countries (Armenia, Australia, Bulgaria, Brazil, Cameroon, Colombia, Czech Republic, Spain, Indonesia, India, Italy, Japan, Nigeria, Philippines, Thailand, Turkey, Taiwan, UK, USA, and Vietnam). How- ever, only participants who provided complete data were included in the analyses (n=1,921) (see Fig. 2, M


to organise and sort emails, or (4) to suggest new restaurants,
(5)
 new clothes, and
(6)
 new music. The other half represented high-stakes situations, i.e. (7) algorithms to support court decisions based on psychological profiles,
(8)
 to select CVs,
(9)
 to make hiring recommendations for a job,
(10)
 to select the best candidate for a position at a university,
(11)
 to control the brakes of autonomous vehicles, and(12)to decide the priority of care in a medical context. Each scenario contained a sentence related to its explainability. These sentences contained information about a specific machine learning method used by the algorithm (e.g. clustering learning methods, classification learning statistical methods, logistic regression methods, dimensionality reduction techniques, supervised statistical methods and clustering statistical methods). The sentence also briefly mentioned the quality of the method.Overall context A new reservation app uses algorithms to make dining recommendations to its users, only revealing the three restaurants in the area available for a reservation that are the best match for your needs. The algorithm is based on information provided to the system by the user about restaurant preferences and requirements.With explainability The algorithm relies on clustering learning methods and has shown a high predictability accuracy across a variety of restaurants.Specific context You decide to use the app to find a recommendation for a dinner with your close friends next Friday. The app produces three restaurants with reservations available at the time you selected.
Scenario 1 -Low stake
The following are examples of two different scenarios used to evaluate trusting
algorithms:
Questions 1. How likely are you to regularly trust this app for decisions regard- ing restaurant reservations? 2. How likely are you to recommend this app for restaurant reservations to others? Scenario 2 -high stake Overall context A new employee selection software uses algorithms to make hiring recommendations to its users, only revealing the top candidates in the candidate pool that are the best match for the company's needs. The algorithm is based on information provided to the system about preferences and requirements for the job. With explainability The algorithm uses clustering statistical techniques and has shown high predictability when selecting candidates. Specific context You decide to use the software to find a recommendation for who to bring in for an onsite interview for an important role in your company. The software produces three recommended candidates who match the criteria. Questions 1. How likely are you to regularly trust this software for decisions regarding hiring? 2. How likely are you to recommend this software for hiring decisions to others?


. The significance level for all statistical tests was set at Î± = 0.05. The model tested was: p âˆ¼ e * S * BLIS + g + a + ADA + c + (1|id) + (1|i) where 'p' is the probability of trusting/recommending/using algorithms, 'e' is the presence of explainability, 'S' is the stake level (i.e. high and low stake), 'BLIS' represents statistical literacy (frequency of correct answers), 'g' represents participant gender, 'a' represents participant age, 'ADA' represents participant familiarity with ADA, 'id' represents subject identification, 'i' represents each of the 12 scenarios, and 'c' represents participant country ('*' represents main effects and interactions.
Only numeric variables are shown in teletype font; other variables are categorical. The variable 'propensity to trust scale' was not added as a covariate as it showed a high correlation with the dependent variable, r (1768) = 0.69, p < 0.001).


Table 1 .
1
Fixed effects for the linear mixed model. The R values correspond to the Nagakawa coefficients
[79]
: R 2 cond = 0.363 and R 2 marg = 0.241. Country names are identified by the ISO 3166 standard. The reference category for the variable 'gender' is female, and the reference category for the variable 'country' is Armenia (AM). Effect sizes for significant variables were estimated following
[80]
 (these values are interpretable as Cohen's d)
Estimate
Std. Error
df
t value
Pr(> |t|)
Effect
size (d)
(Intercept)
1.466e+00
1.157e-01
3.354e+01 12.669
2.46e-14 ***
eWITHOUT
6.231e-02
6.489e-02
2.567e+03 0.96
0.337013
SLS
2.479e-01
3.527e-02
4.350e+04 7.03
2.10e-12 ***
0.217
BLIS
-6.020e-01
1.277e-01
2.457e+03 -4.714
2.57e-06 ***
-0.526
Age
-6.183e-03
1.444e-03
1.917e+03 -4.281
1.95e-05 ***
-0.005
GenderMale
-1.088e-01
2.541e-02
1.889e+03 -4.285
1.92e-05 ***
-0.095
ADA
5.483e-01
1.257e-02
1.895e+03 43.611
< 2e-16 ***
0.480
CountryAU
-1.234e-01
1.018e-01
1.876e+03 -1.213
0.225469
CountryBG
-3.737e-02
6.830e-02
1.880e+03 -0.547
0.584321
CountryBR
8.501e-02
8.216e-02
1.878e+03 1.035
0.300939
CountryCM
-1.420e-01
8.690e-02
1.878e+03 -1.634
0.102445
CountryCO
1.063e-01
1.146e-01
1.877e+03 0.928
0.353747
CountryCZ
-1.467e-01
8.722e-02
1.878e+03 -1.682
0.092771 .
CountryES
9.634e-03
6.978e-02
1.876e+03 0.138
0.890202
CountryID
-8.746e-02
6.744e-02
1.880e+03 -1.297
0.194838
CountryIN
1.470e-02
7.303e-02
1.882e+03 0.201
0.840448
CountryIT
-2.432e-02
6.597e-02
1.989e+03 -0.369
0.712430
CountryJP
-2.351e-01
6.132e-02
1.880e+03 -3.833
0.000131 ***
-0.205
CountryNG
-7.398e-02
7.471e-02
1.879e+03 -0.99
0.322211
CountryPH
-6.087e-02
7.539e-02
1.880e+03 -0.807
0.419529
CountryTH
-6.954e-02
7.514e-02
1.881e+03 -0.925
0.354832
CountryTR
1.614e-01
1.496e-01
1.877e+03 1.079
0.280707
CountryTW
-7.210e-02
7.538e-02
1.881e+03 -0.956
0.338971
CountryUK
-1.577e-01
7.606e-02
1.876e+03 -2.073
0.038301 *
-0.138
CountryUS
-1.184e-01
5.752e-02
1.881e+03 -2.059
0.039599 *
-0.103
CountryVN
-1.397e-01
9.694e-02
1.878e+03 -1.441
0.149782
eWITHOUT:SLS
4.463e-02
5.006e-02
4.353e+04 0.891
0.372671
eWITHOUT:BLIS
2.067e-02
1.695e-01
2.565e+03 0.122
0.902951
SLS:BLIS
1.225e+00
9.136e-02
4.349e+04 13.406
< 2e-16 ***
1.071
eWITHOUT:SLS:BLIS -3.898e-01
1.302e-01
4.351e+04 -2.994
0.002755 **
-0.


Table 2 .Table 3 .
23
Random effects for the linear mixed model. The variance explained by the random factors (estimated via the function gstudy in the gtheory R package) were: ID=16.3% and Item 2.3%. Analysis of Deviance Table (Type III Wald Ï‡ 2 tests) for the fixed effects of the model with the best fit.
Groups
Name
Variance
Std.Dev.
ID
(Intercept)
0.21380
0.4624
Item
(Intercept)
0.03076
0.1754
Residual
1.06489
1.0319
Ï‡ 2
Df
Pr(> Ï‡ 2 )
(Intercept)
160.5147
1
< 2.2e-16 ***
e
0.9221
1
0.3369223
S
49.4186
1
2.068e-12 ***
BLIS
22.2199
1
2.431e-06 ***
Age
18.3263
1
1.861e-05 ***
Gender
18.3575
1
1.831e-05 ***
ADA
1901.9323
1
< 2.2e-16 ***
Country
47.7956
19
0.0002746 ***
e:S
0.7948
0.3726661
e:BLIS
0.0149
0.9029413
S:BLIS
179.7312
1
< 2.2e-16 ***
e:S:BLIS
8.9643
1
0.0027531 **
Signif. codes: *** [0, 0.001], ** (0.001, 0.01], * (0.01, 0.05], . (0.05, 0.1]








An earlier version of this manuscript can be found at https://osf.io/preprints/ psyarxiv/9wh2f. We recommend referring to and citing the current version rather than the earlier one.
 










Automating Public Services and Supporting Civil Servants in using Data Science Technologies




Z
Engin






P
Algorithmic
Treleaven






Government




10.1093/comjnl/bxy082








The Computer Journal




62


3
















Modelling fatality curves of COVID-19 and the effectiveness of intervention strategies




G
L
Vasconcelos






Ams
MacÃªdo






R
Ospina






Fag
Almeida






G
C
Duarte-Filho






A
A
Brum




https://www.medrxiv.org/content/10.1101/2020.04.02.20051557v3


doi:10.1101/2020.04.02.20051557


















Mapping the landscape of Artificial Intelligence applications against COVID-19




J
Bullock






A
Luccioni






K
H
Pham






Csn
Lam






M
Luengo-Oroz




10.1613/jair.1.12162








Journal of Artificial Intelligence Research




69
















The Social Power of Algorithms. Information




D
Beer




10.1080/1369118X.2016.1216147






Communication & Society




20


1














Prioritizing Homeless Assistance Using Predictive Algorithms: An Evidence-Based Approach




H
Toros






D
Flaming










Cityscape




20


1










Publisher: US Department of Housing and Urban Development








Prediction of respiratory decompensation in Covid-19 patients using machine learning: The READY trial




H
Burdick






C
Lam






S
Mataraso






A
Siefkas






G
Braden






R
P
Dellinger




10.1016/j.compbiomed.2020.103949








Computers in Biology and Medicine




124


103949














Undergraduate Topics in Computer Science




W
Ertel




http://link.springer.com/10.1007/978-3-319-58487-4


doi:10.1007/978-3-319-58487-4








Springer International Publishing


Cham






Introduction to Artificial Intelligence








Some Studies in Machine Learning Using the Game of Checkers




A
L
Samuel




10.1147/rd.33.0210








Conference Name: IBM Journal of Research and Development






3














Machine learning: Trends, perspectives, and prospects




M
I
Jordan






T
M
Mitchell




https://www.science.org/doi/10.1126/science.aaa8415








Science




349


6245






American Association for the Advancement of Science












Portrayals and perceptions of AI and why they matter




S
Cave






C
Craig






K
Dihal






S
Dillon






J
Montgomery






B
Singler










DES5612. The Royal Society












Artificial Intelligence: Australia's Ethics Framework




D
Dawson






E
Schleiger






J
Horton






J
Mclaughlin






C
Robinson






G
Quezada






61
















Australia




















The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions




J
Whittlestone






R
Nyrup






A
Alexandrova






S
Cave




10.1145/3306618.3314289








Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. AIES '19


the 2019 AAAI/ACM Conference on AI, Ethics, and Society. AIES '19
New York, NY, USA




Association for Computing Machinery
















Behavioural finance in an era of artificial intelligence: Longitudinal case study of robo-advisors in investment decisions




M
Shanmuganathan




10.1016/j.jbef.2020.100297








Journal of Behavioral and Experimental Finance




27


100297














Human and artificial cognition. Computers and Education: Artificial Intelligence




G
Siemens






F
Marmolejo-Ramos






F
Gabriel






K
Medeiros






R
Marrone






S
Joksimovic




10.1016/j.caeai.2022.100107




2022


100107
















M
A
Devito




10.1080/21670811.2016.1178592


doi:10.1080/21670811.2016.1178592








5








Publisher: Routledge eprint








Organizing music, organizing gender: algorithmic culture and Spotify recommendations




A
Werner




10.1080/15405702.2020.1715980


doi:10.1080/15405702.2020.1715980








18








Publisher: Routledge eprint


Popular Communication








Improving Itinerary Recommendations for Tourists Through Metaheuristic Algorithms: An Optimization Proposal




M
Tenemaza






S
LujÃ¡n-Mora






De
Antonio






A
RamÃ­rez






J




10.1109/ACCESS.2020.2990348






Conference Name: IEEE Access






8














Video surveillance systems-current status and future trends




V
Tsakanikas






T
Dagiuklas




10.1016/j.compeleceng.2017.11.011








Computers & Electrical Engineering




70
















A comprehensive review on intelligent traffic management using machine learning algorithms. Innovative Infrastructure Solutions




Y
Modi






R
Teli






A
Mehta






K
Shah






M
Shah




10.1007/s41062-021-00718-3


doi:10.1007/s41062-021-00718-3








7


128
















T
B
Brown






B
Mann






N
Ryder






M
Subbiah






J
Kaplan






P
Dhariwal




10.48550/arXiv.2005.14165








Language Models are Few-Shot Learners












All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text




E
Clark






T
August






S
Serrano






N
Haduong






S
Gururangan






N
A
Smith








arXiv










10.48550/arXiv.2107.00061














Plagiarism in the age of massive Generative Pre-trained Transformers (GPT-3). Ethics in Science and Environmental Politics




N
Dehouche




10.3354/esep00195








21














Will Human Beings Be Superseded by




Mib
Ugli




10.31149/ijot.v2i10.769








International Journal on Orange Technologies




3


10






Publisher: Research Parks Publishing






Generative Pre-trained Transformer








Building Trusted Startup Teams From LinkedIn Attributes: A Higher Order Probabilistic Analysis




G
Drakopoulos






E
Kafeza






P
Mylonas






H
Katheeri




10.1109/ICTAI50040.2020.00136






2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI); 2020. p
















Application of machine learning and data visualization techniques for decision support in the insurance sector




S
Rawat






A
Rawat






D
Kumar






A
S
Sabitha




10.1016/j.jjimei.2021.100012








International Journal of Information Management Data Insights




1


2


100012














Self-driving cars: A survey




C
Badue






R
Guidolini






R
V
Carneiro






P
Azevedo






V
B
Cardoso






A
Forechi








Expert Systems with Applications














10.1016/j.eswa.2020.113816






165


113816












Distrust of Artificial Intelligence: Sources & Responses from Computer Science & Law




C
Dwork






M
Minow




10.1162/daed_a_01918


doi:10.1162/daeda01918








Daedalus




151


2
















Learning from the Science of Cognition and Perception for Decision Making


Debad SJ






National Academies Press


Washington, DC












Understanding algorithm aversion: When is advice from automation discounted? Journal of Forecasting




A
Prahl






L
Van Swol




https://onlinelibrary-wiley.ez20.periodicos.capes.gov.br/doi/full/10.1002/for.2464








Publisher: John Wiley & Sons, Ltd






36














Psychological reactions to human versus robotic job replacement




A
Granulo






C
Fuchs






S
Puntoni




10.1038/s41562-019-0670-y








Nature Human Behaviour




3


10






Nature Publishing Group






Number: 10 Publisher








Toward understanding the impact of artificial intelligence on labor




M
R
Frank






D
Autor






J
E
Bessen






E
Brynjolfsson






M
Cebrian






D
J
Deming




https://www.pnas.org/doi/10.1073/pnas.1900949116








Proceedings of the National Academy of Sciences


the National Academy of Sciences






116














Machine learning in medicine: Addressing ethical challenges




E
Vayena






A
Blasimme






I
G
Cohen




https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002689








PLOS Medicine




15


11




e1002689. Publisher: Public Library of Science












Ensuring Fairness in Machine Learning to Advance Health Equity




A
Rajkomar






M
Hardt






M
D
Howell






G
Corrado






M
H
Chin








Annals of Internal Medicine
















Publisher: American College of Physicians


https://www.acpjournals.org/doi/abs/10.7326/M18-1990


doi:10.7326/M18-1990






169














Trust in Artificial Intelligence: A global study. The University of Queensland and KPMG Australia




N
Gillespie






S
Lockey






C
Curtis






J
Pool






A
Akbari




10.14264/00d3c94


















Myths and realities of computerphobia: A metaanalysis




L
D
Rosen






P
Maguire




10.1080/08917779008248751


doi:10.1080/08917779008248751








Publisher: Routledge eprint




3


3










Anxiety Research.








Fear of Artificial Intelligence on People's Attitudinal & Behavioral Attributes: An Exploratory Analysis of A.I. Phobia. International European Extended Enablement in Science, Engineering & Management (IEEESEM)




J
Kim








7














Examining its hidden factors and defining it




O
Y
Khasawneh






Technophobia




10.1016/j.techsoc.2018.03.008








Technology in Society




54


















-Ai
Human






Teaming




10.17226/26355




State-of-the-Art and Research Needs


Washington, D.C






National Academies Press; 2022








Friend or frenemy? The role of trust in human-machine teaming and lethal autonomous weapons systems




A
Warren






A
Hillas




10.1080/09592318.2020.1743485


doi:10.1080/09592318.2020.1743485








Publisher: Routledge eprint






31














Fear of Autonomous Robots and Artificial Intelligence: Evidence from National Representative Data with Probability Sampling




Y
Liang






S
A
Lee




10.1007/s12369-017-0401-3








International Journal of Social Robotics




9


3
















Are robots becoming unpopular? Changes in attitudes towards autonomous robotic systems in Europe




T
Gnambs






M
Appel








Computers in Human Behavior


















10.1016/j.chb.2018.11.045






93














Google engineer put on leave after saying AI chatbot has become sentient. The Guardian




R
Luscombe




















GPT-3: What's it good for? Natural Language Engineering




R
Dale






2021












Publisher




Cambridge University Press


27
















10.1017/S1351324920000601














Can artificial intelligence be decolonized? Interdisciplinary Science Reviews




R
Adams




Taylor & Francis eprint






46
















10.1080/03080188.2020.1840225


1080/03080188.2020.1840225. doi:10.1080/03080188.2020.1840225














On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?




E
M
Bender






T
Gebru






A
Mcmillan-Major






S
Shmitchell




10.1145/3442188.3445922


doi:10.1145/3442188.3445922








Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21


the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21
New York, NY, USA




Association for Computing Machinery
















The growing ubiquity of algorithms in society: implications, impacts and innovations




S
C
Olhede






P
J
Wolfe




https://royalsocietypublishing.org/doi/full/10.1098/rsta.2017.0364








Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences




376




Royal Society












The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence




P
Tubaro






A
A
Casilli






M
Coville




http://journals.sagepub.com/doi/10.1177/2053951720919776


doi:10.1177/2053951720919776








Big Data & Society




7


1














Mapping AI Issues in Media Through NLP Methods




M
CrÃ©pel






S
Do






J
P
Cointet






D
Cardon






Y
Bouachera






M
Ehrmann






F
Karsdorp






M
Wevers






T
L
Andrews






M
Burghardt






M
Kestemont










Proceedings of the Conference on Computational Humanities Research 2021


the Conference on Computational Humanities Research 2021




2989








CEUR Workshop Proceedings. Amsterdam, the Netherlands: CEUR; 2021








AIpowered narrative building for facilitating public participation and engagement




F
Marmolejo-Ramos






T
Workman






C
Walker






D
Lenihan






S
Moulds






J
Correa




10.1007/s44163-022-00023-7








Disover Artificial Intelligence




2022


7












Behind the scenes of K-pop fandom: unveiling K-pop fandom collaboration network




J
Kang






J
Kim






M
Yang






E
Park






M
Ko






M
Lee








Quality & Quantity




2022














10.1007/s11135-021-01189-5


doi:10.1007/s11135-021-01189-5






56
















Milton
Milton
Beck






Beck On Linkedin






Atualizamos nossa polÃ­tica global de anÃºncios de vagas para permitir; 2022. LinkedIn. Available from












Big-data literacy as a new vocation for statistical literacy




K
FranÃ§ois






C
Monteiro






P
Allo




10.52041/serj.v19i1.130








Statistics Education Research Journal




19
















Statistical Literacy: A Guide to Interpretation




D
G
Haack








North Scituate, Massachusetts: Duxbury












Enhancing Statistical Literacy: Enriching Our Society




K
K
Wallman




10.2307/2290686








Publisher: [American Statistical Association


Taylor & Francis, Ltd






88














Assessing Statistical Thinking Using the Media




J
Watson








The Assessment Challenge in Statistics Education. Amsterdam: IOSPress
















10.2307/1403713




Gal I. Adults' Statistical Literacy: Meanings, Components, Responsibilities. International Statistical Review / Revue Internationale de Statistique




Wiley




70








International Statistical Institute (ISI)












Oecd






Pisa










Assessment and Analytical Framework: Mathematics, Reading, Science, Problem Solving and Financial Literacy






Organisation for Economic Co-operation and Development












Is there a role for statistics in artificial intelligence?




S
Friedrich






G
Antes






S
Behr






H
Binder






W
Brannath






F
Dumpert








Advances in Data Analysis and Classification




2022














10.1007/s11634-021-00455-6


doi:10.1007/s11634-021-00455-6






16


















D
V
Carvalho






E
M
Pereira






J
S
Cardoso




10.3390/electronics8080832








Machine Learning Interpretability: A Survey on Methods and Metrics. Electronics




8


8


832




Multidisciplinary Digital Publishing Institute














M
Du






N
Liu






X
Hu




10.48550/arXiv.1808.00033


arXiv:1808.00033




Techniques for Interpretable Machine Learning. arXiv; 2019. Number






cs, stat








The Black Box Society: The Secret Algorithms That Control Money and Information




Pasquale
Poluomf








Harvard University Press


Cambridge, Massachusetts London, England






Reprint ediÃ§Ã£o ed








Association for Computing Machinery




M
Mitchell






S
Wu






A
Zaldivar






P
Barnes






L
Vasserman






B
Hutchinson




10.1145/3287560.3287596


doi:10.1145/3287560.3287596








Proceedings of the Conference on Fairness, Accountability, and Transparency. FAT* '19


the Conference on Fairness, Accountability, and Transparency. FAT* '19
New York, NY, USA












Model Cards for Model Reporting








REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets




A
Wang






A
Liu






R
Zhang






A
Kleiman






L
Kim






D
Zhao




10.1007/s11263-022-01625-5








International Journal of Computer Vision




130


7
















The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models




G
Eon






J
Eon






J
R
Wright






K
Leyton-Brown




10.1145/3531146.3533240








2022 ACM Conference on Fairness, Accountability, and Transparency. FAccT '22


New York, NY, USA




Association for Computing Machinery
















On the risk of confusing interpretability with explicability. AI and Ethics




C
Herzog




10.1007/s43681-021-00121-9


doi:10.1007/s43681-021-00121-9








2
















Forty-two countries adopt new OECD Principles on Artificial Intelligence -OECD
















The Ethics of AI Ethics: An Evaluation of Guidelines. Minds and Machines




T
Hagendorff




10.1007/s11023-020-09517-8








30














Principles alone cannot guarantee ethical AI. Nature Machine Intelligence




B
Mittelstadt




10.1038/s42256-019-0114-4








Nature Publishing Group


1








Number: 11 Publisher








I Trust It, but I Don't Know Why: Effects of Implicit Attitudes Toward Automation on Trust in an Automated System




S
M
Merritt






H
Heimbaugh






J
Lachapell






D
Lee
















10.1177/0018720812465081


doi:10.1177/0018720812465081








Human Factors




55


3






Publisher: SAGE Publications Inc












Developing a statistical literacy assessment for the modern introductory statistics course




L
Ziegler






J
Garfield




10.52041/serj.v17i2.164








Statistics Education Research Journal




17


2
















Automated mixed ANOVA modeling of sensory and consumer data




A
Kuznetsova






R
Christensen






C
Bavay






P
Brockhoff




10.1016/j.foodqual.2014.08.004






Food Quality and Preference




40
















Tests in Linear Mixed Effects Models




A
Kuznetsova






P
Brockhoff






R
Christensen






Lmertest Package




10.18637/jss.v082.i13


doi:10.18637/jss.v082.i13








Journal of Statistical Software




82


13
















AIC model selection and Akaike weights




E
J
Wagenmakers






S
Farrell




10.3758/BF03206482






Psychonomic Bulletin & Review




11


1
















performance: An R Package for Assessment, Comparison and Testing of Statistical Models




D
LÃ¼decke






M
Ben-Shachar






I
Patil






P
Waggoner






D
Makowski




10.21105/joss.03139






Journal of Open Source Software




6


60


3139














Generalizability Theory in R. Practical Assessment, Research, and Evaluation




A
Huebner






M
Lucht




10.7275/5065-gc10






24






University of Massachusetts Amherst Libraries






Number: 1 Publisher








Global Validation of Linear Model Assumptions




E
A
PeÃ±a






E
H
Slate




10.1198/016214505000000637






Journal of the American Statistical Association




101


473
















robustlmm: An R Package for Robust Estimation of Linear Mixed-Effects Models




M
Koller




10.18637/jss.v075.i06






Journal of Statistical Software




75


6
















Robustness of linear mixed-effects models to violations of distributional assumptions




H
Schielzeth






N
J
Dingemanse






S
Nakagawa






D
F
Westneat






H
Allegue






C
Teplitsky




10.1111/2041-210X.13434






Methods in Ecology and Evolution




11


9
















The coefficient of determination R 2 and intraclass correlation coefficient from generalized linear mixed-effects models revisited and expanded




S
Nakagawa






Pcd
Johnson






H
Schielzeth




https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0213








Journal of The Royal Society Interface




14


134




Royal Society












Power Analysis and Effect Size in Mixed Effects Models: A Tutorial




M
Brysbaert






M
Stevens




10.5334/joc.10








Journal of Cognition




1


1


9














Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management




M
K
Lee








Big Data & Society
















10.1177/2053951718756684


2053951718756684. Publisher: SAGE Publications Ltd. Available from: https




5
















//
Doi




10.1177/2053951718756684


doi:10.1177/2053951718756684














T
Araujo






N
Helberger






S
Kruikemeier






C
H
De Vreese




AI we trust? Perceptions about automated decision-making by artificial intelligence. AI & SOCIETY














10.1007/s00146-019-00931-w






35














Why do users trust algorithms? A review and conceptualization of initial trust and trust over time




F
Cabiddu






L
Moi






G
Patriotta






D
G
Allen




10.1016/j.emj.2022.06.001








European Management Journal




40


5
















The public perceptions of algorithmic decisionmaking systems: Results from a large-scale survey




B
Aysolmaz






R
MÃ¼ller






D
Meacham








Telematics and Informatics




2023














10.1016/j.tele.2023.101954






79


101954












What influences algorithmic decision-making? A systematic literature review on algorithm aversion




H
Mahmud






Akmn
Islam






S
I
Ahmed






K
Smolander
















10.1016/j.techfore.2021.121390








Technological Forecasting and Social Change




175


121390














Understanding, explaining, and utilizing medical artificial intelligence. Nature Human Behaviour




R
Cadario






C
Longoni






C
K
Morewedge




10.1038/s41562-021-01146-0








Nature Publishing Group


5








Number: 12 Publisher








The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI




D
Shin








International Journal of Human-Computer Studies




2021














10.1016/j.ijhcs.2020.102551






146


102551














H
F
Neyedli






J
G
Hollands






G
A
Jamieson




Beyond identity: incorporating system reliability information into an automated combat identification system. Human Factors


















10.1177/0018720811413767




53














The Effects of Design Features on Users' Trust in and Reliance on a Combat Identification System




L
Wang






G
A
Jamieson






J
G
Hollands




https://journals.sagepub.com/doi/abs/10.1177/1071181311551077


doi:10.1177/1071181311551077








Proceedings of the Human Factors and Ergonomics Society Annual Meeting


the Human Factors and Ergonomics Society Annual Meeting




SAGE Publications Inc




55














Trust in automation: integrating empirical evidence on factors that influence trust. Human Factors




K
A
Hoff






M
Bashir




10.1177/0018720814547570






57














Understanding the Effect of Workload on Automation Use for Younger and Older Adults. Human factors




S
E
Mcbride






W
A
Rogers






A
D
Fisk




10.1177/0018720811421909








53














The Platformisation of Labor and Society




A
A
Casilli






J
Posada








Society and the Internet


Graham M, Dutton WH


Oxford




Oxford University Press




2












The Data Production Dispositif




M
Miceli






J
Posada




10.48550/arXiv.2205.11963






Proceedings of the ACM on Human-Computer Interaction




6


CSCW2














Perceived communicative context and emotional content amplify visual word processing in the fusiform gyrus




S
Schindler






M
Wegrzyn






I
Steppacher






J
Kissler




10.1523/JNEUROSCI.3346-14.2015






The Journal of Neuroscience: The Official Journal of the Society for Neuroscience




35


15
















People matter: Perceived sender identity modulates cerebral processing of socio-emotional language feedback




S
Schindler






J
Kissler




10.1016/j.neuroimage.2016.03.052






NeuroImage




134
















Emergent Analogical Reasoning in Large Language Models




T
Webb






K
J
Holyoak






H
Lu




10.48550/arXiv.2212.09196














Documenting Data Production Processes: A Participatory Approach for Data Work




M
Miceli






T
Yang






Alvarado
GarcÃ­a






A
Posada






J
Wang






S
M
Pohl






M




10.1145/3555623






Proceedings of the ACM on Human-Computer Interaction




6


CSCW2














Constraints on Generality (COG): A Proposed Addition to All Empirical Papers




D
J
Simons






Y
Shoda






D
S
Lindsay




10.1177/1745691617708630






Perspectives on Psychological Science




12


6
















A comparison of direct versus self-report measures for assessing physical activity in adults: a systematic review




S
A
Prince






K
B
Adamo






M
E
Hamel






J
Hardt






S
Gorber






M
Tremblay




10.1186/1479-5868-5-56






International Journal of Behavioral Nutrition and Physical Activity




5


56














Replicability, Robustness, and Reproducibility in Psychological Science




B
A
Nosek






T
E
Hardwicke






H
Moshontz






A
Allard






K
S
Corker






A
Dreber




10.1146/annurev-psych-020821-114157








Annual Review of Psychology




73


1
















The generalizability crisis




T
Yarkoni




e1. Publisher






Behavioral and Brain Sciences




45




Cambridge University Press






Available from: https










10.1017/S0140525X20001685















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]