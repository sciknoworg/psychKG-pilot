You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Algorithms are rapidly diffusing through healthcare systems 1 providing support for outpatient services (e.g., telehealth), and supply to match demand for inpatient care services 
[2]
[3]
[4]
 .
Algorithmic-based healthcare services (i.e., medical "artificial intelligence") are cost-effective, scalable and provide expert-level accuracy in applications ranging from the detection of skin cancer 5 , emergency department triage 
6,
7
 , to diagnoses of COVID-19 from chest X-rays 8 . Adoption of medical AI is critical for providing affordable, high quality healthcare in both the developed and developing world 9 . However, large scale adoption of AI hinges not only on adoption by healthcare systems and providers, but also on patient utilization, and patients are reluctant to utilize medical AI 
[10]
[11]
[12]
 . Patients view medical AI as unable to meet their unique needs 10 , to perform more poorly than comparable human providers 12 , and feel it is harder to hold AI providers accountable for mistakes than comparable human providers 11 .
We propose that another important barrier to adoption of medical AI is its perceived opacity-that it is a "black box" 
13,
14
 . People do not subjectively understand how algorithms make medical decisions, and this impairs their utilization. Moreover, we suggest that this barrier does not rest solely on the perceived opacity of AI. It is also grounded in an illusory understanding of the medical decisions made by human providers. We theorize that people erroneously believe they better understand the decision making of human than AI healthcare providers, and this erroneous belief contributes to an algorithm aversion in healthcare utilization (i.e., greater reluctance to utilize healthcare delivered by AI providers than by comparable human providers).
Our theory is grounded in a reality (i.e., objective knowledge: what people actually know) and an illusion (i.e., subjective knowledge: what people believe they know). Decision processes used by algorithms are perceived to be opaque by patients, because they are often actually opaque to medical professionals and even their creators, who cannot not explain them 
15,16 (i.e., subjective
 and objective knowledge of algorithmic decision processes are similarly poor). We argue that decisions made by human providers appear more transparent, but this perception is an illusion (i.e., subjective knowledge of human decision processes is greater than objective knowledge).
The perceived transparency of human decision making stems from a belief that introspection provides direct access into the psychological processes by which people make decisions 
17
 . But people actually lack access to their own associative machinery 
18,
19
 , and rely on heuristics to understand the decision making of other people 20 . Human decision making is often as much a black box as decisions made by algorithms.
We suggest that these coupled effects make people aware of their limited understanding of medical decisions made by algorithmic providers, but also lead them to overestimate their understanding of (similarly opaque) decisions made by human providers. Indeed, we find that people claim greater subjective understanding for medical decisions made by human than AI providers (i.e., people feel that they know more), but they exhibit similar objective understanding of decisions made by both providers (i.e., people actually know as little about both decision processes). We propose that, in turn, poorer subjective understanding of decisions made by AI than human healthcare providers underlies reluctance to utilize medical AI. Subjective understanding plays a critical role in the adoption of innovations, from new policies to consumer products, and facilitates adoption even when people have little objective understanding of their benefits or how innovations work 
21,
22
 .
We tested our predictions in a set of five pre-registered online experiments with nationally representative and convenience samples, as well as one pre-registered online field study on Google Ads. We find that subjective understanding drives healthcare provider utilization. Moreover, we find that greater subjective understanding of medical decisions made by human than AI healthcare providers contributes to algorithm aversion, but it can be remediated with cost-effective interventions that increase utilization of algorithmic providers without reducing the utilization of human providers. All experiments were approved for use with human subjects by Institutional Review Boards. All conditions, measures, and exclusions are reported; data and pre-registrations are available at: https://osf.io/taqp8/?view_only=c2ac4a617d424a708982e6825a2e149c.


Results


Study 1
Our theory proposes that the greater subjective understanding people claim for medical decisions made by human than algorithmic healthcare providers is due to illusory understanding of decisions made by human providers. Making people explain a medical decision made by a human or algorithmic provider should then reduce subjective understanding of that decision more when the provider is human.
We recruited a nationally representative online sample of 297 US residents (Mage = 45.37; 53% female) from Lucid. We used a 2 (provider: human, algorithm) x 2 (rating order: preintervention, post-intervention) mixed design. Participants were randomly assigned to one of the two provider conditions between-subjects, and ratings of subjective understanding were repeated within-subjects.
We first described triage of a potentially cancerous skin lesion with visual inspection and referred to the healthcare provider as a "doctor" or "algorithm" depending on provider condition. For example, "A doctor [algorithm] will examine the scans of your skin to identify cancerous skin lesions." If risk was assessed to be high, the provider would refer the patient to a dermatologist to determine the appropriate course of action. (see Supplementary Appendix 1 for introductory stimuli used in all studies).
Next, all participants provided a first rating of their subjective understanding of the provider's decision making with a measure adapted from relevant prior research 
22,
23
 ("To what extent do you understand how a doctor [algorithm] examines the scans of your skin to identify cancerous skin lesions?" 1 = not at all, 7 = completely). To test the degree to which this rating was illusory, participants then generated a mechanistic explanation of the provider's decision process (see Supplementary Appendix 2 for instructions). Similar interventions have been successfully implemented to shatter the illusion of understanding in other domains 
23
 . After completing the intervention, participants provided a second rating of their subjective understanding of provider's decision process. Greater reductions in pre-intervention to postintervention ratings indicate a larger illusion of understanding.
To test whether participants exhibited a greater illusion of explanatory depth for the decision processes used by human than algorithmic providers, we regressed subjective understanding on provider (0 = algorithm, 1 = doctor), rating order (0 = pre-intervention, 1 = post-intervention), and their interaction. The analyses revealed the predicted significant interaction, (ΔH -ΔA = -.40, z = -2.61, p = .009; 
Figure 1
). Participants claimed greater subjective understanding of a human than an algorithmic decision processes (pre-intervention: β = 1.08, z = 5.02, p < .001; post-intervention: β = .69, z = 3.19, p = .001). More important, the intervention significantly reduced reported subjective understanding of decisions made by human providers (Mpre = 5.08 vs. Mpost = 4.52, ΔH = -.55, z = -5.27, p < .001), but not of decisions made by algorithmic providers (Mpre = 3.99 vs. Mpost = 3.96, ΔA = -.16, z = -1.47, p = .14). These results hold when including all control variables in the regression, see Supplementary Appendix 3.


Studies 2A & 2B
In Studies 2A-B, we directly test our theory against an obvious alternative explanation, that greater subjective understanding for human than algorithmic healthcare providers is due to differences in objective understanding. We predicted that people claim greater subjective understanding of decisions made by human than algorithmic providers, but possess a similar objective understanding of decisions made by human and algorithmic providers.
We recruited nationally representative samples of US residents (2A: N = 400, Mage = 45.36, 52% female; 2B: N = 403, 45.16, 52% female) from Lucid. Participants were randomly assigned to 1 of 4 conditions in a 2 (provider: human, algorithm) x 2 (understanding type: subjective, objective) between-subject design.
In the same skin cancer detection context as Study 1, we manipulated whether the provider was a "primary care physician" or "machine learning algorithm." Next, we measured either objective or subjective understanding of that medical decision.
In objective understanding assessment conditions, we measured participants' actual understanding of the process by which providers identity cancerous skin lesions. To do this, we developed a quiz in consultation with experts in the relevant medical domain (preliminary visual inspection of moles): a team of dermatologists at a medical school in the Netherlands, and a team of developers of a popular skin cancer detection application in Europe. We identified at least three objective differences in the approach used by primary care physicians versus machine learning algorithms to perform a visual inspection. First, primary care physicians generally consider a predetermined set of features such as the lesion's Asymmetry, Border, Color, Diameter 
24
 , whereas machine learning algorithms do not need to extract these visual features. They use raw pixels to determine visual similarity between the lesion and other lesions 5 . Second, primary care physicians generally use a decision tree in which mole features are considered separately and often sequentially 
25,
26
 , whereas machine learning algorithms consider all features simultaneously.
Third, primary care physicians make a binary decision, assigning the combination of visual symptoms to a low-risk or a high-risk category, whereas machine learning algorithms compute a more granular probability of malignancy 5although medical AI services may communicate a dichotomized low/high risk result to patients.
Building on these 3 differences, we created a 3-item multiple-choice test intended to assess objective understanding. Each test item had 1 correct answer for human providers, 1 (different) correct answer for machine learning algorithms, and 1 incorrect distractor answer (Cronbach's α = .65 in the human condition and α = .67 in the algorithm condition). We scored objective understanding by summing the correct answers; thus, scores ranged from 0 to 3 (Algorithm condition: m = 1.33, se = .09, human condition: m = 1.34 and se = .09).
In subjective understanding assessment conditions in Study 2A, participants reported their subjective understanding of the process by which a primary care physician [machine In subjective understanding conditions in Study 2B, we used a measure of subjective understanding more similar in to the measure of objective understanding, a 3-item binary choice measure using more concrete descriptions of each facet of the decision process, in which participants reported whether (1) or not (0) they subjectively understood the criteria, process, and output of the decision process (See Supplementary Appendix 4). We scored subjective understanding in Study 2B by summing points for all 3 items; scores ranged from 0 to 3
We transformed raw measures of subjective and objective understanding into separate z-scores ( 
Figure 2
). We then compared them by regressing understandings on provider (algorithm = 0, human = 1), understanding type (subjective = 0, objective = 1), and their interaction.
Participants claimed greater subjective understanding of medical decisions made by human than algorithmic providers (2A: β = -.82, t = -6.01, p < .001: 2B: β = -.46, t = -3.33, p = .001), whereas their objective understanding did not differ by provider (2A: p = .91; 2B: p = .43), creating the predicted provider × understanding interaction (2A: β = -.80, t = -4.19, p < .001; 2B: β = -.35, t = -1.78, p = .08). Note that in Study 2B, excluding the participants who failed the manipulation checks increased the interaction coefficient to β = -.70, t = -2.99, p = .003 (Supplementary Appendix 3). Results from both Studies 2A and 2B held when including all control variables in the regressions, see Supplementary Appendix 3.
Taken together with the results of Study 1, medical decisions made by both human and algorithmic providers appear to objectively be a "black box." However, people are more aware of their limited understanding of medical decisions made by algorithmic than human providers.


Studies 3A & 3B
Next, we examined if algorithm aversion in healthcare utilization would be reduced by interventions that successfully reduced differences in the subjective understanding of medical decisions made by human and algorithmic providers. As two main decision processes are used to make mole malignancy risk assessments, and as human and algorithmic providers are each likely to favor one of these two decision processes, we again relied on a team of dermatological experts to develop two different interventions.
In Study 3A, the intervention reduced differences in subjective understanding by explaining how all providers examine mole features (e.g., asymmetry, color) to make a malignancy risk assessment. This decision process better encapsulates how a human provider is likely to assess risk, although it is not inaccurate with respect to an algorithmic provider. In Study 3B, the intervention reduced differences in subjective understanding by explaining how all providers make a malignancy risk assessment; by examining the visual similarity between a target mole and other moles known to be malignant. This decision process better encapsulates how an algorithmic provider is likely to assess risk, although it is not inaccurate with respect to a human provider.
We recruited 1599 US residents (3A: N = 801, Mage = 36.9, 50.0% female and 3B: N = 801, Mage = 45.3, 51.1% female) from Amazon Mechanical Turk (3A) and Lucid (3B). In both studies, participants were randomly assigned to 1 of 4 conditions in a 2 (provider: human, algorithm) x 2 (intervention: control, intervention) between-subject design.
We used the same skin cancer detection context as Study 1 and 2 and manipulated whether the provider was a human doctor or an algorithm (Supplementary Appendix 1). Then, participants in the control condition reported their subjective understanding and healthcare utilization intentions for that provider. Subjective understanding was measured in Study 3A with the item used in Study 2A. In Study 3B, it was measured with the item used in Study 1.
Healthcare utilization intentions were measured in Study 3A with the item, "How likely would you be to utilize a healthcare service that relies on a doctor [algorithm] to identify cancerous skin lesions?" scored on a 7-point scale with endpoints 1 = not at all likely and 7 = very likely. In Study 3B, they were measured with the item, "How likely would you be to utilize a healthcare service that relies on a primary care physician [machine learning algorithm] to identify cancerous skin lesions?" scored on a 7-point scale with endpoints 1 = not at all likely and 7 = very likely.
Participants in the intervention condition first rated their subjective understanding, and then read supplementary information that described how doctors [algorithms] diagnose skin cancer based on photographs of moles, in a single diagram (see 
Figure 3
). Next, they reported their subjective understanding of the medical decision again. Finally, they reported their healthcare utilization intentions.
We first examined the effectiveness of the intervention for each kind of provider. It increased subjective understanding of the decision made by algorithmic providers (3A: Δ = 1.42, t = 12.51, p < .001; 3B: Δ = .56, t = 6.21, p < .001) and human providers (3A: Δ = 1.04, t = 11.08, p < .001; 3B: Δ = 1.39, t = 11.85, p < .001). Second, we compared the extent to which the intervention reduced differences in subjective understanding across providers by comparing ratings in the control condition to post-intervention subjective understanding ratings in the intervention condition. To this end, we regressed subjective understanding ratings on provider (algorithm = 0, human = 1), intervention (control = 0 vs. intervention = 1), and their interaction.
We found a significant provider type × intervention interaction (3A: β = -.49, t = -2.70, p = .007; 3B: β = -1.02, t = -4.57, p < .001), see 
Figure 4
, left panels. The difference in subjective understanding of the decision made by the human and algorithmic provider was smaller in the intervention condition (3A: β = -.58, t = -4.50, p < .001; 3B: β = -1.10, t = -6.97, p < .001) than in the control condition (3A: β = -1.08, t = -8.32, p < .001; 3B: β = -.08, t = -.52, p = .60).
We next tested the impact of the intervention on algorithm aversion in healthcare utilization. We regressed utilization intentions on provider type, intervention condition, and their interaction. As predicted, it revealed a significant provider type × intervention interaction (3A: β = -.49, t = -2.70, p = .007; 3B: β = -.56, t = -2.31, p = .02), see 
Figure 4
, right panels. Algorithm aversion in healthcare utilization--greater reluctance to utilize algorithmic than human providers--was smaller in the intervention condition (3A: β = -.58, t = -4.50, p < .001; 3B: β = -.01, t = -.05, p = .96) than in the control condition (3A: β = -1.08, t = -8.32, p < .001; 3B: β = -.57, t = -3.31, p = .001).
Finally, we tested our full process account by examining whether (i) subjective understanding mediated the influence of healthcare provider on utilization intentions and (ii) whether the increase in subjective understanding due to the intervention reduced algorithm aversion. We estimated a moderated mediation model of healthcare utilization with healthcare provider as predictor, intervention condition as moderator, and subjective understanding as mediator with 5000 bootstrap samples 
27
  To rule out that the effects of the interventions in Studies 3A-B were due to differences in objective understanding between the control and intervention conditions, we ran an additional 3 (intervention: control, Study 3A, Study 3B) x 2(understanding: subjective, objective) betweensubjects pre-registered experiment with 601 participants on Amazon Mechanical Turk. It compared both subjective and objective understanding between control conditions and two conditions that each tested one of the interventions. Results from this experiment show that, relative to a control condition, the interventions used in Studies 3A and 3B significantly increased subjective understanding (respectively, β = .67, t = 4.90, p < .001; β = .82, t = 6.05, p < .001) but did not influence objective understanding (respectively, p = .78; p = .89; all interactions, ts > 3.60, ps < .001). For detailed study descriptions and results, see Supplementary Appendix 5.


Study 4
In Study 4, with Google Ads, we tested whether subjective knowledge-based interventions would effectively reduce algorithm aversion in healthcare utilization in an online field setting. Following Winterich, et al. 
28
 , we published sponsored search advertisements for an algorithm-based skin cancer detection application ( 
Figure 5
). Whenever a user typed a prespecified search keywords (e.g., "skin cancer picture") they might see our sponsored search advertisement near or above the organic results on Google (see Supplementary Appendix 6 for details).
We ran our ad campaign for five days between November 15 and November 19, 2020.
We specified a daily budget of 60 euros; on each day our advertisements stopped when the daily budget was reached. The advertisements generated 14,013 impressions (i.e., how many times the ad was viewed across the following demographics: age: 18-24: 6.9%, 25-35: 5.7%, 35-44: 7.3%, 45-54: 15.1%, 55-64: 35.6%, 65+: 29.4%; gender: 78.8% female) and generated 698 clicks.
All participants were served one of two ads for a skin cancer detection application through Google Ads ( 
Figure 5
). We manipulated whether the sponsored ad did or did not explain the process by which the algorithm triaged moles and tested the impact of that intervention on ad click-through rates. Subjective understanding was manipulated through these descriptions of the application, as pretested in a study reported in Supplementary Appendix 7. In the intervention condition, the advertisement read, "Our algorithm checks how similar skin moles are in shape/size/color to cancerous moles." In the control condition, the advertisement read, "Our algorithm checks if skin moles are cancerous moles." Clicking on the advertisement took participants to the Google App Playstore page of SkinVision, an algorithmic skin cancer detection app (see Supplementary Appendix 6 and 8).
The frequency of appearance of the different advertisements was not random, because the Google Ads platform serves ads according to the goal of the campaign (maximizing clicks, in our case). Hence, we cannot use the actual number of clicks as a dependent variable. Instead, we followed prior research 28 and used the average percentage of clicks per impression or clickthrough-rate. A logistic regression revealed that the click-through-rate was higher in the intervention advertisement (6.36%) compared to the control advertisement (3.29%, β = .69, z = 8.17, p < .001, d = .38).
The results illustrate the efficacy of advertising interventions enhancing subjective knowledge in field settings. Because the assignment of users between ads was non-random, of course, it may have generated unintended variance in the set of users exposed to each ad 29 . Given the replications in Studies 3A and 3B, which featured random assignment, we believe it is unlikely that the results from Study 4 can be solely attributed to ad optimization. We invite future research to further replicate this result by using different methodologies and sampling procedures.


General Discussion
Utilization of algorithmic-based healthcare services is becoming critical with the rise of telehealth services 30 , current surge in healthcare demand 2-4 , and long-term goals of providing affordable and high-quality healthcare 31 in developed and developing nations 9 . Our results yield practical insights for reducing reluctance to utilize medical artificial intelligence. Because the technologies used in algorithmic-based medical applications are complex, providers tend to present AI provider decisions as a "black box." Our results underscore the importance of recent policy recommendations to open this black box to patients and users 
16,
32
 . A simple 1-page visual or sentence that explained the criteria or process used to make medical decisions increased acceptance of an algorithm-based skin cancer diagnostic tool, which could be easily adapted to other domains and procedures.
Given the complexity of the process by which medical artificial intelligence makes decisions, managers now tend to emphasize the outcomes algorithms produce in their marketing to consumers, which feature benefits such as accuracy, convenience and rapidity (i.e., performance), while providing few details about how algorithms work (i.e., process). Indeed, in an ancillary study examining the marketing of skin cancer smartphone applications ( 
Supplementary Appendix 8)
, we find that performance-related keywords were used to describe 57% to 64% of the applications, whereas process-related keywords were used to describe 21% of the applications. Improving subjective understanding of how medical artificial intelligence works may then not only provide beneficent insights for increasing consumer adoption but also for managers seeking to improve their positioning. Indeed, we find increased advertising efficacy for SkinVision, a skin cancer detection app, when advertising included language explaining how it works.
More broadly, our findings make theoretical contributions to the literatures on algorithm aversion and human understanding of causal systems. The literature on algorithm aversion finds that people are generally averse to using algorithms for tasks that are usually done by humans 3334 , on the grounds that algorithms are perceived as unable to learn and improve from their mistakes 35 , unsuitable for subjective or experiential tasks 
36,
37
 , unable to adapt to unique or mutable circumstances 
10,
38
 , and unable to carry responsibility for negative outcomes 11 . Our results illustrate the importance of understanding the causal process relating their inputs (e.g., medical data) to their outputs (e.g., diagnoses). We also identify novel interventions to reduce algorithm aversion in a domain where algorithms are already in widespread use 1 . We invite future research on medical AI adoption with alternative and complementary interventions of health literacy 39 or digital literacy 40 . Finally, people do exhibit an illusion of explanatory depth of causal systems 
[21]
[22]
[23]
41
 in many domains, claiming to better understand a variety of ideas, technologies, and biological systems than they objectively do. Our results suggest an egocentric bias in such illusion of understanding, that they may loom largest in assessments of people-the causal systems most similar to ourselves.


Methods
The present research involved no more than minimal risks and all study participants were 18 years of age or older. Studies 1-3B were approved for use with human participants by the Institutional Review Board on the Charles River Campus at Boston University (Protocol 3632E); informed consent was obtained for all participants. Study 4 used data collected from the Google Ads platform, which is aggregated and fully anonymous at the individual level. It is impossible to identify, interact with, and obtain consent from individual participants. This study was approved by the Erasmus University ERIM Internal Review Board.
All manipulations and measures are reported. Pre-registrations, raw data and Stata syntax files are available on the Open Science Framework at https://osf.io/taqp8/?view_only=c2ac4a617d424a708982e6825a2e149c.
In Studies 1-3B, we recruited participants from the online sample recruiting platform Lucid and Amazon Mechanical Turk. Lucid provides a nationally representative sample with respect to age, gender, ethnicity, and geography 42 . Amazon Mechanical Turk is an online sample recruiting platform that is not nationally representative but has been shown to be a reliable resource 42 . Following prior research, we selected participants with an approval rating above 95% on Amazon Mechanical Turk 43 . Studies 1-3B were conducted on the Qualtrics survey platform.
Condition assignments were random in all our studies, with randomization administered by Qualtrics' software.
Following a general rule of thumb used in recent research 44 , we sought to obtain a minimum of 100 participants per cell. As a result, in Studies 2A and 2B, we decided to target a sample of 100 per cell. In Studies 1 we decided to target a larger sample of 150 per cell to have sufficient power to detect an interaction in a 2 × 2 mixed design (one factor was manipulated between-subject while the other was repeated within-subjects). In Studies 3A and 3B, we decided to target a sample of 200 per cell to have sufficient power to detect moderated mediation. With a small effect size of Cohen's f = .20, a significance level of .05 and n = 100 in two betweensubject conditions, the power to detect a significant effect was 80.36%. With sample sizes of n = 150 and n = 200, the power was 93.23% and 97.88%, respectively.
In Studies 1, 2A, 3A and 3B, as preregistered, we programmed our surveys to automatically exclude participants who failed an attention check at the very beginning of the survey and prior to any manipulation. We have no data for these participants, and because these responses are recorded as incomplete, they did not affect our target sample size. Participants answered the following attention check question: "There is an artist named Frank that paints miniature figures. He usually buys these figures from a company, but the company has gone out of business. After this, Frank decides to hand-carve his own figures. However, Frank's friend tells him that these new figures are significantly worse in quality. Did Frank's friend think that Frank's hand-carved figures were better than the company's?" Answers: yes, no, maybe, cheese plates, movies (Correct answer is "no"). We screened out N = 211 participants in Study 1, N = 255 participants in Study 2A, N = 260 participants in Study 3A and N = 378 participants in Study 3B.
In studies 1, 2A, 3A, 3B, the stimuli included either an image of a human provider or an algorithm, the first result in Google Images with search terms "Doctor" or "Artificial Intelligence" that did not include words or a face (see 
Supplementary Appendix 1)
. Omitting these introductory images in study 2B yielded the same pattern of results.
Based on prior research on resistance to AI in the medical domain 10 , participants also reported three control variables related to skin cancer in Studies 1-3B: their perceived susceptibility to skin cancer ("Relative to an average person of your same age and gender, to what extent do you consider yourself to be at risk of skin cancer (melanoma)?" 1 -Much lower, 5 -Much higher), their self-examination frequency ("Skin self-examination is the careful and deliberate checking for changes in spots or moles on all areas of your skin, including those areas rarely exposed to the sun. How often do you practice skin self-examination?" 1-Never, 5 -Weekly), and their perceived self-efficacy ("In general, to what extent do you feel that you are confident in your ability to conduct skin self-examination?" 1-Not at all confident, 5 -Extremely confident).
In studies 1-3B, we also collected demographic variables at the end of the experiment    In the human provider conditions, we used the same visuals and replaced "algorithm"
with "doctor" (Study 3A) or "machine learning algorithm" with "primary care physician" (Study 3B).  This is how this particular screening would work.
You will go to see your primary care physician. A nurse will then take you to a private room, where she will take high¬ resolution photographs (scans) of your body. Screenings typically include an exam of scalp, face, mouth, hands, feet, trunk and extremities, eyes and eyelids, ears, fingers, toes and toenails, but you can ask for more (or fewer) areas to be checked.
The scans of your body are then analyzed to identify possible cancerous skin conditions. In the next pages, we will ask you questions about your knowledge on skin cancer screening.


General introduction in the ancillary study reported in the discussion of Studies 3A-B
Imagine you have decided to have a skin cancer screening.
A skin cancer screening is a visual inspection of your skin by a medical professional. The objective of this screening is early detection of skin cancer. It does not involve any blood test. The screening takes about 15 minutes to complete.
These skin cancer screenings typically have an accuracy of 89% (that is, they correctly identify cancerous skin lesions 89% of the times). This is how this particular screening would work. You will go to see a healthcare professional, who will take you to a private room and take skin scans (high-resolution photographs) of your body.
In the next pages, we will ask you questions about your knowledge on how skin scans (highresolution photographs) are analyzed to identify possible cancerous skin conditions.
Human provider introduction in studies 1-3B
Study 1, 2B and 3B: "A primary care physician will examine the scans of your skins to identify cancerous skin lesions." Study 2A and 3A: "A doctor will examine the scans of your skin to identify cancerous skin lesions." The sentence was followed by the picture below, except for study 2B.
Study 1, 2B, 3B and ancillary study reported in the discussion of Studies 3A-B: "A machine learning algorithm will examine the scans of your skin to identify cancerous skin lesions." Study 2 and 3A: "An algorithm will examine the scans of your skin to identify cancerous skin lesions."
The sentence was followed by the picture below, except for study 2B.


Supplementary Appendix 2.
Instructions for the open-ended mechanistic explanation in Study 1 Now, we would like to probe your knowledge a little bit more. We would like you to explain how you think a doctor examines skin scans to identify cancerous skin lesions.
Using the space below, please describe all the details you know about how a doctor examines skin scans, going from the first step to the last, and providing the causal connection between the steps.
In We included a manipulation check as the last question of the survey: ("In the scenario you read, the data […] was analyzed by?" Answers: a doctor, an algorithm). We report below how many participants correctly recalled their experimental condition in Studies 1-3, as well as report the results from a logistic regression with manipulation check (0 = algorithm, 1 = doctor) as the dependent variable and experimental condition (0 = algorithm, 1 = doctor) as the independent variable. Manipulations were successful in all studies.
• Study 1: 79.4% of the participants correctly indicated their experimental assignment. The influence of experimental condition on reported manipulation check was significant: β = -2.69, z = -9.38, p < .001 • Study 2A: 81.2% of the participants correctly indicated their experimental assignment.
The influence of experimental condition on reported manipulation check was significant: β = -2.93, z = -11.44, p < .001 • Study 2B: 65.4% of the participants correctly indicated their experimental assignment.
The influence of experimental condition on reported manipulation check was significant: β = -2.69, z = -9.56, p < .001 • Study 3A: 92.2% of the participants correctly indicated their experimental assignment.
The influence of experimental condition on reported manipulation check was significant: β = -5.65, z = -15.72, p < .001 • Study 3B: 82.3% of the participants correctly indicated their experimental assignment.
The influence of experimental condition on reported manipulation check was significant: β = -3.13, z = -16.45, p < .001
We further find that our results are robust to the exclusion of participants who failed the manipulation checks • Study 1: The intervention reduced subjective understanding of decisions made by human providers (ΔH = -.56, z = -4.69, p < .001), but not of decisions made by algorithmic providers (ΔA = -.07, z = -.57, p = .56), creating a significant provider × rating order interaction (ΔH -ΔA = -.49, z = -2.82, p = .005). • Study 2A: Participants claimed greater subjective understanding of medical decisions made by human than algorithmic providers (ΔSubjective = .78, t = 5.26, p < .001) whereas their objective understanding did not differ by provider (ΔObjective = -.05, t = -.39, p = .69), creating a significant provider × understanding type interaction (ΔSubjective -ΔObjective = .84, t = 3.99, p < .001). • Study 2B: Participants claimed greater subjective understanding of medical decisions made by human than algorithmic providers (ΔSubjective = .70, t = 4.28, p < .001) whereas their objective understanding did not differ by provider (ΔObjective = .02, t = .11, p = .92), creating a significant provider × understanding type interaction (ΔSubjective -ΔObjective = .69, t = 2.99, p < .001 Third, we examined the robustness of our results to the inclusion of the three control variables (perceived susceptibility to skin cancer, self-examination frequency, and self-efficacy) as well as gender, age and employment by/as a healthcare provider. Below, we show that our main results hold when including these six variables in our regressions analyses.
• Study 1: The intervention reduced subjective understanding of decisions made by human providers (ΔH = -.55, z = -5.27, p < .001), but not of decisions made by algorithmic providers (ΔA = -.16, z = -1.47, p = .14), creating a significant provider × rating order interaction (ΔH -ΔA = -.40, z = -2.61, p = .009). • Study 2A: Participants claimed greater subjective understanding of medical decisions made by human than algorithmic providers (ΔSubjective = .78, t = 5.69, p < .001) whereas their objective understanding did not differ by provider (ΔObjective = -.02, t = -.12, p = .91), creating a significant provider × understanding type interaction (ΔSubjective -ΔObjective = .80, t = 4.13, p < .001). • Study 2B: Participants claimed greater subjective understanding of medical decisions made by human than algorithmic providers (ΔSubjective = .47, t = 3.42, p = .001) whereas their objective understanding did not differ by provider (ΔObjective = .09, t = .61, p = .54), creating a significant provider × understanding type interaction (ΔSubjective -ΔObjective = .39, t = 1.96, p = .05 "To what extent do you understand how a doctor [algorithm] examines the scans of your skin to identify cancerous skin lesions?" scored on a 7-point scale with endpoints 1 = not at all and 7 = completely understand.
Subjective understanding in study 2B:
Q1. To what extent do you feel that you understand what information a primary care physician [machine learning algorithm] considers when inspecting a skin lesion (a mole) to determine a patient's risk of having skin cancer?
• I feel that I have a general understanding of the information used.
• I feel that I don't have a general understanding of the information used.
Q2. To what extent do you feel that you understand the process by which a primary care physician [machine learning algorithm] considers various criteria to determine the risk of skin cancer?
• I feel that I have a general understanding of the process.
• I feel that I don't have a general understanding of the process.
Q3. To what extent do feel that you understand how a primary care physician [machine learning algorithm] will rate and communicate the risk of skin cancer?
• I feel that I generally understand how the risk is rated and communicated.
• I feel that I don't generally understand how the risk is rated and communicated.
Supplementary Appendix 5. Ancillary Study reported in the discussion of Study 3A-B
Participants and Design. We recruited 601 US residents (Mage = 38.9, 50.9% female) from Amazon Mechanical Turk. Participants were randomly assigned to 1 of 6 conditions via the Qualtrics platform in a 2 (understanding: subjective, objective) x 3 (control, intervention A, intervention B) between-subject design.
Procedure. We used the same skin cancer detection context as Study 1 and 2 and manipulated whether the provider was a human doctor of an algorithm (Supplementary Appendix 1).
We measured subjective understanding as in studies 1, 2A, and 3A-B. We measured objective understanding as in Studies 2A-B. Detailed measured as available in Appendix Participants in the control condition reported their objective or subjective understanding. Participants in the intervention condition were shown the visual stimuli from Studies 3A and 3B (see 
Figure 3
) and then reported their objective or subjective understanding. Each participant only completed one measure of subjective understanding, once.
Results. We transformed raw measures of subjective and objective understanding into separate zscores ( 
Figure 2
). Next, we conducted a regression on understanding with independent variables understanding type (subjective = 0 vs. objective = 1) a dummy for intervention A and a dummy for intervention B, as well as interaction terms between understanding type and intervention 1 dummy as well as interaction term between understanding type and intervention 2 dummy.
The results showed significant understanding × intervention A interaction (β = -.70, t = -3.61, p < .001) and understanding × intervention B interaction (β = -.84, t = -4.33, p < .001). As predicted, planned contrasts revealed that compared to a control condition, the interventions from studies 3A and 3B significantly increased subjective understanding (respectively, β = .67, t = 4.90, p < .001: β = .82, t = 6.05, p < .001). but did not influence objective understanding (respectively, p = .78: p = .89).
learning algorithm] identifies cancerous skin lesions on a scale similar to what was used in Study 1, "To what extent do you understand how a doctor [algorithm] examines the scans of your skin to identify cancerous skin lesions?" scored on a 7-point scale with endpoints 1 = not at all and 7 = completely understand.


. Consistent with our prediction, the indirect effect of healthcare provider on utilization intentions through subjective understanding was significantly stronger (3A: Δ = .20, 95% CI [.05, .37]; 3B: Δ = .60, 95% CI [.35, .89]) in the control condition (3A: β = .29, 95% CI [.16, .45]; 3B: β = .65, 95% CI [.45, .86])--where differences in subjective understanding were larger--than in the intervention condition (3A: β = .09, 95% CI [.02, .17];3B: β = .09, 95% CI[-.12, .21]). These results hold when including all control variables in the regression, see Supplementary Appendix 3.


(
gender, age, and employment by/as a healthcare provider), and a manipulation check question ("In the scenario you read, the data […] was analyzed by?" Answers: a doctor, an algorithm).Additional details about the manipulation check, and analyses including the control variables are described in Supplementary Appendix 3. Effect sizes associated with main hypotheses are detailed in Supplementary Appendix 9.


Figure 1 .
1
Making participants explain provider decision processes reduces their subjective understanding of decisions made by human providers, but not decisions made by algorithmic providers (Study 1; N = 297). Bars ±1 SEM.


Figure 2 .Figure 3 .
23
People report lower subjective understanding of medical decisions made by algorithmic than human providers, but exhibit similar objective understanding of decisions made by human and algorithmic providers. Note: Panel a: Study 2A, N = 400; Panel b: Study 2B, N = 403Interventions in Study 3A (Panel A) and Study 3B (Panel B) from the algorithm condition.


Figure 4 .
4
Explanations reduce the difference in subjective understanding between algorithmic and human decision making (Study 3A: panel a, 3B: panel c), which alleviates algorithm aversion (Study 3A: panel b, 3B: panel d). Note: N = 801 and N = 798 in Studies 3A and 3B, respectively. Bars ±1 SEM.


). • Studies 3A-B: The indirect effect of healthcare provider on utilization intentions through subjective understanding was reduced in the intervention condition (3A: βIntervention= .06, 95% CI [-.01, .14]; 3B: βIntervention= .09, 95% CI [-.05, .23]) compared to the control condition (3A: βControl = .28, 95% CI [.17, .43]; 3B: βControl = .52, 95% CI [.35, .70])), creating a significant provider × intervention interaction (βControl -βIntervention = . 22, 95% CI [.08, .38]); 3B: βControl -βIntervention = . 43, 95% CI [.21, .66]). Measures of objective & subjective understanding in Studies 2A-B Objective understanding in Studies 2A-B Example instructions: "In the next pages we will ask you four questions about how you think a primary care physician [machine learning algorithm] examines the scans of your skin to identify cancerous skin lesions. It is very important that you do not look up or google these answers online, as that would defeat the purpose of our survey --we are interested in people's actual understanding of how primary care physician [machine learning algorithm] conducts these medical tests. So please just answer the following questions to best of your knowledge." Note: for each question, the order of the three options was presented in a random order. Consider broad features and compare the similarity of the person's mole to other moles known to be benign and malignant o Correct answer for machine learning algorithm which determines image visual similarity rather than by using a specific set of hand-crafted features C. Rely mostly on one fundamental feature (such as the mole's shape) which is known to predict if the mole is benign or malignant. Correct answer for primary care physician following a decision tree B. Consider multiple criteria at once/at the same time. Correct answer for algorithm following a more holistic process C. Simulate potential mole growth patterns. Incorrect for both, a primary care physician or a machine learning algorithm would not simulate additional data, Q3. When examining a picture of a person's mole, a primary care physician [machine learning algorithm] will generally calculate if the risk of skin cancer... A. Falls along a range from 0% to 100% chance of cancer o Correct for algorithms which compute the probability of malignancy, although they can communicate binary results to patients. B. Only falls into a low-risk category or a high-risk category o Correct for primary care physicians, whose decision tree's outcomes are generally binary. C. Is absent or present o Incorrect for both, it is unlikely that algorithms or physicians would be generally 100% about their diagnosis.
Supplementary Appendix 4. Q1. When examining a picture of a person's mole to determine the risk of skin cancer, a primary Subjective understanding in study 2A:
care physician [machine learning algorithm] will generally …
A. Consider a check list of features and history to determine if the mole is benign or
malignant.
o Correct answer for primary care physician, following a specific set of features
(e.g., A, B, C, D)
B. This option describes a non-compensatory rule that is incorrect for both primary
care physician or machine learning algorithm
Q2. When examining a picture of a person's mole to determine the risk of skin cancer, a primary
care physician [machine learning algorithm] will generally …
A. Consider various criteria, one criterion at a time and in a certain order
oooo








Acknowledgements
Before joining Rotterdam School of Management, R.C. received funding from the Susilo Institute for Ethics in the Global Economy, Questrom School of Business, Boston University. R.C. thank the Susilo Institute and the Erasmus Research Institute in Management (ERIM) for providing funding for data collection. The authors thank Tobias Sangers, Marlies Wakkee, Andreea Udrea and SkinVision for their feedback on human and algorithmic decision processes.






Data availability
Data from all of the studies reported in this paper are publicly available at https://osf.io/taqp8/?view_only=c2ac4a617d424a708982e6825a2e149c.


Code availability
Analyses were conducted with STATA 16.1 and code from all studies are publicly available at https://osf.io/taqp8/?view_only=c2ac4a617d424a708982e6825a2e149c


Author Contributions
R.C., C.L., and C.K.M designed research; R.C. and C.L. performed research; R.C. analyzed data; and R.C., C.L., and C.K.M wrote the paper.


Competing interests
The authors declare no competing interests.


Control ad
Intervention ad  Participants and Design. We recruited 200 US residents (Mage = 38.16, 41% female) from Amazon Mechanical Turk. The pretest used a one factor, two level (Ad: Intervention, Control) between-subject design. Randomization to condition was conducted by the Qualtrics platform.
Procedure. In an online study, participants saw one of the two advertisements served on Google Ads in Study 4. Subjective understanding was manipulated through the description of the application. In the intervention condition, the advertisement read, "Our algorithm checks how similar skin moles are in shape/size/color to cancerous moles." In the control condition, the advertisement read, "Our algorithm checks if skin moles are cancerous moles."
To measure subjective understanding, participants rated the extent to which, based on the information in the ad, they could understand the process by which the advertised algorithm identifies cancerous skin lesions (1 = not at all, 7 = completely understand).
Results. As expected, participants reported greater subjective understanding in the intervention condition than in the control condition (MIntervention = 4.44 vs. MControl = 3.66, t = 4.90, p < .001).
Supplementary Appendix 8. Ancillary Study reported in General Discussion.
Using the Web Scraping application webscraper.io, we collected data on the smartphone skin cancer detection application found on the Google pay store at https://play.google.com/store/search?q=skin%20cancer%20detection&c=apps. Full data and analyses about this study are available on Open Science Framework at https://osf.io/taqp8/?view_only=c2ac4a617d424a708982e6825a2e149c. For each application, we scrapped the name, the text included in the description and news, the average rating, number of ratings and number of installs. The initial sample consisted of 50 applications. The first author manually screened whether the app was featuring an algorithm which detects skin cancer based on picture inputs. Only 14 apps met the criteria and were included in the following analyses.
We developed a list of keywords for three themes around app functionalities such as accuracy (precision, reliab*, accura*, sensitivity, performan*), speed (speed, immediate, express, instant, fast, quick, rapid) and convenience (easy, simple, convenien*, effortless, painless, simple, straightforward) as well as one list for process-related words (asymmetry, border, color, diameter, similar, overlap, image recognition). For each textual description, we performed binary keyword analyses coded as 1 (the textual description contains at least one keyword from the list) vs. 0 (the textual description does not contain any of the keywords from the list). Results are summarized in the Log odds ratio = .69 .38
Notes: All equations were taken from Borenstein, M., Hedges, L. V., 
Higgins, J. P., & Rothstein, H. R. (2009)
. Introduction to meta-analysis. John Wiley & Sons.
a We computed Cohen's d for pre-post scores. b we computed the Cohen's d for independent groups. c we converted the odds ratio into Cohen's d.
 










High-performance medicine: the convergence of human and artificial intelligence




E
J
Topol




10.1038/s41591-018-0300-7






Nat. Med




25
















Telehealth Transformation: COVID-19 and the rise of Virtual Care




J
Wosik








J. Am. Med. Inform. Assoc




27
















Virtually perfect? Telemedicine for COVID-19




J
E
Hollander






B
G
Carr








N. Engl. J. Med




382
















Covid-19 and health care's digital revolution




S
Keesara






A
Jonas






K
Schulman








N. Engl. J. Med




382


82














Dermatologist-level classification of skin cancer with deep neural networks




A
Esteva








Nature




542
















Conversational agents in healthcare: a systematic review




L
Laranjo








J. Am. Med. Inform. Assoc




25
















Machine Learning-Based Prediction of Clinical Outcomes for Children During Emergency Department Triage




T
Goto






C
A
Camargo






Jr






M
K
Faridi






R
J
Freishtat






K
Hasegawa




10.1001/jamanetworkopen.2018.6937






JAMA Network Open




2
















Doctors are using AI to triage covid-19 patients. The tools may be here to stay




K
Hao








MIT Technology Review
















The application of medical artificial intelligence technology in rural areas of developing countries




J
Guo






B
Li








Health Equity




2
















Resistance to Medical Artificial Intelligence




C
Longoni






A
Bonezzi






C
K
Morewedge




10.1093/jcr/ucz013






J. Cons. Res




46
















Do patients trust computers




M
Promberger






J
Baron




10.1002/bdm.542






Journal of Behavioral Decision Making




19
















What people want from their professionals: Attitudes toward decision-making strategies




J
Eastwood






B
Snook






K
Luther








Journal of Behavioral Decision Making




25
















Big data and black-box medical algorithms




W
N
Price




10.1126/scitranslmed.aao5333






Sci. Transl. Med




10


5333














How the machine 'thinks': Understanding opacity in machine learning algorithms




J
Burrell




10.1177/2053951715622512






Big Data & Society




3


2053951715622512














Can we open the black box of AI?




D
Castelvecchi








Nature News




538


20














Accountable algorithms




J
A
Kroll








University of Pennsylvania Law Review




165


633














Telling more than we can know: Verbal reports on mental processes




R
E
Nisbett






T
D
Wilson




10.1037/0033-295X.84.3.231






Psychol. Rev




84
















Maps of Bounded Rationality: Psychology for Behavioral Economics




D
Kahneman








Am. Econ. Rev




93
















Associative processes in intuitive judgment




C
K
Morewedge






D
Kahneman




10.1016/j.tics.2010.07.004






Trends Cogn. Sci




14
















Valuing thoughts, ignoring behavior: The introspection illusion as a source of the bias blind spot




E
Pronin






M
B
Kugler




10.1016/j.jesp.2006.05.011








J. Exp. Soc. Psychol




43
















Explanation fiends and foes: How mechanistic detail determines understanding and preference




P
M
Fernbach






S
A
Sloman






R
S
Louis






J
N
Shube








J. Cons. Res




39
















Political Extremism Is Supported by an Illusion of Understanding




P
M
Fernbach






T
Rogers






C
R
Fox






S
A
Sloman




10.1177/0956797612464058






Psychol. Sci




24
















The misunderstood limits of folk science: an illusion of explanatory depth




L
Rozenblit






F
Keil




10.1207/s15516709cog2605_1






Cogn. Sci




26
















ABCD rule of dermatoscopy: a new practical method for early recognition of malignant melanoma




W
Stolz








Eur. J. Dermatol




4
















A clinical aid for detecting skin cancer: the triage amalgamated dermoscopic algorithm (TADA)




T
Rogers








The Journal of the American Board of Family Medicine




29
















A Randomized Trial on the Efficacy of Mastery Learning for Primary Care Provider Melanoma Opportunistic Screening Skills and Practice




J
K
Robinson




10.1007/s11606-018-4311-3






J. Gen. Intern. Med




33
















Introduction to mediation, moderation, and conditional process analysis: A regression-based approach




A
F
Hayes








Guilford Press












Knowing What It Makes: How Product Transformation Salience Increases Recycling




K
P
Winterich






G
Y
Nenkov






G
E
Gonzales




10.1177/0022242919842167






J. Marketing




83
















Field studies of psychologically targeted ads face threats to internal validity




D
Eckles






B
R
Gordon






G
A
Johnson




10.1073/pnas.1805363115






Proceedings of the National Academy of Sciences




115




















R
V
Tuckson






M
Edmunds






M
L
Hodgkins






Telehealth








N. Engl. J. Med




377
















How the Affordable Care Act can help move states toward a high-performing system of long-term services and supports. Health Aff




S
C
Reinhard






E
Kassner






A
Houser








30


















S
Radu








U.S. News & World Report
















Clinical versus actuarial judgment




R
Dawes






D
Faust






P
Meehl




10.1126/science.2648573






Science




243
















Making sense of recommendations




M
Yeomans






A
Shah






S
Mullainathan






J
Kleinberg








Journal of Behavioral Decision Making




32
















Algorithm aversion: People erroneously avoid algorithms after seeing them err




B
J
Dietvorst






J
P
Simmons






C
Massey








J. Exp. Psychol. Gen




144


114














Task-Dependent Algorithm Aversion




N
Castelo






M
W
Bos






D
R
Lehmann




10.1177/0022243719851788






J. Marketing Res




56
















Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The "Word-of-Machine




C
Longoni






L
Cian




10.1177/0022242920957347






Effect. J. Marketing
















People Reject Algorithms in Uncertain Decision Domains Because They Have Diminishing Sensitivity to Forecasting Error




B
J
Dietvorst






S
Bharti




10.1177/0956797620948841






Psychol. Sci




31
















Randomized trial of planning tools to reduce unhealthy snacking: implications for health literacy




J
Ayre






C
Bonner






E
Cvejic






K
Mccaffery








PLoS One




14


209863














eHealth literacy: extending the digital divide to the realm of health information




E
Neter






E
Brainin








J. Med. Internet Res




14


19














Missing the trees for the forest: A construal level account of the illusion of explanatory depth




A
L
Alter






D
M
Oppenheimer






J
C
Zemla








J. Pers. Soc. Psychol




99


436














Fighting misinformation on social media using crowdsourced judgments of news source quality




G
Pennycook






D
G
Rand




10.1073/pnas.1806781116






Proceedings of the National Academy of Sciences


the National Academy of Sciences






116














Reputation as a sufficient condition for data quality on Amazon Mechanical Turk




E
Peer






J
Vosgerau






A
Acquisti








Behav. Res. Methods




46
















Temporary sharing prompts unrestrained disclosures that leave lasting negative impressions




R
Hofstetter






R
Rüppell






L
K
John




10.1073/pnas.1706913114






Proceedings of the National Academy of Sciences




114

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]