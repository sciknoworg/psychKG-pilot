You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



A Commentary on
Altered learning under uncertainty in unmedicated mood and anxiety disorders by 
Aylward, J., Valton, V., Ahn, W. Y., Bond, R. L., Dayan, P., Roiser, J. P., et al. (2019)
. 
Nat. Hum. Behav. 3, 1116
-1123
. doi:10.1038
 Understanding how anxious and depressed individuals process information is a central topic in the field of psychiatry. In this regard, 
Aylward et al. (2019)
 utilized computational models of learning to better understand and describe how anxious and/or depressed individuals behave from moment to moment when faced with uncertain situations. Participants performed decision-making tasks characterized by fluctuating rewards and punishment. The authors fit computational models to the collected data from participants in the anxiety and healthy groups using the hierarchical Bayesian estimation with two levels of priors, at individual and group levels, where they set the group prior separately for each group. The three parameters of the winning model (punishment learning rate, lapse parameter, and decay rate) were higher in the symptomatic group than in the healthy group. In short, the authors found that anxious individuals quickly learned about negative phenomena but not about positive phenomena. Notwithstanding, we believe we have identified two methodological issues regarding the statistical analysis of the cited study, shrinkage and a 'two-step approach'.
Visual inspection of parameter estimates 
(Fig. 1A
 in the present article and 
Fig. 2a
 in the original article) indicates that the punishment learning rate clustered at the higher value near its maximum (i.e. 1), suggesting that the decision-making of most participants in the symptomatic group depended solely on punishment outcomes in the immediate past. At first glance, this result appears to be too extreme although it could be a genuine reflection of anxious individuals' characteristics-overreacting when exposed to immediate punishments. However, we suggest that the original results might suffer from statistical bias caused by a property of the hierarchical Bayesian estimation procedure called shrinkage.
Shrinkage is a notable property of hierarchical models, in that less reliable estimates are more strongly biased towards the group mean 
(Efron and Morris, 1977)
. While the shrinkage property may improve the estimation of individual parameters 
(Ahn et al., 2014;
Katahira, 2016)
, it may also lead to an underestimation of group-level variances (individual differences). To examine whether the small variances in Aylward et al.'s results were due to strong shrinkage, we conducted a maximum likelihood estimation with the same dataset since it does not have this shrinkage characteristic and it provides unbiased, albeit noisy, estimates for each participant. We found larger distributions for the punishment learning rate among the anxiety group, which were comparable to those for the healthy group 
(Fig. 1B)
.
These results indicated that a strong shrinkage occurred in the estimates for the punishment learning rate in the anxiety group.
In the original study, both groups were statistically compared regarding their group-level means for each parameter, rather than regarding individual-level estimates for each parameter. Thus, the shrinkage of estimates at the individual-level might not have directly influenced the results. However, too much shrinkage indicates that the variance of group-level distribution might have become improperly small (S1 
Fig.)
. This would also lead to the smaller variance of the posterior group-level mean distribution for the anxiety group (S2 
Fig.)
, which might inflate false-positive rates. A previous study has shown that improperly small population variance is often obtained when the analyzed data do not provide reliable information regarding the variances in study population distributions 
(Gelman, 2013)
. One potential source for this unreliable information refers to the interdependencies between model parameters 
(Scheibehenne and Pachur, 2015)
. These interdependencies make different parameter combinations equally probable, so the reliability of each parameter is diminished. To examine the influence of the interdependency of model parameters on Aylward et al.'s results, we calculated the correlation coefficient of the posterior distribution for the free parameters in the winning model among the symptomatic group.
Indeed, we found that the punishment learning rate, which showed strong shrinkage, correlated with other parameters, including the decay rate (S3 
Fig.)
. Even though the distribution of the punishment learning rate of the second winning model (without decay rate) estimated maximum likelihood estimation (S4B 
Fig.)
 is comparable to the one of the winning model 
(Fig. 1B)
, the parameter on the second winning model estimated with hierarchical Bayesian parameter estimation did not seem to show strong shrinkage (S4A 
Fig. in this manuscript and Fig. 2d
 in 
Aylward et al.)
, like the one with the winning model 
(Fig. 1A)
.
Therefore, interdependent correlations between punishment learning rate and decay rate in the winning model might have caused the observed strong shrinkage.
In addition to the between-group comparison regarding estimated parameters, to investigate the continuous relationship between symptom scores and model parameters, the original authors submitted the point parameter estimates obtained from individual participants into correlational statistical tests.
However, this 'two-step approach' (participant-level point estimates acquired by a hierarchical Bayesian estimation -that is independently applied to each group -being subsequently used in a frequentist test) has been criticized in the literature because it biases the results towards an alternative hypothesis 
(Boehm et al., 2018)
. This happens because the underestimated group-level variance leads to overestimated correlation coefficients, thereby causing Type I error rates to be inflated. Thus, the results shown in 
Fig. 4
 of the original manuscript should be interpreted with caution.
The use of a hierarchical Bayesian approach in computational modeling has been enhanced by the development of open-source software (e.g. hBayesDM 
(Ahn et al., 2017)
, which was used by 
Aylward et al.)
. Although this convenient and useful software may contribute magnificently to the development of research in the fields of psychology and psychiatry, an adequate understanding of their underlying mechanisms is required to ensure appropriate use. For example, the shrinkage degree often depends on the choice of the prior distribution for population distribution variances 
(Gelman, 2006)
. Although properties of the prior distribution used in Aylward et al. (e.g., the Cauchy distribution) have yet to be studied, there seems to be a high probability for strong shrinkage to occur if posterior distributions are near the edge of the original parameter range (around one). We believe that further theoretical consideration about the influences of the prior and the model structure is needed to explore the proper use of hierarchical Bayesian modeling in computational psychiatry.
 






Acknowledgements
This work was supported by the Grant for Research Fellow of JSPS to M.S. and JSPS KAKENHI Grants (JP18KT0021 and JP18K03173) to K.K.






Code availability
All codes used in the analysis are available on the Open Science Framework (https://osf.io/rx8hz/).
 










Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package




W
Ahn






N
Haines






L
Zhang




10.1162/cpsy_a_00002








Comput. Psychiatry




1




















W
Y
Ahn






G
Vasilev






S
H
Lee






J
R
Busemeyer






J
K
Kruschke






A
Bechara


















Decision-making in stimulant and opiate addicts in protracted abstinence: Evidence from computational modeling with pure users


10.3389/fpsyg.2014.00849






Front. Psychol




5














Altered learning under uncertainty in unmedicated mood and anxiety disorders




J
Aylward






V
Valton






W
Y
Ahn






R
L
Bond






P
Dayan






J
P
Roiser




10.1038/s41562-019-0628-0






Nat. Hum. Behav




3
















On the importance of avoiding shortcuts in applying cognitive models to hierarchical data




U
Boehm






M
Marsman






D
Matzke






E
J
Wagenmakers




10.3758/s13428-018-1054-3






Behav. Res. Methods




50
















Stein's paradox in statistics




B
Efron






Morris






C








Scientific American




236


5
















Prior distributions for variance parameters in hierarchical models




A
Gelman








Bayesian Anal




1
















Bayesian data analysis




A
Gelman






J
B
Carlin






H
S
Stern






D
B
Dunson






A
Vehtari






D
B
Rubin








CRC press












How hierarchical models improve point estimates of model parameters at the individual level




K
Katahira




10.1016/j.jmp.2016.03.007






J. Math. Psychol




73
















Using Bayesian hierarchical parameter estimation to assess the generalizability of cognitive models of choice




B
Scheibehenne






T
Pachur




10.3758/s13423-014-0684-4






Psychon. Bull. Rev




22

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]