You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
When an action leads to a desired outcome, we tend to repeat it almost automatically, yet we can quickly adapt our behaviour to changing circumstances when needed. For example, we can almost blindly take the same route to work daily but also plan a detour if a traffic jam is announced. The distinction between model-free and model-based reinforcement learning provides a mathematical formalisation of these adaptive features of human behaviour 
(Daw et al., 2005
(Daw et al., , 2011
. While model-free choices rely on the reward history of previous actions, directly reinforcing those leading to success, model-based control guides behaviour by planning over an internal causal model of the environment 
(Dolan & Dayan, 2013;
Doll et al., 2015;
Gläscher et al., 2010)
. These strategies trade off against each other, as model-free control is more efficient and cognitively cheap, and model-based control is more flexible but computationally more demanding 
Kool et al., 2017;
. However, little is known about how people learn to navigate this trade-off based on environmental demands. Therefore, inspired by previous theories and studies on the learning of cognitive control 
(Abrahamse et al., 2016;
Braem & Egner, 2018;
Doebel, 2020)
, we aimed to test whether people can dynamically learn to adjust their reliance on model-based versus model-free control depending on the environment-specific demands and benefits of using these different strategies.
Previous findings have shown that the regulation of model-based and model-free control can be driven, among others by cognitive load 
(Gershman et al., 2014;
Otto, Gershman, et al., 2013)
, opportunity costs 
(Pezzulo et al., 2013)
, time 
(Keramati et al., 2011)
, stress 
(Otto, Raio, et al., 2013;
Radenbach et al., 2015)
 or amplification of outcomes 
(Kool et al., 2017)
. In this study, we aimed to test whether people can also learn the value of modelbased versus model-free control from more subtle contingencies in the environment that make one or the other strategy more beneficial. To test this question, we implemented a simple manipulation in the deterministic two-step task developed by 
Kool et al. (2017)
. On each trial of this task, participants start in one of two first-stage states. Each of these states is associated with a unique pair of spaceships. For each of these two pairs, the two spaceships deterministically lead to one of two planets, each associated with an alien that provides rewards that drift slowly over time (see 
Figure 1)
. In our version of this task, we varied levels of environmental demand by changing the frequency of repeating versus alternating firststage states (i.e., the initial spaceship pairs to choose from). We reasoned that if spaceship pairs frequently alternate, it is worth mobilising cognitive resources to update the current reward values of each planet associated with the spaceships in each pair (i.e., to be more model-based). However, if spaceship pairs frequently repeat, generalising between pairs has less added benefit, and simply repeating previously rewarded actions (i.e., to act more modelfree) may be sufficient. Therefore, we predicted that people who encounter more alternations between the first-stage states would learn to be more model-based, while those subjected to more first-stage state repetitions may learn to be more model-free.


Methods


Participants
We recruited 144 participants from the University's recruitment platform, who participated for course credit. Of these, four were excluded based on more than 40 missing responses. In the group rewarded more on first-stage repetitions (n=69), there were 47 women, 20 men and one non-binary participant (mean age = 19.29, SD = 2.88, eight lefthanded). In the group rewarded more on first-stage alternations (n=71), there were 57 women and 14 men (mean age = 18.63, SD=1.02, five left or ambidexter). For one participant in the repetition group, demographic data was not saved. Participants were not given bonus payments based on performance, but they were told that the goal of the experiment was to collect as many points as possible.


Materials and procedure
The experiment was programmed in jsPsych (de Leeuw, 2015), retrieved and adapted from Kool and colleagues 
(Kool et al., 2016
(Kool et al., , 2017
, and run online. In each trial, participants were presented with one of two screens (first-stage states), each containing two spaceships (see 
Figure 1
). Participants had to choose between these spaceships, which were presented side-byside using the "F" and "J" keys on the keyboard within 1,500ms. This choice determined which of two planets (second-stage states), a red or a purple one, would then be visited. Importantly, the choices in each first-stage state afforded the opportunity of transitioning to either plane: one ship always led to the purple planet and the other to the red planet. In this second-stage state, participants encountered a single alien that would give the rewards in the form of "space treasure" after a button press. The rewards provided by the aliens were determined according to independent and slowly drifting reward distributions (Gaussian random walk with rewards varying between 0 and 9 (σ = 2)).
Before starting the main experiment, participants underwent extensive training to learn the reward manipulation and transition structure, i.e., which spaceship leads to which planet, followed by 25 complete practice rounds. They were instructed that the goal was to collect as many points as possible throughout the main experiment. In the first half of the main experiment (125 trials, to have sufficient trials in the subsequent test phase, based on 
Kool et al., 2016)
, participants in the first-stage "repetition" group were presented with 80% repetitions of first-stage states, i.e., they encountered the same spaceship pairs in two subsequent trials.
Conversely, participants in the first-stage "alternation" group encountered first-stage alternations 80% of the time. In both groups, we ensured participants saw both stages 50% of the time. In the second half, both groups underwent another trials, in which they saw 50% first-stage repetitions or alternations respectively. At no point in the experiment were participants instructed on the frequency manipulation.
Because the choices between spaceships are equivalent between the first-stage states, this task allows us to distinguish between model-based and model-free strategies. This is possible because only the model-based system transfers experiences learned in one starting state to the other starting state. For a model-based agent, each second-stage outcome affects subsequent first-stage preferences equivalently, i.e., independent of whether the next trial starts with the same pair of spaceships as on the previous trials. This is because model-based agents plan towards the second-stage goals. In contrast, a pure model-free strategy does not transfer experiences between first-stage states since they only learn action-reward associations 
(Doll et al., 2015)
.


Reinforcement model
We formally quantified model-based and model-free contributions to behaviour using a dual-system reinforcement learning model developed by 
Kool et al. (2016)
. In this model, each strategy (model-based and model-free control) learns a Q function which maps each stateaction pair (S, A) to the expected future return using different algorithms. The model-free learner uses a temporal difference learning algorithm (SARSA, 
Sutton & Barto, 2018)
, increasing values for state-action pairs with positive reward prediction errors (PE) and decreasing values for those with negative reward prediction errors across both stages. At each stage, the PE is computed based on the difference between the observed outcome (obtained reward and estimated future returns) and the expected return according to
= ( + ′ ) − ( ℎ )
, where denotes the reward (only received at the second stage), ′ the estimated future return (only present at the first stage, carried over from the second stage values of the according state-action pair at the last encounter), and ( ℎ ) the observed value of the action. The chosen action is then updated according to Model-based control was indexed by weights closer to 1, whereas model-free control was indexed by weights closer to 0.
In addition to and , we estimated an inverse temperature parameter ( ), which determined the exploitation/exploration trade-off between two choice options given their difference in value, an eligibility trace parameter ( ) representing the degree to which first information of the second stage prediction errors are incorporated to the next first stage as well as a choice ( ) and response stickiness parameter ( ), capturing persistence in which spaceship participants choose or which response key they press (by assigning additional value to the 8 values of the actions that were associated with the spaceship and response key selection of the previous trial).
The model was fit to behaviour in MATLAB using maximum a-posteriori estimation using parameter priors based on 
Gershman (2016)
. We fit the model separately for the two halves of the experiment.


Figure 1
Transition structure and task procedure Note. Participants performed an inducer phase in which they encountered 80% first-stage repetitions or 80% first-stage alternations, followed by a test phase in which they encountered 50% first-stage repetitions and alternations, respectively.


Results


Simulating the environment-specific marginal gains of using model-based control
Before running the main analyses on participants' performance, we ran simulations to confirm whether adopting a model-free strategy effectively paid off more when first-stage states repeat vs alternate frequently (in terms of total reward earned). To this end, we simulated data from 125 trials for 500 agents per reward group based on 500 plausible parameter combinations. As expected, our simulated data showed a significant correlation between the model-based weighting parameter and the total rewards earned in the group encountering more first-stage alternations (Pearson's r(498)= 0.168, p < 0.001), indicating that it paid off to be model-based in this group in terms of total rewards earned. However, this correlation was not observed in the group encountering more first-stage repetitions (Pearson's r(498)= 0.036, p < 0.425). A Fisher's z-test indicated a significant difference between these two correlations, p = .017, suggesting there was a reduced need to be model-based in this group, and a cheaper model-free strategy was sufficient regarding total pay-off.


Comparing model-based control in high-versus low-alternation environments
We restrict our analyses to the test phase of the experiment for two reasons. First, both groups performed the same task in this phase but not in the inducer phase. Second, our hypothesis only pertained to the test phase.
As expected, our results revealed that people in the group encountering more firststage alternations showed increased model-based control compared to people in the group encountering more first-stage repetitions (t(128.97) = -2.374, p < .01; one-sided, Welch modification to degrees of freedom used due to inequality of variances). Moreover, they were less persistent in their choices, as indicated by the choice stickiness parameter (t(138) = 2.013, p = .046; see 
Figure 2
 and 
Table 1
). All other parameter estimates did not significantly differ between groups (all ps > .167) and are displayed in 
Table 1
.


Figure 2
Parameter differences between the two groups in the test phase Note. Participants in the group encountering more first-stage alternations (80%) were more model-based in the test phase (50% first-stage alternations and repetitions) than participants in the group encountering more first-stage repetitions (a) and less persistent in their choices (b). c) Probability of staying following a negative or positive prediction error (PE) on alternation and repetition trials. We see a trend of participants in the repetition group being less likely to stay (alternate) following a positive (negative) prediction error on alternation trials (not significant). Overall, they were more likely to stay following first-stage repetitions. response stickiness, w: weighting parameter of model-based control.


Differences in choice behaviour in high-versus low-alternation environments
To get a behavioural estimate of whether participants in the group encountering more repetitions in the inducer phase showed more model-based behaviour in the test phase, we also ran a logistic mixed effects model in brms 
(Bürkner, 2017
(Bürkner, , 2018
(Bürkner, , 2021
. This model predicted whether participants stayed with or switched the second-stage state from the previous trial as a function of the sign of the second reward prediction error (PE) on the last trial (retrieved from the computational model), first-stage state sequence (repetition or alternation) and group (in line with 
Kool et al., 2017)
. In logistic models for this task, a main effect of previous PE reflects model-based control (because it reflects a tendency to visit previously rewarded second-stage state regardless of starting state), whereas an interaction between previous PE and first-stage state reflects model-free control (because the effect of previous outcome depends on whether the same first-stage state is encountered).
We predicted that we would observe an interaction between group, the sign of the previous PE and the first-stage sequence. Specifically, we expected that participants in the group encountering more first-stage alternations would show more stay (leave) behaviour following positive (negative) prediction errors on both repetition and alternation trials (where one has to generalise across stages, i.e., be model-based), whereas participants in the group encountering more first stage repetitions to show more stay (leave) behaviour following positive (negative) PE, but only on repetition trials (where no generalisation is necessary). As expected, we found main effects of previous PE (b=-0.406, 95% CI [-0.519, -0.403]), with participants being more likely to stay following a positive PE, and of transition (b=0.148, 95% CI [0.098, 0.197]), with participants being overall more likely to choose the same state again when they repeated. We also saw an interaction between the sign of the previous PE and transition .039]), with participants being more likely to stay following a positive PE on repetition as compared to alternation trials. This interaction was not further modulated by group (b=-0.004, 95% CI [-0.046, 0.035]) (see 
Figure 2
). We also did not see an interaction between the sign of the previous PE and group (b=0.023, 95% CI [-0.033, 0.083]), though the alternation group showed numerically less stay behaviour following negative prediction errors.


Discussion
The goal of this project was to investigate whether people can learn to arbitrate adaptively between two reinforcement-learning strategies, i.e., model-free and model-based control, depending on features of the environment. While others have shown that people can learn to arbitrate between both strategies based on explicit incentives 
(Kool et al., 2017;
Patzelt et al., 2019)
, we showed that they can also learn to regulate these control modes based on more subtle environmental features. Specifically, we manipulated how frequently participants encountered first-stage alternations or repetitions in the two-step task, thereby influencing which strategy is more effective in terms of a cost-benefit trade-off 
(Kool et al., 2016
(Kool et al., , 2017
Otto & Daw, 2019;
Shenhav et al., 2013)
. As model-based control pays off more when the first-stage states alternate frequently due to having to update values between the two spaceship pairs, we expected and found that participants eventually learned to act more model-based in a subsequent, separate test phase. Our finding provides important evidence for the ability of humans to learn adaptive control settings over time, without explicit instructions or task cues, in line with an emerging learning perspective on cognitive control 
(Abrahamse et al., 2016;
Braem et al., 2024;
Held et al., 2023;
Simoens et al., 2024;
Xu et al., 2024)
. The fact that such learning translates to reinforcement learning further hints at a general self-regulatory mechanism that can explain adaptive parameter settings without necessitating a homunculus or "loan of intelligence" 
(Boureau et al., 2015;
Dennett, 1981)
.
Our findings may further have clinical implications. Specifically, previous research has found interesting interindividual differences in applying either strategy, highlighting an important transdiagnostic mechanism 
(Gillan et al., 2016)
. Extending these findings, others have pointed at interindividual differences in regulating both strategies based on reward incentives that participants were explicitly told to consider 
(Patzelt et al., 2019)
. In contrast, our manipulation relied on the regulation of meta-control based on learned contingencies with environmental demands. As also argued for other disorders 
(Geurts et al., 2009;
Van Eylen et al., 2011)
, the use of explicit task instructions or manipulations can often lead to a failure to detect underlying cognitive impairments in clinical disorders because (difficulties with) our daily activities or everyday life often come without clear instructions on how to act. We, therefore, believe it would be interesting to adapt our manipulation to a within-subject design and study whether (mal)adaptive regulation of reinforcement strategies based on environmental demands could be a clinical marker. Similarly, it would be interesting to study this in development, meaningfully extending findings by 
Smid et al. (2023)
 who found less arbitration between both strategies in children based on explicit stake cues.
One limitation of our work concerns the current criticisms of the strict distinction between model-based and model-free control. For instance, it has been argued that this dichotomy is oversimplifying more complex decision-making structures 
(Collins & Cockburn, 2020)
, that there is no sharply defined line separating them 
(Miller et al., 2019)
, or that the model-free system is hierarchically governed rather than flat as modelled in this paper 
(Dezfouli et al., 2014;
Dezfouli & Balleine, 2012
. Another criticism states that seemingly model-free behaviour in humans may arise simply from inaccurate models rather than a model-free process 
(Feher Da Silva & Hare, 2020)
. We believe these criticisms do not diminish the value of our findings. Our predictions rely on the assumed computational cost of model-based control and its increased usefulness in an environment with more first-state alternations. Because our main interest was to document how people learn to arbitrate the relative costs and benefits of different decision strategies (see also 
Otto et al., 2022)
, showing a similar learned arbitration between this type of model-based control and, for instance, a computationally efficient but incorrect model (a wrong model would take up resources as well, see also, Morris & Cushman, 2019) should thus be equally informative.
In sum, our findings suggest that people can learn to adaptively regulate their reliance on model-based versus model-free control depending on environmental demands. Namely, individuals showed changes in control strategies in response to the frequency of first-stage alternations, favouring model-based control when alternations were frequent and defaulting to the less costly model-free approach when first-stage repetitions dominated. Future research exploring individual differences in model-based versus model-free control could further elucidate how this learning of adaptive control processes may vary across different (sub)clinical populations or stages of development.
Data availability: All data is available at https://osf.io/6gy9c/?view_only=994e80d9931448dea371782c5443a0af. 
the learning rate (the degree to which new information is integrated into the decision process). Hence, the model-free learner learns all first and second-stage stateaction values separately.The model-based algorithm combines the task's transition structure (which first-stage action leads to which second stage) with the current estimate of the second-stage values to determine the first-stage Q values (i.e., these model-based values are identical to the secondstage model-free values of the planets). These model-based and model-free action values are then combined using a weighting parameter according to


Author contributions: CRediT: LKH: Conceptualization, Data curation, Formal analysis, Funding acquisition, Writing -original draft, review and editing. EL: Resources, Validation, Writing -review and editing. WK: Conceptualization, Formal analysis, Resources, Writing -review and editing. SB: Conceptualization, Supervision, Writing -original draft, review and editing.


Table 1
1
Parameter estimates per group
β
α
λ
π
ρ
w
Repetition group
Mean
1.10
0.71
0.58
0.38
-0.12
0.67
SD
1.04
0.37
0.40
0.85
0.52
0.32
25 th percentile
0.50
0.55
0.20
-0.15
-0.35
0.55
Median
0.65
0.91
0.69
0.26
-0.10
0.76
75 th percentile
1.02
1
1
0.86
0.29
0.93
Alternation group
Mean
0.99
0.80
0.56
0.11
-0.14
0.79














Grounding cognitive control in associative learning




E
Abrahamse






S
Braem






W
Notebaert






T
Verguts




10.1037/bul0000047








Psychological Bulletin




142


7














Deciding How To Decide: Self-Control and Meta-Decision Making




Y.-L
Boureau






P
Sokol-Hessner






N
D
Daw








Trends in Cognitive Sciences




19


11


















10.1016/j.tics.2015.08.013














One cannot simply 'be flexible': Regulating control parameters requires learning. Current Opinion in Behavioral Sciences




S
Braem






M
Chai






L
K
Held






S
Xu




10.1016/j.cobeha.2023.101347








55












Getting a Grip on Cognitive Flexibility




S
Braem






T
Egner




10.1177/0963721418787475








Current Directions in Psychological Science




27


6














brms: An R Package for Bayesian Multilevel Models Using Stan




P.-C
Bürkner




10.18637/jss.v080.i01








Journal of Statistical Software




80


1














Advanced Bayesian Multilevel Modeling with the R Package brms




P.-C
Bürkner




10.32614/RJ-2018-017








The R Journal




10


1


395














Bayesian Item Response Modeling in R with brms and Stan




P.-C
Bürkner




10.18637/jss.v100.i05








Journal of Statistical Software




5


100














Beyond dichotomies in reinforcement learning




A
G E
Collins






J
Cockburn




10.1038/s41583-020-








Nature Reviews Neuroscience




21


10
















Model-Based Influences on Humans' Choices and Striatal Prediction Errors




N
D
Daw






S
J
Gershman






B
Seymour






P
Dayan






R
J
Dolan




10.1016/j.neuron.2011.02.027








Neuron




69


6
















Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioural control




N
D
Daw






Y
Niv






P
Dayan




10.1038/nn1560








Nature Neuroscience




8


12
















jsPsych: A JavaScript library for creating behavioral experiments in a Web browser




J
R
De Leeuw








Behavior Research Methods




47


1
















10.3758/s13428-014-0458-y














Brainstorms: Philosophical Essays on Mind and Psychology




D
C
Dennett




10.7551/mitpress/1664.001.0001








The MIT Press












Habits, action sequences and reinforcement learning




A
Dezfouli






B
W
Balleine




10.1111/j.1460-9568.2012.08050.x








European Journal of Neuroscience




35


7
















Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized




A
Dezfouli






B
W
Balleine




10.1371/journal.pcbi.1003364








PLoS Computational Biology




9


12














Habits as action sequences: Hierarchical action control and changes in outcome value




A
Dezfouli






N
W
Lingawi






B
W
Balleine




10.1098/rstb.2013.0482








Philosophical Transactions of the Royal Society B: Biological Sciences




369














Rethinking Executive Function and Its Development




S
Doebel




10.1177/1745691620904771








Perspectives on Psychological Science




15


4














Goals and Habits in the Brain




R
J
Dolan






P
Dayan




10.1016/j.neuron.2013.09.007








Neuron




80


2
















Model-based choices involve prospective neural activity




B
B
Doll






K
D
Duncan






D
A
Simon






D
Shohamy






N
D
Daw




10.1038/nn.3981








Nature Neuroscience




18


5
















Humans primarily use model-based inference in the two-stage task




C
Feher Da Silva






T
A
Hare








Nature Human Behaviour




4


10


















10.1038/s41562-020-0905-y














Empirical priors for reinforcement learning models




S
J
Gershman




10.1016/j.jmp.2016.01.006








Journal of Mathematical Psychology




71
















Retrospective revaluation in sequential decision making: A tale of two systems




S
J
Gershman






A
B
Markman






A
R
Otto




10.1037/a0030844








Journal of Experimental Psychology: General




143


1
















The paradox of cognitive flexibility in autism




H
M
Geurts






B
Corbett






M
Solomon








Trends in Cognitive Sciences




13


2


















10.1016/j.tics.2008.11.006














Characterizing a psychiatric symptom dimension related to deficits in goal-directed control. eLife, 5, e11305




C
M
Gillan






M
Kosinski






R
Whelan






E
A
Phelps






N
D
Daw




10.7554/eLife.11305


















States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning




J
Gläscher






N
Daw






P
Dayan






J
P
Doherty








Neuron




66


4


















10.1016/j.neuron.2010.04.016


















L
K
Held






L
Vermeylen






D
Dignath






R
M
Krebs






W
Notebaert






S
Braem












Reinforcement learning of adaptive control strategies








Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes




M
Keramati






A
Dezfouli






P
Piray




10.1371/journal.pcbi.1002055








PLoS Computational Biology




7


5














When Does Model-Based Control Pay Off?




W
Kool






F
A
Cushman






S
J
Gershman




10.1371/journal.pcbi.1005090








PLOS Computational Biology




12


8














Competition and Cooperation Between Multiple Reinforcement Learning Systems




W
Kool






F
A
Cushman






S
J
Gershman




10.1016/B978-0-12-812098-9.00007-3








Goal-Directed Decision Making




Elsevier
















Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems




W
Kool






S
J
Gershman






F
A
Cushman








Psychological Science




28


9


















10.1177/0956797617708288














Planning Complexity Registers as a Cost in Metacontrol




W
Kool






S
J
Gershman






F
A
Cushman








Journal of Cognitive Neuroscience




30


10


















10.1162/jocn_a_01263














Habits without values




K
J
Miller






A
Shenhav






E
A
Ludvig




10.1037/rev0000120








Psychological Review




126


2
















The opportunity cost of time modulates cognitive effort




A
R
Otto






N
D
Daw








Neuropsychologia




123


















10.1016/j.neuropsychologia.2018.05.006














The Curse of Planning: Dissecting Multiple Reinforcement-Learning Systems by Taxing the Central Executive




A
R
Otto






S
J
Gershman






A
B
Markman






N
D
Daw








Psychological Science




24


5


















10.1177/0956797612463080














Working-memory capacity protects model-based learning from stress




A
R
Otto






C
M
Raio






A
Chiang






E
A
Phelps






N
D
Daw








Proceedings of the National Academy of Sciences




110


52


















10.1073/pnas.1312011110














Incentives Boost Model-Based Control Across a Range of Severity on Several Psychiatric Constructs




E
H
Patzelt






W
Kool






A
J
Millner






S
J
Gershman








Biological Psychiatry




85


5


















10.1016/j.biopsych.2018.06.018














The Mixed Instrumental Controller: Using Value of Information to Combine Habitual Choice and Mental Simulation




G
Pezzulo






F
Rigoli






F
Chersi




10.3389/fpsyg.2013.00092








Frontiers in Psychology
















The interaction of acute and chronic stress impairs model-based behavioral control




C
Radenbach






A
M F
Reiter






V
Engert






Z
Sjoerds






A
Villringer






H.-J
Heinze






L
Deserno






F
Schlagenhauf








Psychoneuroendocrinology




53


















10.1016/j.psyneuen.2014.12.017














The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function




A
Shenhav






M
M
Botvinick






J
D
Cohen




10.1016/j.neuron.2013.07.007








Neuron




79


2














Learning environment-specific learning rates




J
Simoens






T
Verguts






S
Braem




10.1371/journal.pcbi.1011978








PLOS Computational Biology




20


3














Computational and behavioral markers of model-based decision making in childhood




C
R
Smid






W
Kool






T
U
Hauser






N
Steinbeis




10.1111/desc.13295








Developmental Science




26


2














Reinforcement learning: An introduction




R
S
Sutton






A
G
Barto








The MIT Press






2nd ed. (pp. xxii, 526








Cognitive flexibility in autism spectrum disorder: Explaining the inconsistencies?




L
Van Eylen






B
Boets






J
Steyaert






K
Evers






J
Wagemans






I
Noens








Research in Autism Spectrum Disorders




5


4


















10.1016/j.rasd.2011.01.025














Learning where to be flexible: Using environmental cues to regulate cognitive control




S
Xu






J
Simoens






T
Verguts






S
Braem




10.1037/xge0001488








Journal of Experimental Psychology: General




153


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]