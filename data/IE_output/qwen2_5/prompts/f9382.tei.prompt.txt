You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



may be reduced. There has been attempts at "debiasing" medical decision-making (for a review, see 
Ludolph & Schulz, 2018)
. There is sparse research on the use of heuristics in mental health diagnosis, but some studies have indicated some of the same phenomena as in general decision-making. Among the biases that have been found to influence decision-making in medical research are anchoring, confirmation bias and excessive confidence. 
(Curley et al., 1988;
Lund, 1925)
 has shown that the information presented first tend to have a larger influence on the judgement than information presented later. 
Tversky and Kahneman (1974)
 identified anchoring bias as a mechanism where a belief is established based on initial information, and that belief is not adequately adjusted by subsequent information. Anchoring may be caused by serial-position effects 
(Ebbinghaus, 1913)
, where the primacy of the information presented first gives it additional emphasis. The belief-updating model 
(Hogarth & Einhorn, 1992)
 has been suggested to account for various ordereffects. 
Crowley and colleagues (2013)
 argued that anchoring could influence medical diagnoses if the clinician "locks onto" salient symptoms presented early in the diagnostic process, leading to an initial diagnosis that influences the final diagnosis. Studies have shown "first-impressions" to have a large influence final diagnostic decision 
(Bösner et al., 2014
; see e.g., 
Kostopoulou et al., 2017)
. Similar studies of anchoring in mental health diagnosis have shown inconsistent findings 
(Ellis et al., 1990;
Friedlander & Stockman, 1983;
Richards & Wierzbicki, 1990)
. Nevertheless, several studies where participants assess written descriptions of mental health cases have shown that diagnoses tend to be compatible with the first-presented symptoms 
(Cunnington et al., 1997;
Richards & Wierzbicki, 1990)
. For instance, 
Parmley (2006)
 manipulated the order of symptom presentation in case descriptions, and showed that a third of clinicians failed to alter their initial diagnosis when presented with disconfirming evidence. 
Parmley predicted (2006, p.84
) that participants would "fail to alter their initial diagnosis even when new information presented at time two has disconfirming evidence".


1.2.1: Anchoring bias. Research on decision making


1.2.2: Confirmation bias.
We tend to seek or interpret information in ways that can corroborate or support our current beliefs, expectations or a hypothesis at hand. Information that is inconsistent with our beliefs may be ignored or toned down. The more general mechanism has been called a congruence bias 
(Baron et al., 1988)
, while the more specific term confirmation bias 
(Nickerson, 1998)
 is often used to describe situations where new information is interpreted as supporting one's current beliefs. The two terms have been used interchangeably in much of the empirical literature 
Xiao et al., 2021)
. In a diagnostic setting, such a mechanism could lead to closing the exploratory phase prematurely; accepting a diagnosis before it has been fully verified, or neglecting plausible alternatives 
(Croskerry, 2002
(Croskerry, , 2009
Eva, 2001;
Parmley, 2006)
. Studies of mental health diagnoses have shown confirmation bias in the selection of which information to gather 
(Martin, 2000;
Mendel et al., 2011)
 and in how information is interpreted 
(Croskerry, 2002;
Eva, 2001;
Oskamp, 1965;
Parmley, 2006)
.
Most studies of confirmation bias in diagnostic decisionmaking start out by indicating an incorrect diagnosis and examines whether participants are able to switch to the correct diagnosis when provided with additional information 
(Mendel et al., 2011)
. While this approach may make it easier to establish a confirmation bias, an approach where participants are allowed to arrive at an initial diagnosis based on ambiguous information may have higher ecological validity. Such an approach can also be used to examine how congruence influences the ongoing decision process.


1.2.3:
Overconfidence. In addition to evaluating the evidence, clinicians also need meta-cognition about how certain they are about their evaluation 
(Moore & Healy, 2008)
. Confidence in a decision may be affected by the perceived qualitative and quantitative aspects of available information, and on the existence of plausible alternatives (see 
Eva, 2001;
Martin, 2000;
Oskamp, 1965)
. 
Croskerry (2002)
 defined overconfidence bias as a universal tendency to believe that we know more than we do, or to place too much faith in opinions rather than evidence. Overconfidence may lead to diagnostic errors, for instance by leading to applying unsuitable heuristics 
(Berner & Graber, 2008)
. The phenomenon may vary across cultures 
(Yates, 2010)
. It may be difficult to say when the confidence in a decision is "excessive" or "unfounded". In the current approach we operationalize "overconfidence" as an increase in confidence when no further clarifying information is provided.


1.2.4: Interaction of biases in applied decision-making.
Confirmation bias may be closely associated with the anchoring effect, and these two mechanisms may interact to compound errors 
(Croskerry, 2002
(Croskerry, , 2003
. First, clinicians may "lock onto" salient symptoms early in the diagnostic process, which leads them towards a preliminary diagnosis (anchoring). Subsequent processes of seeking and interpreting additional information may be biased towards this initial hypothesis, while alternative plausible explanations may be ignored (confirmation).
Some studies 
(Martin, 2000;
Oskamp, 1965)
 have indicated that confirmation bias may interact with the degree of confidence to influence how diagnostic judgment interprets inconclusive symptom information. 
Croskerry (2002)
 specified that overconfidence may be augmented by anchoring. "Locking onto" salient information early in an encounter with a patient may make the clinician confident that this information is particularly important. In turn this may affect the formation and rigidness of a preliminary diagnosis. Clinicians that feel confident about their diagnostic decision may be more biased in their search and interpretation of additional information (see 
Martin, 2000;
Oskamp, 1965)
. Confidence may thus influence a clinician's decision process, and may act as both an effect and a cause for the cognitive biases.


1.3: Research gap
There is strong empirical basis for cognitive biases like anchoring, confirmation and overconfidence. Given that they have been argued to be relevant for diagnostic decisions 
(Graber et al., 2002)
, it is surprising that there have been inconsistent results when testing how biases influence mental health diagnostics 
(Ellis et al., 1990;
Friedlander & Stockman, 1983;
Oskamp, 1965;
Parmley, 2006;
Richards & Wierzbicki, 1990
). It may not be well established how biases contribute to diagnostic errors, and there are few empirical studies addressing this 
(Norman et al., 2017)
.
It has been argued 
(Saposnik et al., 2016)
 that most studies on the topic are of low quality. There is a need for experiments that control for some of the issues these studies had, like the balance between the severity of the cases and the amount of information provided simultaneously 
(Richards & Wierzbicki, 1990)
. Further, few studies have examined how the biases relate to both seeking and interpreting information (with some exceptions, see 
Martin, 2000;
Mendel et al., 2011)
, and their effects on the diagnostic decision processes.
There appears to be no controlled investigation of how the co-occurrence of various cognitive biases may influence a diagnostic decision process. It would be of value to combine the testing of both anchoring and confirmation bias in one experimental design, and to model decisionmaking as a sequential process where information is gathered and confidence in the decision may vary over time.
Finally, as only a minority of studies use medically trained personnel 
(Blumenthal-Barby & Krieger, 2015)
, it would be of value to establish similar effects in a reasonably realistic problem field in which the participants had relevant training. Relatedly, it has been identified as a gap in the literature that few studies use clinical guidelines to determine diagnostic or treatment errors 
(Saposnik et al., 2016)
.
1.4: Current study 1.4.1: Aims for the current study. The current study seeks to develop an experimental paradigm for testing the interaction of anchoring, confirmation and confidence on seeking information, evaluating information, and making diagnostic decisions. To achieve this, we designed a basic experimental procedure that measures information gathering, choice of and confidence in decisions. The experiment can measure (1) whether participants prefer diagnoses that match the first-presented symptoms, (2) whether participants seek information to corroborate the assumption they currently hold, (3) whether levels of confidence predict corroborating information seeking, and (4) whether diagnosis and confidence changes across the diagnostic process.


1.4.2:
Hypotheses. Based on previous research on anchoring, we expected that the order in which symptoms are presented will affect the choice of a preliminary diagnosis. Our first hypothesis was thus that (H1) participants will be more likely to select the preliminary 
1
 This hypothesis can only be tested on participants that show this response pattern. As this applied to only 12, 20 and 21 participants diagnosis congruent with the first-presented symptoms in a case vignette, rather than selecting the diagnosis congruent with symptoms presented later.
Based on previous research on congruence, we expected that participants would primarily seek out information that appeared to confirm their existing diagnostic beliefs. Our second hypothesis was thus that (H2) participants would request more information related to the diagnosis they already preferred, rather than seeking out information that may support an alternate diagnosis.
Furthermore, we expected such confirmatory styles of information gathering to correspond to higher levels of confidence in one's existing diagnostic beliefs. The third hypothesis was thus that (H3) when confidence was higher more confirmatory information would be preceded requested.
We expected participants who did not change their mind or explore other alternatives would end up with more confidence in their diagnostic decision. The final hypothesis was thus that (H4) participants who prefer the same diagnosis throughout the case exploration and only request confirming information should increase in confidence between their first and final diagnostic decision. 1 1.4.3: Procedure and preregistration. All four hypotheses were explored in three experiments. Hypotheses were preregistered ahead of each experiment's data collection (https://osf.io/dn4rv/registrations). The experiment procedure was tested in classroom experiments in order to efficiently collect data from participants with medical training. Each experiment was done consecutively, building on the analysis of the preceding experiments to allow for iterative improvements in experiment design. This allowed better control for competing hypotheses, and to further investigate earlier findings. This led to removing some details in case descriptions and making a few changes in the materials for Experiment 2 and 3 to make the manipulation more effective (see details below). For further information see the preregistrations and experiment details (https://osf.io/dn4rv/).


2: Methods


2.1: Participants
Participants were advanced medical students with extensive education in somatic and mental health issues, and were drafted from a university hospital in Norway. Most of the students that were present in three lectures participated, constituting 71, 56 and 91 students respectively for the three experiments. In accordance with the preregistration, Experiment 3 was completed in two separate data collection sessions, as the initial session provided a lower turnout than required.
Demographic variables were not collected in any of the experiments to preserve a sense of anonymity. However, we judged the student population from which the sample was drawn to be predominantly female (about 75%), and in their mid-twenties. A lottery was conducted immediately after in the three experiments, it is underpowered to be tested individually, but was tested pooled across experiments. each data collection, in which about 5% of the participants won a gift card for a lunch meal at a campus cafe.


2.2: Experiment overview
Each experiment was conducted in an auditorium during a break in the lecture that the participants attended. The lecturer introduced the experimenters to the classes, and described the project as an investigation of decision-making under uncertainty. The experimenters informed the students that participation was voluntary and anonymous, and that they could withdraw from the investigation at any time without consequences. As the experiment came in the form of an online survey, participation was possible through laptop computers, tablets and smartphones. No personal information such as names or email addresses was collected. The online questionnaires (in Google Forms) were accessed through the university internet connection, causing the IP address to be the same for all participants.
All three experiments had the same overall structure, where two fictitious patient cases were evaluated (see 
Table  1
 for an overview of the steps in responding to each case). Completion of the experiment took about 10 minutes. 
Table 1
: Experiment procedure for all three experiments. All but the last step was presented online.
Step Experiment procedure step 1. Introduction and description of the experiment procedure 2.
Introduction of the two diagnostic options (A and B), and presentation of the ICD-10 diagnosis criteria for both options 3.
First half of initial case description, randomized to present symptoms either in support of A or B 4. T1a choice (Experiment 3 only): State confidence in one of the two diagnoses: A or B


5.
Second half of initial case description, presenting the symptom information not presented in step 3 (supporting either B or A)
6. T1b choice: State confidence in one of the two diagnoses: A or B 7. T1 request: Select one of four options to further explore one of the diagnoses: A1, A2, B1, or B2
8. T2 choice: State confidence in one of the two diagnoses: A or B 9. T2 request: Select one of four options to further explore one of the diagnoses: A1, A2, B1, or B2
10. T3 choice: State final confidence in one of the two diagnoses: A or B


11.
End of the first case, repeat step 2-10 for the second case where A and B are replaced with C and D


12.
Four follow-up questions about what the participant assumed the research hypotheses to be (in Experiment 3 only)
13. Short oral debrief at the end of experiment


2.3: Materials and experiment procedure
We developed text descriptions of two hypothetical patient cases. Both featured symptom information that could be read to support one of two mental health diagnoses as listed in the ICD-10 diagnosis manual. The cases were developed similarly as those used by 
Parmley (2006)
, though considerably shorter. A clinical psychologist verified the materials were verified as clinically relevant for mental health diagnosis. A medical doctor in charge of the medical training evaluated the experiment procedure as a relevant test of the diagnostic approach. The first case presented a choice between (A) dementia and (B) depressive episode, while the second case presented a choice between (C) bipolar mood disorder and (D) borderline personality disorder. The participants were instructed to base their decisions on these criteria, rather than any prior knowledge they had about the diagnoses or assumptions about epidemiology. Except for a few minor changes, the material remained unchanged for all three experiments, and all participants received the same cases in the same order (see 
Table 1
 2 Experiment 1 and 3 presented two symptoms for each of the diagnoses. In an attempt to make the manipulation stronger, Experiment 2 presented two symptoms for the first diagnosis and only one for the other diagnosis. Preceding experiments were also used to adjust the wording of the cases in order to make them be selected equally often. 3 
Figure 1
 illustrates the extent to which this was successful. When Experiment 1 indicated a preference for one diagnosis independent of symptom order, the case description for Experiment 2 and 3 were edited to adjust for this. above). The two diagnostic options and the corresponding ICD criteria were presented before each of the case descriptions (step 2 in 
Table 1
). Each case consisted of an initial vignette describing a hypothetical patient. This vignette first (step 3) presented all the symptoms that supported one of the diagnoses, and then (step 5) presented all the symptoms supporting the opposite diagnosis. 2 In addition, "neutral" symptoms that are compatible with both diagnoses were included to avoid the contrast between the other pieces of information becoming too obvious. The full case descriptions are available as supplemental materials online (https://osf.io/dn4rv). The initial case descriptions were intended to present equally persuasive arguments for both of the diagnoses, without conclusively supporting either of them. 3 4 Participants were subsequently (step 6) asked to decide on a tentative diagnosis (instructed to "select a diagnosis and indicate your confidence in it"). The response was made by clicking on a 10-point scale on which the extreme ends represented the highest degree of certainty for the case's two diagnoses. In Experiment 3, participants also had to answer the same question when they were halfway through the initial case description (step 4), after only symptoms supporting one of the diagnoses had been presented. This was done as a manipulation check for whether the first half of the symptoms led to a compatible decision, and to check whether being forced to make an early decision would enhance a confirmation bias.
After indicating their initial diagnosis (step 7), the participants were asked to select one of four options for getting more information about the symptoms (such as request A2 "Reduced language skills may indicate dementia. Ask the patient about her language use and verbal skills."). Two of the options were explicitly marked as seeking more information about symptoms for one of the diagnoses, while the remaining two were marked as seeking information about the other diagnosis. After selecting an option (step 8), participants received additional information (between 33 and 80 words) relevant to the diagnosis they had selected, but worded in a way that did not conclusively point to either of the diagnoses (such as "[The patient] has thought of herself as polite and articulated but has recently been told by her family that she can be wicked, vulgar and condescending. [The patient] says that this only happens when she talks about topics that upset her."). After receiving the followup information, the participants were again instructed to indicate their confidence in either diagnosis, by responding on the same 10-point scale as in step 6. This was followed (step 9) by a second opportunity to seek follow-up information, choosing among the same four options as before. 
5
 After receiving the second follow-up information, participants were then (step 10) asked to set their final diagnosis in the same way as before.
Step 2-10 were then repeated for the second case in the experiment.
For Experiment 3, four debrief questions were included at the end of the questionnaire (step 12). The first two explored the participants' thoughts about the aim of our study, while the latter two asked about the strategy they had used in their decision-making. The aim of these questions was to check whether participants may have guessed the research hypotheses, and whether this had affected their responses. Additionally, we wanted to explore whether the participants were aware of their own decision-making strategy. Participant answers were scored separately by 3 coders and compared for inter-rater reliability. The raters initially scored 1.8% of the participants differently, which were resolved by discussion.
After completing the online experiment (step 13) the participants were quickly debriefed about the purpose of the experiment, any questions the participants had were answered in plenary, and the gift cards were distributed. Due to time constraints only a short debrief was given verbally in class, while a more thorough debrief and results summary was sent on email).
Data preparation was done in a Google Sheet synchronized to Open Science Framework to provide transparency and version history for all data transformations, and can show the calculation of the indices described below. Statistical analyses were performed in RStudio. Due to our preregistered directed hypotheses, we used one-tailed tests, with a standard alpha cut-off of p < .05. All data (https://osf.io/t3zh4/) and analysis files (https://osf.io/nye24/) are available online.
More detailed descriptions of the experiment procedures and text descriptions of the cases in the original Norwegian and translated to English are available in the preregistrations for each of the experiments (https://osf.io/dn4rv/ registrations). 5 It was possible to request the same follow-up information twice, but this only happened for 1.82% of the cases across all participants. This indicates that the far majority of participants were invested in and attentive to the task. 


3.1: Tests of anchoring
Hypothesis H1 stated that anchoring would lead to the information presented first having a larger effect on the decision, despite overall presenting equal evidence for both diagnoses. An index was created to reflect whether the initial diagnosis (at T1b) matched the symptoms presented first in the descriptions for the two cases (thus having a value of 0, 1 or 2). The null-hypothesis of no effect of symptom order would predict that by chance participants would select the diagnosis matching the symptom presented first in one of the two cases. H1 was thus assessed with a one-sample one-tailed t-test for whether the diagnosis matched the first-presented symptoms for more than one case.
3.1.1: Test of H1 in Experiment 1. For Experiment 1 the number of initial diagnoses matching the first symptoms (M = 1, SD = .74) was identical to the reference constant of 1, thus failing to show a significant difference (t(70) = 0, p = .5).


3.1.2: Test of H1 in Experiment 2. Similarly for
Experiment 2, the number of initial diagnoses matching the first symptoms (M = 1, SD = 0.71) was identical to the reference constant of 1, thus failing to show a significant difference (t(55) = 0, p = .5).


3.1.3: Test of H1 in Experiment 3. Experiment 3 included
a manipulation check by asking participants to make an additional preliminary diagnosis after reading the first half of the symptoms in each case (at T1a). This was done in order to test whether participants in fact indicated the diagnosis supported by the only symptoms they had read so far, and to test whether forcing them to make a decision at this point could lead to anchoring or confirmation bias (on the T1b request). The number of diagnoses at T1a matching the first symptoms (M = 1.53, SD = 0.60) was higher than the reference constant of 1, indicating that the participants were consistent with the symptom information presented so far.
As in Experiment 1 and 2, the H1 test of anchoring in Experiment 3 used responses from the diagnostic decision made after hearing the full initial case description (T1b). The number of diagnoses matching the first symptoms (M = 0.63, SD = 0.69) was lower than the constant of 1, and thus not significant in one-tailed testing against a higher value than 1 (t(90) = -5, p = 1).


3.1.4: Test of H1 pooled across experiments.
In an exploratory follow-up analysis of data from all three experiments was collapsed to make a more robust assessment of H1. The test remained non-significant despite higher power (t(217) = -3.06, p = .99).


3.2: Tests of confirmation bias
Hypothesis H2 stated that a confirmation bias would lead participants to seek out information that could support the diagnosis they already preferred. An index was created to reflect the number of times the participants requested information that could support the diagnosis they had stated to prefer (on T1b and T2). The null-hypothesis of no confirmation bias predicted that participants would be equally likely to investigate the preferred diagnosis as the alternate diagnosis, and would thus seek confirmatory information at two of the four possible opportunities across the two cases. H2 was thus assessed with a one-sample onetailed t-test of whether participants made more than two confirmatory requests.


3.2.1: Test of H2 in Experiment 1.
For Experiment 1, the average number of requests for confirming information (M = 1.93, SD = 0.76) was slightly lower than the reference constant of 2 (t(70) = -0.78, p = .78), and thus did not support the hypothesis.


3.2.2: Test of H2 in Experiment 2.
For Experiment 2, the average number of requests for confirming information (M = 2.23, SD = 1.04) was higher than the reference constant of 2, but barely failing to meet our significance criteria (t(55) = 1.66, p = .051).


3.2.3: Test of H2 in Experiment 3.
For Experiment 3, the average number of requests for confirming information (M = 2.16, SD = 0.95) was somewhat higher than the reference constant of 2, again barely failing to meet our significance criteria (t(90) = 1.64, p = .052).


3.2.4: Test of H2 pooled across experiments.
When collapsing participants across all three experiments, the test for H2 was significant (t(217) = 1.68, p = .047). It should be noted that the effect was small (d = 0.11), reflecting that there were on average 2.11 (of 4 possible) requests for confirmatory information.


3.3: Tests of confidence leading to confirmation
Hypothesis H3 stated that higher confidence in the diagnostic choice should lead to more confirmatory information seeking. Both diagnostic choices for both cases were calculated to a confidence rating (between 1 and 5). H3 predicted than at both time points (T1b and T2), confidence would be higher for the diagnostic questions that preceded request for confirmatory information than for the diagnostic decision that preceded requests for dissenting (null-hypothesis predicted no difference). H3 was tested with one-tailed t-tests for dependent samples for whether confidence was higher for confirmatory than for dissenting requests.


3.3.1: Test of H3 in Experiment 1.
For Experiment 1, the confidence on diagnoses preceding confirmatory request (M = 2.43, SD = 0.85) was somewhat lower than the confidence on diagnoses preceding dissenting request (M = 2.64, SD = 0.88). The effect was thus in the opposite direction of the H3 prediction (t(66) = -1.76, p = .96).


3.3.2: Test of H3 in Experiment 2.
For Experiment 2, the confidence on diagnoses preceding confirmatory request (M = 2.38, SD = 0.91) was very close to the confidence on diagnoses preceding dissenting request (M = 2.38, SD = 1.08), not reaching significance (t(46) = .01, p = .49).


3.3.3:
Test of H3 in Experiment 3. For Experiment 3, the confidence on diagnoses preceding confirmatory request (M = 2.19, SD = 0.75) was close to the levels of confidence on diagnoses preceding dissenting request (M = 2.08, SD = 0.85), not reaching significance (t(80) = 1, p = .16).


3.3.4: Test of H3 across all three experiments.
When collapsing participants across all three experiments, the test for H3 remained non-significant (t(194) = -0.39, p = .65).


3.4: Test of decision process influencing confidence
Hypothesis H4 stated that having a consistent decision and only seeking confirming information should be associated with increased confidence. An index was calculated for participants that preferred the same diagnosis on all three diagnostic questions for a case (T1b, T2 and T3), and that requested congruent information at both opportunities for the case (T1b and T2). The index calculated the change in confidence from the first (T1b) to the last (T3) decision for the case (this could vary from -3 to +3). For participants where both cases fit the criteria, an average of the confidence change in the two cases was used. Since few participants fit these criteria (28%), this was not tested separately for each experiment but was tested pooled across experiments (n = 63). H4 was tested as a one-sample onetailed t-test that there would be a positive change in confidence. This showed that on average these participants increased their confidence (M = .63, SD = 1.39), which was a significant change in the predicted direction (t(67) = 3.75, p < .001, d = 0.45).


3.5: Follow-up analyses 3.5.1: Analysis after removing non-naive participants.
Experiment 3 included questions about what the participants thought the research hypotheses were. These were used in preregistered follow-up analyses that tested whether the hypotheses listed above would be supported when excluding participants that appeared to have correctly guessed the hypotheses. After reviewing the responses, we excluded 19 participants who appeared to have fully or partly guessed the central research questions or any of the hypotheses. This left 71 participants for follow-up analyses. The analyses still showed no significant effects for the three hypotheses (H1: t(71) = -4.39, p = 1, H2: t(71) = 1.07, p = .14, H3: t(63) = 0.86, p = .2).


3.5.2: Two-tailed tests.
All of the tests listed above were one-tailed due to directed hypotheses based on the literature. However, some of the tests (H1 and H3) had effects in the opposite direction than predicted. We thus perform exploratory follow-up two-tailed analyses pooled across experiments in order to examine these patterns. For H1 there was a significant effect (t(217) = -3.06, p = .003, d = .21) of participants selecting the diagnosis matching the last-presented symptoms. There were no significant twotailed effects across experiments for H2 (t(217) = 1.68, p = .094, d = .11) or for H3 (t(194) = -0.93, p = .7, d = .03). 
4.1: Biases in diagnostic decision-making  4.1.1: Summary of results.
 The aim of the current study was to develop an experimental procedure for studying the interaction of anchoring, confirmation and confidence across the decision process. Further, we wanted to test the experiment in moderately sized samples of decision-makers with relevant expertise. To this end, we performed three classroom experiments where advanced medical students were asked to diagnose two hypothetical mental health cases. Across three experiments, we found no support of anchoring (H1), marginal support for confirmation bias (H2), no support for confidence increasing confirmatory information seeking (H3), and support for confidence increasing when being consistent throughout the decision process (H4). The results for all hypotheses are summarized in 
Figure 2
, and each of them are discussed in more detail below.


4: Discussion


4.1.2: No indication of anchoring.
We expected that (H1) the first-presented symptoms would create an anchor for reading the rest of the case description. This would lead participants to favor the diagnosis matching the firstpresented symptoms rather than the diagnosis matching the last-presented symptoms. However, this was not supported in any of the three experiments. In fact, all three experiments showed an effect in the opposite direction, such that the diagnosis more often matched the lastpresented symptoms. This is in contrast to previous studies that have found anchoring for medical decision-making in similar designs that manipulated the order of symptom presentation (see e.g., 
Cunnington et al., 1997;
Richards & Wierzbicki, 1990)
.
The missing anchoring effect leads us to re-examine the stimulus materials. 
Richards and Wierzbicki (1990)
 argued that it can be challenging to create case materials that are sufficiently balanced so that the mere order of symptoms is sufficient to tip the scales in favor of a given diagnosis, while avoiding imbalances due to the differences in description length or severity of symptoms. When designing the current materials, we also attempted to strike a balance between describing symptoms that pointed towards a specific diagnosis, while being sufficiently ambiguous to not conclusively eliminate the competing diagnosis. If our case descriptions were not informative or balanced between the diagnoses, this may have prevented the experiments from producing an anchoring effect. Our counterbalancing of diagnosis order (i.e. half the participants had symptoms of diagnosis A first, while the other half had diagnosis B first) should decrease any effects of poor balance between the diagnoses. Further, examining the data shows that the initial decisions had a roughly normal distribution centered around having low confidence in one of the diagnoses, which indicates that the case descriptions are fairly wellbalanced.
It should be emphasized that instead of showing an anchor effect in preferring the diagnosis that matched the first-presented symptoms, our results showed the opposite pattern, of preferring the diagnosis matching the lastpresented symptoms. This could indicate a recency effect 
(Botto et al., 2014;
Murdock, 1962;
Tzeng, 1973)
. Such an effect could lead to last-presented symptoms being more available in working memory, and thus having a larger impact on the decision (see similar effects in e.g., 
Bergus et al., 1995;
Tubbs et al., 1993)
. The current design and results could also be compared to a "serial anchoring" effect (Bahník et al., 2019). When using two "anchors" in opposite directions, they found the second anchor to impact the decision.
It should be noted that mixed results have been reported when attempting to conceptually replicate previous research designs for medical decision-making 
(Ellis et al., 1990;
Friedlander & Stockman, 1983)
. It is possible that the complexity inherent in real-life decisions among expert decision-makers makes it difficult to replicate anchor effects that have been shown for naive participants making abstract decisions. This could be due to the decision being affected by various prior assumptions, strategies and preferences 
(Hutton & Klein, 1999)
. Nevertheless, our manipulation check (the decision at T1a) indicated that participants favored the diagnosis consistent with the information that had been presented so far.


4.1.3: Confirmatory information seeking.
Confirmation bias was operationalized as the participant requesting follow-up information that could confirm the diagnosis they preferred on the preceding decision (rather than information that could confirm the opposing diagnosis). We expected (H2) that there would be more requests for confirming, rather than dissenting information. This test for confirmation bias approached significance in Experiments 2 and 3, and was significant when collapsed across the three experiments.
The study as a whole thus found indications of confirmation bias when making medical diagnostic decisions, in terms of seeking information that could support the diagnosis one currently holds, rather than seeking information that could falsify the assumption. This finding is compatible with other studies 
(Martin, 2000;
Mendel et al., 2011;
Oskamp, 1965;
Parmley, 2006)
. This indicates that the previously identified confirmation bias phenomenon 
(Jonas et al., 2001;
Schulz-Hardt et al., 2000)
 extends to novel experimental settings. Further, this indicates that the confirmation bias could be relevant for the mental health domain. Decision-makers in this domain should thus be aware that such a bias can lead to nonoptimal information gathering and decision-making 
(Blumenthal-Barby & Krieger, 2015)
.
However, while the argument can be made that the current study as a whole found indications for a confirmation bias, it should be noted that the effect sizes were small, and were close to the preregistered significance threshold. This is in contrast with previous research 
(Martin, 2000;
Mendel et al., 2011;
Oskamp, 1965;
Parmley, 2006)
 that has indicated the confirmation bias to be a robust effect that should reliably reproduce. Nevertheless, most of the studies on biases in diagnostic decision-making also appear to have small effects, which resemble our pooled results. A possible reason for the weak effect in our study could be that we by design used materials that gave ambiguous feedback on the follow-up requests. This may have confused or annoyed the participants, which could motivate them to use a more analytical mode of thinking (see 
Croskerry, 2009)
.
We further do not know the participants' motivation for asking follow-up requests. We assumed that participants asked about symptoms in order to support the associated diagnosis if they found that the symptom is present. However, some participants could have asked about a symptom in order to discount the associated diagnosis if the symptom is absent. If so, such participants' confirmatory information-seeking strategy would not be registered as confirmation in our operationalization. However, similar mechanisms may also have been present in previous studies of confirmation bias, and may be difficult to fully discount in behavioral designs.


4.1.4:
Confidence did not lead to confirmatory information seeking. We expected (H3) that confidence would be higher for the diagnostic decision that was followed by requests for confirmatory information, than for the ones followed by requests for dissenting information. If so, this could indicate a mechanism for confirmation bias. However, although there was an overall tendency for seeking confirmatory information (see H2), there was no support in any of the experiments for increased confirmatory information seeking when participants were more certain of their diagnostic decision. This null-finding thus fails to support a previous finding 
(Martin, 2000)
 of higher confidence leading to more confirmatory information seeking. The absence of such an effect may indicate that confirmatory information seeking is not motivated by the degree of confidence in the preferred decision.
It should be noted that we used a novel response mode in the current experiments, where participants indicated their graded confidence between either diagnosis on a 10point scale. It is possible that participants' use of the scale did not represent their actual degree of confidence. However, note that most participants indicated low confidence in their decisions (see 
Figure 1
), which may be expected given the ambiguous case descriptions and followup information, and may thus be taken to support our measure of confidence. 4.1.5: Increased confidence when not exploring alternatives. We expected (H4) that participants who favored one diagnosis (at T1, T2 and T3 choice) and only sought information about that diagnosis (at T1 and T2 request) would show an increase of confidence in their decision. This was supported (when pooled across experiments), in the sense that the consistent participants increased their confidence during the decision process. As the follow-up information was designed to be ambiguous and should not provide the participants with any additional conclusive information, the increase in confidence could be said to indicate overconfidence 
(Oskamp, 1965)
. Similar effects have been seen in a study that presented more conclusive information 
(Martin, 2000)
, showing that experts were less confident than novices. While subject to cultural variation 
(Yates, 2010)
, overconfidence has previously been shown in Norwegian settings 
(Bratvold et al., 2020;
Jørgensen et al., 2004)
, but to our knowledge not in a medical domain.


4.2: Limitations and suggestions 4.2.1 Alternative experiment designs.
The current study had a novel experiment design in order to test the combination of anchoring, confirmation and confidence on seeking and evaluating information throughout the decision process. To ensure sufficient samples, the experiment was designed to be short so that it could be completed in a break between two lectures. Providing participants with only a limited amount of information about each case may have made it easier to deliberately process the information. This may have resulted in less heuristic processing than in other experiments with more comprehensive patient information, which may account for deviating results (see e.g., 
Mendel et al., 2011)
. One may argue that providing more information would be a more valid representation of real-life mental health decisions.
The case descriptions were designed to be ambiguous and for follow-up requests to not provide any conclusive information. It could be that anchoring had emerged if a clearer indication had been indicated for a given diagnosis in the beginning of the case. However, it may have stretched the validity of the case to first clearly indicate one diagnosis for it later to be contradicted. Another approach could be to provide participants with feedback on the follow-up requests that would resolve their uncertainty. Participants may have used the requests more actively in such a situation, which may have shown a confirmation bias more clearly. However, this would not fit with the current aim of exploring a multi-stepped decision process.
A possible reason for the lack of an anchor effect in the current study may be that participants did not commit to a decision after reading the first symptoms. To test for this, Experiment 3 asked participants to make a preliminary diagnosis (T1a) after hearing only the first half of the initial vignette (all symptoms indicating the same diagnosis). However, this did not have the expected effect that the diagnosis after hearing the full initial vignette (T1b) to be more in line with the preliminary diagnosis. On the contrary, while the decision after hearing all symptoms in Experiment 1 and 2 were evenly distributed between the two diagnoses, in Experiment 3 there was a strong preference for the diagnosis to match the second half of the symptoms. This may be due to demand characteristic effects in Experiment 3 
(Orne, 2006;
Rosenthal, 1963;
Strohmetz, 2008)
, that when participants were asked to diagnose again after being given additional information they believed that a different response was expected. Another approach to this issue may have been to present audio or video recording of patient cases, so that participants had to make evaluations "on the spot" without being able to re-read or reconsider previous information.


Possible participant bias.
Participants knowledge or assumptions about the research hypothesize may change their responses towards or away from the hypothesized results. This was indicated in informal debrief conversations after Experiment 1 and 2. A debriefing survey at the end of Experiment 3 indicated that about a fifth of the participants correctly guessed one or more of the research hypotheses. Removing these participants did not change the significant results from Experiment 3. Nevertheless, such artefacts may have impacted the results in Experiment 1 and 2, as well as in other published literature where prior knowledge is not controlled for.


Statistical power.
The sample size of our current three experiments were restricted by practical concerns (the number of medical students at our local university). The three studies separately (average n = 72) had sufficient power to detect effects of Cohen's d = .36 or larger (given power of .8 and alpha of .05 one-sided). Alternatively, pooling all participants (n = 224) gives sufficient power to detect effects of d = .21. The studies on decision making in mental health cited above often fail to provide sufficient information to calculate effect sizes for anchoring, confirmation bias and overconfidence. Some of the studies (e.g. 
Richards & Wierzbicki, 1990)
 have described their effects to be between weak and moderate (thus corresponding to between 0.2 and 0.5). If so, the current study is more or less sufficiently powered to detect effects of this magnitude, at least when pooled across experiments. It should be mentioned that previous experiments that have established these effects typically have had low power.
Nevertheless, the lack of statistically significant effects in the current study should be interpreted with care, as the experiment may have been underpowered to detect effects. It is possible that the examined biases have weaker effects than previously assumed when tested in somewhat realistic problem-fields in which the decision-maker has relevant expertise. It may be necessary with higher-powered studies and salient manipulations to demonstrate these biases.


4.3: Implications and future research
This is the first study investigating the interaction of anchoring, confirmation and overconfidence bias during a decision process for trained decision-makers making mental health diagnoses. The current experiment procedure and materials may inspire similar explorations of medical decision-making. A transparent research process with preregistration, open materials and data may assist the planning of future studies. To improve ecological validity and measurement specificity one may consider expanding the number of clinical cases, symptom information or follow-up requests. Another approach could be to measure meta-knowledge during the decision process, similar to what we did at the end of Experiment 3. A relevant research program may be to use more salient manipulations in order to "force" the biases, and then gradually making the manipulation more subtle in order to explore whether the biases can still be shown in ecologically valid settings.
The current study failed to find anchoring and yielded unclear results for confirmation bias. This may indicate that reproducing the biases for real-life decision-making may be more challenging than what has been indicated in past literature. While several studies have detected these biases in similar settings 
(Mendel et al., 2011;
Parmley, 2006;
Richards & Wierzbicki, 1990)
, there have also been studies failing to demonstrate the biases 
(Christensen et al., 1995;
Ellis et al., 1990;
Weber et al., 1993)
. Above we argue that the complexity of the medical diagnostic process may yield more opportunities for biases than in simpler one-off decisions (see similar arguments in e.g., 
Blumenthal-Barby & Krieger, 2015;
Saposnik et al., 2016)
. However, such complexity also allows for different processes and interactions to be in effect. Biases that have previously shown in simpler situations be relevant only in a subset of these processes, or that they may be countered by other processes. ognitive biases such as "loss aversion" have been shown to be moderated by factors such as domain knowledge, experience and education 
(Mrkva et al., 2020)
.
The literature has shown that cognitive biases can be produced in certain settings when using specific manipulations. Norman and Eva 
(Norman & Eva, 2010, p. 97)
 argued that some previous demonstrations of biases may "induce error for the sake of determining if the biases exist". Alternative approaches such as the "Naturalistic decision making" 
(Klein, 2015)
 and "ecological rationality" 
(Gigerenzer, 2008)
 have expressed doubts about the extent to which such biases will detriment experts' real-life decision-making. Recent development in psychological science 
(Ioannidis et al., 2014)
 have emphasized how research practices such as analysis flexibility, selective publication and conceptual (rather than direct) replications may have generated and propagated false positive findings. This makes it difficult to know whether published effects generate to other settings. Such practices may have contributed to overestimating the reliability of anchoring and confirmation biases, and may make it difficult to identify the necessary conditions for producing the effects. This may partly explain why the current results deviate from the majority view in the literature.
and ØKB developed the case descriptions and did data collection for Experiments 1 and 2. VTS did data collection for Experiment 3. BS managed the project. All authors contributed in writing the first draft of the manuscript, while BS wrote and edited the current version of the manuscript. Results from Experiments 1 and 2 have previously been part of a master thesis by TF & ØKB, supervised by BS (http://hdl.handle.net/1956/17805). Thanks to our collaborators at the Faculty of Medicine: Anders Baerheim, Øystein Hetlevik, and Thomas Mildestvedt. Thanks to the participants in the three experiments.
Figure 1 :
1
Answers on the response scale indicating the distribution of confidence in diagnosis for case 1 (black) and case 2 (gray) pooled across experiments. Small adjustments were made to the case descriptions between experiments to balance the responses. This resulted in later experiments being closer to a normal distribution (see additional figures online).


Figure 2 :
2
Illustration of the number of responses that support for each hypothesis pooled across the experiments.


In order to counterbalance for effects of the order that the diagnoses were listed in (rather than the intended effect of the order of presenting the symptoms), two different versions of each experiment were made, with the two diagnoses for each case in the order ABCD and BADC. Approximately half of the class (assigned by seating) was asked to answer the ABCD form, while the other half was asked to answer the BADC form. This counterbalancing variation was collapsed in the analysis, as the operationalization of responses only counted whether the selected diagnosis matched the first-presented symptoms or second.








Acknowledgements
The theoretical framework and the experiment designs for the current study was developed by BS, TF and ØKB. TF












Variations on anchoring: Sequential anchoring revisited




Š
Ahník






,
Oudek






L
Vrbová






J
Ájek




10.1017/S1930297500005428








Judgment and Decision Making




14


9














Heuristics and biases in diagnostic reasoning: II. Congruence, information, and certainty. Organizational Behavior and Human Decision Processes




J
Baron






J
Beattie






J
C
Hershey




10.1016/0749-5978(88)90021-0








42














Confirmation and matching biases in hypothesis testing




J
Beattie






J
Baron




10.1080/02724988843000122








The Quarterly Journal of Experimental Psychology
















Clinical reasoning about new symptoms despite preexisting disease: Sources of error and order effects




G
R
Bergus






G
B
Chapman






C
Gjerde






A
S
Elstein








Family Medicine




27


5
















Overconfidence as a Cause of Diagnostic Error in Medicine




E
S
Berner






M
L
Graber








The American Journal of Medicine




121


5










Supplement










10.1016/j.amjmed.2008.01.001














Cognitive Biases and Heuristics in Medical Decision Making: A Critical Review Using a Systematic Search Strategy




J
S
Blumenthal-Barby






H
Krieger




10.1177/0272989X14547740








Medical Decision Making




35


4
















Diagnosing headache in primary care qualitative study of G s' approaches




S
Bösner






S
Hartel






J
Diederich






E
Baum




10.3399/bjgp14X681325








The British Journal of General Practice




64


626
















When working memory updating requires updating: Analysis of serial position in a running memory task




M
Botto






D
Basso






M
Ferrari






P
Palladino




10.1016/j.actpsy.2014.01.012








Acta Psychologica




148
















Production Forecasting: Optimistic and Overconfident-Over and Over Again




R
B
Bratvold






E
Mohus






D
Petutschnig






E
Bickel




10.2118/195914-PA








SPE Reservoir Evaluation & Engineering




23


03
















Pervasiveness of framing effects among physicians and medical students




C
Christensen






P
Heckerung






M
E
Mackesy-Amiti






L
M
Bernstein






A
S
Elstein




10.1002/bdm.3960080303








Journal of Behavioral Decision Making




8


3
















Achieving Quality in Clinical Decision Making: Cognitive Strategies and Detection of Bias




P
Croskerry




10.1197/aemj.9.11.1184








Academic Emergency Medicine




9


11
















The Importance of Cognitive Errors in Diagnosis and Strategies to Minimize Them




P
Croskerry




10.1097/00001888-200308000-00003








Academic Medicine




78


8
















A Universal Model of Diagnostic Reasoning




P
Croskerry




10.1097/ACM.0b013e3181ace703








Academic Medicine




84


8
















Automated detection of heuristics and biases among pathologists in a computerbased system




R
S
Crowley






E
Legowski






O
Medvedeva






K
Reitmeyer






E
Tseytlin






M
Castine






D
Jukic






C
Mello-Thoms




10.1007/s10459-012-9374-z








Advances in Health Sciences Education




18


3
















The effect of presentation order in clinical decision making




J
P W
Cunnington






J
M
Turnbull






G
Regher






M
Marriott






G
R
Norman




10.1097/00001888-199710001-00014








Academic Medicine




72


10










Suppl








Primacy effects in clinical judgments of contingency




M
J
Curley






M
J
Young






M
J
Kingry






F
Yates








8














Memory; a contribution to experimental psychology




H
Ebbinghaus










New York city, Teachers college, Columbia university






with Robarts -University of Toronto












Anchoring errors in clinical judgments: Type I error, adjustment, or mitigation




M
V
Ellis






E
S
Robbins






D
Schult






N
Ladany






J
Banker




10.1037/0022-0167.37.3.343








Journal of Counseling Psychology




37


3
















The influence of differentially processing evidence on diagnostic decision-making




W
K
Eva














McMaster University






PhD Thesis








Anchoring and publicity effects in clinical judgment




M
L
Friedlander






S
J
Stockman




10.1002/1097-4679


4<637::AID- JCLP2270390433>3.0.CO








Journal of Clinical Psychology




39


4
















Why Heuristics Work




G
Gigerenzer




10.1111/j.1745-6916.2008.00058.x








Perspectives on Psychological Science




3


1
















The incidence of diagnostic error in medicine




M
L
Graber




10.1136/bmjqs-2012-001615








BMJ Quality & Safety




22


2










Suppl








Diagnostic error: The hidden epidemic




M
L
Graber






B
Carlson








Physician Executive




37


6
















Diagnostic Error in Internal Medicine




M
L
Graber






N
Franklin






R
Gordon




10.1001/archinte.165.13.1493








Archives of Internal Medicine




165


13
















Reducing Diagnostic Errors in Medicine What's the Goal?




M
L
Graber






R
Gordon






N
Franklin








Academic Medicine




77


10


981














Where the wicked problems are: The case of mental health




B
Hannigan






M
Coffey




10.1016/j.healthpol.2010.11.002








Health Policy




101


3
















Order effects in belief updating: The belief-adjustment model




R
M
Hogarth






H
J
Einhorn




10.1016/0010-0285(92)90002-J








Cognitive Psychology




24


1
















The Two Settings of Kind and Wicked Learning Environments




R
M
Hogarth






T
Lejarraga






E
Soyer




10.1177/0963721415591878








Current Directions in Psychological Science




24


5
















Expert decision making




R
J B
Hutton






G
Klein




10.1002/(SICI)1520-6858


1<32::AID-SYS3>3.0.CO








Systems Engineering




2


1
















Publication and other reporting biases in cognitive sciences: Detection, prevalence, and prevention




J
P A
Ioannidis






M
R
Munafò






P
Fusar-Poli






B
A
Nosek






S
P
David




10.1016/j.tics.2014.02.010








Trends in Cognitive Sciences




18


5
















Confirmation bias in sequential information search after preliminary decisions: An expansion of dissonance theoretical research on selective exposure to information




E
Jonas






S
Schulz-Hardt






D
Frey






N
Thelen




10.1037//0022-3514.80.4.557








Journal of Personality and Social Psychology




80


4
















Better sure than safe? Over-confidence in judgement based software development effort prediction intervals




M
Jørgensen






K
H
Teigen






K
Moløkken




10.1016/S0164-1212(02








Journal of Systems and Software




70


1
















A naturalistic decision making perspective on studying intuitive decision making




G
Klein




10.1016/j.jarmac.2015.07.001








Journal of Applied Research in Memory and Cognition




4


3




















O
Kostopoulou






M
Sirota






T
Round






S
Samaranayaka






B
Delaney














The ole of hysicians' First Impressions in the iagnosis of Possible Cancers without Alarm Symptoms


10.1177/0272989X16644563








Medical Decision Making




37


1














Debiasing Health-Related Judgments and Decision Making: A Systematic Review




R
Ludolph






P
J
Schulz




10.1177/0272989X17716672








Medical Decision Making




38


1
















The psychology of belief: IV. The law of primacy in persuasion




F
H
Lund








Journal of Abnormal and Social Psychology




20


2
















Confirmation bias in the therapy session: The effects of expertise, external validity, instruction set, confidence and diagnostic accuracy




J
M
Martin












The University of Memphis








Confirmation bias: Why psychiatrists stick to wrong preliminary diagnoses




R
Mendel






E
Traut-Mattausch






E
Jonas






S
Leucht






J
M
Kane






K
Maino






W
Kissling






J
Hamann




10.1017/S0033291711000808








Psychological Medicine




12
















The trouble with overconfidence




D
A
Moore






P
J
Healy




10.1037/0033-295X.115.2.502








Psychological Review




115


2
















Associative processes in intuitive judgment




C
K
Morewedge






D
Kahneman








Trends in Cognitive Sciences




14


10
















Moderating Loss Aversion: Loss Aversion Has Moderators, But Reports of its Death are Greatly Exaggerated




K
Mrkva






E
J
Johnson






S
Gächter






A
Herrmann




10.1002/jcpy.1156








Journal of Consumer Psychology




30


3
















The serial position effect of free recall




B
B
Murdock




10.1037/h0045106








Journal of Experimental Psychology




64


5
















Confirmation Bias: A Ubiquitous Phenomenon in Many Guises




R
S
Nickerson




10.1037/1089-2680.2.2.175








Review of General Psychology




2


2
















Dual processing and diagnostic errors




G
R
Norman




10.1007/s10459-009-9179-x








Advances in Health Sciences Education




14


1
















Diagnostic error and clinical reasoning




G
R
Norman






K
W
Eva




10.1111/j.1365-2923.2009.03507.x








Medical Education




44


1
















The Causes of Errors in Clinical Reasoning: Cognitive Biases, Knowledge Deficits, and Dual Process Thinking




G
R
Norman






S
D
Monteiro






J
Sherbino






J
S
Ilgen






H
G
Schmidt






S
Mamede




10.1097/ACM.0000000000001421








Academic Medicine




92


1


23














On the Social Psychology of the Psychological Experiment: With Particular Reference to Demand Characteristics and Their Implications




M
T
Orne








In Sociological Methods. Routledge
















Overconfidence in case-study judgments




S
Oskamp




10.1037/h0022125








Journal of Consulting Psychology




29


3
















The effects of the confirmation bias on diagnostic decision making




M
C
Parmley












Drexel University












Effect of a metacognitive intervention on cognitive heuristic use during diagnostic reasoning




V
L
Payne












University of Pittsburgh ETD












An analysis of clinical reasoning through a recent and comprehensive approach: The dual-process theory




T
Pelaccia






J
Tardif






E
Triby






B
Charlin




10.3402/meo.v16i0.5890








Medical Education Online




16


1








University of Pittsburgh












Anchoring errors in clinicallike judgments




M
S
Richards






M
Wierzbicki




10.1002/1097-4679


3<358::AID- JCLP2270460317>3.0.CO;2-7








Journal of Clinical Psychology




46


3
















On the Social Psychology of the Psychological Experiment , the Experimenter's ypothesis as Unintended Determinant of Experimental Results




R
Rosenthal








American Scientist




51


2
















Cognitive biases associated with medical decisions: A systematic review




G
Saposnik






D
Redelmeier






C
C
Ruff






P
N
Tobler




10.1186/s12911-016-0377-1








BMC Medical Informatics and Decision Making




16


1


138














Biased information search in group decision making




S
Schulz-Hardt






D
Frey






C
Lüthgens






S
Moscovici




10.1037/0022-3514.78.4.655








Journal of Personality and Social Psychology




78


4
















Research Artifacts and the Social Psychology of Psychological Experiments. Social and Personality Psychology Compass




D
B
Strohmetz




10.1111/j.1751-9004.2007.00072.x








2














Putting naturalistic decision making into the adaptive toolbox




P
M
Todd






G
Gigerenzer




10.1002/bdm.396








Journal of Behavioral Decision Making




14


5
















Order effects in belief updating with consistent and inconsistent evidence




R
M
Tubbs






G
J
Gaeth






I
P
Levin






L
A
Van Osdol




10.1002/bdm.3960060404








Journal of Behavioral Decision Making




6


4
















Judgment under Uncertainty: Heuristics and Biases




A
Tversky






D
Kahneman








Science




4157


185














Positive recency effect in a delayed free recall




O
J L
Tzeng




10.1016/S0022-5371(73)80023-4








Journal of Verbal Learning and Verbal Behavior




12


4
















Cognitive diagnostic error in internal medicine




K
Van Den Berge






S
Mamede




10.1016/j.ejim.2013.03.006








European Journal of Internal Medicine




24


6
















Determinants of Diagnostic Hypothesis Generation: Effects of Information, Base Rates




E
U
Weber






U
Bockenholt






D
Hilton






B
Wallace












and Experience (SSRN Scholarly Paper 1323461








Medicine and heuristics: Cognitive biases and medical decision-making




D
F
Whelehan






K
C
Conlon






P
F
Ridgway




10.1007/s11845-020-02235-1








Irish Journal of Medical Science




189


4
















Revisiting status quo bias: Replication of Samuelson and Zeckhauser




Q
Xiao






C
S
Lam






M
Piara






G
Feldman




10.15626/MP.2020.2470








Meta-Psychology




















J
F
Yates




10.1111/j.1751-9004.2009.00253.x








Culture and Probability Judgment. Social and Personality Psychology Compass




4


3
















The challenges in defining and measuring diagnostic error




L
Zwaan






H
Singh




10.1515/dx-2014-0069








Diagnosis




2


2

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]