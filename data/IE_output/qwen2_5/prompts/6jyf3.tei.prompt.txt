You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
With an expanding recent appreciation of the value of quantitative theories that make clear and testable predictions 
(Lee & Wagenmakers, 2014;
Navarro, in press;
Oberauer & Lewandowsky, 2019)
, cognitive models have become increasingly popular.
As a consequence, open science and reproducibility reforms have been expanded to include modeling problems. 
Lee et al. (2019)
 proposed a suite of methods for robust modeling practices largely centred on the pre-and postregistration of models. In the interest of cumulative science, we believe that the development and assessment of cognitive models should also include systematic quantitative reviews of the model parameters. Several model classes, including multinomial processing trees 
(Riefer & Batchelder, 1988)
, reinforcement learning models 
(Busemeyer & Stout, 2002)
, and evidence-accumulation models 
(Donkin & Brown, 2018)
, have now been applied widely enough that sufficient information is available in the literature to arrive at a reliable representation of the distribution of the parameter estimates. In this paper we describe a systematic parameter review focusing on the latter class of models.
A systematic quantitative characterization of model parameters provides knowledge of the likely values of the model parameters and has various benefits. First, it can promote more precise and realistic simulations that help to optimally calibrate and design experiments, avoiding unnecessary experimental costs 
(Gluth & Jarecki, 2019;
Heck & Erdfelder, 2019;
Kennedy, Simpson, & Gelman, 2019;
Pitt & Myung, 2019;
Schad, Betancourt, & Vasishth, 2020)
. Second, knowledge about the parameter space can be crucial in maximum-likelihood estimation where an informed guess of the starting point of optimization is often key to finding the globally best solution 
(Myung, 2003)
. Third-and most important for the present paper-systematic quantitative parameter reviews provide crucial information for the specification of informative prior distributions in Bayesian cognitive modeling. The prior distribution is a key element of Bayesian inference; it provides a quantitative summary of the likely values of the model parameters in the form of a probability distribution. The prior distribution is combined with the incoming data through the likelihood function to form the posterior SYSTEMATIC PARAMETER REVIEWS 4 distribution. The prior distribution is an integral part of Bayesian models, and should reflect theoretical assumptions and cumulative knowledge about the relative plausibility of the different parameter values 
(Lee, 2018;
Vanpaemel, 2011;
Vanpaemel & Lee, 2012)
.
Prior distributions play a role both in parameter estimation and model selection. By assigning relatively more weight to plausible regions of the parameter space, informative prior distributions can improve parameter estimation, particularly when the data are not sufficiently informative, for instance due to a small number of observations. Even as the number of observations grows, informative priors remain crucial for Bayesian model selection using Bayes factors 
(Jeffreys, 1961;
Kass & Raftery, 1995)
. Unfortunately, the theoretical and practical advantages of the prior have been undermined by the common use of vague distributions 
(Gill, 2014;
Trafimow, 2005)
.
The goal of this paper is to illustrate how a systematic quantitative parameter review can facilitate the specification of informative prior distributions. To this end, we first introduce the Diffusion Decision Model (DDM; 
Ratcliff, 1978;
Ratcliff & McKoon, 2008)
, a popular cognitive model for two-choice response time tasks (see 
Ratcliff, Smith, Brown, & McKoon, 2016
, for a recent review). Using the DDM as a case study, we will then outline how we used a systematic literature review in combination with principled data synthesis and data quantification using distribution functions to construct informative prior distributions. Lastly, based on the challenges we faced during the parameter review, we formulate a set of general and DDM-specific suggestions about how to report cognitive modeling results, and discuss the limitations of our methods and future directions to improve them.


Case Study: The Diffusion Decision Model
In experimental psychology, inferences about latent cognitive processes from two-choice response time (RT) tasks are traditionally based on separate analyses of mean RT and the proportion of correct responses. However, these measures are inherently related to each other in a speed-accuracy trade-off. That is, individuals can respond faster at the expense of making more errors. Evidence-accumulation models of SYSTEMATIC PARAMETER REVIEWS 5 choice RT and accuracy have provided a solution for this conundrum because they allow for the decomposition of speed-accuracy trade-off effects into latent variables that underlie performance 
(Donkin, Averell, Brown, & Heathcote, 2009;
Ratcliff & Rouder, 1998;
van Maanen et al., 2019)
. These models assume that evidence is first extracted from the stimuli and then accumulated over time until a decision boundary is reached and a response initiated. Among the many evidence-accumulation models, the DDM is the most widely applied, not only in psychology, but also in economics and neuroscience, accounting for experiments ranging from decision making under time-pressure 
(Dutilh, Krypotos, & Wagenmakers, 2011;
Leite, Ratcliff, Lette, & Ratcliff, 2010;
Voss, Rothermund, & Brandtstädter, 2008)
, prospective memory 
(Ball & Aschenbrenner, 2018;
Horn, Bayen, & Smith, 2011)
 to cognitive control 
(Gomez, Ratcliff, & Perea, 2007;
Schmitz & Voss, 2012)
. (2) the separation of the two response boundaries (a), representing response caution; (3) the mean starting point of evidence accumulation (z), representing response bias; and (4) mean non-decision time (T er ), which is the sum of times for stimulus encoding and response execution. RT is the sum of non-decision time and the time to diffuse from the starting point to one of the boundaries. A higher drift rate leads to faster and more accurate responses. However, responses can also be faster because a participant chooses to be less cautious and thus decreases their boundary separation, which will reduce RT but increase errors, causing the speed-accuracy trade-off. Starting accumulation closer to one boundary than the other creates a bias towards the corresponding response. Starting points z is therefore most easily interpreted in relation to boundary separation a, where the relative starting point, also known as bias, is given by z r = z a . Drift rate can vary from trial to trial SYSTEMATIC PARAMETER REVIEWS  . The DDM assumes that noisy information is accumulated over time from a starting point until it crosses one of the two response boundaries and triggers the corresponding response. The grey line depicts the noisy decision process.
'Response A' or 'Response B' is triggered when the corresponding boundary is crossed.
The DDM assumes the following main parameters: drift rate (v), boundary separation (a), mean starting point (z), and mean non-decision time (T er ). These main parameters can vary from trial to trial: across-trial variability in drift rate (s v ), across-trial variability in starting point (s z ), and across-trial variability in non-decision time (s Ter ).
Starting point can be expressed relative to the boundary in order to quantify bias, where z r = z a = 0.5 indicates unbiased responding. Similarly, across-trial variability in starting point can be expressed relative to the boundary:
s zr = sz a .
according to a Gaussian distribution with standard deviation s v . Both non-decision time and starting point are assumed to be uniformly distributed across trials, with range s Ter and s z , respectively, where s z can be expressed relative to a: s zr = sz a . One SYSTEMATIC PARAMETER REVIEWS 7 parameter of the accumulation process needs to be fixed to establish a scale that makes the other accumulation-related parameters identifiable ). Most commonly this scaling parameters is the moment-to-moment variability of drift rate (s), usually with a value fixed to 0.1 or 1.
Fitting the DDM and many other evidence-accumulation models to experimental data is difficult because of the complexity of the models and the form of their likelihood resulting in high correlations among the parameters (i.e., "sloppiness"; 
Gershman, 2016;
Gutenkunst et al., 2007)
. Informative prior distribution can ameliorate some of these problems. The growing popularity of cognitive modeling has led to extensive application of the DDM to empirical data 
(Theisen, Lerche, von Krause, & Voss, 2020)
, providing us with a large number of parameter estimates to use for constructing informative prior distributions. In 2009, Matzke and Wagenmakers presented the first quantitative summary of the DDM parameters based on a survey of parameter estimates found in 23 applications. However, their survey is now outdated and was not as extensive or systematic as the approach taken here.


Material and Methods
All analyses were written in R or R Markdown 
(Allaire et al., 2018;
R Core Team, 2020)
. The extraced parameter estimates and the analysis code are available on GitHub (http://github.com/nhtran93/DDM_priors) and the project's Open Science Framework (OSF) site: https://osf.io/9ycu5/.


Literature Search
The literature search was conducted according to the PRISMA guidelines 
(Moher, Liberati, Tetzlaff, Altman, & Group, 2009)
. Every step was recorded and the inclusion as well as rejection of studies adhered strictly to the pre-specified inclusion criteria.
Results from different search engines were exported as BibTex files, maintained with reference management software and exported into separate Microsoft Excel spreadsheets.


SYSTEMATIC PARAMETER REVIEWS 8
Search Queries. The literature search was commenced and completed in December 2017. It consisted of cited reference searches and independent searches according to pre-specified queries. Searches in all databases were preformed three times in order to ensure reproducibility. Four electronic databases were searched with pre-specified queries: Pubmed (National Library of Medicine, 2017), PsycInfo (American Psychological Association, 2017), Web of Science (WoS, 2017), and Scopus 
(Elsevier, 2017)
. A preliminary search of the four databases served to identify relevant search strings, which were different for each database (see the Supplementary Materials or https://osf.io/9ycu5/ for details). The searches began from the publication year of the seminal paper by 
Ratcliff (1978)
. The cited reference searches were based on 
Ratcliff and McKoon (2008)
, 
Wiecki, Sofer, and Frank (2013)
, 
and Palmer, Huk, and Shadlen (2005)
, and were performed in both Scopus and Web of Science. These key DDM papers were selected to circumvent assessing an unfeasible number of over 3000 Inclusion and Exclusion Criteria. All duplicated references were excluded.
After obviously irrelevant papers -judged based on title and abstract-were excluded, the full-texts were acquired to determine the inclusion or exclusion of the remaining articles. Articles were included in the literature review if they (i) used the standard DDM according to 
Ratcliff (1978)
 and 
Ratcliff and McKoon (2008)
 
(Arizona, 2010)
. Whenever possible, we extracted parameter estimates for each individual participant; otherwise we extracted the mean across participants or in Bayesian hierarchical applications the group-level estimates.
When the DDM was fit multiple times with varying parameterizations to the same data within one article, we used the estimates corresponding to the model identified as best by the authors, with a preference for selections made based on the AIC 
(Akaike, 1973
(Akaike, , 1974
, in order to identify the best trade-off between goodness-of-fit and parametric complexity 
(Myung & Pitt, 1997)
. When the DDM was applied to the same data across different articles, we extracted the parameter estimates from the first application; if the first application did not report parameter estimates, we used the most recent application that reported parameter estimates. Finally, articles that obtained estimates using the EZ 
(Wagenmakers, Van Der Maas, & Grasman, 2007)
 or EZ2 
(Grasman, Wagenmakers, & van der Maas, 2009)
 methods, or the RWiener R package 
(Wabersich & Vandekerckhove, 2014)
, which all fit the simple diffusion model estimating only the four main DDM parameters 
(Stone, 1960)
, were excluded due to concerns about potential distortions caused by ignoring across-trial parameter variability 
(Ratcliff, 2008
 
(Vandekerckhove & Tuerlinckx, 2008)
 for parameter estimation were assumed to use the DMAT default of s = 0.1, and articles that used HDDM 
(Wiecki et al., 2013)
 or fast-DM 
(Voss & Voss, 2007)
 were assumed to use the default setting of 1. Articles (co-) authored by Roger
Ratcliff were assigned s = 0.1. 
1
 We excluded 25 articles because the scaling parameter was not reported and even if we assumed the scaling parameter to be s, its value could not be determined.
Measurement (RT) Scale. Although the measurement (i.e., RT) scale influences the magnitude of the parameter estimates, none of the articles mentioned explicitly whether the data were fit on the seconds or milliseconds scale. Moreover, researchers did not necessarily report all estimates on the same RT scale. For instance,
T er or s Ter were sometimes reported in milliseconds, whereas the other parameters were reported in seconds. Whenever possible, we used axis labels, captions and descriptions in figures and tables, or the default setting of the estimation software to determine the RT scale. Articles that used the DMAT, HDDM, or fast-DM were assumed to use the default setting of seconds and we assigned an RT scale of seconds to papers authored by Roger Ratcliff 2 even if T er was reported in milliseconds. We also evaluated the 
E(RT ) = (a − z)/v.
We then used the following two-step decision rule to determine the RT scale of each parameter:
1. Determine the RT scale of T er : If estimated T er was smaller than 5, we assumed that T er was reported in seconds; otherwise we assumed that T er was reported in milliseconds.
2. Determine RT scale of remaining parameters: If E(RT ) was smaller than 10, we assumed that the remaining parameters were reported in seconds; otherwise we assumed that the remaining parameters were reported in milliseconds.
Once we determined the RT scale for each parameter, we re-scaled the parameter estimates to the seconds scale. Individual parameters estimates that were considered implausible after the transformation (i.e., outside of the parameter bounds, such as a negative a) were checked manually. In particular, we checked for 1) inconsistencies in the magnitude across the parameter estimates within articles (e.g., a value of a indicative of seconds vs. a value of T er indicative of milliseconds); 2) reporting or typographic errors; 3) extraction errors; and 4) errors in determining the measurement scale, which typically reflected the use of non-standard experiments or special populations. In a number of cases we also revisited and whenever necessary reconsidered the assigned value of s. We removed all parameter estimates from 13 articles that reported implausible estimates reflecting ambiguous or inconsistent RT scale descriptions or clear reporting errors.
Starting Point and Bias. We expressed all starting point z and starting point variability s z estimates relative to a. As the attributions of the response options to the two response boundaries is arbitrary, the direction of the bias (i.e., whether z r is greater or less than 0.5) is arbitrary. As these attributions cannot be made commensurate over articles with different response options, values of z r cannot be meaningfully aggregated over articles. As a consequence, bias, z r , and its complement, 1 − z r , are exchangeable SYSTEMATIC PARAMETER REVIEWS 12 for the purpose of our summary. We therefore used both values in order to create a single "mirrored" distribution. This distribution is necessarily symmetric with a mean of 0.5, but retains information about variability in bias. 3
Drift Rate. There are two ways in which drift rates v can be reported. In the first, positive drift rates indicate a correct response (e.g., "word" response to a "word" stimulus and "non-word" response to a "non-word" stimulus in a lexical decision task) and negative rates indicate an incorrect response. In the second, positive drift rates correspond to one response option (e.g., "word" response) and negative rates to the other option (e.g., "non-word"). Here we adopt the former -accuracy coding-method in order to avoid ambiguity regarding the arbitrary attribution of boundaries to response options. We do so by taking the absolute values of the reported drift rates to construct the prior distribution. Readers who wish to adopt the latter -response coding-method, should appropriately mirror our accuracy-coded priors around 0.


Generating Informative Prior Distributions for the DDM Parameters
After post-processing and transforming the parameter estimates, we combined each parameter type across articles and experimental conditions within each study into separate univariate distributions. We then attempted to characterize these empirical parameter distributions with theoretical distributions that provided the best fit to the overall shape of the distributions of parameter estimates.
Parameter Constraints. In many applications of the DDM, researchers impose constraints on the parameter estimates across experimental manipulations, conditions, or groups, either based on theoretical grounds or the results of model-selection procedures. After extracting all parameters from the best fitting models, we identified parameters that were constrained across within-and between-subject manipulations, conditions, or groups within each study. For the purpose of constructing the prior distributions we only considered these fixed parameters once and did not repeatedly include them in the empirical distributions. For instance, a random dot motion task with three difficulty conditions may provide only one estimate for a constrained parameter (i.e., non-decision time), but three parameters for an unconstrained parameter (i.e., drift rate).


Synthesis across articles.
Most studies reported parameter estimates aggregated across participants, with only eight reporting individual estimates. Before collapsing them with the aggregated estimates, individual estimates were averaged across participants in each study. Parameter estimates were equally weighted when combined across studies as details necessary for weighting them according to their precision were typically not available. We will revisit this decision in the Discussion.
In the results reported in the main body of this article, we aggregated the parameter estimates across all research domains (e.g., neuroscience, psychology, economics), populations (e.g., low/high socioeconomic status, clinical populations), and tasks (e.g., lexical decision, random dot motion tasks). In the supplementary materials,
we provide examples of prior distributions derived specifically for two of the most common tasks in our database (i.e., lexical decision and random dot motion tasks) and priors restricted to non-clinical populations. Data and code to generate such task and population-specific priors are available in the open repository, so that interested readers can construct priors relevant to their specific research questions.
Distributions. A full characterization of the distribution of model parameters takes into account not only the parameters' average values and variability but also their correlations across participants (e.g., people with lower drift rates may have higher thresholds) and potentially even their correlations across studies or paradigms using multilevel structures. Although multivariate prior distributions would be optimal to represent correlations across participants, they require individual parameter estimates for the estimation of the covariance matrices. As only eight studies reported individual parameter estimates, we were restricted to use univariate distributions.
We attempted to characterize the aggregated results using a range of univariate distribution functions that respected the parameter types' bounds (e.g., non-decision SYSTEMATIC PARAMETER REVIEWS 14 time T er must be positive) and provided the best fit to the overall shape of the empirical distributions. We first considered truncated normal, lognormal, gamma, Weibull, and truncated Student's t distribution functions. However, in some cases the empirical distributions clearly could not be captured by the unvariate distributions and were contaminated by outliers due to non-standard tasks, special populations, and possible reporting errors that we not identified during the post-processing steps. We therefore also considered characterizing the empirical distribution using mixture distributions. Mixtures were chosen from the exponential family of distributions that respected the theoretical bounds of the parameter estimates. In particular we used mixtures of two gamma distributions, and truncated normals mixed with either a gamma, lognormal, or another truncated normal distribution. Specifically, we focused on normal mixtures because we assume a finite variance for the parameters and thus the Gaussian distributions represents the most conservative probability distribution to assign to the parameter distributions (for further information see the principles of maximum entropy; 
Jaynes, 1988)
.
The univariate and mixture distributions were fit to the empirical distributions using maximum-likelihood estimation 
(Myung, 2003)
 We propose that the wAIC-selected distributions can be used as informative prior distributions for the Bayesian estimation of the DDM parameters. For simplicity, for parameters where a mixture was the best-fitting distribution, we propose as prior the distribution component that best captures the bulk of the parameter estimates as indicated by the highest mixture weight. We will revisit this choice in the Discussion.  We excluded 38 references because they did not report the scaling parameters and we were unable to reverse engineer them or because of inconsistent RT scale descriptions or clear reporting errors. Thus, we extracted parameter estimates from a total of 158 references. The most common paradigms were various perceptual decision-making tasks (e.g., random dot motion task; 37 references), lexical decision tasks 
33
    Here, we used the DDM as example case of how a systematic quantitative parameter review can be incorporated into modeling practices to provide informative prior distributions for the model parameters. Our empirical distributions of the parameter estimates were largely consistent with those of 
Matzke and Wagenmakers (2009)
, but because our sample was much larger we were better able to capture the tails SYSTEMATIC Informative priors are essential for Bayesian model selection using Bayes factors 
(Jeffreys, 1961;
Kass & Raftery, 1995
 In terms of Bayesian estimation, the extra constraint provided by informative priors can benefit some parameters more than others. In the DDM, for example, the across-trial variability parameters are notoriously difficult to estimate 
(Boehm et al., 2018;
Dutilh et al., 2019)
. This has led to calls for these parameters to be fixed to zero (i.e., use the simple diffusion model; 
Stone, 1960)
 to improve the detection of effects on the remaining parameters (van Ravenzwaaij, 
Donkin, & Vandekerckhove, 2017)
.
PARAMETER REVIEWS 19
Informative priors may provide an alternative solution that avoids the potential systematic distortion caused by ignoring the variability parameters 
(Ratcliff & McKoon, 2008
) and enables the study of effects that cannot be accommodated by the simple diffusion model, such as differences between correct and error RTs 
(Damaso, Williams, & Heathcote, submitted
 we believe that using informative priors based on a range of broadly similar paradigms and heterogeneous populations is better than using vague priors, as long as appropriate caution is exercised in cases when the data are not sufficiently informative and hence the prior dominates inferences about the model parameters. Here, we presented informative prior distributions for parameter estimates aggregated across paradigms and populations and also provided paradigm-specific priors for the two most popular tasks in our database (e.g., lexical decision and random dot motion task) and priors for non-clinical populations.
The variance of the paradigm/population-specific priors showed a general decreasing tendency. The decrease in variance was relatively small for the priors based on non-clinical populations, and was restricted to the main DDM parameters. For the lexical decision task, the variance of all of the priors decreased relative to the overall priors. For the random dot motion task, with the exception of v, all variances decreased, albeit the decrease was negligible for a. To summarize, for the tasks and groups we examined here, paradigm and population heterogeneity appears to introduce additional variability in the parameter estimates, but the degree of additional variability strongly Despite their usefulness, systematic quantitative parameter reviews are not without their pitfalls. Using available cumulative knowledge from past literature always has to be viewed in light of the file drawer problem 
(Rosenthal, 1979
 
van Maanen & Miletić, 2020)
. In the DDM, modelers typically fix the moment-to-moment variability of drift rate s to 0.1 or 1 for scaling purposes. Note, however, that the exact value of the scaling parameter is arbitrary, and -depending on the application-one may chose to estimate s from the data and use other parameters for scaling. We stress the importance of explicitly reporting which parameters are used for scaling purposes and the value of the scaling parameter(s) because the chosen setting influences the magnitude of the other parameter estimates. Another scaling issue relates to the measurement units of the data. For example, RTs are commonly measured in both seconds and milliseconds. Although the measurement scale influences the magnitude of the parameter estimates, none of the articles included in the present parameter review explicitly reported the measurement unit of their data. Further, articles did not consistently report all parameter estimates on the same RT scale (i.e., all parameter estimates reported in seconds, but T er reported in milliseconds). Hence, we urge researchers to make an explicit statement on this matter and whenever possible stick to the same measurement unit throughout an article to avoid any ambiguity.
Second, in cognitive models one parameter is sometimes expressed as a function of one or more other parameters. The DDM, for instance, can be parameterized in terms of absolute starting point z or relative starting point z r = z a (i.e., bias). The choice between z and z r depends on the application but can also reflect default software settings. Although the two parametrizations are mathematically identical and have no consequences for the magnitude of the other parameters, it is clearly important to communicate which parameterization is used in a given application. Third, in many applications, researchers impose constraints on the model parameters across SYSTEMATIC PARAMETER REVIEWS 24 experimental manipulations, conditions, or groups. Such constraints sometimes reflect practical or computational considerations, but preferably they are based on a priori theoretical rationale (e.g., threshold parameters cannot vary based on stimulus properties that are unknown before a trial commences; 
Donkin, Averell, et al., 2009)
 or the results of model-selection procedures (e.g., 
Heathcote, Loft, & Remington, 2015;
Strickland, Loft, Remington, & Heathcote, 2018)
. Regardless of the specific reasons for parameter constraints, we urge modelers to clearly communicate which parameters are hypothesized to reflect the effect(s) of interest, and so which are fixed and which are free to vary across the design. Moreover, we recommend researchers to report the competing models (including the parametrization) that were entertained to explain the data, and indicate the grounds on which a given model was chosen as best, such as AIC 
(Akaike, 1981)
, BIC 
(Schwarz, 1978)
, DIC 
(Spiegelhalter, Best, Carlin, & Van Der Linde, 2002;
Spiegelhalter et al., 2014)
, WAIC 
(Watanabe, 2010)
, or Bayes factors 
(Kass & Raftery, 1995)
. We note that parameter reviews are also compatible with cases where there is uncertainty about which is the best model, through the use of Bayesian model averaging 
(Hoeting, Madigan, Raftery, & Volinsky, 1999)
. In this approach, the parameter estimates used in the review are averaged across the models in which they occur, weighted by the posterior probability of the models.
Model Estimation. In the face of the large number of computational tools available to implement cognitive models and the associated complex analysis pipelines, researchers have numerous choices on how to estimate model parameters. For instance, a variety of DDM software is available, such as fast-DM 
(Voss & Voss, 2007)
, HDDM 
(Wiecki et al., 2013)
, DMC 
(Heathcote et al., 2019)
, DMAT 
(Vandekerckhove & Tuerlinckx, 2008)
, using a variety of estimation methods, such as maximum likelihood, Kolmogorov-Smirnov, chi-squared minimization 
(Voss & Voss, 2007)
, quantile maximum probability 
(Heathcote & Brown, 2004)
, or Bayesian Markov chain Monte Carlo (MCMC; e.g., 
Turner, Sederberg, Brown, & Steyvers, 2013)
 techniques. We encourage researchers to report the software they used, and whenever possible, share their commented code to enable computational reproducibility 
(Cohen-Boulakia et al., 2017;
SYSTEMATIC PARAMETER REVIEWS 25 McDougal, Bulanova, & Lytton, 2016)
. Knowledge about the estimation software can also provide valuable information about the parametrization and scaling issues described above.
Parameter Estimates and Uncertainty. We recommend researchers to report all parameter estimates from their chosen model and not only the ones that are related to the experimental manipulation or the psychological effect of interest. In the DDM in particular, this would mean reporting the across-trial variability parameters, and not only the main parameters (i.e., drift rate, boundary separation, starting point, and non-decision time), even if only a subset of parameters is the focus of the study. Given these problems with reporting, we have decided to give equal weights to all (averaged) parameter estimates regardless of the sample size. The reason for this decision was that studies with large sample sizes typically used a small number of trials and likely resulted in relatively imprecise individual estimates, whereas studies with small sample sizes typically used a large number of trials and likely resulted in relatively precise individual estimates. We reasoned that as a result of this trade-off, the equal weighting may not be necessarily unreasonable. To remedy this problem in future SYSTEMATIC PARAMETER REVIEWS 26 parameter reviews, we urge researchers to either report properly weighted group average estimates or report individual estimates along with measures of uncertainty, let these be (analytic or bootstrapped) frequentist standard errors and confidence intervals (e.g., 
Visser & Poessé, 2017)
, or Bayesian credible intervals and full posterior distributions 
(Eberly & Casella, 2003;
Jeffreys, 1961;
Lindley, 1965)
.
Individual Parameters and Correlations. Ideally, researchers should report parameter estimates for each individual participant. In the vast majority of the studies examined here, only parameters averaged over participants were available. This means that we were unable to evaluate correlations among parameter estimates reflecting individual differences. Such correlations are likely quite marked. For example, in the DDM a participant with a higher drift rate, which promotes accuracy, is more likely to be able to afford to set a lower boundary and still maintain good performance, so a negative correlation between rates and boundaries might be expected. Access to individual parameters would allow estimation of these correlations, and thus enable priors to reflect this potentially important information. As we discuss below, the failure to report individual estimates brings with it important limitations on what can be achieved with the results of systematic parameter reviews.


Limitations and Future Directions
The approach to parameter reviews taken here -obtaining values from texts, tables, and graphs from published papers and performing an aggregation across studies-has the advantage of sampling estimates that are representative of a wide variety of laboratories, paradigms, and estimation methods. Indeed, for the priors presented in 
Figure 3
 we included a few studies with much longer RTs than are typically fit with the DDM (e.g., 
Lerche & Voss, 2019)
. The larger parameter values from these studies had the effect of broadening the tails of the fitted distributions so they represent the full variety of estimates reported in the literature.
However, this approach has a number of limitations beyond those related to the vagaries of incomplete reporting practices just discussed. The first limitation is related SYSTEMATIC PARAMETER REVIEWS 27 to the aggregation of parameter estimates over different designs. The most straightforward example concerns including parameters from studies with long RTs.
The solution is equally straightforward: only including studies with RTs that fall in the range of interest specific to a particular application. A related but more subtle issue occurs in our DDM application where the meaning of the magnitude of the response bias (z r ) parameter is design specific, and so it is difficult to form useful aggregates over different paradigms. To take a concrete example, a bias towards "word" responses over "non-word" responses in a lexical-decision paradigm cannot be made commensurate with a bias favoring "left" over "right" responses in a random dot motion paradigm.
Our approach -forming an aggregate with maximum uncertainty by assuming either direction is equally likely (i.e., mirroring the values)-removes any information about the average direction while at least providing some information about variability in bias. Although this approach likely overestimates the variability of the bias estimates, we believe that overestimation is preferable to underestimation which might result in an overly influential prior distribution. Again, this problem can again be avoided by constructing priors based on a more specific (in this case task-specific) aggregation. Our online data repository reports raw starting point and bias estimates, which combined with the design descriptions from the original papers could be used to perform such an aggregation. The priors for the lexical decision task reported in the Supplementary Materials provide an example that did not require us to mirror the bias estimates. We note, however, that we had to exclude a paper where it was unclear which response was mapped to which DDM boundary, so we would add a reporting guideline that this choice be spelled out. We also note that similar problems with aggregation over different designs are likely to occur for other parameter types and also beyond the DDM, for instance in evidence-accumulation models such as the Linear Ballistic
Accumulator 
(Brown & Heathcote, 2008)
. For instance, if one decomposes drift rates in the DDM into the average over stimuli and "stimulus bias" (i.e., the difference in rates between the two stimulus classes; 
White & Poldrack, 2014)
, then the same issue applies, but now with respect stimuli rather than responses.


SYSTEMATIC PARAMETER REVIEWS 28
The second limitation -which is related to incomplete reporting, but is harder to address within a traditional journal format-concerns obtaining a full multivariate characterization of the prior distribution of parameters that takes into account correlations among parameters as well as their average values and variability. Because most estimates reported in the literature are averages over participants, we were restricted to providing separate univariate characterizations of prior distributions for each parameter. To the degree that the implicit independence assumption of this approach is violated 5 problems can arise. Continuing the example of negatively correlated rates and boundaries, although a higher value of both separately may be quite probable, both occurring together may be much less likely that the product of their individual probabilities that would be implied by independence.
Problems related to this limitation arise, for example, if in planning a new experiment one were to produce synthetic data by drawing parameter combinations independently from the univariate priors in 
Figure 3,
 
(Gunawan, Hawkins, Tran, Kohn, & Brown, 2020)
. This development underscores the need for future systematic parameter reviews to move in the direction of multivariate characterizations. This may be achieved by revisiting the original data sets, which due to open science practices are becoming increasingly available, refitting the DDM, and then using the resulting 5 To be clear, we are not talking about correlations among parameters within a participant, which are a consequence of the mathematical form of the model's likelihood and the particular parameterization adopted for the design. Rather, we are addressing correlations at the population level, i.e., across participants. Although the two types of correlations can be related, they are not the same and in our experience can sometimes differ very markedly.


SYSTEMATIC PARAMETER REVIEWS 29
individual parameter estimates to form multivariate priors. This future direction will be time consuming and computationally challenging, and will no doubt bring with it new methodological problems that we have not addressed here. Nevertheless, we believe that the long-term gains for cognitive modeling will make this enterprise worthwhile.
Appendix A Search Queries for Literature Search
The following search strings were used:
• PsycInfo: ((drift diffusion model* or diffusion decision model or ratcliff diffusion or diffusion model) and (response time* or reaction time*)).mp. and Reaction time.sh. limit to yr="1977 -2017"      Here we provide informative prior distributions for applications with healthy populations, after excluding the 29 clinical studies from the database. We generated informative prior distributions according to the methodology described in the main text. The resulting prior distributions are shown in 
Table D1
 and 
Figure D1
.  
Figure 1
1
illustrates the DDM. Evidence (i.e., grey line) fluctuates from moment to moment according to a Gaussian distribution with standard deviation s, drifting until it reaches one of two boundaries, initiating an associated response. The DDM decomposes decision making in terms of four main parameters corresponding to distinct cognitive processes: (1) the mean rate of evidence accumulation (drift rate v), representing subject ability and stimulus difficulty;


Figure 1 .
1
The Diffusion Decision Model (DDM; taken with permission fromMatzke &    


cited references to the seminal Ratcliff article, with a potentially high number of false positives (in terms of yielding papers that reported parameter estimates), while still maintaining a wide search covering various areas of psychology and cognitive neuroscience.


Figure 2 .
2
PRISMA flow diagram. WoS = Web of Science. RWiener refers to the R package from Wabersich and Vandekerckhove (2014). EZ and EZ2 refer to estimation methods for the simple DDM developed by
Wagenmakers et al. (2007)
 and
Grasman et al. (2009)
, respectively.


Figure 2
2
shows the PRISMA flow diagram corresponding to our literature search.The total of 196 relevant articles (i.e., "Reported estimates" inFigure 2) covered a wide range of research areas from psychology and neuroscience to medicine and economics.


, and recognition memory tasks (17). A total of 29 references included clinical groups and 26


Figure 3 .
3
Prior Distributions for the DDM Parameters. The red lines show the best fitting theoretical distributions or the dominant theoretical distribution components with the highest mixture weight (i.e., the proposed informative prior distributions). The black lines show the non-dominant distribution components. N : number of unique estimates.SYSTEMATIC PARAMETER REVIEWS 17The histograms inFigure 3show the empirical distributions of the parameter estimates. The red lines show the best fitting theoretical distributions or the dominant theoretical distribution components with the highest mixture weight (i.e., the proposed informative prior distributions). The black lines show the non-dominant mixture distribution components. Note that in most cases the the mixture served to inflate the distributions' tails while preserving a single mode.Table 1gives an overview of the informative prior distributions, the corresponding upper and lower bounds (see column "T-LB" and "T-UB"), and whenever appropriate also the mixture weight of the dominant distribution component. The table also shows the upper and lower bounds of the parameter estimates collected from the literature (see column "E-LB" and "E-UB"); these bounds can be used to further constrain parameter estimation by providing limits for prior distributions and bounded optimization methods.The results of the model comparisons are available at https://osf.io/9ycu5/.For drift rate v, the selected model was a mixture of a zero-bounded truncated normal and a lognormal distribution (wAIC = 0.4), with the mixture weight, and the location and scale of the dominant truncated normal component shown in the first row ofTable 1. 4 For boundary separation a, the selected model was a mixture of gamma distributions (wAIC = 0.76), with the shape and scale parameters of the dominant gamma component shown in the second row ofTable 1. For non-decision time T er and across-trial variability in non-decision time s Ter , the selected model was a zero-bounded truncated t distribution (wAIC = 1 for both T er and s Ter ). For mirrored bias z r , the selected model was a truncated t distribution on [0, 1] (wAIC = 1.0). For across-trial variability in drift rate s v , the selected model was a mixture of a zero-bounded truncated normal and a gamma distribution (wAIC = 0.35), where the truncated normal had the highest mixture weight. Lastly, for s zr , the selected model was a truncated normal distribution on [0, 1] (wAIC = 0.74).


Note. N: The number of unique estimates; Weight: The mixture weight of the dominant distribution component; df: degrees of freedom; T-LB: theoretical lower bound of the prior distribution; T-UB: theoretical upper bound of the prior distribution.; E-LB: lower bound of the empirical parameter estimates; E-UB: upper bound of the empirical parameter estimates; *: dominant distribution component.DiscussionThe increasing popularity of cognitive modeling has led to extensive applications of models like the Diffusion Decision Model (DDM) across a range of disciplines. These applications have the potential to provide substantial information about the plausible values of the parameters of cognitive models. We believe that for cognitive models where sufficient information are available in the literature, a systematic quantitative characterization of model parameters can be a very useful addition to existing modeling practices. Parameter reviews can benefit modeling practices in various ways, from facilitating parameter estimation to enabling more precise and realistic simulations to improve study design and calibrate future experiments
(Gluth & Jarecki, 2019;
Heck & Erdfelder, 2019;
Pitt & Myung, 2019)
.


of the parameter distributions. Although, for simplicity, here we suggested single-component distributions as priors, the full mixture distributions that we selected could also be used. Bayesian DDM software, such as the Dynamic Models of Choice software (DMC;
Heathcote et al., 2019)
, can be easily adapted to use any form of univariate prior, including mixtures. In most cases the mixture served to inflate the distributions' tails while preserving a single mode. However, aggregation over heterogeneous studies naturally carries with it the possibility of creating multi-modal prior distributions, as illustrated by the results for s v inFigure 3. If the data proved sufficiently uninformative that such multi-modality carried through to the posterior, caution should be exercised in reporting and interpreting measures of central tendency.Inferring the parameters of complex cognitive models like the DDM from experimental data is challenging because their parameters are often highly correlated.The cumulative knowledge distilled into parameter estimates from past research can practically benefit both traditional optimization-based methods (e.g., maximum likelihood) and Bayesian estimation. In the former case, parameter reviews can provide informed guesses for optimization starting points as well as guidance for configuring bounded optimization methods. Even when powerful and robust optimization algorithms (e.g., particle swarm methods) are used, reasonable initial values and bounds can increase time efficiency and are often helpful for avoiding false convergence on sub-optimal solutions. In the latter -Bayesian case-parameter reviews can facilitate the use of informative prior distributions, which benefits both Bayesian model selection and parameter estimation.


depends on the type of parameter. Our open repository provides the data and code to generate informed prior distributions for any selection of studies included in the database so that researchers can construct informative prior distributions relevant to their own research questions. Naturally, paradigm/population-specific priors are only sensible when a sufficiently large number of parameter estimates are available in the SYSTEMATIC PARAMETER REVIEWS 22 database to fit the theoretical (mixture) distributions, or when researchers can augment the repository with estimates from additional studies.


Ideally, in the process of aggregation used to create prior distributions, estimates should be weighted by their relative uncertainty. The weighing should reflect the uncertainty of the individual estimates resulting from fitting the model to finite data and -if average parameters are used, as was the case here-also sampling error reflecting the sample size used in each study. Although we had access to the sample sizes, most studies reported parameter estimates averaged across participants without accounting for the uncertainty of the individual estimates. Moreover, the few studies that reported individual estimates provided only point estimates and failed to include measures of uncertainty. As a proxy to participant-level measures of uncertainty one may use the number of trials that provide information for the estimation of the various model parameters. However, this approach requires a level of detail about the experimental design and the corresponding model specification (including the number of excluded trials per participant) that was essentially never available in the surveyed studies.


•
Pubmed: ((((diffusion[Text Word] OR drift*[Text Word] OR *DDM[Text Word]) AND model*[Text Word]) AND ("1978"[Date -Publication] : "2017"[Date -Publication])) AND "Reaction time"[MeSH Terms])• Web of Science: (TS=("drift diffusion model") OR TS=("diffusion decision model") OR TS=("ratcliff diffusion") OR TS=("diffusion model")) AND (TS=(response time* OR reaction time*)) AND (SU=(Life Sciences Biomedicine) OR SU=(Social Sciences)) Indexes=SCI-EXPANDED, SSCI, A&HCI, ESCI Timespan=1977-2017• Scopus: (ALL ( "response* time*" ) OR ALL ( "reaction* time*")) AND (TITLE-ABSKEY ( "diffusion model" ) OR TITLE-ABS-KEY ( "drift diffusion model" ) OR TITLE-ABS-KEY ( "*DDM" ) OR TITLE-ABS-KEY ( "diffusion decision model" ) OR TITLE-ABS-KEY ( "Ratcliff diffusion" )) AND PUBYEAR > 1977 AND SUBJAREA ( psyc OR neur OR medi OR soci OR deci OR econ OR mult )


Note. N: The number of unique estimates; Weight: The mixture weight of the dominant distribution component; df: degrees of freedom; T-LB: theoretical lower bound of the prior distribution; T-UB: theoretical upper bound of the prior distribution.; E-LB: lower bound of the empirical parameter estimates; E-UB: upper bound of the empirical parameter estimates;


Figure C1 .
C1
Prior Distributions for the Lexical Decision Task. The red lines show the best fitting theoretical distributions or the dominant theoretical distribution components with the highest mixture weight (i.e., the proposed informative prior distributions). The black lines show the non-dominant distribution components. N : number of unique estimates.


Figure C2 .
C2
Note. N: The number of unique estimates; Weight: The mixture weight of the dominant distribution component; df: degrees of freedom; T-LB: theoretical lower bound of the prior distribution; T-UB: theoretical upper bound of the prior distribution.; E-LB: lower bound of the empirical parameter estimates; E-UB: upper bound of the empirical parameter estimates; *: dominant distribution component. Prior Distributions for the Random Dot Motion Task. The red lines show the best fitting theoretical distributions or the dominant theoretical distribution components with the highest mixture weight (i.e., the proposed informative prior distributions). The black lines show the non-dominant distribution components. N : number of unique estimates. Appendix D Informative Prior Distributions for Non-clinical Populations


Figure D1 .
D1
Note. N: The number of unique estimates; Weight: The mixture weight of the dominant distribution component; df: degrees of freedom; T-LB: theoretical lower bound of the prior distribution; T-UB: theoretical upper bound of the prior distribution.; E-LB: lower bound of the empirical parameter estimates; E-UB: upper bound of the empirical parameter estimates; *: dominant distribution component. Prior Distributions for Non-Clinical Populations. The red lines show the best fitting theoretical distributions or the dominant theoretical distribution components with the highest mixture weight (i.e., the proposed informative prior distributions). The black lines show the non-dominant distribution components. N : number of unique estimates.


reported estimates with respect to the second or millisecond scale by computing a rough estimate of the expected RT for each experimental condition as
SYSTEMATIC PARAMETER REVIEWS
11
plausibility of the
1 Based on personal communication with Roger Ratcliff.2 Based on personal communication with Roger Ratcliff.


, with additional constraints on upper and/or lower bounds. For (mirrored) bias z r and s zr , which are bounded between 0 and 1, we used univariate truncated normal and truncated t distributions on [0, 1]. A lower bound of zero was imposed on all other parameters. We then used AIC weights (wAIC;
Wagenmakers & Farrell, 2004)
 to select the theoretical distributions that struck the best balance between goodness-of-fit and simplicity. A table of the AIC and wAIC values for all fitted univariate and mixture distributions and the code to reproduce this table, can be found in the open repository on GitHub or the OSF.


Table 1
1
Informative Prior Distributions
DDM Parameter N
Distribution
Weight Location/Shape Scale df
T-LB T-UB E-LB E-UB
v
1893 truncated normal* & lognormal 0.85
1.76
1.51
0
+ Inf 0.01
18.51
a
890 gamma* & gamma
0.76
11.69
0.12
0
+ Inf 0.11
7.47
Mirrored z r
203 truncated t
-
0.5
0.05 1.85 0
1
0.04
0.96
T er
857 truncated t
-
0.44
0.08 1.32 0
+ Inf 0
3.69
s v
317 truncated normal* & gamma
0.75
1.36
0.69
0
+ Inf 0
3.45
s zr
278 truncated normal
-
0.33
0.22
0
1
0.01
0.85
s Ter
352 truncated t
-
0.17
0.04 0.88 0
+ Inf 0
4.75


the data across all possible parameter settings with the weights given by the parameters' prior density. The workflow outlined here may therefore facilitate the more principled use of prior information in Bayesian model selection in the context
SYSTEMATIC PARAMETER REVIEWS
20
the probability of of evidence-accumulation models (for recent developments, see Evans & Annis, 2019;
Gronau, Heathcote, & Matzke, 2020).
). Unlike other model-selection methods like the
Deviance Information Criteria (DIC; Spiegelhalter, Best, Carlin, & van der Linde, 2014)
and Widely Applicable Information Criterion (WAIC; Vehtari, Gelman, & Gabry, 2017)
that depend only on posterior samples, Bayes factors depend crucially on the prior distribution even when large amounts of data are available. This is because the marginal likelihood of the competing models is obtained by taking a weighted average of


sampling efficiency and speed up the convergence of MCMC routines.Ideally, informative prior distributions for cognitive models should be based on prior information extracted from experimental paradigms (or classes of paradigms) and participant populations relevant to the research question at hand, although care should be exercised in the latter case where group members fall along a continuum (e.g., age or the severity of a clinical diagnosis). In reality, constructing such highly specific priors might not always be feasible, either because of a paucity of relevant parameter values reported in the literature, or when new paradigms or populations are studied. However,
SYSTEMATIC PARAMETER REVIEWS
21
can also increase
). Of course, in extreme cases, the central tendency of
informative prior distributions may provide guidelines for fixing difficult-to-estimate
model parameters to a constant (e.g., Matzke, Logan, & Heathcote, 2020).
Information about the empirical distribution of parameter estimates, both in
terms of the main body and the tails of the distributions, can especially benefit design
optimization and parameter estimation in non-standard and difficult to access
populations (e.g., Matzke, Hughes, Badcock, Michie, & Heathcote, 2017; Shankle et al.,
2013). For example, in clinical populations long experimental sessions are often
impossible due to exhaustion or attention lapses. Expenses can also be constraining,
such as with studies using costly fMRI methods. Therefore, data are often scarce, with
a total number of trials as low as 100 reported in some DDM applications (e.g.,
O'Callaghan et al., 2017). In these cases, experimental designs can be optimized, and parameter estimation improved, with the aid of informative parameter distributions that put weight on plausible parts of the parameter space. Moreover, informative priors


Our literature review revealed a wide variety of reporting practices, both in terms of what researcher report and how they report their modeling results. The diversity of reporting practices is likely to reflect differences between disciplines and is in itself not problematic. However, we believe that the full potential of cumulative science can only be realized if authors provide sufficient information for others to interpret and reproduce their results. We endorse code and data sharing, and -following Lee et al.(2019)-we strongly urge researchers to provide sufficiently precise mathematical and statistical descriptions of their models, and to post-register exploratory model developments. In what follows, we reflect on the challenges we faced in performing the systematic parameter review, and formulate a set of general and DDM-specific suggestions that aim to increase computational reproducibility and the expected information gain from parameter reviews. Although our recommendations are certainly not exhaustive and do not apply to all model classes, we hope that they provide food SYSTEMATIC PARAMETER REVIEWS 23 for thought for cognitive modelers in general and RT modelers in particular. The following recommendations are aimed at supporting well-informed choices about which model and which model parameters to include in a parameter review. Most parametric cognitive models can be parameterized in various ways. First, some cognitive models require fixing one (or more) parameters to make the model identifiable
). Many researchers
have not published their non-significant results, therefore the literature is biased, and
thus the parameter estimates retrieved from the literature might be biased towards
specific model settings that converged or led to significant results. Furthermore, some
cognitive models are too new and have not been widely applied to empirical data, so
past literature might not provide researchers with a sufficiently reliable representation
of the distribution of the parameter estimates. Therefore, cognitive modelers may not
always be able to incorporate our proposed quantitative parameter review into their
workflow, and should carefully weigh out the feasibility and benefits of such an
endeavour.
Recommendations for Reporting Cognitive Modeling Results
Model Parameterization and Scaling.


potentially producing simulated participants with parameter values that are unlikely in a real experiment. With Bayesian methods, ignoring the correlations among parameters can compromise the efficiency of MCMC samplers and complicate the interpretation of Bayes factors because the resulting uni-variate priors will assign mass to implausible regions of the parameter space. Although standard Bayesian MCMC samplers used for evidence-accumulation models have not taken account of these population correlations, a new generation of samplers is appearing that does


Table C1
C1
Informative Priors Distributions for the Lexical Decision Task
DDM Parameter N
Distribution
Weight Location/Shape Scale df
T-LB T-UB E-LB E-UB
v
382 truncated normal* & truncated normal 0.55
2.33
0.58
0
+ Inf 0.06
8.73
a
130 lognormal* & truncated normal
0.76
0.32
0.32
0
+ Inf 0.74
3.93
z r
38 truncated t
-
0.52
0.01 0.67 0
1
0.31
0.77
T er
152 truncated normal* & lognormal
0.53
0.44
0.03
0
+ Inf 0.35
0.90
s v
83 truncated normal* & gamma
0.89
1.16
0.58
0
+ Inf 0.01
2.68
s zr
69 truncated normal
-
0.34
0.19
0
1
0.01
0.70
s Ter
83 gamma* & truncated normal
0.94
17.37
0.01
0
+ Inf 0
0.83


Table C2
C2
Informative Priors Distributions for the Random Dot Motion Task
DDM Parameter N
Distribution
Weight Location/Shape Scale T-LB T-UB E-LB E-UB
v
144 truncated normal
-
-207.1
25.88 0
+ Inf 0.01
17.65
a
95 gamma* & gamma 0.89
12.19
0.10 0
+ Inf 0.50
5.70
T er
86 Weibull
-
5.12
0.44 0
+ Inf 0.00
0.56


Table D1
D1
Informative Priors Distributions for non-clinical Populations
DDM Parameter N
Distribution
Weight Location/Shape Scale df
T-LB T-UB E-LB E-UB
v
1579 gamma* & gamma
0.65
1.39
1.72
0
+ Inf 0.01
13.79
a
731 truncated normal* & lognormal 0.73
1.24
0.40
0
+ Inf 0.11
6.80
Mirrored z r
164 truncated t
-
0.5
0.04 2.65 0
1
0.04
0.96
T er
718 gamma* & gamma
0.86
22.17
0.02
0
+ Inf 0.00
3.69
s v
282 truncated normal* & gamma
0.79
1.32
0.72
0
+ Inf 0.00
3.45
s zr
248 truncated normal
-
0.33
0.22
0
1
0.01
0.85
s Ter
318 truncated normal* & lognormal 0.80
0.17
.05
0
+ Inf 0
4.75


The bias z r parameters estimated using the HDDM software
(Wiecki et al., 2013)
 are coded as 1 − z r in our parameter review. Note that this has no influence on the resulting prior distribution as we used both z r and 1 − z r to create the prior.


The location and scale parameters of the truncated normal distribution refer to µ and σ and not to its expected value and variance.








Conflict of Interest Statement
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.


Author Contributions
NHT designed the study, collected the data, performed the analyses, and wrote the manuscript. NHT, LvM and DM designed the study and the analyses. DM and AH provided critical feedback and helped shape the manuscript.


Funding
This work was supported by the Utrecht University and the Max Planck Society.
AH was supported by the ARC Discovery Project grant no. DP200100655. DM was supported by a Veni grant (451-15-010) from the Netherlands Organization of Scientific Research (NWO).


Data Availability Statement
The dataset and the analyses can be found at https://osf.io/9ycu5/ and https://github.com/nhtran93/DDM_priors.


Appendix B Parameter Transformations
The reported parameter estimates were re-scaled to an RT scale in seconds and moment-to-moment variability in drift rate s of 1 as follows: We provide two examples of paradigm-specific priors. We chose the two most common tasks in our database: the lexical decision task (33 articles) and the random dot motion task (18 articles). We generated informative prior distributions according to the methodology described in the main text, with the following modifications.
1. With one exception, all lexical decision studies mapped "word" responses onto the upper decision boundary, so z r reflects a bias towards "word" responses over "non-word" responses. In the remaining case, the article did not provide sufficient information to determine the mapping between response options and boundaries.
After excluding this single study, we collapsed all z r estimates in a distribution, but -unlike in the main analyses where we treated the assignment of responses to boundaries as arbitrary -we did not mirror the distribution of z r .
2. As our database contained only 18 random dot motion studies, the number of data points for z r and the across-trial variability parameters was very low with only 10 or even fewer estimates. Therefore, we only constructed prior distributions for v, a, and T er .
The resulting prior distributions for the lexical decision task and the random dot motion task are shown in 
Table C1
 and 
Figure C1
, and 
Table C2
 and 
Figure C2
, respectively.
 










Information theory and an extension of maximum likelihood principle




H
Akaike








Proceedings of the second international symposium on information theory


the second international symposium on information theory


















A new look at the statistical model identification




H
Akaike








IEEE Transactions on Automatic Control




19
















Likelihood of a model and information criteria




H
Akaike




10.1016/0304-4076(81








Journal of Econometrics




16


1




















J
Allaire






J
Cheng






Y
Xie






J
Mcpherson






W
Chang






J
Allen






R
Hyndman












rmarkdown: Dynamic documents for R (R package version 1.0) [Computer software manual








GraphClick




Arizona












Computer software manual








The importance of age-related differences in prospective memory: Evidence from diffusion model analyses




B
H
Ball






A
J
Aschenbrenner




10.3758/s13423-017-1318-4








Psychonomic Bulletin & Review




25


3




















U
Boehm






J
Annis






M
J
Frank






G
E
Hawkins






A
Heathcote






D
Kellen














Estimating across-trial variability parameters of the diffusion decision model: Expert advice and recommendations




E.-J
Wagenmakers




10.1016/j.jmp.2018.09.004








Journal of Mathematical Psychology




87
















The simplest complete model of choice response SYSTEMATIC PARAMETER REVIEWS 40 time: Linear ballistic accumulation




S
D
Brown






A
Heathcote




10.1016/j.cogpsych.2007.12.002








Cognitive Psychology




57


3
















A contribution of cognitive decision models to clinical assessment: Decomposing performance on the Bechara gambling task




J
R
Busemeyer






J
C
Stout




10.1037/1040-3590.14.3.253








Psychological Assessment




14


3
















Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities




S
Cohen-Boulakia






K
Belhajjame






O
Collin






J
Chopard






C
Froidevaux






A
Gaignard






.
.
Blanchet






C




10.1016/j.future.2017.01.012








Future Generation Computer Systems




75
















What does a (hu)man do after (s)he makes a fast versus slow error




K
Damaso






P
Williams






A
Heathcote








submitted








Getting more from accuracy and response time data: Methods for fitting the linear ballistic accumulator




C
Donkin






L
Averell






S
Brown






A
Heathcote




10.3758/BRM.41.4.1095






Behavior Research Methods




41


4
















Stevens' handbook of experimental psychology and cognitive neuroscience




C
Donkin






S
D
Brown








Methodology


E.-J. Wagenmakers & J. T. Wixted




5






John Wiley & Sons, Inc






Response times and decision-making. 4th ed.








The overconstraint of response time models: Rethinking the scaling problem




C
Donkin






S
D
Brown






A
Heathcote








Psychonomic Bulletin & Review




16


6




















G
Dutilh






J
Annis






S
D
Brown






P
Cassey






N
J
Evans






R
P P P
Grasman














The quality of response time data inference: A blinded, collaborative sssessment of the validity of cognitive models




C
Donkin








Psychonomic Bulletin SYSTEMATIC PARAMETER REVIEWS




41
















10.3758/s13423-017-1417-2


doi: 10.3758/s13423-017-1417-2








& Review




26


4














Task-related versus stimulus-specific practice




G
Dutilh






A.-M
Krypotos






E.-J
Wagenmakers




10.1027/1618-3169/a000111






Experimental Psychology




58


6
















Estimating Bayesian credible intervals




L
E
Eberly






G
Casella




10.1016/S0378-3758(02)00327-0








Journal of Statistical Planning and Inference




112


1
















Thermodynamic integration via differential evolution: A method for estimating marginal likelihoods




N
J
Evans






J
Annis








Behavior Research Methods




51
















Empirical priors for reinforcement learning models




S
J
Gershman




10.1016/j.jmp.2016.01.006








Journal of Mathematical Psychology




71
















Bayesian methods: A social and behavioral sciences approach




J
Gill








Chapman & Hall












On the importance of power analyses for cognitive modeling




S
Gluth






J
B
Jarecki




10.1007/s42113-019-00039-w








Computational Brain & Behavior




2


3
















A model of the Go/No-Go task




P
Gomez






R
Ratcliff






M
Perea








Journal of Experimental Psychology: General




136
















On the mean and variance of response times under the diffusion model with an application to parameter estimation




R
P
Grasman






E.-J
Wagenmakers






Van Der






H
L
Maas








Journal of Mathematical Psychology




53


2


















10.1016/j.jmp.2009.01.006














Computing bayes factors for evidence-accumulation models using Warp-III bridge sampling




Q
F
Gronau






A
Heathcote






D
Matzke








Behavior Research Methods




52
















New estimation approaches for the hierarchical Linear Ballistic Accumulator model




D
Gunawan






G
E
Hawkins






M
N
Tran






R
Kohn






S
D
Brown








Journal of Mathematical Psychology




96


102368














Universally sloppy parameter sensitivities in systems biology models




R
N
Gutenkunst






J
J
Waterfall






F
P
Casey






K
S
Brown






C
R
Myers






J
P
Sethna




10.1371/journal.pcbi.0030189






PLoS Computational Biology




3


10
















Reply to Speckman and Rouder: A theoretical basis for QML




A
Heathcote






S
Brown








Psychonomic Bulletin & Review




11


3
















Dynamic models of choice




A
Heathcote






Y.-S
Lin






A
Reynolds






L
Strickland






M
Gretton






D
Matzke




10.3758/s13428-018-1067-y








Behavior Research Methods




51


2
















Slow down and remember to remember! A delay theory of prospective memory costs




A
Heathcote






S
Loft






R
W
Remington








Psychological Review




122
















Maximizing the expected information gain of cognitive modeling via design optimization




D
W
Heck






E
Erdfelder




10.1007/s42113-019-00035-0


doi: 10.1007/s42113-019-00035-0








Computational Brain & Behavior




2


3
















Bayesian model averaging: A tutorial




J
A
Hoeting






D
Madigan






A
E
Raftery






C
T
Volinsky








Statistical Science




14
















What can the diffusion model tell Us SYSTEMATIC PARAMETER REVIEWS 43 about prospective memory?




S
S
Horn






U
J
Bayen






R
E
Smith




10.1037/a0022808






Canadian Journal of Experimental Psychology -Revue Canadienne de Psychologie Experimentale




1
















The relation of Bayesian and maximum entropy methods




E
T
Jaynes








Maximum entropy and bayesian methods in science and engineering


G. J. Erickson & C. R. Smith




Kluwer Academic Publishers










1st ed.










H
Jeffreys




Theory of probability


Oxford, UK




Oxford University Press








3rd ed.








Bayes factors




R
E
Kass






A
E
Raftery








Journal of the American Statistical Association




90
















The experiment is just as important as the likelihood in understanding the prior: A cautionary note on robust cognitive modeling




L
Kennedy






D
Simpson






A
Gelman




10.1007/s42113-019-00051-0








Computational Brain & Behavior




2


3
















Stevens' handbook of experimental psychology and cognitive neuroscience




M
D
Lee








Methodology


E.-J. Wagenmakers & J. T. Wixted




5






John Wiley & Sons, Inc






Bayesian methods in cognitive modeling. 4th ed.












M
D
Lee






A
H
Criss






B
Devezer






C
Donkin






A
Etz






F
P
Leite














Robust modeling in cognitive science




J
Vandekerckhove




10.1007/s42113-019-00029-y






Computational Brain & Behavior




2
















Bayesian cognitive modeling: A practical course




M
D
Lee






E.-J
Wagenmakers








Cambridge University Press












Modeling reaction time and accuracy of multiple-alternative decisions. Attention, Perception, and Psychophysics




F
P
Leite






R
Ratcliff






F
P
Lette






R
Ratcliff




eid=2-s2.0-77949318427{&}doi=10.3758{%}2FAPP.72.1 .246{&}partnerID=40{&}md5=913410025718f979fa4924c25940191d doi: SYSTEMATIC PARAMETER REVIEWS 44








72














Experimental validation of the diffusion model based on a slow response time paradigm




V
Lerche






A
Voss








Psychological Research Psychologische Forschung




83


6
















Introduction to probability theory and statistics from a Bayesian point of view




D
V
Lindley








Cambridge University Press


Cambridge












Failures of cognitive control or attention? the case of stop-signal deficits in schizophrenia




D
Matzke






M
Hughes






J
C
Badcock






P
Michie






A
Heathcote
























Perception, & Psychophysics




79








Attention








A cautionary note on evidence-accumulation models of response inhibition in the stop-signal paradigm




D
Matzke






G
D
Logan






A
Heathcote








Computational Brain & Behavior




3
















Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis




D
Matzke






E.-J
Wagenmakers




















10.3758/PBR.16.5.798






Psychonomic Bulletin & Review




16


5














Reproducibility in computational neuroscience models and simulations




R
A
Mcdougal






A
S
Bulanova






W
W
Lytton




10.1109/TBME.2016.2539602








IEEE transactions on bio-medical engineering




63


10




















D
Moher






A
Liberati






J
Tetzlaff






D
G
Altman






T
P
Group








7












Preferred reporting items for systematic reviews and meta-analyses: The prisma statement


10.1371/journal.pmed.1000097


doi: 10.1371/journal.pmed.1000097








PLOS Medicine




6


7














Tutorial on maximum likelihood estimation




I
J
Myung




10.1016/S0022-2496(02)00028-7






Journal of Mathematical Psychology




47


1
















Applying Occam's razor in modeling cognition: A Bayesian approach




I
J
Myung






M
A
Pitt








Psychonomic Bulletin & Review




4


1
















If mathematical psychology did not exist we would need to invent it: A case study in cumulative theoretical development




D
J
Navarro








Perspectives on Psychological Science






in press








Addressing the theory crisis in psychology




K
Oberauer






S
Lewandowsky








Psychonomic Bulletin & Review




26


5
















Visual hallucinations are characterized by impaired sensory evidence accumulation: Insights from hierarchical drift diffusion modeling in Parkinson's disease




C
O'callaghan






J
M
Hall






A
Tomassini






A
J
Muller






I
C
Walpola






A
A
Moustafa






.
.
Lewis






S
J




10.1016/j.bpsc.2017.04.007








Biological Psychiatry: Cognitive Neuroscience and Neuroimaging




2


8
















The effect of stimulus strength on the speed and accuracy of a perceptual decision




J
Palmer






A
C
Huk






M
N
Shadlen




10.1167/5.5.1


reference{&}D=psyc4{&}NEWS=N{&}AN=2005-06758-001 doi








Journal of Vision




5


5
















Robust modeling through design optimization




M
A
Pitt






J
I
Myung




10.1007/s42113-019-00050-1


doi: 10.1007/s42113-019-00050-1








Computational Brain & Behavior




2


3
















R: A language and environment for statistical computing




R Core Team












Computer software manual








A theory of memory retrieval




R
Ratcliff




10.1037/0033-295X.85.2.59






Psychological Review




85


2
















The EZ diffusion method: Too EZ?




R
Ratcliff








Psychonomic Bulletin & Review




15


6
















The diffusion decision model: Theory and data for two-choice decision tasks




R
Ratcliff






G
Mckoon




873-922.doi:SYSTEMATICPARAMETERREVIEWS4610.1162/neco.2008.12-06-420






Neural Computation




20


4














Modeling response times for two-choice decisions




R
Ratcliff






J
N
Rouder




10.1111/1467-9280.00067


doi: 10.1111/1467-9280.00067








Psychological Science




9


5
















Diffusion decision model: Current issues and history




R
Ratcliff






P
L
Smith






S
D
Brown






G
Mckoon




10.1016/j.tics.2016.01.007






Trends in Cognitive Sciences




20


4
















Multinomial modeling and the measurement of cognitive processes




D
M
Riefer






W
H
Batchelder




10.1037/0033-295X.95.3.318






Psychological Review




95


3
















The file drawer problem and tolerance for null results




R
Rosenthal




10.1037/0033-2909.86.3.638






Psychological Bulletin




86


3
















Toward a principled Bayesian workflow in cognitive science




D
J
Schad






M
Betancourt






S
Vasishth




10.1037/met0000275






Psychological Methods
















Decomposing task-switching costs With the diffusion model




F
Schmitz






A
Voss




10.1037/a0026003






Journal of Experimental Psychology: Human Perception and Performance




38


1
















Estimating the dimension of a model




G
Schwarz








The Annals of Statistics




6




















W
R
Shankle






J
Hara






T
Mangrola






S
Hendrix






G
Alva






M
D
Lee


















Hierarchical Bayesian cognitive processing models to analyze clinical trial data












Alzheimer's & Dementia


https://alz-journals.onlinelibrary.wiley.com/doi/abs/10.1016/j.jalz.2012.01.016






9














Bayesian measures of model complexity and fit




D
J
Spiegelhalter






N
G
Best






B
P
Carlin






A
Van Der Linde








Journal of the Royal Statistical Society
















10.1111/1467-9868.00353


Series B: Statistical Methodology




64














The deviance information criterion: 12 years on




D
J
Spiegelhalter






N
G
Best






B
P
Carlin






A
Van Der Linde








Journal of the Royal Statistical SYSTEMATIC PARAMETER REVIEWS




47




















Society: Series B (Statistical Methodology)




76


3














Models for choice-reaction time




M
Stone








Psychometrika




25


3
















Racing to remember: A theory of decision control in event-based prospective memory




L
Strickland






S
Loft






R
W
Remington






A
Heathcote








Psychological Review




125


6
















Age differences in diffusion model parameters: A meta-analysis




M
Theisen






V
Lerche






M
Von Krause






A
Voss




10.1007/s00426-020-01371-8


doi: 10.1007/s00426-020-01371-8








Psychological Research
















The ubiquitous Laplacian assumption: Reply to Lee and Wagenmakers




D
Trafimow




10.1037/0033-295X.112.3.669






Psychological Review




112


3
















A method for efficiently sampling from distributions with correlated dimensions




B
M
Turner






P
B
Sederberg






S
D
Brown






M
Steyvers








Psychological Methods




18
















Diffusion model analysis with MATLAB: A DMAT primer




J
Vandekerckhove






F
Tuerlinckx








Behavior Research Methods




40


1
















The interpretation of behavior-model correlations in unidentified cognitive models




L
Van Maanen






S
Miletić




10.3758/s13423-020-01783-y






Psychonomic Bulletin and Review
















Core body temperature speeds up temporal processing and choice behavior under deadlines




L
Van Maanen






R
Van Der Mijn






M
H P H
Van Beurden






L
M M
Roijendijk






B
R M
Kingma






S
Miletić






H
Van Rijn




10.1038/s41598-019-46073-3








Scientific Reports




9


1


10053














Constructing informative model priors using hierarchical methods




W
Vanpaemel










Journal of Mathematical Psychology




55


1


















10.1016/j.jmp.2010.08.005








SYSTEMATIC PARAMETER REVIEWS




48












Using priors to formalize theory: Optimal attention and the generalized context model




W
Vanpaemel






M
D
Lee




10.3758/s13423-012-0300-4


doi: 10.3758/s13423-012-0300-4








Psychonomic Bulletin & Review




6
















The EZ diffusion model provides a powerful test of simple empirical effects




D
Van Ravenzwaaij






C
Donkin






J
Vandekerckhove








Psychonomic Bulletin & Review




24
















Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC




A
Vehtari






A
Gelman






J
Gabry








Statistics and Computing




27
















Parameter recovery, bias and standard errors in the linear ballistic accumulator model




I
Visser






R
Poessé




10.1111/bmsp.12100


doi: 10.1111/bmsp.12100








British Journal of Mathematical and Statistical Psychology




70


2
















Interpreting ambiguous stimuli: Separating perceptual and judgmental biases




A
Voss






K
Rothermund






J
Brandtstädter




10.1016/j.jesp.2007.10.009








Journal of Experimental Social Psychology




44


4
















Fast-dm: A free program for efficient diffusion model analysis




A
Voss






J
Voss








Behavior Research Methods




39


4
















The RWiener package: An R package providing distribution functions for the Wiener diffusion model




D
Wabersich






J
Vandekerckhove










R Journal




6


1
















AIC model selection using Akaike weights




E.-J
Wagenmakers






S
Farrell




10.3758/BF03206482


doi: 10.3758/BF03206482








Psychonomic Bulletin & Review




11


1
















An EZ-diffusion model for response time and accuracy




E.-J
Wagenmakers






H
L J
Van Der Maas






R
P P P
Grasman




10.3758/BF03194023


doi: 10.3758/BF03194023








Psychonomic Bulletin & Review




14


1
















Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory




S
Watanabe








Journal of Machine Learning Research




11
















Decomposing bias in different types of simple decisions




C
N
White






R
A
Poldrack








Journal of Experimental Psychology: Learning, Memory, and Cognition




40


2
















HDDM: Hierarchical Bayesian estimation of the Drift-Diffusion Model in Python




T
V
Wiecki






I
Sofer






M
J
Frank




10.3389/fninf.2013.00014








Frontiers in Neuroinformatics




7














Web of Science




Wos























"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]