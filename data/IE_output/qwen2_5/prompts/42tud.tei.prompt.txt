You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



communication are fundamentally multimodal and not simply accompanied by optional nonvocal cues 
(Abner et al., 2015;
Bavelas, 1990;
Goodwin, 1986;
Hagoort & Özyürek, 2024;
Holler & Levinson, 2019;
Keevallik, 2018;
Kendon, 2004;
Kendrick et al., 2023;
Mondada, 2016;
Özyürek, 2021;
Perniss, 2018;
Rasenberg et al., 2022;
Sandler, 2022
Sandler, , 2024
Trettenbrein et al., 2025;
Vigliocco et al., 2014;
Wilkinson et al., 2016)
. Rather than relying solely on speech, manual signs, or text, real-world language use and communicative interactions involve gestures, facial expressions, body posture, gaze, or tactile and haptic cues 
(Checchetto et al., 2018;
Edwards & Brentari, 2020;
Holler & Levinson, 2019;
Özyürek, 2021)
. While research in the language sciences has traditionally focused on humans, recent studies have explored parallel phenomena in non-human primates, who appear to also frequently integrate communicative signals across modalities to enhance meaning, improve signal effectiveness, and adapt to diverse social and environmental contexts 
(Aychet et al., 2021;
Fröhlich et al., 2019;
Fröhlich et al., 2021;
Hobaiter et al., 2017;
Liebal et al., 2013;
Mine et al., 2024;
Wilke et al., 2017)
. This increasing cross-disciplinary interest in multimodality is directly reflected in the increasing number of publications on the topic over the past decades 
(Figure 1
), presumably driven by both theoretical advancements and technological progress in capturing and analyzing multimodal phenomena. However, the concept of multimodality remains diffuse, encompassing a range of interpretations and definitions 
(Fröhlich et al., 2021;
Sandler, 2022)
. Despite growing attention to multimodality and the multimodal nature of communication, current guidelines and standards for multimodal data collection and interpretation remain discipline-specific and fragmented. Previous works offer valuable insights, and practical recommendations for specific fields, such as co-speech gesture studies (e.g., 
Mittelberg, 2007)
 and sign language research (e.g., 
Fenlon & Hochgesang, 2022;
Orfanidou et al., 2015)
, including specific guidance for video data quality and annotation 
(Hanke & Fenlon, 2022;
Perniss, 2015)
. However, researchers investigating multimodal language and communication would benefit from a broader approach incorporating more diverse populations (e.g., children, clinical groups, non-human primates) and specifically focusing on the combination of multiple data streams (e.g., motion capture, physiological sensors, eye-tracking, audio and video streams) and their integration to effectively address interdisciplinary research questions. Furthermore, existing guidelines 
(Fenlon & Hochgesang, 2022;
Mittelberg, 2007;
Orfanidou et al., 2015;
Hanke & Fenlon, 2022;
Perniss, 2015)
 do not fully reflect recent technological advancements, such as semi-automatic annotation tools, computer-vision based, markerless motion tracking methods, and techniques for synchronizing and integrating multimodal data sources. These developments create new methodological opportunities and challenges highlighting the need for a coherent decision-making framework for researchers across disciplines that supports comprehensible, reasonable, and replicable practices for multimodal data collection.
An extensive review on the state of the art of existing research tools is available in a previous publication 
(Gregori et al., 2023)
. The goal of this paper is to outline a decisionmaking framework for multimodal data collection based on practical experiences. The framework is intended to apply to multiple scenarios across disciplines rather than prescribing a single one-size-fits-all protocol. We introduce this approach to help researchers navigate the methodological complexities inherent in multimodal data collection involving both human and non-human communicative behavior. To bridge interdisciplinary gaps and refine best practices, we illustrate updated recommendations through case studies to highlight key aspects. First, we offer structured guidelines for integrating cross-disciplinary methods, combining approaches A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 7 from linguistics, psychology, and computer science. The case studies demonstrate how these different fields can collaborate effectively. Second, we address the challenges posed by new technologies, ensuring the ethical, transparent, and reproducible use of existing tools, as illustrated through our case examples. Finally, we emphasize the importance of standardizing data collection, improving metadata documentation, and refining annotation practices, all of which are demonstrated in the case studies to enhance research interoperability.


What is "Multimodality" in the Cognitive Sciences?
A fundamental challenge in multimodal research is the precise definition of the term multimodality, which can be understood from various interrelated perspectives and does not necessarily have a clear meaning 
(Sandler, 2022)
. Different researchers coming from different fields tackling at times fundamentally different research questions have employed the terms "multimodality" and "multimodal", but not necessarily with the same intended meaning. These different uses of terminology go beyond theoretical disagreements and seem to us to be rooted in differences with regard to whether researchers attempt to describe language or communication as multimodal, or whether they are referring to the kind of data that they are collecting as being multimodal.
One perspective considers multimodality as a property of language and communication, viewing language use as an inherently multimodal phenomenon that spans multiple or different physical channels of transmission rather than being confined to a single modality 
(Cohn & Schilperrord, 2024;
Gregori et al., 2023;
Hagort & Özyürek, 2024)
. Spoken languages primarily rely on the vocal-auditory channel, while gestures rely on the visualkinematic channel. In sign languages, where the vocal-auditory channel is not available, the linguistic signal is mainly conveyed through the hands, the head and parts of the face, but can also include structural elements, such as mouthings, which are influenced by other channels (e.g. written language) and are considered a part of the natural signing 
(Bauer, 2019;
Bauer & Kyuseva, 2022;
Boyes-Braem & Sutton-Spence, 2001;
Mohr, 2012)
. Language can also take the form of a tactile sign language 
(Checchetto et al., 2018;
Edwards & Brentari, 2020;
Mesch & Raanes, 2023)
 or span multiple modalities at once (e.g., vocal signals and gestures complementing each other in spoken interaction 
(Özyürek, 2021;
Wagner et al., 2014)
.
Another perspective approaches multimodality as a property of the data, referring to the integration of multiple data streams, multiple instruments, measurement devices, or acquisition techniques that capture the same phenomenon. This might include modalities such as audio, video, breathing patterns, electroencephalography (EEG), magnetoencephalography (MEG), eyetracking, or motion capture to analyse visual signals like co-speech gestures. In this framework, each acquisition method is referred to as a modality and corresponds to a distinct dataset. The entire setup, in which data from multiple modalities are available, is described as multimodal. A key characteristic of multimodality is complementarity-each modality contributes unique information that cannot straightforwardly be inferred from any other modality within the setup 
(Lahat et al., 2015)
. At the same time, multimodal data may be codependent on each other such that data of one source be considered as the trigger of processes observed in another modality (e.g. fixation-related EEG effects; 
Dimigen & Ehinger, 2021)
.
Thus, to study the relation of co-speech gesture to speech, audiovisual recordings may be combined with data from an eye-tracker and/or a motion-capture device. Whereas the former multimodal understanding of language and communication was brought about primarily by insights from the study of the world's different sign languages 
(Pfau et al., 2012;
Sandler & Lillo-Martin, 2006)
 and from linguistic research on gestures 
(Holler & Levinson, 2019;
Kendon, 1980
Kendon, , 2004
McNeill, 1992;
Müller et al., , 2014
, the latter understanding of multimodality is of major importance to anyone planning to collect data documenting multimodal linguistic phenomena in the former sense.
In practice, these two perspectives interact: Once we acknowledge (1) the inherently multimodal nature of language and communicative behavior, we are compelled (2) to gather richer, multifaceted datasets to capture the full complexity and richness of (non-)human communication. In turn, this approach may reveal even more aspects of the multimodality of communication. To fully embrace this shift from single-channel to multi-channel recording setups it is essential to understand the various types of data and their characteristics-where A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 9 they come from, how they were collected, how they can be analysed, how they should be stored. In other words, such expanded data collection introduces several methodological and ethical challenges. In this paper, we delineate the primary types of data that emerge in multimodal communication research, highlighting their significance to address key research questions, as well as the methodological considerations related to their characteristics, interactions, and analytical challenges. While many of the points we address apply to general study planning and data-collection protocols, we focus here on the additional layers of complexity introduced by the simultaneous capture of multi-channel data, where best-practice guidelines may be either limited or overly specific.


Three Illustrative Case Studies
Throughout this paper, we illustrate the complexities of multimodal data collection by referring to examples from our own distinct studies: eliciting multimodal data in controlled laboratory settings, constructing annotated corpora for large-scale analysis, and gathering naturalistic data in field environments.
As an exemplification of eliciting multimodal data in the lab (Example 1), we examine a gesture elicitation task conducted to inventory the gestural repertoire of non-signing speakers of 
German (Spruijit et al., 2025)
. This experiment is part of a larger project investigating the influence of the gestural repertoire on the acquisition of individual parameters of a signhandshape, location, and movement-by second language learners of a language in a different modality than their native language. For the elicitation study, silent gestures were elicited from 18 native non-signing speakers of German to investigate the systematicity of gestural representations for particular concepts. None of the participants had prior exposure to a signed language. Participants were presented with a concept in written form and were asked to express this concept gesturally and without pointing to objects in their surroundings.
As a second example (Example 2), we refer to the DGS Corpus, an annotated corpus of German Sign Language (DGS), which is a collection of signed conversations that have been enriched with additional information (e.g. annotations of manual and non-manual features) to facilitate linguistic analysis. It is part of the DGS-Korpus project, a long-term initiative aimed at A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 10 developing both a linguistic reference corpus and a corpus-based dictionary of DGS 
(Prillwitz et al., 2008)
. In its first data collection campaign (2010-2012) it collected 560 hours of discourse from 330 participants. A second campaign is currently ongoing 
(Konrad et al., 2024)
.
Recordings cover participants from all regions of Germany and all participants, moderators and technicians present during recording sessions use DGS as their primary language of daily life.
Participants were grouped into pairs and engaged in various conversational tasks, including discussions on assigned topics or historical events, free dialogue, and story retelling 
(Nishio et al., 2010)
. For further details on corpus curation and demographic composition, see 
Schulder et al. (2021)
 and 
Schulder & Hanke (2022)
. 50 hours of the DGS were made available publicly as the Public DGS Corpus, which includes the research dataset "MY DGS-Annotated" that provides full sign annotations and translations in German and English 
(Hanke et al., 2020;
Konrad et al., 2020)
.
While lab studies have a variety of advantages over the control of the environment, not all studies are possible or ideal in the lab. As an example of gathering naturalistic data outside of the lab, we refer to the collection of multimodal data from non-human primates (Example 3).
Here, instead of focusing on a specific case study, we will refer to a group of existing empirical studies on one of our closest living relative, the chimpanzee 
(Hobaiter et al., 2017;
Mine et al., 2024;
Wilke et al., 2017)
. All studies of Example 3 involve video observations of naturalistic interactions in wild or captive settings and annotation of signal production in the different modalities.


Types of Data in Relation to Research Questions and Data Collection Methods
The first step is always to clarify what kind of data is needed to answer a specific research question, and what type of conclusions one would like to be able to draw from the data. Examining short, highly specific phenomena (e.g., individual signs or phonemes, isolated gestures, specific facial expressions, etc.) will necessitate a different experimental design and laboratory setup than studying broader continuous interactions (e.g., dialogues, group interactions, social behaviour, etc.). Specifically, while the former may require an experimental design in which specific conditions, later to be contrasted during analysis, can be elicited by the experimenter from participants (as in Example 1), the latter setup could, in principle, also be implemented using an observational design, which is more suitable for drawing looser correlative conclusions (as in Example 2 from the DGS Corpus). Decisions about which dependent variables and their measurements not only reflect the research question and hypothesis, but also shape the laboratory setup and choice of recording devices.
For instance, in Example 3, early decisions on data collection protocols, such as whether to perform focal sampling or ad libitum video recordings, can significantly influence the observed range of behaviors (i.e. the repertoire), underscoring the importance of careful protocol selection. Moreover, given that repertoires are often established using top-down classification, it is important to ensure the consistency of applied criteria and suitable granularity to ensure that all signals and modalities are equally captured. Bottom-up coding schemes are a good solution to this problem and enhance transparency and reliability (e.g.
ChimpFACS, 
Vick et al., 2007;
ChimpLASG, Zulberti et al., 2024)
. Overall, decisions about the type, scope, granularity, and modality of data should align closely with the target phenomenon to support robust and meaningful conclusions.


Phenomena of Interest and How They Can be Captured
The distinction between single signs or gestures, short sequences, and continuous, longer-duration conversational data reflects divergent methodological approaches in multimodal analysis. Short sequences, encompassing isolated gestures, signs, words, or phrases (e.g., in sign language lexicon studies or imitation experiments), enable granular examination of specific signals, such as the kinematic or acoustic properties of gestures, signs, utterances, or even individual phonemes. In contrast, continuous conversational data-spanning extended interactions like conversations, interviews, or debates-provide contextualized insights into language pragmatics and discourse dynamics. Such naturalistic datasets facilitate multilevel analyses: At the micro level, the tight coupling of gesture and speech can be scrutinized; at the meso level, recurring multimodal themes structuring entire interactions emerge; and at the macro level, broader discourse stretches reveal convergent multimodal patterns that form larger thematic units 
(Ladewig & Horst, 2024;
Müller & Kappelhoff, 2018;
.
This hierarchical framework underscores the necessity of aligning data granularity with research objectives, whether focused on discrete signal properties or the interplay of modalities across discourse scales.
The collection of multimodal data can thus be approached in various ways, depending primarily on the phenomenon or phenomena of interest as well as population being studied. For example, some studies may capture free-flowing interactions, while others may incorporate mild structuring, such as tasks or prompts, to ensure coverage of specific behaviors such as monologues, dialogues, or multilogues. Data from the latter approach allows for studying phenomena such as feedback, turn-taking behavior, interactive dynamics, or collaborative communication. In these contexts, gestures emerge as a means of organizing social interaction which are specialized in fulfilling pragmatic functions ("recurrent gestures", 
Ladewig 2014
Ladewig , 2024
Müller, 2017)
. These include modal, meta-communicative, performative, and interactive/interpersonal functions (e.g., 
Bressem & Müller, 2014;
Kendon, 2004
Kendon, , 2017
Ladewig, 2014)
. A more controlled setting, as in our Example 1, involve a clear task that limited the participants options for responding in a way that would make production of the desired phenomenon (i.e., a gesture describing a given written word or concept in the context of Example 1) by the participants more likely. In non-human primate studies, multimodal communication is often best observed by recording spontaneous interactions between individuals (as in Example 3). In fact, although some studies have successfully triggered the production of multimodal signals from apes interacting with human experimenters (e.g., 
Leavens et al., 2010)
, the ecological validity of such experiments has been questioned (e.g., de  
Waal et al., 2008;
Whiten, 2022)
. A similar problem arises for the study of multimodal perception: while vocal signals can be readily played back to test for individuals' reactions in controlled environments (e.g., 
Mehon & Stephan, 2021)
, the controlled reproduction and manipulation of gestures and facial expressions is still quite limited (but see 
Waller et al., 2016
).
The two main types of data collected in multimodal research are typically classified as elicited or naturalistic. Elicited data is collected in a controlled setting like a stationary or mobile laboratory, where participants are prompted to produce specific responses, and is useful for studying specific linguistic features or behaviors under controlled conditions. Both, Example 1 and Example 2, employed such an approach although there are obvious differences:
Whereas, for Example 1, participants were recorded in a lab located at the university where the researchers carrying out the study were based, data that became part of the DGS Corpus (Example 2) was collected at various sites throughout Germany to avoid participants adjusting their dialectal language use to that of the recording region, so the data collection team would travel instead and bring along the necessary equipment. In contrast, naturalistic data is collected in participants' natural environments without the intervention of experimenters, enabling, as in studies from Example 3, the investigation of multimodal signal use across a variety of contexts and therefore providing a comprehensive understanding of the communication system under investigation. This is particularly relevant in the field of animal communication, as methods for the inference of signals' meanings and functions -two common topics of investigation -can only be derived by contextual information given by the naturalistic settings of interactions.
Moreover, it should also be mentioned here that multimodal data may also be used as stimuli in case of elicitation or any other kind of experiment, which may cause researchers to face similar obstacles during stimulus presentation that may be encountered during data collection. That is, some variables in the design of such stimuli can be more tightly controlled A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 14 than in data collection, while others are also subject to limitations in the collection of multimodal data. For example, in recording video stimuli of a signer or a person gesturing, there may be subtle differences in movements of the torso, head, or a similar subtle difference that could systematically bias participants for or against a certain experimental condition. One way of dealing with this is to repurpose the same methods that can be used to extract relevant information about multimodal data from such as, for example, computer-vision approaches for pose-estimation of people visible in a video and use these methods for controlling stimuli (e.g., to ensure that there are no systematic differences in movement patterns across different conditions of a controlled factorial experiment in the context of sign language research; see e.g., 
Trettenbrein & Zaccarella, 2021;
Trettenbrein et al., 2024)
.


Where to Collect Data? Laboratory vs. Online vs. On-site
Laboratory data is collected in a controlled environment, providing stable conditions throughout the entire duration of the experiment. However, participants must come in person, requiring each slot to be arranged individually or depending on the research question coordinating two or more participants to come to the same appointment. However, an online participant management tool can facilitate the organisation and recruitment of participants.
Running a lab may also require a lab manager, making it the most effortful yet quality-assured environment. This can influence the smaller participant numbers typically seen in lab studies.
In contrast, online data is collected via the individual devices of each participant, thus limiting quality control. Quality can be ensured by including so-called catch trials where participants have to respond in a particular way that differs from the actual task, as well as by asking questions about the input device (laptop, tablet, mobile phone) and the output device (especially relevant for audio, such as the type of headphones 1 ) to include this information as covariate in the analysis. In perception experiments, control is higher than in production experiments conducted online due to high variance in cameras and microphones, and there is no control over the participant's position relative to the devices. The advantages include easy accessibility to large and diverse participant groups and generally a fast data collection completion (a few hours to some days).
Lastly, on-site data is collected outside the lab at the participant's location, making it most commonly used in fieldwork. The data has limited quality control during recording because it depends on the environmental conditions of the site (soundproofing, background noise, general isolation, etc.). However, the data is more naturalistic due to the familiar environmental setting and in many cases can be easier to collect from large participant numbers because the researcher travels to the participants, rather than the other way around.


From whom to collect Data? Population(s) of Interest
Depending on the actual research question, data can be collected from specific demographic or social groups (e.g., children, adults, elderly, bilingual speakers, individuals with communication disorders) or even species (e.g., non-human primates). This approach allows for comparison across different groups, enhancing our understanding of language acquisition, variation, and use in diverse populations. However, collecting data from specific groups may require more effort due to accessibility and special ethical considerations. Working with clinical populations is most effective if an institution has an established collaboration with a clinic. It is also important to consider the appropriate environmental setting for working with a given population, determining whether a lab experiment is feasible or if only an on-site experiment is possible. For the study of non-human primates, additional considerations must be made regarding the rearing conditions of the population under study, i.e. whether captive, semi-wild or wild, and decisions met based on the influence that this factor might have on the research question.


How to Collect Data? Data Collection Methods
In naturalistic settings, data collection typically involves recording participants in realworld interactions, such as everyday conversations, interviews, or group discussions, under minimal researcher intervention. Video and audio equipment is set up to capture both speech, gesture, facial expressions, and other body movements without disrupting the natural flow of communication. Multiple camera angles and microphones help ensure that subtler gestures and simultaneous speakers are documented clearly, although this is not always possible in naturalistic settings like fieldwork. Ethnographic methods, such as participant observation, can also be employed, where researchers spend extended periods immersed in the group or community of interest. This approach allows for deeper insights into how gestures emerge within specific cultural or social routines. Ethical considerations, notably, informed consent, confidentiality, and the right to withdraw, are crucial, as these recordings often contain sensitive or personal content.


Technical Considerations
Recent technological advancements have made it possible to capture high-quality multimodal data by integrating video, audio, and other sensor-based streams such as depth sensors, motion tracking, eye tracking, and physiological sensors (see also 
Gregori et al., 2023)
.
However, the successful implementation of such data collection requires careful planning regarding equipment selection, synchronization, and data management 
(Offrede et al., 2021)
.
Selecting appropriate recording equipment is crucial for ensuring data quality and usability. High-definition (HD) cameras with a minimum resolution of 1920×1080 pixels are generally recommended, although higher resolutions such as 4K or even 6K may be beneficial for datasets intended for long-term reuse or detailed analyses (for example facial details for analysis of emotions). For its second data collection campaign, DGS Corpus upgraded from HD to 6K. For precise motion analysis, particularly involving rapid hand movements such as those observed in sign languages, gestures, or finger spelling, high frame rates (e.g. 100 fps and higher) and fast shutter speeds are essential to minimize motion blur. Adequate, evenly distributed lighting is crucial, particularly at high frame rates, to avoid blurring of rapid movements. Image blur can be a major hindrance for both human annotation and for computer vision applications. Video recorded indoors at 50 frames per second is typically insufficient to capture such rapid movements. Therefore, fast shutter speeds are recommended and can either be set explicitly or achieved by using high frame rates and ensuring good lighting conditions 
(Crasborn & Morgan, 2021)
. The setup includes five cameras: two HD cameras provide frontal views of the informants, two stereo cameras (not visible in the picture) capture the signing from above for 3D reconstruction, and one HD camera records the overall scene, including the moderator (seated between the informants, in the chair that is empty in the picture). Elicitation materials are presented on screens placed low between the signers to avoid obstructing their views. The studio arrangement ensures separate filming of each informant while also offering a comprehensive view of interactions, assisting with subsequent transcription and analysis 
(Hanke et al., 2010)
.
In studies involving dyadic interactions, a common practice is to use at least three cameras: a frontal view for each participant and a side view capturing both interactants simultaneously (see 
Figure 2
). For sign language recordings, occlusion and the lack of depth perception in 2D recordings pose significant challenges. A top-down bird's-eye camera can be valuable for translation, annotation, or gloss transcription 
(Hanke et al., 2010)
. When collecting data for computer vision applications (e.g. full 3-D reconstruction), additional camera angles, such as 45-degree side views ore often needed. For example, each public recording in the DGS Corpus includes pose estimation data . This data, generated using OpenPose 
(Cao et al., 2021)
  Background color selection is a crucial factor in recording. While blue and green screens are commonly used, any uniform background that provides clear contrast with participants' clothing is preferable. Ideally, participants should wear a unicolored shirt, preferably black, to ensure optimal contrast with the background screen as in 
Figure 2
. Blue screens offer a good balance, functioning as a neutral background when watching the video unaltered, while also working well for chroma key operations (i.e., when the background is virtually replaced). Green screens are a typical choice for chroma key operations, but some people report feeling uncomfortable while watching them. However, depending on the research question, unnatural or overly intense background colors and an overly simplified background setting might impede naturalistic and comfortable social interactions and thus needs to be weighed against the technical advantages. Synchronization is a critical aspect of multimodal data collection, particularly when combining different data streams such as audio, video, and sensor inputs (see also 
Gregori et al., 2023)
. Where technically feasible, timecode coordination should be implemented during recording to enable automatic alignment of different video and audio streams (i.e. synchronizing the internal timer generators of all recording devices, and using data recording formats with integrated timestamps). When this is not possible, external synchronization tools should be employed. A common approach is to use a clapperboard or an LED flash at the start A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 20 of each recording session, ensuring a clear visual and auditory marker on all recording devices.
Some devices can or need to be synchronized via external hardware trigger signals or remote synchronization protocols that either ensure synced hardware timing, or reliable recording of the external signal in the recorded data stream to allow for post-hoc alignment. For devices outside the auditory or visual domain (e.g. physiological data, motion sensors, thermal cameras) this is often the only option for synchronization. It is important to consult the documentation of respective devices how precise the achieved synchronization can be (a few ms or a few 100 ms?), e.g. if an experimental effect is expected in the range of ~ 200-400 ms, potential errors from synchronization should be an order of magnitude lower.
Two primary synchronization strategies can be implemented. A centralized recording setup involves a single computer managing all data streams in real time, using tools like Lab Streaming Layer (LSL, https://github.com/sccn/labstreaminglayer). LSL is a software tool and protocol designed for real-time data streaming and synchronization in experimental environments, which facilitates the seamless integration of various data sources and devices by assigning a unique timestamp to each data source. Alternatively, a decentralized approach relies on multiple independent recording devices that have a mechanism for precisely synchronizing their inter time generators prior to a recording, receive external triggers, or integrate a shared timecode signal. In such cases, post-hoc synchronization methods are required (see also 
Nalbantoğlu, H. & Kadavá, Š., 2024)
. Regardless of the chosen strategy, synchronization protocols should always be tested with pilot data before full-scale data collection begins. If no synchronization strategy has been chosen prior to data collection, a last ressort in the case of audiovisual data could be to synchronize via the audio tracks, or to identify clear visual signals visible in all data streams and use an annotation tool like ELAN for determining relative offsets between the signals and control for these 
(Wittenburg et al., 2006)
.
An example of a controlled multimodal data collection setup is the gesture elicitation study (Example 1). Participants were seated against a solid blue background to enhance contrast. The experimental environment was illuminated by four studio lights to ensure consistent lighting conditions. A laptop monitor was placed at a 45-degree angle on the dominant side of the participant, positioned outside their reach to avoid interference with natural gesturing. During the experiment, after being shown a fixation cross accompanied by an auditory signal indicating the upcoming item, written stimuli were presented on the screen for four seconds. Participants produced their gestures during presentation, and were recorded from two camera angles: one directly in front of the participant and another at a 45-degree angle from the non-dominant side. To ensure precise synchronization of data streams-including the two video recordings and the randomly ordered stimulus presentation-button presses were logged in each modality. The experiment presentation software (PsychoPy) automatically recorded these events, while the cameras were positioned to capture the laptop keyboard, allowing for later alignment in ELAN.
In contrast to controlled laboratory settings, naturalistic data collection focuses on capturing real-world interactions with minimal researcher intervention. This approach is often used to study spontaneous communication in everyday conversations, interviews, or group discussions. To ensure that subtle gestures, facial expressions, and body movements are adequately documented, multiple camera angles and microphones are typically used. In addition to video and audio recordings, ethnographic methods, such as participant observation, can be employed. By immersing themselves in a particular community, researchers gain deeper insights into the role of multimodal communication in specific social and cultural contexts. To this day, the best method for studying multimodal communication in non-human species is to observe spontaneous communicative behaviors in naturalistic settings. This does not necessarily need to be in the wild; it can also occur in captive conditions that closely resemble the species' natural habitat (e.g., sanctuaries) or in zoos, depending on the research question, provided that the species can interact freely with one another. It is worth noting that if the goal is to capture the natural socio-ecological variation in which communication occurswhich may be necessary to reliably identify the functions and drivers of multimodal communicationit is preferable to conduct the study in the wild. However, researchers must be aware that naturalistic data collection raises important ethical concerns. Ethical considerations in work with animals and informed consent, confidentiality, and the right to withdraw from the study in work with humans must be strictly upheld, as such recordings often contain sensitive or personally identifiable information.
By addressing these technical considerations, researchers can enhance the quality, reliability, and reproducibility of multimodal data collection, whether conducted in controlled laboratory settings or naturalistic environments.


Data Management, Data Sharing, and Ethics
Data management is of particular importance for multimodal research, as data organization, storage and accessibility get increasingly complex with the amount and diversity of data modalities. Managing multiple video streams, sensors, audio and associated metadata requires thorough planning.


Identify Data Sensitivity and Sharing Goals
A first decision concerns the degree to which the data can or should be shared. Factors include the presence of potentially identifying audio-video recordings and the extent of anonymity participants expect or require. Many researchers are particularly concerned with ensuring participant anonymity and may decide against using video data due to the additional workload involved in safeguarding privacy.
In our Example 1 it was deemed sufficient to share only parts of the data in the form of selected short clips openly, while restricting access to the full dataset. While this is a valid strategy for specific lab experiments, in the case of a public corpus like the DGS, more exhaustive sharing of a curated dataset is the primary goal. In the DGS Corpus, all participants provided informed consent for the use of their video materials in linguistic research and publications 
(Schulder et al., 2021;
Schulder & Hanke, 2022)
. The corpus team decided to release the full data set, but as a curated set of annotated videos under a specific license, while more exhaustive sensitive raw data was stored, but is only made accessible to trusted researchers under an additional agreement. In field work with non-human primates, humans may appear accidentally on some of the video material, and it is feasible to anonymize these if full data sharing is planned. On the other hand, fieldwork involving human participants requires significantly more effort in terms of obtaining consent and ensuring anonymization, and is often not feasible. Clinical data associated with individual level health information comes with its own challenges that need to be considered for each study and dataset individually 
(Kaur & Cheah, 2024)
. In such cases of (potentially) very high data sensitivity, fine-grained layered access strategies can be a solution 
(Marcotte et al., 2023)
.


Determine Level of Anonymization and Data Access
A related decision needs to be made with respect to the balance of privacy protection and research transparency.


Minimal vs. extensive anonymization
In a sign language corpus, or a lab experiments with elicited gestures, face blurring or voice distortion would remove critical information for data analysis. Researchers need to decide whether partial anonymization is feasible, or if restricted sharing is a better compromise. A further compromise could be to process data up to the point where anonymity is given (e.g. documented extraction of higher level features from voice or face) and provide these data for sharing along with the raw data of blurred faces and voice distortion, or with audio cut away from the raw video signal. One solution is the masked-piper tool 
(Owoyele et al., 2022)
 which masks video while overlaying facial, hand, and arm kinematics. This tool uses body tracking to mask the body while preserving background and kinematic data.


Open vs. Controlled-Access Repositories
When anonymization is incomplete, depositing data in controlled-access repositories can protect participants while still allowing for scientific collaboration, review, and re-use of acquired data. For particularly vulnerable populations such as children and clinical patients, researchers typically only share derived data (e.g. transcriptions carefully screened for sensitive information, extracted motion capture kinematics, extracted binary facial expression time courses, etc.) in open repositories, and place raw video/audio under a secure credential-based access system of the institution where the data was collected.


Automated Anonymization tools
Large-scale datasets may benefit from semi-automated anonymization tools (e.g. simple face blurring or more sophisticated approaches 
(Owoyele et al., 2022)
) though researchers should still remain aware of re-identification risks via advanced computer vision or other emerging technologies. Consequently, reevaluating anonymization protocols over time is warranted, especially as analytical techniques evolve 
(Gadotti et al., 2024)
.


Plan Informed Consent and Ethical Review
Multimodal research often requires more elaborate consent materials because of the increased complexities of privacy, potential for re-identification, and associated sharing policies. Study aims, types of data collected, foreseeable risks, and potential future use of the data need to be described in detail to obtain true informed consent, and in accordance with established guidelines of the field, community stakeholders involved in the research, and ethics committees. In particular with respect to data sharing, careful planning is required with respect to ethical issues involved 
(Meyer, 2018)
. When collaborating across multiple institutions or countries, ensure that the data-collection protocol satisfies all relevant institutional review boards (IRBs), national data protection regulations (e.g., GDPR, HIPAA), and disciplinespecific ethics guidelines. Also clarify how participants (or guardians, in the case of children) can request data removal if they later withdraw consent.


Establish Storage, Backup and Long-Term Archiving
Given the typically large file sizes of multimodal data (e.g. up to 4K video recordings, sensor data with high sampling rates), reliable storage solutions and backup protocols are essential. You should always make sure to take care of redundancy (at least two physically separate storage locations), keeping logs of structured metadata (e.g. sensor calibration, environment conditions, participant demographics, and other recording session details). For example, in the lab based study, each camera feed was backed up daily to external drives, session logs were carefully time-stamped to facilitate subsequent annotation in software tools such as ELAN. For field work, data redundancy and safety is at the same time more difficult to achieve and more critical: In the lab, data backup and redundant storage can often be achieved using automated scripts and simple procedures, while more planning is involved in field work (e.g. changing, backup of SD cards). Equipment is typically more safe in the lab and protected against environmental influences and data loss than "in the wild".
In addition, researchers may choose to adopt FAIR (Findable, Accessible, Interoperable, Reusable, see 
Schulder & Hanke, 2022;
Wilkinson et al., 2016)
 data principles, which promote robust documentation and standardized, open metadata. Creating a formal data management plan (DMP) helps ensure consistent practices for backup, versioning, and archiving 
(Michener, 2015)
.


Repository choice for publication and sharing
For vulnerable participant groups and data with only partial anonymization only secure institutional hosting and storage of the data may be feasible. Such repositories may offer only limited data and access managing features that differ substantially across institutions. On the other hand numerous publicly accessible repositories are available that offer flexible features such as layered data access restrictions, custom licenses and advanced data management and version history (see https://www.re3data.org/ 
(Pampel et al., 2013)
 for available repositories and their features). Usage licensing and repository choice is highly dependent on anonymization constraints, needs for access restrictions, and ultimately needs to be in accordance with legal regulations and the explicit consent obtained from the participants. Thus, the ultimate check before publishing a multimodal dataset always needs to be whether the storage and sharing strategies and policies align with the written informed consent. Whenever possible, persistent identifiers (e.g. DOIs) should be attached to archived datasets to allow for citation and tracking.
Tools and web services like OSF, Zenodo, or other specialized repositories often support versioning and fine-grained access controls. It is also advisable to clarify the procedure for "take-down requests", i.e. how participants or their guardians may request partial or complete removal of their data at a later point.
These five decision points-data sensitivity and sharing goals, anonymization and access, informed consent, storage and backup, and repository choice-are essential for a professional data management strategy in multimodal research. As illustrated, there are always trade-offs involved (e.g. open science vs. participant confidentiality, anonymization vs. data completeness), but addressing each decision point optimizes data collection and the opportunities for re-use and sharing for the benefit of scientific advancement.


Discussion
In this paper, we have addressed core methodological, ethical, and practical considerations in collecting multimodal data across diverse contexts, using three examples (structured gesture elicitation, a large-scale sign language corpus, and field-based primate studies). All three examples underscore the tension between control and ecological validity. In a tightly controlled lab elicitation task (Example 1), researchers can target specific phenomena such as silent gestures, but questions arise about generalizability to spontaneous interaction.
Conversely, corpus-based efforts (Example 2) capture rich, large-scale data that span demographic diversity and everyday contexts, yet demand more elaborate synchronization, metadata documentation, and annotation protocols. Field studies (Example 3) maximize ecological validity but face logistical challenges, including minimal control over lighting, participant movement, environmental disturbances, and special requirements for fail-safe equipment.


The Workflow for Multimodal Data Collection: A Flexible Decision Framework
This flexible decision framework for multimodal data collection, presented here as a diagram 
(Figure 3)
, represents a key outcome of our contribution, highlighting the important factors involved in the process. While the list of factors is not exhaustive, and other similarly significant decisions may arise, we believe these are the most crucial and warrant attention.
Considering recent technical advances and the interconnectedness of these factors, as depicted in the diagram, this framework aims to guide researchers in making informed decisions for the collection of multimodal data in language sciences. 


Additional Considerations
We acknowledge that many aspects remain unexamined and additional challenges must still be considered. In this section, we touch upon some of these factors.
Collecting multimodal data from children or clinical populations entails unique methodological, ethical, and practical challenges. Protocols must be adapted to accommodate each group's specific capabilities and constraints. For instance, sensors typically designed for use with healthy adults may require different calibration procedures, hardware modifications, or adaptation protocols when used with children or clinical populations. Additionally, individual differences in motor and cognitive development over time can impact the reliability of certain data readings. This is particularly important when setups integrate multiple data channels, requiring careful control to ensure data accuracy and consistency.
Sensor placement (e.g., for motion capture) and camera framing should be adjusted to account for frequent postural shifts or smaller body sizes. Stimuli and instructions must also be tailored to the developmental stage of participants to elicit behaviors across multiple modalities (e.g., speech and gesture). Shorter tasks, game-like interactions, and visually engaging stimuli can improve participant engagement, particularly in multi-sensor setups or when motiontracking equipment is used. Additionally, real-time monitoring of children's fatigue levels is crucial, as tired or distracted participants may produce incomplete or lower-quality multimodal data.
Individuals with motor or cognitive impairments may face difficulties following specific instructions required for certain recording setups. To minimize participant burden, researchers should adapt the experimental setup including, for example, task instructions and sensor configurations where possible to the needs of the target population (e.g., using remote eye-tracking instead of head-mounted devices) and consider collecting fewer but higher-quality data streams. Reducing complexity in the experimental setup can improve data reliability while ensuring a more inclusive research approach.
Furthermore, identifiable audio-video recordings pose significant privacy concerns, particularly when linked to clinical diagnoses or vulnerable populations such as children.
Researchers must carefully balance the benefits of sharing rich multimodal datasets with the need for strict de-identification protocols and robust consent procedures. Ensuring that participants and their guardians fully understand the implications of data sharing is essential for ethical data collection and management.
Finally, researchers working with small or marginalized language communities, such as Deaf Ukrainians who have migrated to Germany-a minority within a minority-should be particularly mindful of the ethical considerations involved in data collection. Language data from these groups should be gathered by individuals with established relationships within the community. It is crucial to avoid overburdening language communities with repeated data elicitation requests, as they may be navigating other challenges and priorities. Instead, researchers should seek ways to contribute meaningfully to the community, such as supporting the integration of Deaf 2 participants or engaging in initiatives that directly benefit the group.
By addressing these methodological and ethical challenges, researchers can ensure more robust and responsible multimodal data collection practices. Future work should continue refining protocols to improve inclusivity, reliability, and ethical standards in multimodal research across diverse populations.
Figure
Publications Matching "Multimodal Language" in PubMed Note. The increasing interest in multimodality in the language sciences is evidenced in the number of papers published referring to the concept. The histogram shows the total number of papers matching a targeted search on PubMed using the search term "((language) OR (communication)) AND ((multimodal) OR (kinaesthetic))". Because the search produced no relevant findings prior to the 1970s, the figure only displays data from that date until the end of 2024, excluding partial data for the current year 2025. A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 6


Figure
Technical Setup of a DGS Corpus Recording Session (Screenshot from dgskorpus_koe_01_free conversation)Note. The informants are seated facing each other at a distance of approximately three meters.


and post-processed to correct errors, addresses issues such as the misrecognition of nonexistent persons, the incorrect segmentation of a single individual into multiple entities, and the accidental inclusion of the other participant's hand in the opposite front-facing recording. Additionally, frames lost during recognition are recovered. The availability of such refined pose estimation data enables detailed research into the kinematic characteristics of multimodal signals, as demonstrated by Bauer et al. (2024).


Microphone selection is equally important. High-fidelity audio recording ensures that speech and other vocalizations are accurately captured. The placement of microphones must be carefully considered to maximize proximity to the speaker, while minimizing interference with visual data recording. Attention is warranted with respect to ambient noise, reverberation, and other sources of interference. If automatic transcription of speech with speaker diarization is needed, specific microphones for each speaker are advisable.Giventhe large file sizes associated with multimodal data collection, efficient data storage and management strategies are essential. Original recordings should use lossless compression formats (e.g. FFV1, FLAC, WAV) to preserve data quality. However, for longterm storage, accessibility, and sharing, widely supported formats such as MP4 with H.264 or H.265 compression are recommended. Large datasets should be stored in secure repositories with proper metadata documentation to facilitate reproducibility. Technical metadata should include detailed information about the recording devices used, specifying exact models, firmware/software versions, and hardware configurations (e.g., camera sensor specifications, frame rates, shutter speeds, microphone sensitivity, sampling rates). Additionally, precise settings for each device, such as resolution (e.g., HD, 4K, or 6K), compression format (e.g., lossless FFV1 or H.264/H.265 compression), audio encoding standards, and synchronization methods (e.g., embedded timestamps, external trigger signals) should be documented. Information about the physical environment, such as lighting setup, background color and material (particularly for chroma keying), camera and microphone placement, as well as distances and angles relative to participants, should also be included. Further, metadata should clearly describe data management procedures, including file naming conventions, storage formats, version histories, and post-processing steps.


FigureA
Flexible Decision Framework for Multimodal Data Collection Note. The figure illustrates how researchers can move from defining their research scope and hypotheses to selection of participants and methods, thinking about technical setup, and ultimately deciding about a data management plan. The framework highlights key decision points including equipment choice, feasibility checks, synchronization, and ethical considerations. The arrows illustrate interdependence, that is a choice in one domain may affect the available options in other domains. For example, for the analysis of subtle facial expressions high-resolution cameras with higher frame rates are needed. This choice increases the demand for data storage and sophisticated synchronization protocols (especially when multiple cameras are used), while also necessitating enhanced privacy measures and more detailed informed consent procedures.


The proper use of headphones can further be pretested by playing sounds to the left and right ear respectively.


Deaf is capitalized here to recognize the Deaf community as a distinct cultural and linguistic group, while deaf refers to the general medical condition of hearing loss.








Acknowledgements
This collaboration was supported by the German Research Foundation (DFG) priority program SPP 2392 on "Visual Communication" (ViCom).






 










Gesture for Linguists: A Handy Primer




N
Abner






K
Cooperrider






S
Goldin-Meadow








Language and Linguistics Compass




9


11


















10.1111/lnc3.12168














Sequential and network analyses to describe multiple signal use in captive mangabeys




J
Aychet






C
Blois-Heulin






A
Lemasson








Animal Behaviour




182


















10.1016/j.anbehav.2021.09.005














When words meet signs: A corpus-based study on variation of mouthing in Russian Sign Language




A
Bauer








Linguistische Beiträge zur Slavistik


A. Bauer & D. Bunčić




Peter Lang










Specimina philologiae Slavicae








Phonetic differences between affirmative and feedback head nods in German Sign Language (DGS): A pose estimation study




A
Bauer






A
Kuder






M
Schulder






J
Schepens




10.1371/journal.pone.0304040








PLOS ONE




19


5














New Insights Into Mouthings: Evidence From a Corpus-Based Study of Russian Sign Language




A
Bauer






M
Kyuseva




10.3389/fpsyg.2021.779958








Frontiers in Psychology




12














Nonverbal and social aspects of discourse in face-to-face interaction




J
B
Bavelas








Text -Interdisciplinary Journal for the Study of Discourse




10


1-2


















10.1515/text.1.1990.10.1-2.5














The Hands Are The Head of The Mouth. The Mouth as Articulator in Sign Languages




P
Boyes Braem






R
Sutton-Spence








Signum Press


Hamburg












A repertoire of German recurrent gestures with pragmatic functions




J
Bressem






C
Müller




10.1515/9783110302028.1575








Body-Language-Communication: An International Handbook on Multimodality in Human Interaction


C. Müller, A. Cienki, E. Fricke, S. Ladewig, D. McNeill, & J. Bressem




De Gruyter Mouton




38














OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields




Z
Cao






G
Hidalgo Martinez






T
Simon






S.-E
Wei






Y
A
Sheikh








IEEE Transactions on Pattern Analysis and Machine Intelligence




43


1


















10.1109/TPAMI.2019.2929257














The language instinct in extreme circumstances: The transition to tactile Italian Sign Language (LISt) by Deafblind signers




A
Checchetto






C
Geraci






C
Cecchetto






S
Zucchi




10.5334/gjgl.357








Glossa: A Journal of General Linguistics




3


1














A Multimodal Language Faculty: A Cognitive Framework for Communication




N
Cohn






J
Schilperoord








Bloomsbury Academic






1st ed.










10.5040/9781350404861














Definition of minimal contents of dataset for participation ('Intelligent Automatic Sign Language Translation




O
Crasborn






H
Morgan




















Comparing Social Skills of Children and Apes




F
B M
De Waal






C
Boesch






V
Horner






A
Whiten








Science




5863


















10.1126/science.319.5863.569c














Regression-based analysis of combined EEG and eyetracking data: Theory and applications




O
Dimigen






B
Ehinger




10.1167/jov.21.1.3








Journal of Vision




21


1














Feeling phonology: The conventionalization of phonology in protactile communities in the United States




T
Edwards






D
Brentari








Language




96


4


















10.1353/lan.2020.0063














Signed Language Corpora




J
Fenlon






Hochgesang




10.2307/j.ctv2rcnfhc




J. A.






Gallaudet University Press












Multicomponent and multisensory




M
Fröhlich






N
Bartolotta






C
Fryns






C
Wagner






L
Momon






M
Jaffrezic






T
Mitra Setia






M
A
Van Noordwijk






C
P
Van Schaik




















10.1038/s42003-021-02429-y








Biology




4


1














Multimodal communication and language origins: Integrating gestures and vocalizations




M
Fröhlich






C
Sievers






S
W
Townsend






T
Gruber






C
P
Van Schaik




10.1111/brv.12535








Biological Reviews




94


5




















A
Gadotti






L
Rocher






F
Houssiau






A.-M
Creţu






Y.-A
De Montjoye


















Anonymization: The imperfect science of using data while preserving privacy | Science Advances


10.1126/sciadv.adn7053








Science Advances




29


10












Gestures as a resource for the organization of mutual orientation




C
Goodwin




10.1515/semi.1986.62.1-2.29








62














A roadmap for technological innovation in multimodal communication research




A
Gregori






F
Amici






I
Brilmayer






A
Ćwiek






L
Fritzsche






S
Fuchs






A
Henlein






O
Herbort






F
Kügler






J
Lemanski






K
Liebal






A
Lücking






A
Mehler






K
Tien Nguyen






W
Pouw






P
Prieto






P
L
Rohrer






P
G
Sánchez-Ramón






M
Schulte-Rüther






C
I
Eiff








Computer Science (LNCS)




















10.1007/978-3-031-35748-0_30














Extending the Architecture of Language From a Multimodal Perspective




P
Hagoort






A
Özyürek








Topics in Cognitive Science




00


















10.1111/tops.12728














Creating Corpora: Data Collection




T
Hanke






J
Fenlon








Signed Language Corpora


J. Fenlon & J. A. Hochgesang




Gallaudet University Press
















DGS Corpus & Dicta-Sign: The Hamburg Studio Setup




T
Hanke






L
König






S
Wagner






S
Matthes










Proceedings of the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies


the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies


















Extending the Public DGS Corpus in Size and Depth




T
Hanke






M
Schulder






R
Konrad






E
Jahn




E. Efthimiou, S.-E. Fotinea, T. Hanke, J. A. Hochgesang, J
















Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives


Kristoffersen, & J. Mesch


the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives










European Language Resources Association (ELRA








Wild chimpanzees' use of single and combined vocal and gestural signals




C
Hobaiter






R
W
Byrne






K
Zuberbühler




10.1007/s00265-017-2325-1








Behavioral Ecology and Sociobiology




71


6


96














Multimodal Language Processing in Human Communication




J
Holler






S
C
Levinson








Trends in Cognitive Sciences




23


8


















10.1016/j.tics.2019.05.006














Ethical Issues Associated with Managing and Sharing Individual-Level Health Data




S
Kaur






P
Y
Cheah






J
Parker






M
Ali






V
Jonas






C
Muthuswamy






M
J
Saenz






T
C
Smith






K
Voo






Wright




10.1007/978-3-031-41804-4_7








Research Ethics in Epidemics and Pandemics: A Casebook


J. de Vries




Springer International Publishing










S. Bull,








What Does Embodied Interaction Tell Us About Grammar? Research on Language and Social Interaction




L
Keevallik








51
















10.1080/08351813.2018.1413887














Gesticulation and Speech: Two Aspects of the Process of Utterance




A
Kendon




M
















10.1515/9783110813098.207




The Relationship of Verbal and Nonverbal Communication


R. Key




De Gruyter Mouton


25














Gesture: Visible Action as Utterance




A
Kendon




10.1017/CBO9780511807572








Cambridge University Press












Reflections on the "gesture-first" hypothesis of language origins




A
Kendon




















10.3758/s13423-016-1117-3








Psychonomic Bulletin & Review




24


1














Turn-taking in human face-to-face interaction is multimodal: Gaze direction and manual gestures aid the coordination of A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 36 turn transitions




K
H
Kendrick






J
Holler






S
C
Levinson




10.1098/rstb.2021.0473








Philosophical Transactions of the Royal Society B: Biological Sciences




378


20210473


















R
Konrad






T
Hanke






A
Isard






M
Schulder






L
König






J
Bleicken






O
Böse


















Corpus à la carte -Improving Access to the Public DGS Corpus


E. Efthimiou, S.-E














T
Fotinea






J
A
Hanke






Hochgesang






Proceedings of the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources


J. Mesch, & M. Schulder


the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources










{ELRA Language Resources Association (ELRA) and the International Committee on Computational Linguistics








MY DGS -annotated. Public Corpus of German Sign Language




R
Konrad






T
Hanke






G
Langer






D
Blanck






J
Bleicken






I
Hofmann






O
Jeziorski






L
König






S
König






R
Nishio






A
Regen






U
Salden






S
Wagner






S
Worseck






M
Schulder












3rd release (Version 3.0) [Dataset












Dgs-Korpus Project






Hamburg
Idgs






University




10.25592/dgs.corpus-3.0














Recurrent gestures




S
Ladewig








Body-Language-Communication: An International Handbook on Multimodality in Human Interaction


C. Müller, A. Cienki, E. Fricke, S. Ladewig, D. McNeill, & J. Bressem




De Gruyter Mouton




38














Recurrent Gestures: Cultural, Individual, and Linguistic Dimensions of Meaning-Making




S
H
Ladewig




10.1017/9781108638869.003




A. Cienki






Cambridge University Press








The Cambridge Handbook of Gesture Studies








How a yoga pose in an online tutorial takes on meaning as felt sensation




S
H
Ladewig






D
Horst




10.1075/pbns.348.07lad








Media as Procedures of Communication


M. Luginbühl & J. G. Schneider




John Benjamins Publishing Company










Media as processes of doing and perceiving












A
Framework






Multimodal






Collection














Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects




D
Lahat






T
Adali






C
Jutten




10.1109/JPROC.2015.2460697








Proceedings of the IEEE


the IEEE






103








Proceedings of the IEEE








Multimodal communication by captive chimpanzees (Pan troglodytes)




D
A
Leavens






J
L
Russell






W
D
Hopkins








Animal Cognition




13


1


















10.1007/s10071-009-0242-z














Primate Communication: A Multimodal Approach




K
Liebal






B
M
Waller






A
M
Burrows






K
E
Slocombe








Cambridge University Press














10.1017/CBO9781139018111














Tiered Access to Research Data for Secondary Analysis




J
E
Marcotte






S
Rush






K
Ogden-Schuette




10.29012/jpc.825








Journal of Privacy and Confidentiality




13


2














Hand and mind: What gestures reveal about thought (pp. xi, 416)




D
Mcneill








University of Chicago Press












Female putty-nosed monkeys (Cercopithecus nictitans) vocally recruit males for predator defence




F
G
Mehon






C
Stephan




10.1098/rsos.202135








Royal Society Open Science




8


3














Meaning-making in tactile cross-signing context




J
Mesch






E
Raanes




10.1016/j.pragma.2022.12.018








Journal of Pragmatics




205
















Practical Tips for Ethical Data Sharing




M
N
Meyer








Advances in Methods and Practices in Psychological Science




1


1


















10.1177/2515245917747656














Ten Simple Rules for Creating a Good Data Management Plan




W
K
Michener




10.1371/journal.pcbi.1004525








PLOS Computational Biology




11


10






C.E.).








Vocal-visual combinations in wild chimpanzees




J
G
Mine






C
Wilke






C
Zulberti






M
Behjati






A
B
Bosshard






S
Stoll






Z
P
Machanda






A
Manser






K
E
Slocombe






S
W
Townsend




10.1007/s00265-024-03523-x








Behavioral Ecology and Sociobiology




78


10


108














One way of working with speech and gesture data: Methodology for multimodality




I
Mittelberg




10.1075/hcp.18.16mit








Methods in Cognitive Linguistics


M. Gonzalez-Marquez, I. Mittelberg, S. Coulson, & M. J. Spivey




John Benjamins Publishing Company
















The visual-gestural modality and beyond: Mouthings as a language contact phenomenon in Irish Sign Language




S
Mohr








Sign Language & Linguistics




15


2


















10.1075/sll.15.2.01moh














Challenges of multimodality: Language and the body in social interaction




L
Mondada




10.1111/josl.1_12177








Journal of Sociolinguistics




20


3
















How recurrent gestures mean: Conventionalized contexts-of-use and embodied motivation




C
Müller




10.1075/gest.16.2.05mul








Gesture




16


2




















C
Müller






A
Cienki






E
Fricke






S
Ladewig






D
Mcneill




& Teßendorf, S.


















Body-Language




An International Handbook on Multimodality in Human Interaction




De Gruyter Mouton


38














10.1515/9783110261318


















C
Müller






A
Cienki






E
Fricke






S
Ladewig






D
Mcneill




& Teßendorf, S.


















Body-Language




An International Handbook on Multimodality in Human Interaction




De Gruyter Mouton


38












Cinematic Metaphor: Experience -Affectivity -Temporality




C
Müller






H
Kappelhoff




10.1515/9783110580785








De Gruyter












Metaphors for sensorimotor experiences. Gestures as embodied and dynamic conceptualizations of balance in dance lessons




C
Müller






S
Ladewig








Language and the Creative Mind


M. Borkent, B. Dancygier, & J. Hinnell






















A
Framework






Multimodal






Collection






39












Aligning and Synchronizing Multi-Source Video and Audio Recordings




H
Nalbantoğlu






Š
Kadavá
























R
Nishio






S.-E
Hong






S
König






R
Konrad






G
Langer






T
Hanke






C
Rathmann




















Proceedings of the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies


P. Dreuw, E. Efthimiou, T. Hanke, T. Johnston, G. Martínez Ruiz, & A. Schembri


the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies










European Language Resources Association (ELRA)








Multi-speaker experimental designs: Methodological considerations




T
Offrede






S
Fuchs






C
Mooshammer




10.1111/lnc3.12443








Language and Linguistics Compass




15


12
















E
Orfanidou






B
Woll




10.1002/9781118346013




Research Methods in Sign Language Studies: A Practical Guide


Morgan, G.




Wiley








1st ed.








Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information




B
Owoyele






J
Trujillo






G
De Melo






W
Pouw




10.1016/j.softx.2022.101236








SoftwareX




20














Considering the nature of multimodal language from a crosslinguistic perspective




A
Özyürek




10.5334/joc.165








Journal of Cognition




4


1














Making Research Data Repositories Visible: The re3data.org Registry




H
Pampel






P
Vierkant






F
Scholze






R
Bertelmann






M
Kindling






J
Klump






H.-J
Goebelbecker






J
Gundlach






P
Schirmbacher






U
Dierolf




10.1371/journal.pone.0078080








PLOS ONE




8


11














Collecting and Analyzing Sign Language Data




P
Perniss




10.1002/9781118346013.ch4








Research Methods in Sign Language Studies




John Wiley & Sons
















Why We Should Study Multimodal Language




P
Perniss




10.3389/fpsyg.2018.01109








Frontiers in Psychology




9














Sign Language: An International Handbook




R
Pfau






M
Steinbach






Woll




10.1515/9783110261325




B.






De Gruyter












DGS Corpus Project -Development of a Corpus Based Electronic Dictionary German Sign Language / German




S
Prillwitz






T
Hanke






S
König






R
Konrad






G
Langer






A
Schwarz










Proceedings of the LREC2008 3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora


the LREC2008 3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora


















The multimodal nature of communicative efficiency in social interaction




M
Rasenberg






W
Pouw






A
Özyürek






M
Dingemanse




10.1038/s41598-022-22883-w








Scientific Reports




12


1














Redefining Multimodality. Frontiers in Communication




W
Sandler




10.3389/fcomm.2021.758993


















Speech and sign: The whole human language




W
Sandler




10.1515/tl-2024-2008








Theoretical Linguistics




50


1-2
















Sign Language and Linguistic Universals




W
Sandler






D
Lillo-Martin




10.1017/CBO9781139163910








Cambridge University Press














M
Schulder






D
Blanck






T
Hanke






I
Hofmann






S.-E
Hong






O
Jeziorski






L
König






S
König






R
Konrad






G
Langer






R
Nishio






C
Rathmann




10.25592/uhhfdm.14231




Data Statement for the Public DGS Corpus


















M
Schulder






T
Hanke




10.25592/uhhfdm.1866




OpenPose in the Public DGS Corpus (Project Note AP06-2019-01; Project Notes of the DGS Korpus Project






16






IDGS, Hamburg University






DGS-Korpus project








How to be FAIR when you CARE: The DGS Corpus as a Case Study of Open Science Resources for Minority Languages




M
Schulder






T
Hanke




N. Calzolari, F




















P
Béchet






K
Blache






C
Choukri






T
Cieri






S
Declerck






H
Goggi






B
A
Isahara






Framework For Multimodal






Data






41














J
Maegaard






H
Mariani






Mazo






Proceedings of the Thirteenth Language Resources and Evaluation Conference


J. Odijk, & S. Piperidis


the Thirteenth Language Resources and Evaluation Conference










European Language Resources Association








A database for lexical signs of DGS (German Sign Language) and corresponding gestures: transparency, iconicity and gesture-sign overlap




D
Spruijt






P
B
Schumacher






P
Perniss








Open Science Framework
















Modality-Independent Core Brain Network for Language as Proved by Sign Language




P
C
Trettenbrein






N.-K
Meister






K
Slivac






T
A
Finkbeiner






M
Steinbach






A
D
Friederici






E
Zaccarella




10.31234/osf.io/hnzgp


















Controlling Video Stimuli in Sign Language and Gesture Research: The OpenPoseR Package for Analyzing OpenPose Motion-Tracking Data in R




P
C
Trettenbrein






E
Zaccarella




10.3389/fpsyg.2021.628728








Frontiers in Psychology




12














Functional and structural brain asymmetries in sign language processing




P
C
Trettenbrein






E
Zaccarella






A
D
Friederici




10.1016/B978-0-443-15646-5.00021-X








Handbook of Clinical Neurology


C. Papagno & P. Corballis




208






Elsevier












A Crossspecies Comparison of Facial Morphology and Movement in Humans and Chimpanzees Using the Facial Action Coding System (FACS)




S.-J
Vick






B
M
Waller






L
A
Parr






M
C
Smith Pasqualini






K
A
Bard




10.1007/s10919-006-0017-z








Journal of Nonverbal Behavior




31


1
















The Neural Representation of Abstract Words: The Role of Emotion




G
Vigliocco






S.-T
Kousta






P
A
Della Rosa






D
P
Vinson






M
Tettamanti






J
T
Devlin






S
F
Cappa








Cerebral Cortex




24


7


















10.1093/cercor/bht025














Gesture and speech in interaction: An overview




P
Wagner






Z
Malisz






S
Kopp




10.1016/j.specom.2013.09.008








Speech Communication




57
















Macaques can predict social outcomes from facial expressions




B
M
Waller






J
Whitehouse






J
Micheletta








Animal Cognition




19


5


















10.1007/s10071-016-0992-3














Blind alleys and fruitful pathways in the comparative study of cultural cognition




A
Whiten








Physics of Life Reviews




43


















10.1016/j.plrev.2022.10.003














Production of and responses to unimodal and multimodal signals in wild chimpanzees, Pan troglodytes schweinfurthii




C
Wilke






E
Kavanagh






E
Donnellan






B
M
Waller






Z
P
Machanda






K
E
Slocombe








Animal Behaviour




123


















10.1016/j.anbehav.2016.10.024














The FAIR Guiding Principles for scientific data management and stewardship




M
D
Wilkinson






M
Dumontier






Ij
J
Aalbersberg






G
Appleton






M
Axton






A
Baak






N
Blomberg






J.-W
Boiten






L
B
Da Silva Santos






P
E
Bourne






J
Bouwman






A
J
Brookes






T
Clark






M
Crosas






I
Dillo






O
Dumon






S
Edmunds






C
T
Evelo






R
Finkers






B
Mons




10.1038/sdata.2016.18








Scientific Data




3


1














ELAN: A Professional Framework for Multimodality Research




P
Wittenburg






H
Brugman






A
Russel






A
Klassmann






H
Sloetjes










Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06)


the Fifth International Conference on Language Resources and Evaluation (LREC'06)


















ChimpLASG: a form-based approach to the classification of chimpanzee gestures




C
Zulberti






F
Amici






J
Bressem






S
Ladewig






L
Oña






K
Liebal




10.1163/14219980-950101AB








Folia Primatologica




95

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]