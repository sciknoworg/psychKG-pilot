You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



averaging over all previous experiences 
(4,
8)
 and Bayesian updating 
(9)
. In this prior work, learning of environment quality is foregrounded while knowledge of environment structure is assumed. In a homogeneous environment, as is nearly universally employed in these experiments, this is a reasonable assumption as a single experience in a patch can be broadly generalized from across other patches. However, it may be less reasonable in more naturalistic heterogeneous environments with regional variation in richness. To make accurate predictions within a local patch, the forager must learn the heterogeneous structure of the broader environment. How might they rationally do so?
Here, we show that apparent overharvesting in these tasks can be explained by combining structure learning with adaptive planning, a combination of mechanisms with potentially broad applications to many complex behaviors performed by humans, animals, and artificial agents 
(10)
.
We formalize this combination of mechanisms in a computational model. For the structure learning mechanism, we use an infinite capacity mixture model 
(11,
12)
, and for the adaptive planning mechanism, we use a dynamically adjusting, uncertainty sensitive discounting factor 
(13)
. The infinite capacity mixture model assumes that the forager treats structure learning as a categorization problem -one in which they must discover not only a patch's type, or cluster, but also how many patch types varying in richness there are in the environment. And, they must infer these features only based on rewards received. The number of patch types inferred is dependent both on the forager's experience and their prior expectation of environment complexity (i.e. the number of patch types) represented by the parameter, α. In a heterogeneous environment with regions varying in richness, allowing for the possibility of multiple patch types enables better predictions of future rewards 
(Fig. 1AB
).
Unless the forager has strong prior assumptions that there is a single patch type, they will be uncertain regarding their categorization of patches. A rational decision-maker should account for this uncertainty. We therefore adjusted the discount factor on each choice proportionally, capturing the suggestion that it is optimal for a decision-maker using a mental model of the world to set their planning horizon only as far as is justified by their model certainty 
(13)
. We implemented this principle by setting the effective discount factor on each choice to be a linear function of the representational uncertainty, with intercept (γ base ) and slope (γ coef ) terms fit to each participant 
(Fig. 1CD
). This formulation allowed us to test the nested null hypothesis that discount factors would not be sensitive to the agent's fluctuating representational uncertainty.
We tested the model's predictions with a novel variant of a serial stay-switch task ( 
Fig. 2A; (4, 14)
). Participants visited different planets to mine for "space treasure" and were tasked to collect as much space treasure as possible over the course of a fixed length game. On each trial, they had to decide between clusters (α > 0), they learn with experience the cluster-unique decay rates. Initially, the forager is highly uncertain of their predictions. However, with more visitations to different planets, the agent makes increasingly accurate and precise predictions. B.
Without structure learning If the forager's prior assumes a single cluster (α = 0), the forager makes inaccurate and imprecise predictions -either over or underestimating the upcoming decay, depending on the planet type. This inaccuracy persists even with experience because of the strong initial assumption. Uncertainty adaptive discounting. C. High uncertainty When clusters are similarly probable, the posterior entropy is high. This entropy is taken as the forager's internal uncertainty and is used to adjust their discounting rate, γ ef f ective . When uncertainty is high, they discount future value more heavily. D. Low uncertainty When one cluster is much more likely than the others, entropy or uncertainty is low and consequently, future value is discounted less heavily.


Results


Model-free analyses.
Participants adapt to local richness. We first examined a prediction of MVT -foragers should adjust their patch leaving to the richness of the local patch. In the task environment, planets varied in their richness or how quickly they depleted. Slower depletion causes the local reward rate to more slowly approach the global reward rate of the environment. Thus, MVT predicts that stay times should increase as depletion rates slow. As predicted, participants stayed longer on rich planets relative to neutral (t(115) = 19.77, p < .0001) and longer on neutral relative to poor (t(115) = 12.57, p < .0001).
Experience decreases overharvesting. Despite modulating stay times in the direction prescribed by MVT, participants stayed longer or overharvested relative to MVT when averaging across all planets (t(115) = 3.88, p = .00018). However, the degree of overharvesting diminished with experience. Participants overharvested more in the first two blocks relative to the final two (t(115) = 3.27, p = .0014). Our definition of MVT assumes perfect knowledge of the environment. Thus, participants approaching the MVT optimum with experience is consistent with learning the environment's structure and dynamics.
Local richness modulates overharvesting. We next considered how participants' overharvesting varied with planet type. As a group, participants overharvested only on poor and neutral planets while behaving MVT optimally on rich planets ( 
Fig. 3A
; poor -t(115) = 6.92, p < .0001; neutral -t(115) = 9.00, p < .0001; rich -t(115) = 1.38, p = .17).


Environment dynamics modulate decision time and overharvesting.
We also asked how participants adapted their foraging strategy to the environment's dynamics or transition structure. Upon leaving a planet, it was more common to transition to a planet of the same type (80%, "no switch") than transition to a planet of a different type ("switch"). Thus, we reasoned that switch transitions should be points of maximal surprise and uncertainty given their rareness. However, this would only be the case if the participant could discriminate between planet types and learned the transition structure between them.
If surprised, a participant should take longer to make a choice following a rare "switch" transition. So, we next examined participants' reaction times (z-scored and logtransformed) for the decision following the first depletion on a planet. We compared when there was a switch in planet type versus where there was none. As predicted, participants showed longer decision times following a "switch" transition suggesting they were sensitive to the environment's structure and dynamics ( 
Fig. 3B
; t(115) = 2.65, p = 0.0093).
If uncertain, our adaptive discounting model predicts that participants should discount remote rewards more heavily and, consequently, overharvest to a greater extent. To test this, we compared participants overharvesting following rare "switch" transitions to their overharvesting following the more common "no switch" transitions. Following the model's prediction, participants marginally overharvested more following a change in planet type (t(115) = 1.86, p = 0.065). When considering only planets that participants overharvested on on average (poor and neutral), overharvesting was significantly greater following a change ( 
Fig. 3C
; t(115) = 4.67, p < .0001). Participants traveled to different planets and mined for space gems across 5 6-minute blocks. On each trial, they had to decide between staying to dig from a depleting gem mine or incurring a time cost to travel to a new planet. B. Environment structure. Planets varied in their richness or, more specifically, the rate at which they exponentially decayed with each dig. There were three planet types -poor, neutral, and rich -each with their own characteristic distribution over decay rates. C. Environment dynamics. Planets of a similar type clustered together. A new planet had an 80% probability of being the same type as the prior planet ("no switch"). However, there was a 20% probability of transitioning or "switching" to a planet of a different type.    in 90% of simulations. 76% of participants were above this threshold. Thus, most participants were determined to be "structure learners" using our criteria.
The threshold for uncertainty-adaptive discounting was assumed to be 0. A majority of participants, 93%, were above this threshold. These participants were determined to be "adaptive discounters", those who dynamically modulated their discounting factor in accordance with their internal uncertainty.
We next looked for relationships between parameters. Uncertainty should be greatest for individuals who have prior expectations that do not match the environment's true structure, whether too complex or too simple. Consistent with this, there was a non-monotonic relationship between the structure learning and discounting parameters. γ base and γ coef were greatest when α was near its lower bound, 0, and upper bound, 10 (γ base : β = 0.080, p < .0001; γ coef : β = 0.021, p < .0001). An individual's base level discounting constrains the range over which uncertainty can adapt the effective discounting. Reflecting this, the two discounting parameters were positively related to one another (τ = -0.33, p < .0001).
Parameter validation. Correlations with model-free measures of task behavior confirmed the validity of the model's parameters. We interpret α as reflecting an individual's prior expectation of environment complexity. α must reach a certain threshold to produce inference of multiple clusters and consequently, sensitivity to the transitions between clusters. Validating this interpretation, participants with higher fit α demonstrated greater switch costs between planet types (Kendall's τ = 0.17, p = 0.00076). Moreover, this relationship was specific to α. γ base and γ coef were not significantly correlated with switch cost behavior (γ base : τ = -0.036, p = .57; γ coef : τ = -0.10, p = 0.11). This is a particularly strong validation as the model was not fit to reaction time data. Validating γ coef as reflecting uncertainty-adaptive discounting, the parameter was If a participant has knowledge of the environment's planet types and the transition structure between them, then they should be surprised following a rare transition to a different type. Consequently, they should take longer to decide following these transitions. As predicted, participants spent longer making a decision following transitions to different types ("switch") relative to when there was transition to a planet of the same type ("no switch"). This is consistent with having knowledge of the environment's structure and dynamics. C. Overharvesting increases following rare switch transitions. On poor and neutral planets, participants overhavested to a greater extent following a rare "switch" transition relative to when there was a "no switch" transition. This is consistent with uncertainty adaptive discounting. Switches to different planet types should be points of greater uncertainty. This greater uncertainty produces heavier discounting and in turn staying longer with the current option.*p < 0.05, **p < 0.01, ***p < 0.001 correlated with the extent overharvesting increased following In standard economic choice tasks, humans have been shown to act in accordance with rational statistical inference of environment structure. Furthermore, by assuming humans must learn the structure of their environment from experience, seemingly suboptimal behaviors can be rationalized including prolonged exploration 
(16)
, melioration 
(17)
, social biases 
(18)
, and overgeneralization 
(19)
. Here, we extend this proposal to decision tasks with sequential dependencies, which require simultaneous learning and dynamic integration of both the distribution of immediately available rewards and the underlying contingencies that dictate future outcomes. This form of relational or category learning has long been associated with distinct cognitive processes and neural substrates from those thought to underlie reward-guided decisions 
(20)
, including the foraging decisions we investigate here 
(21)
. However, a network of neural regions overlapping those supporting relational learning are more recently thought to play a role in deliberative, goal-directed decisions 
(22,
23)
.
If foragers are learning a model of the environment and using it to make decisions for reward, then this suggests that they may be doing something like model-based reinforcement learning (RL). Seemingly contrary to this, Constantino & Daw (4) found human foragers' choices to be better explained by a MVT model augmented with a learning rule than a standard reinforcement learning model. However, it is important to note that the task environment in that study was homogeneous and the RL model tested was model-free (temporal-difference learning). Thus, the difference in our results could be due to different task environments and class of models. A key way our model deviates from a model-based RL approach is that prospective prediction is only applied in computing the value of staying while the value of leaving is similar to MVT's threshold for leaving -albeit discounted proportionally to the mechanism for how the factor is set let alone dynamically 343 adjusted with experience. In contrast, our model proposes a mechanism through which the discounting factor is rationally set in response to both the external and internal environment.
Finally, our observation that humans adjust their planning horizons dynamically in response to state-space uncertainty may have practical applications in multiple fields. In psychiatry, foraging has been proposed as a translational framework for understanding how altered decision-making mechanisms contribute to psychiatric disorders 
(30)
. A substantial body of work has examined how temporal discounting is impacted in a range of disorders from substance use disorder (31) to depression 
(32)
 to schizophrenia 
(33)
. This wide range has led some to suggest that temporal discounting may be a useful transdiagnostic symptom and a potential target for treatment 
(34)
. However, it remains unclear why temporal discounting is altered in these disorders and how it relates to known risk factors like an unpredictable early life environment. Our findings may provide further insight, suggesting that it may be related to the individual's internal uncertainty over environment structure, and hence may vary across contexts, rather than being stable and trait-like. Potential treatments, rather than targeting temporal discounting, could address its possible upstream cause of uncertainty -increasing the individual's perceived familiarity with the current context or increasing their self-perceived ability to act efficaciously in it. Another application could be in the field of sustainable resource management, where it has recently been shown that, in common pool resource settings (e.g. waterways, grazing fields, fisheries), the distribution of individual participants' planning horizons strongly determines whether resources are sustainably managed 
(35)
. Here, we show that discount factor, set as a rational response to uncertainty about environmental Participation was restricted to workers who had completed at least 100 prior studies and had at least a 99% approval rate. Participants earned $6 as a base payment and could earn a bonus contingent on performance ($0-$4). We excluded 60 participants according to one or more of three criteria: 1.
having average planet residence times 2 standard deviations above or below the group mean (36 participants) 2. failing a quiz on the task instructions more than 2 times (33 participants) or 3. failing to respond appropriately to one or more of the two catch trials (17 participants). On catch trials, participants were asked to press the letter "Z" on their keyboard. These questions were meant to "catch" any participants repeatedly choosing the same option (using key presses "A" or "L") independent of value.
Task Design. Participants completed a serial stay-switch task adapted from previous human foraging studies 
(4,
36)
. Following this initial dig, participants had to decide between staying on the current planet to dig again or leaving to travel to a new planet 
(Fig 2A)
. Staying would further deplete the gem mine while leaving yielded a replenished gem mine at the cost of a longer time delay. They made these decisions in a series of five blocks, each with a fixed length of 6 minutes.
Blocks were separated by a break of participant-controlled length, up to a maximum of 1 minute.
On each trial, participants had 2 seconds to decide via key press whether to stay ("A") or leave ("L"). If they decided to stay, they experienced a short delay before the gem amount was displayed (1. Unlike previous variants of this task, planets varied in their richness within and across blocks, introducing greater structure to the task environment. Richness was determined by the rate at which the gem amount exponentially decayed with each successive dig 
(Fig. 2B
). If a planet was "poor", there was steep depletion in the amount of gems received. Specifically, its 435 decay rates were sampled from a beta distribution with a low 436 mean (mean = 0.2; sd = 0.05; α = 13 and β = 51). In contrast, 437 rich planets depleted more slowly (mean = 0.8; sd = 0.05; α 438 = 50 and β = 12). Finally, the quality of the third planet 439 type -neutral -fell in between rich and poor (mean = 0.5; 440 sd = 0.05; α = 50 and β = 50). The environment dynamics 441 were designed such that planet richness was correlated in time. 442 When traveling to a new planet, there was an 80% probability 443 of it being the same type as the prior planet ("no switch"). If 444 not of the same type, it was equally likely to be of one of the 445 remaining two types 
("switch", Fig. 2C
). This information was 446 not communicated to participants, requiring them to infer the 447 environment's structure and dynamics from rewards received 448 alone.


449
Comparison to Marginal Value Theorem. Participants' planet 450 residence times, or PRTs, were compared to those prescribed 451 by MVT. Under MVT, agents are generally assumed to act 452 as though they have accurate and complete knowledge of the 453 environment. For this task, that would include knowing each 454 planet type's unique decay rate distribution and the total 455 reward received and time elapsed across the environment.


456
Knowledge of the decay rate distributions is critical for 457 estimating Vstay, the anticipated reward if the agent were to 458 stay and dig again.
459 Vstay = rt * d [1] 460
where rt is the reward received on the last dig and d is the 461 upcoming decay. 
V leave = r total t total * t dig [2] 467
r total t total estimates the average reward rate of the environment. 468 Multiplying it by t dig gives the opportunity cost of the time 469 spent exploiting the current planet.


470
Finally, to make a decision, the MVT agent compares the 471 two values and acts greedily, always taking the higher valued 472 option.
473 choice = argmax(Vstay, V leave ) [3] 474
Model.


475
Making the stay-leave decisions. We assume that the forager com-476 pares the value for staying, Vstay, to the value of leaving V leave , 477 to make their decision. Similar to MVT, we assume foragers 478 act greedily with respect to these values.
experiences in the environment 
(4)
. This may be reasonable in homogeneous environments but less so in heterogeneous ones where it could introduce substantial noise and uncertainty. Instead, in these varied environments, it may be more reasonable to cluster patches based on similarity and only generalize from patches belonging to the same cluster as the current one. This selectivity enables more precise predictions of future outcomes.
Clusters are latent constructs. Thus, it is not clear how many clusters a forager should divide past encounters into.
Non-parametric Bayesian methods provide a potential solution to this problem. They allow for the complexity of the representation -as measured by the number of clusters -to grow freely as experience accumulates. These methods have been previously used to explain phenomena in category learning 
(11,
37)
, task set learning 
(19)
, fear conditioning 
(12)
, and event segmentation 
(18)
.
To initiate this clustering process, the forager must assume a model of how their observations, decay rates, are generated by the environment. The generative model we ascribe to the forager is as follows. Each planet belongs to some cluster, and each cluster is defined by a unique decay rate distribution:
d k ∼ N ormal(µ k , σ k ) [4]
where k denotes cluster number. The generative model takes the form of a mixture model in which normal distributions are mixed together according to some distribution P (k) and
observations are generated from sampling from the distribution P (d|k).
Before experiencing any decay on a planet, the forager has prior expectations regarding the likelihood of a planet belonging to a certain cluster. We assume that the prior on clustering corresponds to a "Chinese restaurant process" 
(38)
.
If previous planets are clustered according to p1:N , then for the current planet: planets and the likelihood of the observations given those clus-540 terings. Thus, we approximate the posterior distribution using 541 a particle filter 
(39)
. Each particle maintains a hypothetical 542 clustering of planets which are weighted by the likelihood of 543 the data under the particle's chosen clustering. All simulations 544 and fitting were done with 1 particle which is equivalent to 545 Anderson's local MAP algorithm 
(40)
.
P (k) = n k N +α if k is old


546
With 1 particle, we assign a planet definitively to a cluster. 547 This posterior then determines (a) which cluster's parameters 548 are updated and (b) the inferred cluster on subsequent planet 549 encounters.


550
If the planet is assigned to an old cluster, k, the existing µ k 551 and σ k are updated analytically using the standard equations 552 for computing the posterior for a normal distribution with 553 unknown mean and variance:
554 d = 1 n n i=1 di µ ′ 0 = n0µ0 + nd n0 + n n ′ 0 = n0 + n ν ′ 0 = ν0 + n ν ′ 0 σ 2 0 ′ = ν0σ 2 0 + n i=1 (di −d) 2 + n0n n0 + n (µ0 −d) 2 [6] 555
where d is a decay observed on the current planet, n is the 556 total number of decays observed on the current planet, n0 is 557 the total number of decays observed across the environment 558 before the current planet, µ0 is the prior mean of the cluster-559 specific decay rate distribution and ν0 is its precision. µ  This initial distribution is updated with the depletions 565 encountered on the current planet upon leaving.


566
The goal of this learning and inference process is to support 567 accurate prediction. To generate a prediction of the next decay, 568 the forager samples a cluster according to P (k) or P (k|D) 569 depending on whether any depletions have been observed on 570 the current planet. Then, a decay rate is sampled from the 571 cluster specific distribution, d k . The forager averages over 572 these samples to produce the final prediction.


573
To demonstrate structure learning's utility for prediction, 574 we show in simulation the predicted decay rates on each planet 575 with structure learning 
(Fig. 1A)
 and without 
(Fig. 1B)
. With 576 structure learning, the forager's predictions approach the mean 577 decay rates of the true generative distributions. Without struc-578 ture learning, however, the forager is persistently inaccurate, 579 underestimating the decay rate on rich planets and overesti-580 mating it on poor planets.


581
Adapting the model of the environment. Because the inference pro-582 cess is an approximation and foragers' experience is limited, 583 their inferred environment structure may be inaccurate. Theo-584 retical work has suggested that a rational way to compensate 585 for this inaccuracy is to discount future values in proportion 586 to the agent's uncertainty over their representation of the 587 environment 
(13)
. We quantified an agent's uncertainty by 588 taking the entropy of the approximated posterior distribution over clusters ( 
Fig 1CD)
. We sample clusters 100 times proportional to the posterior. These samples are multinomially distributed. We represent them with the distribution, X: X ∼ M ultinomial(100, K) 
[8]
 sampling 100 times from the distribution, P (k) or P (k|d) depending on whether depletions on the planet have been observed. Uncertainty is quantified as the Shannon entropy of distribution X.
the value of leaving as follows: 
[9]
 γ ef f ective = 1 1 + e (−γ base +γ coef * H(X)) 
[10]
 where γ base and γ coef are free parameters and H(X) is the entropy of the distribution X. 
V leave = r total t total * t dig * γ ef f ective


620
The parameter configuration that produced the lowest MSE 621 on average was chosen as the best fitting for the individual.  
[11]
 TD-Learning The temporal difference (TD) agent learns a state-specific value of staying and digging, Q(s, dig) and a non-state specific value of leaving, Q(leave). The state, s is defined by the gem amounts offered on each dig. The state space is defined by binning the possible gems that could be earned from each dig. The bins are spaced are according to log(bj+1)log(bj) = log(k) where bj+1 and bj are the upper and lower bounds of the bins andd is the mean decay rate. This state space specification is taken from (4). We set bj+1 to 135 and bj to 0 as these were the true bounds on gems received per dig. We setk to 0.5 because this would be the mean decay rate if one were to average the depletions experienced over all planets. The agent compares the two values and makes their choice using a softmax policy. P (ai = dig) = 1 (1 + e (−c−β(Q i (s i ,dig)−Q i (leave))) ) Di ∼ Bernoulli(P (ai))
δi = r i τ i − ρi ρi+1 = ρi + (1 − (1 − α) τ i ) * δi
δi = ri + γτi(Di * Qi(si) + (1 − Di) * Qi(leave)) − Qi(si−1, ai−1)
Qi(si−1, ai−1) = Qi+1(si−1, ai−1) + α * δi 
[12]
 where c, α, β, γ are free parameters. c is a perseveration term, α is the learning rate, β is the softmax temperature, and γ is the temporal discounting factor. Cross Validation Each model's fit to the data was evaluated using a 10-fold cross validation procedure. For each participant, we shuffled their PRTs on all visited planets and split them into 10 separate training/test datasets. The best fitting parameters were those that minimized the sum of squared error (SSE) between the participant's PRT and the model's predicted PRT on each planet in the training set. Then, with the held out test dataset, the model was simulated with the best fitting parameters and the SSE was calculated between the participant's true PRT and the model's PRT. To compute the model's final cross validation score, we summed over the test SSE from each fold. Data sharing statement. All data, data analysis, and model fitting code will be deposited in a public GitHub repository which can be found at https://github.com/noraharhen/Harhen-Bornstein-2022-Overharvesting-as-Rational-Learning.


ACKNOWLEDGMENTS. This work was supported by NIMH
P50MH096889 and a NARSAD Young Investigator Award by the Brain and Behavior Research Foundation to AMB. NCH was supported by a National Defense Science and Engineering Graduate fellowship.
Fig. 1 .
1
Structure learning improves prediction accuracy. A. With structure learning A simulated agent's posterior probability over the upcoming decay rate on each planet is plotted. If the forager's prior allows for the possibility of multiple


Fig. 2 .
2
A. Serial stay-switch task.


Structure learning with adaptive discounting provide the best account of participant choice. To check the models' goodness of fit, we 173 asked whether the compared models could capture key be-174 havioral results found in participants' data. For each model and participant, we simulated an agent with the best fitting 176 parameters estimated for them under the given model. Only 177 the adaptive discounting model was able to account for over-178 harvesting when averaging across all planets (Fig. 4A, t(115) 179 = 9.03, p < .0001). The temporal-difference learning model 180 predicted MVT optimal choices on average (t(115) = 1.09, 181 p = .28) while the MVT learning model predicted underharvesting (t(115) = -7.17, p < .0001). These differences were 183 primarily driven by predicted behavior on the rich planets 184(Fig. 4B).185Model fit was also assessed at a more granular level (stay 186 times on individual planets) using 10-fold cross validation.


187
Comparing cross validation scores as a group, participants' 188 choices were best captured by the adaptive discounting model 189 (Fig. 4C; mean cross validation scores -adaptive discounting:190 16.55, TD: 22.47, MVT learn: 32.31). At the individual level, 191 64% of participants were best fit by the adaptive discounting


192model, 14% by TD, and 22% by MVT learn.


193
Adaptive discounting model parameter distribution. Because the 194 adaptive discounting model provided the best account of choice 195 for most participants, we examined the distribution of individ-196 uals' best fitting parameters for the model. Specifically, we 197 compared participants' estimated parameters to two thresh-198 olds. These thresholds were used to identify whether a par-199 ticipant 1) inferred and assigned planets to multiple clusters 200 and 2) adjusted their overharvesting in response to internal 201 uncertainty. 202 The threshold for multi-cluster inference, 0.8, was computed 203 by simulating the adaptive discounting model 100 times and 204 finding the lowest value that produced multi-cluster inference 205


Fig. 3 .
3
Model-free results A. Planet richness influences over and underharvesting behavior. Planet residence times (PRT) relative to Marginal Value Theorem's (MVT) prediction are plotted as the median (± one quartile) across participants. The grey line indicates the median while the white cross indicates the mean. Individuals' PRTs relative to MVT are plotted as shaded circles. In aggregate, participants overharvested on poor and neutral planets and acted MVT optimally on rich planets. B. Decision times are longer following rare switch transitions.


241a rare transition or "switch" between different planet types 242 (τ = 0.15, p = 0.016). This was not correlated with α nor 243 the baseline discounting factor γ base (α: τ = -0.011, p = .86;244 γ base : τ = 0.082, p = .20). 245 Discussion 246 While Marginal Value Theorem (MVT) provides an optimal 247 solution to patch leaving problems, organisms systematically 248 deviate from it, staying too long or overharvesting. A critical 249 assumption of MVT is that the forager has accurate and com-250 plete knowledge of the environment. Yet, this is often not the 251 case in real world contexts -the ones in which foraging be-252 haviors are likely to have adapted (15). We propose a model of 253 how foragers could rationally learn the structure of their envi-254 ronment and adapt their foraging decisions to it. In simulation, 255 we demonstrate how seemingly irrational overharvesting can 256 emerge as a byproduct of a rational dynamic learning process. 257 In a heterogeneous, multimodal environment, we compared 258 how well our structure learning model predicted participants' 259 choices relative to two other models -one implementing a 260 MVT choice rule with a fixed representation of the environ-261 ment and the other a standard temporal-difference learning 262 algorithm. Importantly, only our structure learning model 263 predicted overharvesting in this environment. Participants' 264 choices were most consistent with learning a representation of 265 the environment's structure through individual patch experi-266 ences. They leveraged this structured representation to inform 267 their strategy in multiple ways. One way determined the value 268 of staying. The representation was used to predict future 269 rewards from choosing to stay in a local patch. The other 270 modulated the value of leaving. Uncertainty over the accuracy 271 of the representation was used to set the discount factor over 272 future value. These results suggest that to explain foraging 273 as it occurs under naturalistic conditions optimal foraging 274 may need to provide an account of how the forager learns to 275 acquire accurate and complete knowledge of the environment, and how they adjust their strategy as their representation is refined with experience.


Fig. 4 .
4
Modeling results A. The adaptive discounting model predicts overharvesting. Averaging across all planets, only the adaptive discounting model predicts overharvesting while the temporal-difference learning model predicts MVT optimal behavior and the MVT learning model predicts underharvesting. This demonstrates that overharvesting, a seemingly suboptimal behavior, can emerge from principled statistical inference and adaptation. B. Model predictions diverge most on rich planets. Similar to participants, the greatest differences in behavior between the models occurred on rich planets. C. The adaptive discounting model provides the best account for participant choices. The adaptive discounting model had the lowest mean cross validation score indicating it provided the best account of participant choice at the group level.agent's internal uncertainty over their representation's accuracy. In the former respect, our model parallels the framework discussed by Kolling & Akam (10) to explain humans sensitivity to the gradient of reward rate change during foraging observed by Wittman et al (24). Given that computing the optimal exit threshold under a pure model-based strategy would be highly computationally expensive, Kolling & Akam (10)'s suggest pairing model-based patch evaluation with a modelfree, MVT-like exit threshold. Under their proposal, the agent 320 leaves once the local patch's average predicted reward rate 321 over n time steps in the future falls below the global reward 322 rate. We build on, formally test, and extend this proposal by 323 explicitly computing the representational uncertainty at each 324 trial and adjusting planning horizon accordingly. 325 While learning a model of the environment is beneficial, it 326 is also challenging and computationally costly. With limited 327 experience and computational noise, an inaccurate model of the 328 environment may be inferred. An inaccurate model, however, 329 can be counteracted by adapting certain computations. In this 330 way, lowering the temporal discounting factor acts as a form 331 of regularization or variance reduction (13, 25-28). Empirical 332 work has found humans appear to do something like this in 333 standard intertemporal choice tasks. Gershman & Bhui (29) 334 found evidence that individuals rationally set their temporal 335 discounting as a function of the imprecision or uncertainty of 336 their internal representations. Here, we found that humans 337 while foraging act similarly, overharvesting to a greater extent 338 at points of peak uncertainty. While temporal discounting has 339 been proposed as a mechanism of overharvesting previously (3-340 5), the discounting factor is usually treated as a fixed, subject-341 level parameter, inferred from choice. Thus, it provides no342


Fig. 5 .
5
Parameter distributions A. Participants learned the structure of the environment. Distribution of participants' priors over environment complexity, α. Each individual's parameter is shown relative to a baseline threshold, 0.8. This threshold is the lowest value that produced multi-cluster inference in simulation. Most participants (76%) fall above this threshold indicating a majority learned the environment's multi-cluster structure. B. Environment complexity parameters were positively related to reaction time sensitivity to transition frequency. An individual must infer multiple planet types to be sensitive to the transition structure between them. In terms of the model, this would correspond to having a sufficiently high environment complexity parameter. Validating this parameter, it was positively correlated with individual's modulation of reaction time following a rare transition to a different planet type. C. Participants adapted their discounting computations to their uncertainty over environment structure. Distribution of participant's uncertainty adaptation parameter, γ coef . Each individual's parameter is shown relative to a baseline of 0. A majority were above this threshold (93%) indicating most participants dynamically adjusted their discounting, increasing it when they experienced greater internal uncertainty. D. Uncertainty adaptation parameters were positively related to overharvesting sensitivity to transition frequency. If an individual increases their discounting to their internal uncertainty over environment structure, then they should discount more heavily following rare transitions and stay longer with the current option. Consistent with this, we found that the extent an individual increased their overharvesting following a rare transition was related to their uncertainty adaptation parameter.structure, directly impacts the degree to which an individual tends to (over)harvest their locally available resources. The present work suggests that policymakers and institution designers interested in producing sustainable resource management outcomes should focus on reducing uncertainty -about the contingencies of their actions, and the distribution of rewards that may result -for individuals directly affected by resource availability, thus allowing them to rationally respond with an increased planning horizon and improved outcomes for all participants.MethodsParticipants. We recruited 176 participants from Amazon Mechanical Turk (111 male, ages 23-64, Mean=39.79, SD=10.56).


With the goal of collecting as much space treasure as possible, participants traveled to different planets to mine for gems. Upon arrival to a new planet, they performed an initial dig and received an amount of gems sampled from a Gaussian distribution with a mean of 100 and standard deviation (SD) of 5.


5 s). The length of the delay was determined by the time the participant spent making their previous choice (2 -RT s). This ensured participants could not affect the environment reward rate via their response time. If they decided to leave, they encountered a longer time delay (10 s) after which they arrived on a new planet and were greeted by a new alien (5 s). On trials where a decision was not made within the allotted time (2 s), participants were shown a timeout message for two seconds.


planet is poor 0.5 if planet is neutral 0.8 if planet is rich 463 V leave is estimated using the total reward accumulated, 464 r total , total time passed in the environment, t total , and the 465 time delay to reward associated with staying and digging, t dig . 466


new Where n k is the number of planets assigned to cluster k, α is a clustering parameter, and N is the total number of planets encountered. The probability of a planet belonging to an old cluster is proportional to the number of planets already assigned to it. The probability of it belonging to a new cluster is proportional to α. Thus, α controls how dispersed the clusters are -the higher α is the more new cluster creation is encouraged. The ability to incrementally add clusters as experience warrants it makes the generative model an infinite capacity mixture model. After observing successive depletions on a planet, the forager computes the posterior probability of a planet belonging to a cluster: J is the number of clusters created up until the current planet, D is a vector of all the depletions observed on the current planet, and all probabilities are conditioned on prior cluster assignments of planets, p1:N . Exact computation of this posterior is computationally demanding as it requires tracking all possible clusterings of


′ 0 and 560 ν ′ 0
0
are the posterior mean and variance respectively.


561
If the planet is a assigned to a new cluster, then a new 562 cluster is initialized with the following distribution:563 dnew ∼ N ormal(µ = 0.5, σ = 0.5) [7] 564


Model fitting. We compared participant PRTs on each planet to those predicted by the model. A model's best fitting parameters were those that minimized the difference between the607 true participant's and simulated agent's PRTs. We considered 608 1000 possible sets of parameters generated by quasi-random 609 search using low-discrepancy Sobol sequences (41). Prior 610 work has demonstrated random and quasi-random search to 611 be more efficient than grid search (42) for parameter opti-612 mization. Quasi-random search is particularly efficient with 613 low-discrepancy sequence, more evenly covering the parameter 614 space relative to true random search. 615 Because cluster assignment is a stochastic process, the pre-616 dicted PRTs vary slightly with each simulation. Thus, for each 617 candidate parameter setting, we simulated the model 50 times 618 and averaged over the mean squared error (MSE) between 619 participant PRTs and model-predicted PRTs for each planet.


622
Model Comparison. We compared three models: the structure 623 learning and adaptive discounting model described above, a 624 temporal difference model previously applied in a foraging 625 context, and a MVT model that learns the mean decay rate 626 and global reward rate of the environment.627 MVT-Learning In this model, the agent learns a threshold 628 for leaving which is determined by the global reward rate, ρ (4). 629 ρ is learned with a simple delta rule with α as a learning rate 630 and taking into account the temporal delay accompanying an 631 action τ . The value of staying is d * ri where d is the predicted 632 decay and ri is the reward received on the last time step. The 633 value of leaving,V leave , is the opportunity cost of the time spent634 digging, ρ * t dig . The agent chooses an action using a softmax 635 policy with temperature parameter, β which determines how 636 precisely the agent represents the value difference between the 637 two options. 638 P (ai = dig) = 1 (1 + e (−c−β(d * r i −ρ * t dig )) )


Learning the structure of the environment. Learning the structure 480 of the environment affords more accurate and precise predic-481 tions which support better decision-making. Here, the forager 482 predicts how many gems they'll receive if they stay and dig 483 again and this determines the value of staying, Vstay. To gen-484 erate this prediction, a forager could aggregate over all past 485














Optimal foraging, the marginal value theorem. Theor




El Charnov








Popul. Biol




9
















Subjective costs drive overly patient foraging strategies in rats on an intertemporal foraging task




Am Wikenheiser






Stephens






Redish








Proc. Natl. Acad. Sci. U. S. A




110
















Rats value time differently on equivalent foraging and delay-discounting tasks




Ec Carter






Redish








J. Exp. Psychol. Gen




145
















Learning the opportunity cost of time in a patch-foraging task




Sm Constantino






Daw








Cogn. Affect. Behav. Neurosci




15
















Monkeys are more patient in a foraging task than in a standard intertemporal choice task




Tc Blanchard






Hayden








PLoS One




10


117057














Rats exhibit similar biases in foraging and intertemporal choice tasks




Ga Kane








Elife




8














Planning and acting in partially observable stochastic domains




Lp Kaelbling






Ml Littman






Cassandra








Artif. intelligence




101
















Biased belief updating and suboptimal choice in foraging decisions




N
Garrett






Daw








Nat. Commun




11


3417














Uncertainty drives deviations in normative foraging decision strategies




Zp Kilpatrick






Davidson






El Hady


















reinforcement?) learning to forage optimally




N
Kolling






Akam








Curr. Opin. Neurobiol




46
















A more rational model of categorization




A
N
Sanborn






Griffiths






Navarro


















Context, learning, and extinction




Sj Gershman






Y
Blei






Niv








Psychol. Rev




117
















The dependence of effective planning horizon on model accuracy




N
Jiang






Kulesza






Singh






Lewis




















From creatures of habit to goal-directed learners: Tracking the developmental emergence of model-based reinforcement learning




J
H
Decker






Otto






Daw






Hartley








Psychol. science




27
















Time discounting and time preference in animals: a critical review




By Hayden








Psychon. bulletin & review




23
















Structure learning in human sequential decision-making




De Acuña






Schrater








PLoS Comput. Biol




6


1001003














Melioration as rational choice: sequential decision making in uncertain environments




Cr Sims






Neth






Jacobs






Gray








Psychol. Rev




120
















Structuring memory through Inference-Based event segmentation




Ys Shin






Dubrow








Top. Cogn. Sci




13
















Cognitive control over learning: creating, clustering, and generalizing task-set structure




Age Collins






Frank








Psychol. Rev




120
















Interactive memory systems in the human brain




Ra Poldrack








Nature




414
















Neural mechanisms of foraging




N
Kolling






Behrens






Mars






Rushworth








Science




336
















Cortical and hippocampal correlates of deliberation during modelbased decisions for rewards in humans




Am Bornstein






Daw








PLoS computational biology




9


1003387














Hippocampal contributions to model-based planning and spatial memory




Om Vikbladh








Neuron




102
















Predictive decision making driven by multiple time-linked reward representations in the anterior cingulate cortex




Mk Wittmann








Nat. Commun




7


12327














Biasing approximate dynamic programming with a lower discount factor in Advances in Neural Information Processing Systems




M
Petrik






; D
Scherrer






D
Koller






Y
Schuurmans






Bengio






Bottou








Curran Associates, Inc


21












On overfitting and asymptotic bias in batch reinforcement learning with partial observability




V
Francois-Lavet






G
Rabusseau






Pineau






Ernst






Fonteneau








J. Artif. Intell. Res




65
















Using a logarithmic mapping to enable lower discount factors in reinforcement learning




M
H Van Seijen






Fatemi






Tavakoli


















Discount factor as a regularizer in reinforcement learning




R
Amit






Meir






Ciosek


















Rationally inattentive intertemporal choice




Sj Gershman






Bhui








Nat. Commun




11


3365














A primer on foraging and the Explore/Exploit Trade-Off for psychiatry research




Ma Addicott






Pearson






Sweitzer






Barack






Platt








Neuropsychopharmacology




42
















Steep delay discounting and addictive behavior: a meta-analysis of continuous associations




M
Amlung






Vedelago






Acker






Balodis






Mackillop








Addiction




112
















Temporal discounting in major depressive disorder




Pulcu








Psychol. Med




44
















Delay discounting in schizophrenia




Ea Heerey






Robinson






Mcmahon






Gold








Cogn. Neuropsychiatry




12
















Delay discounting as a transdiagnostic process in psychiatric disorders: A meta-analysis




M
Amlung








JAMA Psychiatry




76
















Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse




W
Barfuss






Donges






Vv Vasconcelos






Kurths






Levin








Proc. Natl. Acad. Sci




117
















Chronic and acute stress promote overexploitation in serial decision making




Jk Lenow






Constantino






Daw






Phelps








J. Neurosci




37
















Rational approximations to rational models: alternative algorithms for category learning




A
N
Sanborn






Griffiths






Navarro








Psychol. Rev




117
















Mixtures of dirichlet processes with applications to bayesian nonparametric problems




Ce Antoniak








Ann. Stat




2
















Particle filters for mixture models with an unknown number of components




P
Fearnhead








Stat. Comput




14
















The adaptive nature of human categorization




Jr Anderson








Psychol. Rev




98
















Distribution of points in a cube and approximate evaluation of integrals




Im Sobol








Zh. Vych. Mat. Mat. Fiz




7
















Random search for hyper-parameter optimization




J
Bergstra






Bengio























"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]