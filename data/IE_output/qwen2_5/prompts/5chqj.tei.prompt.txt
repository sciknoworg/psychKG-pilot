You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Theories of choice have long described the decisionmaking process as one where people evaluate and compare the individual options available to them over time 
(Weber & Johnson, 2009)
. Nearly all accounts of decisionmaking explicitly or implicitly rely on the assumption that each option under consideration enjoys a degree of support, and that choices are made by comparing these degrees of support against one another 
(Wald & Wolfowitz, 1949;
Link & Heath, 1975)
 or against a criterion metric like a level of aspiration or certainty 
(Siegel, 1957;
Payne et al., 1980)
. These option-specific theories of representation are now ubiquitous 
(Ratcliff et al., 2016;
Busemeyer et al., 2019)
 *Corresponding author Peter D. Kvam https://orcid.org/0000-0002-3195-8452, email: pkvam@ufl.edu. This work was supported by a SEED grant from the University of Florida Informatics Institute and a grant from the National Science Foundation (SES-2237119). All data and study materials are available on the Open Science Framework at osf.io/gwz3j. Some of these results were presented at the 2019 meeting of the Society for Mathematical Psychology.
and have formed the basis for investigations into the neural antecedents of decision making 
(Gold & Shadlen, 2007;
Schurger et al., 2012;
Mulder et al., 2014)
.
To make these predictions for choice and response times, diffusion and racing accumulator models assign an accumulator to each option 
(Vickers, 1970;
Smith & Vickers, 1988;
Brown & Heathcote, 2008;
Smith & Van Zandt, 2000;
Usher & McClelland, 2001;
Trueblood et al., 2014;
Heathcote & Matzke, 2022)
, or track the balance of evidence between two 
(Ratcliff & McKoon, 2008;
Baum & Veeravalli, 1994)
 or more options 
(Busemeyer & Townsend, 1993;
Krajbich & Rangel, 2011;
Tajima et al., 2019;
McMillen & Holmes, 2006)
. A decision is made when one of the accumulators or the balance of evidence exceeds a particular threshold. These ideas have carried over to decision neuroscience, where a great deal of neuroimaging work has gone into identifying neural correlates of model parameters 
(Basten et al., 2010;
Forstmann et al., 2016;
Bogacz, 2007)
, such as the values of accumulators 
(Purcell & Palmeri, 2017;
Schall, 2019)
 or the balance of evidence among available options 
(Gold & Shadlen, 2007;
Philiastides et al., 2006;
Brody & Hanks, 2016)
. Among the areas thought to represent this balance of evidence or accumulator values are lateral intraparietal 
[LIP]
 cortex 
(Shadlen & Newsome, 2001;
Churchland et al., 2008;
M. L. Platt & Glimcher, 1999)
, frontal eye fields 
[FEF]
 
(Hanes & Schall, 1996)
, superior colliculus 
(Ratcliff, 2003)
, subthalamic nucleus 
(Keuken et al., 2015)
 and prefrontal cortex 
(Chau et al., 2014)
. In most accounts, perceptual information from sensory areas is summarized in terms of a single decision variable that represents the posterior probability or relative value 
(Basten et al., 2010)
 of each choice option under consideration 
(Gold & Shadlen, 2007;
Meyniel et al., 2015)
. The value of the decision variable is then continuously compared to a threshold that controls the initiation of motor actions toward one of the available responses 
(Heitz, 2014)
.
These strategies -where support is accumulated for or balanced between specific options -are optimal in traditional decision-making paradigms where a choice is made from a small set of perpetually available options. Storing and updating a balance of support between alternatives minimizes response times for a desired level of accuracy when options are static, in both binary and multiple choice scenarios 
(Bogacz, 2007;
Tajima et al., 2019)
. Critically, diffusion and racing accumulator models of choice require a decision maker to track the accumulation of support for different options over time. Simply put, support is represented in effectively every model of dynamic decision making as both (1) specific to the set of alternatives under consideration, and (2) cumulative. However, these two assumptions create a conundrum when we consider cases where a decision maker's options change over time, since these modeling approaches are challenged by situations where the options are not static. These types of decisions are common, as the availability of choice options often changes in the real world. For example, investors and stock brokers make decisions about buying and selling different assets, but any given asset is not always available for purchase. Knowing that they will buy certain stocks at particular prices if they become available requires representing support or value for options that are not currently for sale. Understanding how humans make decisions in rapidly evolving environments with shifting alternatives can enrich current theories of choice.
These assumptions pose considerable barriers when it comes to applying models to understand real-world choices, as well. For example, when buyers go onto the housing market, they must be prepared for the possibility that a house they like will be bought before they can place a bid. Likewise, they must be able to accumulate support for new houses that come onto the market and compare them to houses that have been on the market for some time already 
(Marsh & Gibb, 2011)
. If support for an option under consideration increases monotonically over time on average, then it is difficult to see how decision makers could accomplish this type of problem: the support for old options would remain high, while any new options added to the market would take a protracted period of time to accumulate sufficient support to compete against existing ones. This makes it exceptionally difficult to envision how dynamic models can be applied to address problems in dynamic and competitive markets such as real estate 
(D'Lima et al., 2022;
Handwerker et al., 2020)
. To approach this problem, we focus first on simple perceptual decisions.
Addressing these complex real-world problems in dynamic markets requires us to develop models of decisionmaking that accommodate shifting sets of response options. One way to approach this problem is to consider the possibility that people might represent support for features of stimuli, rather than representing support for the available response options. We refer to these feature-based representations of beliefs or preferences as alternative-general theories of decision-making. In this paper, we aim to test them against traditional alternative-specific theories where only the support for individual options is stored and updated.
Part of the reason for developing feature-based, alternative-general models is that alternative-specific theories make for a difficult starting point when considering how people could make decisions among sets of choice options that change over time. Removing an option from the set of available options requires a store of evidence to be abandoned, or completely invalidates a representation of support based on the balance between options. Likewise, adding a new option would require a completely new representation of support, either re-setting the entire decision process or requiring decision-makers to impute support for a new option from support for existing ones. When condensed to a single decision variable or set of accumulators, it is reasonable to disregard information favoring options outside the choice set -certainly one can disregard information in a color stimulus favoring the color blue when the only options for responding are "green" and "yellow". This corresponds to the axiom in expected utility theory of independence from irrelevant alternatives 
(Savage, 1954;
von Neumann & Morgenstern, 1944)
 -the idea that optimal decision makers should not be influenced by choice options or stimuli that are not relevant to the choice at hand. Despite this, it is well-known in the decision-making literature that people violate this axiom even in perceptual choice 
(Trueblood et al., 2013)
.
In these cases, representing support only for the options that are currently available -as in the theories we outlined above -is not flexible enough to allow for effective decisionmaking. This kind of problem requires us to understand not just how support for options changes over time, but understand how a person's underlying preferences or beliefs change over time above and beyond the options they are considering. As such, this paper aims to overcome barriers related to alternative-specific models by first developing alternative-general models of choice under shifting sets of alternatives in perceptual paradigms, forming a basis from which we can create theories of choice in dynamic markets.
Shifting evidence and phantoms.
Substantial progress has been made in accounting for how people decide when the evidence or stimulus is nonstationary 
(Holmes et al., 2016;
Tsetsos et al., 2011)
. There is now a quite substantial literature showing how drift rates, describing the momentary degree of support for different options, might change over time as decision-makers attend different sources of evidence 
(Busemeyer et al., 2019;
Diederich & Trueblood, 2018;
Roe et al., 2001;
Krajbich et al., 2012
). Yet almost no progress has been made in cases where the choice options themselves are non-stationary. Perhaps the most well-developed literature is on the effect of "phantom" alternatives -options that appear to be available at the initiation of the decision process but cannot actually be selected 
(Pratkanis & Farquhar, 1992;
Trueblood & Pettibone, 2017;
Chau et al., 2014)
. These briefly presented options affect choice outcomes as well as the neural representation of evidence during decision making: Signals in dlPFC thought to encode value differences between two options are altered when an unavailable, phantom option is added to a stimulus 
(Chau et al., 2014)
. Decision makers' susceptibility to phantom alternatives suggests that the representations of evidence during decision-making may be influenced by options outside of those that are immediately available. These effects are typically accommodated by adding accumulators for unavailable choice options and by introducing interactions between accumulators for both the phantom and available choice options 
(Usher & McClelland, 2001;
Trueblood et al., 2014;
Roe et al., 2001)
. Both neural and cognitive models of phantom alternatives maintain the assumption that the underlying representations of evidence during decisionmaking code specifically for the degree or balance of support among a small set of choice options and assume that all of the choice options are known from the beginning of a trial, even if they become unavailable. Here, we expand this work and its implications for dynamic theories of choice by examining cases where options are added, not merely removed.
As we have reviewed, decisions among changing sets of options are exceptionally challenging from the perspective of accumulator and diffusion models, as accumulation of support for new options can only begin at the time of their introduction -not before. We ultimately reconcile this problem by suggesting that decision-makers are capable of representing support for multiple options that are outside of the immediate choice set, using the feature-based, alternativegeneral evidence representations described above. This signals a major departure from the alternative-specific representations of evidence in accumulator and balance-of-evidence models that have dominated the decision literature.


Definitions, logic, and motivation
It is helpful at this point to define more precisely what we mean by alternative-specific theories of decision-making and outline their predictions for experimental results. This preempts the methods to an extent, but we hope it helps frame the problem and the modeling approaches that follow. We define alternative-specific accounts as models that represent stimulus information at any given time only with respect to the support it provides for the choice options under consideration. When an option changes, an alternative-specific account should be forced to restart the evidence accumulation process for the new option, retaining only evidence for options that are still under consideration or discarding all evidence altogether. Such theories are the dominant views in evidence accumulation modeling, which represent stimulus information as a balance of support or set of accumulators for available choice options. This type of model is put to the test using a two-stage procedure, examining how changes in the response options affect accuracy and response times. During an initial stage (400ms), a participant is offered two choice options and presented with stimulus information that can inform this decision. After this initial stage, the stimulus remains and continues to provide information, but one or both choice options are swapped out for something else. Performance in this twostage condition can be compared against a control condition where neither choice option changes at all, allowing us to examine how the first stage of evidence accumulation impacts response times and accuracy. In alternative-specific theories of choice, there should naturally be substantial differences between these two (switch and non-switch) conditions.
Allowing for some flexibility in specific modeling approach, there are three predictions an alternative-specific model could make for behavior in this paradigm. First, and most consistent with current accounts of binary choice, a decision-maker could simply ignore or discard evidence from the first stage and restart the accumulation process at the onset of the second stage. Second, some evidence from the first stage could be retained and re-assigned to the new options, carrying over neural activity in evidence accumulation or motor regions to the new set of options once they appear. As we suggest below, this could result from mechanisms like residual activation of (pre)motor cortex, change detection failures, and priming. Finally, the last possibility is that people represent evidence in an alternative-specific way and attempt to impute the degree of support for new options from support from the old options, based on their similarity to the original set. We visit all three of these hypotheses and use them to motivate the experiments that follow.
These three possibilities are tested in two experiments. In Experiment 1, we compare a variety of switch conditions against non-switch conditions, ultimately ruling out the first prediction (that evidence from the first stage is discarded) by showing that response times in the switch conditions are much too fast for evidence to have been reset for the second stage after options changed. In fact, a critical comparison between switch conditions and non-switch conditions where the final (post-switch) options and stimulus are matched with the control condition found no difference in mean accuracy or response times. This provides circumstantial evidence against the second and third alternative-specific predictions as well. These remaining two possibilities -fixed carryover from the first to second stage and variable carryover resulting from imputation -are then tested using quantitative model comparisons. In effect, Experiment 1 can qualitatively exclude alternative-specific models of the first type, and find quantitative evidence against specific models of the second and third type.
In Experiment 2, we put all three possibilities to a qualitative test. No matter which of the three views one takes, alternative-specific theories demand that evidence should not be accumulated during the first stage for options that are not available during the first stage. As a result, evidence for options that are not available for selection during the first stage should not influence decisions in the second stage. The second experiment explores this possibility by using an experimental design where first stage evidence should be ignored by virtue of the stimulus itself changing. This study examines how manipulations of first-stage evidence, which are entirely irrelevant to the choices that participants are asked to make, impact decisions. Any effect of first-stage (unavailable-option) manipulations in this paradigm would qualitatively rule out all alternative-specific accounts of evidence accumulation. This experiment ultimately yields even stronger conclusions than the first, as it is capable of falsifying the entire class of alternative-specific theories.
In anticipation of the results and as part of the motivation for the approaches we used in these experiments, we also test a newer class of models we refer to as alternative-general theories. In these approaches, evidence is represented for options that are not available to choose. This may occur because the stimulus and its history are represented, or because the decision-maker anticipates a future change in the set of response options available. In Experiment 1, we test these types of theories qualitatively and quantitatively, carrying out a formal model comparison against traditional binary choice models. In Experiment 2, we test a particularly challenging prediction of alternative-general theories, which is that irrelevant evidence for unavailable choice options should impact choices after a set of options changes. We contrast these predictions, and the posterior model predictions, against those of the alternative-specific theories described above. These approaches are developed in greater below in the "Alternativegeneral models" section.
Both experiments use very simple perceptual decisions as a testing ground for these tests of alternative-specific theories. Although they are simpler than the most highprofile instances of these choices, like house-hunting or job search, perceptual decisions offer tests of fundamental decision mechanisms -mechanisms we share with other animals 
(Cisek & Pastor-Bernier, 2014;
Santos & Rosati, 2015)
 -on which more complex multi-attribute models can be built 
(Krajbich et al., 2012;
Roe et al., 2001;
Busemeyer et al., 2019)
. They also allow for precisely time-controlled tests of entire classes of theories, pointing us in the right direction toward future accounts of decision-making.


Modeling approaches
In addition to the experimental tests that aim to qualitatively rule out classes of theories, we tested a few specific models both to illustrate their unique challenges and to provide a proof-of-concept that alternative-general models could work better than alternative-specific ones. Our goal with the model fitting and comparison was to test three a priori plausible explanations for performance on decision tasks with options that changed over time. These models are exemplars that correspond to many of the best and most common dynamic models of decision-making, though they are not meant to provide a comprehensive account of every possible model that could be applied to this task. Indeed, part of the barrier to doing so is that the number of dynamic decision models implementing accumulator and/or diffusion processes has become great 
(Busemeyer et al., 2019;
Ratcliff et al., 2016;
Heathcote & Matzke, 2022)
. This makes efforts at ruling out entire classes of models all the more important 
(Townsend, 2008)
, as we do in the studies presented here.
Despite this, it is useful to examine exemplar models to determine how we might expect a class of models to perform quantitatively. Here, we examine several candidate models and outline the reasoning behind using the alternativespecific models we chose.


Preliminary model selection
There were several important aspects of the stimuli in our experiments that helped constrain the features of plausible models that we might consider. First, the stimuli in our experiments were dynamic, with the number of dots or orientations changing from moment to moment. Response times in Experiment 1 were also particularly important, meaning we could not reasonably use any of the (many) static models of choice, ranging from simple probit and logistic choice rule models to heuristics 
(Gigerenzer et al., 2011;
Shah & Oppenheimer, 2008)
. Given the perceptual nature of the stimuli, most of these models would not apply anyway, as they require separable cues to be applied to specific decisions.
Second, the stimuli were stochastic, varying from moment to moment according to a statistical distribution -the orientations were drawn from a wrapped normal and the dots were drawn from truncated normal distributions. This makes it difficult to justify ballistic accumulation models 
(Brown & Heathcote, 2005
, 2008
, which effectively assume a constant signal from the stimulus on each trial. We expect that a linear ballistic accumulator model might perform similarly to the racing diffusion model we used in the main text anyway 
(Donkin et al., 2011)
, meaning that its performance would likely not change the conclusions of either study.
Based on these observations, we wanted a model with (a) dynamic evidence accumulation, and (b) stochastic / moment-to-moment fluctuations in accumulation. The RDM 
(Tillman et al., 2020;
Hawkins & Heathcote, 2021;
Ratcliff et al., 2007)
 and DDM 
(Ratcliff, 1978;
Ratcliff & McKoon, 2008;
Ratcliff et al., 2016)
 were the simplest models that clearly fit the bill. These models were supplemented with a new carryover parameter (ω), allowing them to handle priming and failures of change detection. As a result, they were slightly more complex than the GSR in terms of the number of parameters. We additionally tested two even more complex versions of these models that granted additional flexibility, which are presented in the supplementary materials.
It may be possible to use other, more complex models like the leaky competing accumulator model 
(Usher & McClelland, 2001
) to achieve similar performance on the task. However, this model comes with a serious concern regarding how mechanisms like lateral inhibition between competing accumulators would work. If a new option were to appear with no support, lateral inhibition would suppress its accumulation to the point where it might never be chosen over an existing option. This is obviously not what happened in the experiments, as participants frequently selected new options in every condition of Experiment 1 and Experiment 2. To apply a model with lateral inhibition to shifting sets of options, the inhibition would likely have to occur before novel options appeared -moving us to an alternative-general representation of evidence anyway. Therefore, we do not examine models with lateral inhibition, as the most coherent way for them to work would contradict the accumulator / alternative-specific representations that they use. New continuous-response versions of the LCA like the Radial-Basis LCA may be able to solve this problem by explaining how lateral inhibition works in an alternative-general framework 
(Hadian Rasanan et al., 2022)
.


Priming and change detection
From the perspective of the decision-maker, part of the challenge of accompanying shifting sets of options comes from (a) noticing that a change has occurred when an option has appeared or been replaced, and (b) (not) allowing support for previous options to influence the balance of support among new options. The first component is examined in work on change detection, which elucidates the visual mechanisms that underlie the ability to perceive a perceptual change. It entails not only the processes of identifying what the change is and where it occurs but also the implication that the observer is aware and successfully acknowledges its presence 
(Rensink, 2002)
. Past work has shown that under certain conditions, observers may fail to detect a perceptual change despite encountering large, noticeable, and repetitive changes in visual stimuli. This occurs even when the subject possesses prior knowledge or expectations of the change, referred to as change blindness 
(Rensink, 1997;
Simons & Rensink, 2005)
. Despite a lack of awareness, reaction times (RTs) tend to be longer on trials where a change has occurred, even if a participant fails to notice it 
(Thornton & Fernandez-Duque, 2002;
Koivisto & Revonsuo, 2003)
, often leading to errors and suboptimal decision-making 
(Durlach, 2004)
. Change blindness occurs not only when changes remain unattended by observers 
(Rensink, 1997)
, but also when a change is actively observed 
(Caplovitz et al., 2008)
 but not fully processed due to informational overload of attentional processes in dynamic tasks. However, the changes may still be registered implicitly and processed to a certain extent, even while remaining outside the reach of conscious awareness 
(Beck et al., 2007;
Vachon et al., 2012)
.
The presence of change blindness suggests that participants may not always be able to catch when their set of choice options changes over time, even when they are aware that a switch will occur, and sometimes even when they see it occur. Therefore, the calibration of evidence accumulation processes we might expect to see may not happen some proportion of the time, or may not happen entirely on each trial where the option set changes. Additionally, variations in change blindness could potentially explain individual differences in observed reaction times in relation to the task, as well as differences in which models fit best.
Of course, change blindness is not the only reason why participants may carry evidence over between sets of choice options. It is also possible that a stimulus or set of options may prime a particular response. Visual priming influences the processing and perception of visual stimuli by leveraging prior exposure and activation of relevant neural pathways 
(Schacter & Buckner, 1998;
Gauthier, 2000;
Wiggs & Martin, 1998)
. For example, manipulations involving prestimulus visual cues result in faster responses that are consistent with the cue 
Wang et al., 2015)
, which appears to result from increased activity in early visual areas 
(Liu et al., 2005)
. Exposure to a certain stimulus influences a person's response to a subsequent stimulus by accelerating their response 
(Tulving & Schacter, 1990)
, such as generating anticipatory motor activity or motor preparation. Priming impacts perceptual processes by reducing reaction times, improving accuracy, or enhancing the processing of specific features or attributes of a stimulus 
(Eimer & Schlaghecken, 2003)
.
As a result, having a set of particular options may actually prime the corresponding responses. Below, we present an orientation decision task where the options are presented before the stimulus. As a result, the pre-stimulus options may actually act in the same way as visual cues, priming particular stimulus orientations in advance of a trial. This can have two effects: first, it results in greater processing of option-consistent information. Second, it may actually prime the corresponding response as well -creating greater activity in motor areas that would be used to make that response 
(Jaśkowski, 2007;
Rosenbaum & Kornblum, 1982)
. In either case, the presence of options -even ones that are later replaced -is likely to affect the evidence accumulation process, as well as how evidence is generated for new response options when they are added to the choice set.
In the alternative-specific view, priming mainly affects support for the options available. However, if evidence is retained for options that are not inside the choice set, then priming may also apply to options that are not (yet) available for choice. Given the large body of work on priming effects, many of which involve inducing support for options that are not currently being entertained or even before it is known a decision or action will be taken 
(Herr, 1986)
. From this point of view, results from priming almost necessitate alternativegeneral representations of evidence during choice.
In this paper, we account for change blindness and visual priming by allowing evidence accumulation during an initial phase to influence evidence accumulation in a later phase. Specifically, in the models we test below, we include a parameter that specifies the degree to which evidence is "carried over" from one option to another, similar one. In the experimental paradigms we use, the same two colors are used to specify the initial set of options (e.g., blue and orange) and the final set of options. Based on the findings outlined above, we might therefore expect that some proportion of the initial evidence for "blue" is carried over onto the blue option during the second phase, even when the blue option changes position, due to priming of the blue response. The stronger the effect of priming and the weaker a participant's ability to detect the change from phase one to phase two of a trial, the more we might expect evidence to be carried over. This parameter of the models therefore allows us to gauge the degree to which priming and change detection failure influence the evidence accumulation process.
Two of these models, including accumulator and diffusion, are shown in 
Figure 1
. In this paradigm, one option in the choice set (A) is replaced by another (C) partway through the decision process. Both accumulator and diffusion models can handle priming or change detection using an ω parameter, which controls the degree to which evidence is carried over from the first to the second phase. As shown in the bottom panels of 
Figure 1
, values of ω close to 0 result in the evidence state(s) re-setting to zero, while values of ω close to 1 result in the evidence being fully carried over to the new option in the same color (A → C). This modification to accumu-lator and diffusion models allows them to use initial phases of evidence accumulation to inform later phases, although it does not make them fully "alternative-general" such that evidence can be flexibly re-mapped to any new option. We review models that can do this in the next section.


Alternative-general models
Although alternative-specific accounts of decision processes have been at the forefront of research on dynamic decisions, they are not the only models of these processes one could create. Thurstonian models like General Recognition Theory [GRT] 
(Ashby & Townsend, 1986)
 suggest that decision-makers represent stimuli in terms of their underlying features, and then map these stimulus features onto support for different response options as they are considered over time 
(Ashby, 2000)
. This draws focus from representing support for options to representing a more generalized notion of beliefs or preferences that is not specific to the particular options under consideration. Instead, support for each option is secondary to assessing the properties or features of the stimulus itself. Uncertainty about stimulus features translates to uncertainty about support for different options as stimulus information is sampled over time 
(Lee & Hare, 2023)
.
We define these models, where support is represented for options that are not concurrently under consideration, as alternative-general models of behavior. These models comprise all of the models that are not alternative-specific, although we only consider some specific examples here. As a result, they make opposing predictions for our experiments: in our first experiment, participants should be able to perform well even when their options change partway through a trial; and in the second experiment, participants should be influenced by irrelevant evidence for options that are not available during an initial phase of stimulus presentation.
Models of decision-making along a continuum, where participants can enter a response anywhere along a continuous response scale 
Smith, 2016a;
, have begun to adopt this approach to evidence representation . The use of a continuum means that there is a span of responses available for selection at any point in time, corresponding to a distribution of support across many response possibilities . Similar to GRT, evidence accumulation in continuous models represents information about stimulus features, which is then mapped onto support for different choice options . For example, a stimulus might be represented in terms of the orientations it has produced, or an impression of its central tendency and variability across samples . Retaining this stimulus information allows models to handle cases where the choice options change over time: the underlying representation of stimulus features is mapped initially onto support for the original set of choice options, but it is maintained and re-mapped onto a Outline of the task and models used in Experiment 1. (A) Diagram of the high-level structure of an experimental task where one option (blue) changes partway through a trial. (B) Two versions of a racing diffusion model 
[RDM]
, and what they might predict would happen during evidence accumlation in this paradigm (C) Two versions of a diffusion decision model 
[DDM]
 and their predictions for how evidence might accumulate (D) Outline of the geometric similarity representation, where stimulus evidence is accumulated and dynamically mapped onto support for different choice options even as they change over time.
new set of options when they are added to or removed from the consideration set.
Examples of both alternative-specific and alternativegeneral models are shown in 
Figure 1
. The racing diffusion model 
(RDM, bottom-left;
Tillman et al., 2020;
Hawkins & Heathcote, 2021;
Ratcliff et al., 2007)
 might account for behavior on the tasks by assigning one accumulator to one color or initial response, and transfer support to a new option based on what color it is or how similar it is to the original options. In this model, support for Option A and Option B are accumulated during phase 1, before the options change. If Option A (light blue in 
Figure 1
) then changes to Option C (dark blue in 
Figure 1
, then a decision maker maps the degree of support from A → C, depending on the degree of carryover ω. In the case where one or both options change, the state of each accumulator may be partially reset, such that the total evidence is reduced by a fixed proportion. 1 The decision-maker then continues to accumulate support for options B and C during phase 2 after the options have changed. This continues until one accumulator reaches the threshold, at which point the winning accumulator determines a response.
We compare this against two other models. The first is the classic diffusion decision model 
[DDM]
, shown in the bottom-middle of 
Figure 1
. In this model, evidence is represented as a single state representing the balance of support between A and B in stage 1, and then B and C during stage 2. As before, evidence accumulates as usual during stage 1, and is carried over to stage 2 based on the value of the carryover parameter ω. The balance of support can transfer fully (ω = 1) in cases where a specific color is primed or assigned support rather than a specific option, or it can fully reset (ω = 0) when a decision maker starts the decision process over when their options change. In other cases, this balance of evidence may even reverse, such as cases where the options trade places (ω = −1), as we explore in the supplementary materials. From the reset time, the accumulation process unfolds exactly as a typical diffusion model, terminating choice when the evidence state crosses the upper or lower choice boundary (color coded in 
Figure 1
 to correspond to which option would be chosen from which boundary). 2 
1
 We tested a version of the RDM where only the options that changed were reset, but this model was much too heavily biased toward options that did not change. As a result, it provided much worse fits to the data than the RDM and DDM presented here. 
2
 We also tested a version of the DDM where drift rates were permitted to vary freely across all stimulus and switch conditions. Drift rates were very difficult to recover in this model, and it did not fit better than the other models described here for any participant. A similar RDM, with freely-varying drift rates across all conditions, could not be recovered. We therefore do not include either of these Finally, we compare both alternative-specific models against a competitor derived from continuous-response models, referred to as the geometric similarity representation (GSR) model . This model represents accumulated support as a multidimensional state in a space defined by the stimulus dimensions, as opposed to one defined by the available choice options. For example, the orientation of a stimulus from 0-180 degrees might correspond to directions of 0-180 or 0-360 degrees from the origin, while the distance from the origin describes the strength of a participant's belief relative to these orientations (using polar coordinates of angle and radius; . The degree of support for a particular option is given by comparing this evidence state to a vector describing a choice option. The evidence state is updated as stimulus information comes in, and a decision is made when the state crosses a spatial threshold as shown in 
Figure 1
 (bottom-right). This vector-based representation of choice options allows it to accommodate similarities among choice options that are critical to accounting for choices and response times in both discrete and continuous tasks , but the most important property of this model is that the representation of decision evidence is distinct from representations of the choice options. When the options change over time, the vectors representing response options change, and support is re-mapped onto the new options (light blue changes to dark blue choice boundary in 
Figure 1
). This model therefore predicts that participants can continue accumulating stimulus information and assign it onto support for new options as they appear.
A formal description of each of these models is provided in the following section, but these three models exemplify many of the most important -and common -properties of dynamic theories of choice 
(Heathcote & Matzke, 2022)
. They also implement what we consider to be the three most plausible competing hypotheses about how people make decisions when their options change over time: (1) people store support for different options in separate accumulators, and transfer some proportion of accumulated evidence to a new option based on its similarity to the old options as well as processes like priming and change detection; (2) people represent a balance of support between options, and carry over some degree of support to a new balance when the options shift; and (3) people store stimulus evidence separate from support for each of the options, and use feature-based representations of evidence to assign support to new options as they appear.
The studies we describe below provide the opportunity to make strong inferences (J. 
Platt, 1964)
 between entire classes of models: accumulator and balance-of-evidence diffusion models that use alternative-specific representations of decision evidence, versus continuous models and (stochastic) GRT that propose alternative-general representations of evidence. We compare three different models in the text here, each embodying different assumptions about how evidence is accumulated and mapped onto support for each option. These are shown in 
Figure 1
. The former models predict that decisions among shifting sets of options should be very difficult, requiring decision makers to either throw out evidence they have collected when a new option is added and start the decision over (which would result in very long response times) or impute evidence for each new option when it is added (which will result in inaccuracy in choices). Conversely, continuous and GRT models predict that participants should be able to flexibly re-map accumulated evidence (about the stimulus) onto new response options as they are appear, and should thus show response times and accuracy that are comparable with conditions where the choice options do not change.


Study 1: Orientation
In the first study, the goal was to assess the patterns of accuracy and response times in a task where the options could shift during a trial. It used an orientation detection task where participants were asked to match a dynamic stimulus to a pair of response options. Partway through a trial, one or both options could change, such that participants had to re-assess the degree of support for the options in front of them based on the new orientations of their response options. Participants knew in each condition when and how many response options would change, allowing us to examine behavior under different (expected) shifts in the response options. In turn, we tested how each of the models outlined above could account for both accuracy and response time in classic control conditions (where no options changed) and compared them against different conditions where the response options changed over time. This included qualitative comparisons in terms of how well they could capture the patterns of accuracy and response times, as well as a formal model comparison where we assigned each model a posterior probability based on each participant's data.
All procedures were approved and carried out according to the University of Florida IRB, protocol #202100295. These studies were not preregistered.


Methods
An outline of the structure of this task is shown at the topleft of 
Figure 1
: participants view a jittering Gabor patch stimulus  whose orientation is drawn from a wrapped normal distribution with mean µ and standard deviation σ. Their task is to determine whether the mean orientation of the stimulus is closer to the blue or orange lines, each corresponding to different orientations. On some trials, the blue and/or orange options would change orientation free-drift models in the text or supplement. after 250ms or 400ms. There were ten ways in which the options could change during a trial. These dynamic changes of the stimuli could be broadly categorized by difficulty; they either made the decision easier (conditions 2, 9, and 10; listed below), harder (conditions 3, 4, 8, and 11), or had no net effect (conditions 5-7). We examine model performance across all 11 conditions and examine exactly how the options changed in each one, but focus on the conditions that made a choice easier in our basic behavioral analyses. If participants are more accurate and respond more quickly when a switch makes a choice easier compared to the no-switch condition, or (as we ultimately find) if the switch does not impede progress at all, then we can qualitatively rule out alternative-specific models that entirely reset the accumulation process when options change.


Participants
In our first study, a total of 41 participants each completed two sessions of the experiment. We collected 41 participants with the aim of retaining at least 30 after removing participants who did not provide high-quality data. This sample size was chosen to allow us to construct 95% HDIs of approximately ±4% on accuracy and approximately ±150ms on comparisons of response times between conditions, while identifying differences between conditions of 6% or 200ms with a Bayes factor of 3 ("moderate" support for / against the null) or greater.
All participants were between the ages of 18 and 25, and recruited from the Psychological & Brain Sciences undergraduate participant pool at Indiana University and the University of Florida. Of these participants, seven had response times less than 200 ms on at least 30% of their trials, indicating that they were simply clicking through the experiment. These participants were excluded, leaving 34 participants whose data were retained for further analysis. Each participant served as a mini-experiment testing each theory 
(Smith & Little, 2018)
; having 34 allowed us to characterize the proportion of participants favored by each model while providing suitable group-level constraints on individual-level estimates in hierarchical analyses.
The final sample had an average age of 22.58 years (SD = 2.75) and included 20 female and 14 male participants.


Procedure and conditions
Participants were briefed on the purpose of the study and completed informed consent prior to completing 24 training trials and then 592 full trials (16 blocks of 37 trials), which included 472 switch trials and 120 non-switch trials, randomly shuffled. Participants were told prior to each trial whether it would be a switch or non-switch trial, allowing them to use non-reset strategies for non-switch conditions if they so chose. If participants did not know whether a trial was going to switch, they may have withheld responses or even delayed the evidence accumulation process until a switch would have occurred. Therefore, to avoid contaminating responses in the non-switch condition, we notified participants so that they would not wait in anticipation of a switch that was not coming. In addition to the switch types, the coherence of the stimulus and degree of match to the available options was manipulated across trials. Upon completing the experiment, participants were debriefed on their performance and the purpose of the study.
Upon entering the laboratory and completing informed consent, each participant was briefed on the content of the study and completed 24 training trials. These included 12 non-switch and 12 switch trials, and participants were given feedback after every trial regarding the true orientation of the stimulus and its relation to the response they chose. Upon completing training, participants completed up to 592 trials of the full experiment. These included 480 switch trials -48 of each of the 10 switch types -and 112 non-switch trials. These trials were randomly shuffled and then blocked into 16 blocks of 37 trials. Some participants were unable to complete the experiment in an hour and therefore left before finishing all of the trials. Because trials were shuffled and therefore missing at random, their data were included in analyses.
A diagram of the structure of each trial is shown in 
Figure  2
. Prior to the start of each trial, participants would see a small white circle in the middle of the screen, which served as a visual fixation. Participants clicked the mouse to indicate they were ready to start each trial, and the options appeared on screen. The response options were presented as a fragmented line, extending from 350 pixels from the center of the screen to 520 pixels from the center of the screen on either side of the stimulus. These are displayed as blue and orange bars in 
Figure 2
 and 
Figure 1
. As shown, a blue and an orange line of approximately the same luminance were displayed on the screen to indicate the orientations of the response options. The orientations of these lines were based on the stimulus settings at the start of a trial, and adjusted based on the switch condition.
The stimulus appeared 500ms after a participant released the mouse button to indicate they were ready to start the trial. Each stimulus consisted of a jittering Gabor patch whose orientation was updated every 17 ms (60Hz). Each Gabor had a phase of 0, frequency of 1/10 (10 gratings per patch), brightness of 100, and aspect ratio of 1 -they were not designed to be visually unclear, as the uncertainty of the stimulus was induced by the randomly drawn orientations. Specifically, the orientations on each screen refresh were drawn from a wrapped normal distribution with a uniformly drawn mean on [0,180] degrees and a standard deviation of 15 (high coherence) or 30 (low coherence) degrees. The response options were placed so that one option was clockwise from the mean orientation of the stimulus and the other was counter-clockwise from the stimulus mean. The options started at either 15 degrees (low discriminability) or 30 degrees apart from one another (high discriminability), and the stimulus was tilted 1/3 of the way toward the target option (5 degrees / low match) or 2/3 of the way toward the target option (10 degrees / high match), relative to the zero or neutral point directly in between the options. The stimulus and option properties were varied so that the final positions of the response options in the switch conditions had ultimately the same distribution of properties (coherence, discriminability, and match) as the non-switch condition.
After appearing, the stimulus and initial response options remained on screen for 400ms, drawing and displaying a new Gabor patch orientation every 17ms (60Hz). In the switch condition, the options then changed, but the stimulus maintained the exact same mean and standard deviation for the duration of the trial. There were ten different ways in which the options could change in the switch conditions. These conditions were designed to be balanced such that an option changing did not give a participant any information about which option was correct. Each condition can be understood in terms of which option(s) changed and how. We refer to the target option as the option which was initially favored by the stimulus prior to the switch, and the lure option as the one which was initially disfavored by the stimulus prior to the switch. In some conditions, the lure and target necessarily changed so that the lure option was the correct option following the change in the set of options. We therefore refer to the target as the option that would have been correct prior to the switch, the lure as the option that would have been incorrect prior to the switch, the correct option as the option that is correct after the switch, and the incorrect option as the one that is incorrect after the switch. The ten conditions were:
1. Rotate -increase match: Both options moved clockwise or counterclockwise by the same amount. The rotation moved the target response option (orientation that was initially supported during phase one) toward the stimulus mean while simultaneously moving the lure response option (orientation that was initially disfavored) further away from the stimulus mean.
2. Rotate -decrease match: Both options moved clockwise or counterclockwise by the same amount. The rotation moved the target response option away from the stimulus mean while moving the lure response toward the stimulus mean.
3. Rotate -flip match: Similar to the rotate -decrease match, but the options moved far enough that the lure option became the correct option (became closer to the stimulus mean) and the target moved far enough that it became the incorrect option.
4. Converge: Both options moved an equal distance toward the stimulus mean.
5. Diverge: Both options moved an equal distance away from the stimulus mean.
6. Swap: The two options switched places, so that blue became orange and orange became blue.
7. Single option in (lure): The lure (but not the target) option moved, so that it ended the trial closer to the stimulus mean than it was at the beginning of the trial 8. Single option in (target): The target (but not the lure) option moved, so that it ended the trial closer to the stimulus mean.
9. Single option out (lure): the lure option moved further away from the stimulus mean.
10. Single option out (target): the target option moved further away from the stimulus mean.
These conditions are shown along the bottom of 
Figure  2
. The light orange / blue lines indicate locations of the response options that were vacated partway through the trial, while darker / more solid lines indicate their positions at the end of a trial. Note that the underlying distribution of orientations used to generate the stimulus never changed, no matter what happened with the options. This ensured that the stimulus would never become irrelevant, and therefore would be informative for any future options that appeared. Unlike Study 2, this incentivized participants to use stimulus information that they collected during the first phase (prior to options changing) and thus to adopt an alternative-general representation of evidence.
Example. An example trial might proceed as follows. Prior to the trial, a participant is shown a screen reading "This will be a SWITCH trial. The options available for choice will change partway through this trial." and "Click the middle of the fixation circle to start the trial." At this time, prior to the start of the trial, two options are displayed -a blue set of lines corresponding to a 70-degree orientation (diagonal, leaning to the right) and an orange set of lines corresponding to a 120-degree orientation (diagonal, leaning to the left). Next, the participant clicks the fixation circle -an empty circle with a white edge drawn at a 20-pixel radius from the center of the screen. After 500 ms, the stimulus appears. Its average orientation is drawn to be vertical, on average (90 degrees), but the stimulus is quite noisy (SD = 30 degrees). Every 17 ms, the orientation of the stimulus changes -from 145 degrees to 106, 22, 116, 99, 50, 76, and 101 degrees, and so on. After 400 ms, or about 23 orientation draws, the blue lines move to indicate a new orientation of 80 degrees. The participant then watches the stimulus continue to draw new orientations until they click the left mouse button (blue) or right mouse button Diagram of the stimulus (A), timeline (B), and switch conditions (C) from Experiment 1. The bars in the middle of each circle correspond to a distribution of orientations that are drawn from the stimulus on every screen update. (A) The orientation of the stimulus on each screen update was drawn from a wrapped-normal distribution; this is shown using an asterisk diagram, where the thick line corresponds to the mean stimulus orientation and thinner lines correspond to variability from update to update in its orientation. (B) The timeline of a switch trial from stimulus onset to response; non-switch trials simply kept the stimulus on screen until a response was made. (C) The 10 different types of switches that could occur on switch trials. Lighter lines correspond to stage 1 orientations; darker lines correspond to stage 2 (post-switch) orientations. The mean stimulus orientation never changed.
(orange) to indicate their response. This is a single option in (target) trial, because the initially better option (70 degrees is closer to 90 than 120 degrees is) got closer to the mean orientation of the stimulus when the switch occurred.
If the participant made a correct response -i.e., the mean stimulus orientation was actually closer to the option they picked -they would receive feedback of "Correct! +10 points" displayed in green. If not, they would receive feedback of "Incorrect. +0 points" displayed in red. Additionally, if their response took more than 4 seconds, they would see an additional screen with yellow text reading "Your last response was a bit slow, try to hurry up." displayed for 1.5 seconds. If their response was received within 250 ms of the start of the trial, they would instead see yellow text reading "Slow down! Make sure you are gathering enough information to make good choices." displayed on screen for 5 seconds to disincentivize participants from simply clicking through the experiment.
The rotate-increase match, single option in (target), and single option out (lure) conditions should be the easiest out of these. However, if evidence is accumulated in advance of the switch, then the no-switch condition should still show the best performance by virtue of having an additional 400ms of evidence for the available options. It is theoretically possible to violate this order by imposing lower thresholds in one condition relative to the others, but this would naturally come at the cost of severely reduced accuracy 
(Wickelgren, 1977;
Heitz, 2014)
. Therefore, if any of the switch conditions show faster response times with equal or greater accuracy than the non-switch or flip conditions, it casts doubt on alternativespecific accounts of behavior on the task.


Model parameters & estimation
Due to the nature of the stimuli, we had to simulate the performance of each model, similar to the process needed for models with dynamic evidence 
(Holmes et al., 2016)
. The GSR model is also a model that must often be simulated anyway , except in special cases where it reduces to a case like the diffusion or circular diffusion model 
(Kvam et al., in press)
. In this paradigm, the GSR model predictions could not be derived analytically and instead our comparisons were made by drawing a large number of simulated trials from each model and comparing the distribution of simulated trials against the real data. While there are now effective methods for approximating the probability density of a simulated model (see Holmes, 2015; Turner & Sederberg, 2014), this is not absolutely necessary for estimating the parameters of the models and comparing them to one another. Instead, we opted to pursue a new approach to model fitting 
(Radev et al., 2020)
 and comparison 
(Radev et al., 2021)
 that uses deep neural networks to approximate the relationship between data and model or data and parameters.
Formally, the goal of model fitting was to create a neural network that can learn the relationship between model M and observed data D. We know the relationship M → D, which is how data can be simulated from the model. Inverting this relationship gives us parameter or model inference, D → M. A neural network can therefore learn to carry out parameter and model inference by taking many sets of observed data D i that have been simulated from a model M i and using a supervised learning algorithm to adjust its parameters until it approximates this relationship to a reasonable degree.
To accomplish this, we used a neural network with 5 layers: an input layer for data, three hidden layers of decreasing size to iteratively condense the information in the inputs 
(Walczak & Cerpa, 1999;
Stathakis, 2009)
, and finally either a regression layer (for parameter estimation) or classification layer (for model comparison). There were 132 inputs in the input layer (see below), 75 inputs in the first hidden layer, 50 in the second hidden layer, 25 in the third hidden layer, and either 4 nodes in a regression layer for parameter estimation networks or 3 nodes in a softmax layer (describing posterior probabilities of the three competing models) for model comparison networks. Each layer was fully connected to the previous layer and had a hyperbolic tangent activation function. We tried other versions of the network with ReLU or leaky ReLU activation functions, but these models failed to converge or did not produce results that were as good as the networks we tested (see 
Sokratous et al.,
 in press, for a more thorough investigation of network architectures that can be used for this purpose). The neural networks were fit using the ADAM algorithm 
(Kingma & Ba, 2014)
 with an initial learning rate of .001, L2 regularization set at 0.00001, and 5000 training epochs.
Because the data from each participant may not contain the exact same number of trials overall or in each condition, the data have to be summarized in a consistent input format for the neural network. We opted to summarize the data in terms of accuracy, mean response times, and the 10/30/50/70/90 th response time quantiles for correct and incorrect responses in each of the 11 conditions. Quantilebased methods for model fitting have been used effectively for many years in dynamic models of RTs 
(Heathcote et al., 2002)
, and provide a concise but rich characterization of the pattern of response times within each condition. In addition, they can be computed for every input data set while yielding precise parameter estimates and good model recovery, as we show below. In the case that there were no incorrect responses for a particular participant and condition (which was extremely rare, given the difficulty of the task), we input 0s for the RT quantiles. Put together, this yielded 12 inputs per condition (1 accuracy, 1 mean RT, 5 quantiles for correct, and 5 quantiles for incorrect) for a total of 132 inputs to the neural network. All data from each model and from each participant was summarized in terms of these statistics before being fed into the model for training, fitting, or model comparison.
For parameter estimation, we used artificial data sets of 500,000 simulated participants for each model. Half of these (250,000) were held out as validation sets, while the other half were used to train the neural network to carry out parameter estimation. Below in 
Figures 3-6
, we present the performance of each modeling network for both training and validation sets, which indicate that there was only mild overfitting that did not detract from the ability of the networks to perform out-of-sample prediction.
All parameters used in each of the models are restricted to either [0,∞] or [0,1]. Those restricted to positive numbers were log-transformed (i.e., the network estimated the log of the parameters, rather than each parameter on its bounded scale), and those restricted on [0,1] were logit-transformed. This ensured that the neural network would produce parameter estimates that were within the theoretical range of each model. In the supplementary materials, we explore versions of the DDM where the carryover could be negative, but the best-fitting models, presented in the main text, had positive carryover.
All three models used the same four base parameters, which were a drift scalar, drift variability, threshold, and nondecision time. However, the DDM and RDM included a fifth carryover parameter ω, which controlled the degree to which evidence was transferred from the options during the first phase of accumulation into the second phase. To capture the overall drift rates in each model and phase, the degree of match between the stimulus and each response option was first computed for each stimulus and option that appeared during a trial. The degree of match was calculated by computing the average cosine between the orientation of the response options and the orientations of the stimulus across a trial. Taking the cosine allowed us to transform differences in the circular distribution into linear differences in drift rates. This allowed us to objectively account for variation in the coherence of the stimulus, degree of match between stimulus and response option, discriminability of the response options, and the random stochastic elements of the stimulus. This cosine metric for the degree of match -which theoretically could vary between -1 and 1, but in practice varied between approximately .5 and 1 -was scaled by the drift scalar to give a drift rate for each accumulator on each trial or phase. Below, we examine how this interacted with the other parameters in each model to produce response times and accuracy data.


Racing diffusion model
The racing diffusion model was simulated using five parameters: a drift scalar δ, a drift variability parameter ν, a threshold θ, a non-decision time parameter τ, and the carryover parameter ω. As outlined above, the drift rates for each of the response options µ (two before the switch µ A1 /µ B1 , two after the switch µ A2 /µ B2 ) were given by multiplying δ by the degree of match between the stimulus and each response option.
The drift rate for an accumulator followed a normal distribution, with mean µ and variance ν. While some drift variability was captured by computing the average degree of match between stimulus and response option, drift variability wound up being important for the RDM and DDM to capture the shape of response time distributions across trials -the GSR model performed much better than either RDM when drift variability was removed from all models. Drift variability in this case reflects random variability in attention or neural activation from trial to trial, and helps account for the slightly faster response times on correct trials (M = 1.58s) than incorrect trials (M = 1.64s) in Experiment 1. These are commonly referred to as slow errors, and are a key indicator that drift variability may be important to capture the patterns of response times 
(Heathcote & Matzke, 2022;
Ratcliff et al., 2015)
.
Accumulation in the RDM started with each accumulator equal to zero, s A (0) = 0 and s B (0) = 0. To simulate the accumulation process, each accumulator was incremented at each time step
s A (t + 1) = s A (t) + ∆ • (µ A + W A (t)) and s B (t + 1) = s B (t) + ∆ • (µ B +W B (t))
, where W (t) is a normal random variable with mean 0 and standard deviation set to .1. The value of one time step ∆ was fixed to 10ms, allowing a sufficiently granular approximation of a continuous-time diffusion process. To get the position of the accumulators at 400ms, for example, we can calculate s A (40) and s B (40), which is normally distributed with mean of .4 times their drift rate and standard deviation of .04. This gave the location of each accumulator at the time the options changed.
The second phase of accumulation was simulated by reassigning the values in each accumulator to the new set of response options. This was done by fixing each accumulator to a particular color: the accumulator for "blue" in the first phase was transferred to support for "blue" in the second phase, and likewise for the "orange" accumulator, regardless of whether the blue or orange options change locations. This was necessary to account for differences between nonswitch conditions and swap switch conditions in Experiment 1, where the colors changed locations.
A decision was triggered in this model whenever one of the accumulators reached the threshold θ. Once this occurred, their overall response time was given by the number of steps n t it took for the first accumulator to reach the threshold, plus the non-decision time:
RT ∼ Gamma(n t , ∆) + τ (1)
In a continuous-time random walk, the time between steps is exponentially distributed with mean ∆, resulting in a gamma distribution of hitting times Gamma(n t , ∆) when n t steps are taken along the way 
(Ross et al., 1996)
.
To generate the artificial data sets we used for training, we drew the drift scalar, variability, threshold, non-decision time, and carryover parameter values from the following distributions:
δ ∼ Gamma(2, .5) ν ∼ Gamma(1, 1) θ ∼ Gamma(2, .5) τ ∼ Gamma(.5, .5) ω ∼ Uni f orm(0, 1)
In addition to the parameters, we randomly sampled the stimuli from one of our 34 participants on each of the 500,000 runs and used those stimuli to drive a simulated accumulation process. The parameter estimation network was therefore trained on the same number and variety of stimuli as our actual participants, making the inputs representative of the real data that the network would eventually fit. This should set the parameter estimation and model comparison networks up as well as possible to handle the data from real participants.
Results of the trained network are shown in 
Figure 3
. As shown, there was a very close correspondence between the true parameters used to generate both the training and validation data, and the parameters estimated by the model. The loss for the validation set was slightly below that of the training set, indicating some mild overfitting, but the overall performance of the network was still reasonably good. We should therefore expect that the parameter values estimated by the RDM network should correspond well to the data. Because we were primarily interested in identifying best-fitting parameter values and not in quantifying uncertainty of each one, we did not construct a second network to estimate posterior variance 
(Sokratous et al., in press;
Radev et al., 2020)
. Instead, we were interested in model comparison and assigning posterior probabilities to each model -a process we describe below.


Diffusion decision model
The DDM that we used in these experiments was in many ways similar to the RDM described above. The main difference was that rather than tracking support for each option separately and using an absolute evidence rule, the DDM tracked the balance of support between each pair of options Parameter recovery performance for the RDM. In each plot, the x-axis shows the true parameter values that were used to generate the simulated data (blue = training set, orange = validation set), while the y-axis shows the estimated parameter values from the neural network. Linear correlations for training (blue) and out-of-sample validation (orange) are shown in the upper-left corner of each plot.
that was shown on screen. Because there were only two options ever available for selection, this allowed the DDM to be applied as if it were a binary choice task.
To construct the drift rate in the DDM, we first computed the average degree of support for each option by comparing the stimuli to the available alternatives. Next, we took the difference between these two degrees of support and multiplied them by the drift scalar µ, and added drift variability ν to compute the overall drift rates. These drift rates were entered into the simulated accumulation process that drove responses on each task.
As in the RDM, the degree to which evidence was carried over from the first to the second stage of accumulation was controlled by the carryover parameter ω. Values of ω = 0 would mean that a participant completely starts over as soon as their options change, which in some ways is the only "valid" option-specific strategy to implement here, as the balance is invalidated when options change. Values of ω = 1 would mean that a participant transfers the balance of evidence between an initial set of options and the revised set of options completely based on their color, even if one or both options change. These parameters therefore account for any priming or change non-detection that result in evidence during stage 2 being influenced by evidence during stage 1.
The evidence accumulation process stops as soon as the balance of support crosses an upper or lower boundary θ, as in the traditional DDM. The corresponding option is then selected. We assume that the upper boundary corresponds to the correct choice option, while the lower boundary corresponds to the incorrect choice option. Response times are computed in the same way as the RDM using Equation 1. We also assume no start point bias in the accumulation process, as participants have no information about which option is likely to change or which one is likely to be correct a priori, nor do they have stronger incentives to respond to one or the other.
The results of the model recovery study for the DDM are shown in 
Figure 4
. As with the RDM, the network performed extremely well on both the training and validation sets. We can therefore expect the parameter estimates and corresponding posterior predictions to provide close approximations of the best-fitting DDM when the network is applied to real data.


Geometric similarity representation model
While alternative-specific models can accommodate flexible remapping of evidence, to do so requires a carryover parameter ω to be estimated for each switch condition. This approach was explored in the supplementary materials. Building evidence-general models in this way has two serious drawbacks. First, because alternative-specific models with flexible remapping require an additional parameter for every possible kind of remapping, they can quickly become overparameterized. This may not be a serious issue for an experiment with one or two types of evidence remapping. However, complex situations, like those we might encounter in real life, might require 10s or even hundreds of carryover parameters. In Experiment 1, fully flexible remapping resulted in 10 freely estimated carryover parameters in the DDM and RDM and conferred little to no benefit in terms of quantitative fit compared to the RDM and DDM with a single carryover parameter ( 
Figure S2
). The second issue is that carryover parameter estimates hold little value in terms of developing a general theory. Without an overarching theory about how and why evidence is remapped, we cannot use carryover parameter estimates from a previous task to make informed predictions about untested types of evidence remapping. In the interest of parsimony, quantitative fit, and the development of cumulative theory, we favor building an alternative-general model from alternative-general theory, rather than adding flexible evidence remapping to alternative-specific theories.
As an example of an alternative-general model, the GSR model uses a representation of evidence during choice that did not depend on the available choice options. Instead, it represents the accumulated evidence from the stimulus as a two-dimensional state s. The two-dimensional state de- Parameter recovery performance for the DDM. In each plot, the x-axis shows the true parameter values that were used to generate the simulated data (blue = training set, orange = validation set), while the y-axis shows the estimated parameter values from the neural network. Linear correlations for training (blue) and out-of-sample validation (orange) are shown in the upper-left corner of each plot.
scribes the current orientation that is best supported by the available evidence (direction relative to the origin) and the degree of support for that orientation (radius). For example, a state of [1,0] would indicate support for a vertical orientation in 
Figure 5
. This state starts a trial at s(0) = [0, 0], indicating no bias toward any particular stimulus orientation. The 180 degrees of stimulus orientation are wrapped around a circle as shown in 
Figure 5
. The choice options are then represented by directions within the circle. Diagram of the structure of the GSR model for a decision between two orientations (blue and orange). Evidence accumulates until it crosses one of the two option boundaries, at which point the corresponding response is initiated.
As a decision-maker gathers noisy information from the stimulus, they drift toward its mean orientation, but this drift is noisy due to the varying orientations of the stimulus. Each step of the accumulation process is therefore described as a bivariate normal distribution, so that
s(t + 1) = s(t) + ∆ • (µ + W(t)).
(2)
As in the RDM, its movement is influenced both by the (now two-dimensional) drift µ and (two-dimensional) white noise with mean 0 and standard deviation .1. The twodimensional drift µ, with x and y drift components, can be re-parameterized in terms of the direction and magnitude of drift using polar coordinates 
(Smith, 2016b)
. The direction, naturally, is toward the mean orientation of the stimulus (e.g., 35 degrees in 
Figure 5
). The magnitude is then set by our free parameters, drawn from a normal distribution with mean δ and variability ν. This results in the same number of parameters to specify drift and drift variability in the GSR model as in the RDM and DDM, matching them in terms of complexity and structure.
The evidence in the state s is mapped onto support for an option A by taking the component of the state s along a vector d A describing that option s A = comp d A (s). For example, if the vector describing one of the response options is d A = [1, 0](see 
Figure 5)
, then the degree of support for a response at 90 degrees is simply the y-coordinate of the state s.
As in the other models, an option is selected when the degree of support for that option exceeds threshold θ. This can be represented as a line perpendicular to the vector describing the option at a radius θ from the origin. Two options and their relation to the state are shown in 
Figure 5
: the blue option (22.5 degrees) is selected when the state crosses the blue line, while the orange option (77.5 degrees) is selected when the state crosses the orange line. Here, we use a simple version of the GSR where all the thresholds are at a distance θ, but this threshold appears to actually be adjusted based on the similarity of the options to one another . It's likely that we could achieve better fits with similaritysensitive choice boundaries, but for our current purposes, all lines describing choice boundaries lie tangent to the circle with radius θ centered on the origin.
When new options are added to the set, the degree of support for the new option is computed in the same way -by taking the component of the state along the vector describing the new option. This creates a new line tangent to the circle shown in 
Figure 5
. As before, evidence accumulation proceeds until the state crosses one of the remaining choice boundaries. Since the GSR model is a continuous-time random walk as well, response times are given by Equation 1.
We used the same method to fit the model as with the RDM -a neural network with 132 inputs, three hidden layers of decreasing size, and a regression layer to predict the drift scalar, drift variability, threshold, and non-decision time parameters of the model. This network was trained in the same way, using 500,000 simulated data sets split into training and validation.
The results of this network are shown in 
Figure 6
. In general, the network performs similarly to the RDM and DDM networks, though with slightly worse recovery of thresholds in the validation set. Still, it recovers the parameters of the GSR model with high enough precision that we can expect the estimates and model fits in the main text to accurately represent the true generative parameters of a GSR process. Certainly, the posterior predictions generated by these estimates ( 
Figure 7
) represent sensible and high-quality fits to the data.


Model comparison
In addition to estimating the parameters of each model, we also aimed to compare how well each participant's data were explained by different models. Specifically, we sought to assign a probability to each model for each participant quantifying how (relatively) likely it was that they were best fit by the RDM, DDM, or GSR model. To do so, we used a feed-forward neural network designed for data classification 
(Radev et al., 2021)
, which takes a set of input data and assigns probabilities to each one of the generative models under consideration.
The neural network had a similar structure and training process to each of the parameter estimation networks described above. It had 132 input nodes, corresponding to the accuracy, mean RT, and RT quantiles for each of the 11 conditions of the study. It then had three fully-connected hidden layers with 75, 50, and 25 nodes in each layer. As above, each of the hidden layers had a hyperbolic tangent activation function. Rather than a number of nodes corresponding to the number of parameters, the final layer had only 3 nodes, which corresponded to the degree of activation in the network for each of the three models. The activation of these nodes were then passed through a softmax layer to transform them into model probabilities. The network was trained using an ADAM algorithm with an initial learning rate of .001, L2 regularization set at 0.00001, and 5000 training epochs.
To train the network, we generated a total of 750,000 sim-ulated data sets -250,000 from each model -and divided it in half so that there were 375,000 training sets and 375,000 validation sets. Both the training and validation data were randomly shuffled within their set, ensuring that both the training and validation sets contained the exact same number of inputs from each model but that the order was not systematic. We thereby set the prior probabilities of each model equal to 1/3, as the classification network had no reason to expect one model over another as its input. Once the network had been trained, we applied it to the validation set to examine how well it recovered the true model for each input data set. 
Table 1
 Confusion matrix for the neural network trained to carry out model classification. The rows correspond to which model was actually used to generate a data set, and the columns correspond to which model was assigned the highest likelihood by the network. The model confusion matrix from the classification network is shown in 
Table 1
. As shown, the network generally performs at a high level, successfully recovering the true generative model approximately 94% of the time (average accuracy = 93.86%). It had high accuracy across the board, indicating that none of the models was particularly easier than the rest to identify.
Critically, the marginal probabilities indicate that the network was very close to unbiased, with an overall marginal posterior probability of 1.01 for the DDM indicating only very slightly higher-than-optimal rates of selection. Therefore, the results of the model comparison presented in the Results sections below are likely to be highly accurate and representative of the true posterior probabilities of the models.
Data, analysis code, and model fitting and comparison code can all be found on the associated Open Science Framework repository at osf.io/gwz3j. Additional models are also tested in the supplementary materials and provided on OSF 
(Kvam et al., 2024)
.


Results
For each experiment, we evaluated behavior alongside how well the models could account for it. For any comparisons between conditions in terms of accuracy or response times, we report the mean estimate [M], a 95% highest density interval [HDI] on the estimate (Kruschke, 2014), and a


Figure 6
Parameter recovery performance for the GSR model. In each plot, the x-axis shows the true parameter values that were used to generate the simulated data (blue = training set, orange = validation set), while the y-axis shows the estimated parameter values from the neural network. Linear correlations for training (blue) and out-of-sample validation (orange) are shown in the upper-left corner of each plot.
Bayes factor [BF] quantifying the odds against a null hypothesis 
(Rouder et al., 2012)
 to allow the reader to understand the precision of each estimate as well as how the conditions compare to one another 
(Kruschke & Liddell, 2018)
. The Bayes factor was corrected for multiple comparisons by fixing the prior that the null hypothesis holds across all comparisons to 0.5 
(Westfall et al., 1997)
. In cases where the Bayes factor favors the null, we report this as BF 0 rather than BF.
Overall, participants were more accurate in the highcoherence condition (M = .73, 95% HDI = [.71, .73]) than in the low-coherence condition (M = .64, 95% HDI = [.63, .66], BF > 1000), and they were also faster in the highcoherence (M = 1.53, 95% HDI = [1.52, 1.55]) than in the low-coherence condition (M = 1.67, 95% HDI = [1.65, 1.69]; BF > 1000). This indicated that participants were sensitive to the properties of the stimulus. However, the main objects of interest were accuracy and response times as they related to manipulations of the choice options.
Participants' mean response times in each condition are shown in 
Figure 7A
 (top), as is their average accuracy (bottom). If participants are discarding evidence when their options change, then we should expect performance in the nonswitch condition (leftmost points) to be equal or superior to all other conditions. If this is true, then making a decision easier -by (a) making the target option closer to the true orientation, or (b) making the lure option farther from the true orientation of the stimulus, or (c) both -should not result in greater accuracy or faster response times. We tested this by comparing the non-switch condition against the target-in (target gets closer to the true orientation), lure-out (lure gets further from the true orientation), and increased-match rotation (target gets closer and lure gets further from the true orientation).
The descriptive statistics showing the overall pattern of mean accuracy and response times across conditions are provided in 
Table 2
. Ultimately, the predictions of an alternative-specific theory were not borne out in the data. Mean RTs in the non-switch condition were not faster than those in the increased-match rotation (BF 0 = 23.21), the single target-in condition (BF 0 = 21.24), or the single lure-out condition (BF 0 = 7.45). Yet compared to the non-switch condition, accuracy was higher in all three of these conditions including increased-match rotation (BF = 50.26), singlechange target-in (BF = 668.71), and single-change lure-out (BF = 1.79). This indicates clearly that participants improved in all three conditions where we could expect a participant tracking the stimulus information to out-perform the non-switch condition, providing preliminary support for an alternative-general account of behavior.
The fact that performance in the non-switch condition is comparable or even worse than performance in the easier switch conditions indicates that either (a) both switch and non-switch conditions take advantage of the evidence accumulated before 400ms, as in alternative-general models; or (b) that both switch and non-switch conditions begin accumulation at 400ms. The carryover parameter allows RDM and DDM to do the former, but they cannot do so in a way that fully captures the pattern of switches that a set of alternatives undergoes.
The patterns of behavior in other conditions are not directly pertinent to the question of whether participants are improving by tracking the stimulus information in an alternative-general way, but they needed to be included to balance out the other conditions and to evaluate a secondary control condition (swap switch). Information on mean accuracy and RTs for these conditions is provided in 
Table 2
, and they do provide some constraints on what explanations are likely within different classes of models. We review the implications of those results in the supplementary material.


Critical comparisons
A key aim of Experiment 1 was to identify specific conditions that completely ruled out particular classes of models. If participants completely throw out evidence from the first 400ms of a switch trial, then difference in response times between conditions where the options changed and where the  options did not change, matched on final difficulty, should be at least 400ms. To carry out this comparison, we compared the configuration of response options at the end of a switch trial (in terms of relative positions of options and difficulty of stimulus) against those of a non-switch trial. This was tested using a mixed effects model, predicting the overall difference between switch and non-switch conditions while controlling for variability across types of switch conditions and participants as random factors. After doing so, we computed the Bayes factor for each comparison using a Bayesian ANOVA (JASP Team, 2020) to evaluate the degree to which the data increased or decreased support for a null (no-difference) effect of the switch vs no-switch manipulation. 3 This was done for both accuracy and log-transformed response times, using a binomial link in the former case and a logit link in the latter.


Condition
Mean  
Table 2
 Pattern of accuracy and response times across conditions of Experiment 1.
Overall, accuracy did not appear to differ between switch (M = .66) and non-switch / control conditions (M = .69), with the Bayes factor slightly favoring the null / no effect of switch (BF 0 = 3.47). Furthermore, response times differed by much less than 400ms. Mean response times for the nonswitch conditions (M = 1.54s) were less than 100ms faster than those in the switch conditions (M = 1.64s). In fact, this difference was not even credible -the Bayes factor for the comparison between these two conditions weakly favored the null / no-difference conclusion (BF 0 = 2.61).
Taken together, this is strong evidence against alternativespecific models; participants were able to maintain essentially the same level of accuracy and speed in their responses even when a different set of options was presented for the first 400ms of a trial in the switch condition. Our participants were instead able to adjust on the fly and maintain a high level of performance even when they did not know what the final set of options would be, suggesting that they must have a representation of stimulus information that can be readily re-mapped onto support for new options as they appear.


Model recovery
The behavioral data provide support in principle for alternative-general accounts of performance on the task, but it is possible that individual participants or other factors are driving these effects, or that alternative-specific models can still provide a satisfactory account of performance by carrying over evidence from one phase to the next on each trial (e.g., due to visual priming or change detection failures). To quantify and formally compare these theories, we fit all three models -RDM, DDM, and GSR model -to each participant's data. Because these models are simulation-based, we used a deep-learning approach to carry out model fitting and model comparison. We trained four networks: one each to estimate the parameters of the three models and another to classify observed data in terms of which model was most likely a posteriori. This allowed us to estimate best-fit model parameters for each model, and then to assign posterior probabilities to each model quantifying how likely it was that each model best accounted for each participant's data.
The results of these analyses are shown in 
Figure 7
, with the RDM in purple, the DDM in yellow, and the GSR model in red. Panel A shows the model predictions for mean accuracy and response times -in general, all three models do a good job. They are all relatively comparable in terms of accounting for mean response times, although the RDM tends to slightly overestimate mean RTs in most conditions. When it comes to accounting for accuracy, the GSR model edges out both alternative-specific models by being better able to capture the patterns of mean accuracy across conditions.
Here, none of the models are doing a particularly bad job and they are generally able to handle the gross patterns of accuracy. However, a key condition comparison that differentiates the models is the difference between Converge (↓ discriminability) and Diverge (↑ discriminability). Both the RDM and DDM predict that accuracy should be higher in the latter condition, but the GSR correctly predicts that it should be lower. The RDM in fact exaggerates the differences in mean response times between these conditions (it undershoots the Converge condition and overshoots the Diverge condition) in an attempt to compensate for this error in accuracy predictions and reduce the predicted difference between conditions, but the DDM makes the wrong prediction for empirical patterns of both response times and accuracy. The reason for this is that discriminability is a constant that factors into drift rates in the DDM (and in all random walk models based on optimal sequential probability ratio tests 
Wald & Wolfowitz, 1949;
Kvam & Pleskac, 2016
) -as discriminability increases, drift rate increases, and so accuracy increases while RT decreases. We tested another version of the DDM where drift rates could change across all conditions, but this model was outperformed by all the models reported here. As a result, the best DDM prediction is em-pirically incorrect. The GSR accounts for this phenomenon because the Diverge condition leads to two response options that do not line up with the stimulus evidence (i.e., neither choice boundary lies directly along the evidence accumulation trajectory), meaning that it predicts longer response times and lower or comparable accuracy in the Diverge condition relative to the Converge condition.
The effect of manipulating discriminability in our experiment is rather important for arbitrating among different continuous-response models. We revisit this in greater depth in the supplementary material, and examine how different patterns of behavior resulting from manipulations of discriminability can qualitatively inform our understanding of the cognitive mechanisms underlying continuous and multialternative choice.
Model predictions for RT distributions in each condition are shown in Panel B. There are a few observations that stick out here. First, the DDM struggles in many cases to capture the leading edge of response time distributions. We suspect that this is due to its inability to discriminate between cases where one option changes or both options change. In either case, the DDM tracks only the accumulation of the balance of support between options. Because of this, it tended to predict that the balance of evidence was not strongly carried over into the second phase of evidence accumulation, reflected in estimates of the carryover parameter being quite low in the DDM (M = .15) relative to the RDM (M = .85). As a result, evidence in the DDM had to "catch up" to the other two models in the second phase by having much higher drift rates, creating distributions of response times with a sharp leading edge and insufficiently long tails. These narrower distributions also created a "lump" in the response time distributions: all three models are accounting for behavior across levels of coherence and aggregating them in the predicted distributions, but the narrow distributions for each condition predicted by the DDM results in a clearer second mode than the other models. Ultimately, its predictions did not align well with most of the response time and accuracy data and the DDM was only favored for 3 participants.
Finally, we carried out a model comparison on each participant's data, using a neural network trained to assign posterior probabilities to each model for a set of behavioral data. The results are shown in 
Figure 7
, Panel C. While there is heterogeneity across participants, the GSR model is the clear overall winner, as the best-fitting model for 20 out of the 34 participants. The RDM was a distant second, accounting best for only 11 participants, while the DDM fit best for only 3 participants. Looking across the entire data set, the aggregate posterior probability of the RDM was 33.92%, the DDM was 8.98%, and the GSR was 57.10%. Posterior probabilities for each participant and model, parameter estimates for each model and participant, and MATLAB code for training and comparing the networks are provided on the Open Science Framework at osf.io/gwz3j.


Preliminary discussion
Overall, the results of Study 1 favored alternative-general models over alternative-specific ones. The critical empirical tests indicated that there were no significant differences in performance between a condition that started with options A,B and changed to C,D and a control condition where C,D was always available. This means that participants were not deterred by the options switching, and were easily able to take the evidence accumulated during the first stage and map it onto new options.
These results are also reflected in the model fits, where the GSR model produced generally closer fits to the accuracy and response time data compared to the more flexible DDM and RDM. This is despite the inclusion of a priming/forgetting parameter in both the DDM and RDM that allowed them to transfer some of the accumulated evidence into the new evidence accumulation state(s) after the options had changed. In the empirical results, the GSR is favored by a counterintuitive pattern of performance where participants are slower and less accurate when their options are easier to discriminate. The GSR explains this result as an issue where the stimulus does not match well to either response option. While the RDM is able to handle this issue to an extent, driving its drift rates in terms of the match between the stimulus and available response options, the DDM struggles because the difference between the two options does not change or even improves with discriminability.
In line with these results, the neural network trained to do model classification naturally assigned greater posterior likelihoods to the GSR than to the RDM or DDM. While some participants were best fit by the alternative-specific strategies (mostly the RDM), the majority appear to be following alternative-general choice strategies where they represent the stimulus in terms of its underlying features (orientation) and map accumulated feature information onto support for different responses as they appear.
While providing some evidence for alternative-general strategies, there are some issues with Experiment 1 that might prevent us from making strong categorical claims about what strategy people are using. This initial study strongly incentivizes an alternative-general strategy, because the stimulus information never changes and can therefore be gainfully considered during both the first and second stages. Strategies that fully dismiss all of the evidence collected during stage 1 would clearly be inferior to ones that use some of the information from the first 400ms to inform decisions. The second experiment therefore explores an experimental design where early stimulus information is irrelevant to the choice participants eventually make -disincentivizing alternativegeneral strategies.
Despite the qualitative results that the RDM and DDM struggle to explain, the first experiment also relies partly on quantitative model comparison. Therefore, the second study aimed to provide clear-cut empirical evidence support for alternative-general models in a paradigm where it was not advantageous to accumulate evidence during the first phase prior to the options changing. It aims to carry out strong inference (J. 
Platt, 1964)
, such that the alternative-specific and alternative-general models make qualitatively different predictions and can be ruled out on the empirical evidence alone. Of course, we still present posterior predictions that illustrate the gap, but the results could easily be taken alone as support for alternative-general theories.


Study 2: Dots & location priming
While Study 1 provided strong support for alternativegeneral accounts of dynamic choice, its superior performance was based largely on better quantitative performance. As shown in 
Figure 7
, the RDM and DDM still accounted well for data on the tasks, even if they were inferior to the GSR model in terms of fit for most participants. Study 2 carried out a more decisive, qualitative test of alternativespecific models by comparing how stimulus evidence presented before the options changed influenced decisions. In addition, to avoid having to negotiate two different metrics (accuracy and response time) and their relationship 
(Wickelgren, 1977)
 as a potentially confounding factor, Study 2 focused on accuracy as the lone measure of performance by removing the stimulus from the screen after a fixed period of time. Instead, the stimulus was presented for a limited time during which participants were not allowed to respond, to ensure that participants saw the exact same stimuli across conditions. As in Study 1, the experiment was programmed and run using MATLAB 9.7 
(Inc., 2019b)
 and Psychtoolbox-3 
(Brainard, 1997)
.
In this study, we tested how well each of the models could account for a kind of feature-based priming effect. Specifically, we primed a particular location on the screen by making it brighter during stage 1 of each trial, but the primed location was outside of the available response options. Any alternative-specific model should not be sensitive to the prime, while any alternative-general model should be. This location-based prime did not prime a particular response option, which was defined by a box presented on the screen. Therefore, if participants are representing support for locations (alternative-general representation), they should accumulate support for the location that is primed during the first stage, resulting in a bias toward that response option. As a result, we should see greater accuracy when the subsequent stimulus is congruent with the primed location, and lower accuracy when the subsequent stimulus is incongruent with the primed location.
Conversely, if a participant is representing support strictly for response options (alternative-specific representation), they should not be affected by the prime because it does not occur inside the box corresponding to a response. This paradigm allows us to make a strong, qualitative test between classes of models. If there is an effect of the prime, it provides clear support for the class of alternative-general representations of evidence (like the GSR) over the entire class of alternative-specific representations (like the RDM and DDM).


Methods
The stimulus used in Study 2 is shown in 
Figure 8A
: participants viewed two strips of flickering dots (similar to work by 
Zeigenfuse et al., 2014)
, each with a colored box superimposed over part of the strip. Participants were asked to determine which box was most likely to have more dots in it after the end of a trial (i.e., on the next draw). The boxes corresponded to the choice options -the blue box to a left-click response, and the orange box to a right-click response. Partway through a trial, one or both of the boxes would change location. After a fixed amount of time, the stimulus and boxes would disappear and participants would be asked which box would contain more dots on the next screen update.
Participants were informed that the stimulus itself could change in terms of where the peak brightness of the strip was, but would do so 250ms before the options changed. This made it so that the locations along each strip that were brightest at the beginning of the trial (before the options changed) were not the same as the locations that were brightest at the end of the trial (after the options changed). Moreover, it ensured that the optimal strategy was actually to ignore information presented before the options changed, deliberately disincentivizing an alternative-general task.
We retain the terminology of the first experiment with regard to the two options: the "target" option refers to the option that is initially superior (correct) at the start of a trial, while "lure" refers to the option that is initially inferior (incorrect) at the start of a trial. After the switch occurs, the lure may become the correct option and the target incorrect. We refer to the final correct and incorrect options as simply correct and incorrect, to reduce confusion.
As in the previous study, zero, one, or both options could change. The differences between conditions that we sought to examine were determined by whether an option moved to a location that was previously brighter before the option change (i.e., primed). This change could result in a box moving to a location that was brighter in stage 1 and stage 2 (target-better), moving to a location that was brighter in stage 1 and darker in stage 2 (target-worse), moving to a location that was darker in stage 1 and brighter in stage 2 (lure-better), or moving to a location that was darker in stage 1 and darker in stage 2 (lure-worse). When two options changed, both the target and lure could change, resulting in four conditions: one option moving to a location that was brighter in stage 1 and 2 while the other option moves to a location that was darker in stage 1 and 2 (target-better, lure-worse), one option moving to a location that was brighter in stage 1 and darker in stage 2 while the other option moved to a location that was darker in stage 1 and brighter in stage 2 (target-worse, lurebetter), both options moving to a location that was brighter in stage 1 and 2 (both better), or both options moving to a location that was brighter in stage 1 and darker in 2 (both worse).
In addition to these conditions, which examined how primed locations would affect evidence accumulation, there were conditions where the option change resulted in no net difference between the options (no net change). This could occur whether no options changed, one option changed, or both options changed. We include all non-primed conditions under this umbrella.
In each condition, the stimulus was presented for a fixed amount of time (1.8s) and then removed from the screen, allowing us to examine accuracy alone as a metric of performance across conditions.


Participants
In the second study, a total of 34 participants between the ages of 18 and 30 were recruited through flyers and listserv emails with the goal of retaining at least 30 participants after applying our filters (>55% accuracy on the task, response times averaging less than 5 seconds after stimulus offset). One participant was removed from analysis for failing to achieve better than 55% accuracy, leaving us with 33 participants. As before, the target sample size of 30 was chosen for three reasons: to impose reasonable group-level constraints on individual-level estimates in hierarchical analyses, to ensure HDIs on estimates of accuracy within each condition were within approximately ±2%, and so that Bayes factors were moderate or stronger (BF > 3) for differences in accuracy of ±3% or greater between conditions. Because we had the same number of participants but fewer conditions (and thus more trials per condition), our power to detect effects was slightly higher in this study than in Experiment 1.
Participants were paid $15 for attending the session plus an additional $5 for attaining at least 60% accuracy or $10 for attaining 80% accuracy. The task was generally quite challenging, with most participants obtaining around 70% accuracy, making the higher threshold aspirational rather than clearly achievable. However, two participants were able to achieve this higher criterion (80% accuracy) during the task.
The final sample of participants was primarily University of Florida students, with an average age of 19.53 years (SD = 1.82), and it included 27 female and 6 male participants.


Stimulus
As shown at the top-right of 
Figure 8
, participants were presented with two horizontal strips of flashing dots. Within each strip, the number of flashing dots in each column of the stimulus was randomly drawn on each screen update (60Hz) and set so that, on average, there were more dots on one side of the strip than the other. For example, one stimulus might have its brightest point one degree to the right of center while the other stimulus had its brightest point two degrees to the left of center. From this brightest spot, average brightness fell off linearly. This results in the triangle-shaped levels of brightness (average # of dots) shown in 
Figure 8
.
Along each strip of flashing dots (upper and lower), a box was displayed indicating the area that participants should attend and compare against one another. The upper box was blue and the lower box was orange. Participants were instructed to respond with the left mouse button if the blue box contained more dots at the end of a trial, and right mouse button if the orange box contained more dots at the end of a trial. Participants were specifically instructed that the stimulus would change on some trials, so they should pay attention to the most recent screen updates when making their decisions.
The stimuli were displayed on a black background as white dots, refreshed at a rate of 60Hz. Each stimulus consisted of two strips of flashing dots, where each strip was approximately 1/10 the height and 1/4 the width of the screen, whose resolution was 1920 × 1080 pixels. The bottom of the upper strip was placed 5 pixels above the center of the screen and the top of the lower strip was 5 pixels below the center of the screen, to allow participants to easily view both strips at once. Within each strip, the number of dots in each column of pixels was drawn from a truncated normal distribution N(µ x , σ) truncated at 
[0,
108]
. The value of µ (mean number of dots in each column) was set by a triangular distribution as shown in 
Figure 8
, such that the brightness or number of dots fell off linearly from the central, brightest spot. For each strip of dots, the location of maximum brightness x max (µ x max = 100) was randomly drawn from a uniform distribution across the width of the strip. From this location, the average brightness decreased linearly, so that µ x = .5•|x−x max |, where x max was the location of the brightest point and x was the horizontal location of a particular column of dots within a strip. This yielded a linear gradient of dot brightness for each strip. There was no bias toward either side for either strip, meaning that the top and bottom both had their brightest point on the left or right about half the time -and that the brightest point was on the same side about half the time and on opposite sides half of the trials.
On each screen update, the number of white dots in each column was randomly drawn from this distribution with mean µ x and standard deviation σ = 10 (high coherence) or σ = 25 (low coherence), each presented on half of the trials to manipulate difficulty. This created a noisy pattern of flashing dots in each strip, including brighter and darker patches that should have been apparent to participants. Diagram of the stimulus (A), timeline (B), and switch conditions (C) from Experiment 2. (A) The peaked diagrams indicate the mean brightness of the stimulus in each column of dots along the stimulus, with lighter lines and boxes corresponding to the stimulus during phase 1 and darker lines and boxes corresponding to the stimulus during phase 2. Circles correspond to the mean brightness within each box. (B) The time coures of a trial from stimulus onset to response. (C) The four types of switches that could happen to an option in the one-switch condition. These four types are crossed with one another in the two-switch condition to create target-better/lure-worse, target-worse/lure better, both-better, and both-worse conditions.
As in Study 1, there could be a change in 0, 1, or 2 of the options. Non-switch (neither box moved), single-switch (one of the two boxes moved), and double-switch (both boxes moved) conditions were separated into blocks. In the nonswitch block trials, both colored boxes remained stationary and participants had to simply evaluate which box contained more dots after the switch occurs. In the single switch condition, one of the boxes would move horizontally after 1000-1400ms (uniform-randomly drawn) on each trial. Similarly, in the double switch block, both boxes would move horizontally toward either the left or right on each trial. In both single and double switch conditions, the boxes could move such that the target box (one with initially more dots) moved to a location that had previously been brighter (target-better) or darker (target-worse), and/or move such that the lure box (one with initially fewer dots) moved to a location that had previously been brighter (lure-better) or darker (lure-worse) as described above.
On each trial, the brightest spot of each strip shifted slightly to the left or to the right, by a distance up to the width of the box. Critically, the stimulus changed 250ms before the options did. This was done so that on trials where the boxes changed, the location that either box moved to was not actually superior or inferior to its original location so that there was no residual visual brightness from prior to the stimulus change. This means that the location the box moved to was, on average, no better or worse than the location it originated. This is critical to understanding the outcomes of the experiment. If participants only pay attention to the number of dots within the boxes, they will be no more or less accurate in the switch conditions than the non-switch conditions. Therefore, any differences in performance across the switch-type conditions must be driven by information that participants considered that (a) was outside the boxes / available options, and (b) occurred prior to the switch.
For every correct decision the subjects made, they were awarded 10 points. The individual's accuracy score was shown right after they made a response on each trial. At the end of each block, the participants were offered the opportunity to take a short break. The entire study lasted approximately 1 hour. At the end of the experimental task, a participant's cumulative score was displayed along with their payment. Following completion of the study, participants were debriefed on the study purposes, their performance, and completed payment.


Procedure and conditions
Participants completed informed consent and were informed of the purposes of the study. They then completed 10 training trials, including one of each of the main condi-tions of the experiment. Before and after these training trials, participants were instructed that the stimulus would change before the boxes did, and that their decisions should reflect the number of dots in each box after the switch occurs ("The windows may move on some trials. Each block will indicate if / how many windows will move. On all trials, your decision should favor whichever window has more dots at the *end* of a trial -i.e., after the windows have moved, on trials where they change. Note that the stimulus will change before the windows do, making it extra important to pay attention to the end of the trial.").
The goal of these instructions was to ensure that participants had an incentive to ignore information outside the boxes during the first phase of a trial, before the boxes moved. This diverges from Experiment 1, where participants could gather useful information before the choice options shifted. Experiment 2 therefore provided a particularly strong test of both participants' willingness to use alternative-general evidence accumulation strategies and their ability to ignore extraneous stimulus information about unavailable locations (options) when inappropriate to the task at hand.
After training, participants completed 450 trials of the main experiment, organized into 15 blocks of 30 trials. This included 5 blocks each of no-switch trials (neither box moved), single-switch trials (one box moved), and doubleswitch trials (both boxes moved). Participants were instructed on the content of each block at the beginning of the block and reminded of what type of trial they were about to see prior to the start of each trial. Participants would start a trial by viewing this reminder and then clicking the mouse to initiate a trial. Upon clicking, they would see a white fixation circle for 500ms before the stimulus appeared.
For the choice options, an initial location for the blue and orange boxes was set by first calculating a location in the upper and lower dot strips that would have an equal mean number of dots. The initial locations of the boxes were then set by randomly offsetting each box from this equivalent location by ± 20 pixels. The choice options remained where they were for at least 800ms and up to 1200ms (uniformly randomly drawn). The switch time was determined prior to the start of a trial, as was the time at which the stimulus would shift. The stimuli changing (both strips) preceded the options changing by 250 ms. This ensured that there were no residual visual artifacts from the pre-shift dot arrays on apparent brightness within the shifted boxes.
The ways in which the stimuli and boxes changed were carefully manipulated to create the four switch-type conditions. The stimulus could shift left or right, as could the boxes. When the box shifted toward an area that was brighter at the onset of a trial (as in 
Figure 8C
), this was referred to as a "target" trial. When a box shifted toward an area that was darker at the onset of a trial (as in 
Figure 8
, bottom-right), this was referred to as a "lure" trial. However, whether or not this shift actually led to the final box location containing more dots depended on the interaction between the box shift and the stimulus shift that preceded it. When the stimulus moved more than the box did on target trials, that meant that the actual number of dots in the box would decrease from the beginning to the post-switch period, resulting in that option becoming worse with the shifts (target-worse). When the stimulus did not shift as much as the box on target trials, or shifted toward the box, then the shift resulted in more dots and the new box location was truly superior to its initial location (target-better). The same set-up was used for the lure switch conditions as well. When the stimulus moved further than the box on lure trials, it resulted in a higher number of dots in the box (lure-better) and when the stimulus did not move as far or moved in the opposite direction, it resulted in a lower number of dots (lure-worse) than at the beginning of a trial.
Here is where the logic of the experimental test of alternative-specific and alternative-general theories is borne out. If a participant considers stimulus information outside of the boxes before the switch, then they will believe that target trials have more dots in the box than at the beginning of a trial, and thus be more likely to select the target box because it moved to a previously-superior location. Conversely, they should also believe that lure trials will have fewer dots in the box at the end than at the beginning of the trial. Therefore, participants viewing target trials should be more accurate on target-better and lure-worse trials (where their beliefs about the location to which the box moved is correct) and less accurate on target-worse and lure-better trials (where their beliefs are contradicted by the stimulus switch). A participant who ignores information outside of the boxes, or ignores information that is presented prior to the switch, will not make these mistakes and will therefore have identical accuracy in target-better, target-worse, lure-better, lure-worse, and baseline conditions, and essentially identical accuracy in all of the two-switch conditions.
An alternative-specific representation will collect support only for the options that are available, meaning that they will never differentiate between a target and a lure box. Therefore, the DDM and RDM must predict that it does not matter whether a box moves to a location that was previously better or previously worse and accuracy should be exactly the same across conditions. Moreover, it does not matter if evidence is carried over from stage 1: the evidence that should tip their support against the lure or toward the target is not contained in the initial boxes, but outside them.
By contrast, an alternative-general representation will collect support for locations across the stimulus strip and possibly (and incorrectly) carry over this support when a new option shifts to that location. As a result, the GSR predicts that a person may select the target box and avoid the lure box because they move to locations that were better (worse, resp.).


Model fitting and comparison
The model estimation process for Study 2 was substantially simpler than that for Study 1, primarily because there was no response time data to fit. Instead, we can compare the models based purely on their predictions for accuracy. The predictions of the three models for Study 2 are based on the number of dots that appeared along each stimulus strip, and can be computed quite simply. The RDM predicts that the total accumulated evidence for each option is the sum of the number of dots that appeared in each box across the entirety of a trial. The DDM predicts that the accumulated evidence consists only of the difference in the number of dots between the two boxes. As before, we allow the RDM and DDM to carry over some evidence from stage 1 (before the boxes move) to stage 2 (after the boxes move). However, this has a minimal effect on their predictions, because both models predict that participants will limit themselves to considering only information for the options in front of them and not consider support for unavailable response locations.
In contrast, the GSR predicts that the total accumulated evidence for each option will be the sum of the number of dots that appeared across the entire trial at the final locations of the boxes. 
4
 For each of the models, we calculated the total or balance of evidence for each option that we would expect a participant to have collected at the end of the trial, referenced below as d for the number of dots contributing to the evidence in each model. This was repeated for every trial and participant in the experiment.
Because the trial ended after a pre-specified period of time, there is no need to estimate a threshold or non-decision time for any of the models. Instead, we estimated a drift scalar -which was a linear transformation on the accumulated evidence from each model -and a drift variability parameter. Note that drift variability and diffusion rates will have the exact same effect, adding a fixed variance to each accumulator based on the length of the trial -so we group them into a single variable. With the variability parameter, the accumulated evidence in each model followed a normal distribution with mean d • δ and standard deviation ν. The probability that a person is correct on a given trial is determined by the likelihood that the accumulator for A (whose value is s A = d A • δ + N(0, ν 2 )) is greater than the accumulator for B (whose value is s B = d B • δ + N(0, ν 2 )). In other words, accuracy will be determined by the probability that
s A > s B , or s A − s B > 0.
Expanding this latter equation makes it clear that what we are doing is evaluating the cumulative density of a normal distribution against zero. The probability that a person choose option A is therefore 1 − Φ(δ
• (d A − d B ), 2ν 2 , 0), where Φ(µ, σ 2 , x)
is the cumulative density function for the normal distribution.
For each model, we simply fit the values of δ and ν to the accuracy data from each participant, based on the objective values for d A and d B , using maximum likelihood estimation. These estimates were then used to generate accuracy predictions for each participant from each model. The aggregate results are shown below. As shown, increasing or decreasing the drift in the GSR based on the number of dots at the future location of the boxes yields superior accuracy predictions.


Results
As outlined above, both the RDM and DDM predict that there should be no differences in accuracy across the switch conditions. They predict slightly higher accuracy in the noswitch conditions, because the same boxes were on screen longer and thus provided a longer evidence accumulation period. There was also small variability in the stimuli presented in each condition, as with any probabilistically-generated stimulus. With around 1000 trials per condition total, this had a minimal effect. As a result, accuracy across switch conditions should not change according to the RDM and DDM. The only reason that there should be systematic accuracy differences across switch conditions is if participants were accumulating support from locations that were outside the boxes even before they changed location. Therefore, when they were fit to the data from Study 2, the DDM and RDM predicted the same accuracy across conditions. This is a strong qualitative prediction from both models that cannot be resolved by assuming evidence carry-over or optionbased priming; it is a fundamental test of the entire class of alternative-specific versus the class of alternative-general evidence accumulation models. Surprisingly, the stimulus coherence manipulation -the standard deviation of the number of dots in each row -did not have a substantial impact on accuracy (BF 0 1 = 53.10). We assumed that greater stimulus noise would result in lower accuracy, but the accuracy in the high-SD condition (M = .69, 95% HDI = [.68, .70]) was the same as the low-SD condition (M = .69, 95% HDI = 
[.68, .70]
). Fortunately, the conclusions we could draw from this experiment related to the models did not depend at all on the coherence manipulation. In fact, it allowed us to simplify the analyses by removing coherence as a covariate in subsequent analyses.
Accuracy in each of the conditions is shown in 
Table  3
. Accuracy differed across switch conditions (Bayesian ANOVA BF > 1,000) and across different number of boxes that moved (0, 1, or 2; BF > 1,000). All models were able 
Table 3
 Accuracy for each of the switch conditions for Experiment 2, broken down by how the options changed when a single box moved (1 change) or when both boxes moved (2 change). These results are shown displayed visually in 
Figure 3
.
Condition  
61.60, 67.59]
 to account for differences in accuracy resulting from differences in the number of boxes, as boxes that did not move allowed for evidence to accumulate across the entire trial. However, only alternative-general models can account for differences in switch conditions. The key finding that invalidates alternative-specific models -which should ignore information outside the boxes -is the variation in accuracy across switch conditions. Specifically, accuracy was higher in the target-better (M = .84, 95% HDI = 
[.82, .86
 This pattern of results is something we would only expect if participants were accumulating support for locations that were outside of their response set during the first phase, as the evidence after the switch occurred was identical across all these conditions. The results therefore conclusively support an alternative-general model of decision-making on this task.
Model fitting and comparison is at this point redundant with the empirical results. As outlined above, we are effectively fitting a probit choice model for each model, where predicted accuracy is a function of the mean number of dots that each model predicts should be incorporated into the decision, plus a choice variability parameter. The main difference is that the DDM and RDM only predict that dots within the boxes should be integrated into decisions, while the GSR predicts that dots across all locations along each stimulus strip should be integrated into decisions.
The results of model fitting are shown in 
Figure 9
. The GSR model fits the variations in accuracy across the switch conditions nearly perfectly 
(Figure 9
), as the predicted number of dots captures not only what was present at a location at the end of a trial but also how many dots were present at that location at the beginning of the trial. Conversely, both the RDM and DDM are unable to capture this variation across conditions. This is because they include no information about the primed location from stage 1 that contained additional dots, which was outside of the consideration set at the time it was presented.
In terms of quantitative fits, there were no participants whose accuracy data were better fit by the RDM or DDM than the GSR. This is largely because the lower limit of performance of the GSR is equal to that of the other two models, and there is no additional information (like response times) or fewer parameters (parsimony) that could tip the balance in favor of the alternative-specific models. In addition, no participant simultaneously had greater accuracy in target-worse than target-better and greater accuracy in the lure-better than lure-worse condition, which is the main pattern of data that would contradict the GSR model. From a falsification standpoint 
(Popper, 1963)
, this again provides support -via the lack of evidence against -the alternative-general GSR.


Discussion
The pair of studies covered here provide both quantitative and qualitative support for alternative-general representations of evidence during perceptual choices among shifting sets of options. The critical comparisons in Study 1 showed that performance, in terms of both accuracy and response time, was essentially the same whether or not a switch had occurred partway through the trial. This indicated that whether or not an option was included in the initial set of Pattern of mean accuracy (bars) for each switch-pattern condition of Experiment 2. Data are shown as bars, while predictions from the the RDM, DDM, and GSR models are shown as purple, yellow, and red lines, respectively. Error bars indicate one ±1 unit of standard error.
choice options had little to no bearing on whether (or how fast) it would be selected when it became available. Furthermore, a quantitative model comparison indicated that a clear majority of participants' behavior was best explained by an alternative-general over several competing alternativespecific models.
The paradigm used in Study 1 incentivized the use of alternative-general strategies by providing useful information from the beginning of every trial, prior to the option switch. Study 2 addressed this by introducing a paradigm where the optimal strategy was an alternative-specific one, ensuring that information presented at the start of a trial was not useful for choice. Yet this study found patterns of accuracy that could only be explained by participants using evidence that was outside of their choice options. Despite monetary incentives against such strategies, participants appeared to persist with an alternative-general strategy, creating the patterns of accuracy we observed in 
Figure 9
. Together, the experiments and modeling results comprehensively reject the class of alternative-specific models that have dominated dynamic decision-making for the last half-century, and suggest that future work should explore the different cognitive mechanisms implemented in alternative-general ones instead.
In some ways, our results are consistent with the existing literature on priming. Priming inherently creates support for an action or a decision th"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]