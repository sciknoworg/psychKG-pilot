You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
As humans, we can introspect on our own decisions and provide confidence estimates that reflect the likelihood of these decisions being correct 
(Koriat, 2006;
Mamassian, 2016;
Metcalfe & Shimamura, 1994)
. However, the computations behind confidence remain debated and thus revealing them is seen as one of the central medium-term goals for the field 
(Rahnev et al., 2022)
. Popular cognitive modelling frameworks including signal detection theory 
(Bang et al., 2019;
Boundy-Singer et al., 2023;
Green, D.M. and Swets, 1966;
Rausch et al., 2020;
Rausch & Zehetleitner, 2016;
Shekhar & Rahnev, 2018
, 2024a
, sequential sampling 
(Pleskac & Busemeyer, 2010;
Ratcliff & Rouder, 1998;
Ratcliff & Starns, 2009
, 2013
 and Bayesian inference 
(Adler & Ma, 2018b
, 2018a
Denison et al., 2018;
Fleming & Daw, 2017;
Li & Ma, 2020)
 have all attempted to capture the mechanisms underlying confidence generation and successfully provided insights into these processes. However, there are two major limitations associated with these models. Firstly, cognitive modelling has almost exclusively been applied to tasks involving only two choices 
(Rahnev, 2020)
. Despite notable attempts 
(Li & Ma, 2020)
, extending these models to more complex tasks involving multiple choices has proved difficult. For instance, classic SDT is fundamentally limited to 2-choices tasks and Bayesian computations become intractable for a large number of alternatives . Secondly, these models have only been applied to tasks involving simple visual stimuli such as Gabor patches and moving dots since traditional models of decision making are not image computable (i.e. they cannot process images involving complex features).
Therefore, it is still unclear how confidence is generated for decisions involving naturalistic stimuli and multiple alternatives.
One promising approach for addressing this question is to use convolutional neural networks (CNNs). The advantage of CNNs is that they are image computable models of vision that are capable of human-level object recognition 
(Bowers et al., 2023;
Wichmann & Geirhos, 2023)
. Further, they can easily be applied to decision making tasks with several alternatives. These features of CNNs can potentially be leveraged to model confidence mechanisms in tasks involving naturalistic images and multiple alternatives. However, although CNNs perform object classification tasks with high levels of accuracy, they cannot yet be considered as good models of human vision as it is unclear whether their decisionmaking mechanisms are similar to humans 
(Bowers et al., 2023;
Wichmann & Geirhos, 2023)
. Particularly, their behavior has not been extensively validated on human data for tasks involving simple psychological manipulations. Therefore, standard CNNs may not yet be suitable for modelling and inferring the processes occurring in humans.
To address these challenges, we recently developed a dynamic neural network model, called RTNet, which combines the image processing capabilities of CNNs with mechanisms drawn from empirically tested cognitive models 
(Rafiei et al., 2024)
. Specifically, RTNet has two modules: (1) a CNN with stochastic weights that generates noisy evidence at each time step and (2) an evidence accumulator which integrates this noisy evidence towards a threshold ( 
Figure 1A)
. Critically, RTNet reproduces several key signatures of human perceptual decisions and also predicts human RT, choice and confidence for novel, individual images.
Further, RTNet clearly outperformed other biologically inspired CNN architectures involving processes such as recurrence 
(BLNet;
Spoerer et al., 2020)
, parallel processing (CNet ; 
Iuzzolino et al., 2021)
, and resource-efficient processing (MSDNet; 
Huang et al., 2017)
.
Based on these findings, RTNet can be considered as a promising model of human decision making that can be further applied towards examining confidence mechanisms in humans. The first is a Bayesian neural network whose weights are stochastic such that at each processing step, a unique feedforward CNN is instantiated. As a result of the stochastic weights, the network processes the same image at each time step and generates noisy evidence through the activations of its output layer. The evidence for each choice option is then accumulated by an evidence accumulator module towards a pre-defined threshold. (B) Task. Subjects performed a digit discrimination task where they were presented with a noisy hand-written image of a digit between 1 and 8. The Subjects decided which digit was presented and then reported their confidence on a scale from 1-4. (C) The four experimental conditions. Task difficulty as well as speed-accuracy trade-off instructions were manipulated in a 2x2 factorial design. Task difficulty was adjusted by adding noise to the images, whereas speed-accuracy trade-off was manipulated by instructing subjects to focus on either accuracy or speed.
Previous studies have proposed a range of different strategies for computing confidence from the available perceptual evidence. For instance, confidence can be computed using all the evidence available for the perceptual decision. A popular example of this strategy is the Softmax transformation, which is currently the standard method for computing confidence in neural networks. Softmax normalizes evidence by accounting for its spread across all choice options 
(Guo et al., 2017)
. However, it has not been tested in humans using cognitive models. Another confidence strategy that also uses the spread across all choice options is to compute the entropy of the evidence distribution across choices (Entropy strategy; 
Li & Ma, 2020;
Spoerer et al., 2020)
. In contrast to Softmax and Entropy, other proposed strategies only utilize a subset of the available evidence. Specifically, confidence may be computed as the difference in evidence between the two most likely options (Top2Diff strategy; 
Li & Ma, 2020)
. Alternatively, another popular theory is that confidence only considers the evidence in favor of the chosen response (i.e., the "positive evidence"), neglecting all evidence against the choice (PE strategy; 
Koizumi et al., 2015;
Maniscalco et al., , 2021
Odegaard et al., 2018;
Peters et al., 2017)
.
Despite the fact that different confidence strategies have been proposed, there is currently little empirical work to distinguish between them. Perhaps the only attempt to quantitatively compare some of these strategies in the context of a multi-alternative task was made by 
Li & Ma (2020)
. The authors applied Bayesian modelling to a 3-choice task and showed that human confidence was most likely to be based on the difference in probability between the two most likely choices, rather than on the probability of the choice itself or the entropy associated with distribution of probabilities. However, Li & Ma's task was unique in that, unlike most of the literature, none of the alternatives were objectively correct on any given trial. Therefore, whether these findings would extend to tasks that feature more complex stimuli, higher number of alternatives, and the presence of an objective correct answer remains to be tested.
Here, we compared human confidence judgments in a complex, 8-choice digit discrimination task ( 
Figure 1B)
 with confidence ratings generated by RTNet using each of the four confidence strategies described above: Softmax, Entropy, Top2Diff, and PE. The task involved discriminating hand-written MNIST stimuli 
(Deng, 2012)
 and featured difficulty and speed-accuracy trade-off manipulations ( 
Figure 1C)
. We found that the Top2Diff strategy emerged as the best model in providing quantitative fits, reproducing qualitative signatures of human confidence, and predicting confidence for individual images. These results suggest that confidence in humans is computed as the difference in evidence between the top two options and establish CNNs as promising models for inferring mechanisms underlying human decisions in naturalistic settings.


Results
We compared the ability of four confidence generation strategies -Softmax, Entropy, Top2Diff, and PE -to fit human confidence in an 8-choice digit discrimination task. Each strategy was implemented within our recently-developed neural network model, RTNet 
(Rafiei et al., 2024)
. In our previous work, we had trained RTNet on the MNIST dataset and then fit the noise and boundary parameters (see Methods) to match human performance for difficulty and speed-accuracy trade-off manipulations. Here, we fit the same pre-trained RTNet model to the human confidence ratings using the activations in the last layer of RTNet. Specifically, for each confidence strategy, we fit a set of three confidence criteria that transform the network's continuous, raw confidence output into 4-point confidence ratings. We fit each confidence strategy to the data of each human subject separately. We then assessed the quality of model fits and the predictions of each strategy for the confidence associated with previously unseen individual images.


The Top2Diff strategy yields the best fit to human confidence
We first compared how well the four confidence strategies fit the observed data using AIC scores. Lower AIC scores indicate better fits to the data. AIC comparisons revealed that the Top2Diff strategy significantly outperformed all other strategies by generating the lowest AIC values 
(Figure 2A)
 points. These differences correspond to the Top2Diff strategy being, on average, 6.3 x 10 9 times more likely than PE, 4.2 x 10 12 times more likely than Softmax, and 1.0 x 10 37 times more likely than Entropy. In other words, the Top2Diff strategy provided a much better quantitative fit to the human data compared to any of the other three strategies. 
Figure 2
. Comparing the ability of Top2Diff, PE, Softmax, and Entropy to fit human confidence ratings. We examined both the quantitative and qualitative fits of the four confidence strategies (Top2Diff, PE, Softmax, and Entropy). Each strategy was built on the output of RTNet. A) The Top2Diff strategy outperformed all other confidence strategies by at least 45 AIC points. Error bars depict 95% confidence intervals for the difference in AIC scores between Top2Diff and each of the remaining strategies. B) Confidence decreases with task difficulty. All confidence strategies are able to reproduce this qualitative pattern, but the Top2Diff model provided the closest fits to the data. C) There is no significant change in confidence between the speed and accuracy focus conditions. All models incorrectly predict that confidence should be higher in the accuracy-compared to the speed-focus condition, but Top2Diff model again provides the closest fits to the data. D) Confidence increases with task performance for correct trials but decreases with task performance for incorrect trials giving rise to a folded-X pattern. All strategies except Entropy can reproduce this qualitative pattern, but the Top2Diff model provides the closest fits to the data. Error bars depict SEM. SSE, sum of squared errors (smaller values indicate better fits).
To gain further insight into the performance of the four confidence strategies, we compared how well each strategy can fit different qualitative patterns of confidence.
Specifically, we examined the effects of the two behavioral manipulations (task difficulty and speed-accuracy trade-off) on confidence. In addition, we examined the folded-X pattern which is popularly regarded as a critical signature of human confidence 
(Hangya et al., 2016;
Sanders et al., 2016)
.
First, we evaluated the effect of task difficulty manipulation on confidence. For humans, we found that confidence decreased from 3.50 in the easy to 3.14 in the difficult condition (t(59) = 11.768, p < 0.001; 
Figure 2B
). All strategies could replicate this general pattern (Top2Diff: easy = 3.49, difficult = 3.25; PE: easy = 3.39, difficult = 3.28; Softmax: easy = 3.48, difficult = 3.25; Entropy: easy = 3.27, difficult = 3.07). However, the Top2Diff strategy most closely matched the observed magnitude of change in confidence as measured by the average sum of squared errors across subjects (Top2Diff = 0.048; PE = 0.067; Softmax = 0.135; Entropy = 0.275). Further, these errors were significantly lower on average for the Top2Diff model compared to Softmax (t(59) = 2.103, p = 0.040) and Entropy (t(59) = 3.845, p < 0.001) but not PE (t(59) = 1.904, p = 0.062).
Second, we evaluated the effect of the speed-accuracy trade-off manipulation on confidenceClick or tap here to enter text.. In humans, there was no significant change in confidence from the speed-focus to the accuracy-focus condition (speed-focus = 3.31, accuracy-focus = 3.33; t(59) = 0.612, p = 0.543; 
Figure 2C
). However, all models showed an increase in confidence from the speed-focus to accuracy-focus condition (Top2Diff: 3.16 vs. 3.58; PE: 2.90 vs. 3.77; Softmax: 3.05 vs. 3.68; Entropy: 2.47 vs. 3.87). Once again, computing the average sum of squared errors across subjects showed that the Top2Diff model provided the closest match to human confidence (Top2Diff = 0.152; PE = 0.466; Softmax = 0472; Entropy = 1.420). These errors were significantly lower for the Top2Diff model compared to all other models (PE: t(59) = 9.755, p < 0.001; Softmax: t(59) = 3.006, p = 0.004; Entropy: t(59) = 7.339, p < 0.001).
Finally, we assessed whether our human subjects and models generated the "folded-X pattern" 
(Hangya et al., 2016;
Sanders et al., 2016)
where, as the task gets easier, confidence for correct trials increases but confidence for error trials decreases. Indeed, human confidence followed this folded-X pattern, such that the easy condition exhibited higher confidence for correct trials (difficult = 3.39, easy = 3.66; t(59) = 9.727, p < 0.001; 
Figure 2D
) but lower confidence for error trials (difficult = 2.74, easy = 2.68; t(59) = -2.095, p = 0.041). All strategies predicted that confidence for correct trials should be higher for the easy compared to the difficult condition (Top2Diff: difficult = 3.46, easy = 3.62; PE: difficult = 3.38, easy = 3.45; Softmax: difficult = 3.43, easy = 3.57; Entropy: difficult = 3.20, easy = 3.33). However, there was more variability in this pattern for error trials, with the Entropy and Softmax strategies failing to predict a clear difference in confidence for error trials (Top2Diff: difficult = 2.95, easy = 2.80; PE: difficult = 3.14, easy = 3.04; Softmax: difficult = 2.99, easy = 2.97; Entropy: difficult = 2.89, easy = 2.91). The Top2Diff model produced the most precise fits to the folded-X pattern observed in humans as measured by the sum of squared errors across subjects (Top2Diff = 0.208; PE = 0.554; Softmax = 0.499; Entropy = 0.699). Again, these errors were significantly lower on average for the Top2Diff model compared to all other models (PE: t(59) = 6.202, p < 0.001; Softmax: t(59) = 3.703, p < 0.001; Entropy: t(59) = 5.918, p < 0.001). Overall, as may be expected from the AIC fits, the Top2Diff model produced the most consistently accurate matches to the observed patterns of human confidence across all three qualitative patterns.
The Top2Diff strategy yields the best predictions of confidence for individual images Unlike traditional cognitive models which are not image computable, CNNs can generate predictions of confidence at the level of individual images. Therefore, we used this ability of the CNNs to test which confidence strategy generated the best predictions of confidence for novel, individual images. We first compared the observed and predicted distributions of confidence by plotting histograms of average confidence for all 480 unique images. The average human confidence ratings for individual images showed a continuous, unimodal distribution with a peak near the middle of the range ( 
Figure 3A)
. Similarly, the Top2Diff strategy also generated a continuous, unimodal distribution that also peaked near the middle of the range. In contrast, the PE, Softmax, and Entropy strategies showed deviations from this pattern. While the PE strategy also showed a unimodal distribution, it predicted a peak for the highest values of the range.
Most strikingly, Softmax and Entropy both generated strong bimodal distributions. In particular, the Entropy model, generated no confidence values that fell within the middle of the observed range (between 2.98 and 3.68). Overall, the Top2Diff model generated the closest match to the observed distribution of image-by-image average confidence. The distributions of confidence were unimodal and continuous for humans, Top2Diff, and PE. However, Softmax and Entropy generated strong bimodal distributions. (B) We correlated image-by-image predictions of average group confidence with observed average confidence in humans. The Top2Diff model's predictions produced the highest correlations with group confidence in humans. Dots represent the 480 unique images. (C) Comparing the predictions of group confidence between models from panel B. The correlations between model predictions and human confidence were significantly higher for Top2Diff compared to all other models. (D) The models' image-by-image predictions of confidence for individual subjects. The predictions of the Top2Diff model showed significantly higher correlations with individual subject confidence compared to all other strategies. Dots represent individual subjects. Error bars show SEM. *p<0.05; ***p<0.001.
We next turned to the main question of assessing the models' ability to predict human confidence for individual images. We correlated the image-by-image confidence value predicted by each strategy with the average image-by-image confidence values across the group of human subjects. All strategies yielded correlations that were significantly greater than zero (Top2Diff: r = 0.469; PE: r = 0.341, Softmax: r = 0.199, Entropy: r = 0.161; all p's < 0.001; 
Figure 3B
). Critically, the Top2Diff strategy produced significantly higher correlations than all other strategies (PE: z(479) = 2.37, p = 0.018; Softmax: z(479) = 4.75, p < 0.001; Entropy: z(479) = 5.35; p < 0.001, test for comparing r-values; 
Figure   3C
). Notably, the Softmax and Entropy models produced substantially lower correlations, likely due to their non-human-like bimodal distribution of average imageby-image confidence. Overall, the Top2Diff strategy generated image-by-image predictions for confidence that provided the best match to image-by-image average human confidence.
Beyond assessing average confidence across the group, we also examined how well each model could predict image-by-image confidence for individual subjects. We simulated the models separately for each subject using the confidence criteria estimated from model fitting to each individual. Then, we correlated observed and predicted image-by-image confidence for each subject. Again, all confidence strategies yielded correlations that were significantly greater than zero (Top2Diff: r = 0.18, PE: r = .073, Softmax: r = .135, Entropy: r = .064; all p's < 0.001; 
Figure 3D
). However, the Top2Diff strategy yielded significantly higher correlations compared to all other strategies (PE: t(59) = 12.95, p < 0.001, Softmax: t(59) = 5.38, p < 0.001, Entropy: t(59) = 12.367, p < 0.001, pairwise t-tests). We also assessed how well individual confidence could be predicted by the rest of the group's confidence -an estimate of the noise ceiling or the upper bound for model performances given inter-subject variability. The Top2Diff strategy's predictions were at 45.1% of the noise ceiling, whereas the other models' predictions were much lower (PE: 18.0%, Softmax: 33.2% and Entropy: 15.8%).
Together, these findings show that the Top2Diff strategy provides the best image-byimage predictions of confidence for both the group and individual subjects.
Only the Top2Diff strategy predicts group confidence in humans better than individual subjects Individual subjects can themselves serve as models of group behavior. Here, we tested how well our models can predict group behavior compared to individual subjects. We correlated the image-by-image confidence predictions of models as well as the imageby-image confidence of individual subjects to the corresponding average confidence of the group. The difference between the model-to-group correlations and subject-togroup correlations provide a measure of how well the model predicts the group over individual subjects. We found that only the Top2Diff model was able to significantly predict group confidence better than individuals (Î” = 0.050, t(59) = 2.481, p = 0.016; 
Figure 4A
). In fact, all other models performed significantly worse than the individual subjects in predicting group confidence (PE: Î” = -0.215, t(59) = -10.566, p < 0.001; Softmax: Î” = -0.067, t(59) = -2.602, p = 0.012; Entropy: Î” = -0.229, t(59) = -9.302, p < 0.001). Further, the Top2Diff model outperformed 61.7% of individual subjects in predicting the group, whereas the PE, Softmax and Entropy models only performed better than 10%, 41.6%, and 15% of individual subjects, respectively. These findings show that Top2Diff was the only strategy that served as a better model for predicting group confidence compared to individual human subjects. Models' predictions of average group confidence compared to predictions from individual subjects. Only the Top2Diff model was significantly better than individual subjects at predicting the group (better than 37 out of 60 subjects). Dots represent individual subjects. Error bars show SEM. **p<0.01; ***p<0.001. (B) The relationship between a subject's similarity to the group and their similarity to the model. Only for the Top2Diff model, there was a significant relationship between these two quantities, indicating that the model successfully captured the data from subjects who were the most similar to the group data. Dots represent individual subjects.
Subjects more similar to the group mean are better predicted by the Top2Diff strategy 
Figure 4A
 revealed substantial variability in how similar subjects were to the group mean. Subjects who were worse in predicting the group mean may have had especially strong biases or simply provided noisy data. If so, good models of human behavior are likely to be better predictors for individuals who are more similar to the group than for individuals who are most different from the group. Indeed, we found that subjects whose confidence had a higher image-by-image correlation with the group mean were also better predicted by the Top2Diff model (r = 0.483, p < 0.001; 
Figure 4B
). However, for all other strategies, there was no significant relationship between the subject's similarity to the group and the predictions of each strategy (PE: r = 0.074, p = 0.57; Softmax: r = 0.137, p = 0.30; Entropy: r = 0.034, p = 0.80), suggesting that these models were unable to encapsulate the relationship of individual subjects to the group.
Other CNN architectures perform substantially worse than RTNet and fail to generate consistent support for any confidence strategy
Our results demonstrate that the Top2Diff strategy clearly outperforms all other strategies when instantiated in RTNet. However, it is not clear how the four strategies would perform within other CNN architectures. Testing across architectures would enable us to understand how well other CNNs architectures can fit human data relative to RTNet and whether the same confidence strategies are consistently supported across model architectures. For conciseness, we only report three analyses: a) AIC comparisons, b) fits to the folded-X pattern and c) image-by-image predictions of individual subject confidence. These three analyses are representative of the three main aspects of our model assessment because they test models on their ability to 1) quantitatively fit the data, 2) capture qualitative patterns in the data, and 3) predict human responses for individual images. Similar to what was done for RTNet, we used three other CNN architectures -MSDNet 
(Huang et al., 2017)
, BLNet 
(Spoerer et al., 2020)
 and CNet 
(Iuzzolino et al., 2021)
 -that were previously trained on MNIST and fit to human choice data 
(Rafiei et al., 2024)
. We then fit subject-specific confidence criteria to these models to obtain confidence on a 4-point scale.
We first assessed the models' fits to the data by computing average AIC values across the 60 subjects for the 16 models (4 architectures x 4 confidence strategies). We found that all four RTNet models generated substantially lower AIC values compared to the remaining 12 models (3 architectures x 4 confidence strategies) ( 
Figure 5A
, 
Supplementary Table 1
). The Top2Diff strategy instantiated within RTNet remained the best model, with other strategies instantiated within RTNet falling between 45-170 AIC points behind it. In contrast, the four confidence strategies instantiated within MSDNet were 430-778 points worse than the RTNet-Top2Diff model. BLNet and CNet produced even worse fits with strategies instantiated in BLNet falling behind the RTNet-Top2Diff model by 2860-3087 AIC points and strategies within CNet falling behind RTNet-Top2Diff by 3245-3528 AIC points. All AIC comparisons were significant as assessed by computing bootstrapped 95% confidence intervals. These results corroborate our previous findings that RTNet provides the best fits to human data 
(Rafiei et al., 2024)
. The poor fits to data by other CNN architectures suggest that these architectures are unable to simultaneously fit the observed patterns of choices and confidence. However, since fitting models to choices and confidence was done in separate steps, it is important to rule out the possibility that these poor fits are due to a failure of models to fit confidence alone. Therefore, we correlated each subject's average confidence to the model's average confidence for that subject. We found that all models and strategies produced very high correlations (median r = 0.934, all r's > 0.75, all p's < 0.001), confirming that the fitting procedure was successful ( 
Figure 5B)
. Instead, the poor fits for MSDNet, BLNet, and CNet were likely a result of the failure of these architectures to fit the pattern of human choices underlying confidence, in spite of being able to fit the overall confidence for each individual subject well.
Having established that RTNet provides the best overall fits to human data, we proceeded to look at how different confidence strategies perform within other CNN architectures. We examined the architectures in the order of the quality of their overall fit, starting with MSDNet and proceeding to BLNet and CNet.  
Figure 6A
).
Further, all models except PE were able to produce the folded-X pattern, but the Top2Diff model produced the closest fits to the observed pattern as measured by the average sum of squared errors across subjects (Top2Diff = 0.361; PE = 0.553; Softmax = 0.532; Entropy = 0.374). The errors produced by Top2Diff were significantly lower compared to the PE and Softmax models but not the Entropy model (PE: t(59) = 3.557, p < 0.001; Softmax: t(59) = 4.524, p < 0.001; Entropy: t(59) = 0.440, p = 0.661). Surprisingly, despite the PE strategy providing the worst AIC scores and folded-X pattern predictions, it produced the best predictions of individual confidence (Top2Diff: r = 0.122; PE: r = 0.169; Softmax: r = 0.118; Entropy = 0.118; all p's < 0.001) and all comparisons of correlations between PE and other models were significant (Top2Diff: t(59) = 6.01, p < 0.001; Softmax: t(59) = 5.90, p < 0.001; Entropy: t(59) = 7.29, p < 0.001). Second, we examined the results for BLNet, which was the third best-performing CNN architecture far behind both RTNet and MSDNet. Quantitative comparisons showed the PE, Entropy, and Top2Diff models having similar performance and far outperforming Softmax 
(Figure 6B)
 comparisons showed that all models were able to fit the observed folded-X pattern, with the PE, Top2Diff, and Entropy models producing similar fits and Softmax producing the worst fit (Top2Diff = 0.446; PE = 0.439; Softmax = 0.596; Entropy = 0.466). Indeed, the PE model, which produced the lowest errors, was only significantly better compared to Softmax but not to Top2Diff and Entropy (Top2Diff: t(59) = 0.149, p = 0.882; Softmax: t(59) = 2.037, p = 0.046; Entropy: t(59) = 0.460, p = 0.647). Image-level predictions were relatively similar across strategies, with the Top2Diff model producing highest correlations (Top2Diff: r = 0.20; PE: r = 0.184; Softmax: r = 0.191; Entropy = 0.196; all p's < 0.001), which were significantly better compared to PE (t(59) = 4.71; p < 0.001) but not compared to Softmax (t(59) = 1.52; p = 0.135) or Entropy (t(59) = 1.372; p = 0.175).
Finally, we examined the results for CNet, which was the worst-performing CNN architecture. Quantitative comparisons showed that the Entropy model outperformed  
Figure 6C
). All models except PE were able to predict the folded-X pattern (the PE model predicted a decrease in confidence from the difficult to easy condition for both correct and error trials). Among the other models, Entropy produced the closest fits to observed pattern (Top2Diff = 0.454; Softmax = 0.737; Entropy = 0.446). The fits yielded by Entropy were only significantly better compared to the Softmax model but not the Top2Diff model (Top2Diff: t(59) = 0.177, p = 0.860; Softmax: t(59) = 3.217, p = 0.002).
Finally, the Top2Diff, Entropy, and Softmax models produced similar image-by-image correlations with human confidence (Top2Diff: r = 0.177; PE: r = 0.129; Softmax: r = 0.168; Entropy = 0.170; all p's < 0.001). Nevertheless, the predictions of Top2Diff, the best performing model on this test, were significantly better than those made by PE (t(59) = 5.60; p < 0.001) and Softmax (t(59) = 2.23; p = 0.029) but not better than those made by the Entropy model (t(59) = 1.846; p = 0.07).
Overall, these results highlight the fact that the performance of different confidence strategies strongly depends on the CNN architecture. Importantly, only within the best performing CNN architecture, RTNet, we find a single confidence strategy (Top2Diff) that consistently gives the best qualitative and quantitative fits as well the best imagelevel predictions of confidence. For all other architectures, no single strategy was found to simultaneously produce the best fits to data as well as the best image-by-image predictions of confidence. Nevertheless, the Top2Diff strategy was consistently among the best models for all architectures.


Discussion
We instantiated four different confidence strategies in a recently-developed neural network model of perceptual decisions, RTNet. We found that human confidence is best described by a strategy that computes confidence as the difference in evidence between the top-two choices (Top2Diff). The Top2Diff strategy not only produced the best quantitative fits to human data but also reproduced all qualitative signatures of human confidence and generated the best image-by-image predictions of confidence ratings. These findings demonstrate that Softmax, which is currently the dominant way of obtaining confidence from neural network models, is not appropriate for deriving human-like confidence.
Our findings suggest that confidence considers only a specific subset of the available evidence, rather than the entire distribution of evidence. Our results are in line with previous findings from 
Li & Ma (2020)
 and 
Xue et al. (2024)
who found that the Top2Diff strategy provided the best description of human confidence. Importantly, both of these previous papers employed 3-choice tasks with relatively simple stimuli that consisted of colored dots. Our study extends these findings to more complex stimuli and tasks that involve many more choice options.
These findings add to a growing body of evidence against the notion that the positive evidence heuristic underlies human confidence. In spite of its popularity 
(Koizumi et al., 2015;
Maniscalco et al., , 2021
Peters et al., 2017)
, the PE mechanism has been challenged by numerous recent findings from cognitive as well as neural network modelling 
(Rausch et al., 2020;
Shekhar & Rahnev, 2024a
, 2024b
Webb et al., 2023)
. These studies have shown that the PE model consistently produces some of the worst fits to human data compared to other existing models and that the PE mechanism is, in fact, not necessary to account for behavioral effects that were previously assumed to be its signatures 
(Koizumi et al., 2015;
.
Our results also challenge Softmax as a model of human confidence. Softmax is used in most neural network models to derive the confidence of the network 
(Guo et al., 2017)
. This is a natural choice due to the simplicity of the method and its desirable mathematical properties. Consequently, the few papers that have compared confidence in neural networks vs. humans have also used Softmax 
(Rafiei et al., 2024;
Shekhar & Rahnev, 2024b;
Webb et al., 2023)
 but never compared these results to other confidence strategies.
Our results demonstrate that Softmax is inappropriate for the purposes of building models of human confidence.
Finally, we also demonstrate that the Entropy strategy is also not a good model of human confidence. This finding is in line with the results of 
(Li & Ma, 2020)
 for simpler 3-choice tasks. One reason for why both Entropy and Softmax do not perform well may be that both strategies derive confidence ratings using the entire distribution of sensory evidence.
However, while this type of computation may not match well what humans do, it must be noted that it produces more informative confidence ratings. Therefore, Entropy and Softmax may indeed be better suited to derive confidence from neural networks when the goal is to maximize insight into the network's performance rather than model human behavior.
A prominent theory of confidence generation that we could not test here is the Bayesian confidence hypothesis, which postulates that confidence is computed optimally as the probability of being correct 
(Aitchison et al., 2015;
Hangya et al., 2016;
Sanders et al., 2016)
.
Recent studies have strongly challenged this assumption by showing that models that derive confidence directly from perceptual evidence substantially outperform models which use probability computations 
(Adler & Ma, 2018b
, 2018a
Denison et al., 2018;
Li & Ma, 2020;
Rausch et al., 2020;
Shekhar & Rahnev, 2024a;
Xue et al., 2023)
. However, in the current study, we could not directly test the Bayesian confidence hypothesis since this would require knowledge of the probability distributions governing the evidence generated by the networks. Most cognitive models include explicit assumptions about the distributions of sensory evidence, which enable us to compute the posterior probabilities associated with choices. In contrast, the statistical properties of the evidence generated by neural networks performing tasks using complex stimuli and involving multiple choices are not well understood and are unlikely to be well described by simple assumptions used in cognitive models. Therefore, in order to test Bayesian confidence hypothesis using neural networks, one needs to first model the internal distributions of evidence generated by these networks and also make the assumption that humans can derive and use these distributions to correctly compute the probability of being correct.
Why does human confidence resemble the Top2Diff strategy despite the suboptimality of its underlying computation? One possibility is that the Top2Diff strategy may be preferred over more optimal strategies that use all available information since these optimal strategies can require very complex computations. Particularly, in situations where there are many alternatives and the possible choices are not clearly defined, Bayesian and Entropy computations would become intractable as they require the system to compute probabilities associated with all possible choices 
(Rahnev, 2020)
. On the other hand, the PE heuristic minimizes computational effort but neglects too much available information, thus producing confidence that may not be sufficiently informative about one's perceptual accuracy. Therefore, the Top2Diff strategy may have emerged as a viable solution to the trade-off between maximizing the information content of confidence ratings and minimizing computational costs.
We observed strikingly large differences in quantitative model fits between the four CNN architectures we tested (RTNet, MSDNet, BLNet and CNet). It is surprising that BLNet and CNet, which incorporate biological mechanisms of recurrence 
(Goetschalckx et al., 2023;
Kar et al., 2019;
Kar & DiCarlo, 2021;
Koivisto et al., 2011;
Spoerer et al., 2020)
 and parallel processing 
(Iuzzolino et al., 2021)
, performed substantially worse than RTNet and MSDNet. While RTNet is expected to perform well as it implements an empirically validated mechanism of evidence accumulation 
(Forstmann et al., 2016;
Ratcliff & McKoon, 2008;
Ratcliff & Rouder, 1998;
Ratcliff & Starns, 2013)
, it is less clear why MSDNet -which is not built based on biologically relevant mechanisms -outperformed BLNet and CNet. One thing to note is that MSDNet and RTNet are smaller networks compared to CNet and BLNet.
Indeed, CNet uses the deeper ResNet-18 architecture, and BLNet is a seven-layer network that gets unrolled across eight time steps into a deeper network. It may be that very deep networks trained on simple tasks like MNIST make for poor models of human perceptual decision making. Another possibility that explains these AIC differences is that the biological mechanisms implemented by BLNet and CNet may be unnecessary to explain human perceptual decisions for relatively easy tasks such as digit discrimination, compared to more complex perceptual tasks, such as scene discrimination 
(Goetschalckx et al., 2023)
, or specific manipulations, such as masking 
(Tang et al., 2018)
. On the other hand, the evidence accumulation mechanism described by RTNet may be more important for decision making that involves simple perceptual discrimination.
One limitation of our study was the inability to simultaneously fit both choice and confidence data to the neural network models. Since neural networks have parameters in the order of millions, it becomes computationally intractable to simultaneously tune all these parameters and fit the observed human data. Therefore, we used a general-purpose training method to teach the models to perform the task and then optimized two critical parameters that controlled the models' performance levels. Even then, we fit these two parameters to the average human data (instead of each subject separately) because all CNN models required significant computational resources and processing time to generate responses even on a single trial. Instead, the only parameters we could fit on a subject-bysubject basis using traditional exhaustive search standard in cognitive models 
(Acerbi & Ma, 2017;
Nelder & Mead, 1965)
 were the three confidence criteria. Future work should focus on developing methods that would allow traditional maximum likelihood estimation to be applied to neural network models of decision making.
In conclusion, we show that a novel neural network model of human decision making, RTNet, can be used to infer the mechanisms of confidence generation in humans for multichoice tasks with relatively complex stimuli. Particularly, the strategy which computes confidence as the difference in evidence between the top-two choices (Top2Diff) emerged as the clear winner over other strategies that base confidence on the whole distribution of evidence or neglect choice-incongruent evidence. Our results highlights neural network models as promising tools to test hypotheses about the mechanisms of decision-making under naturalistic settings.


Methods


Stimuli and task
Sixty-four subjects (31 female; age, 18-32 years) with normal or corrected-to-normal vision participated in the experiment. All subjects signed informed consent and were compensated monetarily for their participation. The protocol was approved by the Georgia Institute of Technology Institutional Review Board (protocol no. H15308). This experiment has been previously reported in 
Rafiei et al. (2024)
 and all the details can be found in the original publication. Below, we briefly describe the experimental design.
Subjects performed a digit discrimination task with eight choices and reported confidence. On each trial, a fixation cross was presented for 500-1000 ms followed the presentation of a stimulus for 300 ms. The stimulus was a digit between 1-8 and superimposed with noise. Subjects reported the perceived digit by pressing a key between 1-8 and subsequently reported their confidence on a scale from 1-4 via another key press (where 1 corresponds to lowest confidence and 4 corresponds to highest confidence). The response screens stayed on until subjects made a response.
We manipulated task difficulty as well as speed-accuracy trade-off (SAT) instructions.
Task difficulty was manipulated by corrupting the pixels with uniform noise. Easier stimuli contained lower pixel noise on average. SAT was manipulated by instructing subjects to make their responses as fast or as accurate as possible. Trials containing easy and difficult stimuli were interleaved whereas trials with different SAT instructions were blocked and presented alternately.
The stimuli were obtained from the publicly available MNIST dataset containing 60,000 training images and 10,000 testing images. The stimuli shown to subjects were taken only from the testing set to ensure that both subjects and networks were tested on novel images. 480 images were randomly selected and evenly distributed across the four experimental conditions. Subjects completed 4 blocks of 60 trials each (960 trials in total as each of the 480 images was presented twice). The experiment was designed in the MATLAB v.2020b environment using Psychtoolbox 3 
(Brainard, 1997)
. The stimuli were presented on a 21.5-inch Dell P2217H monitor (1,920 Ã— 1,080 pixel resolution, 60
Hz refresh rate). The subjects were seated 60 cm away from the screen and provided their responses using the keyboard.


Behavioral analyses
We excluded subjects based on preregistered criteria (https://osf.io/kmraq). These criteria resulted in the exclusion of four subjects in total (two subjects for not following SAT instructions and two subjects for ceiling effects on confidence). For each individual subject, we computed average confidence as a function of task difficulty and SAT condition to assess how these factors affect confidence. We also assessed confidence ratings for the folded-X pattern, which is popularly regarded to be a signature of human confidence. The folded-X pattern is obtained by computing confidence as a function of task difficulty separately for correct and error trials.


Network architecture and implementation
We first briefly describe the four network architectures and their implementation. We trained 60 instances of each network architecture using different random initializations of the network's weights to allow for individual differences in learning. All networks were implemented in Python (version 3.10.11).


RTNet
The RTNet architecture consists of two modules. The first module is a Bayesian neural network (BNN) whose weights are learned as posterior probability distributions rather than point estimates 
(Jospin et al., 2020)
. Due to stochasticity in the BNN's weights, a unique feedforward network gets sampled at each time step and repeated processing of the same image generates variable activations in the network's final layer. The second module consists of an evidence accumulator that receives these noisy activations and integrates the evidence towards a pre-defined threshold. Evidence is accumulated independently for each choice option and the model chooses the option for which the evidence first hits the threshold. Response time corresponds to the number of evidence samples that were required to reach the threshold. The network was implemented within the AlexNet architecture 
(Krizhevsky et al., 2012)
 consisting of five convolutional layers and three fully connected layers.


CNet
The Cascaded parallel network (CNet) uses parallel processing and introduces propagation delays between convolutional layers using skip connections 
(Iuzzolino et al., 2021)
. Even though all the network's layers process input parallelly at any given time, propagation delays cause early layers to receive input faster and achieve stable activations sooner. For the same amount of processing time, the later layers receive only partial input from the earlier layers and take more processing steps to achieve stabilization of their output. As a result, simple features propagate faster through the network while more complex features require greater processing time, naturally leading to the trade-off between processing speed and stimulus complexity found in humans.
The network's decision can be generated by setting a threshold at the output layer and the decision time is determined by the number of processing steps that are required to reach the threshold. The network was implemented within the ResNet-18 architecture since it requires network architectures with skip connections 
(He et al., 2015)
. ResNet-18 contains 17 convolutional layers organized in eight residual blocks and one fully connected output layer with a softmax activation function that generates the decision.


BLNet
BLNet is a recurrent convolutional neural network (RCNN) that uses bottom-up and lateral connections to recurrently feed each layer's input back to itself 
(Spoerer et al., 2020)
. Therefore, each layer receives feedforward input from the previous convolutional layer as well as recurrent input from itself in the form of its own activations at the previous time step. Time steps are defined in terms of feedforward sweeps and after each feedforward sweep, the network's readout is evaluated. If the readout crosses a predefined threshold, the network chooses the option with the highest readout. The number of feedforward sweeps preceding the decision determines the response time.
Recurrent processing is a biologically inspired mechanism that can dynamically adjust a network's computational power. For instance, setting a higher threshold will lead to the network undergoing a larger number of feedforward and recurrent computations, effectively resulting in a deeper network being unrolled. We implemented BLNet as described in the original publication that introduced this network 
(Spoerer et al., 2020)
, which consists of seven convolutional layers and a final readout layer with a softmax activation function. The network was unrolled across time for a maximum of eight time steps.


MSDNet
MSDNet uses a standard feedforward CNN with early-exit classifiers after each convolutional layer 
(Huang et al., 2017)
. The classifiers compute the evidence at each layer using a softmax function and if the evidence crosses a pre-defined threshold, the network stops processing input and generates a response based on that layer's output.
The decision corresponds to the choice option that generates the highest softmax value and the response time corresponds to the layer at which the decision was generated. As in the original publication, MSDNet was implemented with the AlexNet architecture with five convolutional layers and three fully connected layers.


Fitting CNN models to human choices
Our central goal was to compare model fits to human confidence across network architectures and confidence strategies. However, due to computational constraints, it is not possible to simultaneously fit both choice and confidence data to the models via traditional maximum likelihood estimation (MLE) methods. Therefore, we followed a step-wise approach where we 1) trained the models on the general task performed by humans, 2) fit the models to human choices by optimizing two additional parameters (noise in the stimuli and decision threshold) that gave us the closest match to the average human performance, and 3) generated confidence using each of the four strategies and fit three confidence criteria separately for each individual human subject in order to create subject-specific fits for the confidence ratings. In Step 1, we trained all four networks on the MNIST dataset to classify handwritten digits. All the networks were trained to achieve an accuracy of >97% on the training set. The details of the training procedure can be found in the original publication. Training the models and tuning the choice parameters (Steps 1 and 2) were previously done by 
Rafiei et al. (2024)
. In the current study, we simulated these pre-trained and tuned networks to generate confidence using four different strategies (Step 3). Importantly, in Step 2, we fit two choice parameters which were the same for all human subjects, but in Step 3, we fit individual confidence criteria that differed from subject to subject. Below, we describe the procedures for Steps 2 and 3 in more detail.
Fitting the pre-trained models to human choices (Step 2 of fitting)
To fit the models to human choices, we matched all the models' accuracy levels to those observed in humans by tuning two additional parameters. We separately matched the network's performance in each experimental condition to the average accuracy observed for that condition. To match accuracies across the difficulty levels, we adjusted the noise in the images. Images with higher levels of noise lead to lower accuracy. To mimic the effect of the speed-accuracy trade-off manipulation on accuracy, we adjusted the networks' thresholds. A higher threshold leads to longer processing times and higher accuracies.
We estimated the parameters using a coarse search followed by a fine-grained search and chose the parameters that gave us the closest match to average human accuracy for each condition. For RTNet, the best match to human accuracy was achieved for noise levels of 2.1 for easy images and 4.1 for difficult images, and threshold values of 3 for the speed condition and 6 for the accuracy condition. For CNet, the closest match was achieved for noise levels of 1.42 for easy images and 1.83 for difficult images and threshold values of 0.83 for the speed condition and 0.9 for the accuracy condition and.
For BLNet, the best match to human accuracy was obtained when the noise levels were set to 0.55 for easy images and 1.2 for difficult images and when thresholds were set to 0.4 for the speed condition and 0.95 for the accuracy focus condition. For MSDNet, the closest match was achieved for noise levels of 1.9 for easy images and 3.0 for difficult images and for threshold values of 0.58 for the speed condition and 0.82 for the accuracy condition.
In addition to the procedure above where we fit the two parameters to optimize the fit to average accuracy in each condition, we performed a separate fitting procedure that fit each network's responses to the entire 8x8 response matrix using the maximum likelihood estimation (MLE). Neither methods provided significantly better overall AIC scores compared to the other (t(15) = 0.891; p = 0.387, two-sided paired t-test on model AIC scores). Therefore, we only report findings from the first method here and report findings from the second method in the Supplementary 
(Supplementary Methods;   Supplementary Figures 1-4; Supplementary Table 2
).
Fitting the model responses to human confidence (Step 3 of fitting)


Implementing different confidence strategies
We implemented four confidence strategies within each network architecture: PE, Top2Diff, Softmax and Entropy. We first obtained the logits or raw activations from the networks' output layer for each of the eight choice options, = [ 1 , 2 , â€¦ 8 ]. The positive evidence hypothesis posits that confidence is based on the evidence in favor of the chosen option. Therefore, under the PE model, confidence was computed as the activation associated with the chosen response i.e. = max( ). According to the Top2Diff model, confidence was computed as the difference in activation between the chosen response and the response that generated the second highest evidence, such that 2 = max( ) âˆ’ 2( )where max2( ) is the second highest value in the vector . Since the activations are unbounded, they can take arbitrarily large values which can be problematic when fitting models to the data. Therefore, for both the PE and Top2Diff models, the raw activation scores across trials were normalized by their standard deviations to restrict their ranges. For the Softmax model, we first applied the softmax transformation to the raw activations, such that = âˆ‘ 8


=1
, where is the softmax value associated with the â„Ž choice. The softmax function transforms the activations into probability scores where âˆˆ [0,1] and âˆ‘ = 1 8 =1
. Softmax confidence was then obtained as = max( ). Finally, for the entropy model, the entropy,
, associated with the output distribution was computed based on the probability scores obtained from the softmax transformation as = âˆ’ âˆ‘ * log( )
8 =1
.
Since confidence should increase with decreasing uncertainty or entropy, we defined confidence as the negative of entropy such that = âˆ’ .


Performing model fitting
Human confidence responses were obtained on a four-point scale. However, the models' confidence was derived from their raw and continuous activations, making it difficult to directly compare the models' confidence to humans. Therefore, we ran a model fitting procedure to fit the models' confidence responses separately for each individual human subject. We defined a set of three confidence criteria, = [ 1 , 2 , 3 ], for each subject that would transform the continuous confidence responses into subject-specific 4-point confidence, thus allowing direct comparisons with humans.
Model fitting was based a maximum likelihood estimation (MLE) procedure that aimed to find a set of parameters that maximized the log-likelihood of the model associated with the full probability distribution of responses. The probability distribution of responses was computed as an 8 x 8 x 4 response matrix representing the eight stimulus classes and 8 x 4 responses (eight choices with four confidence ratings for each choice). For each model, we pooled the responses across the 60 model instances and binned them according to their associated stimulus, choice, and confidence to obtain the response probability matrix. Log-likelihood was computed as:
âˆ‘ log( ) Ã— , ,
, where refers to the response probability (computed from the model's simulated responses) and refers to the number of trials associated with stimulus class = {1,2, â€¦ , 8}, choice = {1,2, â€¦ , 8} and, confidence response = {1,2,3,4}. The parameter search was conducted using the Bayesian Adaptive Direct search (BADS) toolbox 
(Acerbi & Ma, 2017)
. Quantitative model comparisons were made using a goodness-of-fit measure called the Akaike information criterion (AIC) which is computed as = âˆ’2 + 2 , where refers to the log-likelihood associated with the maximum-likelihood estimates of the parameters and refers to the number of model parameters. For all models, was fixed as three (corresponding to the three confidence criteria) because we were only fitting the model's responses to confidence.


Generating the model's confidence responses for individual subjects
After obtaining the MLE of the confidence criteria for each individual subject, we used these parameters to simulate subject-specific confidence ratings. To generate these ratings, we first derived raw confidence for each trial by applying the confidence generation strategies defined above. To generate subject-specific confidence, we applied that subject's estimated confidence criteria to the raw confidence values after aggregating the networks' output across all 60 instances. The confidence criteria were defined as = [ 0 , 1 , 2 , 3, , 4 ], where 0 = âˆ’âˆž and 4 = âˆž, and were applied to the raw confidence values ( ) such that falling within the interval [ âˆ’1 , )
resulted in a confidence rating of where âˆˆ {1, 2, 3, 4}.
Qualitatively assessing models' confidence responses and predictions
We analyzed whether the models' confidence ratings reproduced the qualitative patterns of confidence observed in humans. Specifically, we simulated each subject's confidence separately using their individual parameter estimates and analyzed whether these simulated confidence ratings reproduced the observed effects of the difficulty manipulation, the SAT manipulation, and the folded-X pattern. Mean confidence for each condition was computed separately for each simulated subject and then averaged across subjects.
We further tested how well models could predict human confidence for novel, individual images at the level of the group as well as individual subjects. For testing group level confidence, we computed the average confidence observed for each unique image across the 60 human subjects and for the 60 simulated subjects (separately for each model). The correlation between the observed and predicted image-by-image group confidence was computed using Pearson's correlation coefficient. Further, to test whether these correlations were significantly different from each other, we used the Fisher's r-to-z transformation and two-sided paired t-tests. To assess confidence predictions for individual subjects, we computed image-by-image confidence separately for each simulated subject and correlated these quantities with the imageby-image confidence observed for that subject.


Data and code availability
All data and code have been made publicly available at https://osf.io/xcv98/
Figure 1 .
1
RT architecture and task. (A) RTNet architecture. RTNet consists of two modules.


Figure 3 .
3
Assessing model predictions of confidence for unseen, individual images. (A) Histograms of confidence for humans and models across the 480 unique images.


Figure 4 .
4
Assessing characteristics of group-level predictions of confidence. (A)


Figure 5 .
5
Comparing model fits across all CNN architectures and confidence strategies. (A) Quantitative comparisons using AIC scores. The Top2Diff strategy within the RTNet model gave the best AIC fits the data. Positive AIC differences indicate support for the RTNet-Top2Diff model. Regardless of the confidence strategy, the RTNet architecture significantly outperformed all other CNN architectures. Error bars show bootstrapped 95% confidence intervals. (B) Model fits compared to observed mean confidence for individual subjects. All CNN architectures and confidence strategies are able to fit the observed mean confidence for individual subjects (median r = 0.934, all p's < .001). Each dot represents an individual subject.


Figure 6 .
6
Comparing confidence strategies within other CNN architectures. Results for (A) MSDNet, (B) BLNet, and (C) CNet. Each panel shows AIC-based quantitative fits (left), qualitative fits to the folded-X pattern (middle), and image-by-image correlations between each confidence strategy and individual human subjects (right). The figure demonstrates that the performance of different confidence strategies strongly depends on the CNN architecture. Further, for all three models, no single strategy provides the best fits across the three metrics examined here. Dots represent individual subjects. Error bars show SEM. *p<0.05; ***p<0.001; n.s., not significant. SSE, sum of squared errors (smaller values indicate better fits).


.
Specifically, Top2Diff outperformed the PE strategy by an average of 45.12 points (95% CI = [30.60, 57.86), the Softmax strategy by 58.15 points (95% CI = [43.94, 97.74]), and the Entropy strategy by 170.39 (95% CI = [121.70, 234.04])


First, we examined the results for MSDNet, the second best-performing CNN architecture after RTNet. For MSDNet, the Top2Diff strategy yielded the lowest AIC values. Specifically, Top2Diff outperformed the Entropy model by an average of 26.68 points (95% CI = [15.93, 37.91), the Softmax model by 95.53 points (95% CI = [39.22, 213.99]), and the PE model by 348.34 points (95% CI = [309.86, 397.35];


.
Specifically, PE, the best-performing model, outperformed Entropy by 3.70 AIC points (95% CI = [-40.05, 42.12]), Top2Diff by 6.65 AIC points (95% CI = [-42.96, 54.35]), and Softmax by 226.74 AIC points (95% CI = [129.73, 59.46]). Qualitative


Top2Diff by 59.88 AIC points (95% CI = [30.62, 91.97]), Softmax by 149.20 AIC points (95% CI = [102.41, 211.89]), and PE by 282.51 AIC points (95% CI = [218.77, 349.79],








Competing Interests
The authors declared no competing interests.
 










Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search




L
Acerbi






W
J
Ma








Advances in Neural Information Processing Systems


















Comparing Bayesian and non-Bayesian accounts of human confidence reports




W
T
Adler






W
J
Ma




10.1371/journal.pcbi.1006572








PLoS Computational Biology




14


11














Limitations of Proposed Signatures of Bayesian Confidence




W
T
Adler






W
J
Ma




10.1162/NECO_A_01141








Neural Computation




30


12
















Doubly Bayesian Analysis of Confidence in Perceptual Decision-Making




L
Aitchison






D
Bang






B
Bahrami






P
E
Latham




10.1371/journal.pcbi.1004519








PLoS Computational Biology




11


10














Sensory noise increases metacognitive efficiency




J
W
Bang






M
Shekhar






D
Rahnev




10.1037/xge0000511








Journal of Experimental Psychology: General




148


3
















Confidence reflects a noisy decision reliability estimate




Z
M
Boundy-Singer






C
M
Ziemba






R
L T
Goris




10.1038/s41562-022-01464-x








Nature Human Behaviour




7


1
















Deep problems with neural network models of human vision




J
S
Bowers






G
Malhotra






M
DujmoviÄ‡






M
Llera Montero






C
Tsvetkov






V
Biscione






G
Puebla






F
Adolfi






J
E
Hummel






R
F
Heaton






B
D
Evans






J
Mitchell






R
Blything




10.1017/S0140525X22002813








Behavioral and Brain Sciences




46














The Psychophysics Toolbox




D
H
Brainard










Spatial Vision




10


4
















The MNIST database of handwritten digit images for machine learning research




L
Deng




10.1109/MSP.2012.2211477








IEEE Signal Processing Magazine




29


6
















Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence




R
N
Denison






W
T
Adler






M
Carrasco






W
J
Ma




10.1073/pnas.1717720115








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






115














Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation




S
M
Fleming






N
D
Daw




10.1037/rev0000045








Psychological Review




124


1


















B
U
Forstmann






R
Ratcliff






E
J
Wagenmakers




10.1146/ANNUREV-PSYCH-122414-033645








Sequential Sampling Models in Cognitive Neuroscience: Advantages, Applications, and Extensions






67


641












Computing a human-like reaction time metric from stable recurrent vision models




L
Goetschalckx






L
N
Govindarajan






A
K
Ashok






A
Ahuja






D
L
Sheinberg






T
Serre




















Signal Detection Theory and Psychophysics




D
M
Green






J
A
Swets








John Wiley












On Calibration of Modern Neural Networks




C
Guo






G
Pleiss






Y
Sun






K
Q
Weinberger










34th International Conference on Machine Learning






3














A Mathematical Framework for Statistical Decision Confidence




B
Hangya






J
L
Sanders






A
Kepecs




10.1162/NECO_a_00864








Neural Computation


















Deep Residual Learning for Image Recognition




K
He






X
Zhang






S
Ren






J
Sun




10.1109/CVPR.2016.90








Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition


the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
















Multi-Scale Dense Networks for Resource Efficient Image Classification




G
Huang






D
Chen






T
Li






F
Wu






L
Van Der Maaten






K
Weinberger










6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings
















Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss




M
L
Iuzzolino






M
C
Mozer






S
Bengio










Advances in Neural Information Processing Systems






33














Hands-on Bayesian Neural Networks --a Tutorial for Deep Learning Users




L
V
Jospin






W
Buntine






F
Boussaid






H
Laga






M
Bennamoun




10.1109/MCI.2022.3155327








IEEE Computational Intelligence Magazine




17


2
















Fast Recurrent Processing via Ventrolateral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition




K
Kar






J
J
Dicarlo




10.1016/J.NEURON.2020.09.035








Neuron




109


1
















Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior




K
Kar






J
Kubilius






K
Schmidt






E
B
Issa






J
J
Dicarlo




10.1038/s41593-019-0392-5








Nature Neuroscience




22


6
















Recurrent Processing in V1/V2 Contributes to Categorization of Natural Scenes




M
Koivisto






H
Railo






A
Revonsuo






S
Vanni






N
Salminen-Vaparanta




10.1523/JNEUROSCI.3074-10.2011








The Journal of Neuroscience




31


7


2488














Does perceptual confidence facilitate cognitive control? Attention




A
Koizumi






B
Maniscalco






H
Lau




10.3758/S13414-015-0843-3








Perception & Psychophysics




77


4
















Metacognition and consciousness. The Cambridge Handbook of Consciousness




A
Koriat




10.1017/CBO9780511816789.012








3














ImageNet Classification with Deep Convolutional Neural Networks




A
Krizhevsky






I
Sutskever






G
E
Hinton










Advances in Neural Information Processing Systems
















Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis




H
H
Li






W
J
Ma




10.1038/s41467-020-15581-6








Nature Communications




11


1
















Visual Confidence. Annual Review of Vision Science




P
Mamassian




10.1146/ANNUREV-VISION-111815-114630








2














The signal processing architecture underlying subjective reports of sensory awareness




B
Maniscalco






H
Lau




10.1093/nc/niw002








Neuroscience of Consciousness




2016


1














Tuned inhibition in perceptual decision-making circuits can explain seemingly suboptimal confidence behavior




B
Maniscalco






B
Odegaard






P
Grimaldi






S
H
Cho






M
A
Basso






H
Lau






M
A K
Peters




10.1371/JOURNAL.PCBI.1008779








PLOS Computational Biology




17


3














Heuristic use of perceptual evidence leads to dissociation between performance and metacognitive sensitivity




B
Maniscalco






M
A K
Peters






H
Lau




10.3758/s13414-016-1059-x








Perception, & Psychophysics




78


3










Attention








Metacognition: Knowing about Knowing




J
Metcalfe






A
P
Shimamura






J
Metcalfe






A
P
Shimamura










MIT Press. -References -Scientific Research Publish. In MIT Press


Cambridge, MA












A Simplex Method for Function Minimization




J
A
Nelder






R
Mead




10.1093/COMJNL/7.4.308








The Computer Journal




7


4
















Superior colliculus neuronal ensemble activity signals optimal rather than subjective confidence




B
Odegaard






P
Grimaldi






S
H
Cho






M
A K
Peters






H
Lau






M
A
Basso




10.1073/PNAS.1711628115/SUPPL_FILE/PNAS.201711628SI.PDF








Proceedings of the National Academy of Sciences of the United States of America


the National Academy of Sciences of the United States of America






115














Perceptual confidence neglects decision-incongruent evidence in the brain




M
A K
Peters






T
Thesen






Y
D
Ko






B
Maniscalco






C
Carlson






M
Davidson






W
Doyle






R
Kuzniecky






O
Devinsky






E
Halgren






H
Lau




10.1038/s41562-017-0139








Nature Human Behaviour




1


7


139














Two-stage dynamic signal detection: A theory of choice, decision time, and confidence




T
J
Pleskac






J
R
Busemeyer




10.1037/a0019737








Psychological Review




117


3
















The neural network RTNet exhibits the signatures of human perceptual decision-making




F
Rafiei






M
Shekhar






D
Rahnev




10.1038/s41562-024-01914-8








Nature Human Behaviour




2024
















Confidence in the Real World




D
Rahnev




10.1016/j.tics.2020.05.005








Trends in Cognitive Sciences




24


8


















D
Rahnev






T
Balsdon






L
Charles






V
De Gardelle






R
Denison






K
Desender






N
Faivre






E
Filevich






S
M
Fleming






J
Jehee






H
Lau






A
L F
Lee






S
M
Locke






P
Mamassian






B
Odegaard






M
Peters






G
Reyes






M
Rouault






J
Sackur






A
Zylberberg




10.1177/17456916221075615








Consensus Goals in the Field of Visual Metacognition






17














The diffusion decision model: Theory and data for twochoice decision tasks




R
Ratcliff






G
Mckoon




10.1162/neco.2008.12-06-420








Neural Computation
















Modeling Response Times for Two-Choice Decisions




R
Ratcliff






J
N
Rouder




10.1111/1467-9280.00067








Psychological Science
















Modeling confidence and response time in recognition memory




R
Ratcliff






J
J
Starns




10.1037/a0014086








Psychological Review




116


1
















Modeling confidence judgments, response times, and multiple choices in decision making: recognition memory and motion discrimination




R
Ratcliff






J
J
Starns




10.1037/a0033152








Psychological Review




120


3
















Visibility is not equivalent to confidence in a low contrast orientation discrimination task




M
Rausch






M
Zehetleitner




10.3389/FPSYG.2016.00591/BIBTEX








Frontiers in Psychology




7


183184














Cognitive modelling reveals distinct electrophysiological markers of decision confidence and error monitoring




M
Rausch






M
Zehetleitner






M
Steinhauser






M
E
Maier




10.1016/j.neuroimage.2020.116963








NeuroImage




218














Signatures of a Statistical Computation in the Human Sense of Confidence




J
I
Sanders






B
Hangya






A
Kepecs




10.1016/j.neuron.2016.03.025








Neuron




90


3
















Distinguishing the Roles of Dorsolateral and Anterior PFC in Visual Metacognition




M
Shekhar






D
Rahnev




10.1523/JNEUROSCI.3484-17.2018








The Journal of Neuroscience




38


22
















The nature of metacognitive inefficiency in perceptual decision making




M
Shekhar






D
Rahnev




10.1037/rev0000249








Psychological Review




128


1
















How do humans give confidence? A comprehensive comparison of process models of perceptual metacognition




M
Shekhar






D
Rahnev




10.1037/XGE0001524








Journal of Experimental Psychology. General




153


3
















Human-like dissociations between confidence and accuracy in convolutional neural networks




M
Shekhar






D
Rahnev




10.1101/2024.02.01.578187


2024.02.01.578187








BioRxiv
















Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision




C
J
Spoerer






T
C
Kietzmann






J
Mehrer






I
Charest






N
Kriegeskorte








PLoS Computational Biology




10


16
















10.1371/JOURNAL.PCBI.1008215


















H
Tang






M
Schrimpf






W
Lotter






C
Moerman






A
Paredes






J
O
Caro






W
Hardesty






D
Cox






G
Kreiman








115








National Academy of Sciences of the United States of America






Recurrent computations for visual pattern completion. Proceedings of the










10.1073/PNAS.1719397115/SUPPL_FILE/PNAS.1719397115.SAPP.PDF














Natural statistics support a rational account of confidence biases




T
W
Webb






K
Miyoshi






T
Y
So






S
Rajananda






H
Lau




10.1038/s41467-023-39737-2








Nature Communications




14


1
















Are Deep Neural Networks Adequate Behavioral Models of Human Visual Perception? Annual Review of Vision Science




F
A
Wichmann






R
Geirhos




10.1146/ANNUREV-VISION-120522-031739








9














Challenging the Bayesian confidence hypothesis




K
Xue






M
Shekhar






D
Rahnev




10.31234/OSF.IO/MF5ZP


















A novel behavioral paradigm reveals the nature of confidence computation in perceptual decision making




K
Xue






M
Shekhar






D
Rahnev




10.1167/JOV.24.10.407








Journal of Vision




24


10

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]