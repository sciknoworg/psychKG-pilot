You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Trust in different forms of knowledge predicts belief in epistemically suspect claims: Development and validation of the Foundations of Knowledge Questionnaire
Beneath our overt beliefs about the world lie more fundamental beliefs about the nature of knowing. How do we know that something is true? When someone makes a truth claim they provide reasons and explanations for their convictions to ensure what they believe makes sense to themselves and those around them 
(Kuhn, 2001
). This process reflects broader epistemic beliefs regarding how knowledge is acquired. Indeed, people may differ in what sources, forms, and justifications for knowledge that they rely on. Measuring individual differences in epistemic beliefs could be key to understanding a wide range of psychological phenomena, including political divisions, motivations, worldviews, and behaviours. The "post-truth crisis", for instance, may hinge heavily on the epistemology underlying how people evaluate information 
(Friedman, 2023;
Levy, 2023;
Uscinski et al., 2024)
. In this paper, we introduce and validate a novel psychometric tool to characterise individual differences in ways of knowing-the Foundations of Knowledge Questionnaire.
The broad existing literature on epistemic cognition includes various models for capturing its development and variation 
(Greene et al., 2008;
Hofer, 2000;
Kuhn et al., 2000;
Perry, 1970;
Schommer, 1990)
. Several multidimensional self-report tools have also been designed to capture individual differences in epistemic beliefs 
(Conley et al., 2004;
Ferguson et al., 2013;
Hofer, 2000;
Schommer, 1990;
Schraw et al., 2002)
. These tools typically include topics such as: whether knowledge changes over time, whether knowledge is about discrete facts, whether abilities are innate, whether learning is quick, and how knowledge is justified. Beliefs such as these have shown utility in predicting outcomes such as academic achievement and learning 
(Cano, 2005;
Conley et al., 2004;
Greene et al., 2018;
Hofer & Pintrich, 1997;
Mason et al., 2013)
. However, much work, including the design of such scales, is almost exclusively tailored to student outcomes. It is less clear how epistemic beliefs relate to outcomes beyond the classroom, including topics of broader interest to the social and behavioural sciences such as beliefs, attitudes, reasoning, and decision-making.
The dimensions covered by past scales are also incredibly broad with most dimensions primarily related to the nature of learning. How people justify truth claims, however, stands out as perhaps the most important epistemic belief for understanding the way in which people reason, evaluate information, and form beliefs about the world. Apart from a delineation between justification by authority and personal justifications 
(Bråten et al., 2013;
Ferguson et al., 2013;
Greene et al., 2008)
, existing models do not adequately capture the various forms of knowledge justifications that individuals might trust.
Beyond broad, student-focused investigations of epistemic cognition, more recent work has demonstrated that epistemic beliefs do indeed predict consequential outcomes in the general adult population. For example, 
Garrett & Weeks (2017)
 developed a three-factor scale of 'Faith in Intuition,' 'Need for evidence,' and 'Truth is political', which were found to be associated with conspiracist ideation and misperceptions about political topics 
(Garrett & Weeks, 2017;
Young et al., 2022)
. 
Ståhl and colleagues (2016)
 also devised two scales to measure the importance and moral imperative of rationality, and these were found to predict belief in unsubstantiated claims 
(Ståhl & van Prooijen, 2018)
 and engagement in science 
(Choung et al., 2020)
. However, Garrett and Weeks' (2017) inventory covers a range of loosely related constructs that were pooled together to test a priori theoretical predictions. The scales may therefore be less suitable for answering broader research questions. Moreover, Stahl and colleagues' (2016) scales focus only on rationality. These measures suggest that knowledge justifications, and the truth sources people value, are an important aspect of epistemic belief (e.g., authority, intuition, rationality), but there is not yet a comprehensive, general-purpose measure to characterise individual differences in knowledge foundations. Teasing apart what these foundations are may help to better explain social, behavioural, and political differences between people.
Our aim in the present series of studies was to create and test a short survey designed to measure the different knowledge justifications or foundations people rely on to determine what they consider to be true. In a world where attitudes to social and political issues are increasingly polarised, and where misinformation and alternative facts are of greater interest in public discourse and policy, a measure such as this is timely and could be of use to a wide range of contemporary cognitive, social, and behavioural research interests. To create a questionnaire useful for myriad research questions, we used a data-driven approach. We first pooled a large set of items to capture a wide array of possible 'ways of knowing'. In Study 1, we determine the appropriate factor structure for these items via an Exploratory Factor Analysis (EFA) before whittling down the number of items. In Study 2, we conduct a confirmatory factor analysis and assess the internal consistency, convergent validity, and discriminant validity of each factor. In Study 3, we assess the test-retest reliability of each factor, and whether scores are predictive of behaviour. In Study 4, we test whether factor scores predict belief in epistemically suspect and implausible claims. The latter study not only test the validity of the Questionnaire as a whole but sheds important insights about the formation of beliefs like conspiracy theories, pseudoscience and the paranormal.


Study 1: Scale Development and EFA
To devise our questionnaire, we generated a large pool of items either gathered from prior questionnaires or created ourselves. We first contemplated the various ways of knowing that people might defer to when determining what is true. In the modern era, the scientific method is commonly seen as the most reliable and accurate way of attaining knowledge about the world. However, people may also regard the scientific method as distinct from the institutions, organisations and experts that conduct and disseminate scientific findings. Thus, science as a method, and science as an institution, could be viewed as separate sources or justifications for knowledge.
Alternative ways of knowing also exist. A few non-scientific methods or ways of knowing regularly appear in textbooks on behavioural research methods (e.g., 
Jhangiani et al., 2019;
Privitera, 2022;
Thomas, 2021)
. These methods include tenacity-knowledge based on tradition or standing the test of time; authorityknowledge disseminated by an expert or respected source; intuition-knowledge based on subjective hunches or internal 'gut' feelings; empiricism-knowledge derived from observation and sensory experience; and rationalism-knowledge derived from logic and/or reason. Last, we reasoned that religion, spirituality, and cultural learning have been central to human understanding of the world throughout history and across cultures 
(Harari, 2014;
Norenzayan, 2010;
Saroglou, 2011;
Tomasello et al., 1993)
. With these various ways of knowing in mind, we scoured related psychometric scales and questionnaires 
(Dagnall et al., 2019;
Epstein et al., 1996;
Ferguson et al., 2013;
Nadelson et al., 2014;
Ståhl et al., 2016;
Stanovich & West, 2007)
. From these, we selected and/or adapted 25 items and then created an additional 35 items of our own to capture the various forms of knowing outlined above. The purpose of Study 1 is to conduct an EFA on the 60-item pool we generated to determine a suitable underlying factor structure and to reduce redundancy in the items. We expected the results of the EFA to somewhat reflect the categories of knowing identified above, but our analysis was exploratory.


Method


Transparency and Openness
The preregistered methods and planned analyses can be found on the OSF along with the full item list, data and analytic script:
https://osf.io/yhzpn/?view_only=036b82eda63a415a8fc5404ee030c443. Data for all four studies were analyzed using R, version 4.3.1 (R Core Team, 2023) and visualized using the ggplot2 
(Wickham, 2016)
 and semPlot packages 
(Epskamp, 2022)
.


Participants
We collected data from 500 US adult, English-speaking participants. We determined that a minimum sample size of 300 participants would be a sufficient for an EFA, in line with various recommendations (DeVellis, 2017; 
Kyriazos, 2018;
Tinsley & Tinsley, 1987)
. However, we preregistered to recruit 500 participants for good measure and to account for possible exclusions. Only one participant was excluded for failing our attention checks, leaving 499 participants. The average age of the sample was 36.95 (SD = 13.38, Range: 18-74), 49.10% identified as woman/female (48.50% man/male), 60.92% identified as White/Caucasian (13.43%
Asian, 10.82% Hispanic, 10.62% African American), 68.94% had tertiary level education, and 90.98% spoke English as a first language.


Procedure
Participants first read an information sheet, provided consent, and completed a Captcha question. They then completed all 60 items in random order along with two attention checks (e.g. "Please select Strongly agree") and one additional item ("There are many different but equally valid ways of understanding the world."). An example item from the 60-item pool was "I rely on reason and logic to figure out what is true". Participants rated their agreement with each of the 63 statements on a scale from 1 (strongly disagree) to 7 (strongly agree). Nine items were reverse coded.
Finally, participants answered demographic questions before being debriefed.


Results and discussion
After checking if the data were suitable for factor analysis (KMO = .94, Bartlett's test of sphericity: χ²(1770) = 16833.54, p < .001), we examined the Scree plot, which suggested a six-factor solution. Parallel analysis, however, suggested an eight-factor solution. We conducted an EFA for six-factor and eight-factor solutions with Promax rotations using the 'psych' package in R 
(Revelle, 2023)
. The eightfactor solution had some poor loadings and significant cross-loadings whereas the six-factor solution had clearer and more distinct loadings, which mapped onto interpretable constructs 
(Table 1)
. We therefore proceeded with the six-factor solution. In identifying these six factors, we reduced the number of items by selecting the five questions with the largest loadings within each factor (bolded in 


Methods


Design and Participants
We employed an observational design in which we correlate scores on each 


Epistemic Pluralism Scale.
Epistemic pluralism refers to the idea that there are multiple ways of knowing.
We created a scale to examine whether those who agree with the idea of epistemic pluralism also score highly on multiple factors in the Foundations of knowledge Questionnaire. This scale had not been validated. It originally included five items, but we removed one because it lowered internal consistency, possibly because it was reverse-scored. An example item is "There are many different but equally valid ways of understanding the world." The Likert response scale ranges from 1(Strongly disagree) to 7(Strongly agree). Higher scores indicate greater belief in epistemic pluralism.


Trust in Science and Scientists Scale (TSSS).
This is a self-report scale devised by 
Nadelson et al. (2014)
 to quantify trust in the scientific process and the credibility of scientists. This scale was included to evaluate the convergent validity of the Expert authority factor. It typically consists of 21 items, but we only used 19 items because two were included in the EFA in Study
1. An example item is: "Scientists provide unbiased information." Each statement was rated via a 5-point Likert scale from 1 (Strongly disagree) to 5 (Strongly agree).
Higher scores indicate a higher degree of trust in science and scientists. An attention check question is also included.


Epistemic Orientation Short Scale (EOSS).
This is a 13-item questionnaire was originally designed to measure the epistemic orientation of psychotherapists 
(Palma et al., 2018)
. It contains three subfactors: rationalism, intuitionism, and empiricism, which appear to conceptually resemble the Reason, Intuition and Personal Experience factors. We included this questionnaire to validate these three factors. An example item is: "My opinions are commonly based on feelings and intuitions". The response scale ranges from 1 (strongly disagree) to 5 (strongly agree). Higher scores suggest greater orientation towards rationalism, intuitionism, and empiricism, respectively.


Religious Commitment Inventory (RCI).
This is a 10-item self-report measure created by 
Worthington et al. (2003)
 to measure adherence to religious values, beliefs, and practices. This scale was included to validate the convergent validity of the Faith factor. Responses are given on a 5-point Likert scale from 1 (Not at all true of me) to 5 (Totally true of me). A sample item is "My religious beliefs influence my daily decisions." Higher scores indicate greater religious commitment.


HEXACO Openness to Experience Scale (OE).
The HEXACO personality inventory was developed by 
Ashton & Lee (2009)
.
The Openness to Experience subscale assesses a preference for novel experiences and appreciation of art, emotion, adventure, and unusual ideas. It comprises 16
items, each rated on a 5-point Likert scale from 1 (Strongly disagree) to 5 (Strongly agree). An example item is, "I enjoy the beauty of nature and the arts." An attention check was also included among these items. Higher scores indicate greater openness to experience. This subscale was included to examine the convergent validity of the Culture factor because both scales appear to focus on cultural practices and valuing the viewpoints and experiences of others.


Rational-Experiential Inventory: Need for Cognition and Faith in Intuition.
This is a self-report questionnaire devised by 
Epstein et al. (1996)
, extending on 
Cacioppo & Petty (1982)
, to measure analytic and intuitive thinking styles. These are measured independently via the Need for Cognition (NFC) and Faith in Intuition (FI) subscales. The NFC subscale consists of 19 items capturing a preference for analytic thinking. An example item is: "I would rather do something that requires little thought than something that is sure to challenge my thinking abilities." The FI subscale normally includes 12 items, but we removed one because it was identical to an item in the Foundations of Knowledge Questionnaire. An example item is: "My initial impressions of people are almost always right." Whereas the FI scale focuses on feelings and people, we regard it as distinct from the Intuition factor, which focuses on facts and knowledge.
Responses to both subscales are given using a 5-point Likert scale from 'completely false' to 'complete true'. Higher scores on NFC suggest a stronger preference for cognitive activities and higher scores on FI suggest an orientation to intuitive thinking. This inventory was included to test the convergent validity of the Reason and Intuition factors, respectively. Each scale was administered separately.
An attention check question was included among the NFC items.


Social Desirability Scale (SDS).
This scale measures a tendency to respond in a socially desirable way to survey questions. It was included to assess discriminant validity of each foundational factor. We used 
Reynolds' (1982)
 13-item questionnaire adapted from 
Crowne & Marlowe (1960)
. Participants respond to each item with either 'true' or 'false'. An example item is: "I have never deliberately said something that hurt someone's feelings." Higher scores indicate less inclination to respond in a socially desirable way.


Satisfaction with Life Scale (SWLS).
This scale measures global life satisfaction 
(Diener et al., 1985)
 and includes five items. It was included to assess the discriminant validity of each factor in the Foundations of Knowledge Questionnaire. The response options range from 1(Strongly disagree) to 7(Strongly agree). An example item is "In most ways, my life is close to my ideal." Higher scores indicate greater satisfaction with life.


Hypotheses
We expected the Foundations of Knowledge Questionnaire's six-factor structure to demonstrate good fit. We also expected good internal consistency for all factors (α > .70), as well as strong convergent validity such that each factor correlated positively with scales of similar constructs. More specifically, we 1) predicted a strong positive correlation (r > .5) between the Expert authority factor and Trust in Science and Scientists, 2) between the Reason factor and both Rationalism and Need for Cognition, 3) between the Intuition factor and both Intuitionism and Faith in Intuition, and 4) between the Faith factor and Religious Commitment. We also predicted moderate positive correlations (r > .3) between 5) the Personal experience factor and Empiricism and 6) between the Culture factor and Openness to Experience. We expected lower correlations for the latter two comparisons because the factor constructs appear to map less well onto the chosen validation scales. For one, the Empiricism scale encompasses knowledge that can be objectively observed and measured whereas the Personal Experience factor more specifically captures reliance on subjective, first-hand experience. Similarly,
Openness to Experience captures receptiveness to new ideas and values, whereas our Culture factor more specifically reflects reliance on knowledge passed on from others and cultural learning. Finally, we predicted 7) strong discriminant validity such that each factor in the Foundations of Knowledge Questionnaire shares little association (rs < .2) with social desirability and satisfaction with life.


Results and discussion


CFA
A CFA was conducted using the 'lavaan' package 
(Rosseel, 2012)
 in R to evaluate the fit of a six-factor structure (see 
Figure 1)
 Reason (α = .84), Intuition (α = .89), Personal Experience (α = .81), Faith (α = .93),
and Culture (α = .78).


Figure 1.
Confirmatory Factor Structure of the Foundations of Knowledge Questionnaire.
Note. This figure visualizes the CFA model examining the six factors in the Foundations of Knowledge Questionnaire: Expert Authority (ExA), Reason (Rsn), Intuition (Int), Personal Experience (PEx), Faith (Fth), and Culture (Ctr). It displays the loadings of each item onto their respective factors, highlighting the strength and significance of these relationships. Paths between variables are represented with arrows, with their width and colour indicating the strength and nature of the relationships (positive in green; negative in red). The values displayed represent standardized path coefficients. Latent variables (factors) are depicted as ovals and observed variables (items) in rectangles. All items load strongly and positively onto their respective factors and there are varying levels of overlap between the factors.


Convergent and discriminant validity
The strength and direction of correlations between each factor and existing scales of similar constructs were largely as expected (see 
Figure 2
). All scales and subscales used for validation had adequate to excellent internal consistency (α range: .75 to .97). As expected, 1) the Expert authority factor was strongly positively correlated with Trust in Science and Scientists, 2) the Reason factor was strongly positively correlated with Rationalism, 3) the Intuition factor was strongly positively correlated with Intuitionism and Faith in Intuition, 4) the Faith factor was strongly positively associated with Religious Commitment, 5) the Personal experience factor was moderately correlated with Empiricism, and 6) the Culture factor was moderately correlated with Openness to Experience. Additionally, these correlations were the strongest in their respective columns in 
Figure 2
. Strong discriminant validity for the Foundations of Knowledge factors was also evidence by 7) weak or null associations between each factor and both social desirability and life satisfaction, as predicted.
These data provide support for strong convergent and discriminant validity of each factor in the Foundations of Knowledge Questionnaire.  Correlations between the Foundations of Knowledge Questionnaire factors and existing psychometric scales.
Note. Correlations between the six factors in the Foundations of Knowledge Questionnaire and existing psychometric scales (TSS = Trust in science and scientists, NFC = Need for Cognition, FI = Faith in intuition, RCI = Religious commitment inventory, OE = Openness to experience, SD = Social desirability, SWL = Satisfaction with life). Darker blue squares indicate stronger positive correlations whereas darker red squares indicate stronger negative correlations. Bolded values denote tests we hypothesised to be statistically significant. Asterisks indicate statistically significant correlations (p < .05). Correlations generally demonstrate strong convergent and discriminant validity for each factor, with Personal experience the only somewhat ambiguous factor.
Looking across the rows in 
Figure 2
, there were some additional moderate correlations. These mostly reflect the interrelations between the various Foundations of Knowledge factors (see 
Figure 1
). That said, one complicated finding was the equivalent correlations between Reason and Empiricism and between Personal experience and Empiricism. We included the Empiricism scale to validate only the Personal Experience factor, anticipating that it shares resemblance with empiricism but with the caveat that it more precisely captures subjective observations as opposed to objective, empirical data. In hindsight, it is likely that reasoned, logical whole; general speaking, the higher people score across multiple factors, the more they agree with the idea of epistemic pluralism.


Study 3: Test-retest Reliability and Criterion Validity
After having identified the structure of the Foundations of Knowledge Questionnaire and assessing each factor's convergent and discriminant validity, the aim of Study 3 is to assess the stability of each factor by examining test-retest reliability, and to explore whether each factor is predictive of behavioural outcomes (criterion validity). We anticipate that factor scores will be stable when administered several weeks apart. We also expect that each factor will predict behaviours associated with their supposed latent construct, as well as how persuaded people are by evidence that draws on the foundation underlying the factor. We report on a two-part study in which participants complete the Foundations of Knowledge Questionnaire, along with behavioural tasks and questions, at two time points.


Methods


Design and Participants
This was an observational study in which we examined the correlation between scores on each Foundations of Knowledge factor and scores on behavioural questions and tasks. The preregistered methods, hypotheses and planned analyses can be found on the OSF along with the data, materials and analytic scripts:
https://osf.io/qsz7b/?view_only=e89361adb26f48c894282c48a25da5bb. Participants were recruited from the US via prolific to participate in two surveys spaced three weeks apart. We sampled 350 participants at Time 1. A priori power calculations indicated that accounting for exclusions and attrition of up to 30%, a sample size of at least 245 at Time 2 would be sufficient to detect small-to-moderate correlations (r ≥ .18) with 80% power. Five participants were excluded at Time 1 for failing attention checks, leaving 345 participants for analysis. Of these, 258 completed the survey at Time 2, but eight were excluded for failing attention checks, leaving 250.
At Time 1, the average age of the eligible participants was 37.59 (SD = 13.15, Range: 18-78), 47.54% identified as woman/female (48.70% man/male), 66.67% identified as White/Caucasian (10.72% African American, 9.57% Asian, 8.12% Hispanic), 64.06% had tertiary level education and 91.88% spoke English as a first language.


Materials, and Procedure
Participants at Time 1 completed the Foundations of Knowledge Questionnaire followed by the Cognitive Reflection Test (CRT) and a series of behavioural frequency questions. Participants were then re-invited to take part in another survey three weeks later. Returning participants completed the Foundations of Knowledge Questionnaire again followed by an evidence evaluation task. The questions and tasks administered are detailed below in chronological order. Data on age, gender, ethnicity, education level, and English as a first language were collected at the end of the survey at both time points. Data on political orientation and sciencespecific education were also collected at Time 1.


Time 1
Epistemic pluralism. This scale was identical to the four-item Epistemic pluralism scale from Study 2.


Cognitive Reflection Test (CRT).
The CRT is a problem-solving task designed to measure the tendency to override immediate, intuitive but incorrect responses to a problem and reflect further to find a correct answer. We included the CRT to assess the criterion validity of the Reason and Intuition factors because it is perhaps the most widely used behavioural task for assessing analytic vs intuitive thinking. The version of the CRT used here included seven items; three from 
Frederick (2005)
 and four from 
Thomson & Oppenheimer (2016)
. An example item is: "A bat and a ball cost $1.10 in total. The bat costs $1 more than the ball. How much does the ball cost?" The Reflective score is the total number of correct responses across all seven items. The Intuitive score is the total number of incorrect, intuitive responses across the seven items. After participants answered these seven problems, we asked them to estimate how many questions they answered correctly and answer a multiple-choice question about their decision-making style during the CRT task.


Behavioural frequency questions. Participants answered nine behavioural
frequency questions to explore whether the Faith, Culture and Personal Experience factors are predictive of more general behaviour. We included three questions from Pennycook and colleagues' (2012) Religious Engagement Scale designed to index religious engagement and level of participation. An example item was: "Aside from weddings and funerals, how often do you attend religious services?" This scale was used to validate the Faith factor. We then adapted the Religious Engagement Scale sticking with a similar format, to create two new three-item scales. The first was designed to measure artistic and culture engagement and validate the Culture factor. An example item was: "How often do you attend cultural events and/or cultural sites (e.g., festivals, exhibitions, museums)?". The second adapted scale was designed to measure engagement in behaviour related to practicality and first-hand experience (first-hand engagement) and to validate the Personal Experience factor. An example item was: "When faced with a new task or challenge (e.g., fixing something, remedying ailments, cooking, navigating, assembling furniture), how often do you dive in and attempt to solve it by doing rather than seeking out instructions or advice first?" All nine questions had a response scale ranging from either 1 to 6 or 1 to 7, with a maximum possible score of 19 for each set of questions, as per Pennycook and colleagues' original scale. Higher scores reflect greater engagement in the respective domain. The order of the nine questions was randomised.


Time 2
Evidence evaluation task. We created 18 vignettes with the same basic structure, each designed to assess how much participants were persuaded by different forms of evidence. Three vignettes were designed to draw on evidence from each of the different knowledge foundations. They covered fictional claims on six different topics: a doctor's medical diagnoses, a juror decision, a novel product, a proposed diet, a new policy, and a supposed historical event. An attention check vignette was also included. An example vignette drawing on the expert authority foundation was as follows:
Imagine waking up with a range of peculiar symptoms that leave you feeling concerned. You decide to consult a medical doctor. After examining your symptoms, conducting various tests, and carefully analyzing the results, she diagnoses you with an unusual illness called 'intraarterial capitis', which can be treated with regular medication. If you were in this situation -given the doctor's diagnosis -how persuaded would you be that the diagnosis is correct?
Participants read and rated the persuasiveness of each vignette in random order using scale from 0 (not at all persuasive) to 100 (completely persuasive). A score for each set of vignettes was computed by averaging the ratings across the three vignettes targeting each respective foundation. Each score was then subtracted from the participant's mean rating across all 18 vignettes to arrive at a mean-adjusted persuasiveness score for each factor. These adjusted scores were used for analysis to account for participants' response bias or 'credulity'.


Hypotheses
To demonstrate that each Foundations of Knowledge factor has good testretest reliability, we expected strong correlations between factor scores at times 1 and 2 (rs > .7). We also had several predictions regarding validity at Time 1. We expected that 1) those with a tertiary degree in science would score higher than those without on the Expert Authority factor. We also expected a moderate positive correlation (r > .3) between: 2) the Reason factor and CRT Reflective scores; 3) between the Intuition factor and CRT Intuitive scores; 4) between the Faith factor and religious engagement; 5) between the Culture factor and cultural engagement; and 6) between the Personal Experience factor and first-hand engagement. Moreover, we expected 7) the total score across all factors to be strongly predictive of belief in Epistemic Pluralism.
For Time 2, we predicted moderate positive correlations (rs > .3) between each factor and the mean-adjusted persuasiveness scores for the vignettes targeting the intended factor. For example, higher scores on the Intuition factor should positively correlate with the mean-adjusted score for the vignettes drawing on trust in intuition to determine truth.


Results and Discussion


Test-retest
Test-retest reliability was good-to-excellent for all six Foundations of Knowledge factors despite their brevity. All correlations between factors at Times 1 and 2 are presented in 
Figure 3
. As anticipated, correlations for each factor between the two time points were equal to .7 or greater. Scores on each factor therefore appear stable over time. Test-retest correlations for the factors in the Foundations of Knowledge Questionnaire.
Note. Correlations between the six foundations in the Foundations of Knowledge Questionnaire at Time 1 (Top) and Time 2 (Left). Darker blue squares indicate stronger positive correlations whereas darker red squares indicate stronger negative correlations. Asterisks indicate statistically significant correlations (p < .05). Correlations on the diagonal display the test-retest reliability. Each factor demonstrated good to excellent test-retest reliability.


Time 1
Counter to expectations, those who had a tertiary level science degree (M = 5.16, SD = 0.99) did not score higher on the Expert Authority factor than those who without a tertiary science degree (M = 4.93, SD = 1.23), W = 10653, p = .174.
Although this result does not support the validity of the Expert Authority factor as anticipated, perhaps this criterion was a poor choice in hindsight. Being educated in science may not be necessary to trust in experts and scientists. In fact, inside knowledge about the complexity and uncertainty of the scientific enterprise might instead undermine or qualify blind trust in experts and scientists. Highly educated parents, for instance, hold more negative attitudes toward vaccinations 
(Hak et al., 2005)
.
Correlational data for testing the remaining hypotheses at Time 1 are presented in 
Figure 4
. As expected, the Reason factor was positively correlated with the CRT Reflective score and the Intuition factor was positively correlated with the CRT Intuitive score, but the strength of these correlations was smaller than expected. This provides weak evidence for the ability of these factor to predict performance on problem solving tasks. Of course, the Reason and Intuition factor scores reflect trust in certain forms of evidence, whereas the CRT purportedly measures analytic vs miserly information processing style 
(Frederick, 2005)
. Indeed, more recent research has suggested that the CRT more likely measures mental and numeric ability as opposed to cognitive style 
(Erceg et al., 2020;
Martire et al., 2023;
Otero et al., 2022;
Patel et al., 2019)
. The underlying constructs measured by the Foundations of Knowledge factors and the CRT may be quite distinct.
As expected, Faith scores were strongly positively associated with religious engagement and Culture scores were moderately positively related to artistic and cultural engagement. As such, we have evidence of good validity for these factors in predicting general behaviour. The Personal Experience factor and first-hand engagement were also positively correlated (r = .26), as hypothesised, albeit less than expected, lending some support to the criterion validity of the Personal Experience factor as well. There were several other small to moderate relationships, but these largely reflect the intercorrelations between the Intuition, Culture, and Personal Experience factors, which given their correlations, are likely to predict similar behaviours. Correlations between each factor in the Foundations of Knowledge Questionnaire and CRT and behavioural frequency scores.
Note. Correlations between the six Foundations of Knowledge factor and behavioural outcomes: CRT reflective and intuitive scores, first-hand engagement (FH_eng), religious engagement (Rel_eng) and cultural engagement (Cul_eng). Darker blue squares indicate stronger positive correlations whereas darker red squares indicate stronger negative correlations. Bolded values indicate correlations linked to a hypothesis. Asterisks indicate statistically significant correlations (p < .05). These relationships provide mixed evidence for the validity of the factors in predicting general behaviour. However, some criteria may not have been well-suited.
Overall, our results at Time 1 provide mixed evidence for the criterion validity of the Foundations of Knowledge factors. Scores on some factors were moderately predictive of general behaviour whereas others showed weak or ambiguous relationships. Of course, mixed findings may reflect the limited options when deciding how to validate the questionnaire rather than limitations of the Foundations of Knowledge Questionnaire and its factors.
Finally, we assessed whether the Foundations of Knowledge Questionnaire as a whole predicts belief in Epistemic pluralism. We summed scores across all Questionnaire as a whole is therefore somewhat predictive of belief in epistemic pluralism, but less than anticipated. Moreover, some factors did not add predictive power or showed a negative relationship (ie. Intuition and Faith), suggesting that some ways of knowing are exclusive or incompatible with others. For example, believing strongly in religious sources of knowledge may be incompatible with believing in reason as a way of knowing. We therefore do not recommend computing and using a total score for the Foundations of Knowledge Questionnaire. Each factor should be used in isolation.


Time 2
With the data gathered at Time 2, we examined whether the Foundations of Knowledge factors were associated with how people evaluate evidence. If the knowledge foundations capture a tendency to trust certain ways of knowing over others, then this ought to manifest in how persuaded people are by certain forms of reasoning. The correlations between each Foundations of Knowledge factor and the mean-adjusted score for the six sets of vignettes (each drawing on separate knowledge foundations) are displayed in 
Figure 5
.
As expected, scores on the Expert Authority, Reason, Intuition, Faith, and Culture factors were positively correlated with the persuasiveness of vignettes drawing on the intended foundation (rs > .25). For example, those who scored higher on the Expert Authority factor found vignettes drawing on claims made by experts and institutional authorities more persuasive than those who scored lower on this factor, and those who scored higher on the Culture factor found claims drawing on ancient wisdom or lived experience more persuasive than those who scored lower on this factor. Even though the correlations expected for Reason and Culture were slightly lower than expected, each correlation of interest above was the strongest positive association in its respective row in 
Figure 5
.
Counter to hypotheses, however, scores on the Personal Experience factor were not significantly correlated with persuasiveness ratings for vignettes targeting first-hand experience and observation. One reason for this null result may be the validity of the vignettes targeting this foundation. Personal experience is subjective in nature and the vignettes were hypothetical and may not have aligned with the participants' actual experiences nor the degree to which they trust it when evaluating claims. More generally, this study may be limited by the vignettes that we devised and used given that they themselves have not been validated as a psychometric tool. In any case, the findings at Time 2 turned out largely as expected. The results in their totality, with the exception of the Personal Experience factor, suggest that the Foundations of Knowledge Questionnaire is predictive of how persuasive people find knowledge claims that draw on the same foundations. Correlations between each Foundations of Knowledge factor and mean-adjusted vignette persuasiveness scores.
Note. Correlations between the six factors in the Foundations of Knowledge Questionnaire and vignette persuasiveness scores. Darker blue squares indicate stronger positive correlations whereas darker red squares indicate stronger negative correlations. Bolded values indicate correlations linked to a hypothesis (values along the diagonal). Asterisks indicate statistically significant correlations (p < .05). Each factor, except for Personal experience predicted the persuasiveness of the factorrelevant vignettes.


Study 4: Belief in epistemically suspect and implausible claims
Thus far we have found that the six-factor Foundations of Knowledge Questionnaire has a suitable factor structure and that each factor displays good reliability indices, convergent and discriminant validity. We also have evidence of the questionnaire's capacity to predict how people rate the persuasiveness of knowledge claims. There were, however, some challenges in identifying appropriate materials for validating each factor, and the questionnaire as a whole. The first aim of Study 4 is therefore to further examine the utility of the questionnaire by observing how well it predicts outcomes of consequence.
A second complementary aim is to examine the idea that trusting different knowledge foundations predicts belief in unsubstantiated claims. Modern times have been referred to as the post-truth era in which some people appear to operate in alternative fact environments 
(Levy, 2023;
Lewandowsky et al., 2017)
 This post-truth environment can be characterized by a collective societal shift towards 'other ways of knowing' and valuing personal beliefs, emotions, and ideologies over objective facts 
(Garrett & Weeks, 2017)
. We designed the Foundations of Knowledge Questionnaire to capture trust in different justifications or ways of knowing so it follows that these epistemic beliefs should predict how much people believe in claims that run counter to prevailing and established scientific evidence. If so, our findings would offer insight into the formation of such beliefs, and what makes them compelling to some people but not others. To achieve the present aims we therefore test whether the factor scores on the Foundations of Knowledge Questionnaire predict belief in epistemically suspect and implausible claims like conspiracy theories, pseudoscience and the paranormal.


Method


Design
In the present study, we employed both a quasi-experimental and correlational design. The preregistered methods, hypotheses and planned analyses can be found on the OSF along with the data, materials and analytic scripts: https://osf.io/72usm/?view_only=884c39cf3f68474c9de2a43bf8fb1827. Participants were labelled as either believers or non-believers of implausible claims based on the Belief in Epistemically Implausible Claims (BEIC) Protocol (Martire et al., 
[2020]
).
Participants' belief in epistemically suspect claims was also measured on a continuous scale via the Epistemically Suspect Belief Scale (ESBS; 
Šrol [2022]
).
Whereas the BEIC Protocol classifies participants into two groups (believers and non-believers) that can be compared quasi-experimentally. the ESBS conceptualises belief on a continuous dimension that can be used for correlational research. We planned to compare believers and non-believers on each of the six Knowledge Foundations, and scores on each factor were correlated with belief in epistemically suspect claims.


Participants
We aimed to collect data from 400 participants via Prolific. Participants were pre-screened using the BEIC Protocol (Martire et al., 
[2020]
; See Materials), which included four key questions: Vaccines are harmful and this fact is covered up, Global warming is a hoax, The Earth is flat and The Apollo moon landings never happened and were staged in a Hollywood film studio. Those who rated the truth of any of these claims > 60 (out of 100) were recruited as presumptive believers and those who rated all four claims < 40 were recruited as presumptive non-believers. We recruited 200 presumptive believers and 200 presumptive non-believers for this study.
We preregistered to exclude participants who failed at least 2 (of the 3) attention checks from all analyses. Accounting for up to 50 exclusions, a priori power analyses indicated that a sample size of 350 was sufficient to detect small to medium correlations (r = .15) with at least 80% power. We also planned to exclude participants who did not meet the criteria for believer (one claim > 60) or non-believer (all four claims < 40) for between-groups analyses. Accounting for these exclusions, we aimed to recruit a sample of at least 320, which was sufficient to detect small to medium effects for between-group comparisons with at least 80% power.
None of the 400 participants were excluded for failing attention checks. The mean age for the full sample was 37.14 (SD = 13.55), 69.0% identified as man/male (29.5% woman/female), 72.0% identified as White/Caucasian (10.5% Hispanic, 7.0% African-American), 68.5% reported tertiary level education, and 60.5% spoke English as a first language. Of these, 200 were categorised as believers and 174 were categorised as non-believers based on the BEIC protocol, with 26 not meeting the criteria for either believer or non-believer.


Materials and Procedure
All participants first completed the BEIC Protocol, then the ESBS, and then the Foundations of Knowledge Questionnaire. The item order within each measure was randomised. These questionnaires were completed towards the end of another study whose aims are beyond the scope of the present work. BEIC protocol. This protocol was used as a pre-screening tool and administered again in the present study to classify participants as either believers or non-believers of implausible claims 
(Martire et al., 2020
(Martire et al., , 2023
. The protocol included eight general knowledge items (e.g., Spiders have six legs) intermixed with the four implausible claims (mentioned in the Participants section). Participants rated how true they think each claim is from 0 (not at all true) to 100 (definitely true).
ESBS. This scale is designed to measure belief in a range of epistemically suspect claims 
(Šrol, 2022)
. It contains 28 questions in total that are divided into three subfactors: the paranormal, conspiracy theories, and pseudoscience. An example item is: "There are no such things as ghosts". The rating scale ranges from 1(completely disagree) to 5 (completely agree). Higher indicate greater belief.


Hypotheses
Even though this is the first study to investigate the relationship between knowledge foundations and beliefs such as conspiracy theories, pseudoscience and the paranormal, prior research provides some indication as to what we might expect.
Trust in science and scientists or authority is negatively correlated with epistemically suspect beliefs 
(Fasce & Picó, 2019;
Vranic et al., 2022)
 as are measures related to reason, evidence, and logic 
(Garrett & Weeks, 2017;
Ståhl & van Prooijen, 2018)
. On the other hand, trust in intuition is positively related to conspiracy theories and other unsubstantiated beliefs 
(Bensley et al., 2022;
Garrett & Weeks, 2017)
. Religiosity is also related to rejection of evolution and climate change 
(Ecklund et al., 2017;
Frenken et al., 2023)
. Additionally, citing personal experience has also been used to justify unsubstantiated beliefs 
(Lobato & Zimmerman, 2019)
. We therefore expected believers to score lower than non-believers on the Expert Authority and Reason factors, but higher on the Intuition, Personal Experience, and Faith factors. Similarly, ESBS scores were expected to correlate negatively with scores on the Expert Authority and Reason factors, but correlate positively with scores on the Intuition, Personal Experience, and Faith factors. We did not have strong hypotheses about the Culture factor.


Results and Discussion


Comparison between groups
We first examined whether believers of implausible claims differ from nonbelievers across each knowledge foundation. The data are displayed in 
Figure 6
. A between groups Pillai's trace MANOVA with each factor score as a dependent variable revealed that the two groups differed significantly across the six foundations, F(6, 372) = 52.10, p < .001, Pillai's trace = .46. We then conducted follow-up univariate ANOVAs. Believers (M = 3.44) scored lower than non-believers (M = 5.17) on the Expert Authority factor, F(1, 372) = 225.48, p < .001 and lower on the Reason factor (M = 5.36 vs. M = 5.66), F(1, 372) = 11.33, p = .001. However, believers (M = 5.07) scored higher than non-believers (M = 4.34) on the Intuition factor, F(1, 372) = 52.43, p < .001, higher on the Personal Experience factor (M = 5.37 vs. M = 4.97), F(1, 372) = 14.64, p < .001, and higher on the Faith factor (M = 3.19 vs. M = 2.09), F(1, 372) = 51.12, p < .001. Believers (M = 5.38) and non-believers (M = 5.24) did not differ statistically on the Culture factor, F(1, 372) = 2.90, p = .090. Each of these results was in line with a priori predictions.


Correlational data
Zero-order correlations ( 
Figure 7A
) revealed that scores on the Expert Authority factor were strongly negatively related to belief in epistemically suspect claims as measured using the ESBS, and scores on the Reason factor were moderately negatively correlated with suspect claims. Scores on the Intuition and Faith factors were strongly positively predictive of belief in suspect claims, and the Personal Experience factor was moderately positively correlated with suspect beliefs. In addition, the Culture factor was moderately positively correlated with suspect beliefs. 


Differences between believers and non-believers of implausible claims on the Foundations of Knowledge factors.
Note. Raincloud plots visualising the differences between believers and nonbelievers of epistemically implausible claims (as defined by the BEIC protocol) on each of the Foundations of Knowledge factors. Believers are displayed in yellow and non-believers in purple. Rainclouds represent the distributions, and the raindrops represent each individual data point. Non-believers scored higher on the Expert authority and Reason factors whereas believers scored higher on the Intuition, Personal experience and Faith factors. No differences were found between the groups on the Culture factor. 


Relationships between the Foundations of Knowledge factors and epistemically suspect beliefs.
Note. Visualisation of the relationships between the knowledge foundation factors scores and epistemically suspect beliefs. Panel A displays the zero-order correlations between each factor and suspect beliefs as an average, and grouped into paranormal, conspiracy theories and pseudoscientific beliefs. Darker blue squares indicate stronger positive correlations whereas darker red squares indicate stronger negative correlations. Asterisks indicate statistically significant correlations (p < .05). Panel B displays the beta values for each factor in a linear multiple regression model predicting epistemically suspect beliefs. Error bars represent the mean standard error. The Expert authority and Reason factors negatively predict suspect beliefs whereas Intuition, Personal Experience, Faith, and Culture are positive predictors.
An exploratory multiple regression analysis revealed that scores across the factors were predictive of belief in epistemically suspect claims, F(6, 392) = 86.24), p < .001 ( 
Figure 7B
). The full model explained a substantial amount of variance (56.9%; RSE = .537) in epistemically suspect belief. Moreover, each knowledge foundation added unique predictive power. The Expert Authority (β = -0.23) and
Reason factors were unique negative predictors (β = -0.13) of suspect beliefs. The Intuition (β = 0.15), Personal Experience (β = 0.11), Faith (β = 0.14) and Culture (β = 0.15) factors were unique positive predictors. This pattern of results did not differ even when age, gender and education were added as controls.
These findings in their totality show that foundational epistemic beliefs about what constitutes valuable knowledge is highly predictive of beliefs in conspiracy theories, pseudoscience and the paranormal, adding to the existing literature on the epistemologies that underlie belief in unsubstantiated claims 
(Garrett & Weeks, 2017;
Ståhl & van Prooijen, 2018)
. Each factor in the Foundations of Knowledge Questionnaire provides a piece of the puzzle to understanding people's beliefs and these insights may be key to explaining why people find unsubstantiated claims persuasive. Knowing of these individual differences may also inform how to more effectively communicate with those who hold unconventional beliefs.


General Discussion
In the present paper, we have introduced and validated the Foundations of Knowledge Questionnaire-a brief, general-purpose psychometric tool for measuring trust in various ways of knowing. Such a measure extends on prior scales of epistemic beliefs (e.g., 
Bråten et al., 2013;
Garrett & Weeks, 2017;
Schommer, 1990;
Ståhl et al., 2016)
, by providing a comprehensive assessment of the various knowledge justifications that people value when determining what is true. Here we reported on four studies that outline the development and validation of this questionnaire. We first generated a large item pool to reflect various sources of knowledge that people may value. In Study 1, we identified six factors-Expert Authority, Reason, Intuition, Personal Experience, Faith, and Culture-each representing a separate knowledge foundation. We then added two additional items to improve internal consistency. In Studies 2-4, we examined the reliability and validity of the questionnaire. The factor structure was found to have satisfactory to good model fit, and each factor's internal consistency ranged from acceptable to excellent. Each factor generally showed good convergent and discriminant validity, correlating with existing constructs as expected. Each factor also displayed good test-retest despite each scale's brevity, exhibiting stable scores over a three-week interval (rs > .07). We found varied levels of support for each factor's capacity to predict general behaviours, including educational attainment and problem solving.
Scores on each factor, except for Personal Experience, however, were predictive of how persuasive people found claims and arguments in a vignette-style evidence evaluation task. Finally, and most interestingly, the Foundations of Knowledge Questionnaire explained a great deal of variation in epistemically suspect and implausible claims, with each factor providing unique predictive power. This novel questionnaire therefore provides a broad picture of the diverse justifications of knowledge that people value and demonstrates utility in predicting important outcomes with significant explanatory power.


Even though most of the factors in the Foundations of Knowledge
Questionnaire exhibited good psychometric properties, the Personal Experience factor consistently stood out as the most puzzling. It displayed somewhat unclear convergent validity and was limited in predicting general behaviour or the persuasiveness of vignettes. Testing its underlying latent construct, however, was challenging due to the absence of similar scales and measures in the existing literature. Of course, this fact underscores the factor's novelty. Moreover, the items comprising the factor appear face valid, and demonstrate acceptable to good reliability indices. Most importantly, this factor was predictive of belief in epistemically suspect claims above and beyond the other factors in the questionnaire. Future work is necessary to reveal more about the nature and utility of the Personal Experience factor, and the notion of trust in subjective observation and experience.
The Foundations of Knowledge Questionnaire may aid researchers in conceptualising and measuring underlying individual differences in worldview that characterise the "post-truth" era. Fundamental differences in epistemic cognition may help to explain how people come to believe falsehoods (e.g., 
Garrett & Weeks, 2017;
Lewandowsky et al., 2017;
Ståhl & van Prooijen, 2018)
 and suspect beliefs may rest on alternative epistemologies that defy conventional standards of evidence 
(Harambam & Aupers, 2021;
Lewandowsky et al., 2017)
. Morsels of evidence from past work suggests that belief in unsubstantiated claims is associated with less trust in science and scientists and need for empirical evidence, overconfidence, and greater trust in intuition and religiosity 
(Ecklund et al., 2017;
Rozek et al., 2021;
Vranic et al., 2022)
. The Foundations of Knowledge Questionnaire goes further in providing a broad snapshot of the diverse forms of knowledge that people value (authority, reason, intuition, experience, faith, and culture), showing that these foundations predict how persuaded people are by arguments (Study 3) and belief in epistemically suspect and implausible claims (Study 4).
Considerable empirical work has aimed to understand the reasoning processes that underlie belief in epistemically suspect claims. One proposition is that thinking in a miserly manner, as opposed to an analytic manner, causes people to believe accept suspect claims 
(Jastrzębski & Chuderski, 2022;
Pennycook et al., 2012;
Pennycook & Rand, 2019;
Swami et al., 2014;
Yelbuz et al., 2022)
. On the other hand, people may be motivated to believe in suspect beliefs because it is useful for protecting social and political identity, vested interests and or personal worldviews and ideology 
(Hornsey, 2020;
Lewandowsky & Oberauer, 2016;
Miller & Farhart, 2016)
. The relationship between knowledge foundations and belief, however, suggests that people may come to believe suspect claims not necessarily because they are lazy or biased, but because their reasoning is based on different underlying assumptions about what constitutes a valued source of knowledge. That said, these explanations may not be mutually exclusive.
It is also important to differentiate knowledge foundations from cognitive styles. Cognitive styles, such as those measured by the Rational-Experiential Inventory 
(Epstein et al., 1996)
, tend to focus on how people make decisions in terms of processes (e.g., analytical vs. intuitive). The foundations of knowledge, on the other hand, represent the ways of knowing that people rely on to determine what is true. Cognitive styles and knowledge foundations may be related but they represent distinct constructs, but making decisions quickly or frugally is not the same as deliberating and deciding in the end to "trust your gut" or intuition. Indeed, we only found weak association between the Reason factor and Need for Cognition in Study 2 and weak associations with the CRT in Study 3.
More broadly, it may be that some individuals see value in certain knowledge foundations but not others. Some individuals may also value one or two foundations much more than others, whereas other people may value many foundations relatively equally. These are possibilities that require further empirical investigation.
When navigating claims, arguments, and facts in the real world, the knowledge foundations a person relies on may greatly affect the conclusions and opinions they reach. Moreover, if an individual trusts strongly in multiple ways of knowing, they may struggle to reconcile conflicts between different sources when determining what is true. Though these questions are beyond the scope of the present study, they may be of theoretical interest to those who study personality, social and political divides, reasoning, learning, and belief in the social and cognitive sciences.
Of course, most of the studies here were based on online US samples. More work is needed to establish the utility of the Foundations of Knowledge Questionnaire, and the broader idea of differences in knowledge foundations crossculturally. Future research could explore the validity and utility of this psychometric tool in diverse contexts and populations to better understand the universality and variability of trust in different knowledge foundations.


Conclusions
We have reported on a series of studies to validate a new self-report tool for measuring and characterising differences in the foundational knowledge sources that people rely on. The Foundations of Knowledge Questionnaire has potential to advance understanding about the formation of beliefs, differences in reasoning, and polarization of opinion. In an era underscored by a complex information landscape, examining how and why people differ in what they consider valuable or credible forms of evidence may be indispensable to answering pressing societal questions.
One finding contrary to hypotheses was the Reason factor showing only a weak-to-moderate correlation with Need for Cognition. This result does not necessarily cast doubt about the Reason factor's convergent validity given that it still strongly positively correlated with Rationalism. Instead, enjoying and engaging in cognitively demanding tasks (as is what Need for Cognition measures) may be largely distinct from relying on reason to determine what is true. Broadly speaking, cognitive style is likely not the same as trust or distrust in various forms of evidence.


Figure 2.


argument, as captured by the Reason factor, often requires support from empirical evidence, especially given the emphasis on evidence in the item wording. In retrospect, it is less surprising that scores on the Reason factor are moderately associated with Empiricism. The equivalent correlations we found are less likely to reflect an issue with validity, but rather the selection of an imprecise scale for validating the Personal Experience factor. As far as we know, there is no existing scale to better measure reliance on first-hand, experience and observation.Last, we explored whether the Foundations of Knowledge factors can predict how strongly people think that there are multiple valid methods for acquiring knowledge (Epistemic pluralism). The four items of the Epistemic Pluralism scale showed good internal consistency (α = .79). Scores on all factors except for Faith (r = -.01) positively correlated with Epistemic pluralism (ps < .05, r range: .17[Reason]    to .48[Culture]). Multiple linear regression revealed that the six factors explained 29.2% of the variance in epistemic pluralism. This exploratory analysis provides some support for the validity of the Foundations of Knowledge questionnaire as a


Figure 3.


Figure 4.


Foundations
of Knowledge factors to generate a total score. There was a small-tomoderate correlation between this total score and Epistemic Pluralism, r(343) = .18, p < .001. Zero order correlations revealed that Expert Authority, Reason, Personal Experience, and Culture were positively correlated with Pluralism, but Intuition was not significant associated, and Faith was negatively associated. Multiple linear regression revealed that all factors together explained 19.4% of the variance in Epistemic Pluralism, with Expert Authority (β = .13), Personal Experience (β = .25), Culture (β = .37) and Faith (β = -.18) explaining unique variance (ps < .05). The


Figure 5.


Figure 6.


Figure 7.


Table 1 .
1
Factor loadings for the Exploratory Factor Analysis with a six-factor solution.
ITEM
EXPERT
INTUITION
REASON
FAITH
CULTURE
PERSONAL
AUTHORITY
EXPERIENCE
1
0.53
0.33
2R
0.29
0.14
-0.37
0.13
-0.14
3
0.62
0.29
4
0.40
0.38
-0.20
0.13
5
0.42
0.50
-0.18
6
0.46
0.43
-0.10
0.13


Table 1
1
). We identified five suitable items for all but the Personal Experience factor. The 29 chosen items all had loadings of at least 0.45 onto their respective factors and cross loadings less than 0.3. The items in each factor appeared face valid. Internal
consistency for the selected items ranged from good to excellent for Expert authority
(α = .88), Intuition (α = .86), Reason (α = .81), Faith (α = .92) and Personal
experience (α = .79), but was comparatively lower for Culture (α = .69). Given only
four items loaded appropriately onto the Personal experience factor, and internal
consistency was low for the Culture factor, we created an additional item for each to
round out the 31-item version of the Foundations of Knowledge Questionnaire.
Study 2: CFA and convergent/discriminant validity
In Study 1, we identified 31 items designed to capture various trusted
justifications for knowledge, which included six factors: Expert authority, Reason,
Intuition, Personal experience, Faith, and Culture. In Study 2, our aim was to conduct
a Confirmatory Factor Analysis (CFA) to determine whether the refined Foundation of
Knowledge Questionnaire's factor structure is adeqaute, and to assess the
convergent and discriminant validity of each factor by examining how they relate to
existing psychometric scales.


. The model demonstrated satisfactory to good fit with the data overall. Though the chi-square test of goodnessof-fit was significant, χ²(419) = 1033.60, p < .001, the Comparative Fit Index (CFI) of .904 and the Tucker-Lewis Index (TLI) of .893 indicate acceptable to good fit. The Standardized Root Mean Square Residual (SRMR) was .055 and the Root Mean Square Error of Approximation (RMSEA) at .061, also indicating acceptable to good fit. Collectively, these fit indices suggest that the six-factor model adequately represents the underlying data structure. Additionally, Cronbach's alphas, revealed
good to excellent internal consistency for each factor: Expert Authority (α = .86),








Author Contributions. As per the CRediT framework, SGR contributed to conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, validation, visualization, writing-original draft and writingreview and editing. KAM and KF contributed to Conceptualization, methodology, funding acquisition, supervision, and writing-review and editing.
 










The HEXACO-60: A short measure of the major dimensions of personality




M
C
Ashton






K
Lee




10.1080/00223890902935878








Journal of Personality Assessment




91


4
















Skep]cism, cynicism, and cogni]ve style predictors of the generality of unsubstan]ated belief




D
A
Bensley






C
Watkins






S
O
Lilienfeld






C
Masciocchi






M
P
Murtagh






K
Rowan




10.1002/acp.3900








Applied Cogni8ve Psychology




36


1
















Jus]fica]on beliefs and mul]ple-documents comprehension




I
Bråten






L
E
Ferguson






H
I
Strømsø






Ø
Anmarkrud




10.1007/s10212-012-0145-2








European Journal of Psychology of Educa8on




28


3
















The Need for Cogni]on




J
T
Cacioppo






R
E
Pery








Journal of Personality and Social Psychology




42


1
















Epistemological beliefs and approaches to learning: Their change through secondary school and their influence on academic performance




F
Cano




10.1348/000709904X22683








Bri8sh Journal of Educa8onal Psychology




75


2
















The role of epistemic beliefs in predic]ng ci]zen interest and engagement with science and technology




H
Choung






T
P
Newman






N
Stenhouse




10.1080/21548455.2020.1774094








Part B: Communica8on and Public Engagement
















Changes in epistemological beliefs in elementary science students




A
M M
Conley






P
R
Pintrich






I
Vekiri






D
Harrison




10.1016/j.cedpsych.2004.01.004








Contemporary Educa8onal Psychology




29


2
















A new scale of social desirability independent of psychopathology




D
P
Crowne






D
Marlowe




10.1037/h0047358








Journal of Consul8ng Psychology




24


4
















An evalua]on of the belief in science scale




N
Dagnall






A
Denovan






K
G
Drinkwater






A
Parker




10.3389/fpsyg.2019.00861








Fron8ers in Psychology




10


861
















R
F
Devellis




Scale Development: Theory and Applica8ons










4th ed.). Sage








The Sa]sfac]on With Life Scale




E
D
Diener






R
A
Emmons






R
J L
Sem






S
Griffin








Journal of Personality Assessment




49


1














Examining Links Between Religion, Evolu]on Views, and Climate Change Skep]cism




E
H
Ecklund






C
P
Scheitle






J
Peifer






D
Bolger




10.1177/0013916516674246








Environment and Behavior




49


9


















S
Epskamp






S
S






N
J






V
M






J
T D




Path Diagrams and Visual Analysis of Various SEM Packages' Output
















Individual differences in intui]veexperien]al and analy]c-ra]onal thinking styles




S
Epstein






R
Pacini






V
Denes-Raj






H
Heier








Journal of Personality and Social Psychology




71


2
















A reflec]on on cogni]ve reflec]on -tes]ng convergent/divergent validity of two measures of cogni]ve reflec]on




N
Erceg






Z
Galić






M
Ružojčić




10.1017/s1930297500007907








Judgment and Decision Making




15


5


















A
Fasce






A
Picó




10.1002/acp.3501




Conceptual founda]ons and valida]on of the Pseudoscien]fic Belief Scale. Applied Cogni8ve Psychology






33














Epistemic beliefs and comprehension in the context of reading mul]ple documents: Examining the role of conflict




L
E
Ferguson






I
Bråten






H
I
Strømsø






Ø
Anmarkrud




10.1016/j.ijer.2013.07.001








Interna8onal Journal of Educa8onal Research




62
















Cogni]ve reflec]on and decision making




S
Frederick




10.1257/089533005775196732








Journal of Economic Perspec8ves




19


4
















On the Rela]on Between Religiosity and the Endorsement of Conspiracy Theories: The Role of Poli]cal Orienta]on. Poli8cal Psychology




M
Frenken






M
Bilewicz






R
Imhoff




10.1111/pops.12822








44














Post-Truth and the Epistemological Crisis




J
Friedman




10.1080/08913811.2023.2221502








Cri8cal Review






35














Epistemic beliefs' role in promo]ng mispercep]ons and conspiracist idea]on




R
K
Garrer






B
E
Weeks




10.1371/journal.pone.0184733








PLoS ONE




12


9
















Modeling epistemic and ontological cogni]on: Philosophical perspec]ves and methodological direc]ons




J
A
Greene






R
Azevedo






J
Torney-Purta




10.1080/00461520802178458








Educa8onal Psychologist




43


3
















A Meta-Analy]c Review of the Rela]onship Between Epistemic Cogni]on and Academic Achievement




J
A
Greene






B
M
Car]ff






R
F
Duke




10.1037/edu0000263.supp








Journal of Educa8onal Psychology




110


8
















Nega]ve avtude of highly educated parents and health care workers towards future vaccina]ons in the Dutch childhood vaccina]on program




E
Hak






Y
Schönbeck






H
De Melker






G
A
Van Essen






E
A M
Sanders




10.1016/j.vaccine.2005.01.074








Vaccine




23


24
















From the unbelievable to the undeniable: Epistemological pluralism, or how conspiracy theorists legi]mate their extraordinary truth claims




J
Harambam






S
Aupers




10.1177/1367549419886045








European Journal of Cultural Studies




24


4
















Sapiens: A brief history of humankind




Y
N
Harari












Random House








Dimensionality and Disciplinary Differences in Personal Epistemology




B
K
Hofer




10.1006/ceps.1999.1026








Contemporary Educa8onal Psychology




25


4
















The Development of Epistemological Theories: Beliefs About Knowledge and Knowing and Their Rela]on to Learning




B
K
Hofer






P
R
Pintrich








Review of Educa8onal Research




67


1
















Why Facts Are Not Enough: Understanding and Managing the Mo]vated Rejec]on of Science. Current Direc8ons in Psychological Science




M
J
Hornsey




10.1177/0963721420969364








29














Analy]c thinking outruns fluid reasoning in explaining rejec]on of pseudoscience, paranormal, and conspiracist beliefs




J
Jastrzębski






A
Chuderski




10.1016/j.intell.2022.101705








Intelligence




95














The science of psychology




R
S
Jhangiani






I
A
Chiang






C
Curler






D
C
Leighton










Research Methods in Psychology












4th ed.








How do people know?




D
Kuhn








Psychological Science




12


1
















The development of epistemological understanding




D
Kuhn






R
Cheney






M
Weinstock








Cogni8ve Development




15
















Applied Psychometrics: Sample Size and Sample Power Considera]ons in Factor Analysis (EFA, CFA) and SEM in General




T
A
Kyriazos




10.4236/psych.2018.98126








Psychology




08
















It's Our Epistemic Environment, Not Our Avtude Toward Truth




N
Levy




10.1080/08913811.2022.2149108








That MaRers. Cri8cal Review




35


1-2
















Beyond Misinforma]on: Understanding and Coping with the "Post-Truth" Era




S
Lewandowsky






U
K H
Ecker






J
Cook




10.1016/j.jarmac.2017.07.008








Journal of Applied Research in Memory and Cogni8on




6


4
















Mo]vated Rejec]on of Science




S
Lewandowsky






K
Oberauer




10.1177/0963721416654436








Current Direc8ons in Psychological Science




25


4
















Examining how people reason about controversial scien]fic topics. Thinking and Reasoning




E
J C
Lobato






C
Zimmerman




10.1080/13546783.2018.1521870








25














Limited not lazy: a quasi-experimental secondary analysis of evidence quality evalua]ons by those who hold implausible beliefs




K
A
Mar]re






B
Growns






A
S
Bali






B
Montgomery-Farrer






S
Summersby






M
Younan




10.1186/s41235-020-00264-z








Cogni8ve Research: Principles and Implica8ons






5












Thinking false and slow: Implausible beliefs and the Cogni]ve Reflec]on Test. Psychonomic Bulle8n and Review




K
A
Mar]re






S
G
Robson






M
Drew






K
Nicholls






K
Faasse




10.3758/s13423-023-02321-2








30














Besides knowledge: A crosssec]onal study on the rela]ons between epistemic beliefs, achievement goals, selfbeliefs, and achievement in science




L
Mason






P
Boscolo






M
C
Tornatora






L
Ronconi




10.1007/s11251-012-9210-0








Instruc8onal Science




41


1
















Conspiracy Endorsement as Mo]vated Reasoning: The Modera]ng Roles of Poli]cal Knowledge and Trust




J
M
Miller






C
E
Farhart




10.1111/ajps.12234








American Journal of Poli8cal Science




60


4
















I Just Don't Trust Them: The Development and Valida]on of an Assessment Instrument to




L
Nadelson






C
Jorcyk






D
Yang






M
Smith






S
Matson






K
Cornell






V
Hus]ng




10.1111/ssm.12051








Measure Trust in Science and Scien]sts. School Science and Mathema8cs




114


2
















Why we believe: Religion as a human universal




A
Norenzayan








Human morality and sociality: Evolu8onary and compara8ve perspec8ves


H. Hogh-Olesen




Palgrave Macmillan
















Cogni]ve reflec]on, cogni]ve intelligence, and cogni]ve abili]es: A meta-analysis. Intelligence




I
Otero






J
F
Salgado






S
Moscoso




10.1016/j.intell.2021.101614








90












Epistemic orienta]on short scale: Development and validity evidence in a sample of psychotherapists 1




E
M S
Palma






S
M G
Gondim






C
V N
Aguiar




10.1590/1982-4327e2817








Paideia




69


28














Insight Problem Solving, and the Implica]ons for Understanding Real-World Judgments and Beliefs




N
Patel






S
G
Baker






L
D
Scherer




10.1037/xge0000592








Journal of Experimental Psychology: General




148


12










Evalua]ng the Cogni]ve Reflec








Analy]c cogni]ve style predicts religious and paranormal belief




G
Pennycook






J
A
Cheyne






P
Seli






D
J
Koehler






J
A
Fugelsang




10.1016/j.cogni]on.2012.03.003








8














Lazy, not biased: Suscep]bility to par]san fake news is beRer explained by lack of reasoning than by mo]vated reasoning. Cogni8on




G
Pennycook






D
G
Rand




10.1016/j.cogni]on.2018.06.011








188














Forms of intellectual and ethical development in the college year: A scheme




W
G
Perry








Holt, Rinehart and Winston












Introduc]on to scien]fic thinking




G
J
Privitera








Research Methods for the Behavioral Sciences




Sage Publica]ons










2nd ed.








R: A language and environment for sta8s8cal compu8ng. R Founda]on for Sta]s]cal Compu




R Core Team






















W
Revelle






Procedures for Psychological, Psychometric, and Personality Research


edures for Psychological, Psychometric, and Personality Research










2.4.1








Development of reliable and valid short forms of the marlowecrowne social desirability scale




W
M
Reynolds








Journal of Clinical Psychology




38


1
















lavaan: An R Package for Structural Equa]on Modeling




Y
Rosseel








Journal of Sta8s8cal SoZware




8


2














Understanding Vaccine Hesitancy in the Context of COVID-19: The Role of Trust and Confidence in a Seventeen-Country Survey




L
S
Rozek






P
Jones






A
Menon






A
Hicken






S
Apsley






E
J
King




10.3389/ijph.2021.636255








Interna8onal Journal of Public Health




66


636255














Believing, bonding, behaving, and belonging: The big four religious dimensions and cultural varia]on




V
Saroglou








Journal of Cross-Cultural Psychology




42


8
















Effects of beliefs about the nature of knowledge on comprehension




M
Schommer








Journal of Educa8onal Psychology




82


3
















Development and valida]on of the Epistemic Belief Inventory (EBI)




G
Schraw






L
D
Bendixen






M
E
Dunkle








Personal epistemology: The psychology of beliefs about knowledge and knowing


B. K. Hofer & P. Pintrich


















Individual differences in epistemically suspect beliefs: the role of analy]c thinking and suscep]bility to cogni]ve biases. Thinking and Reasoning




J
Šrol




10.1080/13546783.2021.1938220








28














Epistemic ra]onality: Skep]cism toward unfounded beliefs requires sufficient cogni]ve ability and mo]va]on to be ra]onal




T
Ståhl






J
W
Van Prooijen




10.1016/j.paid.2017.10.026








Personality and Individual Differences




122
















Moralized ra]onality: Relying on logic and evidence in the forma]on and evalua]on of belief can be seen as a moral issue




T
Ståhl






M
P
Zaal






L
J
Skitka




10.1371/journal.pone.0166332








PLoS ONE




11


11














Natural myside bias is independent of cogni]ve ability. Thinking and Reasoning




K
E
Stanovich






R
F
West




10.1080/13546780600780796








13














Analy]c thinking reduces belief in conspiracy theories




V
Swami






M
Voracek






S
S]eger






U
S
Tran






A
Furnham




10.1016/j.cogni]on.2014.08.006








8














Research: The search for knowledge




G
C
Thomas




10.1007/978-3-030-64865-7








Research Methodology and Scien8fic Wri8ng




Springer Interna]onal Publishing










2nd ed.








Inves]ga]ng an alternate form of the cogni]ve reflec]on test




K
S
Thomson






D
M
Oppenheimer




10.1017/s1930297500007622








Judgment and Decision Making




11


1
















Uses of Factor Analysis in Counseling Psychology Research




H
E A
Tinsley






D
J
Tinsley








Journal of Counseling Psychology




34


4
















Cultural learning




M
Tomasello






A
C
Kruger






H
H
Ratner








Behavioral and Brain Sciences




16


3
















The importance of epistemology for the study of misinforma]on




J
Uscinski






S
Lirrell






C
Klofstad




10.1016/j.copsyc.2024.101789








Current Opinion in Psychology




Elsevier B




57












I Did My Own Research




A
Vranic






I
Hromatko






M
Tonković




10.3389/fpsyg.2022.931865








Overconfidence, (Dis)trust in Science, and Endorsement of Conspiracy Theories. Fron8ers in Psychology, 13
















ggplot2: Elegant Graphics for Data Analysis




H
Wickham










Springer-Verlag












The Religious Commitment Inventory-10: Development, refinement, and valida]on of a brief scale for research and counseling




E
L
Worthington






N
G
Wade






T
L
Hight






J
S
Ripley






M
E
Mccullough






J
W
Berry






M
M
Schmir






J
T
Berry






K
H
Bursley






L
Connor




10.1037/0022-0167.50.1.84








Journal of Counseling Psychology




50


1
















Reflec]ve thinking predicts lower conspiracy beliefs: A meta-analysis




B
E
Yelbuz






E
Madan






S
Alper




10.1017/s1930297500008913








Judgment and Decision Making




17


4
















I feel it in my gut




D
G
Young






E
K
Maloney






A
Bleakley






J
B
Langbaum








Epistemic Mo]va]ons, Poli]cal Beliefs, and Mispercep]ons of COVID-19 and the 2020




















U
S
Presiden






Elec




10.5964/jspp.7823








Journal of Social and Poli8cal Psychology




10


2















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]