You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Guideline for Reporting Standards of Eye-tracking Research in Decision Sciences
Reproducibility is an important element in ensuring that insights from scientific studies stand the test of time 
(Open Science Collaboration, 2015)
. A critical component of reproducibility is transparency in reporting how data were collected, aggregated and analyzed.
Specifically, the method section should provide information on "how the study was conducted, including conceptual and operational definitions of the variables used in the study." 
(VandenBos, 2010, p. 29)
.
In recent years, reproducibility has gained increased focus, and several calls have been made for more comprehensive and transparent reporting 
(Asendorpf et al., 2013;
Eich, 2014;
Nosek et al., 2015;
Spellman, 2015;
Vazire, 2015)
. One way to improve the desired transparency and reproducibility is to standardize reporting guidelines. Such guidelines typically contain a list of items that inform authors about what technical and analytical details should be reported (e.g., 
JARS, 2008)
. While some reporting guidelines are general, for instance, the number of participants, each discipline and area is likely to require more specific reporting. As a consequence, several specialized guidelines have been developed over the years, for instance, reporting of fMRI and MRI studies 
(Poldrack et al., 2008
, Nichols, Das, Eickhoff, Evans et al., 2017
, experiments conducted on the internet 
(Reips, 2002)
, computational studies 
(Stodden et al., 2016)
 and meta-analyses 
(Moher, Liberati, Tetzlaff, & Altman, 2009)
.
Despite this diversity in the guidelines, many disciplines are still lacking such specialized reporting standards. One of these areas is eye-tracking research. In recent years, the application of the eye-tracking methodology has grown rapidly in the behavioral sciences. Lower costs of eye-trackers, easier handling and a broader interest in process models in general have all contributed to this development 
(Schulte-Mecklenbeck et al., 2017)
. Nevertheless, eye-tracking requires complex apparatus and provides many degrees of freedom in terms of experimental and stimulus design, technical configuration, data pre-processing, and analysis 
(Orquin & Holmqvist, 2017)
. With a growing community of researchers and many degrees of freedom in running eyetracking studies, we believe that eye-tracking is in particular need of specialized reporting standards.
Reporting guidelines tend to be constructed through discussions among small groups of experts, and the criterion for inclusion or exclusion of items is agreement in discussion (e.g., 
JARS, 2008)
. While expert discussions may go a long way in determining important reporting issues, we aimed for an additional step to allow us to document the identification and in-or exclusion of each reported item. This approach has two distinct advantages. 1) The selection process is better documented; we are able to provide a full list of items that are of interest to eyetracking researchers (i.e., that have been reported in published papers), and that we considered for inclusion in the final reporting guideline. The informed reader may therefore decide for herself whether to report otherwise excluded items. 2) A systematic approach to developing reporting guidelines can easily be adopted as a blueprint by other disciplines in need of specialized guidelines.
We approached the development in two steps. First, we developed an inclusive list of reporting items by coding a large set of eye-tracking papers in behavioral decision-making published between 2007 and 2017. The list was enriched with items from the literature on eyetracking methodology. Second, this comprehensive list of items was evaluated by an expert panel who judged the necessity of each item for reproducibility. These steps result in (1) an extensive list including all items identified through our literature coding procedure. This list serves as a reference for conducting eye-tracking studies. It highlights the large set of decisions that have to be made during the design and data collection stage of an eye-tracking experiment. (2) The output of the expert panel is a condensed reporting guideline which includes only those items deemed necessary by a majority of experts. The reporting guideline is easier to apply than the complete list and we hope this will promote its adoption and thereby enhance the reproducibility and transparency of eye-tracking research. To ensure that the basic data are available to the interested reader we programmed a Shiny-App (https://decisionlab.shinyapps.io/iGuidelines/) that allows access to all the included citations and codings we report here.


Method
The reporting guideline was developed in five steps shown in 
Figure 1
 below. The main idea of this approach is to take advantage of the large body of literature to identify reporting items that might have been overlooked by individual experts while also allowing for the possibility that important items may have been ignored in the literature. This initial list was enriched by an additional set of items identified in the eye-tracking methodology literature. The full list contains 70 items related to eye-tracking methodology or analysis (see 
Table A1
 in the Appendix).
Literature search. To narrow down the available literature, we limited ourselves to eyetracking studies on behavioral decision-making. Through a literature search in the Web of Knowledge and Google Scholar databases, we identified 215 articles including a total of 268 empirical studies. Search keywords were combinations of 'eye-tracking', 'decision-making', 'choice' and 'judgment'. We identified additional articles by searching the reference lists of the respective articles. Only experimental research published between 2009 and 2017 was included.
Running head: REPORTING STANDARDS The articles identified covered a broad range of disciplines, such as cognitive science, marketing, economics, developmental research, vision research, and human-computer interaction.
Coding the literature. Each of the 215 articles included were coded on the 70 items by at least two independent coders. Disagreements between coders were resolved by a third coder. All four authors and three research assistants contributed to coding the articles. To train the coders and ensure coder reliability, an initial set of articles were coded and discussed by the entire team.
Codes were entered into an online form to limit errors.
Expert panel and final guideline. The 70 items were evaluated by 39 eye-tracking experts, who indicated the importance of each item for the evaluation and reproducibility of eye-tracking studies. The response options were 'necessary', 'helpful', 'not helpful', and 'no opinion'. We identified an expert as a researcher who had published at least one scientific article about eyetracking in a peer-reviewed journal (a criterion that excluded five participants in the survey).
Respondents in the final sample had published on average five articles on eye-tracking (M = 4.7, SD = 5.4, range = 1:20). The final reporting guideline consists of those items rated as necessary by the majority of experts (see 
Table 1
). 
Table A1
, that shows the percentages of articles reporting an item from the comprehensive item list and, additionally, the percentage of experts judging the items as necessary for the minimal reporting guideline. Based on the 30 items in the minimum reporting guideline we coded all 215 papers on whether an item was reported or not. The results of this coding are illustrated in 
Figure 2
. None of the articles coded reported enough information for reproducibility based on the minimal requirements we suggest. Approximately 70 percent of all papers reported less than 50 percent of the necessary items. Interestingly, there is little agreement between what is reported and what experts deem important. The correlation between the proportion of articles reporting an item and the proportion of experts indicating the item as necessary is r = -.02, p = .89.


Results


Summary statistics for the coding of all included articles is provided in
Running head: REPORTING STANDARDS 
Figure 2
. Overview of reporting practices in eye-tracking studies. Each paper is represented by a column (tick on the x-axis), a black rectangle denotes that the item given on the y-axis was reported. The list of items corresponds to the Minimal Guideline presented below, these items are explained in more detail below.


Reporting guideline
Despite this rather sobering picture in terms of what actually gets reported we believe that introducing a standard for reporting might increase these numbers substantially. Our suggested reporting guideline is shown in 
Table 1
 below. In what follows we explain all the included items in more detail and discuss reasons why the items are essential for reproducibility.  The recording of eye-movements objectively measure the spatial orientation of the eye and pupil size at a given point in time. Observable gaze behavior is often interpreted as an indicator of cognitive processes; in many behavioral experiments, the eye movement is not the objective of the study so much as the observable output of an underlying psychological process. When making inferences about unobserved cognitive processes based on eye movements, we typically have to make assumptions about the relation between observed and unobserved variables. These auxiliary assumptions may be more or less reasonable. For instance, researchers often wish to assume a strong relationship between the eye and the mind, the so-called eye-mind assumption 
(Just & Carpenter, 1980)
 which states that stimuli that what is currently fixated on is being processed cognitively. However, this assumption has been falsified in various instances 
(Huettig, Rommers & Meyer, 2011
, Orquin & Holmqvist, 2017
. Therefore, the auxiliary assumptions linking the dependent measurement with the construct under investigation must be reported. By explicitly stating auxiliary assumptions, other researchers may decide for themselves whether a dependent measure is construct valid and whether to trust the conclusions in the article.
Running head: REPORTING STANDARDS


Describe the Devices and Software
Eye-tracker. One of the most constitutive aspects of eye-tracking data is the eyetracker. The abundance of sampling techniques, sampling rates (e.g., 1000 vs. 30 Hz) and software has resulted in very diverse eye-tracking devices in terms of temporal and spatial precision and accuracy. Stating the model, producer, and type of eye-tracker is therefore no trivial matter as it will allow other researchers to consult documentation about the hardware.
However, it is important not to rely on the accessibility of online documentation from hardware producers as these may cease to exist and the websites with documentation may be deleted.
Monitor. Usually eye-tracking devices can be linked up to a wide set of monitors and projectors. As screen sizes and resolution may vary, their measures are required to make sense of the stimulus size in degrees of visual angle and the distance between specific areas of interest (AOIs). To enable reproducibility, measures on screen size and resolution must be available.
This will allow other researchers to mimic the original test and to obtain data on, for instance, saccades of similar length and AOIs of similar size. If a study is re-done with equipment of a different size or resolution, the outcome will most likely be imprecise stimuli and a different information search process.
Software. Eye-tracking studies can be organized using a variety of experimental software, much of which is offered by the equipment manufacturers (e.g., Experiment Center). Lately, however, open source alternatives, featuring interfaces for many eye-tracking devices, have emerged (e.g., Psychopy). All of this presentation software has its specific characteristics, such as randomization and latencies, which may affect the data. When collecting data, specifically when generating individual fixations or pupil dilations from raw gaze data, it takes quite some pre-processing of the data before the data set is usable. The various procedures of filtering, aggregation and event detection can be carried through using a software add-on with the eyetracking device. These software packages, or if the researchers' own pre-processing programs are used, will determine the quality of the data set to be analyzed subsequently. The settings for preprocessing programs (e.g., what is the maximum dispersion of a fixation, how is the pupil size being normalized) differ across eye-tracking providers and program versions. If individually scripted software is applied in a study, the source code should be published because the original software is the key to completely reproducing an experiment and any pre-processing of the data. fixations falling inside the area are assigned to the object of interest. Research has shown that how the AOI is specified both in terms of space and place may affect the data set and the outcome 
(Orquin, Ashby, Clarke, 2016)
. As a result, the absolute and relative size of the AOI, the distance between the AOIs and their potential overlap must be stated. Furthermore, even the slightest details such as the interaction between the individual eye-tracker characteristics (i.e., accuracy and precision) and the size of the AOI are so sensitive that the data set and results may change.
Stimuli. Eye-tracking studies and the visual stimuli used are interlinked. Therefore the stimuli used as well as the stimulus preparation method must be specified. Also, whether stimuli are luminance matched must be reported for pupillometric eye-tracking research. Luminance matching reduces the volatility of the pupil size due to changes in light conditions and is of particular importance when we want to measure pupil dilation. If differences in light sources can be ruled out, it is easier to interpret pupil data and it also reduces noise when collecting eye position measures.
Procedure. To give a full account of how to conduct an eye-tracking study, a number of questions concerning the length and form of the stimuli presentation as well as the experiment setup must be answered. All these elements have an impact on the validity of the study and its conclusions. The length of the inter-stimulus interval (the time between the stimulus presentations) should be stated as this piece of information is crucial in terms of resting phases and potential baseline measures, because it allows assessing the relative differences in pupil size between when the eye is stimulated and when it is at a baseline. Since pupil contraction has some latency, it needs time to return to its baseline (approximately 4 s). A fixation cross before the stimulus presentation serves to focus or divert attention on or away from a particular position on the screen. Information on its position (e.g., at the middle of the screen) and presentation length therefore improves the evaluation of the meaning of the first fixation. Another equally important item in the interpretation of the data is the length of the stimulus presentation; for example, a fixed exposure time of 2000 ms vs free exposure time permitting participants to decide when to Running head: REPORTING STANDARDS continue, calls for radically different interpretations of the same analysis. With free exposure time, the number and duration of fixations can easily be compared between AOIs or participants, while the fixed exposure time makes it easier to compare when AOIs are fixated. In free exposure, trials are typically of different lengths, and comparing when AOIs are fixated raises the question of timing relative to the beginning or end of the trial or the proportion of trial length.
When multiple AOIs are involved, there is the issue of counterbalancing -that is, do the objects of interest switch position (e.g., between the top and bottom or left and right side of the screen). If counterbalancing is neglected, the interpretation of the eye movement distribution may be distorted as the eye movements under question may be confounded with the natural reading direction 
(Orquin & Holmqvist, 2017)
. Temporal ordering of the stimuli (e.g., random or fixed presentation order) may also be an issue as are the number of trials. At the outset of eye tracking studies, decision times tend to be long and more information tends to be scrutinize in comparison to later on in the study (e.g., 
Fiedler, GlÃ¶ckner, Nicklisch & Dickert, 2013)
. 
Orquin, Bagger and Mueller Loose (2013)
 used this observation to demonstrate that there is an increase in top-down control of eye movements over time. Particularly for within subject designs, it is relevant to know the type of stimuli and their temporal ordering. For the purpose of understanding the surroundings of the data collection and how it was controlled, all details about the location used must be described (e.g., in a controlled lab environment versus a public area, etc.).


Data Pre-processing
Dependent measures. Before the data analysis most researchers pre-process their data with one or more data aggregations and transformation steps, for instance, aggregation of fixations (i.e., defining the minimum fixation length and maximum dispersion) or pupil size (i.e., algorithm used to calculate pupil size e.g., as a difference measure to baseline or as a relative change in size) from raw data. Eye-tracking equipment providers often supply pre-processing programs that can handle these aggregations. Data characteristics must be reported as the provider-set defaults are not standardized, and decisions should be made in accordance with the stimuli used 
(Manor & Gordon, 2003)
. Besides the standard algorithms, many analyses are based on within-AOI fixation aggregations or other transformations such as computation of transition indices or metrics. The computation of these metrics will, of course, affect subsequent analyses; sometimes the correct computation can be disputed, for instance, in terms of transition index development 
(BÃ¶ckenholt & Hynan, 1994)
.
Data Quality. To understand the conclusions drawn from an experiment, it is important to be able to assess the quality of the data, and further the quality of the results 
(Orquin & Holmqvist, 2017)
. Data quality in eye-tracking studies can refer to several, often general measures in eye-tracking studies: the proportion of participants and trials excluded from the analysis and the reasons for these exclusions, the percentage of data samples for which the eyetracker could not obtain the gaze position, also referred to as lost data, and whether data quality was monitored during the experiment.


Discussion -Limitations
In eye-tracking studies, there are certain limitations due to the dependent measures and the construct of interest being separated. For example, a fixation cannot always be interpreted as information processing, and the absence of a fixation cannot necessarily be interpreted as the absence of processing as eye-trackers do not measure the use of peripheral vision. Stimulus design, sample and data quality issues must also be reported and discussed to provide a transparency in the warranted and unwarranted conclusions. Conveying these details to readers allow them to draw their own conclusions with regard to the validity of the results and their interpretation.


Discussion
Transparent reporting of scientific research is, without a doubt, one of the stepping stones to replicability and scientific progress. However, it can be difficult to know what information is necessary to report for others to assess and replicate our research. Several guidelines have been proposed to encourage more complete and transparent reporting. These guidelines range from the general, such as the APA reporting guideline that encompasses most psychology articles, to the more specific, such as reporting guidelines for meta-analyses, online studies, or computational studies 
(Field et al., 2008;
Moher, Liberati, Tetzlaff, Altman, & Group, 2009;
Poldrack et al., 2008;
Reips, 2002;
Taylor et al., 2008;
Tooth, Ware, Bain, Purdie, & Dobson, 2005
).
Here we focus on eye-tracking research, an area undergoing explosive growth, and for which no specific reporting guidelines exist. We approach the development of the guideline in a systematic way. By coding eye-tracking papers in behavioral decision-making from 2007 to Running head: REPORTING STANDARDS 2017, we developed a list of reporting items; we then let a panel of experts judge the necessity of each item for reproducibility. We include all items in the guideline judged as necessary by the majority of experts and present both the complete list of items and the shorter reporting guideline. The complete list is useful when conducting eye-tracking studies in that it helps uncover many of the methodological decisions eye-tracking researchers have to make. The shorter reporting guideline is mainly intended as an assistance for documenting the study in scientific journal articles.
The guideline is not intended as an inflexible requirement for publication but rather as a dynamic list that may change over time as the hardware, software and methodologies evolve. We address the dynamic character of the field in providing a web-based platform that is open to explore the collected data in more detail 1 . We do wish to emphasize that the guideline cannot replace critical reflection. A common approach to methods and reporting is to copy previous research, which unfortunately can lead to propagation of errors, such as the exclusion of fixations shorter than 300 msec 
(Orquin & Holmqvist, 2017)
. A better approach is to reflect on the particulars of each study and make decisions with regard to methods and reporting that are justified by logical and scientific arguments, not by what previous researchers have done. In relation to this guideline, we therefore hope that it will not replace critical reflection but rather serve as a tool for better methodological reasoning.   
Figure 1 .
1
Overview of the steps in the development of the reporting guideline Developing the coding scheme. Based on an initial set of 12 eye-tracking articles, a list of criteria was developed and revised in four rounds through discussions between all four authors.


Software used to pre-process the eye-tracking data Stimulus presentation software Material Description of how AOIs were defined Absolute size of the AOIs Relative size of AOIs and content within the AOIs Minimal distance between AOIs Overlap between the AOIs Description of the stimulus Method for stimulus preparation Luminescence matched Procedure Setup Inter stimulus interval Length of fixation cross presentation Position of the fixation cross Length of stimulus presentation Counter balancing of the position Order of stimulus presentation Running head: REPORTING STANDARDS Number of trials Settings and locations where data was collected Results Data quality Monitoring of data quality during experiment Proportion of trials excluded for the analysis Reasons for exclusion Number of participants excluded from the analysis Quality threshold for data exclusion Percentage of lost data Dependent measures Aggregation method for fixations Additional transformation of the data Discussion Limitations due to the use of eye-tracking methodology Introduction (auxiliary assumptions)


Running head: REPORTING STANDARDS Material Areas of Interest. Most eye-tracking research relies on areas of interest (AOI) for data aggregation and analysis. AOIs are defined as an area around an object of interest, and all


Table 1 .
1
The reporting guideline for eye-tracking research in behavioral sciences.


https://decisionlab.shinyapps.io/iGuidelines/.








Appendix
 












J
B
Asendorpf






M
Conner






F
De Fruyt






J
De Houwer






J
J
Denissen






K
Fiedler






.
.
Perugini






M




Recommendations for increasing replicability in psychology






















European Journal of Personality




27


2














Caveats on a process-tracing measure and a remedy




U
BÃ¶ckenholt






L
S
Hynan








Journal of Behavioral Decision Making




7


2
















Business not as usual




E
Eich








Psychological Science




25


1
















Social value orientation and information search in social dilemmas: An eye-tracking analysis. Organizational behavior and human decision processes




S
Fiedler






A
GlÃ¶ckner






A
Nicklisch






S
Dickert








120














The minimum information about a genome sequence (MIGS) specification




D
Field






G
Garrity






T
Gray






N
Morrison






J
Selengut






P
Sterk






.
.
Ashburner






M








Nature Biotechnology




26


5
















Using the visual world paradigm to study language processing: A review and critical evaluation




F
Huettig






J
Rommers






A
S
Meyer








Acta psychologica




137


2
















Reporting standards for research in psychology: why do we need them? What might they be?






The American Psychologist




63


9










JARS








Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement




D
Moher






A
Liberati






J
Tetzlaff






D
G
Altman








Annals of internal medicine




151


4
















Best practices in data analysis and sharing in neuroimaging using MRI




T
E
Nichols






S
Das






S
B
Eickhoff






A
C
Evans






T
Glatard






M
Hanke






.
.
Proal






E








Nature Neuroscience




20


3
















Promoting an open research culture




B
A
Nosek






G
Alter






G
C
Banks






D
Borsboom






S
D
Bowman






S
J
Breckler






.
.
Contestabile






M








Science




348


6242
















Estimating the reproducibility of psychological science






Open Science Collaboration




349


6251


4716








Science








Areas of interest as a signal detection problem in behavioral eye-tracking research




J
L
Orquin






N
J
Ashby






A
D
Clarke








Journal of Behavioral Decision Making




29


2-3
















Running head: REPORTING STANDARDS












Learning affects top down and bottom up modulation of eye movements in decision making




J
L
Orquin






M
P
Bagger






S
Mueller Loose








Judgment and Decision Making




8


6
















Threats to the validity of eye-movement research in psychology




J
L
Orquin






K
Holmqvist








Behavior research methods




50


4
















Guidelines for reporting an fMRI study




R
A
Poldrack






P
C
Fletcher






R
N
Henson






K
J
Worsley






M
Brett






T
E
Nichols








Neuroimage




40


2
















Standards for Internet-based experimenting




U
D
Reips








Experimental Psychology




49


4
















Process-tracing methods in decision making: On growing up in the 70s




M
Schulte-Mecklenbeck






J
G
Johnson






U
BÃ¶ckenholt






D
G
Goldstein






J
E
Russo






N
J
Sullivan






M
C
Willemsen








Current Directions in Psychological Science




26


5




















V
Stodden






M
Mcnutt






D
H
Bailey






E
Deelman






Y
Gil






B
Hanson














Enhancing reproducibility for computational methods




M
Taufer








Science




354


6317




















C
F
Taylor






D
Field






S
A
Sansone






J
Aerts






R
Apweiler






M
Ashburner














Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project




A
Brazma








Nature Biotechnology




26


8
















Quality of reporting of observational longitudinal research




L
Tooth






R
Ware






C
Bain






D
M
Purdie






A
Dobson








American Journal of Epidemiology




161


3
















Publication manual of the




G
R
Vandenbos








American Psychological Association


Washington, DC






6th ed.












S
Vazire




head: REPORTING STANDARDS






Editorial. Social Psychological and Personality Science




7


1

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]