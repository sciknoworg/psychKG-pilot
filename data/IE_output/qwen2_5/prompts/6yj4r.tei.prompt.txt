You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1–3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



the sample values are normally (i.e., Gaussian) distributed, whereas anti-compression may be beneficial when the distribution is uniform (e.g., 
Li et al., 2017;
Spitzer et al., 2017;
Summerfield & Li, 2018)
.
However, other factors may also play a role in the use of compressive or anti-compressive weighting (see also 
Summerfield & Parpart, 2021)
. Many of the experimental tasks in which anti-compression has been observed posed relatively high cognitive demands, for instance, requiring evaluation of relational information in rapid sequential displays (e.g., 
Tsetsos et al., 2012;
Tsetsos et al., 2016)
, and the degree of anti-compression has been found to increase with task complexity 
(Spitzer et al., 2017;
Vanunu et al., 2019)
. Compression, in contrast, has often been observed in more direct perceptual judgments of ensemble information, such as the average magnitude or orientation of a stimulus set (de Gardelle 
& Summerfield, 2011;
Katzin et al., 2021;
Li et al., 2017;
Vandormael et al., 2017)
. The type of distortion observed (i.e., compression or anti-compression) may thus hinge on the processing resources required to evaluate the sample information in the context of the specific decision task at hand.
Here, we show in simulations that compression confers performance benefits under noise (see also 
Juechems et al., 2021;
Li et al., 2017)
 only when the individual samples in a set can be evaluated with relatively little processing effort. In contrast, when their evaluation is more demanding-such that limited processing resources need to be traded off between the different samples in a trial-we find the optimal policy to be anti-compression. In other words, whether compression or anti-compression is the optimal policy for noisy observers may depend on the sample-wise processing requirements imposed by the task. We tested this proposal in a large participant sample (N = 586), manipulating the task requirement (simple averaging vs. comparison of number sequences; see Methods). We additionally manipulated the distribution of sample values (uniform vs. Gaussian, see above, 
(Li et al., 2017;
Spitzer et al., 2017)
), and their range (1 to 9 vs. 100 to 900). We considered the role of range because there is some evidence that nonlinear distortions might be more pronounced with higher than with lower numbers 
(Birnbaum & Chavez, 1997;
Wakker & Deneffe, 1996)
. Our result show that the type of distortion (compression or anti-compression) was determined solely by the processing requirement of the task, indicating that participants adopted the best weighting policy in the respective decision context. Thus, we present a theoretical framework supported by empirical data to explain a previously puzzling heterogeneity in findings, namely, that decision makers sometimes overweight and sometimes underweight extreme values in multi-sample judgments.


Methods


Transparency and Openness
All data and analysis code as well as experimental materials are available at https: //osf.io/x83pk 
(Clarmann von Clarenau et al., 2022)
. The data were analyzed using Matlab version R2020b (The MathWorks, 2020a), with the Statistics and Machine Learning Toolbox (The MathWorks, 2020e), the Econometrics Toolbox (The MathWorks, 2020b), the Parallel Computing Toolbox (The MathWorks, 2020c), the Optimization Toolbox (The MathWorks, 2020d), and the BayesFactor Toolbox 
(Krekelberg, 2022)
. The study design and analyses were not preregistered. The study was approved by the ethics committee of the Max Planck Institute for Human Development.


Participants
We aimed at recruiting 800 young adults (n=100 per condition) via Prolific (https: //www.prolif ic.co). The sample sizes were inferred from model simulations and from previous in-lab studies of psychometric distortions, and were generously increased to account for anticipated drop-outs in online testing. Of the respondents, N = 778 participants (222 female, 442 male, 114 data on sex unavailable; mean age 24.83 ±5.02 years, range 18-41 years) took part in the experiment. All participants gave written informed consent prior to performing the experiment and received a basic reimbursement of 5.40 GBP per hour and a performance-dependent bonus of up to 0.9 GBP. Participants were excluded if they failed on attention checks (see below) or if their performance in the main task was not significantly OVER-AND UNDERWEIGHTING OF EXTREME VALUES 4 above chance (p < 0.01, binomial test against 0.5, corresponding to at least 56% correct choices). We further excluded n = 13 subjects who had participated repeatedly (in different conditions). After exclusion, n = 586 participants (185 female, 390 male, 11 data on sex unavailable; M age = 24.74 ±4.85 years) were retained in the analysis (mean n = 73.4 per experimental condition; min: 68, max: 78).


Stimuli, Task, and Procedure
The experiment was performed online using Qualtrics (https://www.qualtrics.com). In each of the 8 experimental conditions, participants were asked to judge sequences of 8 number samples displayed in red or blue ( 
Figure 1a
). The beginning of each trial was indicated by a fixation cross displayed in the middle of the screen for 1 second. Subsequently, 8 samples (Arabic digits; 4 in red and 4 in blue font, presented in random order) were sequentially displayed at fixation at a rate of 350ms per sample. Each sample was softly faded out after 270ms to smoothen stimulus transitions. The sample values were drawn from the range of either 1 to 9 (in steps of 1; small-range conditions) or 100 to 900 (in steps of 100; large-range conditions).
After the last sample, participants were asked to enter a binary decision via key press. In the "averaging task" (single-stream conditions), participants were asked to indicate whether the average of all samples (regardless of color) was larger or smaller than a reference value (5 in the small-range conditions; 500 in the large-range conditions). In the "comparison task" (dual-stream conditions), they were asked to indicate whether the red samples had a higher average value than the blue samples or vice versa. Thus, both tasks required participants to evaluate all 8 number samples. The two tasks differ in the processing requirements associated with each sample: in the single-stream condition, all numerical sample values are evaluated within the same frame of reference (i.e., a larger number always evidences "larger"). In the dual-stream condition, in contrast, the decision value of a sample flips depending on its color (i.e., a red number evidences "red" when it is large, but "blue" when it is small, and vice versa for blue numbers; see Computational Modeling, Eq. 2), which renders its evaluation inherently more effortful. In both tasks, a response had to be given within 3 seconds; otherwise a time-out message was displayed and the trial was discarded from analysis (this applied to 0.43% of trials, on average).
We also manipulated the distribution from which the sample values were drawn ( 
Figure   1b
). In uniform conditions 
(Figure 1b, upper panel)
, the sample values were drawn from a uniform distribution. In Gaussian conditions 
(Figure 1b, lower panel)
, the values were drawn from truncated normal distributions with a standard deviation of σ = 3 (small-range conditions) or σ = 300 (large-range conditions). To compensate for anticipated differences in choice difficulty, we moderately shifted the Gaussian mean away from the midpoint of the sample range (by 0.6 in the small-range and by 60 in the large-range conditions); we derived the magnitude of the shift that would be necessary to approximate similar performance levels as with uniform samples from pilot data and model simulations. In the single-stream (averaging) conditions, positive/negative shifts were randomly varied across trials. In the dual-stream (comparison) conditions (where red and blue samples had opposite decision values, 
Figure 1b
), the shifts were applied with opposite signs (positive/negative) to the distributions from which the red and blue samples in a trial were drawn (with random assignment across trials). Trials on which no objectively correct response could be given because the mean of the number stream was exactly 5 (single-stream condition; 5.67% of trials) or identical for red and blue samples (dual-stream condition, 5.32% of trials) were excluded from analysis. The three experimental factors (i.e., "task", "range", and "distribution") were fully crossed in a 2 × 2 × 2 between-subjects design. In each group, participants performed 250 trials in blocks of 50. After each block, summary performance feedback was provided (percentage of correct choices). After every 25th trial, participants were asked to perform a brief attention check task consisting of 4 trials. Here, they were shown a geometric shape (square or circle) and asked to indicate its name via key press. If a participant passed fewer than 3 (75%) of the trials on any given attention check, the experimental session was terminated and their data were discarded (see Participants). After the main task, participants in the Gaussian conditions additionally performed the Berlin Numeracy Test 
(Cokely et al., 2012
) and a brief number line estimation task ("number-to-position" 
;
Siegler & Opfer, 2003)
, the results of which are not reported here. Participants who successfully completed the experiment were paid a performance-dependent bonus (up to 0.9 GBP) on top of their basic reimbursement. The study was approved by the ethics committee of the Max Planck Institute for Human Development.


Descriptive Analysis
We used a reverse correlation approach 
(Neri et al., 1999;
Spitzer et al., 2016)
 to calculate decision weights for each sample value (1-9). In the single-stream conditions, we calculated for each sample value (e.g., 3) the proportion of times a sample of this value was followed by a "larger" choice. In the dual-stream condition, decision weights for each sample value were computed analogously, as the proportion of times the color of the sample (i.e., red or blue) was subsequently chosen to be larger. In either task, the choice proportions quantify the influence a sample value wielded on a participant's decision, yielding a psychometric weighting function over sample values, where a choice proportion of 0.5 (indifference) corresponds to zero decision weight (see 
Figure 3
, left and right y-axis labels). For comparison with model predictions (see below), we computed analogous weighting functions also from the choice probabilities (Eq. 5) predicted under the best-fitting model parameterization for each participant.


Psychometric Model
We modeled decisions in our tasks using a simple psychometric model as reported in 
Spitzer et al. (2017)
. In the model, each sample value X (normalized to range from −1 to 1) is transformed into a subjective decision value dv according to a sign-preserving power function of the form:
dv = X + b |X + b| × |X + b| k ,
(1)
where k < 1 implies underweighting (i.e., compression) and k > 1 implies overweighting (i.e., anti-compression) of extreme values, relative to a linear transformation (i.e., k = 1; 
Figure   2a
) 
DV = N i=1 dv i × c i g ,
(2)
where c i is an indicator variable (+1 or −1) that codes the color category of the sample (i.e., red or blue). In the dual-stream conditions, c i effects a sign-flip of dvs for one color relative to the other, effectively implementing a comparison between the two color categories. In the single-stream conditions (where color is irrelevant), c i is fixed at +1.
g is a scaling factor that normalizes the gain of the transformation in Eq. 2 (quantified by the integral of the decision values; 
Figure 2a
-c) to be constant for any values of k and b:
g = |f + b| k |f | .
(3)
We considered two variants of gain normalization. In one variant 
(Figure 2b
), we defined f as the 9 possible input values of X (see Eq. 1), which normalizes the function gain over the range of stimuli that could potentially occur in the experiment (e.g., 1 to 9 in the small-range condition). In a second, more refined variant, we computed g on the individual trial level,
where f refers to the concrete sequence of input values X presented on a given trial (see 
Figure 2c
). This second type of normalization ensures that equivalent gain of processing is available for each individual number sequence, for every parameterization of Eq. 2 (see


Simulation Results for details).
To capture potential recency effects (i.e., greater weighting of samples occurring closer to the decision; 
Hogarth & Einhorn, 1992)
, we also included a leakage parameter l that modulates the weight of a sample as a function of its temporal position i = 1...N (with N = 8 samples) in the number stream:
DV = N i=1 dv i × c i g × (1 − l) N −i ,
(4)
where larger values of l indicate a stronger recency effect. The trial-level DV was then transformed into a choice probability CP using a logistic choice rule,
CP = 1 1 + e −DV s ,
(5)
where CP is the probability of choosing "larger" (single-stream condition) or "red > blue"
(dual-stream condition) and s reflects decision noise (with higher values indicating more random choices).


Parameter Estimation
The psychometric model was fitted to the experimental data of each participant individually by minimizing the negative log-likelihood of the model given the observed choices.
Parameter estimation was restricted to ranges derived from previous work (k: 0-5, s: 0-5, b: 
Spitzer et al., 2017)
. Model performance was assessed using mean
−1-1, l: −0.5-1, see also
BICs on the group level and examined statistically using conventional inferential statistics (two-tailed). For our main analyses of the empirical data ( 
Figure 3
-5), we used the model without gain normalization (i.e., g was set to 1). Note that g is not a free model parameter but acts as a scaling factor on noise parameter s, which does not systematically affect the other model parameters or the model's goodness of fit 
(Spitzer et al., 2017
, see also Parameter Recovery below). When comparing the empirical estimates of k and s with those values that optimized performance in our simulations 
(Figure 2e-f)
, we fitted the model with the same gain-normalization settings as were used in the respective simulations to warrant comparability.


Parameter Recovery
To ensure that our estimated model parameters were valid, we performed parameter recovery simulations. Specifically, we iteratively simulated group data sets analogous to those obtained in our experiment. Across iterations, we varied the value of each parameter (in steps of 0.2) within its range (see above) while fixing the remaining parameters at their empirical estimates. Binary choices were generated by drawing for each trial from a binomial distribution with p = CP . We then fitted our model to the simulated choice data, using the 


Performance Simulations
We used the psychometric model to simulate task performance under different parameter settings. Accuracy was inferred from the predicted choice probabilities CP (if the correct choice was "larger"; 1 − CP otherwise). We simulated performance across different values of k (0 to 2.5 in steps of 0.01) and s (0 to 2 in steps of 0.01). For each value of k, we examined the difference in accuracy relative to linear (i.e., undistorted) transformation (i.e., k = 1) at any given noise level s (see 
Figure 2d
-f). In our a priori model simulations shown in 
Figure   2d
-f, we set l = 0 (i.e., no leakage) and b = 0 (i.e., no offset bias). However, qualitatively OVER-AND UNDERWEIGHTING OF EXTREME VALUES 10 very similar results were obtained when using the empirical estimates of l and b derived from our participants, both in the single-stream and in the dual-stream tasks. Note that in our model the single-and dual-stream conditions are formally equivalent (see Eq. 2), except for a difference in how bias (b) affects choice (see also Results). Thus, the simulation results illustrated in Figures 2d-f hold a priori for averaging-and comparison tasks alike.


Results


Simulation Results
We examined choice behavior in a sequential averaging/comparison task 
(Figure 1)
 where observers judge a sequence of 8 numbers. We used a generic psychometric model that formalizes single-stream (i.e., mean >/< 5) and dual-stream judgments (i.e., mean[red]>/<mean 
[blue]
) in exactly the same way (see Methods). The simulation results reported in the following thus hold identically for both types of task. In simulating the effects of compressive or anti-compressive distortions of sample values (here 1 to 9) on task performance, we considered the nature of potential processing limitations in our multi-sample tasks. At first, we replicated previous findings showing that, without further assumptions, compressive distortions (k < 1) are performance-maximizing in the face of decision noise (s ≫ 0; Figure 2a and d; see also e.g., 
Juechems et al., 2021;
Li et al., 2017)
.
However, compressive transformations are also characterized by overall greater gain (in terms of the integral of decision values |dv|) than linear or anti-compressive transformations 
(Figure 2a
, inset bar graph). In other words, a compressive weighting policy can be expected to be relatively more costly (e.g., in terms of metabolic resources) for biological observers 
(Baddeley et al., 2000;
Kostal et al., 2013;
Li et al., 2017)
.
It is commonly assumed that the gain of neural information processing is a finite resource 
(Cowan, 2001;
Lennie, 2003;
Marois & Ivanoff, 2005;
Tombu et al., 2011)
. Thus, in some task contexts (e.g., multi-sample tasks with high computational demands), giving a higher decision weight to one sample may come at the cost of other samples. For instance, selectively focusing on one type of stimulus may lead to reduced processing of other stimuli 
(Alonso et al., 2011;
Eldar et al., 2013)
. In previous work, such processing limit was modeled by normalizing the function gain |dv| over the range of input values (here, 1 to 9) for every value of k 
(Figure 2b
, see Eq. 3; see 
Li et al., 2017;
Spitzer et al., 2017)
. When we repeated the present simulation with this gain normalization, compression (k < 1) maximized performance only for Gaussian distributed samples 
(Figure 2e
, lower panel; see also 
Li et al., 2017)
. For uniformly distributed samples, in contrast, performance was maximized by anticompression (k > 1; 
Figure 2e
, upper panel; see also 
Spitzer et al., 2017)
.  Critically, for uniformly distributed samples, the normalization shown in 
Figure 2b
 is equivalent to limiting the gain of processing that is available to an observer (on average) on any given trial. For Gaussian distributed samples, in contrast, compressive distortions (k < 1) will still require a higher gain of processing (i.e., greater resources) overall, since mid-range samples (e.g., 4 or 6), which are more resource-intensive under k < 1 (see 
Figure   2b
), will occur relatively more frequently. To resolve this imbalance (see bar graph in 
Figure   2b
), we implemented gain normalization on the single-trial level 
(Figure 2c
), such that each trial sequence was transformed with equivalent gain of processing for any value of k (see bar graph in 
Figure 2b)
. Note that such trial-level gain normalization implements a hypothetical "what-if" scenario, where an observer would have processed each trial with the same cognitive resources, but in result of different distortions k would have allocated these resources differently to the individual values (1-9) of the sample range. When we repeated our simulations with this type of gain normalization, performance under noise (s > 0) was always maximized by anti-compression (k > 1), regardless of whether samples were uniformly or Gaussian distributed 
(Figure 2f
).
In other words, whereas compression can improve the performance of a noisy observer by increasing the overall gain of processing (effectively, leading to a steeper psychometric function), anti-compression is favorable in situations where processing resources are limited, as anti-compression yields higher performance with equivalent gain of processing per trial.
Importantly, our simulations predict that the type of distortion that optimized performance under noise will not primarily depend on the distribution (i.e., uniform or Gaussian; see 
Li et al., 2017;
Spitzer et al., 2017;
Summerfield and Li, 2018)
 from which the samples were drawn. Instead, it should hinge on the extent to which limited processing resources need to be traded off between the different samples in a trial. We assume this trade-off to be weak (favouring compression as an optimal policy) in tasks where evaluating the decision value of a sample is relatively easy, and to be stronger (favouring anti-compression) when sample evaluation is more resource-intensive, so that it would exceed an observer's processing capacities to evaluate each sample in full.


Human Participant Results
We tested whether human observers would adopt performance-enhancing weighting policies in an online experiment with n = 586 participants. We manipulated the samplelevel processing demand of the task 
(Figure 1a
) by asking the participants to either judge the average of the whole stream (single-stream condition) or to decide whether the red or the blue numbers had a larger average (dual-stream condition). In the dual-stream condition, the decision value of a number sample flips depending on its color (see Methods, Psychometric Model), posing an additional processing requirement for each sample. Across subgroups of participants in each task, we further manipulated the range of sample values (1 to 9 or 100 to 900) as well as their distribution (uniform or Gaussian, see 
Figure 1b
).


Descriptive results
Participants' average choice accuracy was 79.80 ± 0.44% in the averaging task (singlestream conditions) and 75.52±0.44% in the comparison task (dual-stream conditions). A 2 × "range" (small, large) showed a main effect of "task" [F (1, 578) = 51.67, p < 0.001, η 2 = 0.08], confirming that the comparison task was more difficult. In addition, there was a main effect of "distribution" [F (1, 578) = 34.25, p < 0.001, η 2 =0.05], indicating higher performance in the Gaussian (M = 79.41 ± 0.44%) than in the uniform conditions (M = 75.96 ± 0.45%). No other main effects or interactions were significant (all F < 1.2, all p > 0.28, all η 2 < 0.002). We inspected descriptive weighting functions (see Methods) to gauge how strongly each numerical sample value contributed to choice (see 
Figure 3
, solid lines). The weighting functions showed different shapes depending on the task (averaging or comparison). While a compressive curve (i.e., relatively shallower local slopes near extreme values than near intermediate values) was evident in the averaging task (single-stream conditions), an anticompressive shape (i.e., steeper local slopes near extreme values) emerged in the comparison task (dual-stream conditions). Descriptively, this pattern was evident for both sample ranges (small and large; see 
Figure 3a
,b and 
Figure 3c
,d, respectively) and for both distribution types (see 
Figure 3
 top and bottom rows).


Modeling results
We next fitted our psychometric model to the empirical choice data. Our main interest was in the k parameter, which captures underweighting (k < 1; compression) or overweighting (k > 1; anti-compression) of extreme values. The best-fitting estimates of k 
(Figure 4a
) corroborate our observations with the psychometric weighting functions (see 
Figure 3
). In the averaging task (single-stream conditions), k was significantly For further inspection, we performed a 2 × 2 × 2 ANOVA (specified as for accuracy above)
of the estimates of k in each experimental condition. The analysis showed a main effect of task [single-or dual-stream: F (1, 578) = 311.28, p < 0.001, η 2 = 0.35], but no other main effects or interactions (all F < 3.46, all p > 0.06, all η 2 < 0.004). In other words, the shape of distortion (compression or anti-compression) was significantly modulated only by the "task" requirement (averaging or comparison), and not by the other factors under study ("range"
and "distribution").
We next examined effects of the manipulations on decision noise s 
(Figure 4b)
. A 2 × 2 × 2 ANOVA (specified as above) showed a main effect of "task" [F (1, 578) = 33.98, p < 0.001, η 2 = 0.05], reflecting that the dual-stream condition was more difficult than the single-stream condition (see also Descriptive Results; mean s: 1.41, SE = 0.07 vs. 0.9, SE = 0.05). Further, also mirroring the results for accuracy, there was a main effect of "distribution" [F (1, 578) = 7.08, p = 0.008, η 2 = 0.01], with lower s in the Gaussian than in the uniform conditions (mean s: 1.04, SE = 0.06, vs. 1.26, SE = 0.06). The difference in s (and accuracy) between the two distribution types likely reflects that their difficulty levels could be pre-experimentally matched only in approximation (see Methods) based on smaller pilot samples. No other main effects or interactions were significant (all F (1, 578) < 2.92, all p > 0.09, all η 2 < 0.005).
Analogous analyses for the bias (b) and leakage (l) parameters showed no differences between conditions (all F < 1.71, all p > 0.19, all η 2 < 0.003), with the exception that b differed between tasks [F (1, 578) = 84.88, p < 0.001, η 2 = 0.13; see 
Figure 4
]. We refrain from interpreting this latter effect because, for technical reasons, the estimates of b are not directly comparable between the two tasks (e.g., b can effect an overall displacement of the psychometric functions in the averaging task but not in the comparison task; see Eq. 1 and 2). For completeness, we report that b was significantly positive (> 0) in both tasks (single-stream conditions: Z = 8.52, p < 0.001, r = 0.03, Wilcoxon signed-rank test against 0; dual-stream conditions: Z = 9.19, p < 0.001, r = 0.03, Wilcoxon signed-rank test against 0), consistent with previous findings 
(Spitzer et al., 2017)
. Lastly, the leakage parameter l was significantly larger than 0 (indicating greater weighting of later samples) in our tasks, consistent with previous findings 
(Anderson, 1964;
Appelhoff et al., 2022a;
Cheadle et al., 2014;
Hubert-Wallander & Boynton, 2015;
Spitzer et al., 2017;
Summerfield & Tsetsos, 2015;
Weiss & Anderson, 1969;
Yashiro et al., 2020)
.


Correlation between compression and decision noise
Our performance simulations not only indicated that the ideal type of distortion (compression or anti-compression) should depend on the task requirements, but also that the degree of distortion (of either type) should increase with the level of decision noise (s) (see 
Figure 2d
 and f). Hence, if participants adopted ideal policies given their individual noise levels, we would expect to observe opposite correlations between the distortion parameter k and noise s in the two task conditions. Specifically, for participants with higher noise levels (s), estimates of k should be lower (k ≪ 1, stronger compression) in the single-stream conditions and higher (k ≫ 1, stronger anti-compression) in the dual-stream conditions. Our data indeed support this prediction: There was a negative correlation between k and s in the single-stream conditions 
(Figure 5a
, r = −0.32, p < 0.001), but a positive correlation in the dual-stream conditions 
(Figure 5b
, r = 0.24, p < 0.001). Both correlations were robust to the exclusion of outliers near the parameter boundaries (excluding data points < 0.1 or > 4.9 in either k or s: single-stream: r = −0.25, p < 0.001; dual-stream: r = 0.18, p = 0.003). As the correlations were of opposite signs, they are unlikely to be an artefact of residual cross-talk between parameters in model fitting 
(Krefeld-Schwalb et al., 2022, see Methods, Parameter Recovery)
. Together, these results corroborate the complex relationship between psychometric distortions and decision noise that we identified in our simulations of theoretically ideal policies 
(Figure 2
). 


Comparing experimental results with predicted optimal distortions
To compare participants' behavior with the results of our optimality simulations 
(Figure 2d-f)
, we repeated our model fitting with the respective gain normalizations illustrated in 
Figure 2d
-f (see also Methods, Eq. 3). The results for the single-stream conditions matched reasonably well with our simulations without gain limitations (2d), as we would expect given that sample evaluation in this task was relatively easy. In the dual-stream conditions, in contrast, there was a trend towards the ideal solution under a trial-level gain limit (for the uniform and Gaussian conditions alike; 2f), as we would expect given that the higher demands of this task forced participants to trade off processing resources between samples.
Quantitatively, the degree of either type of distortion fell short of the model-predicted optimum under the respective noise level (see 
Figure 2e and f)
. However, our model simulations only delineate the endpoints of a continuum (from no to full exhaustion of resources) on which we assume our empirical tasks to differ. Indeed, it would be straightforward to extend our framework to simulate any position a task may take between these two extremes [i.e., through parameterization of g: g partial = 1 + α(g − 1), where α (ranging from 0 to 1) would reflect the extent to which processing resources are exhausted].


Complementary Analyses
We performed supplementary analyses to back up our key empirical finding of compression in the single-stream task and anti-compression in the dual-stream task. First, we used Bayesian t-tests to corroborate that distortions k were only modulated by the factor "task" (single-or dual-stream) and not by the other factors under study ("distribution" and "range"). The Bayes factors showed extreme evidence for an effect of factor "task" (single or dual stream; BF 10 > 100), but moderate evidence for the null hypothesis (no effect) when examining the factor "distribution" (BF 10 = 0.11), and anecdotal evidence for the null when examining the factor "range" (BF 10 = 0.604).
Second, we tested directly for local slope differences in the descriptive weighting functions to examine whether they showed the hallmarks of compression or anti-compression in the single-and dual stream conditions, respectively (collapsed across the other manipulations). Specifically, we compared the mean local slope of outlying values (e.g., 1-2 and 8-9) against that of inlying values (e.g., 4-5 and 5-6) using paired t-tests. As expected, the difference in local slopes was significantly negative (i.e., steeper for inlying values) in the Lastly, given the observation of an overall bias (b) towards larger numbers (b > 0; see also 
Spitzer et al., 2017)
, we asked whether the above slope differences were only driven by large sample values. However, the same pattern was evident when restricting the analysis to the lower end of the sample range (e.g., comparing 1-2 vs. 3-4; single-stream condition: M = −0.01, SE = 0.004, t(295) = −2.23, p = 0.026; dual-stream condition: M = 0.01, SE = 0.005, t(289) = 2.22, p = 0.027). Together, these supplementary results fully support the main findings from our computational modeling analysis.


Discussion
Evaluating samples of magnitude, such as in decisions based on numbers, is integral to adaptive human behavior. Previous research has found evidence for opposite types of distortion of numerical values-compression and anti-compression-in tasks requiring the integration of number sequences 
(Li et al., 2017;
Spitzer et al., 2017;
Vanunu et al., 2021)
. Here, we showed that whether people subjectively compress or anti-compress numerical values depends on whether they are asked to assess the average value of a single stream or to compare the values of two interleaved streams. Arguably, the latter task is cognitively more effortful, because evaluating a sample's decision value for the comparison requires more cognitive operations (see also 
Appelhoff et al., 2022b)
. The pattern of results matches the predictions of our simulations with a psychometric model, which showed that compression yields a performance benefit for noisy observers when tasks are within their processing limit, whereas anti-compression improves performance in computationally demanding tasks (i.e., where processing a sample properly comes at the cost of missing the decision information in other samples). Taken together, our results show that participants adopted a favorable weighting policy in the respective task context, given their capacity limitations-in other words, that their choice of weighting policy was adaptive.
Our findings speak to the long-standing question as to why human observers distort objective magnitude information in decision making. It has recently been proposed that well-known distortions, such as those of value and probability information in choices between monetary gambles 
(Kellen et al., 2016;
Tversky & Kahneman, 1992)
, may serve a rather basic goal, namely to maximize objective returns 
(Juechems et al., 2021;
Spitzer et al., 2017)
. A central insight from this recent work has been that for noisy observers (e.g, humans and other biological agents), distortions of sample information can lead to higher returns than a normatively "correct" linear mapping 
(Juechems et al., 2021;
Li et al., 2017)
. Here, we have substantially extended this approach by showing that very different types of distortion (i.e., compression or anti-compression) can optimize the performance of noisy observers, depending on the extent to which a task taxes their processing capacities.
Importantly, the basic shape of distortion (compression or anti-compression) was not related to the overall difficulty of choice. In our simulations, the ideal extent of either type of distortion grew with higher levels of late decision noise, that is, with overall declining choice accuracy. Likewise, in our empirical data, noisier participants (with lower choice accuracy) showed stronger compression in simple numerical averaging, but stronger anticompression in the more effortful comparison task. As an explanation for this pattern, we propose that limitations (or bottlenecks) at different stages of the processing hierarchy may impact differently on the shape of psychometric distortions.
Previous work has suggested a distinction between "early" sensory noise (e.g., due to limits in sensory acuity; 
Lavie & Fockert, 2003;
Pelli, 1991;
Treisman & Geffen, 1967)
 and "late" decision noise (e.g., due to the difficulty of integrating multiple feature values into a binary choice; 
Baek & Chong, 2020;
Drugowitsch et al., 2016;
Findling & Wyart, 2021;
Juslin & Olsson, 1997;
Solomon, 2020;
Summerfield & Parpart, 2021)
 as limiting factors in human decision making. The present findings highlight another processing bottleneck intermediate to these early and late processing stages: the difficulty of evaluating the decisional meaning of a sample according to task requirements. We assume this processing to occur downstream to sensory encoding and upstream to combining the information from different samples into a final choice. Multi-sample tasks that load strongly on this intermediate 
(sample-by-sample)
 bottleneck may enforce a trade-off of processing resources between samples and promote selective integration 
(Tsetsos et al., 2016)
 of extreme values (i.e., anti-compression) as a performance-maximizing policy. Protection against late decision noise, in contrast, can be achieved through stronger distortions of either type (see also 
Li et al., 2017;
Spitzer et al., 2017)
, depending on task context. In this framework, the optimal weighting policy under late noise may even be linear (undistorted; as observed in, e.g., 
Kang & Spitzer, 2021)
, namely, if a task poses moderate sample-level demands.
Contrary to our expectations based on recent work 
(Li et al., 2017;
Prat-Carrabin & Woodford, 2020;
Spitzer et al., 2017;
Summerfield & Li, 2018)
, we found no effect of the "distribution" of sample values (uniform or Gaussian) or their "range" (small or large) on the shape of psychometric weighting. This observation is consistent with the results from our model simulations, which assume an upper bound on the gain of processing available to an observer on any given trial (cf. 
Li et al., 2017;
Spitzer et al., 2017)
. It remains to be shown whether our experimental results with symbolic numbers will generalize to other input formats (e.g., using analogue sensory continua; 
Liu et al., 2015;
Marinova et al., 2020;
Pekár & Kinder, 2019;
Rosenbaum et al., 2021;
, where the distribution and range of input values may play an additional role. For instance, while the discrete numerical samples in our experiment were easily readable (i.e., early sensory noise was presumably negligible), other sensory-perceptual inputs may be more prone to, for example, range adaptation effects 
(Brenner et al., 2000;
Fairhall et al., 2001;
Smirnakis et al., 1997;
Wark et al., 2007)
, which might also impact the shape of psychometric weighting.
An alternative explanation for our experimental findings warrants consideration. Our averaging task required numbers to be evaluated against a fixed reference value (e.g., 5), whereas our comparison task involved the contrasting of two time-evolving streams of evidence. Whether people compress or anti-compress samples might thus depend not on samplelevel demands, but on whether samples are evaluated against a fixed or a variable reference value (our psychometric model is agnostic about this). However, this alternative explanation seems unlikely in light of recent findings of anti-compression in tasks that required evaluation within a fixed reference frame 
(Rosenbaum et al., 2021;
Vanunu et al., 2019)
, similar to our averaging task. The study by 
Vanunu et al. (2019)
 also showed stronger anti-compression in a computationally taxing condition (see also 
Spitzer et al., 2017)
, which is consistent with the explanation we propose here, in terms of processing demands on the individual sample level.
We note for completeness that in addition to our main finding of adaptive distortions, participants' choice behavior also showed characteristics that were not encompassed by our model simulations: a "leakage" of sample information over time (i.e., a "recency" bias towards later presented samples), and an overall bias towards larger numbers (e.g., choices were more strongly driven by sample values "9" than "1", although the latter provided equally strong objective evidence). Both these biases have been reported many times before in previous work 
(Anderson, 1964;
Appelhoff et al., 2022a
Appelhoff et al., , 2022b
Cheadle et al., 2014;
Hubert-Wallander & Boynton, 2015;
Luyckx et al., 2019;
Spitzer et al., 2017;
Summerfield & Tsetsos, 2015;
Weiss & Anderson, 1969;
Yashiro et al., 2020)
, but their precise origin and functional role remain unclear. We did not find these overall biases modulated in interpretable ways by our present experimental manipulations, leaving their further investigation to future work.
A limitation of our simulation framework is that it cannot be used to predict the extent to which a given task will exhaust an observer's sample-level processing capacities.
It thus remains difficult to determine a priori which kind of distortion (compression or anticompression) would maximize a noisy observer's returns. A fruitful future direction could be to quantify, using simulations and/or neuroscientific approaches, the extent to which processing resources are expected to be traded off between the samples in a given task, as hypothesized here. Furthermore, while our study highlights sample-level processing demands as a key determinant of psychometric distortions, other factors may also play a role 
(Pachur et al., 2018;
Pachur et al., 2017;
Rosenbaum et al., 2021;
Vanunu et al., 2020)
. For instance, 
Rosenbaum et al. (2021)
 showed that the type of stimulus information (numerical or sensory-perceptual; which is unaccounted for in our model) can alter the weighting of samples in ensemble judgements. Finally, while our framework formally describes subjective distortions as parameterized psychometric functions, similar behaviors might arise from (mixtures of) heuristic policies 
(Gigerenzer et al., 2011)
, such as selective counting of sample values that a participant deems diagnostic in a given trial.
In conclusion, our study provides a theoretical framework and empirical data to explain why decision makers may over-or underweight extreme values in multi-sample judgments.
Rather than only reflecting idiosyncratic quirks of the human mind, subjective distortions of decision information may improve the objective performance of capacity-limited observers.
Our results reconcile conflicting findings about the form of such performance-maximizing distortions and suggest that humans may intuitively adopt a decision policy that is the best fit for the task at hand.


Constraints on Generality
Our study examined a large international sample of adult participants aged between 18 and 41 years who in all likelihood possessed basic numeracy skills. The participants completed the study remotely via a computer browser in a location of their choice. We have no reason to assume that different results would be obtained in a laboratory setting. In terms of materials and stimuli, we used ranges of symbolic numbers with a limited granularity of 9 discrete steps. Whether or not the results generalize to more finely sampled number ranges, to non-symbolic numbers, and/or to non-numerical magnitudes yet remains to be shown.
. Non-zero values of parameter b indicate an offset bias in terms of a shift of the function's indifference point. The dvs of the N = 8 individual samples in a trial are accumulated to yield a trial-level decision value DV :


same procedure as in the modeling of the empirical data. The recovered mean parameter values mostly correlated strongly with their respective values in the simulation (single-stream conditions: mean r = 0.84, min: r = 0.71, max: r = 0.92; dual-stream conditions: mean r = 0.60, min: r = 0.45, max: r = 0.77), while cross-correlations between different parameters were generally lower (single-stream condition: mean r = 0.03, min: r = −0.04, max: r = 0.19; dual-stream condition: mean: r = 0.05, min: r = −0.05, max: r = 0.17). The parameter of our main interest (k) was recovered well both in the single-stream (mean r = 0.85) and in the dual-stream conditions (mean r = 0.54), without major distortions by other model parameters (single-stream conditions: min r = −0.03, max r = 0.09; dualstream conditions: min r = −0.01, max r = 0.17).


Figure 1 .
1
Task. a, Example stimulus sequence in the averaging/comparison task. Eight number samples (drawn from 1 to 9 in the small-range condition; from 100 to 900 in the large-range condition) appeared in either red or blue font at a rate of 350ms/sample. In the averaging task (single-stream condition), participants were asked to indicate whether the average of all samples (regardless of color) was larger or smaller than 5 in the small-range condition or than 500 in the large-range condition. In the comparison task (dual-stream condition), participants were asked to indicate whether the red samples had a larger or a smaller average value than the blue samples. b, Distribution of sample values in the uniform(top)  andGaussian (bottom)  conditions. Colored digits on bottom illustrate the mapping (x-axis) onto red and blue sample stimuli (cf. a) in the single-and dual-stream condition, respectively. The panel for the Gaussian condition shows two distributions because we varied trial-by-trial within each task whether the mean of the distribution was in favor of "smaller" or "larger" (see Methods).


Figure 2 .
2
Simulated task performance under processing limitations. a, Functions mapping numerical sample values (1 to 9) onto decision values dv, for exemplary values of exponent k (see Equation 1). Inset bar graph shows the mean integral ( |dv| over sample values) for trials with uniformly (left) and Gaussian (right) distributed samples (see Figure 1b). b, same as a, but normalized to have equivalent gain ( |dv| across the range of sample values 1 to 9) for each value of k. c, Decision values normalized for equivalent gain on any given trial ( |dv| across the samples occurring in a trial) for each value of k. Illustrated is an example trial with 8 samples in the single-stream (averaging) task. d-f, Performance simulations. Colormaps show the difference in choice accuracy ∆(acc) relative to linear weighting (k = 1) across values of k and s, for the different types of gain normalization illustrated in a-c. Solid black lines indicate the maximal performance improvement under each noise level. Dotted white lines indicate k = 1. Green and blue dots show empirical parameter estimates in the single-stream (green) and dual-stream (blue) conditions. Large dots: mean estimates, error bars show SEM. Small dots: individual participants. Note that fitting with gain normalization in e and f yields numerically smaller estimates of decision noise s, but equivalent patterns of distortions k. Parameter estimates outside the axis limits are plotted at the plot boundaries. Upper panels: uniform conditions (see Figure 1b, upper panel); lower panels: Gaussian conditions (see Figure 1b, lower panel).


Figure 3 .
3
Descriptive weighting functions and model fits. Decision weights (see Methods) for numbers 1 to 9 (small range condition, a,b) and 100 to 900 (large range condition, c,d). Green: single-stream conditions (a,c); blue: dual-stream conditions (b,d); Dots: behavioral data; error bars show SEM. Colored lines: predictions from the fitted psychometric model (see Figure 4). Black lines show the model predictions for k = 1. Dashed horizontal lines indicate indifference (i.e., decision weight = 0 or choice probability = 0.5, see left and right y-axes). Upper panels: uniform conditions; lower panels: Gaussian conditions.


smaller than 1 (M = 0.66, SE = 0.03; Z = −11.21, p < 0.001, r = −0.04, Wilcoxon signedrank test against 1), indicating compression of decision values. In contrast, in the comparison task (dual-stream conditions), k was significantly larger than 1 (M = 1.86, SE = 0.06; Z = 11.77, p < 0.001, r = 0.04, Wilcoxon signed-rank test against 1), indicating anti-compression.


Figure 4 .
4
Parameter estimates. Parameter estimates for exponent k (a), noise parameter s (b), offset bias b (c) and leakage l (d). Upper panels: uniform conditions; lower panels: Gaussian conditions. Black dots show the average across individuals, error bars show SEM. Colored dots show the parameter estimates for the individual participants. The shaded half-violin outline illustrates the probability density of the parameters, smoothed by a kernel density estimator. Green: single-stream conditions (averaging task); blue: dual-stream conditions (comparison task). Light colors: small-range conditions, dark colors: large-range conditions. The black horizontal lines indicate k = 1 (no distortion) in a, b = 0 (no bias) in c, and l = 0 (no leakage) in d. in each of the experimental conditions (single-stream conditions: Z = 8.95, p < 0.001, r = 0.03, Wilcoxon signed-rank test against 0; dual-stream conditions: Z = 9.95, p < 0.001, r = 0.03, Wilcoxon signed-rank test against 0). Thus, we generally observed recency effects OVER-AND UNDERWEIGHTING OF EXTREME VALUES 20


Figure 5 .
5
Correlations between k and noise (s) across participants in (a) singlestream conditions and (b) dual-stream conditions. Red lines show the linear trend. Higher levels of decision noise were associated with stronger compression in the single-stream (averaging) conditions, but with stronger anti-compression in the dual-stream (comparison) conditions.


single-stream task (M = −0.041, SE = 0.003, t(295) = −12.31, p < 0.001), but significantly positive (i.e., steeper for outlying values) in the dual-stream task (M = 0.017, SE = 0.003, t(289) = 5.55, p < 0.001). Thus, the key features of compression or anti-compression where significantly evident even in model-free analysis of the descriptive data.














Resource allocation in the brain




R
Alonso






I
Brocas






J
Carrillo




10.1093/restud/rdt043








Review of Economic Studies




81


2
















Test of a model for number-averaging behavior




N
H
Anderson




10.3758/BF03342858








Psychonomic Science




1


7
















Control over sampling boosts numerical evidence processing in human decisions from experience




S
Appelhoff






R
Hertwig






B
Spitzer




10.1093/cercor/bhac062








Cerebral Cortex




33


1
















Eeg-representational geometries and psychometric distortions in approximate numerical judgment




S
Appelhoff






R
Hertwig






B
Spitzer




10.1371/journal.pcbi.1010747








PLOS Computational Biology




18


12
















Information theory and the brain




R
Baddeley






P
Hancock




10.1017/CBO9780511665516




& Földiàk, P.






Cambridge University Press












Distributed attention model of perceptual averaging




J
Baek






S
C
Chong




10.3758/s13414-019-01827-z








Perception, & Psychophysics




82










Attention








Possible principles underlying the transformations of sensory messages




H
Barlow




10.7551/mitpress/9780262518420.003.0013




W. A. Rosenblith






MIT Press








Sensory communication








Exposition of a new theory on the measurement of risk




D
Bernoulli










Econometrica




22


1
















Decision by sampling implements efficient coding of psychoeconomic functions




R
Bhui






S
Gershman




10.1037/rev0000123








Psychological Review




125


6
















Tests of theories of decision making: Violations of branch independence and distribution independence




M
H
Birnbaum






A
Chavez








Organizational Behavior and human decision Processes




71


2
















Adaptive rescaling maximizes information transmission




N
Brenner






W
Bialek






R
Steveninck




10.1016/S0896-6273(00)81205-2








Neuron




26


3
















Adaptive gain control during human perceptual choice




S
Cheadle






V
Wyart






K
Tsetsos






N
Myers






V
De Gardelle






S
H
Castañón






C
Summerfield




10.1016/j.neuron.2014.01.020








Neuron




81


6
















Over-and underweighting of extreme values [data set




V
Clarmann Von Clarenau






S
Appelhoff






T
Pachur






B
Spitzer




10.17605/OSF.IO/X83PK


















Measuring risk literacy: The Berlin Numeracy Test




E
Cokely






M
Galesic






E
Schulz






S
Ghazal






R
Garcia-Retamero








Judgment and Decision Making




7


1
















The magical number 4 in short-term memory: A reconsideration of mental storage capacity




N
Cowan




10.1017/S0140525X01003922








Behavioral and Brain Sciences




24


1
















Robust averaging during perceptual judgment




V
De Gardelle






C
Summerfield




10.1073/pnas.1104517108








Proceedings of the National Academy of Sciences




108


32
















Computational precision of mental inference as critical source of human choice suboptimality




J
Drugowitsch






V
Wyart






A.-D
Lodeho-Devauchelle






E
Koechlin




















10.1016/j.neuron.2016.11.005








Neuron




92


6














The effects of neural gain on attention and learning




E
Eldar






J
Cohen






Y
Niv




10.1038/nn.3428








Nature Neuroscience




16


8
















Efficiency and ambiguity in an adaptive neural code




A
Fairhall






G
Lewen






W
Bialek






R
Steveninck




10.1038/35090500








Nature




412
















Elemente der Psychophysik




G
Fechner












Elements of psychophysics












Härtel
Breitkopf Und
















Computation noise in human learning and decision-making: Origin, impact, function. Current Opinion in Behavioral Sciences




C
Findling






V
Wyart




10.1016/j.cobeha.2021.02.018








38














Heuristics: The foundations of adaptive behavior




G
Gigerenzer






R
Hertwig






T
Pachur








Oxford University Press












Order effects in belief updating: The beliefadjustment model




R
M
Hogarth






H
J
Einhorn




10.1016/0010-0285(92)90002-J








Cognitive Psychology




24


1


90002














Not all summary statistics are made equal: Evidence from extracting summaries across time




B
Hubert-Wallander






G
Boynton




10.1167/15.4.5








Journal of Vision




15














Optimal utility and probability functions for agents with finite computational precision




K
Juechems






J
Balaguer






B
Spitzer






C
Summerfield




10.1073/pnas.2002232118








Article e2002232118






118












Thurstonian and brunswikian origins of uncertainty in judgment: A sampling model of confidence in sensory discrimination




P
Juslin






H
Olsson




10.1037/0033-295X.104.2.344








Psychological review




104
















Concurrent visual working memory bias in sequential integration of approximate number




Z
Kang






B
Spitzer




10.1038/s41598-021-84232-7








Scientific Reports




11














The averaging of numerosities: A psychometric investigation of the mental line. Attention




N
Katzin






D
Rosenbaum






M
Usher




10.3758/s13414-020-02140-w








Perception & Psychophysics




83


3
















How (in)variant are subjective representations of described and experienced risk and rewards? Cognition




D
Kellen






T
Pachur






R
Hertwig




10.1016/j.cognition.2016.08.020








157














Metabolic cost of neuronal information in an empirical stimulus-response model




L
Kostal






P
Lánský






M
D
Mcdonnell








Biological Cybernetics




107


3


















10.1007/s00422-013-0554-6














Structural parameter interdependencies in computational models of cognition




A
Krefeld-Schwalb






T
Pachur






B
Scheibehenne








Psychological Review




129


2


313














Bayesfactor: Release 2022 (v2.3.0) (Version v2.3.0)




B
Krekelberg




10.5281/zenodo.7006300


















The influence of attention on value integration. Attention, Perception, & Psychophysics




M
A
Kunar






D
G
Watson






K
Tsetsos






N
Chater




10.3758/s13414-017-1340-7








79














Contrasting effects of sensory limits and capacity limits in visual selective attention




N
Lavie






J
Fockert




10.3758/BF03194795








Perception & Psychophysics




65


2
















The cost of cortical computation




P
Lennie




10.1016/S0960-9822(03)00135-0


1016/S0960-9822(03)00135-0








Current Biology




13


6
















Robust averaging protects decisions from noise in neural computations




V
Li






S
Herce Castañón






J
A
Solomon






H
Vandormael






C
Summerfield




10.1371/journal.pcbi.1005723








Article e1005723






13












Symbolic integration, not symbolic estrangement, for double-digit numbers




A
S
Liu






C
D
Schunn






J
A
Fiez






M
E
Libertus




D. C. Noelle, R. Dale, A. S


















J
Warlaumont






T
Yoshimi






C
D
Matlock






Jennings




Proceedings of the 37th Annual Meeting of the Cognitive Science Society


& P. P. Maglio


the 37th Annual Meeting of the Cognitive Science Society
















Extreme outcomes sway risky decisions from experience




E
Ludvig






C
Madan






M
Spetch




10.1002/bdm.1792








Journal of Behavioral Decision Making




27
















Living near the edge: How extreme outcomes and their neighbors drive risky choice




E
A
Ludvig






C
R
Madan






N
Mcmillan






Y
Xu






M
L
Spetch








Journal of Experimental Psychology: General




147


12














Extreme outcomes sway risky decisions from experience




E
A
Ludvig






C
R
Madan






M
L
Spetch








Journal of Behavioral Decision Making




27


2


















F
Luyckx






H
Nili






B
Spitzer






C
Summerfield




Neural structure mapping in human probabilistic reward learning


D. Lee, J. I. Gold, D. Lee, & M. Chafee


















10.7554/eLife.42816














Numerals do not need numerosities: Robust evidence for distinct numerical representations for symbolic and non-symbolic numbers




M
Marinova






D
Sasanguie






B
Reynvoet




10.1007/s00426-019-01286-z








Psychological Research




85


2
















Capacity limits of information processing in the brain




R
Marois






J
Ivanoff




10.1016/j.tics.2005.04.010








Trends in Cognitive Sciences




9


6
















Probing the human stereoscopic system with reverse correlation




P
Neri






A
J
Parker






C
Blakemore








Nature




401


6754
















Prospect theory reflects selective allocation of attention




T
Pachur






M
Schulte-Mecklenbeck






R
O
Murphy






R
Hertwig




10.1037/xge0000406








Journal of Experimental Psychology: General




147


2
















How the twain can meet: Prospect theory and models of heuristics in risky choice




T
Pachur






R
S
Suter






R
Hertwig




10.1016/j.cogpsych.2017.01.001








Cognitive Psychology




93
















The interplay between non-symbolic number and its continuous visual properties revisited: Effects of mixing trials of different types




J
Pekár






A
Kinder




10.1177/1747021819891068








Quarterly Journal of Experimental Psychology




73


5


















D
Pelli




Computational models of visual processing


M. A. Landy & J. A. Movshon




MIT Press










Noise in the visual system may be early








Efficient coding of numbers explains decision bias and noise




A
Prat-Carrabin






M
Woodford




10.1101/2020.02.18.942938


















Ensemble perception: Extracting the average of perceptual versus numerical stimuli. Attention, Perception, & Psychophysics




D
Rosenbaum






V
De Gardelle






M
Usher




10.3758/s13414-020-02192-y








83














Distinct strategies for estimating the temporal average of numerical and perceptual information




H
Sato






I
Motoyoshi




10.1016/j.visres.2020.05.004








Vision Research




174
















High-value decisions are fast and accurate, inconsistent with diminishing value sensitivity




B
R
Shevlin






S
M
Smith






J
Hausfeld






I
Krajbich








Proceedings of the National Academy of Sciences




119


6


2101508119














The development of numerical estimation: Evidence for multiple representations of numerical quantity




R
S
Siegler






J
E
Opfer




10.1111/1467-9280.02438








Psychological Science




14


3
















Adaptation of retinal processing to image contrast and spatial scale




S
M
Smirnakis






M
J
Berry






D
K
Warland






W
Bialek






M
Meister




10.1038/386069a0








Nature




386
















Five dichotomies in the psychophysics of ensemble perception. Attention, Perception, & Psychophysics




J
Solomon




10.3758/s13414-020-02027-w








83














Rhythmic gain control during supramodal integration of approximate number




B
Spitzer






F
Blankenburg






C
Summerfield




10.1016/j.neuroimage.2015.12.024








NeuroImage




129
















Selective overweighting of larger magnitudes during noisy numerical comparison




B
Spitzer






L
Waschke






C
Summerfield




10.1038/s41562-017-0145








Nature Human Behaviour




1














Perceptual suboptimality: Bug or feature? Behavioral and Brain Sciences, 41, Article e245




C
Summerfield






V
Li




10.1017/S0140525X18001437


















Normative principles for decision-making in natural environments




C
Summerfield






P
Parpart




10.31234/osf.io/s2wvz








PsyArXiv Preprints
















Do humans make good decisions?




C
Summerfield






K
Tsetsos




10.1016/j.tics.2014.11.005








Trends in Cognitive Sciences




19


1




















I
The Mathworks




9.0.2037887








9


Natick, Massachusetts, United States












Econometrics toolbox




I
The Mathworks










Natick, Massachusetts, United State












Optimization toolbox




I
The Mathworks










Natick, Massachusetts, United State












Optimization toolbox




I
The Mathworks










Natick, Massachusetts, United State












Statistics and machine learning toolbox




I
The Mathworks










Natick, Massachusetts, United State












A unified attentional bottleneck in the human brain




M
Tombu






C
Asplund






P
Dux






D
Godwin






J
Martin






R
Marois




10.1073/pnas.1103583108








Proceedings of the National Academy of Sciences




108


33
















Selective attention: Perception or response? Quarterly




A
Treisman






G
M
Geffen




10.1080/14640746708400062








Journal of Experimental Psychology




19


1
















Salience driven value integration explains decision biases and preference reversal




K
Tsetsos






N
Chater






M
Usher




10.1073/pnas.1119569109








Proceedings of the National Academy of Sciences




109


24




















K
Tsetsos






R
Moran






J
C
Moreland






N
Chater






M
Usher






C
Summerfield


















Economic irrationality is optimal during noisy decision making


10.1073/pnas.1519157113








Proceedings of the National Academy of Sciences




113


11














Advances in prospect theory: Cumulative representation of uncertainty




A
Tversky






D
Kahneman




https : //EconPapers.repec.org/RePEc:kap:jrisku:v:5:y:1992:i:4






Journal of Risk and Uncertainty




5


4
















Robust sampling of decision information during perceptual choice




H
Vandormael






S
Castañón






J
Balaguer






V
Li






C
Summerfield




10.1073/pnas.1613950114








Proceedings of the National Academy of Sciences




114


10
















Elucidating the differential impact of extremeoutcomes in perceptual and preferential choice




Y
Vanunu






J
Hotaling






B
Newell




10.1016/j.cogpsych.2020.101274








Cognitive Psychology




119














How top-down and bottom-up attention modulate risky choice




Y
Vanunu






J
M
Hotaling






M
E
Le Pelley






B
R
Newell








Proceedings of the National Academy of Sciences




118


39


2025646118














Constructing preference from sequential samples: The impact of evaluation format on risk attitudes. Decision




Y
Vanunu






T
Pachur






M
Usher




10.1037/dec0000098








6














Eliciting von neumann-morgenstern utilities when probabilities are distorted or unknown




P
Wakker






D
Deneffe








Management Science




42


8
















Sensory adaptation




B
Wark






B
Lundstrom






A
Fairhall




10.1016/j.conb.2007.07.001








Current Opinion in Neurobiology




17


4
















Subjective averaging of length with serial presentation




D
J
Weiss






N
H
Anderson




10.1037/h0028028








Journal of Experimental Psychology




82


1
















Perception and decision mechanisms involved in average estimation of spatiotemporal ensembles




R
Yashiro






H
Sato






T
Oide






I
Motoyoshi




10.1038/s41598-020-58112-5








Scientific Reports




10















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]