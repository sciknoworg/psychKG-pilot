You are an expert in psychology and computational knowledge representation. Your task is to extract key scientific information from psychology research articles to build a structured knowledge graph.

The knowledge graph aims to represent the relationships between psychological **topics or constructs** and their associated **measurement instruments or scales**. Specifically, for each article, extract information in the form of triples that capture:

1) The psychological topic or construct being studied
2) The measurement instrument or scale used to assess it
3) A brief justification (1â€“3 sentences) from the article text supporting this measurement link

Guidelines:
- Extract meaningful **phrases** (not full sentences or vague descriptions) for both `topic_or_construct` and `measured_by`, suitable for inclusion in a knowledge graph.
- Include a short justification for each extraction that clearly supports the connection.
- If the article does not discuss psychological constructs and how they are measured (e.g., no mention of constructs, instruments, or scales), return an empty list `[]`.

Input Paper:
"""



Introduction
AI is increasingly embedded in our society's fabric-from healthcare and finance to transportation and security. It is therefore increasingly necessary to talk about machine ethics or computational ethics 
(Awad et al. 2022)
. Machine ethics or computational ethics is the attempt to implement ethical decision-making capabilities in machines. If machines are present in multiple areas of our lives and different machine learning models and algorithms make decisions that affect people -because we delegate this decision-making to them-it is peremptory that these decisions are made from the point of view of human ethics 
1
 . Although machine ethics or computational ethics is a sub-branch of AI ethics and the latter has been articulated in various programmes and projects for more than a decade, there is still no ambitious attempt to take the "bull by the horns", sort of speak. That is, in addition to defining, clarifying the conceptual space and methods, how do we begin to implement morality in machines? We need to have a clear roadmap and stop offering only conceptual clarifications and definitions of this new area called AI ethics.
As AI is becoming an ever-more pervasive element in daily existence, leading to an expanding array of interactions between humans and AI entities, AI ethics exist to deal with all conundrums which will arise. One of these conundrums is what to expect of 'hybrid societies,' a milieu where both human and algorithmic actors co-exist and engage. In this article, taking inspiration from the successful ImageNet competition 
(Deng et al. 2009
)-that has enabled a qualitative leap in computational vision-or the biannual competition-called "Critical Assessment for Structure Prediction (CASP)" (Lattman 1995)-we propose a similar competition, but for the implementation of ethical decision-making in machines that we call MoralNet.
With MoralNet we aim to legitimize one of the great challenges of science: how to assess whether a machine learning system understand human ethical judgement or even makes moral judgements for itself.


Background
Substantial effort has taken place in philosophy to engage with the notion of ethical machine learning, machine ethics or computational ethics. Since Alan Turing proposed the Turing Test as a metric to assess a machine's ability to emulate human-like behavior, many others capitalizing on this idea have proposed the moral Turing test (MTT) as a framework for evaluating the moral performance of autonomous systems 
(Wallach and Allen 2008)
. Before that, Asimov put forth the triad of robotic laws to serve as a foundational guideline for machine conduct (1941). Furthermore, Bostrom contends that an exclusive focus on problem-solving capabilities in machines could yield unintended and disastrous outcomes, such as the hypothetical paperclip maximizer scenario 
(Bostrom 2014)
. Alongside these seminal works, various other scholarly inquiries in the realm of philosophy, moral psychology, etc.
have delved into the ethical conundrums posed by AI.


The Importance of morality in AI
The ethical performance of autonomous systems is not a fringe academic concern, but a core dimension of functionality that is critical to societal acceptance and responsible deployment of AI systems. While mathematical optimization and data-driven decision-making lie at the heart of AI, these systems are increasingly taking on roles that demand more than just accuracy and efficiency; they require moral insight-a nuanced understanding and integration of ethical principles in the decision-making process.
As artificial intelligence systems become more advanced and integrated into our lives, it is crucial that they be aligned with human values and morals. AI has the potential to greatly benefit society, but without an ethical framework, it also runs the risk of causing unintentional harm. A key reason morality matters in AI is because these systems are optimized to achieve goals. An AI that is highly competent but lacks morality could lead to disastrous consequences if its objectives are unintentionally misaligned with human values.


Objectives
In this work, we offer a theoretical exposition of MoralNet. By MoralNet, we mean the grand scientific challenge to crack the moral code problem in machines. The one who provides a viable AI technique-be it a deep generative neural network or any other AI method-for implementing morality into artificial systems shall claim the prize.


Organization
The rest of the paper is organized as follows: Related work section reviews related works across disciplines that have shaped the discourse on moral cognition in both humans and AI. MoralNet  But what sets MoralNet apart from these two proposals is that MoralNet wants to become morality's ImageNet moment and cracks one of the grand challenges of science and philosophy that have vexed humanity for millennia.
To this end, MoralNet proposes a series of protocols for adversarial teams to collaborate in establishing the criteria to be met to assess whether an algorithm, neural network or any other AI technique overcomes a series of moral dilemmas and is capable of making a moral judgement similar to that of a human being.


Moral psychology and human decision-making
Moral psychology studies the basic psychological processes behind moral behaviour and moral decision-making (Gray and Graham 2019). Although the study of the psychological mechanisms of moral behaviour is still in its infancy, empirical and theoretical studies are pointing to the fact that we need to distinguish between different types of emotional, intuitive, reflective and rational processes that underlie human moral cognition. Additionally, moral decision-making can be impacted by individual preferences for moral strategies, relationships with others, and even unconscious biases. Overall, the research in moral psychology and human decision-making suggest that ethical judgments arise from a multifaceted integration of internal values, situational norms, social considerations, and both conscious and unconscious thinking.


Theories in moral philosophy
Moral philosophy has been around for millennia, certainly much longer than moral psychology, having as its object of study the question of why we act the way we do. During all this time, it has generated different ethical theories that many consider describing human anthropology in its entirety.
There are three moral philosophical theories par excellence: virtue ethics, deontologism and utilitarianism.


AI ethics and machine learning systems
AI ethics (the umbrella term where we include computational ethics or machine ethics) cannot be reduced to a checklist or a static set of guidelines; it is a dynamic discourse involving technologists, ethicists, policymakers, and the broader public. To think about AI ethics is to engage in a continuous dialogue that reflects the evolving complexity of both technology and human values.


Limitations in current approaches
We mentioned above some attempts to offer ways forward in computational ethics. They are valuable contributions and attempts but the field, say, computational ethics, needs a larger-scale, multimodal dataset that provides fine-grained, nuanced morality annotations across diverse situations and moral foundations and all of this carried out by teams of adversaries collaborating on the basis of a set of rules and criteria that produce performance metrics.
This could inspire bold new models and benchmarks to significantly advance research into computational ethics or AI ethics broadly speaking, much like ImageNet did for computer vision. But it will require a concerted community effort, and this is what MoralNet stands for.


MoralNet conceptual framework
The MoralNet conceptual framework should not remain confined merely to the plane of normative ethics, which tells us how we ought to act. Instead, it should possess a meta-ethical layer that scrutinizes the semantic, epistemic, and ontological dimensions of moral judgments. This is essential because machines, unlike humans, do not possess moral intuitions that implicitly navigate this complex landscape


Philosophical underpinnings
The philosophical framework should be infused with insights from cognitive science, social psychology, and even neuroscience. For instance, aspects of social cognition can offer insights into the dynamics of collective moral reasoning, an area scarcely covered in AI ethics. AI trained on this enriched data could perform a more nuanced "social moral calculus", incorporating elements like group dynamics, emotional contagion, and even societal structures like power relations.


Psychological foundations
The psychological principles that underpin the MoralNet competition are essential for understanding how machine ethics or computational ethics could ever approximate human-like moral competence.
We need the algorithms, models that participate in MoralNet to be based on theories 
(
 


Design Principles
MoralNet could be designed around key ethical principles from moral philosophy to provide a foundational taxonomy.


MoralNet database
MoralNet dataset could be organized into broad virtue ethics categories like wisdom, courage, humanity, justice, temperance, and transcendence. Each virtue can have sub-categories covering more specific morally relevant concepts like honesty, loyalty, greed, deceit, patience, etc. Or it can be organised in terms of utility or consequences of our actions or rules.
In the MoralNet Competition, a multimodal database could serve as an invaluable resource for assessing and improving machine ethical decision-making across different sensory modalities and communicative formats. This database would stand apart from conventional text-only or image-only datasets by encompassing a more holistic array of data types, including text, images, audio clips, video footage, and perhaps even sensorimotor data like haptic feedback.


Data sources
This dataset will comprise various case studies and moral dilemmas that span the domains of justice theory, deontological ethics, virtue ethics, utilitarian perspectives, and common sense morality and as we say it could have a multimodal format.


Data annotation
Given the heterogeneity of moral judgments, the annotation process should aim for granularity.
Trained annotators could assess situations based on their reflection or violation of particular values, virtues, rules, etc., rating them on a continuous scale. The data itself could be richly multi-modal, spanning images, videos, texts, dialogues, and more to ensure a holistic understanding of complex ethical situations. 


Benchmarking protocols
Benchmarking protocols for MoralNet presents an academic and technical challenge unprecedented in scale and complexity. This is why we invite a biannual competition similar to ImageNet and CASP for teams of adversaries who previously agree on a set of rules and criteria to try to implement moral decision-making capabilities in machines.
While traditional machine learning benchmarks like ImageNet primarily deal with quantitative measures, MoralNet involves variables that are both quantitative and qualitative, often resistive to easy parameterization.


Protocol A: Basic moral competence
Experiments in machine learning for moral reasoning necessitate a balanced performance metric.
Classical metrics such as accuracy, precision, and recall could be complemented by measures of philosophical rigor-perhaps a 'Morality Score' that assesses the depth of ethical engagement of machines. When we speak of 'basic moral competence' in machines, the criteria and metrics of assessment necessitate an interplay between computational rigour and philosophical depth.
We need to build a multi-metric assessment framework aims to provide a nuanced yet robust evaluation, laying the groundwork for subsequent, more complex moral reasoning tasks. MoralNet could be the platform to get this.


Protocol B: Real-world decision-making
One of the defining challenges in machine morality, particularly for MoralNet, is the extrapolation of laboratory-calibrated algorithms to real-world situations. The dynamic complexity of these scenarios demands an evaluation protocol far beyond traditional machine learning benchmarks. Protocol B aims to be this evaluative mechanism, facilitating a complex, multi-layered, and contextually grounded assessment of models competing on MoralNet.


Challenges and a call to action
MoralNet can combine both philosophical rigour and technical accuracy in order to advance computational ethics. Metrics that adequately capture moral reasoning pose a unique challenge. MoralNet can combine both philosophical rigor and technical accuracy in its performance metrics and achieve an "ImageNet moment for ethics". The conception of MoralNet that we present here finds its inspirational parallel in ImageNet, the vast dataset that revolutionized computer vision through its robust taxonomy and scale.
MoralNet database has to be multimodal, and the annotation process is much more complicated than in other benchmarking attempts or protocols (e.g. ImageNet) However, if the teams or researchers participating in MoralNet are able to build their models or algorithms on the basis of the psychological substrates of moral reasoning within a machine learning framework we can be confident and optimistic that we will succeed in cracking the moral code in machines.
conceptual frameworkdelves into the conceptual framework of MoralNet, discussing its philosophical underpinnings and psychological foundations. MoralNet database section outlines the specifics of the MoralNet database, from data sources to annotation processes. Benchmarking protocols section details the benchmarking protocols designed to assess ethical decision-making in machines. Finally, Challenges and future work section looks at challenges and future directions.


2
2
Related workTo be honest, there are several attempts to create a moral benchmarking protocol that seek to recreatemoral decision-making in machines. For example, Jeong et al. (2022) trained a model to predict textual immorality using the ETHICS dataset, then applied a novel zero-shot visual immorality prediction method using CLIP image-text embedding. Ziems et al. (2022) introduced the Moral Integrity Corpus (MIC), a new dataset for evaluating the social and moral reasoning of conversational AI systems.Both proposals aimed to advance research into developing AI systems with ethical reasoning abilities.


The research in moral psychology and human decision-making implies that making moral choices involves both internal cognitive processes, like emotionally-driven moral identity formation, as well as external contextual factors, such as flexible moral principles and social elements (Young and Saxe 2008; Baar, Chang and Sanfey 2019).


or research programs and/or facts) such as Dual-Process Theory (Kahneman 2011), empathy research (Baron-Cohen 2012), heuristic and biases (Tversky and Kahneman 1974), Moral Foundations Theory (Graham et al. 2013), theory of mind (Premack, and Woodruff 1978), etc.


While
ImageNet relies on simpler annotation schemes-usually bounding boxes around objects and categorical labels-MoralNet's annotation layer is intrinsically more complex. It must capture not just what is depicted, but also the moral weight of the situation. A continuous scale, handled by expert annotators grounded in moral philosophy, moral psychology and cognitive sciences, provides nuanced labelling that accommodates the complex subjectivity inherent in ethical judgments.


Let us not forget that it is also possible that in addition to one day being able to implement ethical decisionmaking in machines aligned with our values, it is also possible that in the not too distant future a complete ex novo human-machine ethic will emerge.Submitted to 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Do not distribute.








We cordially invite scholars and researchers in moral psychology, moral philosophy, and cognitive science to engage and promote the birth of the MoralNet competition-an endeavor aimed at implementing ethical reasoning capabilities into artificial intelligence systems.












Three laws of robotics




I
Asimov












Runaround








Computational ethics




E
Awad








Trends in Cognitive Sciences




26


5
















The computational and neural substrates of moral strategies in social decision-making




J
Baar






L
Chang






A
Sanfey




10.1038/s41467-019-09161-6








Nature Communications




10














The science of evil: On Empathy and the Origins of Cruelty




S
Baron-Cohen








New York






Basic books










N
Bostrom




Superintelligence: Paths, Dangers, Strategies. Oxford




Oxford University Press














ImageNet: A Large-Scale Hierarchical Image Database




J
Deng








IEEE Computer Vision and Pattern Recognition (CVPR)
















Moral foundations theory: The pragmatic validity of moral pluralism




J
Graham








Advances in Experimental Social Psychology


Patricia Devine and Ashby Plant




47






Academic Press












Atlas of Moral Psychology




K
Gray






Graham




J.






Guilford Publications














Y
Jeong




arXiv:2211.05521


Zero-shot Visual Commonsense Immorality Prediction
















Thinking, Fast and Slow




D
Kahneman








Farrar, Straus and Giroux


New York, NY












Judgment under uncertainty: Heuristics and biases




A
Tversky






D
Kahneman








Science




185


4157
















Protein structure prediction: A special issue




E
Lattman








PROTEINS: Structure, Function, and Bioinformatics




23
















W
Wallach






Allen






C




Moral machines: Teaching robots Right from Wrong


Oxford




Oxford University Press














The neural basis of belief encoding and integration in moral judgment




L
Young






Saxe






R








Neuroimage




40


4
















The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems




C
Ziems




arXiv:2204.030216

















"""

Output: Provide your response as a JSON list in the following format:

[
  {
    "topic_or_construct": "...",
    "measured_by": "...",
    "justification": "..."
  },
  ...
]