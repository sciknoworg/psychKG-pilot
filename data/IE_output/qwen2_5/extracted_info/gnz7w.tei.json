[
  {
    "topic_or_construct": "Cognitive biases and heuristics",
    "measured_by": "Cognitive Reflection Test, Semantic Illusions",
    "justification": "LLMs, similar to humans, have been shown to exhibit cognitive biases and heuristics such as the representativeness heuristic and the availability heuristic. Studies have used tests like the Cognitive Reflection Test and semantic illusions to assess these biases. For example, LLMs have been observed to respond to the 'bat and ball' problem in the same intuitive but incorrect way as humans do."
  },
  {
    "topic_or_construct": "System 1 decision-making",
    "measured_by": "Two-armed bandit tasks",
    "justification": "The review mentions using two-armed bandit tasks to investigate the decision-making behaviour of LLMs under uncertainty. This task has been used to assess how LLMs such as GPT exhibit tendencies toward exploitation and adaptability, similar to human psychological approaches to System 1 decision-making."
  },
  {
    "topic_or_construct": "System 2 decision-making",
    "measured_by": "Chain-of-thought prompting",
    "justification": "The review discusses how chain-of-thought reasoning can prompt LLMs to engage in more deliberate and analytical thinking, similar to System 2 decision-making in humans. This method involves instructing the model to think step-by-step, which helps it to perform more complex tasks with greater accuracy."
  },
  {
    "topic_or_construct": "Hallucinations in LLMs",
    "measured_by": "Comparative analysis of generated content against factual databases",
    "justification": "Hallucinations in LLMs are detected through an analysis of generated content against factual databases to identify instances where the LLM confidently produces false or fabricated information. These evaluations help measure the extent of non-human cognitive biases specific to LLMs."
  },
  {
    "topic_or_construct": "Overconfidence in LLMs",
    "measured_by": "Confidence-based responses and assessments of fact-checking",
    "justification": "The review mentions measuring overconfidence in LLMs by assessing their tendency to overestimate their judgments, even in the presence of incorrect responses. This is typically measured by analyzing their confidence levels in the accuracy of their answers compared to actual correctness."
  }
]