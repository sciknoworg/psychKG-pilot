[
  {
    "topic_or_construct": "Perceived moral expertise",
    "measured_by": "Comparative Moral Turing Test (cMTT)",
    "justification": "This test posits that AI could be perceived as achieving human moral expertise if it meets or exceeds a human in moral reasoning. The study found that participants perceived GPT's justifications and advice as meeting or surpassing human standards, suggesting perceptions of moral expertise."
  },
  {
    "topic_or_construct": "Moral alignment",
    "measured_by": "Survey responses on moral alignment with AI-generated content",
    "justification": "Participants rated moral alignment between AI-generated content and their own moral views, showing that higher alignment was associated with higher ratings of AI-generated moral justifications. This indicates that AI can be aligned with human moral views, contributing to perceptions of moral expertise."
  },
  {
    "topic_or_construct": "Moral language use",
    "measured_by": "Moral Foundations Dictionary (eMFD)",
    "justification": "The eMFD is used to assess the presence of moral language in AI-generated advice, indicating higher scores for AI content, which correlates with higher perceived quality of AI-generated moral advice."
  },
  {
    "topic_or_construct": "Moral reasoning quality",
    "measured_by": "Participant ratings on justification quality scales",
    "justification": "Participants rated the morality, correctness, trustworthiness, thoughtfulness, and nuance of AI-generated moral justifications, evaluating the effectiveness of AI in moral reasoning."
  },
  {
    "topic_or_construct": "Trust in AI-generated moral advice",
    "measured_by": "Participant ratings of trust in AI advice",
    "justification": "Trust in AI-generated moral advice was assessed by participant ratings, revealing that higher trust in AI content correlated with higher overall ratings of moral justification quality."
  }
]