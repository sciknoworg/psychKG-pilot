<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Questioning AI: How Racial Identity Shapes the Perceptions of Algorithmic Bias</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soojong</forename><surname>Kim</surname></persName>
							<email>sjokim@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Communication</orgName>
								<orgName type="institution">University of California Davis</orgName>
								<address>
									<country>Unites States</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center on Philanthropy and Civil Society</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joomi</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Advertising &amp; Public Relations</orgName>
								<orgName type="institution">University of Georgia</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poong</forename><surname>Oh</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Wee Kim Wee School of Communication and Information</orgName>
								<orgName type="department" key="dep2">Author Note Soojong Kim https://orcid.org</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>0000-0002-1334-5310, 0000-0001-6651-4432</postCode>
									<settlement>Joomi Lee https://orcid.org/, Poong Oh</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of California Davis</orgName>
								<address>
									<addrLine>361 Kerr Hall</addrLine>
									<postCode>95616</postCode>
									<settlement>Davis</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Questioning AI: How Racial Identity Shapes the Perceptions of Algorithmic Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated decision-making</term>
					<term>Artificial intelligence</term>
					<term>Race</term>
					<term>Discrimination</term>
					<term>Bias</term>
					<term>Fairness</term>
					<term>Trust</term>
					<term>Emotion</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Growing concerns indicate that automated decision-making (ADM) may discriminate against certain social groups, but little is known about how social identities of people influence their perceptions of biased automated decisions. Focusing on the context of racial disparity, this study examined if individuals&apos; social identities (white vs. People of Color) and social contexts that entail discrimination (discrimination target: the self vs. the other) affect the perceptions of algorithm outcomes. A randomized controlled experiment (N = 604) demonstrated that a participant&apos;s social identity significantly moderated the effects of the discrimination target on the perceptions. Among POC participants, algorithms that discriminate against the subject decreased their perceived fairness and trust, whereas among white participants opposite patterns were observed. The findings imply that social disparity and inequality, and different social groups&apos; lived experiences of the existing discrimination and injustice should be at the center of understanding how people make sense of biased algorithms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With advances in artificial intelligence (AI), the use of automated decision-making (ADM) has been increasing in various domains, including news creation and recommendation <ref type="bibr" target="#b17">(Diakopoulos &amp; Koliska, 2017;</ref><ref type="bibr" target="#b79">Thurman et al., 2019)</ref>, healthcare and medical diagnosis <ref type="bibr" target="#b31">(Jha &amp; Topol, 2016;</ref><ref type="bibr" target="#b85">Yu &amp; Kohane, 2019)</ref>, and policing and law enforcement <ref type="bibr" target="#b34">(Kennedy et al., 2011;</ref><ref type="bibr" target="#b56">Nissan, 2017)</ref>. Although people generally expect algorithms to outperform human decisionmaking in some aspects (e.g., <ref type="bibr" target="#b23">Grace et al., 2018;</ref><ref type="bibr" target="#b24">Grady, 2020;</ref><ref type="bibr" target="#b33">Kahng, 2021)</ref>, there have been growing concerns about potentially biased and inaccurate outcomes produced by machines, which may contribute to perpetuating and exacerbating the existing inequality and discrimination in society <ref type="bibr" target="#b36">(Koenecke et al., 2020;</ref><ref type="bibr" target="#b58">Obermeyer et al., 2019;</ref><ref type="bibr" target="#b83">Williams et al., 2018)</ref>. For social scientists, engineers, policymakers, and journalists in this critical time of socio-technological changes, it is important to understand the reaction to and perception of the new technology in society and the public's concern and skepticism against it <ref type="bibr" target="#b18">(Dietvorst et al., 2014;</ref><ref type="bibr" target="#b19">Dolata et al., 2021;</ref><ref type="bibr" target="#b45">Logg et al., 2019)</ref>.</p><p>Here, we investigate how the public perceives ADM processes that discriminate against users based on their social identities. Although recent studies have demonstrated that possibilities of algorithmic biases can undermine individuals' perceptions of fairness and trust <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b39">Lee &amp; Baykal, 2017)</ref>, not much research has been done on how different social groups process biased algorithms cognitively and emotionally. The current research explores this issue focusing on racial identities. Because racial minorities undergo discrimination and prejudice against them that affect various aspects of their lives, including their economic status, social relationships, job performance, and educational achievement (e.g., <ref type="bibr" target="#b11">Broman et al., 2000;</ref><ref type="bibr" target="#b12">Brown et al., 2000;</ref><ref type="bibr" target="#b25">Greenhaus et al., 1990)</ref>, the way they perceive, react to, and cope with racial disparities is different from that of the majority group <ref type="bibr" target="#b30">(Jacob et al., 2022;</ref><ref type="bibr" target="#b70">Sellers &amp; Shelton, 2003)</ref>.</p><p>Although earlier findings indicate that beliefs in the fairness of algorithms lead racial minorities to prefer ADM over human decision-making <ref type="bibr" target="#b8">(Bigman et al., 2021;</ref><ref type="bibr" target="#b10">Bonezzi &amp; Ostinelli, 2021)</ref>, questions remain regarding social situations wherein people confront algorithms that produce discriminatory outcomes against certain social groups.</p><p>In this regard, the present research examines if people show significant differences when they face automated decisions that discriminate against themselves compared with those discriminating against others in a different social group. To understand how algorithmic biases impact different dimensions related to experiencing discrimination, we compared perceived fairness, trust, perceived pervasiveness, the tendency to question the ADM process, and negative emotional responses to an outcome, across two social situations that entail different directions of bias: discrimination against the self and others. We also tested whether cognitive and emotional responses to disparities in ADM differ across people's social identities (racial minority and majority) and across different decision-making contexts. Finally, we discuss how algorithmic biases could heighten people's concerns and aversion to algorithms and what the implications of this dynamic are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceptions of Automated Decision-Making</head><p>ADM refers to computational processes of decision-making that involve the use of data and algorithms <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b54">Newell &amp; Marabelli, 2015)</ref>. The concepts of ADM and algorithms are particularly useful for social science research because they allow researchers to capture and represent the perspectives of most technology users who may not fully comprehend detailed procedures and mechanisms within decision-making machines. This is one reason why these concepts are attracting a growing number of researchers in various fields (e.g., <ref type="bibr" target="#b1">Araujo et al., 2020;</ref><ref type="bibr" target="#b8">Bigman et al., 2021;</ref><ref type="bibr" target="#b38">Lee, 2018;</ref><ref type="bibr" target="#b54">Newell &amp; Marabelli, 2015)</ref>. Additionally, these overarching concepts are beneficial for scientific research aiming to investigate the social applications and implications of machine learning and artificial intelligence that are evolving rapidly and generating new concepts and terms at a remarkable pace.</p><p>Focusing on factors influencing people's preference and avoidance of ADM, previous research identified general appreciation and reliance on algorithmic advice among lay people <ref type="bibr" target="#b45">(Logg et al., 2019;</ref><ref type="bibr" target="#b79">Thurman et al., 2019)</ref>. These attitudes, however, can be easily converted to algorithm aversion and preference for human-made decisions after people experience algorithm errors <ref type="bibr" target="#b18">(Dietvorst et al., 2014)</ref>. Individuals with expertise, who tend to have higher confidence in their own judgments, also prefer their self-judgment over advice given by machines <ref type="bibr" target="#b45">(Logg et al., 2019)</ref>. Overall, individuals' perceptions of their self-characteristics as well as personal experiences of ADM are important determinants of algorithm appreciation and aversion.</p><p>Recent approaches have further inspected detailed cognitive dimensions that are closely linked to algorithm appreciation and avoidance. Perceived fairness of algorithms, trust in automated decision-making, and emotional reactions to algorithm outcomes have been identified as some of the most critical aspects of human perceptions related to ADM <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b38">Lee, 2018;</ref><ref type="bibr" target="#b39">Lee &amp; Baykal, 2017;</ref><ref type="bibr" target="#b40">Lee &amp; Rich, 2021;</ref><ref type="bibr" target="#b82">Wang et al., 2020)</ref>. First, perceived fairness depends on whether the algorithm treats everyone equally, independent of biases or personal preferences <ref type="bibr" target="#b38">(Lee, 2018)</ref>. The concept of trust has been adapted from interpersonal contexts, which referred to a psychological state in which people have the intention to accept other people's behavior with positive expectations. For technological artifacts that possess human-like characteristics, trust is considered fundamental to human-machine relationships <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b38">Lee, 2018;</ref><ref type="bibr" target="#b50">Mcknight et al., 2011)</ref>. For technologies that are expected to function fairly and accurately, trust is associated with functional aspects of the outcome, such as functionality, reliability, and usefulness of ADM <ref type="bibr" target="#b14">(Choung, David, &amp; Ross., 2022)</ref>.</p><p>Perceptions of fairness and trust are also closely related to emotional responses to algorithm outcomes <ref type="bibr" target="#b38">(Lee, 2018)</ref>. In social situations, attribution to intentionality and agency in one's behavior is a key determinant of emotional reactions to the behavior <ref type="bibr" target="#b6">(Betancourt &amp; Blair, 1992)</ref>. Violation of equality, in particular, tends to induce negative emotional responses. When equality is violated, individuals seek further explanations about the intention of the violator, and strong negative reactions (e.g., anger) can be evoked if the violation is deemed intentional <ref type="bibr" target="#b72">(Shaver, 1985)</ref>. When the violation is considered unintentional, trust in the violator can mitigate negative emotional reactions <ref type="bibr" target="#b75">(Stouten et al., 2006)</ref>.</p><p>Similar attribution processes may occur during the evaluation of algorithms and their outcomes. Trust in AI was found to predict positive attitudes and emotional attachment toward AI technologies <ref type="bibr" target="#b14">(Choung et al., 2022)</ref>. On the other hand, Lee (2018) tested whether people would show less emotional reactions to ADM compared with human decision-making, due to the perceived lack of intentionality and agency in ADM. However, the result showed similar or even more negative reactions to ADM. These findings imply that cognitive appraisals of perceived fairness, trust, as well as emotional reactions may alter attributional processes involved in algorithm appreciation or aversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Disparities in ADM and Algorithmic Bias Perceptions</head><p>Although automated decision-making aims to offer a cost-efficient, accurate, and objective alternative to potential inaccuracy and inconsistency in human decision-making, it can still generate unfair outcomes that often reflect and even amplify existing inequality and injustice in society <ref type="bibr" target="#b2">(Barocas et al., 2019;</ref><ref type="bibr" target="#b29">Hooker, 2021)</ref>. While deliberate corrections and interventions are necessary to mitigate algorithmic biases, examining cognitive responses to the biases is also crucial in understanding the consequences of discrimination and injustice that machines could engender and in envisioning better socio-technological systems for the future <ref type="bibr" target="#b3">(Benjamin, 2019;</ref><ref type="bibr" target="#b57">Noble, 2018;</ref><ref type="bibr" target="#b60">O'Neil, 2016)</ref>.</p><p>Scholars have been paying increasing attention to the connection between the perception of ADM and racial disparity <ref type="bibr" target="#b8">(Bigman et al., 2021;</ref><ref type="bibr" target="#b62">Parra et al., 2021)</ref>. Past research suggests that existing racial inequality in society shapes the reactions to and perceptions of algorithms <ref type="bibr" target="#b22">(Eubanks, 2018;</ref><ref type="bibr" target="#b36">Koenecke et al., 2020;</ref><ref type="bibr" target="#b80">Vincent &amp; Viljoen, 2020)</ref>. For example, a study found that the threat of racial inequality increases individuals' preference for ADM, particularly among minority populations <ref type="bibr" target="#b8">(Bigman et al., 2021)</ref>. It implies that the belief that non-human agents make fairer and less biased judgments can promote the acceptance of machine-driven decisions.</p><p>Another study also found that people are less likely to recognize racial or gender disparities in ADM than they are for human decisions when assuming algorithms are fair <ref type="bibr" target="#b10">(Bonezzi &amp; Ostinelli, 2021)</ref>. Empirical evidence also revealed that situations, where algorithm processes may neglect unique characteristics of individuals and generate disadvantageous outcomes against them, can stimulate resistance to ADM <ref type="bibr" target="#b46">(Longoni et al., 2019)</ref>. These findings suggest that, without contextual information signaling potential algorithmic biases, people are less likely to cast doubt on ADM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Identity, Social Disparity, and the Perception of ADM</head><p>The literature on social perception and attribution suggests that bias perceptions in social contexts are largely influenced by both the direction of bias (i.e., favorable or unfavorable to whom) and the perceiver's social identity <ref type="bibr" target="#b48">(Major et al., 2002;</ref><ref type="bibr" target="#b63">Phillips &amp; Jun, 2022)</ref>. Individuals make asymmetrical inferences about themselves versus others <ref type="bibr" target="#b65">(Pronin et al., 2004)</ref>. It was found that when people make judgments about themselves relative to others, they view themselves as less biased and objective than others, and more readily detect biases in others. Importantly, the attribution to discrimination framework <ref type="bibr" target="#b48">(Major et al., 2002)</ref> suggests that individuals attribute their negative life experiences to discrimination when (1) they (or their group) are treated unjustly/unequally and (2) the disparity was based on social identity.</p><p>Furthermore, attribution to discrimination can be reinforced by personal experiences of being targeted by discrimination and prejudice. When it comes to the perception of discrimination in ADM, the literature provides little knowledge about how social identities play a role during the attribution processes related to automated decision-making. Even without the consideration of social identities, past research showed mixed findings on the effect of the favorability of the outcome, or perceptions of favorability. For example, some studies found a significant influence of favorable algorithms on perceived fairness (e.g., <ref type="bibr" target="#b82">Wang et al., 2020)</ref>, while others report marginal or null effects (e.g., <ref type="bibr" target="#b44">Li &amp; Xing, 2022)</ref>.</p><p>The current study predicts that, in the social context of race, social identity shapes the reactions to racially biased automated decisions. Racial minorities experience discrimination and injustice in their daily lives, which in turn can lead to various deleterious outcomes, including economic disadvantages <ref type="bibr" target="#b27">(Hangartner et al., 2021;</ref><ref type="bibr" target="#b61">Pager &amp; Shepherd, 2008;</ref><ref type="bibr" target="#b68">Rosen et al., 2021)</ref>, physical and psychological distress <ref type="bibr" target="#b4">(Berger &amp; Sarnyai, 2015;</ref><ref type="bibr" target="#b11">Broman et al., 2000;</ref><ref type="bibr" target="#b12">Brown et al., 2000)</ref>, and the degradation of job and academic performances <ref type="bibr" target="#b25">(Greenhaus et al., 1990;</ref><ref type="bibr" target="#b73">Stevens et al., 2018;</ref><ref type="bibr" target="#b84">Wong et al., 2003)</ref>. How people experience and cope with racial discrimination depends on social identities of individuals <ref type="bibr" target="#b30">(Jacob et al., 2022;</ref><ref type="bibr" target="#b70">Sellers &amp; Shelton, 2003)</ref>.</p><p>The current study adopts the concept of "perceived pervasiveness" to capture how social groups perceive the degree to which discriminatory algorithm outcomes are pervasive and likely to occur in their everyday lives. Minority groups frequently experience negative events that involve discrimination and injustice against their social identities <ref type="bibr" target="#b48">(Major et al., 2002;</ref><ref type="bibr" target="#b78">Sue, 2010)</ref>.</p><p>Consequently, these populations tend to be more vigilant and perceptive toward indications of biased decision-making that may suggest discrimination against them <ref type="bibr" target="#b48">(Major et al., 2002)</ref>. It is possible that racial minorities would report greater increases in perceived pervasiveness when ADM produces biased decisions against them, compared to members of the racial majority. Such perceptions can influence their attitudes toward algorithms and AI, their support for related policies, and other aspects of their lives <ref type="bibr" target="#b69">(Schmitt et al., 2003;</ref><ref type="bibr" target="#b77">Stroebe et al., 2011)</ref>.</p><p>Thus, we hypothesized that minority groups experiencing unfavorable automated decisions, compared with minorities facing decisions that discriminate against other social groups, would perceive that biased outcomes are more likely to happen and less fair, and the processes generating these discriminatory outcomes are less trustable. Similarly, minority groups may feel more negative emotion and be more likely to question ADM processes when automated decisions discriminate against their own social groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H1</head><p>. Racial minorities will show greater reductions in (a) perceived fairness and (b) trust in ADM when ADM generates decisions biased against them, compared with the racial majority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2</head><p>. Racial minorities will show greater increases in (a) perceived pervasiveness, (b) negative emotion, and (c) tendency to question an outcome when ADM generates decisions biased against them, compared with the racial majority.</p><p>Previous research investigated the public's ability to recognize and cast doubt on biased decisions made by algorithms <ref type="bibr" target="#b62">(Parra et al., 2021)</ref>, but questions remain regarding how the magnitudes of reactions vary depending on social situational contexts. People are generally averse to resorting to ADM regarding essential matters of everyday life, such as important health or financial issues <ref type="bibr" target="#b46">(Longoni et al., 2019)</ref>. Prior work suggests that situations involving explicit economic disadvantages or advantages (for example, monetary incentives and produce prices) may increase the sensitivity to the fairness and reliability of a decision-making process <ref type="bibr" target="#b21">(Esarey et al., 2012;</ref><ref type="bibr" target="#b53">Moliner et al., 2013)</ref>. Even if individuals experience the same situations involving ADM in which they are discriminated against their race, the interpretation of the incidences may differ between white people and People of Color.</p><p>H3. People will show lower (a) perceived fairness and (b) trust in ADM when an ADM outcome is accompanied by explicit economic consequences compared with when the outcome is not connected with explicit economic consequences.</p><p>H4. People will show greater (a) negative emotion and (b) tendency to question the outcome when an ADM outcome is accompanied by explicit economic consequences compared with when the outcome is not connected with explicit economic consequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample</head><p>Participants were recruited online via Prolific, a survey recruitment platform. People who were at least 18 years and located in the United States received a digital flyer about the experiment. People could click on a link in the flyer to access the online experiment created with Qualtrics, a survey managing system.</p><p>In total, 658 participants completed the experiment. 54 participants among them failed to follow the instruction or did not pass the attention check, and their responses were removed from the results. The rest 604 participants were analyzed in this research. For analytical purposes, we categorized all participants into two social (race) groups: the white group (N = 442, 73.2%) and the People of Color (POC) group (N = 162, 26.8%). Participants who identified themselves as a man, woman, non-binary, and others ("Prefer not to disclose" and "Prefer to self-describe") accounted for 42.7%, 53.8%, 3.0%, and 0.5%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>A randomized controlled experiment was conducted to test the research hypotheses.</p><p>Informed consent was obtained from the first webpage of the experiment. On the next webpage, participants were instructed to think of one of their friends whose race is different from theirs.</p><p>The instruction also stated that the friend should be a person of the same gender, age, education level, and economic status as the participant. They were asked to keep thinking of the friend they chose when considering the scenarios that followed. To check if participants followed the instruction correctly, a question was asked about the race of the chosen friend.</p><p>Nine scenarios were given to each participant, one scenario at a time. Multiple scenarios allowed us to compare and analyze their perspectives across different situations. The number of scenarios and questions was intentionally limited to keep the online experiment duration within an optimal range of 10 to 15 minutes, as recommended to prevent participant fatigue, ensure data quality, and meet participant expectations <ref type="bibr" target="#b20">(Dynata, 2023;</ref><ref type="bibr" target="#b67">Revilla &amp; Ochoa, 2017)</ref>. Each scenario described a distinct situation in which participants and their friend use an identical technology involving ADM that resulted in a racially discriminatory outcome. The scenarios described realistic situations that have been observed in the real world and discussed in previous studies <ref type="bibr" target="#b0">(Acikgoz et al., 2020;</ref><ref type="bibr" target="#b9">Binns et al., 2018;</ref><ref type="bibr" target="#b51">A. P. Miller &amp; Hosanagar, 2019;</ref><ref type="bibr" target="#b62">Parra et al., 2021)</ref>. All scenario descriptions followed the narrative format proposed by <ref type="bibr" target="#b62">Parra et al. (2021)</ref>. For example, a scenario reads as follows:</p><p>You and your friend are both applying for the same financial product (such as a credit card, a personal loan, and a mortgage) on the same banking website. The website collects information about the users and automatically evaluates applications based on the information. The products that are offered to you charge higher interest rates than those offered to your friend.</p><p>The domains discussed in the scenarios included finance, the labor market, public service, media, and health and safety, as listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Participants were randomly assigned to one of the two conditions with different discrimination targets: discrimination against the self (the "self" condition) and discrimination against the other (the "other" condition). Participants in both conditions read the same scenarios, and only the target of discrimination varied depending on the condition. The scenarios in the self condition explained situations where an outcome disadvantages the participant compared with the friend. The example scenario quoted above was for the self condition. In the other condition, participants were given scenarios in which an outcome disadvantages the friend. For example, the aforementioned scenario described the identical situation in the other condition but ended with a different statement "The products that are offered to your friend charge higher interest rates than those offered to you." The order of presenting the scenarios was randomly determined for each participant. In each of the two conditions, 302 participants were assigned.</p><p>Participants answered a set of questions after reading each scenario. These questions measured perceived fairness, the tendency to question the outcome, and the likelihood of each situation in real life. Participants could start responding to these questions after reading a scenario for at least 10 seconds. An instructed response item was included as an attention check item in the middle of the experiment <ref type="bibr" target="#b26">(Gummer et al., 2021)</ref>. At the end of the experiment, participants were asked about their demographic characteristics. The median duration of the entire experimental process was 672 seconds. Subjects received monetary compensation for their participation. The current research was exempted by the Institutional Review Board of [a university name removed for the blind review process.] The experiment was conducted in August 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>All variables were measured on a 7-point Likert scale ranging from 1 to 7. Perceived fairness (M = 3.1, SD = 1.1) was measured with a question ("How fair or unfair is this outcome for you?") in line with previous studies <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b38">Lee, 2018)</ref>. Trust in ADM (M = 2.7, SD = 1.0) was measured using a question ("How do you trust that this [website/technology/program/platform/app] makes a good-quality decision?"), adapting a question from previous research <ref type="bibr" target="#b38">(Lee, 2018)</ref>. Negative emotion (M = 4.6, SD = 1.3, α = 0.93) was measured with three questions <ref type="bibr" target="#b38">(Lee, 2018)</ref>. Tendency to question the outcome (M = 5.0, SD = 1.1, α = 0.91) was operationalized by a measure of a participant's agreement with two statements ("This outcome is problematic" and "This outcome is questionable"). Perceived pervasiveness (M = 3.9, SD = 1.2) was measured with a question ("How likely is this outcome to happen in your everyday life?"). For the main outcome analysis, each dependent variable was averaged across all nine scenarios for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analyses</head><p>Multiple linear regressions examined the effect of the discrimination target on each dependent variable, moderated by a participant's race group. We first evaluated the coefficients and statistical significance of the interaction between the discrimination target and the race group. Additional subgroup analyses were also conducted to investigate the effect of the discrimination target in each race group (POC and white) for further investigations of moderation effects on each dependent variable. Although dividing participants into each subgroup inevitably limits the statistical power of analysis, subgroup analyses could still provide helpful insights into individuals' perceptions within each race group and their specific contexts.</p><p>In addition, we compared the marginal means of the outcome variables in each scenario and each condition. For this analysis, we estimated a new statistical model that includes the scenario as a predictor. The analysis estimated a random effect linear regression model that predicts a dependent variable as a function of the scenario, the discrimination target, the race group, and the interaction between the target and the race group, accounting for the correlation within a subject. The estimated marginal mean of a dependent variable was then calculated for each scenario in each condition based on the estimated random effect model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrimination Target × Race Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceived Fairness</head><p>The influence of discrimination against the self on perceived fairness, compared with discrimination against the other, was significantly different between POC and white participants.</p><p>As Table 2 presents, the interaction between the discrimination target and the race group on the average perceived fairness was significant (B = -0.531, SE = 0.200, p = .008). To further understand the nature of the interaction, we then analyzed responses from POC and white participants separately. The subgroup analysis indicated that the reactions of POC and white participants were opposite. Among POC, the average perceived fairness was lower in the self condition than the other condition (B = -0.483, SE = 0.168, p = .005), while the average perceived fairness among the white group was higher in the self condition than the other condition (B = 0.048, SE = 0.105, p = .645). Even when perceived fairness was analyzed for each scenario, a consistent pattern was observed. As displayed in <ref type="figure">Figure 1</ref>, despite some variations across the scenarios, the signs of the discrimination target × the race group interactions were negative in all nine scenarios, and statistical significance was found for four of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust in ADM</head><p>The influence of discrimination against the self on trust in ADM, compared with discrimination against the other, also showed a significant difference between POC and white participants. The interaction between the discrimination target and the race group on trust in ADM was significant (B = -0.674, SE = 0.173, p &lt; .001) as <ref type="table">Table 2</ref> presents. When POC and white subgroups were analyzed separately, the reactions of the two groups were opposite.</p><p>Among POC participants, trust was lower in the self condition than in the other condition (B = -0.396, SE = 0.147, p = .008), while trust among white people was higher in the self condition than the other condition (B = 0.278, SE = 0.090, p = .002). When each scenario was analyzed separately, as visualized in <ref type="figure">Figure 1</ref>, the aforementioned pattern was consistent: a negative sign for the target × the race group interaction in all nine scenarios. Statistical significance was found for seven out of the nine scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tendency to Question the Outcome</head><p>Regarding the tendency to question ADM outcomes, the influence of the discrimination target varied depending on the race group. <ref type="table">Table 2</ref> shows that the interaction between the discrimination target and the race group on the average tendency to question ADM outcomes was significant (B = 1.155, SE = 0.195, p &lt; .001). When we looked into POC and white subgroups separately, the responses of POC and white participants displayed striking contrast.</p><p>Among POC participants, the average tendency to question was greater in the self condition than the other condition (B = 0.473, SE = 0.163, p = .004), while among white participants it was lower in the self condition than in the other condition (B = -0.683, SE = 0.102, p &lt; .001). As another robustness check, the tendency to question ADM outcomes was calculated separately for each scenario. The result indicates that despite minor variations, the discrimination target × the race group interactions were positive and statistically significant in all nine scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Emotion</head><p>There was a difference in the influence of the discrimination target on the average negative emotion between white and POC participants: The interaction between the discrimination target and the race group on the average negative emotion was significant (B = 1.021, SE = 0.243, p &lt; .001). The subgroup analysis showed that the average negative emotion of the two groups, the white and the POC groups, shifted in opposite directions. <ref type="table">Table 2 presents</ref> that, while among POC participants discrimination against the self significantly increased the average negative emotion, compared with discrimination targeting other (B = 0.790, SE = 0.209, p &lt; .001), discrimination against the self reduced the average negative emotion among white participants, although it was only marginally significant (B = -0.231, SE = 0.156, p = .067). We also evaluated negative emotion in each scenario. The result supported the aforementioned finding: the discrimination target × the race group interactions were positive and statistically significant in eight out of the nine scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceived Pervasiveness</head><p>In terms of the perceived pervasiveness of the outcomes in everyday life, the influence of the discrimination target was significantly different between white and POC participants. <ref type="table">Table 2</ref> presents that the interaction between the discrimination target and the race group on the average perceived pervasiveness of the outcome was significant (B = 0.928, SE = 0.215, p &lt; .001). The subgroup analysis revealed what this significant interaction indicated. When the sample was divided into the two groups, the average perceived pervasiveness among POC participants did not show a significant difference between the self condition and the other condition (B = 0.110, SE = 0.191, p = .567), but the average perceived pervasiveness among white participants significantly decreased with discrimination targeting the self (B = -0.819, SE = 0.110, p &lt; 0.001).</p><p>The analysis of perceived pervasiveness in each scenario showed that the discrimination target × the race group interactions were positive in all nine scenarios and statistically significant in seven of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of the Friend's Race among POC Participants</head><p>During the experiment, most POC participants (N = 94, 58.0%) considered their white friend, and the rest chose to consider a non-white friend, such as a Black participant choosing an Asian friend. (All white participants chose a POC friend.) Thus, we conducted a robust check to examine the influence of the friend's race among POC participants.</p><p>For perceived fairness, trust in ADM, the tendency to question the outcome, and negative emotion, discrimination against the self changed the dependent variables in the same direction regardless of the friend's race, as visualized in <ref type="figure">Figure 2</ref>. Specifically, regardless of their friend's race, POC participants reacted to the discriminatory situations targeting the self with less perceived fairness, less trust, a higher tendency to question the outcome, and higher negative emotion. The results also suggest that POC participants who considered a white friend might show stronger reactions to the discriminatory scenario than those who chose another non-white race. Regarding perceived pervasiveness, subject-targeting discriminatory situations involving a white friend and a POC friend led to a higher and a lower perceived pervasiveness, respectively, compared with other-targeting discrimination. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes and compares the estimated marginal means of the outcome variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal Means by Scenarios</head><p>It depicts a pattern consistent across the two experimental conditions. In both conditions, perceived fairness and trust in ADM were at their lowest level in the three scenarios about insurance premiums (Scenario 9), salaries of recommended jobs (Scenario 2), and interest rates of financial products (Scenario 1). These three scenarios were the highest in the pervasiveness to question the outcome and negative emotion. Contrarily, three scenarios that induced the highest perceived fairness and trust discussed the calculation of disease risk (Scenario 8), the evaluation of job interviews (Scenario 4), and failure rates in voice recognition (Scenario 3). These three scenarios were the lowest in terms of the tendency to question the outcome and negative emotion.</p><p>Perceived pervasiveness showed a pattern distinguished from the other variables. While most scenarios produced similar levels of perceived pervasiveness, the scenario about job interview evaluation (Scenario 4) exhibited the highest perceived pervasiveness, which is significantly different from other scenarios' results. The perceived pervasiveness of the scenario about availability on a booking website (Scenario 6) was significantly lower than in other scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This study examined how social identities influence the perceptions of automated decisions that discriminate against certain social groups. The results demonstrate the significant impacts of social identities that shape cognitive and affective responses to algorithmic biases. First, lending support to H1, discrimination against the self decreased POC's perceived fairness and trust compared with discrimination against the other, whereas white individuals exhibited the opposite pattern. Further, as predicted in H2, discrimination unfavorable to the subject significantly increased the tendency to question the outcome, negative emotion, and perceived pervasiveness of the incidence among racial minorities, but these outcomes were lower with discrimination targeting the other among the racial majority group. The significant interactions indicate that the effects of discrimination targets were significantly different between the two racial groups. These results provide compelling new evidence that social identity plays an essential role in shaping people's cognitive and affective reactions to algorithmic bias.</p><p>Contrasting with the previous reports on marginal or null effects of social/group identities <ref type="bibr" target="#b82">(Wang et al., 2020)</ref>, this finding highlights the importance of social groups' innate characteristics in people's experiences of algorithms and AI <ref type="bibr" target="#b40">(Lee &amp; Rich, 2021)</ref>.</p><p>It is worth emphasizing that these results clearly reflect what People of Color and white people experience and observe in real-world situations. POC participants reported that discrimination targeting themselves was more likely to happen in the real world than discrimination targeting other races. White people also responded that ADM situations were more likely to discriminate against POC people than themselves, implying that people in the majority group are also aware of disparities and discrimination imposed on racial minorities.</p><p>These results align with previous reports on racially biased automated decisions <ref type="bibr" target="#b3">(Benjamin, 2019;</ref><ref type="bibr" target="#b13">Buolamwini &amp; Gebru, 2018;</ref><ref type="bibr" target="#b58">Obermeyer et al., 2019;</ref><ref type="bibr" target="#b68">Rosen et al., 2021)</ref>. Also, the results suggest that social disparity and inequality, and the lived experience of the existing discrimination and injustice are at the center of understanding the perception of automated decisions impacting a large number of people. An excessive focus on mathematical fairness or one-size-fits-all approaches, without taking into account diversity, inclusion, sociocultural reality, and public perception in designing, deploying, and evaluating algorithms, may lead to a "false equality" that actually exacerbates existing inequalities. A deep understanding of sociocultural factors is also essential to inform potential solutions that have been proposed to address biased AI, such as Explainable AI, which, while not a cure-all solution, could constitute one aspect of a larger approach to address the broader issue of algorithmic fairness and accountability <ref type="bibr" target="#b16">(de Bruijn et al., 2022)</ref>.</p><p>Furthermore, this research discovered that perceived fairness and trust are lower when discriminatory outcomes accompany economic disadvantages, compared with outcomes not involving explicit economic penalties, supporting H3. Contrarily, the findings also indicated that the tendency to question the outcome and negative emotion are higher in discriminatory situations involving economic disadvantages imposed on the self, supporting H4. Combined with the result presented earlier, these outputs enable us to reconcile two seemingly conflicting arguments: people experiencing biased algorithms react to economic advantages and disadvantages, but the importance of social identities emerges when we start considering the detailed context of discrimination, such as which social group is being discriminated by algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of Findings</head><p>The present findings have several important implications. First, this study reveals that social identity shapes the perceptions of ADM. While knowledge has been thin about the linkages between racial identity and racially biased automated decisions, this study provides new empirical evidence that discriminatory situations caused by machines could be perceived very differently depending on people's racial identity. It also means that research frameworks that neglect existing social inequality and injustice may provide only a partial explanation of how people process AI-augmented discriminations and how they navigate through the increasingly complex socio-technological system. Likewise, algorithmic adjustments focusing only on offering myopic economic incentives to mitigate biased outcomes will not encompass systematic disparities across different social identity groups and their situational contexts. Third, the results of this study evince people's clear reaction when one of the core principles of ADM, fairness, is violated. Our finding shows that people clearly recognize the target of discrimination and respond with sharp changes in their perceptions and emotions. It implies the possibility that repeated and large-scale exposure to algorithms that discriminate against a significant portion of a population (e.g., <ref type="bibr" target="#b13">Buolamwini &amp; Gebru, 2018;</ref><ref type="bibr" target="#b36">Koenecke et al., 2020;</ref><ref type="bibr" target="#b58">Obermeyer et al., 2019)</ref> could corrode public trust and credence in AI and produce considerable confusion, conflict, and disruption in society. Lastly, this study reveals that the context of discrimination also influences the magnitude of public reactions. We found that the perception of ADM is affected more when ADM resulted in an economic (dis)advantage, but it is worth noticing that reactions to noneconomic discrimination are also not ignorable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The present research is not without limitations. First, the participants were not a representative sample of the U.S. population, and one should be cautious in generalizing the current findings. Also, since the experiment was conducted with U.S residents, its specific social and racial context of this study should be carefully interpreted in considering the current findings in other social and cultural contexts. Second, we observed individuals' reactions when they compare themselves with one of their friends, and future research should investigate the perception of biased ADM on various types of social networks, such as how algorithm bias affects connections in working or learning environments. Third, we categorized participants into the POC and white groups, and research with larger samples and more detailed analyses will help a more contextualized understanding of the subgroups within the POC group, including Black, American Native, and Asian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Scholars have warned that the increasing use of algorithms and AI might increase the risk of racial discrimination and exacerbate the existing disparity and inequality in society <ref type="bibr" target="#b36">(Koenecke et al., 2020;</ref><ref type="bibr" target="#b52">J. Miller, 2020;</ref><ref type="bibr" target="#b58">Obermeyer et al., 2019;</ref><ref type="bibr" target="#b83">Williams et al., 2018)</ref>. Systematic social disparity and inequality differentially affect various aspects of individuals' daily life depending on their social identity. The current study incorporates individuals' social identities to understand their cognitive and affective responses to racially biased automated decisions and demonstrates that people are able to recognize when their beliefs in fairness and equality of ADM are violated, discriminating against racial minorities. Individuals' social identity is a crucial predictor that shapes the magnitude of their resistance and aversion to algorithms. Overall, findings from this study contribute empirical evidence of public perceptions and reactions related to potential algorithmic biases that are important for researchers, policymakers, and practitioners to understand the negative outcomes of algorithmic bias in society and develop fairer and equitable socio-technological systems. "You and your friend are both applying for the same financial product (such as a credit card, a personal loan, and a mortgage) on the same banking website. The website collects information about their users and automatically evaluates applications based on the information.</p><p>The products that are offered to you charge higher interest rates than those offered to your friend." 2 Labor market "You and your friend are both looking for similar jobs on the same website. The website collects information about their users and automatically recommends job positions based on the information.</p><p>The positions that the website recommends to you offer lower salaries than those recommended to your friend."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Information technology</head><p>"You and your friend use the same smartphone model and the same voice-recognition system (for example, Siri, Alexa, or Google Assistant). Both of you use the voice recognition system routinely for everyday use.</p><p>You notice that the voice recognition system fails to recognize your voice more frequently than it fails with your friend's voice." 4 Labor market "You and your friend applied for the same job position in which you both are seriously interested. You both have similar levels of experience, knowledge, and skills related to the position.</p><p>Each of you conducted an online interview with the company and was asked the same questions by an automated interview program. Applications and recorded responses are automatically rated by the program. You noticed that you and your friend did equally well in the interview and provided similar answers to the interview questions.</p><p>Two weeks after the interview, your friend is offered the position, but you do not receive any offer." 5 Public service "You and your friend have the same nationality. You both are going through an automated immigration kiosk at an airport. The kiosk uses face recognition technology to verify travelers' identities.</p><p>The kiosk directs you to see an immigration officer and provide further information while your friend is cleared to go through."</p><p>6 Hospitality "You and your friend are each booking a similar hotel room using the same travel booking website. The website collects information about their users and automatically recommends hotel rooms based on the information.</p><p>The website shows you fewer available rooms than it does for your friend." 7 Media "You and your friend both regularly write posts on similar topics on the same online social network platform (for example, Facebook, Instagram, and TikTok). The platform automatically examines posts created by users and identifies objectionable content that needs to be flagged or removed.</p><p>Your posts are found objectionable by the platform more frequently than those written by your friend." 8 Health and Safety "You and your friend both have similar diets and daily routines, and are feeling just fine. Both of you are using the same health assessment app. The app collects information about their users and automatically estimates the risk of infectious diseases based on the information.</p><p>The app suggests that your risk of contracting infectious diseases is higher than your friend's." 9 Health and Safety "You and your friend live in the same neighborhood and have similar cars and driving patterns. Both of you are purchasing the same car insurance plan from an insurance company. The company uses a program that automatically calculates each customer's insurance premium based on information about the customers they collect.</p><p>The company charges you more money than that it charges your friend for the same insurance plan."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head><p>Scenario 1 in the self condition Scenario 1 in the other condition Example (Scenario 1) "… The products that are offered to you charge higher interest rates than those offered to your friend." "…The products that are offered to your friend charge higher interest rates than those offered to you." <ref type="figure">Figure 1</ref>. Responses by conditions, races, and scenarios. The dependent variable is shown on the Y-axis. Each dependent variable was measured for a single scenario (e.g., "Perceived fairness 5" is perceived fairness measured in Scenario 5). The X-axis represents experimental conditions. The red and blue lines represent POC and white participants, respectively. The dots indicate averages, and an error bar represents the standard error of a mean. The coefficient and the p-value of the target × race interaction effect on each outcome are displayed inside the plot. *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05. <ref type="figure">Figure 2</ref>. Average outcomes among POC participants by the friend's race (N = 162). The dependent variable is shown on the Y-axis. The X-axis represents experimental conditions. The green and blue lines represent participants who chose a POC friend and a white friend, respectively. The dots indicate averages, and an error bar represents the standard error of a mean. The coefficient and the p-value of the target × race interaction effect on each outcome are displayed inside the plot. *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05. A difference between two EMMs is statistically significant at α = 0.05 with the Bonferroni correction if their respective arrows ("comparison arrows") do not overlap <ref type="bibr" target="#b43">(Lenth, 2022)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>All statistical analyses were conducted on an open-source statistical software, R (version 4.0.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Estimated Marginal Means of Outcomes by Scenarios and Conditions. The red dots indicate estimated marginal means (EMMs). The gray bars represent the 95% confidence intervals of the EMMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Scenarios used in the experiment</figDesc><table><row><cell>A</cell><cell></cell><cell></cell></row><row><cell>Index</cell><cell>Domain</cell><cell>Scenario given in the self condition</cell></row><row><cell>1</cell><cell>Finance</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 2</ref><p>. The interaction effect between the discrimination target and the race group of the subject on experimental outcomes. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Justice perceptions of artificial intelligence in selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Acikgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Compagnone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laske</surname></persName>
		</author>
		<idno type="DOI">10.1111/ijsa.12306</idno>
		<ptr target="https://doi.org/10.1111/ijsa.12306" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Selection and Assessment</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="416" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In AI we trust? Perceptions about automated decision-making by artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Helberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kruikemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>De Vreese</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-019-00931-w</idno>
		<ptr target="https://doi.org/10.1007/s00146-019-00931-w" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fairness and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="https://fairmlbook.org" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Race after technology: Abolitionist tools for the new Jim code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polity</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">More than skin deep&quot;: Stress neurobiology and mental health consequences of racial discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sarnyai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stress</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.3109/10253890.2014.989204</idno>
		<ptr target="https://doi.org/10.3109/10253890.2014.989204" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Cognition (Attribution)-Emotion Model of Violence in Conflict Situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Blair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="350" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0146167292183011</idno>
		<ptr target="https://doi.org/10.1177/0146167292183011" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Threat of racial and economic inequality increases preference for algorithm decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marciano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.106859</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.106859" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">106859</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">It&apos;s Reducing a Human Being to a Percentage&quot;; Perceptions of Justice in Algorithmic Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lyngs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
		<idno type="DOI">10.31235/osf.io/9wqxr</idno>
		<ptr target="https://doi.org/10.31235/osf.io/9wqxr" />
	</analytic>
	<monogr>
		<title level="m">ACM CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2018-01-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can algorithms legitimize discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostinelli</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000294</idno>
		<ptr target="https://doi.org/10.1037/xap0000294" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="459" />
			<date type="published" when="2021" />
			<publisher>APA PsycArticles</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Experience and Consequences of Perceived Racial Discrimination: A Study of African Americans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Broman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mavaddat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1177/0095798400026002003</idno>
		<ptr target="https://doi.org/10.1177/0095798400026002003" />
	</analytic>
	<monogr>
		<title level="j">Journal of Black Psychology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="180" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Being black and feeling blue&quot;: The mental health consequences of racial discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Neighbors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Sellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1090-9524</idno>
		<ptr target="https://doi.org/10.1016/S1090-9524" />
		<imprint>
			<date type="published" when="2000" />
			<publisher>Race and Society</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Fairness, Accountability and Transparency</title>
		<meeting>the 1st Conference on Fairness, Accountability and Transparency</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trust in AI and Its Role in the Acceptance of AI Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/10447318.2022.2050543</idno>
		<ptr target="https://doi.org/10.1080/10447318.2022.2050543" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Bruijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janssen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.giq.2021.101666</idno>
		<ptr target="https://doi.org/10.1016/j.giq.2021.101666" />
	</analytic>
	<monogr>
		<title level="j">Government Information Quarterly</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">101666</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithmic Transparency in the News Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koliska</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2016.1208053</idno>
		<ptr target="https://doi.org/10.1080/21670811.2016.1208053" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="809" to="828" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sociotechnical view of algorithmic fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dolata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwabe</surname></persName>
		</author>
		<idno type="DOI">10.1111/isj.12370</idno>
		<idno>isj.12370</idno>
		<ptr target="https://doi.org/10.1111/isj.12370" />
	</analytic>
	<monogr>
		<title level="j">Information Systems Journal</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Survey Length Best Practices: Are Shorter Surveys Better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynata</surname></persName>
		</author>
		<ptr target="https://www.dynata.com/survey-length-best-practices-are-shorter-surveys-better/" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What Motivates Political Preferences? Self-Interest, Ideology, and Fairness in a Laboratory Democracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Esarey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrilleaux</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1465-7295.2011.00394.x</idno>
		<ptr target="https://doi.org/10.1111/j.1465-7295.2011.00394.x" />
	</analytic>
	<monogr>
		<title level="j">Economic Inquiry</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="604" to="624" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automating inequality: How high-tech tools profile, police, and punish the poor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eubanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>St. Martin&apos;s Press</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dafoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.1.11222</idno>
		<ptr target="https://doi.org/10.1613/jair.1.11222" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="729" to="754" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A.I. Is Learning to Read Mammograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grady</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2020/01/01/health/breast-cancer-mammogram-artificial-intelligence.html" />
		<imprint>
			<date type="published" when="2020-01-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effects of Race on Organizational Experiences, Job Performance Evaluations, and Career Outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Greenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Wormley</surname></persName>
		</author>
		<idno type="DOI">10.2307/256352</idno>
		<ptr target="https://doi.org/10.2307/256352" />
	</analytic>
	<monogr>
		<title level="j">The Academy of Management Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="86" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using Instructed Response Items as Attention Checks in Web Surveys: Properties and Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roßmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silber</surname></persName>
		</author>
		<idno type="DOI">10.1177/0049124118769083</idno>
		<ptr target="https://doi.org/10.1177/0049124118769083" />
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="264" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monitoring hiring discrimination through online recruitment platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hangartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siegenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">589</biblScope>
			<biblScope unit="issue">7843</biblScope>
			<biblScope unit="page">7843</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/s41586-020-03136-0</idno>
		<ptr target="https://doi.org/10.1038/s41586-020-03136-0" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moving beyond &quot;algorithmic bias is a data problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2021.100241</idno>
		<ptr target="https://doi.org/10.1016/j.patter.2021.100241" />
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Systematic Review of Black People Coping With Racism: Approaches, Analysis, and Empowerment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ouimet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1177/17456916221100509</idno>
		<ptr target="https://doi.org/10.1177/17456916221100509" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting to Artificial Intelligence: Radiologists and Pathologists as Information Specialists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2353" to="2354" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">10.1001/jama.2016.17438</idno>
		<ptr target="https://doi.org/10.1001/jama.2016.17438" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AI system outperforms humans in designing floorplans for microchips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-021-01515-9</idno>
		<ptr target="https://doi.org/10.1038/d41586-021-01515-9" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">594</biblScope>
			<biblScope unit="issue">7862</biblScope>
			<biblScope unit="page" from="183" to="185" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Risk Clusters, Hotspots, and Spatial Intelligence: Risk Terrain Modeling as an Algorithm for Police Resource Allocation Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Caplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Piza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Quantitative Criminology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="362" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10940-010-9126-2</idno>
		<ptr target="https://doi.org/10.1007/s10940-010-9126-2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Racial disparities in automated speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koenecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nudell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quartey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mengesha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rickford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="7684" to="7689" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1915768117</idno>
		<ptr target="https://doi.org/10.1073/pnas.1915768117" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951718756684</idno>
		<ptr target="https://doi.org/10.1177/2053951718756684" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Algorithmic Mediation in Group Decisions: Fairness Perceptions of Algorithmically Mediated vs. Discussion-Based Social Division</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baykal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2998181.2998230</idno>
		<ptr target="https://doi.org/10.1145/2998181.2998230" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
		<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1035" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3411764.3445570</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445570" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">emmeans: Estimated Marginal Means, aka Least-Squares Means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Lenth</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=emmeans" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revealing Factors Influencing Students&apos; Perceived Fairness: A Case with a Predictive System for Math Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491140.3528293</idno>
		<ptr target="https://doi.org/10.1145/3491140.3528293" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM Conference on Learning @ Scale</title>
		<meeting>the Ninth ACM Conference on Learning @ Scale</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2018.12.005" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Resistance to Medical Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/jcr/ucz013</idno>
		<ptr target="https://doi.org/10.1093/jcr/ucz013" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Antecedents and consequences of attributions to discrimination: Theoretical and empirical advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Quinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mccoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Advances in Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="251" to="330" />
			<date type="published" when="2002" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S0065-2601(02</idno>
		<ptr target="https://doi.org/10.1016/S0065-2601(02" />
		<imprint>
			<biblScope unit="page" from="80007" to="80014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Trust in a specific technology: An investigation of its components and measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Thatcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Clay</surname></persName>
		</author>
		<idno type="DOI">10.1145/1985347.1985353</idno>
		<ptr target="https://doi.org/10.1145/1985347.1985353" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How Targeted Ads and Dynamic Pricing Can Perpetuate Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hosanagar</surname></persName>
		</author>
		<ptr target="https://hbr.org/2019/11/how-targeted-ads-and-dynamic-pricing-can-perpetuate-bias" />
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2019-11-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Is an Algorithm Less Racist Than a Loan Officer? The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2020/09/18/business/digital-mortgages.html" />
		<imprint>
			<date type="published" when="2020-09-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Perceived Reciprocity and Well-Being at Work in Non-Professional Employees: Fairness or Self-Interest?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Martínez-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Peiró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cropanzano</surname></persName>
		</author>
		<idno type="DOI">10.1002/smi.2421</idno>
		<ptr target="https://doi.org/10.1002/smi.2421" />
	</analytic>
	<monogr>
		<title level="j">Stress and Health</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="39" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Strategic opportunities (and challenges) of algorithmic decision-making: A call for action on the long-term societal effects of &apos;datification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marabelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Strategic Information Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jsis.2015.02.001</idno>
		<ptr target="https://doi.org/10.1016/j.jsis.2015.02.001" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Digital technologies and artificial intelligence&apos;s present and foreseeable impact on lawyering, judging, policing and law enforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nissan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-015-0596-5</idno>
		<ptr target="https://doi.org/10.1007/s00146-015-0596-5" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Algorithms of oppression: How search engines reinforce racism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Noble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>New York University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6464</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<idno type="DOI">10.1126/science.aax2342</idno>
		<ptr target="https://doi.org/10.1126/science.aax2342" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Weapons of math destruction: How big data increases inequality and threatens democracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O'neil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>First edition. Crown</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The Sociology of Discrimination: Racial Discrimination in Employment, Housing, Credit, and Consumer Markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shepherd</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.soc.33.040406.131740</idno>
		<ptr target="https://doi.org/10.1146/annurev.soc.33.040406.131740" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="209" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Likelihood of Questioning AI-based Recommendations Due to Perceived Racial/Gender Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dennehy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TTS.2021.3120303</idno>
		<ptr target="https://doi.org/10.1109/TTS.2021.3120303" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Technology and Society</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Why benefiting from discrimination is less recognized as discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="825" to="852" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/pspi0000298</idno>
		<ptr target="https://doi.org/10.1037/pspi0000298" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Objectivity in the Eye of the Beholder: Divergent Perceptions of Bias in Self Versus Others</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="781" to="799" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0033-295X.111.3.781</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.111.3.781" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ideal and Maximum Length for a Web Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Revilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ochoa</surname></persName>
		</author>
		<idno type="DOI">10.2501/IJMR-2017-039</idno>
		<ptr target="https://doi.org/10.2501/IJMR-2017-039" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Market Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="557" to="565" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Racial Discrimination in Housing: How Landlords Use Algorithms and Home Visits to Screen Tenants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M E</forename><surname>Garboden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cossyleon</surname></persName>
		</author>
		<idno type="DOI">10.1177/00031224211029618</idno>
		<ptr target="https://doi.org/10.1177/00031224211029618" />
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="787" to="822" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Women&apos;s emotional responses to the pervasiveness of gender discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Branscombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Postmes</surname></persName>
		</author>
		<idno type="DOI">10.1002/ejsp.147</idno>
		<ptr target="https://doi.org/10.1002/ejsp.147" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The role of racial identity in perceived racial discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Sellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Shelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1079" to="1092" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0022-3514.84.5.1079</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.84.5.1079" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">The attribution of blame: Causality, responsibility, and blameworthiness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Shaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Racial/ethnic disparities in US college students&apos; experience: Discrimination as an impediment to academic performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American College Health</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/07448481.2018.1452745</idno>
		<ptr target="https://doi.org/10.1080/07448481.2018.1452745" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Violating Equality in Social Dilemmas: Emotional and Retributive Reactions as a Function of Trust, Attribution, and Honesty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="894" to="906" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0146167206287538</idno>
		<ptr target="https://doi.org/10.1177/0146167206287538" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Is the world a just place? Countering the negative consequences of pervasive discrimination by affirming the world as just: Negative consequences of discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stroebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Dovidio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ellemers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>John</surname></persName>
		</author>
		<idno type="DOI">10.1348/014466610X523057</idno>
		<ptr target="https://doi.org/10.1348/014466610X523057" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="484" to="500" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Microaggressions in everyday life: Race, gender, and sexual orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Sue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">My Friends, Editors, Algorithms, and I: Examining audience attitudes to news selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Helberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trilling</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2018.1493936</idno>
		<ptr target="https://doi.org/10.1080/21670811.2018.1493936" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="469" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Racist Algorithms or Systemic Problems? Risk Assessments and Racial Disparities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Viljoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Criminal Justice and Behavior</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1576" to="1584" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0093854820954501</idno>
		<ptr target="https://doi.org/10.1177/0093854820954501" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376813</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376813" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">How Algorithms Discriminate Based on Data They Lack: Challenges, Solutions, and Policy Implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shmargad</surname></persName>
		</author>
		<idno type="DOI">10.5325/jinfopoli.8.2018.0078</idno>
		<ptr target="https://doi.org/10.5325/jinfopoli.8.2018.0078" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Policy</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The Influence of Ethnic Discrimination and Ethnic Identification on African American adolescents&apos; School and Socioemotional Adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sameroff</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-6494.7106012</idno>
		<ptr target="https://doi.org/10.1111/1467-6494.7106012" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1197" to="1232" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Framing the challenges of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjqs-2018-008551</idno>
		<ptr target="https://doi.org/10.1136/bmjqs-2018-008551" />
	</analytic>
	<monogr>
		<title level="j">BMJ Quality &amp; Safety</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="238" to="241" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
