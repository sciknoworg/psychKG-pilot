<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Innovating Skin Cancer Detection with Transfer Learning, Feature Fusion, Efficient Data Management, Data Augmentation and Interdisciplinary Expertise for Better Outcomes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Musa</forename><surname>Cim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turanli</forename><surname>Mirac</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Bournemouth University United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Dijital Health and Artificial Intelligence Bournemouth University United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Bournemouth University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Performance Metrics of Deep Learning Models on Nailmelanoma Dataset Before Data Augmentation Model Accuracy % F-1 Score % Precision % Recall</orgName>
								<address>
									<addrLine>94 99 Mobile Net 91 95 93 98 Mobile Netv2</addrLine>
									<postBox>% VGG19 85 91 88 95 ResNet1 01 72 83 87 79 ResNet1 52V2 94 98 98 99 Xception 90 95 92 97 Inceptio nV3 88 93 88 99 MobileN et 90 95 95 95 MobileN etv2 84 90 86 95 93 96</postBox>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Innovating Skin Cancer Detection with Transfer Learning, Feature Fusion, Efficient Data Management, Data Augmentation and Interdisciplinary Expertise for Better Outcomes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Intelligence</term>
					<term>Machine Learning</term>
					<term>Melanoma</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Data Transformation</term>
					<term>Transfer Learning</term>
					<term>Wearable Devices</term>
				</keywords>
			</textClass>
			<abstract>
				<p>There is a growing acceptance of the use of Artificial Intelligence (AI) in the use of healthcare, including in dermatology.AI is better at recognizing images and learning patterns and due to this, i has grown into a powerful tool that can sometimes be more accurate than people and this has been demonstrated in recent studies showcasing the potential of AI. According to the World Cancer Research Fund International, the global incidence of skin cancer may be lesser than it is because of the challenges in data collection, different types of skin cancer, interpretation of the results and demographics. Early detection and diagnosis in the majority of cases saves life. This has been proven overtime with most Non-communicable diseases NCDs and this is no different with skin cancer. This is where the application of AI in the diagnosis comes to play. In this study, we discussed the application of AI in Melanoma diagnosis. The use of machine learning methods like Reinforcement Learning (RL), Support Vector Machines (SVM), K-Nearesr Neighbour (KNN) and Deep learning models like Convolutional Neural Network (CNN), transfer learning and a combination of both models were explored and showed significant accuracies in Melanoma diagnosis compared to the traditional methods. We also looked at the economic impact of AI in healthcare, its cost-effectiveness and public perception of using AI. This study further emphasises the growing importance of AI in healthcare and its integration into Melanoma diagnosis. It also explored the importance of AI in assisting dermatologists in the early detection of skin cancer thereby saving lives.AI has the impact to make our medical decision making easier, faster and with better accuracy and precision thereby increasing access to care.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Malignant melanoma is a very dangerous type of skin cancer that needs to be found quickly so that it can be treated effectively and the patient's outlook is improved. The morbidity and death rate as a result of melanoma is increasing compared to nonmelanoma skin cancer as reported in USA where it is accounted as the third most commonly diagnosed cancer <ref type="bibr">(Marghoob et al., 2009;</ref><ref type="bibr">Stiff et al., 2020)</ref>. Regular testing methods work, but they are limited in how fast, accurately, and widely they can be used. Because it is better at recognizing images and learning patterns, artificial intelligence (AI) has grown into a powerful tool that can sometimes be more accurate than people and this has been demonstrated in recent studies showcasing the potential of AI. Globally, AI based on Machine Learning and Deep Learning methods has gained tremendous interest in this era (Xiaohong et al., 2020). In the area of dermatology, we look into what happens when different AI methods are put together. The methods include deep learning, machine learning, and specific methods like Support Vector Machines (SVM), Convolutional Neural Networks (CNN), and Reinforcement Learning (RL). This study looks at how well, what problems, and what new features might come up with artificial intelligence tools used in dermatology to help with diagnosis and treatment. In addition, we look at how the teamwork of doctors, engineers, and other health care workers has made this growth possible. The huge impact AI has had on the costs of diagnosing melanoma, the progress made in teaching and training dermatologists, how people think about and use AI in healthcare, and the creative ways smart devices can find cancer are all taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>Melanoma is the most dangerous type of skin cancer, and getting it diagnosed early is very important for treatment to work. The way diagnoses are done in this area is changing because of artificial intelligence, which also helps doctors make decisions. In this part, we talk about how deep learning and machine learning methods, two types of artificial intelligence, are being developed and used to find cancer. As technology has improved, machine learning methods like Reinforcement Learning (RL), Support Vector Machines (SVM), and K-Nearest Neighbors (KNN), as well as deep learning methods like Convolutional Neural Networks (CNN) and transfer learning, have become more important in this difficult task. Through an in-depth literature study, this section looks at how these methods can be used to find melanoma, what they have accomplished, what problems they face, and how they add to the field of dermatology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning</head><p>Catarina Barata and her colleagues used reinforcement learning to make their AI models better by making changes based on what doctors said. This made it possible to correctly identify melanoma rise from 61.4% to 79.5% of the time <ref type="bibr" target="#b0">[1]</ref>. Researchers tested this method with 89 dermatologists and found that it helped them make better treatment decisions. It blends previous AI methods with tips from experts. For finding melanoma, a new Deep Deterministic Policy Gradients (DDPG) method was tried on the International Skin Imaging Collaboration (ISIC) 2017 dataset in a study that combined reinforcement learning (RL) with deep learning. The exact job of skin lesion segmentation requires this advanced algorithm, which is an expert at working with high-dimensional, continuous action spaces. Using a Markov decision process (MDP) to model skin disease segmentation was meant to improve segmentation accuracy over time by accurately simulating the complicated decision-making processes of doctors. The RL model works in a set state space and performs actions to enhance segmentation masks based on a payment function that aims to lower the L2 mean square error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before RL Implementation</head><note type="other">After</note><p>Using an updated Experience Replay Memory (ERM) that gave students a variety of useful learning experiences was an important part of the training. Achieving 96.33% accuracy for nevi, 95.39% accuracy for melanoma, and 94.27% accuracy for seborrheic keratosis <ref type="bibr" target="#b19">[18]</ref> was the result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Support Vector Machine and K-Nearest Neighbors Methods</head><p>Within their work on using machine learning to diagnose cancer, Sepehr Salem Gahfarrokhi and his colleagues did a good job with the Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) methods. Combining the Radial Basis Function (RBF) kernel and Lagrange coefficients with SVM improves the separation of skin disease images by making a hyperplane for each classification. One other method is KNN, a non-parametric supervised classification approach that names things during the learning phase and sorts data points correctly during the testing phase by how close they are to their nearest neighbors <ref type="bibr" target="#b7">[8]</ref>. Following the steps of Data Preparation, Feature Extraction, and Feature Vector Creation in another SVM work by Saranagata Kundu and colleagues, the dataset was used to train the SVM model. An accuracy of 87.14% was reached by the model on the ISIC dataset during the review step <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CNN (Convolutional Neural Network) and Transfer Learning</head><p>In order to find malignant melanoma cancer, M. Senthil Sivakumar and his friends have created a model that works with a web tool. When they made their model, they used the ResNet50 framework for data collection, preprocessing, segmentation, improvement, feature extraction, classification, and analysis. Their efforts to improve classification accuracy included lowering noise, filtering, and mixed hybrid pooling. They got an amazing 94% accuracy rate and an F1 score of 93.9% <ref type="bibr" target="#b12">[12]</ref> when they tested their system on a set of 3,300 images from the International Skin Imaging Collaboration (ISIC).</p><p>A collection called Nailmelanoma, which is made up of pictures of nail melanoma, was used by Mujahid Hussain and his coworkers in another study. There are seven deep learning models that were trained on this dataset to find nail cancer very accurately. These are VGG19, ResNet101, ResNet152V2, Xception, InceptionV3, MobileNet, and MobileNetv2. When they used these models <ref type="bibr" target="#b11">[11]</ref>, they also used statistics to show how using data augmentation affected success measures. Another research using the SIIM-ISIC Melanoma Classification Challenge 2020 dataset <ref type="bibr" target="#b28">[27]</ref> looked at different CNN models, including EfficientNet, ResNet, Inception, VGG, and MobileNet. Data imbalance was fixed by using methods such as picture resizing and basic enhancement after stressing the significance of data preparation. Furthermore, the study dealt with this mismatch by combining CycleGAN and Semi-Supervised GAN (SGAN). CycleGAN changes pictures between harmless and harmful places, and SGAN improves datasets that don't have many labels. The study was evaluated using AUC and binary cross-entropy loss, which showed how well EfficientNetB0 did in transfer learning and how well SGAN could generalize in classifying cancer <ref type="bibr" target="#b17">[16]</ref>.</p><p>A tweaked version of the AlexNet convolutional neural network and morphological segmentation techniques were combined by Govindaswamy and coworkers to create a method for finding melanoma. Start by using Histogram Equalization (HE) to make the contrast between pixels in dermoscopy pictures better. For example, this step is very important for telling the difference between skin, hair, and possible cancerous spots in skin pictures. When the pixels' spatial features are improved, the Non-Subsampled Contourlet Transform (NSCT) turns them into frequency-time features.</p><p>Within the feature calculation stage, Local Ternary Patterns (LTP) are utilized to examine pixel associations in the improved pictures, which assists in the detection of melanoma. It then uses different functions to add to these features, which raises the number of feature factors and makes a feature matrix. pictures of melanoma skin cancer can be told apart from pictures of healthy skin by using this grid to feed it into a convolutional neural network for classification <ref type="bibr" target="#b18">[17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Machine learning (ML) and deep learning (DL) employ a multitude of performance metrics, including:</head><p>Utilizing a confusion matrix to assess the performance of a classification model on a set of true values that are known in advance. It provides a breakdown of the number of accurate and inaccurate predictions by class.  The F1-Score, which provides a balance between Precision and Recall, is the harmonic mean of the two. It proves to be advantageous in situations where both false positives and false negatives must be considered.</p><formula xml:id="formula_0">1 = 2 × × + Equation 5: F1-Score Metric Formula.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenges and Risks</head><p>3.1 Problems can arise when using uneven datasets to find skin cancer using deep learning. pictures of mild skin diseases are more common in these sets than pictures of severe ones. Although this problem can be fixed, the mismatch makes it hard for programs to learn well. Therefore, these methods don't work as well when the data isn't fair <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Most of the pictures used to train artificial intelligence models are of people with fair skin, which is a problem for skin cancer study that needs more diverse data. Therefore, these models are good at finding skin problems in light-skinned people but might not be as good for people with darker skin because there aren't enough pictures of them <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Numerous pictures that have been</head><p>properly labeled are needed for artificial intelligence models, especially those that learn from examples (like supervised learning models). Clearly labeling these pictures takes a lot of time and money, and usually requires the help of expert doctors. Not having accurate labels could make AI not learn correctly, resulting in good performance on training pictures but not in real-life situations <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Skin lesions need to be correctly categorized by Convolutional Neural</head><p>Networks (CNNs) that are sufficiently complicated. These kinds of complicated problems usually need big sets of data to be solved more accurately. In contrast, using big datasets to train these complicated models can take a lot of time. For overfitting, where the model is too closely matched to the training data and might not work well with new, unrelated data, there is also a chance of performance going down in real-life situations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5</head><p>In perfect lighting and distance situations, DSLR cameras usually take high-quality dermoscopic pictures. Although deep learning algorithms that are taught on these kinds of datasets can make diagnoses that are pretty accurate, they do much worse when tried on pictures taken with smartphones at different distances and lighting conditions. Patients' self-taken, often low-quality, and not digitally dermatology-compatible skin pictures are especially hard to deal with <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.6</head><p>Artificial intelligence models often use data from medical institutions or public libraries. Skin tumor masks weren't available in the Kaggle Melanoma 2020 dataset for one study, so researchers used a model called DoubleU-Net to make them. The use of patient data to train artificial intelligence models brings up concerns about preserving patient privacy. Ensuring the legal and moral use of patient data means protecting privacy, getting permission, and thinking about how decisions made by AI might affect care and outcomes for patients <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b15">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.7</head><p>Computer programs called Convolutional Neural Networks (CNNs) often make decisions that don't make sense because they use weights that people can't understand. There isn't enough information available to make it easy to understand how these models get their results. Although these models are not completely clear, they are not widely used or trusted in clinical settings. This is because patients and healthcare workers may not trust a system they don't fully understand, which could be called a "black box" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>Modern deep learning methods was used in one of the study reviewed to show a diverse approach that can improve the accuracy of finding cancer. Transforming the large SIIM_ISIC 2020 dataset <ref type="bibr" target="#b28">[27]</ref> into the simpler tfrecord format was the first step in the plan. This format makes working with data much easier and faster. Using featurelevel fusion learning to combine dermatoscopic pictures with relevant patient information is a key part of our method; this increases the analysis depth of the model. We increased the size of our dataset even more by rotating, shearing, zooming, shifting, and changing the colors of the data. This made it more varied and made it easier for the model to work in a wider range of real-world situations. The accuracy and usefulness of our data in model training processes are guaranteed by a careful study and preparation of information. An important part of our method is using transfer learning to make our model work better. We do this by using the EfficientNet model, which is known for being good at many medical imaging tasks. Using this combined method will greatly improve the accuracy and speed of finding melanoma, making it a new and useful addition to the field of medical image analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The dataset will be changed into tfrecord format: Tfrecord is a special file format that works with TensorFlow.</head><p>Because it's so big, the SIIM_ISIC 2020 dataset <ref type="bibr" target="#b28">[27]</ref> can take a long time to load and process. It also takes up a lot of storage space and is hard to handle because the information is kept separate from the picture files.</p><p>Since it keeps data in a set of binary files, tfrecord can help you save disk space and speed up the loading and processing of data at this time. Also, it can work well with TensorFlow, which lets changes be made to data while it is being stored. Furthermore, it can store different kinds of data (such as text, pictures, numbers, etc.) in a single file, which makes it simpler to handle this information <ref type="bibr" target="#b20">[19]</ref>.</p><p>This means that tfrecord is a great way to save time and resources when working with a big dataset like the SIIM_ISIC 2020 dataset <ref type="bibr" target="#b28">[27]</ref>. 31 GB of computer storage is used by files that hold JPEG pictures and their information. But when we save these files to our hard drive in tfrecord format with their information and a 256x256 size, they take up about 1 GB of space. This is a must for handling info quickly and managing it well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Level Fusion Learning</head><p>At an early stage of the network architecture, feature-level fusion learning integrates feature representations obtained from various data sources <ref type="bibr" target="#b25">[24]</ref>. Dermatoscopic images and pertinent patient metadata are utilized in our particular application of these systems. We generate a unified representation that incorporates the distinct information delivered by each method by combining the feature maps acquired from these sources.</p><p>By incorporating feature-level data from both dermatoscopic images and clinical information, our model has the capability to augment the decision-making progress. By capitalizing on the complementarity that exists between image features and clinical data, this model may be capable of detecting melanoma with greater precision, thanks to the collaboration between fusion learning and transfer learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation</head><p>In order to increase the quantity and variety of a dataset, data transformation is a prevalent technique, particularly in CNNs: By providing a model with a wider range of training data, this approach enhances its generalizability and facilitates greater adaptation to variations in the real world. Methods for transforming medical data and matrices applicable to skin lesion images, the subject of this article, are detailed in the following section.</p><p>The process by which an image is rotated at a specific angle is known as rotation. Because patient positions can fluctuate, it is essential to account for these changes when processing medical images. This method is quite useful in such cases. By simulating deformations brought about by the perspective of anatomical structures, shear can be utilized to cut an image along a single axis. An instance in which a bone or organ appears skewed in the image is one in which this may serve as a remedy. In order to enlarge or reduce an image by a specified factor, one must zoom in or out. It can be employed in medical imaging to represent variations in patient sizes and scanning resolutions. A specified quantity of horizontal or vertical displacement of the image constitutes a shift. This enables fragmentary views of the scanning area or subtle variations in patient position to be represented during the scanning process. Altering the saturation value of a given color in an image is designated as saturation adjustment. One can attain consistent outcomes by employing Saturation to simulate color variations caused by diverse devices and illumination conditions.</p><p>By shifting from the RGB to the HSV (Hue, Saturation, Value) color space, saturation adjustment is commonly executed. Thus, the Saturation value can be effectively isolated. Generally speaking, the saturation value ranges from zero (indicating complete desaturation) to one (indicating complete saturation). A factor of increase or decrease, or a specific value, can be applied to the present saturation value in order to modify it.</p><p>The calculation for the new saturation value ′ is as follows, where represents the initial saturation value and denotes the adjustment factor:</p><formula xml:id="formula_1">′ = × Equation 10: Saturation Adjustment.</formula><p>In this case, saturation increases if &gt; 1, and decreases if &lt; 1.</p><p>Following the adjustment of the saturation value in the HSV color space, the image is transformed back into the RGB color space to restore it to its original state. Contrast: Changing the disparity between the light and dark regions of an image is the contrast operation, which can affect the image's sharpness and level of detail. Contrast adjustments in medical images can facilitate diagnosis by increasing the differentiation between tissues.</p><p>The following transformation may be applied to each pixel in the image in order to modify contrast:</p><formula xml:id="formula_2">= ⋅ + Equation 11: Contrast Adjustment.</formula><p>In this context, the variable denotes the pixel value of the source image, whereas signifies the pixel value that has been altered. denotes the quantity of luminance to be added to the image, while is the factor utilized to adjust the contrast.</p><p>Adjustment of the intensity of brilliant and dark regions can improve the visibility of finer details in medical images through brightness adjustment. To achieve luminance adjustment for individual pixels in the image, the subsequent procedure can be executed:</p><formula xml:id="formula_3">= + ℎ ℎ Equation 12: Brightness Adjustment.</formula><p>In this case, ℎ ℎ can take on either a positive or negative value to adjust the brightness level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Methods for Efficient Metadata Preparation and Analysis</head><p>A clear indication of which columns contain missing data and to what degree can be obtained through the visualization of missing data. Determining data cleansing and populating strategies is highly dependent on this information. It is crucial to perform missing data analysis in order to increase the precision of machine learning models and preserve the integrity of the dataset. When gender information is absent from a record in the melanoma dataset, the prevailing gender value,'male', is used to complete the entry. Filling in lacking data frequently employs this technique. In the age column, the median value of the given dataset is utilized to populate any missing values.</p><p>Less susceptible to outlier influence, the median functions as a measure of central tendency. These techniques guarantee the reasonable completion of absent data, thereby enhancing the efficacy of our model's training process.</p><p>Numerical data is generally more effective for training machine learning models. Thus, the conversion of categorical data to numeric values is required, including gender, anatomy, and diagnosis. Implementing this procedure enhances the efficiency of our model in handling categorical data. Such a transformation of data can have a substantial impact on the efficacy of deep learning models in particular.</p><p>Graphics pertaining to the data analyses we have performed on CSV data are provided below:     Utilizing the expertise of models that have been pre-trained on extensive and varied datasets, transfer learning has demonstrated remarkable efficacy in the domain of image recognition.</p><p>With regard to the extraction of critical features from skin images, these pre-trained models may yield superior outcomes in the detection of melanoma.</p><p>Extensive computational resources and data are necessary to train a model from the ground up using medical images such as melanoma.</p><p>Consequently, targeted applications of a model, such as melanoma detection, are possible by fine-tuning the initial and final layers of a previously effective model that has been trained.</p><p>The results of the classification process utilizing transfer learning and ISIC-2020 data are presented in the table below, which was retrieved from the ISIC Archive Live Leader Board <ref type="bibr" target="#b27">[26]</ref>. The Efficient Net model has the greatest AUC value, as the graph illustrates. In addition to producing positive findings for skin cancer diagnosis, the Efficient Net model has shown efficacy in a number of other investigations, including those involving the identification of COVID-19 <ref type="bibr" target="#b22">[21]</ref>, lung cancer <ref type="bibr" target="#b23">[22]</ref>, and Alzheimer's disease <ref type="bibr" target="#b24">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL AUC</head><p>Especially on large-scale data sets like Image Net, the transfer learning model Efficient Net has shown great accuracy rates in benchmark testing. It has been able to provide higher performance with less processing power when compared to previous models.</p><p>Furthermore, because of its depth, breadth, and resolution scaling, Efficient Net enables improved identification of intricate skin lesions like melanoma.</p><p>High generalization across different tasks and data sets is shown by Efficient Net. As such, it is applicable to a broad variety of visual recognition tasks in addition to one particular kind of work. Depending on the varying capacity needs, one may choose from the model's numerous variants, ranging from B0 to B7. In order to improve speed, the model also makes use of methods like Drop Connect and cross-layer connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5</head><p>In our testing environment, the aforementioned techniques were implemented, and TPU processing units were utilized to optimize their performance and accelerate processing <ref type="bibr" target="#b26">[25]</ref>.</p><p>The line graphs that illustrate our success metrics on the SIIM-ISIC dataset <ref type="bibr" target="#b28">[27]</ref> are as follows. The aforementioned measurements were computed using the EfficientNet B-4 model. The graphs illustrate the outcomes, both with and without the supplementary data augmentation that was previously described.  The absence of any data transformations in the dataset resulted in a decrease in the validation AUC value from 0.83 to 0.70. The graph illustrates the criticality of incorporating real-world variables into deep learning models through data modification.</p><p>Following the data augmentation, experiments were conducted utilizing every version of the EfficientNet model, ranging from B0 to B4. The validation AUC value of the EfficientNet B-4 model was the highest among all the models that were evaluated. The graph illustrates a significant increase in this value from 0.6096 during the initial epoch to 0.8660 during the final one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Interdisciplinary Collaboration in AI-Driven Dermatology</head><p>Thanks to AI integration into dermatology, huge improvements are observed in healthcare. Dermatology is focusing on skin and its diseases. Since naturally dermatology is based on visual changes on skin, this field is rapidly evolving thanks to AI developments, especially image based approaches.However, interdisciplinary collaboration is key for fulfilling success in this field.From different expertises, different people who have different backgrounds are coming together in that sense. These people are technology specialists, dermatologists, doctors and other healthcare professionals. They are brainstorming and collaborating.Each individual is bringing different perspective to the topic. By doing so, at the end this different backgrounds create a holistic approach for innovations. When we are able to integrate AI models and approaches into dermatology, it not only improves diagnostic accuracy but also it will help us to personalize the way doctors interact and deal with patients. This collaboration will help us to solve complex challenges for example early detection of skin cancers like melanoma.</p><p>When it comes to clinical decision-making, high-quality AI-based support outperforms doctors or AI alone in terms of diagnostic accuracy, and it benefits less experienced clinicians the most. AI-driven multiple class probabilities outperformed contentbased image retrieval (CBIR) representations of AI in the context of mobile technology, and AI-based support proved useful in telemedicine triage and second opinion simulations. AI has the potential to mislead all levels of clinicians, even experts, when used by non-experts.</p><p>[28]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Participants of that collaboration</head><p>Technologists are trying to develop AI models and algorithms, which can process various images containing skin conditions. By doing so, they are aiming to increase accuracy and efficiency of the models used in previous times expecially traditional methods. On the other hand, healthcare professionals, dermatologists contributing by sharing their detailed experince in skin disease and also in patient care. Help of dermatologists is very important while technologists are preparing the dataset, because the images and visual contents used must be comprehensive and represantative of various skins conditions. So, they are ensuring AI approach is clinically safe. Other healthcare professionals, nurses, physicians and administrators, they are dealing with practical sides of the system including communication with patients and adapting into this hybrid system. It is crucial to take feedback about AI tool from these professionals, so that technologist can update and change the software accordingly.</p><p>Multiple different disciplines are used together and combined. By using these new approaches, developments in technology and AI created an opportunity. Various fields are affect by developments in AI technology. One of them is dermatology. AI models and algorithms are able to detect more than 2000 skin diseases and conditions.The worldwide scarcity of dermatologists and the pressing medical knowledge gaps in underdeveloped and distant countries make this collaboration all the more crucial. Especially in challenging conditions, these abilities are revolutionising the treatment of skin disorders.In clinical practices, AI systems such as deep learning and other approaches and algorithms like neural networks are widely used. Increasing popularity of smartphones and similar technologies, image recognition and big data are more commonly used technologies these days.</p><p>These systems apply sophisticated techniques such as ensemble Bayesian fully convolutional networks and spatially adaptive re-weighting methods to distinguish between benign and malignant cancers based on large-scale public skin lesion image datasets.</p><p>[29]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Economic Impact of AI in Melanoma Diagnosis</head><p>Various studies shows us that accuracy of AI models are more successful than human healthcare professionals and experts.Highlight key studies demonstrating AI's diagnostic accuracy and speed. When comparing human and ai in that sende using the same dataset, the model's performance is contrasted with that of 157 dermatologists from 12 German university hospitals. With an area under the curve of 94.4%, sensitivity of 85.0%, and specificity of 95.0%, the experimental results demonstrate that our suggested approach outperforms all dermatologists and achieves higher performance than the state-of-the-art approach.</p><p>[30]</p><p>Melanoma requires accurate forecasts at the early stage to ensure effective treatment.</p><p>The integration AI models into this field creates huge improvements not only in methodologies but also it creates new economic dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Cost-Effectiveness</head><p>Most skin cancer deaths are caused by melanoma, one of the most dangerous types of the disease. Dermatologists visually inspect skin lesion photos to begin the disease detection process; if a suspicion arises, additional pathological analyses will be carried out. In up to 95% of cases, cancer is curable if caught early.</p><p>[30]</p><p>For decades as traditional methods, dematoscopic analysis and biopsy are widely used. If at early stages, dignasos of melanoma can be achieved this will reduce cost and effort in terms of advanced treatments for later stages of disease. However, at the beginnig there is initial fixed costs. Implementing and developin AI based system into clinics and hospitals requires initial expenses. This expenses include training costs, technology costs for both hardware and software. But overtime cost savings from early detections and reduced misdiagnoses can offset these intial expenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">AI in Dermatological Education and Training</head><p>In last decade, AI integration to many sectors played a significant role of developments. And dermatology is one of these fields. Integration to AI is impacting clinical practices but also it is impacting current educational strategies and training landscape. This shift from traditional methods to AI enhanced methods will require new educational strategies and curriculams.Traditional methods and materials heavily rely on textbooks and direct patient communications. But now students have AI based tools and simulations as well. By this new tools students will be able to have dynamic and interactive learning experince. By using image recognition technologies students will be more confident while dealing with complex dermatological conditions. So new cirriculums aim to make students capable of using new technologies and tools. This will help fostering a synergy between human and ai.Integration to medical curriculam will be containing introduction modules for AI and tools. For better understanding of skin conditions there will be virtual reality simulations as well.</p><p>For example, AI based image tools can provide various different skin images from database of dermatological cases. It will help students to have broad access to tools and complex cases while learning.In addition to that, healtcare companies can help these educational purposes by providing their own data and experince. Despite multiple advantages of this integration, There are some challenges. There should be education professionals and they should be educated to effectively guide students and teach them. These all development and challenges can create international opportunity to solve these issues and in return humanity as whole will be able to use common and efficient healthcare system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Tutoring Systems</head><p>An intelligent tutoring system featuring a virtual microscope, encoded knowledge base, and server-based WSI application was created by Crowley et al. SlideTutor simulates one-on-one tutoring by providing step-by-step feedback while teaching algorithmic problem-solving in the visual classification of inflammatory dermatoses. In visual classification, the successive algorithmic steps start with morphologic feature identification, move on to making a differential diagnosis, and end with a final diagnosis. By offering a foundation for feature-based diagnostic reasoning, intelligent tutoring systems have the potential to enhance student performance in dermatopathology. <ref type="bibr">[31]</ref> Study presents a comprehensive framework for developing knowledge-based medical education programs that instruct in diagnosic classification problem solving. The method is based on our earlier research outlining the development of proficiency in pathology classification problem solving.</p><p>The structure surrounds the conventional Intelligent Designing a tutoring system inside the Unified Problem-solving Architecture using the Method Description Language (UPML), encouraging the modularity and reuse of components. Considering the domain task ontology, case data, and domain ontology, the The expert model's methods for solving abstract problems Make a graph of the dynamic solution. Interaction between students and the solution graph passes via a layer of instruction, It results from another set of abstract problem-solving approaches and educational ontologies, in reaction to the the student model's current status. We delineate the benefits and drawbacks of this broad strategy, and describe how SlideTutor, a developing tool, has implemented it. An intelligent system for tutoring in dermatology.</p><p>[32]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Public Perception and Acceptance of AI in Healthcare</head><p>According to the research, people's main fears are related to applications and technology of AI. Majority of people have positive opinions. And people are asuming that AI might replace human doctors at some point. Some people have negative opinions related to AI. This negativity might be caused by of trust in technology. <ref type="bibr">[33]</ref> Achieving technical success of ai in healthcare is important and it is a sign of success in this field. But another factor should also be evaluated and that is the acceptance of public.Success of these ai based models and technologies will be determined how patients and public perceive these technologies.There are various factors which are affecting opinions of public. These opinions are related to AI enhanced solutions. We can say that trust,ethical concerns are some examples. We can also add communication between patients and doctors. If public is educated and informed about what are the advantages, benefits of AI and what are the limitations, their concerns will be reduced naturally.</p><p>They should be informed how these ai systems collect data, how they make decisions, these processes should be clearly explained to people. By doing so, the fear of people will be decreasing and it will make people to prevent misconceptions.</p><p>Artificial intelligence is affecting various fields including medical ones. As a result, healthcare systems might be improved by the help of AI. In addition to that, studies have demonstrated its effectiveness in many medical fields. However, there are some problems and challenges while these fields are trying to adapt to AI. For example, patients are not accepting new approaches quickly. Most people are thinking in a positive way about new technologies like AI. But there are still people who are sceptical and they prefer humans instead of machines. Further research is advised to ensure that AI is applied properly, safely, and with a patient centered approach in healthcare settings.</p><p>[34]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Wearable Devices</head><p>With the quick development of sensor and computing platforms, wearable systems have been introduced. Wearable technology has produced new applications in all areas of medicine in recent years. The purpose of this review is to outline the most recent advancements in wearable system-based cancer detection and to pinpoint the main obstacles preventing its widespread clinical adoption. <ref type="bibr">[35]</ref> In terms of patients to be monitored regularly and continuously, wearable technologies are very useful. In last decade wearable devices are becoming popular in various fields and different use cases.</p><p>Specifically in dermatology, using wearable device in melanoma detection, it gives doctors and patients huge benefits. These devices are containing AI models and algorithms in their software and also they contain advanced sensors, by using these software and hardware systems continous skin monitoring is possible. So, this will enable healthcare professionals and doctors to be able to detect melanoma in early stages. The sensors which are placed in wearable devices are coming with their own abilites. For example, these sensors are able to monitor color, texture, and temperature of skin. These parameters are critical ones in detecting melanoma. These sensors are very sensitive and they can even detect changes which human eye cannot.Inside the software of the wearable devices, algorithms are trying to detect any changes and also since these models are trained on big datasets of dermatological images, they are able to detect visual changes.</p><p>While technology inside the wearable device is important, experience of patients and design of the device is also very important. These devices should be designed and created in a way that patients are able to use it comfortably, and also the appearance of device should be appealing to wear. In terms of security and privacy the personal and sensitive data which is collected by these devices should be stored in a safe environment. Encryption and secure protocols should be used while storing and transfering the data of patients. In addition to these factors, patients with limited access to healthcare centers, or people who are living in rural areas, they will be able to share their conditions via internet connection to their doctors and hospitals. These remote access and remote communication between patients and healthcare professional will be very benefical for these people who has limited access.</p><p>However, there is one important thing that technology producers should be aware of. And that is the battery life of these devices. Especially, it is important for patients who are in serious condition and they should be monitored continously. So for these people, battery life and real time data sharing is vital. Also, for people who are living in rural areas, who has limited access to healthcare centers, they also should be able to informed about battery life, and necessary equipments should be given and they should be trained for changing and renewing the battery of the wearable device. These precautions are essential for people who are suffering from skin conditions, by these improvements data will be collected continously. Early detection of melanoma requires advanced technology and proper planning to monitor patients. In addition to that, further support might be necessary for elderly people, because they are having difficulty in adapting new technologies. So, using wearable devices and their regular checks should be completed by the patients themselves or by their young relatives or family members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Conclusion</head><p>In conclusion, delving into the realm of AI's application in melanoma detection brings to light a revolutionary yet evolving role. The progress in deep learning and machine learning has significantly enhanced the accuracy of early detection and diagnosis. Nonetheless, challenges such as dataset imbalances, representation issues, bias and ethical concerns regarding patient data privacy persist as avenues for further enhancement.</p><p>Forthcoming times, the focal point will shift towards augmenting the diversity and inclusivity of datasets, making certain that AI technologies cater to diverse demographics effectively. The ethical application of AI, especially concerning the protection of patient data, will persist as a significant challenge, demanding stringent regulations and transparent procedures.</p><p>Multidisciplinary collaboration, economic viability, educational advances, public trust, and breakthrough technological development like wearable gadgets will determine the future fate of artificial intelligence in dermatology. As an increasing number of medical services are driven by artificial intelligence (AI), its acceptance will depend on an open communication strategy and public information education. Personalized healthcare is also becoming an important player in wearable technology, especially in diagnosing initial melanoma.</p><p>In summary, AI is on its way in melanoma detection.</p><p>Continued research, development and collaboration of AI can indeed revolutionize healthcare by increasing its effectiveness and efficiency while at the same time ensuring equality for all.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of Deep Deterministic Policy Gradients (DDPG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>AlexNet and Morphological Segmentation for Skin Cancer Analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Conversion of JPEG Format Images and CSV FormatMetadata into TFRecord Format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Feature-Level Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>⌉ 6 :</head><label>6</label><figDesc>Equation Matrix of the Rotation Process in XY Dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>] 7 :</head><label>7</label><figDesc>Equation Matrix of the Shear Process in XY Dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>] 8 :</head><label>8</label><figDesc>Equation Matrix of the Zoom Process in XY Dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 ⌉ 9 :</head><label>19</label><figDesc>Equation Matrix of the Shift Process in XY Dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Relationship Between the RGB and HSV Color Systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of the Anatomy Field Based on the Empty Status of the Gender Field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Analysis of Missing Anatomy Data in MelanomaDataset. The left panel shows the distribution of gender for cases with and without missing anatomy data. The right panel compares the age distribution for these two groups, highlighting differences in age profiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Distribution of Melanoma Cases. The left panel shows the frequency of benign and malignant cases. The right panel compares the age distributions for benign and malignant cases, highlighting age-related trends in melanoma diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Gender Distribution. The graph shows the number of men and women with benign and malignant diagnoses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Melanoma Distribution by Anatomy and Diagnosis. The left panel shows case frequencies by anatomical site, and the right panel diagnosis types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Performance Metrics After Data Augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Performance Metrics Before Data Augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Validation AUC Values of EfficientNet Versions B0 to B4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance Metrics of Deep Learning Models on Nailmelanoma Dataset After Data Augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Confusion Matrix.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell><cell>F-1</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell></cell><cell>%</cell><cell>Score</cell><cell>%</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell>%</cell><cell></cell><cell></cell></row><row><cell>VGG19</cell><cell>93</cell><cell>96</cell><cell>95</cell><cell>98</cell></row><row><cell>ResNet1</cell><cell>81</cell><cell>89</cell><cell>89</cell><cell>88</cell></row><row><cell>01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet1</cell><cell>96</cell><cell>98</cell><cell>97</cell><cell>99</cell></row><row><cell>52V2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xceptio</cell><cell>95</cell><cell>95</cell><cell>94</cell><cell>97</cell></row><row><cell>n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inceptio</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nV3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reinforcement learning model for AI-based decision support in skin cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C F</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Akay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Apalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Argenziano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lallas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malvehy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zalaudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-023-02475-5</idno>
		<ptr target="http://dx.doi.org/10.1038/s41591-023-02475-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1941" to="1946" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artificial intelligence and melanoma: A comprehensive review of clinical, dermoscopic, and histologic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Stiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Knackstedt</surname></persName>
		</author>
		<idno type="DOI">10.1111/pcmr.13027</idno>
		<ptr target="http://dx.doi.org/10.1111/pcmr.13027" />
	</analytic>
	<monogr>
		<title level="j">Pigment Cell &amp; Melanoma Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Artificial intelligence-based image classification methods for diagnosis of skin cancer: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knackstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2020.104065</idno>
		<ptr target="http://dx.doi.org/10.1016/j.compbiomed.2020.104065" />
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Artificial intelligence image recognition of melanoma and basal cell carcinoma in racially diverse populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Papay</surname></persName>
		</author>
		<idno type="DOI">10.1080/09546634.2021.1944970</idno>
		<ptr target="http://dx.doi.org/10.1080/09546634.2021.1944970" />
	</analytic>
	<monogr>
		<title level="j">Journal of Dermatological Treatment</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2257" to="2262" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine Learning and Its Application in Skin Cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Cockerell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pietkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Giulini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldust</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijerph182413409</idno>
		<ptr target="http://dx.doi.org/10.3390/ijerph182413409" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Environmental Research and Public Health [online]</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">13409</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Prospective validation of dermoscopy-based open-source artificial intelligence for melanoma diagnosis (PROVE-AI study). npj Digital Medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Kurtansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dauscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haliasos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Marghoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Rotemberg</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-023-00872-1</idno>
		<ptr target="http://dx.doi.org/10.1038/s41746-023-00872-1" />
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recent Advances in Melanoma Diagnosis and Prognosis Using Machine Learning Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mosley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wheless</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11912-023-01407-3</idno>
		<ptr target="http://dx.doi.org/10.1007/s11912-023-01407-3" />
	</analytic>
	<monogr>
		<title level="j">Current Oncology Reports</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="635" to="645" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Malignant melanoma diagnosis applying a machine learning method based on the combination of nonlinear and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salem Ghahfarrokhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khodadadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fattahi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2022.104300</idno>
		<ptr target="http://dx.doi.org/10.1016/j.bspc.2022.104300" />
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection of Melanoma Skin Cancer Using Hybrid Machine Learning Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karforma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science and Culture</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<date type="published" when="2023-03" />
		</imprint>
	</monogr>
	<note>Available from</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.36094/sc.v89.2023.detection_of_melanoma_skin_cancer.kundu.70</idno>
		<ptr target="http://dx.doi.org/10.36094/sc.v89.2023.detection_of_melanoma_skin_cancer.kundu.70" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning in computational dermatopathology of melanoma: A technical systematic literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lodde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schadendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kukuk</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2023.107083</idno>
		<ptr target="http://dx.doi.org/10.1016/j.compbiomed.2023.107083" />
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transfer learning-based quantized deep learning models for nail melanoma classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Siyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Dharejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-023-08925-y</idno>
		<ptr target="http://dx.doi.org/10.1007/s00521-023-08925-y" />
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22163" to="22178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning in skin lesion analysis for malignant melanoma cancer identification. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gurumekala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Priyadharshini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s11042-023-16273-1</idno>
		<ptr target="http://dx.doi.org/10.1007/s11042-023-16273-1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Survey on Recent Trends in Medical Image Classification Using Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Solatidehkordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zualkernan</surname></persName>
		</author>
		<idno type="DOI">10.3390/app122312094</idno>
		<ptr target="http://dx.doi.org/10.3390/app122312094" />
	</analytic>
	<monogr>
		<title level="m">Applied Sciences</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">12094</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing Bias in Skin Lesion Classifiers With Contemporary Deep Learning and Post-Hoc Explainability Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2023.3289320</idno>
		<ptr target="http://dx.doi.org/10.1109/access.2023.3289320" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="78339" to="78352" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Melanoma classification from dermatoscopy images using knowledge distillation for highly imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Adepu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sahayam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arramraju</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2023.106571</idno>
		<ptr target="http://dx.doi.org/10.1016/j.compbiomed.2023.106571" />
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the Performance of CNN and GAN models for Melanoma Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
		<idno type="DOI">10.1109/aisp53593.2022.9760626</idno>
		<ptr target="http://dx.doi.org/10.1109/aisp53593.2022.9760626" />
	</analytic>
	<monogr>
		<title level="m">2022 2nd International Conference on Artificial Intelligence and Signal Processing</title>
		<imprint>
			<publisher>AISP</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Detection and segmentation of melanoma skin cancer in dermoscopy images using modified Alexnet convolutional neural network-morphological methodology. Concurrency and Computation: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Govindaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mallappa</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpe.7266</idno>
		<ptr target="http://dx.doi.org/10.1002/cpe.7266" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Reinforcement Learning Algorithm for Automated Detection of Skin Lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">A</forename><surname>Usmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jaafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.3390/app11209367</idno>
		<ptr target="http://dx.doi.org/10.3390/app11209367" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Applied Sciences [online</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tensorflow Records? What they are and how to use them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gamauf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Review of Deep Transfer Learning and Recent Advancements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Arabnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasheed</surname></persName>
		</author>
		<idno type="DOI">10.3390/technologies11020040</idno>
		<ptr target="http://dx.doi.org/10.3390/technologies11020040" />
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technologies</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automated medical diagnosis of COVID-19 through EfficientNet convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>De La Torre Díez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2020.106691</idno>
		<ptr target="http://dx.doi.org/10.1016/j.asoc.2020.106691" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lung-EffNet: Lung cancer classification using EfficientNet from CT-scan images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Iftikhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alam</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.engappai.2023.106902</idno>
		<ptr target="http://dx.doi.org/10.1016/j.engappai.2023.106902" />
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A modified 3D EfficientNet for the classification of Alzheimer&apos;s disease using structural magnetic resonance images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1049/ipr2.12618</idno>
		<ptr target="http://dx.doi.org/10.1049/ipr2.12618" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
	<note>IET Image Processing [online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Detection algorithm for pigmented skin disease based on classifier-level and feature-level fusion. Frontiers in Public Health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpubh.2022.1034772</idno>
		<ptr target="http://dx.doi.org/10.3389/fpubh.2022.1034772" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance Evaluation of Novel Convolution Neural Network Architecture for Melanoma Skin Cancer Diagnosis on Different Hardware Processing Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Umar</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/1950/1/012039</idno>
		<ptr target="http://dx.doi.org/10.1088/1742-6596/1950/1/012039" />
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12039</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ScienceDirect Topics in Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anon</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/topics/computer-science" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A patient-centric dataset of images and metadata for identifying melanomas using clinical context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kurtansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Betz-Stablein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caffery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chousakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guitera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lioprys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malvehy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musthaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stratigos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note>Sci Data</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/s41597-021-00815-z</idno>
		<ptr target="https://doi.org/10.1038/s41597-021-00815-z" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-computer collaboration for skin cancer recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.1038/s41591-020-0942-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1229" to="1234" />
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Artificial Intelligence in Dermatology Image Analysis: Current Developments and Future Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Koban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Schenck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Giunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3390/jcm11226826</idno>
	</analytic>
	<monogr>
		<title level="j">J Clin Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">6826</biblScope>
			<date type="published" when="2022-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AI outperformed every dermatologist in dermoscopic melanoma diagnosis, using an optimized deep-CNN architecture with custom mini-batch logic and loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-96707-8</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17485</biblScope>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Artificial intelligence in dermatopathology: Diagnosis, education, and research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Motaparthi</surname></persName>
		</author>
		<idno type="DOI">10.1111/cup.13954</idno>
	</analytic>
	<monogr>
		<title level="j">J Cutan Pathol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1061" to="1068" />
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A general architecture for intelligent tutoring of diagnostic classification problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Medvedeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Annu Symp Proc</title>
		<imprint>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Public Perception of Artificial Intelligence in Medical Care: Content Analysis of Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<idno type="DOI">10.2196/16649</idno>
	</analytic>
	<monogr>
		<title level="j">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">16649</biblScope>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patient and general public attitudes towards clinical artificial intelligence: a mixed methods systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1016/S2589-7500(21)00132-1</idno>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="599" to="611" />
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Systematic Review of Wearable Systems for Cancer Detection: Current State and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10916-017-0828-y</idno>
	</analytic>
	<monogr>
		<title level="j">J Med Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
