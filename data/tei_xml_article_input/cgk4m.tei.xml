<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What kind of &apos;Explainable AI&apos; do users need?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Buccella</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">University at Albany SUNY</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Linstead</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fowler School of Engineering</orgName>
								<orgName type="institution">Chapman University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Maoz</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Interdisciplinary Brain and Behavioral Sciences</orgName>
								<orgName type="institution">Chapman University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What kind of &apos;Explainable AI&apos; do users need?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Intelligence</term>
					<term>Explainable AI</term>
					<term>Reasons</term>
					<term>Cooperation</term>
					<term>Decision-making</term>
					<term>Users</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Much attention is currently given to generative artificial intelligence (AI), but most currently available AI technologies consist in so-called decision support systems (DSSs). DSSs are meant to improve the efficiency of human decision-making by providing (ideally) relevant information about viable alternatives in domains such as medical diagnostics, insurance, finance, or the criminal justice system. However, AI technologies can only bring about their benefits if a critical mass of people are interested in using them. In this work, we argue that, to fulfill their &apos;mission&apos; of helping humans make better decisions, DSSs must have the ability to provide explanations for their outputs that play the role of reasons when communicating these outputs to human users. After we clarify what we mean by &quot;reasons&quot; and motivate our claim that providing reasons for recommendations is important in the context of productive collaboration between humans and DSSs, we will discuss two possible objections to our proposal. In replying to the objections, we will further defend our suggestion that a DSS capable of providing reasons for its outputs has the potential to increase users&apos; willingness to utilize the system as a decision-making aid.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, the use of artificial intelligence (AI) systems has proliferated. AI now routinely assists humans in tasks that require fast and efficient analysis of 'big data' toward prediction of future outcomes. Machine-learning algorithms that crunch vast amounts of data promise to improve our quality of life by quickly 'learning' people's preferences and habitual decision patterns-though, unfortunately, also their biases (e.g. <ref type="bibr" target="#b16">Eubanks, 2018)</ref>. Although much attention is currently given to large language models and generative AI (e.g. ChatGPT, Claude, Gemini, Llama), the vast majority of currently available AI technologies consists in so-called decision support systems (DSSs), meant to improve the efficiency of human decision-making by providing them with (ideally) relevant information about viable alternatives in varied domainsfrom medical diagnostics, insurance, and finance to the criminal justice system. Crunching such vast amounts of data would have otherwise taken a human a long time-perhaps even many lifetimes. Here we focus on these DSSs.</p><p>However, as was made abundantly clear during the Covid-19 pandemic, when many rejected the protective and sometimes life-saving vaccines that were developed at record speed and high cost via novel technology, even the most efficient and useful technology will only help humans who are willing to engage with it. Similarly, in the case of AI, these new and powerful technologies can only bring about their benefits if a critical mass of people are interested in using them. We argue that, to fulfill their 'mission' of helping humans make better decisions (and thus increasing human productivity) in various contexts, including high-stakes ones, DSSs must have the ability to provide explanations for their outputs that play the role of reasons when communicating these outputs to human users 4 . After we clarify what we mean by "reasons" and motivate our claim that providing reasons for recommendations is important in the context of productive collaboration between humans and DSSs, we will discuss two possible objections to our proposal. In replying to the objections, we will further defend our suggestion that a DSS capable of providing reasons for its outputs has the potential to increase users' willingness to utilize the system as a decision-making aid. We leave a systematic empirical investigation of our suggestion to future work. Instead, we set ourselves a more modest goal here: to make the case for work in explainable AI (XAI) and human-computer interaction to incorporate more explicitly the idea of reasons (alongside other kinds of explainability-enhancing strategies) and for XAI researchers to take on the challenge of developing strategies to provide their models with reasongiving capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Explainable AI and the challenge of intelligibility for users</head><p>As DSSs become more powerful, a key obstacle that remains for their integration into human decision-making is that state-of-the-art models employ deep neural networks, which are notoriously 'black boxes' <ref type="bibr" target="#b21">(Guidotti et al., 2018b;</ref><ref type="bibr" target="#b30">Liang et al., 2022;</ref><ref type="bibr" target="#b61">von Eschenbach, 2021;</ref><ref type="bibr" target="#b62">Weld and Bansal, 2019)</ref>. In other words, such networks are highly interconnected and complex, containing up to billions of parameters to train. Therefore, given a trained and working DSS, it is typically very difficult, if not impossible, for humans to completely understand how the system 'decided' to issue a specific output given a certain input by tracking the many layers and myriad connections leading from the input to the output. Though, importantly, such understanding is critical when the purpose is to utilize the machine to make good, evidence-based decisions.</p><p>By way of an example, imagine a medical DSS trained to analyze patients' charts and predict whether a patient is a good candidate for a certain surgery. A typical DSS would likely output a label (e.g., "surgery recommended" or "surgery not recommended") and a degree of confidence that the label is correct (e.g., 72%). Now let us further imagine a surgeon that independently examines the chart and concludes that the patient is not a good candidate for that surgery. This surgeon then goes on to consult with the DSS (say, as a second opinion), only to get the output "surgery recommended" with 72% confidence and no additional explanation.</p><p>Without additional information-such as the specific features of the patient that drove the decision (e.g. age, sex, medical history, etc.) and how those features relate specifically to the surgery being considered-the surgeon may well be disinclined to seriously consider the DSS's output as a legitimate 'opinion' worth considering. So, perhaps after another look at the chart, they may decide that the DSS's recommendation is wrong and be less inclined to consult it in the future. The situation might become even worse if a patient were exposed to the conflicting diagnoses of their surgeon ("surgery not recommended") and the DSS ("surgery recommended", 72% confidence). The patient is then likely to lower their trust in both diagnoses, resulting in negative repercussions for the perceived competence of the surgeon.</p><p>A well-built and well-trained DSS is likely to have been trained or huge amounts of relevant data and to have relied on the patient's demographics, medical history, and much more for its diagnosis and confidence score. But if the AI cannot relay how that information and that knowledge has led it to its diagnosis in any particular case, the user's willingness to trust the DSS's diagnosis and use it again may greatly diminish.</p><p>To make up for this lack of intelligibility, researchers have tried to fashion DSS with features to make the machine's outputs and internal processes more explainable to humans. XAI can take many forms, especially since the very meaning of the term seems to vary across research fields, depending on the type of AI architecture and overall explanatory goals <ref type="bibr" target="#b1">(Adadi and Berrada, 2018;</ref><ref type="bibr" target="#b14">Doran et al., 2018;</ref><ref type="bibr" target="#b22">Gunning and Aha, 2019;</ref><ref type="bibr" target="#b26">Kasirzadeh, 2021;</ref><ref type="bibr" target="#b29">Langer et al., 2021;</ref><ref type="bibr" target="#b60">Vilone and Longo, 2021)</ref>. To some, explainability in AI coincides with interpretability (sometimes also known as 'transparency'), that is, a way for people who are familiar with the technical details of the model to figure out how an output was derived from an input. This type of explainability is importantly different from a less detail-oriented and more user-centered one, which might be termed 'comprehensibility' <ref type="bibr" target="#b14">(Doran et al., 2018)</ref>. A comprehensible system issues its output together with additional information (keywords, visualizations, etc.) which, when properly interpreted by a human, reveal how a certain input was transformed into a certain output. <ref type="bibr">5</ref> Although the latter kind of explainability does not require a lot of technical knowledge about the inner workings of the model, in current systems it still requires a human interpreter-in particular, their reasoning abilities and background knowledge-for the actual explanation to be explicitly articulated. The additional information issued by such an explainable machine might thus be likened to a cartographic map: it contains an abundance of details, but humans still must know how to read the map and use it to navigate the world for it to fulfill its purpose.</p><p>If the purpose is for end-users-especially those with no technical expertise in machine learning-to obtain intelligible, valuable, and pertinent information to support their decisionmaking processes, especially in potentially high-stakes situations (e.g., doctors in medical diagnostics), are interpretability and comprehensibility as defined above enough? Do interpretability and comprehensibility sufficiently encourage non-expert users to integrate machine recommendations into their deliberations? For both questions, we think that the answer is 'No'. Interpretability and comprehensibility as described above are both passive notions, that is, they focus on providing the user with additional information about the machine under the assumption that the user already knows how to use that information to make decisions. In other words, interpretability and comprehensibility are not designed to capture the essentially interactive nature of human decision-making and tool use (see below)-both key components of the phenomenon we are interested in, i.e., decision-making informed by artificially intelligent tools.</p><p>Humans make the majority of their decisions either directly or indirectly for the sake of coordinating their behavior with other humans, achieving common goals, planning joint actions, etc. The social context greatly influences, both ontogenetically and phylogenetically, preferences and values, which in turn are what guides and motivates decisions. Additionally, humans learn to use tools largely in social contexts and through cooperation <ref type="bibr" target="#b55">(Sterelny, 2012)</ref>. If DSSs are to become tools capable of enhancing the quality of human decision-making writ large, they must be designed to 'fit' with what we already know about how and why humans make decisions as smoothly as possible. Therefore, explainable decision support systems should be designed with particular attention to the social and interactive aspects of human decision-making and, relatedly, to how humans explain their actions to other humans <ref type="bibr" target="#b12">(De Graaf and Malle, 2017;</ref><ref type="bibr" target="#b34">Lombrozo, 2016;</ref><ref type="bibr" target="#b35">Lombrozo and Carey, 2006;</ref><ref type="bibr" target="#b38">Miller, 2019)</ref>. But what are these aspects, exactly?</p><p>Empirical evidence seems to suggest that users prefer interactive AI systems, which can directly answer questions and elaborate on their suggestions on-demand over more passive systems that simply lay out information <ref type="bibr" target="#b28">(Kunkel et al., 2019;</ref><ref type="bibr" target="#b31">Lim et al., 2009a)</ref>. Moreover, several studies in human-computer interaction and related fields suggest that humans tend to adopt cognitive attitudes similar to those underlying interactions with other humans or animals when interacting with advanced machines <ref type="bibr" target="#b0">(Abercrombie et al., 2023;</ref><ref type="bibr" target="#b7">Chaves and Gerosa, 2021;</ref><ref type="bibr" target="#b42">Nass and Moon, 2000;</ref><ref type="bibr" target="#b45">Rapp et al., 2021;</ref><ref type="bibr" target="#b50">Rosenthal-von der Pütten et al., 2013)</ref>. It has also been argued that the 'perceived humanness' of an artificial agent positively correlates with measures of user satisfaction, trust, engagement, etc. <ref type="bibr" target="#b9">(Christoforakos et al., 2021;</ref><ref type="bibr" target="#b19">Go and Sundar, 2019;</ref><ref type="bibr" target="#b36">Lu et al., 2022;</ref><ref type="bibr" target="#b45">Rapp et al., 2021;</ref><ref type="bibr" target="#b47">Rheu et al., 2021;</ref><ref type="bibr" target="#b57">Toader et al., 2020)</ref>. Thus, for example, increasing the interactivity of explanations to better resemble human-to-human interaction, at least to an extent, could be beneficial to users' willingness to incorporate AI in their decision-making</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>processes. 6</head><p>As a second example, consider a DSS designed to give personalized insulin intake recommendations to patients with type-1 diabetes <ref type="bibr" target="#b6">(Buch et al., 2018)</ref>. Van der Waa and colleagues <ref type="bibr" target="#b59">(Van Der Waa et al., 2021)</ref> found that such a system's ability to articulate the 'rule' it used to come up with particular predictions in an easily understandable way (even at the expense of precision with respect to technical details) had a positive effect on users' self-reported understanding of the system and significantly improved users' ability to identify which factors <ref type="bibr">6</ref> In this respect, for instance, it is not surprising that AI systems trained on human-produced text or speech which can directly answer questions and can have prolonged 'conversations' with users (e.g. OpenAI's ChatGPT or Google's Gemini) have become one of the most popular ways to conduct internet searches (which these days are the starting point for a lot of decisions). Humans value the possibility to refine their queries, ask for more detail, and, more generally, 'personalize' their information-gathering practices so that decisions can be made as quickly and seamlessly as possible.</p><p>most influenced a specific recommendation. These results are consistent with what studies on earlier-generation AI recommenders found <ref type="bibr" target="#b56">(Stumpf et al., 2009)</ref>. Namely, that 'rule-based' explanations positively affect users' willingness to contribute feedback to improve the performance of the system, suggesting that 'rule-based' explanations are considered conducive to productive human-machine collaboration. Moreover, users seeking to make real-life decisions with the help of AI models might not have much use for the exact, technical details regarding how the model transforms an input into an output. In other words, the specific values of the 'weights' (the connections between the nodes of the neural network, which are the typical parameters on which the learning of modern AI models depends) would be of little use for the average surgeon when deciding whether to agree with a system that just told them their patient should undergo surgery with 72% confidence. Indeed, such fine-grained details are usually quite removed from the user's goals and desiderata, which mostly focus on practical usefulness, on maintaining autonomy and control over the machine, and on understanding how the machine's overall objective (i.e. maximizing prediction accuracy) relates to the specifics of the context of application <ref type="bibr" target="#b23">(Herlocker et al., 2000;</ref><ref type="bibr" target="#b27">Kulesza et al., 2013;</ref><ref type="bibr" target="#b29">Langer et al., 2021;</ref><ref type="bibr" target="#b32">Lim et al., 2009b)</ref>.</p><p>So: assuming that the goal of DSSs like our fictional 'surgery predictor' or the diabetes monitoring system is to aid, rather than completely replace, human decision-makers, what is it about certain explanation 'styles', and in particular rule-based explanations, that makes them suitable to support and encourage the use of DSSs? Our answer draws on insights from philosophy of mind and the philosophy of action: explainable DSSs should be able to provide users with reasons for their recommendations, since reason-giving is a central aspect of how humans develop an understanding of a decision problem, which is in turn a necessary step towards finding a solution, that is, making the decision. We also argue that rule-based explanations are particularly suitable to be interpreted as reasons by users, thus supporting efforts in XAI that focus on this <ref type="bibr" target="#b20">(Guidotti et al., 2018a;</ref><ref type="bibr" target="#b51">Rudin and Shaposhnik, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Cooperation, rules, and reasons</head><p>In order to integrate the machine's recommendations into our decision-making processes, that is, to appropriately cooperate with the machine during deliberation, we must understand (i) why the machine issued that specific recommendation in a particular instance, e.g., why it issues the label "surgery recommended" with 72% confidence, (ii) what the machine is generally designed to do (i.e., its 'objective function'), and (iii) what it is 'trying' to do in the current context of application (e.g. predict whether a patient is a good candidate for a certain surgery based on their current and past relevant medical conditions). Only with this information in place can a user-a surgeon and even more so a patient-properly consider the machine's output as a possible decision alternative and determine whether they agree with it or not. In other words, an AI system that can properly serve as a decision-making aid for humans must display what Zhou and Danks <ref type="bibr">(Zhou and Danks, 2020)</ref> call "function-based intelligibility". They argue that, despite its inability to "answer all possible why-questions, particularly those that focus on the internal operations of the system," this type of intelligibility is the most useful to users in the context of cooperative tasks (like the type in which AI can help with making decisions), as it "provides <ref type="bibr">[users]</ref> with exactly the abilities that they require to efficiently integrate, appropriately use, and accurately interpret the performance of the autonomous system so that they can reach their goals" (p. 197).</p><p>And explanations that afford function-based intelligibility do so in virtue of playing the role of reasons, or so we argue. First, notice that the 'reasoning' behind specific outputs, which is what function-based intelligibility is all about, can be also understood as the set of 'rules' or 'principles' the machine used to come up with that specific recommendation (think about how a mathematical proof consists of several inferential steps based on the application of mathematical 'rules', i.e. axioms and theorems). So, an explanation that affords function-based intelligibility is an explanation that redescribes a certain input-output relation as the instantiation of a (likely simplified and idealized) rule. 7</p><p>Philosophers like <ref type="bibr" target="#b63">Wittgenstein (1953)</ref> and <ref type="bibr" target="#b53">Sellars (1954)</ref> have argued that describing a behavior as the instantiation of a rule is a way of describing the behavior as if it were an intentional action, and to represent it as an instance of a more general and reproducible action-type. By way of a simplified example, one of the rules of basketball is that players must dribble the ball whenever they want to move around with it. Through the rule, a new action-type is codifiedi.e., the action-type "moving around dribbling the ball"-together with a series of normative criteria to assess whether a particular action is a token of that type and thus a concrete instantiation of the rule. Now notice that, in the appropriate context, mentioning the rule an action instantiates can be considered an acceptable explanation for the action, that is, an appropriate answer to the question "Why?". To illustrate, let's keep using the same basketball example (while however keeping in mind that it is a highly artificial one, which differs from real life in important ways).</p><p>If during a game of basketball someone were to ask you "why are you moving around dribbling the ball?", "because one of the rules of basketball is that players must dribble the ball whenever they want to move around with it" would be a legitimate answer, serving at the same time as an articulation of the rule and an explanation of your action. Sellars distinguishes between merely 7 State-of-the-art AI systems are now typically based on artificial neural networks and therefore do not have rules explicitly programmed into them. However, it is still the case that, during training, a neural net 'comes up' with some kind of systematic ways of representing information-learning rules from the training data. Because these representations are distributed all the way throughout the network, they are often too complex and convoluted to be clearly interpreted by humans as a small set of rules. Nevertheless, they are, for all intents and purposes, rules.</p><p>conforming to a rule and following a rule <ref type="bibr" target="#b53">(Sellars, 1954)</ref>, and argues that the latter requires an agent to have the explicit intention of following the rule. If one describes one's action in terms of the rule that codifies the corresponding action-type, this in itself is proof that one either performed that action intentionally, or is at least capable of performing that action intentionally, whether or not one did so in that particular case.</p><p>If we now apply this argument to the case of AI systems and the explanations of their recommendations, we can say that an explanation that describes what the machine is doing in intentional terms is basically articulating the (machine equivalent of the) intention with which the machine issues its outputs. Another philosopher, G. E. M. Anscombe <ref type="bibr" target="#b2">(Anscombe, 2000)</ref>, argued that intentions are special interpretations/descriptions of actions which capture the sense in which an action is done for a reason. As a consequence, to reveal the intention with which an action is done is equivalent to articulating the reason why the action is done. 8 By describing the machine's behavior in a particular case, i.e., a particular input-output relation, as the instantiation of a rule, therefore, an explanation of why the behavior occurred that can be interpreted by users as if the behavior were intentional, thus allowing the explanation to work as a reason. 9</p><p>We say "can work as reasons" instead of "are reasons" because, first, we do not want to commit to the claim that machines genuinely have intentions just yet, though we leave open the possibility that future empirical and philosophical investigations into AI behavior will establish that machines can (and perhaps do) have intentions. For our present purposes, it is enough that humans act as if machines have intentions for the purpose of explaining the machine's behavior in a way that allow us to rely on familiar 'mental' concepts <ref type="bibr" target="#b13">(Dennett, 1987)</ref>. Second, the very notion of a reason denotes a functional role (more precisely, several different functional roles 10 ) rather than a monadic property, that is, explanations do not possess, strictly speaking, the property of being reasons, even when issued by humans. Rather, explanations with certain characteristics may or may not be suitable to fulfil the functional role of a reason; a role which, in turn, exists only within the specific human social practice of giving and asking for reasons <ref type="bibr" target="#b4">(Brandom, 2000</ref><ref type="bibr" target="#b5">(Brandom, , 1995</ref>.</p><p>In the next section, we discuss two potential objections to the idea that AI systems should have the ability to issue explanations that can play the role of reasons. Replying to these objections will give us the opportunity to elaborate on why exactly we think that having AI explanations play the role of reasons is the best way to encourage the use of AI systems as decision-making aids by people with no technical machine learning expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Objections and Replies</head><p>The first objection we consider states that since AI systems don't really have intentions and don't really act for reasons, making them issue explanations that play the role of reasons might lead to misplaced trust and might thus mislead users, especially those with little technical knowledge about AI, regarding the actual cognitive abilities of these systems. The risk is that users will anthropomorphize the AI system and will thus be 'morally confused' about how to properly interact with it (e.g. <ref type="bibr" target="#b52">Schwitzgebel, 2023;</ref><ref type="bibr" target="#b58">Turkle, 2024)</ref>. And if so, our proposal would be in explicit tension with one of our main motivations for writing this paper -namely, to find ways to convince more and more people with no machine-learning expertise to use AI systems.</p><p>These broadly ethical worries are legitimate, and other work by one of the authors of this paper explicitly considers them (citation deleted). However, it should be noted that very often humans don't have a way to check whether the reasons provided by another human as explanations of why they did something are correct, or whether the action really was done for those reasons, or whether it was done for some reason at all. Nevertheless, it is a well-established idea in organizational psychology that the mere availability of a reason-like explanation for a behavior has a positive effect on at least initial willingness to engage in some cooperative task (e.g. <ref type="bibr" target="#b15">Edgar et al., 2021;</ref><ref type="bibr" target="#b25">Jonker et al., 2010;</ref><ref type="bibr" target="#b37">Mathieu et al., 2000)</ref>. And a willingness to engage is the necessary first step: without it, any discussion about trust, whether misplaced or not, would be highly premature.</p><p>Moreover, we are not discussing a context in which a human completely delegates and gives up control over a decision. And because the cases we are interested in here are not cases in which the human lets the machine decide on its own, but rather the human uses the machine as an 'advisor', a 'second opinion' that ensures a more rigorous deliberation process, excessive anthropomorphization does not seem as worrisome or ethically problematic. A more balanced attitude towards the anthropomorphization and consequent misplaced trust worry, perhaps, would be to see it as a bullet that must be bitten in order to make machines less alien or intimidating to non-experts. Indeed, anthropomorphization allows users to rely on familiar cognitive attitudes and communicative patterns -though we should be aware of the possibility of an 'uncanny valley effect' <ref type="bibr" target="#b8">(Cheetham et al., 2015;</ref><ref type="bibr" target="#b10">Ciechanowski et al., 2019;</ref><ref type="bibr" target="#b41">Mori et al., 2012)</ref>, especially with systems equipped with a conversational interface powered by LLMs.</p><p>Findings from an online survey on trust and explainability in machine learning (N=100 participants) run by our group on the Prolific platform also suggest that non-expert users see the ability to provide reasons as conducive to trust regardless of whether the reasons provided by the machine indeed accurately track the low-level algorithmic details. In the survey, people with little to no technical expertise were asked whether they agreed with the claim 'In order for me to trust an AI, that AI must be able to explain why it makes decisions'. On a Likert scale from 1 (Strongly disagree) to 5 (Strongly agree), the average answer was 3.72, which is significantly higher than the 'neutral' answer (3 on a scale of 1 to 5). 11 Similarly, participants seemed to reliably agree with the claim 'If I do not know that an AI's internal algorithm is working properly, but that AI is able to explain why it makes decisions, I trust that AI' (average response on the Likert scale was 3.25). 12 Collectively, these two sample-questions from our survey suggest that being able to have at least some understanding of why the machine issues an output is generally perceived as having a positive effect on trust, thus potentially counteracting the negative effects of excessive anthropomorphization. Lastly, since the goal is for DSSs to assist humans in their decisions instead of making decisions autonomously, thus maintaining a 'human in the loop' at all times, reason-giving capabilities seem to be a good way to ensure that the human in question maintains accountability for a decision made with the assistance of a DSS by ensuring that the human understands why a certain output was issued <ref type="bibr" target="#b3">(Baum et al., 2022)</ref>.</p><p>The second objection we consider states that requiring explanations at the level of reasons prevents us from taking full advantage of AI tools' power, because it forces the AI to communicate with us at a higher level of abstraction, thus possibly lowering its accuracy in favor of more intelligibility for humans 13 . For instance, it is easy to imagine a system being 'forced' to take into account fewer variables so as to make the post-hoc explanation easier to understand, or altogether producing 'made up' explanations that do not accurately reflect the system's functioning but still 'sound plausible'. These situations are not ideal, as they would clearly diminish the quality of the recommendations. The latter situation in particular is representative of a more general and widely discussed problem vexing many AI systems (and especially generative AI systems), known as the problem of 'machine hallucinations', which is often mentioned as a core ethical issue that should discourage users from relying on AI systems to make important decisions, since these systems may have been trained on false or misleading information that then leads them to issue erroneous outputs (e.g. <ref type="bibr" target="#b17">Farquhar et al., 2024;</ref><ref type="bibr" target="#b18">Ghassemi and Nsoesie, 2022;</ref><ref type="bibr" target="#b44">Noble, 2018;</ref><ref type="bibr" target="#b46">Rawte et al., 2023)</ref>. Therefore, the objection goes, requiring AI systems to have reason-giving capabilities cannot possibly be the best way to encourage nonexperts to utilize these systems as decision-making aids.</p><p>This objection echoes what some critics have been saying about the role of even lowerlevel forms of AI explainability, such as interpretability/transparency, not just reason-giving <ref type="bibr" target="#b24">(Humphreys, 2004;</ref><ref type="bibr" target="#b33">Lipton, 2018;</ref><ref type="bibr" target="#b49">Robbins, 2019)</ref>. These critics emphasize an apparent tension between two ideas. On the one hand is the idea, generally accepted even by the public, that AI systems do some things better and faster than humans. A chatbot like ChatGPT, for example, can generate a better-than-average college essay or write an OK-sounding reference letter starting from a random CV in a few seconds. Even less advanced systems, such as the DSS we are concerned with here, can still analyze a large dataset in a fraction of the time needed by a human while taking into account many more variables. It is quite easy to see what makes these technologies attractive in contexts in which time and fine-grained classifications are critical (e.g., medical diagnosis).</p><p>On the other hand, there is the idea, popular among computer ethicists and philosophers of technology, that AI systems should explain themselves to all 'stakeholders', not just expert engineers <ref type="bibr" target="#b26">(Kasirzadeh, 2021;</ref><ref type="bibr" target="#b29">Langer et al., 2021;</ref><ref type="bibr">Zhou and Danks, 2020)</ref>. Thus, many ask: How can we expect AI models to perform 'super-humanly' and provide us with novel insights, if they are forced to 'dumb things down' or even just translate things in terms and concepts familiar to humans all the time? If the purpose is to utilize AI for faster and better data analysis and prediction compared to what humans are capable of, wouldn't enhancing its non-expert human intelligibility essentially defeat this very purpose? Shouldn't we focus on creating AI tools that are so accurate and reliable that the need for explanations becomes less pressing?</p><p>We reply by pointing out that, though AI systems may well fall short of agency and may well not really have the mental states we usually associate with agency (see our reply to the first objection). their complexity makes them importantly different from other technologies to which the narrow notion of reliability and the (allegedly) consequent lack of need for explainability most clearly apply. AI systems, in other words, are not like humans (or many other non-human animals), but they are not like, e.g., toasters, either. If they were, the question of how to have humans most productively collaborate with AI systems in high-stakes decision-making contexts would not even arise. You probably do not collaborate with your toaster; at most, you consider it a reliable piece of equipment. And indeed you would probably not ask the toaster 'why?' if it malfunctions, neither would you expect it to answer you by disclosing its intentions and the adverse circumstances that prevented it from functioning correctly (e.g., you would not expect the toaster to explain that it really wanted to toast your bread, but that a short circuit in its heating element made toasting impossible). A toaster either works or it doesn't, it either toasts bread properly or it doesn't, and a failure to toast bread properly is a failure to be useful as the tool that it is. And since there is nothing else to it, that is, there is no other sense in which a toaster can be useful even in a case in which it malfunctions, reliability is enough, and explanations are in this context clearly superfluous.</p><p>But today's AI systems are not like toasters. Some of them (e.g., the experimental.</p><p>AutoGPT and likely more so its future instantiations) can plan their own course of action based on the user's requests and execute it largely autonomously by applying the 'knowledge' they acquired during training; and it seems likely that systems like that will become the norm soon enough. Moreover, even in cases in which the AI system's recommendation is considered wrong by the user, there is still something to be gained by investigating why it was wrong. Sometimes, by probing the system further through a request for explanation, users can discover that in fact the system was not wrong, thus convincing the user to take on the recommendation (cf. <ref type="bibr" target="#b3">Baum et al., 2022)</ref>. What it means for an AI system to 'malfunction' has additional layers to it which do not apply to toasters 14 , and that is why the ability to explain why an output was issued is a necessary component of what makes AI systems the tools that they are, and a necessary condition for their usefulness as tools, in science and beyond <ref type="bibr" target="#b11">(Creel, 2020)</ref>. Combining this with what we said previously about the kinds of explanations that are more likely to play a beneficial role in enhancing cooperation given how humans act and deliberate, we reach our desired conclusion: AI systems meant to be used as decision-making aids should be able to provide explanations that can play the role of reasons for their users.</p><p>14 One way to think about this claim is by analogy with the discussion around intentionality in the philosophy of mind, and in particular around 'naturalistic' theories of intentionality such as teleosemantics (e.g. <ref type="bibr" target="#b39">Millikan, 2004</ref><ref type="bibr" target="#b40">Millikan, , 1984</ref><ref type="bibr" target="#b43">Neander, 2007)</ref>. Although we can describe an intentional state with false or inaccurate content as a case of 'malfunctioning', it is still a contentful state which plays a cognitive role. Being sometimes inaccurate is part of the function of intentional states, while in non-intentional systems to perform a function inaccurately just is to fail to perform the function altogether. The functional complexity of AI systems makes it possible for us to take the 'intentional stance' <ref type="bibr" target="#b13">(Dennett, 1987)</ref> on them, and, as others have argued, that is a key aspect of what makes them useful decision-making aids for humans <ref type="bibr" target="#b64">(Zerilli et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we argued that current XAI methods that aim at interpretability and comprehensibility alone are not sufficient for humans with no technical machine learning expertise to effectively use DSSs as decision-making aids. Explaining the technical features of the algorithm or creating map-like visualizations of the features that most prominently influenced a specific output are worth the effort, but an additional 'layer' of explainability is necessary if we want AI-supported decision-making to become widespread among non-experts.</p><p>Indeed, a non-expert user collaborating with a DSS to solve a decision-problem (e.g. a doctor who wants to 'consult' with a DSS for medical diagnosis) needs intelligible access to information about what the system is 'doing' in the particular case the user is interested in, that is, information about why the system issued the recommendation it did. To give users access to such information, we argued that AI systems should be able to provide explanations that function as reasons, that is, explanations that redescribe the system's input-output relation as an instance of an actiontype specified by a rule, which in turn is a way of understanding the system's behavior at least as if it were intentional.</p><p>Of course, treating something which is not an agent as if it were an intentional agent, who can act for reasons, can be dangerous and ethically problematic. However, we argued that the mere ability to rephrase explanations so that they can function as reasons is still compatible with users being very aware that the AI system is only a tool. While in some cases anthropomorphizing AI can generate misplaced trust, there is another facet to consider. Namely, users would be able to employ conceptual categories and cognitive attitudes they are already familiar with (i.e., those employed when collaborating with other humans), thus making using AI less intimidating and potentially more effective-again, keeping in mind the risk of 'uncanny valley effects' <ref type="bibr" target="#b8">(Cheetham et al., 2015;</ref><ref type="bibr" target="#b10">Ciechanowski et al., 2019;</ref><ref type="bibr" target="#b41">Mori et al., 2012)</ref>.</p><p>Finally, we also acknowledged the objection stating that, since AI is a tool, perhaps the goal should not even be thought of in terms of genuine collaboration, but rather more similarly to how we use toasters. This might be a good point if DSSs were like toasters, but they are importantly not. DSSs' behavior can productively be described as intentional and, even (and perhaps especially) when they are 'wrong', they still provide something relevant for the user to consider in their deliberations, even if only by making the possibility of disagreement salient <ref type="bibr" target="#b3">(Baum et al., 2022)</ref>. Thus, we think that making DSSs capable of issuing explanations that function as reasons is not a hindrance, but it should be seen as part of what these systems are currently designed to do, that is, instrumental to their success as tools-at least for now.</p><p>In conclusion, novel technologies can only be revolutionary if enough people 'buy in', and AI-powered DSSs are no different. If we want DSSs to become the potentially disruptive decisionmaking aid that tech companies have promised us, it is paramount that explainability is not only kept at the forefront of AI research agendas, but that it is also tailored specifically towards nonexpert users and their needs. Making DSSs able to issue reason-like explanations accomplishes just that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statements and Declarations</head></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As clarified above, we focus here on DSSs giving reasons, not on generative AI. Some generative AI system, especially large language models (or LLMs), will output text that reads like reasons when prompted to do so by their users. However, it does not seem that this reason-like text output by LLMs plays the motivating role that we would expect reasons to play for the LLMs. We offer a detailed discussion of reasons below. But a full discussion whether the text output by generative AI should be viewed as reasons is beyond the scope of this manuscript.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For example, a tool like LIME (Local Interpretable Model-agnostic Explanations) when applied to any AI recommender system aims to issue keywords corresponding to the features the classifier considered most 'important' in driving its outputs<ref type="bibr" target="#b48">(Ribeiro et al., 2016)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">It should be clarified that, for Anscombe, intentions are not the only things that can play the role of reasons. Motives, for example, can also be interpreted as reasons in certain contexts. Her famous example is someone answering the question "Why did you kill him?" with "He killed my father". In a sense, the latter seems to be an acceptable answer to the question, and the answer, again in a sense, explains why the action was carried out. However, motives importantly differ from intentions, and the different ways in which these two notions play the role of reasons is an important aspect of what grounds the distinction for Anscombe. 9 In the study by Stumpf and colleagues<ref type="bibr" target="#b56">(Stumpf et al., 2009)</ref>, rule-based explanations are expressed through the phrase "the reason the system [issued prediction A] is because the rule was…"(Ibid., 7). This suggests that, in most cases, the relationship between explanations that redescribe the machine's functioning in broadly intentional terms and reasons is assumed rather than justified. In this section, we provide some justification for this relationship based on how philosophers think about intentional actions and rules.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10"><ref type="bibr" target="#b54">Sinnott-Armstrong (2022)</ref>, for instance, distinguishes between epistemic, practical, and explanatory reasons as three different functional roles which, indeed, can be played by the very same proposition at the same time (like in his example of "Because I knew I would win a lot of money" as an answer to the question "Why did you bet so much on the game?" (p. 129)), but they need not be.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Two-sided paired t test against the neutral rating (i.e. '3' on the Likert scale), t = 7.09, p &lt; 0.001. 12 Two-sided paired t test against the neutral rating, t = 2.39, p = 0.019.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">A trade-off has been previously demonstrated between accuracy and interpretability in machine-learning models, see for example<ref type="bibr" target="#b30">Liang et al. (2022)</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by The John Templeton Foundation (grant #: 61283), the John E. Fetzer Institute (grant #: 4189.00), and Templeton World Charity Foundation (grant #: TWCF-2022-30259). The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation, Templeton World Charity Foundation, Inc., or the Fetzer Institute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head><p>The authors have no relevant financial or non-financial interests to disclose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of AI use</head><p>Google Gemini was used to compose an early draft of the "Conclusion" section.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On Anthropomorphism in Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abercrombie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dinkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Talat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.290</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.emnlp-main.290" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Presented at the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing. Presented at the the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023" />
			<biblScope unit="page" from="4776" to="4790" />
		</imprint>
	</monogr>
	<note>Mirages</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2870052</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2870052" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Intention, Library of philosophy and logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E M</forename><surname>Anscombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From Responsibility to Reason-Giving Explainable Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Speith</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-022-00510-w</idno>
		<ptr target="https://doi.org/10.1007/s13347-022-00510-w" />
	</analytic>
	<monogr>
		<title level="j">Philos. Technol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Articulating Reasons: An Introduction to Inferentialism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brandom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge and the Social Articulation of the Space of Reasons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brandom</surname></persName>
		</author>
		<idno type="DOI">10.2307/2108339</idno>
		<ptr target="https://doi.org/10.2307/2108339" />
	</analytic>
	<monogr>
		<title level="j">Philosophy and Phenomenological Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Artificial intelligence in diabetes care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varughese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maruthappu</surname></persName>
		</author>
		<idno type="DOI">10.1111/dme.13587</idno>
		<ptr target="https://doi.org/10.1111/dme.13587" />
	</analytic>
	<monogr>
		<title level="j">Diabet. Med</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="495" to="497" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How Should My Chatbot Interact? A Survey on Social Characteristics in Human-Chatbot Interaction Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Chaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gerosa</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2020.1841438</idno>
		<ptr target="https://doi.org/10.1080/10447318.2020.1841438" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="729" to="758" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arousal, valence, and the uncanny valley: psychophysiological and self-report findings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheetham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jancke</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00981</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00981" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connect With Me. Exploring Influencing Factors in a Human-Technology Relationship Based on Regular Chatbot Use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Christoforakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Feicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinkofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Löscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diefenbach</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2021.689999</idno>
		<ptr target="https://doi.org/10.3389/fdgth.2021.689999" />
	</analytic>
	<monogr>
		<title level="j">Front. Digit. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">In the shades of the uncanny valley: An experimental study of human-chatbot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ciechanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Przegalinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gloor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2018.01.055</idno>
		<ptr target="https://doi.org/10.1016/j.future.2018.01.055" />
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="539" to="548" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transparency in complex computational systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Creel</surname></persName>
		</author>
		<idno type="DOI">10.1086/709729</idno>
		<ptr target="https://doi.org/10.1086/709729" />
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="568" to="589" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How people explain action (and autonomous intelligent systems should too)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>De Graaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented at the 2017 AAAI Fall Symposium Series</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The intentional stance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Dennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What does explainable AI really mean? A new conceptualization of perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better Decision-Making: Shared Mental Models and the Clinical Competency Committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Passiment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Hauer</surname></persName>
		</author>
		<idno type="DOI">10.4300/JGME-D-20-00850.1</idno>
		<ptr target="https://doi.org/10.4300/JGME-D-20-00850.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Graduate Medical Education</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eubanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>St. Martin&apos;s Publishing Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting hallucinations in large language models using semantic entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-024-07421-0</idno>
		<ptr target="https://doi.org/10.1038/s41586-024-07421-0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">630</biblScope>
			<biblScope unit="page" from="625" to="630" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">In medicine, how do we machine learn anything real? Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Nsoesie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2021.100392</idno>
		<ptr target="https://doi.org/10.1016/j.patter.2021.100392" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="100392" to="100392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Humanizing chatbots: The effects of visual, identity and conversational cues on humanness perceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sundar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2019.01.020</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2019.01.020" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="304" to="316" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Local Rule-Based Explanations of Black Box Decision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1805.10820</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1805.10820" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
		<ptr target="https://doi.org/10.1145/3236009" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARPA&apos;s Explainable Artificial Intelligence Program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v40i2.2850</idno>
		<ptr target="https://doi.org/10.1609/aimag.v40i2.2850" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining collaborative filtering recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work</title>
		<meeting>the 2000 ACM Conference on Computer Supported Cooperative Work</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Extending Ourselves: Computational Science, Empiricism, and Scientific Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Humphreys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemsdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vermeulen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Shared Mental Models -A Conceptual Analysis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445866</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445866" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Too much, too little, or just right? Ways explanations impact end users&apos; mental models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/VLHCC.2013.6645235</idno>
		<ptr target="https://doi.org/10.1109/VLHCC.2013.6645235" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Symposium on Visual Languages and Human Centric Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Donkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300717</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300717" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What do we want from Explainable Artificial Intelligence (XAI)?-A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Speith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page">103473</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Examining the utility of nonlinear machine learning approaches versus linear regression for predicting body image outcomes: The U.S. Body Project I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Frederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Lledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rosenfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Berardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Linstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Maoz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bodyim.2022.01.013</idno>
		<ptr target="https://doi.org/10.1016/j.bodyim.2022.01.013" />
	</analytic>
	<monogr>
		<title level="j">Body Image</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="32" to="45" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Why and why not explanations improve the intelligibility of context-aware intelligent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Avrahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Why and why not explanations improve the intelligibility of context-aware intelligent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Avrahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explanatory Preferences Shape Learning and Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lombrozo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.08.001</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.08.001" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="748" to="759" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Functional explanation and the function of explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lombrozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carey</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2004.12.009</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2004.12.009" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="167" to="204" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring consumer-perceived humanness of online organizational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vielledent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.107092</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.107092" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The influence of shared mental models on team process and performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Heffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Cannon-Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2018.07.007</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2018.07.007" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Varieties of Meaning: The 2002 Jean Nicod Lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Millikan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Language, Thought, and Other Biological Categories: New Foundations for Realism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Millikan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Uncanny Valley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Macdorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kageki</surname></persName>
		</author>
		<idno type="DOI">10.1109/MRA.2012.2192811</idno>
		<ptr target="https://doi.org/10.1109/MRA.2012.2192811" />
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Machines and Mindlessness: Social Responses to Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moon</surname></persName>
		</author>
		<idno type="DOI">10.1111/0022-4537.00153</idno>
		<ptr target="https://doi.org/10.1111/0022-4537.00153" />
	</analytic>
	<monogr>
		<title level="j">Journal of Social Issues</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="81" to="103" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Teleological Theories of Mental Content: Can Darwin Solve the Problem of Intentionality?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neander</surname></persName>
		</author>
		<editor>Ruse, M.</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
	<note>The Oxford Handbook of Philosophy of Biology</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Algorithms of oppression: How search engines reinforce racism. xv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Noble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">229</biblScope>
		</imprint>
	</monogr>
	<note>Algorithms of oppression: How search engines reinforce racism</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The human side of human-chatbot interaction: A systematic literature review of ten years of research on text-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Curti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boldi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2021.102630</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2021.102630" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A survey of hallucination in large foundation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rawte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05922</idno>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Systematic Review: Trust-Building Factors and Implications for Conversational Agent Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rheu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh-Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2020.1807710</idno>
		<ptr target="https://doi.org/10.1080/10447318.2020.1807710" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1602.04938</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1602.04938" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A Misdirected Principle with a Catch: Explicability for AI. Minds and Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-019-09509-3</idno>
		<ptr target="https://doi.org/10.1007/s11023-019-09509-3" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="495" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An Experimental Study on Emotional Reactions Towards a Robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rosenthal-Von Der Pütten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Krämer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sobieraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Eimler</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12369-012-0173-8</idno>
		<ptr target="https://doi.org/10.1007/s12369-012-0173-8" />
	</analytic>
	<monogr>
		<title level="j">Int J of Soc Robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Globally-Consistent Rule-Based Summary-Explanations for Machine Learning Models: Application to Credit-Risk Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shaposhnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">AI systems must not confuse users about their sentience or moral status. Patterns 4, 100818</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwitzgebel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2023.100818</idno>
		<ptr target="https://doi.org/10.1016/j.patter.2023.100818" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Some Reflections on Language Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sellars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="204" to="228" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What are reasons?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sinnott-Armstrong</surname></persName>
		</author>
		<idno type="DOI">10.1093/oso/9780197572153.003.0015</idno>
		<ptr target="https://doi.org/10.1093/oso/9780197572153.003.0015" />
	</analytic>
	<monogr>
		<title level="m">Free Will: Philosophers and Neuroscientists in Conversation</title>
		<editor>Maoz, U., Sinnott-Armstrong, W.</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Evolved Apprentice: How Evolution Made Humans Unique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sterelny</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9780262016797.001.0001</idno>
		<ptr target="https://doi.org/10.7551/mitpress/9780262016797.001.0001" />
		<imprint>
			<date type="published" when="2012" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Interacting meaningfully with machine learning systems: Three experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2009.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2009.03.004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human Computer Studies</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="639" to="662" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Măcelaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ighian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Rădulescu</surname></persName>
		</author>
		<idno type="DOI">10.3390/su12010256</idno>
		<ptr target="https://doi.org/10.3390/su12010256" />
	</analytic>
	<monogr>
		<title level="j">The Effect of Social Presence and Chatbot Errors on Trust. Sustainability</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Who Do We Become When We Talk to Machines? An MIT Exploration of Generative AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Turkle</surname></persName>
		</author>
		<idno type="DOI">10.21428/e4baedd9.caa10d84</idno>
		<ptr target="https://doi.org/10.21428/e4baedd9.caa10d84" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evaluating XAI: A comparison of rulebased and example-based explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Waa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nieuwburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neerincx</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2020.103404</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2020.103404" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Notions of explainability and evaluation approaches for explainable artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vilone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Longo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2021.05.009</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2021.05.009" />
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="89" to="106" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Transparency and the Black Box Problem: Why We Do Not Trust AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Von Eschenbach</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-021-00477-0</idno>
		<ptr target="https://doi.org/10.1007/s13347-021-00477-0" />
	</analytic>
	<monogr>
		<title level="j">Philosophy and Technology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1607" to="1622" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The challenge of crafting intelligible intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3282486</idno>
		<ptr target="https://doi.org/10.1145/3282486" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Philosophical investigations. Philosophische Untersuchungen., Philosophical investigations. Philosophische Untersuchungen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wittgenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<pubPlace>Macmillan, Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gavaghan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-018-0330-6</idno>
		<ptr target="https://doi.org/10.1007/s13347-018-0330-6" />
	</analytic>
	<monogr>
		<title level="j">Philos. Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="661" to="683" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">2020. Different &quot;intelligibility&quot; for different folks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Danks</surname></persName>
		</author>
		<idno type="DOI">10.1145/3375627.3375810</idno>
		<ptr target="https://doi.org/10.1145/3375627.3375810" />
	</analytic>
	<monogr>
		<title level="m">AIES 2020 -Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</title>
		<imprint>
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
