<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Algorithmic Gender Bias: Investigating Perceptions of Discrimination in Automated Decision-Making</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soojong</forename><surname>Kim</surname></persName>
							<email>sjokim@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Communication</orgName>
								<orgName type="institution">University of California Davis</orgName>
								<address>
									<country>Unites States</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center on Philanthropy and Civil Society</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poong</forename><surname>Oh</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Wee Kim Wee School of Communication and Information</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joomi</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Advertising &amp; Public Relations</orgName>
								<orgName type="institution">University of Georgia</orgName>
								<address>
									<addrLine>United States Author Note Soojong Kim https://orcid.org</addrLine>
									<postCode>0000-0002-1334-5310</postCode>
									<settlement>Poong Oh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of California Davis</orgName>
								<address>
									<addrLine>361 Kerr Hall</addrLine>
									<postCode>95616</postCode>
									<settlement>Davis</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Algorithmic Gender Bias: Investigating Perceptions of Discrimination in Automated Decision-Making</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated decision-making</term>
					<term>Artificial intelligence</term>
					<term>Gender</term>
					<term>Identity</term>
					<term>Bias</term>
					<term>Fairness</term>
					<term>Equality</term>
					<term>Trust</term>
					<term>Emotion</term>
				</keywords>
			</textClass>
			<abstract>
				<p>With the widespread use of artificial intelligence and automated decision-making (ADM), concerns are increasing about automated decisions biased against certain social groups, such as women and racial minorities. The public&apos;s skepticism and the danger of algorithmic discrimination are widely acknowledged, yet the role of key factors constituting the context of discriminatory situations is underexplored. This study examined people&apos;s perceptions of gender bias in ADM, focusing on three factors influencing the responses to discriminatory automated decisions: the target of discrimination (subject vs. other), the gender identity of the subject, and situational contexts that engender biases. Based on a randomized experiment (N = 602), we found stronger negative reactions to automated decisions that discriminate against the gender group of the subject than those discriminating against other gender groups, evidenced by lower perceived fairness and trust in ADM, and greater negative emotion and tendency to question the outcome. The negative reactions were more pronounced among participants in underserved gender groups than men. Also, participants were more sensitive to biases in economic and occupational contexts than in other situations. These findings suggest that perceptions of algorithmic biases should be understood in relation to the public&apos;s lived experience of inequality and injustice in society.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Making</head><p>The use of automated decision-making (ADM) is increasingly replacing human decisionmaking processes in various domains, including the news media <ref type="bibr" target="#b22">(Diakopoulos &amp; Koliska, 2017;</ref><ref type="bibr" target="#b100">Thurman et al., 2019)</ref>, healthcare <ref type="bibr" target="#b41">(Jha &amp; Topol, 2016;</ref><ref type="bibr" target="#b114">Yu &amp; Kohane, 2019)</ref>, and law enforcement <ref type="bibr" target="#b46">(Kennedy et al., 2011;</ref><ref type="bibr" target="#b72">Nissan, 2017)</ref>. With the widespread use of algorithms, concerns are growing about the possibilities of biased and unfair automated decisions, which could contribute to worsening the existing social inequality and disparity <ref type="bibr" target="#b6">(Benjamin, 2019;</ref><ref type="bibr" target="#b73">Noble, 2018;</ref><ref type="bibr" target="#b77">O'Neil, 2016)</ref>.</p><p>Algorithms often produce unequal, unfair, and even unjust outcomes <ref type="bibr" target="#b3">(Barocas et al., 2019;</ref><ref type="bibr" target="#b36">Hooker, 2021)</ref>. Existing inequality and injustice in society can easily penetrate into algorithms during the algorithms' lifecycle "from problem selection to post deployment considerations" (I. <ref type="bibr">Chen et al., 2021, p. 123)</ref>. Especially, evidence of gender discrimination in ADM has been accumulating in a wide array of areas, including housing markets <ref type="bibr" target="#b2">(Asplund et al., 2020)</ref>, job markets (L. <ref type="bibr" target="#b18">Chen et al., 2018;</ref><ref type="bibr" target="#b37">Imana et al., 2021;</ref><ref type="bibr" target="#b109">C. Wang et al., 2022)</ref>, facial recognition <ref type="bibr" target="#b29">(Fosch-Villaronga et al., 2021;</ref><ref type="bibr" target="#b68">Menezes et al., 2021)</ref>, and web search engines <ref type="bibr" target="#b44">(Kay et al., 2015;</ref><ref type="bibr" target="#b64">Makhortykh et al., 2021;</ref><ref type="bibr" target="#b78">Otterbacher et al., 2017;</ref><ref type="bibr" target="#b107">Vlasceanu &amp; Amodio, 2022)</ref>. The alarming evidence highlights the urgent need for the evaluation of the social impacts and consequences of machine-driven discrimination and injustice, the understanding of the skepticism and anxiety of the public, and the improvement of the design and implementation of socio-technological systems <ref type="bibr" target="#b23">(Dietvorst et al., 2014;</ref><ref type="bibr" target="#b24">Dolata et al., 2021;</ref><ref type="bibr" target="#b58">Logg et al., 2019;</ref><ref type="bibr" target="#b76">O'Connor &amp; Liu, 2023;</ref><ref type="bibr" target="#b115">Zhang &amp; Yencha, 2022)</ref>.</p><p>Despite the concerns and needs, key questions regarding human perceptions of discriminatory algorithmic outcomes remain under-investigated. While algorithm-involved discrimination, inequality, and bias are widely conceived as problematic and dangerous (J. <ref type="bibr" target="#b70">Miller, 2020;</ref><ref type="bibr" target="#b99">Thune, 2022;</ref><ref type="bibr" target="#b101">Verma, 2022)</ref>, knowledge is scarce about the effects of specific factors constituting discriminatory situations. Among these factors, this study focuses on the target of discrimination, social identities, and situational contexts.</p><p>First, although the target of discrimination (i.e., a social group that is being unfairly disadvantaged compared with other groups), is an important factor constituting the context of discrimination <ref type="bibr" target="#b82">(Phillips &amp; Jun, 2022;</ref><ref type="bibr" target="#b85">Pronin et al., 2004)</ref>, the evidence on its role and effects in machine-induced discrimination is inconclusive. While some scholars found that decisions disfavoring individuals elicited negative reactions to ADM <ref type="bibr" target="#b113">(Yalcin et al., 2022)</ref>, others could not identify significant cognitive effects of favoring decisions <ref type="bibr" target="#b57">(Li &amp; Xing, 2022)</ref>.</p><p>Second, different groups have different experiences of and attitudes toward discrimination in society, and social identities may shape the perceptions of and reactions to machine-induced discrimination. Members of certain social groups, such as women, LGBTQ, racial minorities, and people with disabilities, frequently experience discrimination and injustice against their social identities, which negatively influences their lives in numerous ways, such as their economic and financial resources <ref type="bibr" target="#b31">(Gharehgozli &amp; Atal, 2020</ref><ref type="bibr">), health (Brown et al., 2000</ref><ref type="bibr" target="#b67">Manuel, 2018)</ref>, and job performances <ref type="bibr">(Greenhaus et al., 1990;</ref><ref type="bibr" target="#b98">Sue, 2010)</ref>. However, evidence is thin about whether and how different social groups' perceptions of algorithmic discrimination are. This gap extends to the understanding of the perceptions of algorithmic gender discrimination, where empirical studies are growing but still tend to be explorative <ref type="bibr" target="#b108">(Vorisek et al., 2023;</ref><ref type="bibr" target="#b111">R. Wang et al., 2020;</ref><ref type="bibr" target="#b115">Zhang &amp; Yencha, 2022)</ref> despite growing recognition of its importance.</p><p>Lastly, algorithmic biases have been documented in various contexts, and it was shown that factors involving the situations of technology use affect human acceptance of and reactions to AI <ref type="bibr" target="#b59">(Longoni et al., 2019)</ref>. However, there is a lack of investigation into whether and how the contexts of decision-making shape individuals' responses to disparities in ADM. It is possible that situations entailing overt and immediate financial consequences evoke greater reactions toward gender-biased algorithms.</p><p>To better our knowledge about these puzzling issues, the current research aimed to examine how people perceive potentially gender-biased automated decisions and algorithms.</p><p>Specifically, we questioned how people perceive algorithms that discriminate against people's own social groups compared with other social groups and how cognitive and emotional reactions to discriminatory decisions are shaped by gender identities. This research is one of the first attempts to focus exclusively on the influence of gender identity on the perception of biased algorithm outcomes, particularly in the context of gender-based discrimination. For this purpose, this study investigated multiple dimensions of perceptions of algorithms, including perceived fairness, trust in ADM, the tendency to question the outcome, the perceived likelihood of experiencing similar discriminatory situations in everyday life, and the emotional reaction to the biases. These dimensions were examined across two situations with different targets of discrimination. We also tested multiple realistic decision-making contexts, including financial, health, and public service contexts, in examining algorithm perceptions.</p><p>The rest of this paper is organized as follows. We first review the previous research on perceptions of algorithms and present the research questions and hypotheses of the current study.</p><p>We then describe the methodology and the results of the study. Based on the research findings, we discuss the implications of the study for the understanding of public perceptions of algorithmic discrimination and injustice. Finally, we conclude the paper with the limitations and future research directions. Throughout this paper, we use the term "algorithm" to indicate a computational subprocess or a machine that calculates decisions, while "ADM" was used to indicate decision-making processes or an act of making decisions based on algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception of Algorithms and ADM</head><p>Past research on perceptions of algorithms examined general attitudes toward them, such as whether people would prefer algorithmic decisions over human decisions <ref type="bibr" target="#b58">(Logg et al., 2019)</ref>.</p><p>Despite the public's general acceptance of and positive attitudes toward automated decisions <ref type="bibr" target="#b58">(Logg et al., 2019;</ref><ref type="bibr" target="#b100">Thurman et al., 2019)</ref>, people often refuse to use algorithms. For example, a study found an aversion to AI-based medical decisions compared with decisions made by human healthcare providers <ref type="bibr" target="#b59">(Longoni et al., 2019)</ref>. Instances of algorithmic errors and their potential significance can also contribute to the aversion and refusal to the adoption of algorithms.</p><p>Observing an error in ADM appeared to reduce the confidence and intention to choose algorithms over human decision-makers, even in situations where algorithms outperform humans <ref type="bibr" target="#b23">(Dietvorst et al., 2014)</ref>. Concerns about potential biases in ADM can result in algorithm aversion, which is often characterized by decreased perceived fairness and trust in ADM <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b52">Lee &amp; Baykal, 2017)</ref>.</p><p>Several cognitive and emotional dimensions have been studied as factors influencing the public's algorithm appreciation and aversion, including perceived fairness, trust in ADM, and emotional reactions <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b51">Lee, 2018;</ref><ref type="bibr" target="#b52">Lee &amp; Baykal, 2017;</ref><ref type="bibr" target="#b53">Lee &amp; Rich, 2021;</ref><ref type="bibr" target="#b111">R. Wang et al., 2020)</ref>. First, perceived fairness refers to the extent to which people believe that algorithms treat everyone equally without prejudices or subjective judgments <ref type="bibr" target="#b51">(Lee, 2018)</ref>. The perception of fairness accompanies social comparison in which people conceive automated decisions generated for them in comparison with decisions produced for others <ref type="bibr" target="#b93">(Skitka et al., 2003;</ref><ref type="bibr" target="#b111">R. Wang et al., 2020)</ref>. Second, scholars have found that trust in ADM can be affected by beliefs that ADM will result in positive outcomes, such as achieving a specific goal <ref type="bibr" target="#b15">(Calhoun et al., 2019;</ref><ref type="bibr" target="#b51">Lee, 2018)</ref>; and expected traits of the outcomes, such as the accuracy, reliability, and usefulness of algorithms <ref type="bibr" target="#b19">(Choung, David, &amp; Ross., 2022)</ref>. A recent review based on the European Union's guidelines for trust in artificial intelligence technologies also asserted that AI systems should ensure respect for human control, prevention of harm, fairness, and explicability <ref type="bibr" target="#b43">(Kaur et al., 2022)</ref>. Third, people can react to ADM emotionally. Lee (2018) identified similar or more negative emotions toward algorithm-generated decisions than human decisions in the context of hiring and work evaluation. The negative emotion was attributed to the dehumanizing and demeaning experience of being evaluated by machines.</p><p>The social psychological attribution framework can be useful in understanding perceived fairness and emotional responses in algorithm-related situations. In social settings, emotional responses to others' behaviors are altered by the perceived degree of intentionality and the agency attributed to the behavior <ref type="bibr" target="#b7">(Betancourt &amp; Blair, 1992)</ref>. For example, when equality is violated in a social situation, people make judgments about whether the violator had the intention to discriminate. They tend to show stronger negative reactions to intentional discriminatory behaviors than unintentional ones <ref type="bibr" target="#b92">(Shaver, 1985;</ref><ref type="bibr" target="#b96">Stouten et al., 2006)</ref>. These pieces of evidence suggest that if negative outcomes are attributed to algorithms, it may lead people to evaluate the machines as less efficient, likable, and predictable <ref type="bibr" target="#b90">(Shank &amp; DeSanti, 2018)</ref>.</p><p>Previous work also suggests that several cognitive and emotional appraisals are associated with perceptions of algorithms and attribution of positive or negative outcomes. When people assume that algorithms make fairer and less biased decisions than human decisionmakers, they are less likely to recognize racial and gender disparities in ADM <ref type="bibr" target="#b14">(Bonezzi &amp; Ostinelli, 2021)</ref>. However, observing or experiencing unequal and biased ADM in social contexts could result in immediate re-evaluation that may lead to algorithm aversion or refusal <ref type="bibr" target="#b23">(Dietvorst et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Target of Discrimination, Social Identity, and the Perception of Algorithms</head><p>When examining perceptions of discrimination and injustice, it is easy to overlook how people recognize and respond to decisions favoring their own group over others and only focus on decisions disfavoring individuals. Previous research suggests that favoring decisions, compared with those disfavoring individuals, are less likely to be perceived as discriminatory, inducing more positive reactions to the decision-makers <ref type="bibr" target="#b82">(Phillips &amp; Jun, 2022)</ref>. Regarding machine-driven discrimination, some scholars reported mixed results on the influence of the directions of favoritism in ADM. For example, a study found that automated decisions in favor of consumers were less positively appreciated than favoring decisions made by humans, whereas disfavoring decisions elicited negative reactions to both machine-and human-made decisions <ref type="bibr" target="#b113">(Yalcin et al., 2022)</ref>. Contrarily, another study on students' perceptions of algorithms in educational settings did not identify significant effects of favoring decisions on perceived fairness <ref type="bibr" target="#b57">(Li &amp; Xing, 2022)</ref>. These mixed findings indicate the need for further investigations to clarify how the target of discrimination affects the perceptions of algorithms.</p><p>Thus, the current study evaluates how the target of algorithmic discrimination affects individuals' perceptions of automated outcomes. In the current research, focusing on gender identity, we examined how individuals appraise fairness and trustworthiness of algorithms that produce discriminatory outcomes based on the certain aspect of individuals, and to what extent they s the biased outcomes. The current research hypothesizes the following. H1: Biased automated decisions that discriminate against the subject' gender will lead to negative perceptions associated with ADM, indicated by (H1a) lower perceived fairness <ref type="formula">H1b</ref>lower trust in ADM, (H1c) a higher negative emotional reaction, and (H1d) a higher tendency to question the outcomes, compared with decisions discriminating against other gender groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gender Difference in the Perception of Biased Algorithms</head><p>People often attribute negative life experiences to inequality and discrimination in society, and the attribution process influences how they perceive bias <ref type="bibr" target="#b61">(Major et al., 2002;</ref><ref type="bibr" target="#b82">Phillips &amp; Jun, 2022;</ref><ref type="bibr" target="#b85">Pronin et al., 2004)</ref>. Underserved or marginalized social groups frequently undergo negative life events that entail discrimination and injustice against their social identities <ref type="bibr" target="#b61">(Major et al., 2002;</ref><ref type="bibr" target="#b98">Sue, 2010)</ref>. Accordingly, underserved populations tend to be more aware of and sensitive to the signs of biased decision-making that signal the risk of discrimination against them <ref type="bibr" target="#b61">(Major et al., 2002)</ref>. A study found that women who faced an overt sign of bias were significantly more likely to attribute their failure to discrimination than those not facing the overt sign <ref type="bibr" target="#b63">(Major et al., 2003)</ref>.</p><p>The current study predicts that gender identity shapes the reactions to gender-biased ADM. A considerable number of women experience gender discrimination, inequality, injustice, and harassment in their life <ref type="bibr" target="#b25">(England et al., 2020;</ref><ref type="bibr" target="#b80">Parker &amp; Funk, 2017;</ref><ref type="bibr" target="#b94">SteelFisher et al., 2019)</ref>.</p><p>Gender discrimination can influence not only social and economic outcomes but also physical and mental well-being <ref type="bibr" target="#b28">(Fischer &amp; Holz, 2010;</ref><ref type="bibr" target="#b88">Shaffer et al., 2000;</ref><ref type="bibr" target="#b103">Vigod &amp; Rochon, 2020)</ref>. It is known that in general women have higher awareness and sensitivity to discriminatory situations and develop more proactive coping strategies than men <ref type="bibr" target="#b12">(Blodorn et al., 2012)</ref>.</p><p>Existing inequality in society shapes the reactions to and perceptions of algorithms <ref type="bibr" target="#b48">(Koenecke et al., 2020;</ref><ref type="bibr" target="#b104">Vincent &amp; Viljoen, 2020)</ref>. Thus, gender identity may also influence the perception of machine-induced discrimination. People's social identities influence their perception of algorithms and automated decisions <ref type="bibr" target="#b33">(Grgic-Hlaca et al., 2018;</ref><ref type="bibr" target="#b32">Grgić-Hlača et al., 2022)</ref>, and the growing body of literature is focusing on the gender difference in the perception of algorithms <ref type="bibr" target="#b84">(Pierson, 2018;</ref><ref type="bibr" target="#b111">R. Wang et al., 2020;</ref><ref type="bibr" target="#b115">Zhang &amp; Yencha, 2022)</ref>. For example, Wang et al. <ref type="formula">2020</ref>indicated that female participants reported lower perceived fairness about biased algorithmic decisions compared with male participants, but the difference was not statistically significant, suggesting the need for more empirical evidence of gender differences in perceptions of algorithms. <ref type="bibr" target="#b115">Zhang and Yencha (2022)</ref> reported that female participants maintained more negative views toward hiring algorithms compared with male participants, suggesting that gender identity may also shape the perception of biased algorithmic decisions.</p><p>Therefore, the present research examines how gender identity shapes individuals' reactions to gender-biased ADM. We hypothesized:</p><p>H2: Women will show (H2a) lower perceived fairness, (H2b) lower trust in ADM than men, (H2c) more negative emotion, and (H2d) a higher tendency to question an outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Context of Use</head><p>Algorithmic biases have been documented in various contexts, such as healthcare (e.g., <ref type="bibr" target="#b21">Cirillo et al., 2020;</ref><ref type="bibr" target="#b74">Obermeyer et al., 2019)</ref>, employment (e.g., <ref type="bibr" target="#b50">Lambrecht &amp; Tucker, 2019)</ref>, and financial decisions (e.g., <ref type="bibr" target="#b35">Hassani, 2021)</ref>. The contexts of decision-making may shape individuals' responses to gender disparities in ADM. People are often reluctant to use ADM for critical issues, such as issues that can affect their health or finance <ref type="bibr" target="#b59">(Longoni et al., 2019)</ref>.</p><p>In this study, we hypothesize that situations featuring overt economic consequences evoke greater reactions toward gender-biased algorithms, based on two reasons. First, contexts with overt economic benefits or disadvantages, such as financial incentives and product pricing, tend to raise individuals' sensitivity to the fairness and reliability of the decision-maker <ref type="bibr" target="#b26">(Esarey et al., 2012;</ref><ref type="bibr" target="#b71">Moliner et al., 2013)</ref>. Second, financial domains are indeed particularly associated with gender disparities including the wage gap <ref type="bibr" target="#b31">(Gharehgozli &amp; Atal, 2020;</ref><ref type="bibr" target="#b112">Williams et al., 2010)</ref> and discrepancy in financial resources <ref type="bibr" target="#b106">(Vlachantoni, 2012)</ref>.</p><p>H3: Biased automated decisions resulting in obvious economic consequences will lead to (H3a) lower perceived fairness, (H3b) lower trust in ADM, (H3c) greater negative emotions, (H3d) a higher tendency to question the outcome, compared with those with less obvious economic consequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited participants from an online participant pool, Prolific. To participate in the study, participants must be at least 18 years old and located in the United States. Participants who satisfied these conditions visited an online experiment webpage created with Qualtrics, a survey creation tool.</p><p>The number of people who completed the experiment was 642. Among them, responses from 40 people were filtered out due to the failure of following the experimental instructions or providing the correct answer to the attention check question. This study reports the results based on the responses of 602 participants.</p><p>Among the 602 participants, those who solely identified themselves as white and Black accounted for 74.6% (N = 449) and 7.5% (N = 45), respectively. Their average age was 38.6 years (SD = 14.8). Participants who self-identified as a man, a woman, and a non-binary were 43.2% (N = 260), 53.0% (N = 319), and 3.0% (N = 18), respectively. The rest, 0.8 % (N = 5), chose "Prefer not to disclose" or "Prefer to self-describe."</p><p>Due to the limited number of participants self-identifying as non-binary or other categories, the present study first focused on the two largest gender groups, men and women, which accounted for 96.2% (N = 579) of the sample, and then conducted additional analyses including the non-binary participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>To test the hypotheses of the present research, we conducted an online randomized experiment. After providing informed consent on the first webpage of the experiment, participants were asked to consider a friend of a gender different from theirs but of the same race, age, education level, and economic status as the participant. The instruction asked to keep considering the chosen friend when considering the scenarios following the instruction. We included a question asking the gender of the chosen friend, to make sure participants followed the instruction.</p><p>Participants were shown nine scenarios one at a time. Each scenario described a realistic situation in which the participant and the chosen friend use the same technology based on algorithms, such as a system automatically rating the interview performance of job candidates, but receive a gender-biased outcome. Past studies have reported that these scenarios have actually occurred in the real world <ref type="bibr" target="#b0">(Acikgoz et al., 2020;</ref><ref type="bibr" target="#b11">Binns et al., 2018;</ref><ref type="bibr" target="#b69">A. P. Miller &amp; Hosanagar, 2019;</ref><ref type="bibr" target="#b81">Parra et al., 2021)</ref>. Starting from the list of scenarios used in Parra et al.</p><p>(2021), we updated and edited them and added more scenarios, following the same narrative format. An example scenario reads as follows:</p><p>"You and your friend applied for the same job position in which you both are seriously interested. You both have similar levels of experience, knowledge, and skills related to the position.</p><p>Each of you conducted an online interview with the company and was asked the same questions by an automated interview program. Applications and recorded responses are automatically rated by the program. You noticed that you and your friend did equally well in the interview and provided similar answers to the interview questions. Two weeks after the interview, your friend is offered the position, but you do not receive any offer."</p><p>The scenarios covered topics related to media, finance, public service, the labor market, and health and safety. A complete list of scenarios is presented in <ref type="table" target="#tab_0">Table S1</ref> in the online supporting information, which is accessible with the following link: https://osf.io/2vf5c/?view_only=3e950e234f9e49eaaaae842420ea139e.</p><p>Participants were assigned to one of the two experimental conditions randomly. In both conditions, participants were given the same scenarios, but the discrimination target differed depending on the condition. The two conditions represented different discrimination targets: In the "subject-targeting" condition, ADM discriminated against the subject compared with the friend, whereas it discriminated against the friend in the "other-targeting" condition compared with the subject. For example, the aforementioned example was a scenario in the subjecttargeting condition, whereas the scenario in the other-targeting condition stated that "you are offered the position, but your friend does not receive any offer." One scenario was given to a participant at a time, and the presentation order was randomly determined for each participant.</p><p>For the subject-targeting and other-targeting conditions, 302 and 300 participants were assigned, respectively.</p><p>After considering a scenario, participants responded to questions measuring perceived fairness, trust in ADM, the tendency to question the outcome, negative emotion, and the likelihood that each situation happens in real life. An attention check question was also included in the middle of the experiment. After considering the nine scenarios, participants also answered demographic questions. The median duration of the experiment was 677 seconds. Participants were given monetary compensation for their participation. This research was exempted by the Institutional Review Board of [a university name redacted for the blind review.] The experimental data collection was completed in August 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>The following variables were measured on a 7-point Likert scale (from 1 to 7). The questionnaire to measure these variables is included in the online supporting information. For the summary statistics of dependent variables shown below, each variable was averaged across all nine scenarios. Perceived fairness (M = 3.2, SD = 1.1) was measured with a question based on past research <ref type="bibr" target="#b1">(Araujo et al., 2020;</ref><ref type="bibr" target="#b51">Lee, 2018)</ref>. Trust in ADM (M = 2.8, SD = 0.9) was measured with a question based on previous research <ref type="bibr" target="#b51">(Lee, 2018)</ref>. Negative emotion (M = 4.4, SD = 1.3, α = 0.93) was measured with three questions <ref type="bibr" target="#b51">(Lee, 2018)</ref>.</p><p>To measure participants' broader awareness and perception of algorithmic discrimination, we included two new measures. The tendency to question the outcome represents a broad skepticism about the outcome, while perceived fairness, trust, and negative emotions focus on a specific aspect of algorithms and outcomes. It was measured by asking participants to rate their agreement with each of the following two statements, "This outcome is problematic." and "This outcome is questionable." on a 7-point scale from 1 to 7. The two responses were then averaged to create a single variable (M = 5.0, SD = 1.0, α = 0.91). The perceived likelihood of the outcome detects the subjective evaluation of the possibility of a certain outcome. This variable (M = 4.0, SD = 1.1) was measured on a 7-point scale from 1 to 7 using a question: "How likely is this outcome to happen in your everyday life?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analyses</head><p>Multiple linear regressions were conducted to test H1 and H2. As a robustness check, we also conducted subgroup analyses to examine the effect of the discrimination target on each dependent variable in each gender group. Even though dividing the sample into smaller subgroups unavoidably reduces the statistical power of analysis, the separate investigation of a subgroup can offer useful insights into the contexts of a specific gender group.</p><p>To answer H3, we compared the reactions of participants in each scenario and condition.</p><p>For this purpose, we estimated the marginal means of each dependent variable based on a random effect linear regression model. The model predicted a dependent variable as a function of the scenario, the condition, and the gender group of a participant and also accounted for the correlation within a subject. Based on the fitted model, the marginal mean of a dependent variable was estimated in each scenario and condition.</p><p>All statistical analyses reported in this study were executed on R, an open-source statistical software (version 4.0.3), using packages named "lme4" <ref type="bibr" target="#b4">(Bates et al., 2015)</ref> and "emmeans" <ref type="bibr" target="#b56">(Lenth, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of the Discrimination Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceived Fairness</head><p>The effect of subject-targeting discrimination on perceived fairness was highly significant, controlling for the subject's gender. As <ref type="table" target="#tab_0">Table 1</ref> presents, the subject-targeting discrimination significantly decreased the average perceived fairness (Model 1: B = -0.511, SE = 0.088, p &lt; .001) compared with the other-targeting discrimination, lending support for H1a. This model also shows that the average perceived fairness was significantly lower among female participants than male participants, controlling for the condition (B = -0.203, SE = 0.089, p = .022). This result is consistent with H2a. The gender difference is visualized in <ref type="figure" target="#fig_0">Figure 1</ref>-A as the gap between two intercepts of the lines representing males (purple) and females (orange).</p><p>The results from the gender moderation model did not support a statistically significant difference in the effects of the discrimination target among females and among males (Model 2: B = -0.257, SE = 0.177, p = .146). <ref type="table" target="#tab_1">Table 2</ref> presents the results of the subgroup analysis. Specifically, when responses from the two gender groups were analyzed separately, participants who identified themselves as female (B = -0.626, SE = 0.113, p &lt; .001) and male (B = -0.369, SE = 0.138, p = .008) both showed significantly lower perceived fairness in the subject-targeting condition than in the othertargeting condition. <ref type="figure" target="#fig_0">Figure 1</ref>-A visualizes the decrease in both gender groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust in ADM</head><p>The effect of subject-targeting discrimination on trust in ADM was highly significant, adjusting for the subject's gender. As predicted in H1b, the subject-targeting discrimination significantly decreased the average trust in ADM (Model 1: B = -0.307, SE = 0.077, p &lt; .001) <ref type="table" target="#tab_0">(Table 1)</ref>. This model also indicates that the average trust was significantly lower among female participants than male participants, controlling for the condition (B = -0.164, SE = 0.077, p = .035). The gender difference is represented by the difference in two intercepts of the lines indicating females and males, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_0">-B.</formula><p>The results from the gender moderation model in <ref type="table" target="#tab_0">Table 1 (Model 2)</ref> indicate that a statistically significant difference was not detected in the effect of the discrimination target between the female group and the male group (B = -0.078, SE = 0.155, p = .614). The subgroup analysis in <ref type="table" target="#tab_1">Table 2</ref> also shows that the average trust significantly declined both among female participants (B = -0.342, SE = 0.101, p &lt; .001) and male participants (B = -0.264, SE = 0.119, p = .027). The decrease in both groups was visualized in <ref type="figure" target="#fig_0">Figure 1</ref>-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Emotion</head><p>Consistent with H1c, the effect of the subject-targeting discrimination on negative emotion was highly significant, controlling for the subject gender as shown in <ref type="table" target="#tab_0">Table 1</ref> (Model 1:</p><formula xml:id="formula_1">B = 0.771, SE = 0.102, p &lt; .001).</formula><p>The same model also indicated that the average negative emotion was significantly higher among female participants than male participants, controlling for the condition (B = 0.302, SE = 0.102, p = .003). The surge in negative emotion induced by the subject-targeting discrimination is also shown in <ref type="figure" target="#fig_0">Figure 1</ref>-C. The greater negative emotion among females was visualized as a greater intercept of the line representing females. The gender moderation model indicated that the subject-targeting discrimination's effect was greater among the female group than the male group (Model 2: B = 0.416, SE = 0.204, p = .042). The subgroup analysis in <ref type="table" target="#tab_1">Table 2</ref> pointed out that the average negative emotion significantly increased among female participants (B = 0.957, SE = 0.125, p &lt; .001), and it also increased among male participants (B = 0.542, SE = 0.167, p = .001) but by a smaller size than females. The gender difference corresponds to the fact that the slope of the female line in <ref type="figure" target="#fig_0">Figure   1</ref>-C is greater than that of males.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Tendency to Question the Outcome</head><p>The effect of the subject-targeting discrimination on the tendency to question the outcome was positive but statistically insignificant (Model 1: B = 0.161, SE = 0.085, p = .058), lending partial support for H1d. This model evinced that the average tendency was significantly higher among female participants than male participants, controlling for the condition (B = 0.258, SE = 0.085, p = .002).</p><p>The gender moderation model (Model 2 in <ref type="table" target="#tab_0">Table 1</ref>) explains that the effect of the subjecttargeting discrimination was greater in the female group than in the male group (B = 0.339, SE = 0.170, p = .047). Based on the subgroup analysis presented in <ref type="table" target="#tab_1">Table 2</ref>, we identified that the average tendency significantly increased among female participants (B = 0.312, SE = 0.106, p = .003), but there was a decrease among male participants, which did not show statistical significance (B = -0.026, SE = 0.136, p = .847). This difference between the two gender groups can also be found in <ref type="figure" target="#fig_0">Figure 1</ref>-D by comparing the upward-sloping line of the female group and the relatively flat line of the male group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Perceived Likelihood of the Outcome</head><p>The effect of the subject-targeting discrimination on the perceived likelihood of the outcome was negative and insignificant (Model 1: B = -0.021, SE = 0.096, p = .828). Also, this model did not support a statistically significant difference between males and females, controlling for the condition (B = -0.066, SE = 0.096, p = .494). The gender moderation model shows how gender groups reacted to subject-targeting discrimination differently. <ref type="table" target="#tab_0">Table 1</ref> shows that the effect was significantly greater for the female group and the male group (B = 0.408, SE = 0.192, p = .034). The subgroup analysis in <ref type="table" target="#tab_1">Table 2</ref> reveals that the average perceived likelihood increased among female participants (B = 0.162, SE = 0.125, p = .198) but decreased among male participants (B = -0.246, SE = 0.147, p = .096). <ref type="figure" target="#fig_0">Figure 1</ref>-E visualizes the gender difference with the upward and the downward sloping lines representing female and male participants, respectively. <ref type="figure" target="#fig_1">Figure 2</ref> presents the estimated marginal means of each dependent variable. The three scenarios about interest rates of recommended financial products (Scenario 1), salaries of recommended jobs (Scenario 2), and costs of an insurance plan (Scenario 9) were among the lowest in perceived fairness and trust in ADM. The other two scenarios about availability on a booking website (Scenario 6) and content moderation on social media (Scenario 7) were also in the group of scenarios with the lowest perceived fairness and trust. For the tendency to question the outcome and negative emotion, these five scenarios were among the highest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal Means by Scenarios</head><p>Contrarily, two scenarios that induced the highest perceived fairness and trust were about the calculation of disease risk (Scenario 8) and failure rates in voice recognition (Scenario 3).</p><p>These two scenarios were the lowest in the tendency to question the outcome and negative emotion.</p><p>In terms of perceived likelihood, most scenarios induced similar degrees of perceived likelihood. But the comparison shows that the scenario about costs of an insurance plan (Scenario 9) was one of the scenarios with the highest perceived likelihoods. On the other hand, the scenario about an immigration kiosk (Scenario 5) and a booking website (Scenario 6) was significantly lower than other scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-binary Participants</head><p>Among the respondents of this study, 18 participants identified themselves as non-binary.</p><p>We conducted additional analyses including the non-binary group to explore the extent to which non-binary participants perceive biases in ADM differently with women and men. The full data set for this analysis included 260, 319, and 18 participants who identified themselves as a man, a woman, and a non-binary person, respectively. The results are shown in <ref type="table" target="#tab_1">Table S2</ref> in the online supporting information. In general, the averages of perceived fairness trust were lower among non-binary participants compared with male participants, controlling for the experimental condition, whereas the average negative emotion and the average tendency to question the outcome were greater among non-binary participants than males. The magnitudes and signs of the coefficients for the non-binary participants suggest that their reactions were in the same direction as women but with greater magnitudes (Model 1 in <ref type="table" target="#tab_1">Table S2</ref>). Also, the magnitudes and signs of the interaction between the non-binary gender group and the condition suggest that the subject-targeting discrimination had greater effects in the non-binary group than in the male group, and the differences might be greater than those between females and males.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The current study examined the roles of the target of discrimination, gender identity, and situational contexts in shaping perceptions of algorithmic discrimination in ADM systems. The results support the hypotheses of the study, providing insight into the factors that influence public perceptions of algorithmic bias and discrimination.</p><p>First, supporting H1, biased automated decisions that discriminated against the subject's gender were perceived as less fair and trustworthy, compared with decisions discriminating against other gender groups. Second, biased automated decisions that discriminated against the subject's gender led individuals to question the outcomes and feel more negative emotions, compared with decisions discriminating against other gender groups, thus, H2 was supported.</p><p>These findings suggest that the target of discrimination plays a crucial role in determining the perception of the discriminatory situation. Consistent with H3, the results also found differences in individual perceptions depending on participants' gender identities, showing that women had lower perceived fairness and trust in ADM than men. Fourth, H4 was supported by the result that women, compared with men, showed a higher tendency to question an outcome, negative emotion, and perceived likelihood. Fifth, we found that responses to biased automated decisions become more negative when situations entail short-term or long-term financial consequences, such as disparity in prices, costs, and job opportunities. Lastly, in additional analysis, we found evidence suggesting that the effect of subject-targeting discrimination could be more pronounced among the non-binary group than among females and males. This aligns with existing evidence of the prevalence of AI bias against gender minorities in society and their reactions to the emerging technology (e.g., <ref type="bibr" target="#b29">Fosch-Villaronga et al., 2021;</ref><ref type="bibr" target="#b108">Vorisek et al., 2023)</ref>.</p><p>It should be highlighted that, although the discrimination target and social identities were found to be strong predictors of the perceptions, general reactions to discriminatory situations were negative regardless of the discrimination target and the subject's gender, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For example, the average trust in ADM among women decreased from 2.87 in the other-targeting situation to 2.53 in the subject-targeting one, but both values are far below 4 on the 7-point scale, indicating overall distrust of the biased algorithms. The average tendency to question the outcome among women, as another example, rose from 4.94 in the other-targeting condition to 5.26 in the subject-targeting one, but these averages are greater than 4, indicating that participants tend to question the outcome in both situations. Thus, one should not erroneously conclude that some gender groups thought discrimination against others was fair, justifiable, or pleasing. This type of potential misinterpretation is not only factually false but also an incorrect representation of what the empirical results depict here.</p><p>The findings of this study have important implications. First, this study revealed that the same discriminatory situation involving algorithms can be perceived very differently depending on how the perceiver's social group is treated in a certain situation. This finding indicates that public perception of algorithms and possible discrimination caused by algorithmic biases should be understood considering social identities, discrimination targets, and situational contexts of use. Although some studies suggest that people are more tolerant and comfortable with machineinduced discrimination <ref type="bibr" target="#b9">(Bigman et al., 2020;</ref><ref type="bibr" target="#b39">Jago &amp; Laurin, 2022)</ref>, this study shows that the level of "tolerance" may depend on these factors.</p><p>Especially, the findings call for the need for careful consideration in taking into account the users' experiences and opinions regarding the target of discrimination and their social characteristics when preventing the creation and use of biased algorithms. This is crucial in ensuring fair and equitable outcomes for all social groups, particularly for those who may be disproportionally and negatively affected by algorithmic discrimination, such as underrepresented and marginalized groups. The reliance on the experience and opinions of majority or advantaged groups, who are less likely to experience algorithmic bias and show weaker responses to potential discrimination, without taking into account the perspectives of underrepresented and under-served groups may result in the perpetuation of algorithmic discrimination. Thus, this study calls for greater inclusivity and diversity in the ADM development process, as well as ongoing evaluation and accountability measures.</p><p>Second, this study found that women have a higher sensitivity to discriminatory ADM outcomes, evidenced by lower perceived fairness and trust; and higher negative emotion and tendency to question automated outcomes among women than men. Their generally negative emotions toward discriminatory situations involving ADM might be related to the fact that women are more likely to experience unfavorable gender discrimination and its negative consequences in their real life. In other words, existing social discrimination, inequality, and injustice and how they are experienced by underserved and marginalized social groups should be at the core of the investigation of new technologies that have large-scale and substantial impacts on society, such as AI.</p><p>Lastly, this study evinces that, people were able to detect social disparities in ADM and sharply adjusted their reactions based on the observed "direction" of discrimination. Participants in this research recognized whether a situation was discriminatory and then reacted with generally negative emotions toward biased decisions. Participants identified who was being discriminated against and responded with clear and sharp decreases in cognitive and emotional acceptance when their social group was targeted. Given that underserved and marginalized groups are more likely to experience biases in automated processes in the real world (e.g., <ref type="bibr" target="#b2">Asplund et al., 2020;</ref><ref type="bibr" target="#b18">L. Chen et al., 2018;</ref><ref type="bibr" target="#b48">Koenecke et al., 2020;</ref><ref type="bibr" target="#b74">Obermeyer et al., 2019)</ref>, expansive exposure to machine-driven discrimination may contribute to strong disagreement about a wide adoption of AI technologies, along the existing line of disparity and inequality in society. It also implies that repeated and expansive exposure to algorithmic bias may lead to the corrosion of public trust and faith in automated processes, which may affect the public's support for AI-related policies.</p><p>The findings underscore the necessity of devising specific policies that account for the nuanced ways in which social identity and the context of discrimination shape perceptions of algorithmic bias. This involves crafting guidelines that guarantee diverse representation and inclusivity, particularly focusing on the impacted individuals in various situations, during the design and testing phases of algorithmic systems. Also, the creation of transparent and equitable processes for evaluating and measuring the risks and significance of algorithm biases, taking into account the varied perceptions of their impacts, is critical. As previously mentioned, relying predominantly on the responses of majority groups and overlooking the detailed context of discrimination may lead to algorithms that reinforce and exacerbate social inequalities. Lastly, it is essential to encourage research into the experiences of diverse social groups with AI, particularly those historically marginalized or underrepresented. Such efforts, which resonate with the growing and urgent calls for AI safety and trust <ref type="bibr" target="#b87">(Sanger &amp; Kang, 2023)</ref>, will foster a more in-depth understanding of AI's societal impacts, an area that remains underexplored but integral to the aims of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The limitations of the present research are as follows. First, the sample was not representative of the U.S. population, which may limit the generalizability of the findings to the broader population. This is a common limitation of studies that use convenience samples, and further research with a more diverse and representative sample would be needed to confirm the current results. Future studies will be able to advance the literature by examining the interplay between other demographic factors, individual backgrounds, and beliefs concerning social issues in relation to the perception of biases in algorithmic outcomes. To this end, future research methodologies may incorporate a range of design strategies, including the use of statistical controls for relevant covariates or the application of representative or quota sampling to ensure the generalizability of findings. Additionally, while this study did not evaluate the processes underlying perceptions of bias in algorithmic decisions, future investigations are encouraged to explore these mechanisms. In such efforts, statistical techniques such as structural equation modeling may prove valuable in disentangling the complex relationships. Second, the study included a limited number of participants identified as non-binary, which may have limited the ability to identify potential differences in perceptions of algorithmic bias among this group.</p><p>Future research should aim to include a larger and more diverse sample of participants to better understand the experiences and perspectives of different gender identities. Lastly, the scenarios used in the study focused on consumer and user contexts, and further research is needed to explore potential algorithmic bias in other domains, such as the justice system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The current study investigated the role of the target of discrimination, gender identity, and situational contexts in shaping perceptions of algorithmic discrimination in ADM systems.</p><p>The findings of this research indicate that social identities and contextual factors play a significant role in shaping individuals' perceptions of and attitudes toward algorithms. As AI continues to advance and become more integrated into our daily lives, it is crucial for policymakers, technology designers, and society as a whole to understand these factors in order to develop effective AI policies, design technology that is more inclusive and beneficial, and anticipate the potential social consequences of AI. By considering the ways in which different groups of people perceive and respond to algorithms in various contexts, we can work towards creating more equitable and beneficial socio-technological systems and policies in the future.     <ref type="bibr" target="#b56">(Lenth, 2022)</ref>. Each pair of a "Other condition" panel and a "Self condition" panel is based on a random effect model based on N = 5,211 responses clustered within 579 subjects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Averages of the Dependent Variables by the Condition and the Subject Gender. A dot represents an average, and an error bar represents the standard error of a mean. *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Estimated Marginal Means of the Dependent Variables. The orange dots indicate estimated marginal means (EMMs). A gray bar indicates a 95% confidence interval of an EMM. Any two EMMs whose black arrows ("comparison arrows") do not overlap are different, and the difference is statistically significant at α = 0.05 with the Bonferroni correction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The effects of the discrimination target on the dependent variables Note. *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05. N = 579. The experimental outcomes were averaged across the nine scenarios for each participant. Unstandardized coefficient (B). Standard error (SE). The reference category for the subject gender is man.</figDesc><table><row><cell></cell><cell>Avg. perceived fairness</cell><cell>Avg. trust in ADM</cell><cell>Avg. negative emotion</cell><cell>Avg. tendency to question</cell><cell>Avg. perceived likelihood</cell></row><row><cell>Model 1 (Main effect)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell></row><row><cell>Target subject</cell><cell>-0.511 (0.088) ***</cell><cell>-0.307 (0.077) ***</cell><cell cols="2">0.771 (0.102) *** 0.161 (0.085)</cell><cell>-0.021 (0.096)</cell></row><row><cell>Female</cell><cell>-0.203 (0.089) *</cell><cell>-0.164 (0.077) *</cell><cell>0.302 (0.102) **</cell><cell>0.258 (0.085) **</cell><cell>-0.066 (0.096)</cell></row><row><cell>Constant</cell><cell>3.549 (0.081) ***</cell><cell>3.019 (0.071) ***</cell><cell cols="3">3.837 (0.094) *** 4.757 (0.078) *** 4.078 (0.088) ***</cell></row><row><cell>Model 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Moderation by</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell></row><row><cell>Gender)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target subject × Female</cell><cell>-0.257 (0.177)</cell><cell>-0.078 (0.155)</cell><cell>0.416 (0.204) *</cell><cell>0.339 (0.170) *</cell><cell>0.408 (0.192) *</cell></row><row><cell>Female</cell><cell>-0.071 (0.127)</cell><cell>-0.124 (0.111)</cell><cell>0.089 (0.146)</cell><cell>0.085 (0.122)</cell><cell>-0.275 (0.138) *</cell></row><row><cell>Target self</cell><cell>-0.368 (0.132) **</cell><cell>-0.264 (0.115) *</cell><cell cols="2">0.542 (0.152) *** -0.026 (0.126)</cell><cell>-0.246 (0.143)</cell></row><row><cell>Constant</cell><cell>3.472 (0.097) ***</cell><cell>2.995 (0.085) ***</cell><cell cols="3">3.962 (0.112) *** 4.859 (0.093) *** 4.201 (0.106) ***</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The effects of the discrimination target on the dependent variables Note. *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05, N = 579. The experimental outcomes were averaged across the nine scenarios for each participant. Unstandardized coefficient (B). Standard error (SE).</figDesc><table><row><cell></cell><cell>Avg. perceived</cell><cell>Avg. trust in</cell><cell>Avg. negative</cell><cell>Avg. tendency to</cell><cell>Avg. perceived</cell></row><row><cell></cell><cell>fairness</cell><cell>ADM</cell><cell>emotion</cell><cell>question</cell><cell>likelihood</cell></row><row><cell>Female subgroup</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell></row><row><cell>Target subject</cell><cell cols="3">-0.626 (0.113) *** -0.342 (0.101) *** 0.957 (0.125) ***</cell><cell>0.312 (0.106) **</cell><cell>0.162 (0.125)</cell></row><row><cell>Constant</cell><cell>3.401 (0.078) ***</cell><cell>2.872 (0.069) ***</cell><cell>4.051 (0.086) ***</cell><cell>4.943 (0.073) ***</cell><cell>3.925 (0.086) ***</cell></row><row><cell>Male subgroup</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell><cell>B (SE)</cell></row><row><cell>Target subject</cell><cell>-0.369 (0.138) **</cell><cell>-0.264 (0.119) *</cell><cell>0.542 (0.167) **</cell><cell>-0.026 (0.136)</cell><cell>-0.246 (0.147)</cell></row><row><cell>Constant</cell><cell>3.472 (0.102) ***</cell><cell>2.995 (0.088) ***</cell><cell>3.962 (0.123) ***</cell><cell>4.859 (0.101) ***</cell><cell>4.201 (0.109) ***</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Justice perceptions of artificial intelligence in selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Acikgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Compagnone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laske</surname></persName>
		</author>
		<idno type="DOI">10.1111/ijsa.12306</idno>
		<ptr target="https://doi.org/10.1111/ijsa.12306" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Selection and Assessment</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="416" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In AI we trust? Perceptions about automated decision-making by artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Helberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kruikemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>De Vreese</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-019-00931-w</idno>
		<ptr target="https://doi.org/10.1007/s00146-019-00931-w" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auditing Race and Gender Discrimination in Online Housing Markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Asplund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International AAAI Conference on Web and Social Media</title>
		<meeting>the Fourteenth International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fairness and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="https://fairmlbook.org" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fitting Linear Mixed-Effects Models Using lme4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.18637/jss.v067.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v067.i01" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Race after technology: Abolitionist tools for the new Jim code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polity</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Cognition (Attribution)-Emotion Model of Violence in Conflict Situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Blair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="350" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0146167292183011</idno>
		<ptr target="https://doi.org/10.1177/0146167292183011" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Algorithmic Discrimination Causes Less Moral Outrage than Human Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arnestad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Psyarxiv</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/m3nrp</idno>
		<ptr target="https://doi.org/10.31234/osf.io/m3nrp" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">It&apos;s Reducing a Human Being to a Percentage&quot;; Perceptions of Justice in Algorithmic Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lyngs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
		<idno type="DOI">10.31235/osf.io/9wqxr</idno>
		<ptr target="https://doi.org/10.31235/osf.io/9wqxr" />
	</analytic>
	<monogr>
		<title level="m">ACM CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2018-01-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Responding to sex-based discrimination: Gender differences in perceived discrimination and implications for legal decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blodorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kordys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group Processes &amp; Intergroup Relations</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/1368430211427172</idno>
		<ptr target="https://doi.org/10.1177/1368430211427172" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can algorithms legitimize discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostinelli</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000294</idno>
		<ptr target="https://doi.org/10.1037/xap0000294" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="459" />
			<date type="published" when="2021" />
			<publisher>APA PsycArticles</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linking precursors of interpersonal trust to human-automation trust: An expanded typology and exploratory experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bobko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Gallimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Trust Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="46" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/21515581.2019.1579730</idno>
		<ptr target="https://doi.org/10.1080/21515581.2019.1579730" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ethical Machine Learning in Healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-biodatasci-092820-114757</idno>
		<ptr target="https://doi.org/10.1146/annurev-biodatasci-092820-114757" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biomedical Data Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="144" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating the Impact of Gender on Rank in Resume Search Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174225</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174225" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trust in AI and Its Role in the Acceptance of AI Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/10447318.2022.2050543</idno>
		<ptr target="https://doi.org/10.1080/10447318.2022.2050543" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cirillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Catuara-Solarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Subirats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mellino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gigante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rementeria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mavridis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-0288-5</idno>
		<ptr target="https://doi.org/10.1038/s41746-020-0288-5" />
	</analytic>
	<monogr>
		<title level="j">Npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algorithmic Transparency in the News Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koliska</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2016.1208053</idno>
		<ptr target="https://doi.org/10.1080/21670811.2016.1208053" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="809" to="828" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sociotechnical view of algorithmic fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dolata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwabe</surname></persName>
		</author>
		<idno type="DOI">10.1111/isj.12370</idno>
		<idno>isj.12370</idno>
		<ptr target="https://doi.org/10.1111/isj.12370" />
	</analytic>
	<monogr>
		<title level="j">Information Systems Journal</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Progress toward gender equality in the United States has slowed or stalled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mishel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1918891117</idno>
		<ptr target="https://doi.org/10.1073/pnas.1918891117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="6990" to="6997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What Motivates Political Preferences? Self-Interest, Ideology, and Fairness in a Laboratory Democracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Esarey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrilleaux</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1465-7295.2011.00394.x</idno>
		<ptr target="https://doi.org/10.1111/j.1465-7295.2011.00394.x" />
	</analytic>
	<monogr>
		<title level="j">Economic Inquiry</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="604" to="624" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automating inequality: How high-tech tools profile, police, and punish the poor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eubanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>St. Martin&apos;s Press</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Testing a Model of Women&apos;s Personal Sense of Justice, Control, Well-Being, and Distress in the Context of Sexist Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Holz</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1471-6402.2010.01576.x</idno>
		<ptr target="https://doi.org/10.1111/j.1471-6402.2010.01576.x" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Women Quarterly</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gendering algorithms in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosch-Villaronga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poulsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Søraa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Custers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="31" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3468507.3468512</idno>
		<ptr target="https://doi.org/10.1145/3468507.3468512" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting the gender wage gap in the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gharehgozli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Atal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eap.2020.04.008</idno>
		<ptr target="https://doi.org/10.1016/j.eap.2020.04.008" />
	</analytic>
	<monogr>
		<title level="j">Economic Analysis and Policy</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grgić-Hlača</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Redmiles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00808</idno>
		<ptr target="http://arxiv.org/abs/2005.00808" />
		<title level="m">Dimensions of Diversity in Human Perceptions of Algorithmic Fairness</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grgic-Hlaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="903" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3178876.3186138</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186138" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Societal bias reinforcement through machine learning: A credit scoring perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Hassani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s43681-020-00026-z</idno>
		<ptr target="https://doi.org/10.1007/s43681-020-00026-z" />
	</analytic>
	<monogr>
		<title level="j">AI and Ethics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Moving beyond &quot;algorithmic bias is a data problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2021.100241</idno>
		<ptr target="https://doi.org/10.1016/j.patter.2021.100241" />
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auditing for Discrimination in Algorithms Delivering Job Ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Imana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="3767" to="3778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3442381.3450077</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450077" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Assumptions About Algorithms&apos; Capacity for Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Laurin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="595" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/01461672211016187</idno>
		<ptr target="https://doi.org/10.1177/01461672211016187" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adapting to Artificial Intelligence: Radiologists and Pathologists as Information Specialists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2353" to="2354" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<idno type="DOI">10.1001/jama.2016.17438</idno>
		<ptr target="https://doi.org/10.1001/jama.2016.17438" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Trustworthy Artificial Intelligence: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uslu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Rittichier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durresi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491209</idno>
		<ptr target="https://doi.org/10.1145/3491209" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unequal Representation and Gender Stereotypes in Image Search Results for Occupations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3819" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2702123.2702520</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702520" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Risk Clusters, Hotspots, and Spatial Intelligence: Risk Terrain Modeling as an Algorithm for Police Resource Allocation Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Caplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Piza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Quantitative Criminology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="362" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10940-010-9126-2</idno>
		<ptr target="https://doi.org/10.1007/s10940-010-9126-2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Racial disparities in automated speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koenecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nudell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quartey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mengesha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rickford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="7684" to="7689" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1915768117</idno>
		<ptr target="https://doi.org/10.1073/pnas.1915768117" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Algorithmic Bias? An Empirical Study of Apparent Gender-Based Discrimination in the Display of STEM Career Ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.2018.3093</idno>
		<ptr target="https://doi.org/10.1287/mnsc.2018.3093" />
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2966" to="2981" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951718756684</idno>
		<ptr target="https://doi.org/10.1177/2053951718756684" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Algorithmic Mediation in Group Decisions: Fairness Perceptions of Algorithmically Mediated vs. Discussion-Based Social Division</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baykal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2998181.2998230</idno>
		<ptr target="https://doi.org/10.1145/2998181.2998230" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
		<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1035" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3411764.3445570</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445570" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">emmeans: Estimated Marginal Means, aka Least-Squares Means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Lenth</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=emmeans" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revealing Factors Influencing Students&apos; Perceived Fairness: A Case with a Predictive System for Math Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491140.3528293</idno>
		<ptr target="https://doi.org/10.1145/3491140.3528293" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM Conference on Learning @ Scale</title>
		<meeting>the Ninth ACM Conference on Learning @ Scale</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2018.12.005" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Resistance to Medical Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/jcr/ucz013</idno>
		<ptr target="https://doi.org/10.1093/jcr/ucz013" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Antecedents and consequences of attributions to discrimination: Theoretical and empirical advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Quinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mccoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Advances in Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="251" to="330" />
			<date type="published" when="2002" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S0065-2601(02</idno>
		<ptr target="https://doi.org/10.1016/S0065-2601(02" />
		<imprint>
			<biblScope unit="page" from="80007" to="80014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attributions to discrimination and selfesteem: Impact of group identification and situational ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Quinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmader</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-1031</idno>
		<ptr target="https://doi.org/10.1016/S0022-1031" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="551" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Detecting Race and Gender Bias in Visual Representation of AI on Web Search Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makhortykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Urman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ulloa</surname></persName>
		</author>
		<editor>L. Boratto, S. Faralli, M. Marras, &amp; G</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stilo</surname></persName>
		</author>
		<title level="m">Advances in Bias and Fairness in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="36" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-78818-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78818-6_5" />
		<imprint>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Racial/Ethnic and Gender Disparities in Health Care Use and Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Manuel</surname></persName>
		</author>
		<idno type="DOI">10.1111/1475-6773.12705</idno>
		<ptr target="https://doi.org/10.1111/1475-6773.12705" />
	</analytic>
	<monogr>
		<title level="j">Health Services Research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1407" to="1429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bias and Fairness in Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<idno type="DOI">10.1109/SIBGRAPI54419.2021.00041</idno>
		<ptr target="https://doi.org/10.1109/SIBGRAPI54419.2021.00041" />
	</analytic>
	<monogr>
		<title level="m">34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How Targeted Ads and Dynamic Pricing Can Perpetuate Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hosanagar</surname></persName>
		</author>
		<ptr target="https://hbr.org/2019/11/how-targeted-ads-and-dynamic-pricing-can-perpetuate-bias" />
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2019-11-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Is an Algorithm Less Racist Than a Loan Officer? The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2020/09/18/business/digital-mortgages.html" />
		<imprint>
			<date type="published" when="2020-09-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Perceived Reciprocity and Well-Being at Work in Non-Professional Employees: Fairness or Self-Interest?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Martínez-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Peiró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cropanzano</surname></persName>
		</author>
		<idno type="DOI">10.1002/smi.2421</idno>
		<ptr target="https://doi.org/10.1002/smi.2421" />
	</analytic>
	<monogr>
		<title level="j">Stress and Health</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="39" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Digital technologies and artificial intelligence&apos;s present and foreseeable impact on lawyering, judging, policing and law enforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nissan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-015-0596-5</idno>
		<ptr target="https://doi.org/10.1007/s00146-015-0596-5" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Algorithms of oppression: How search engines reinforce racism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Noble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>New York University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6464</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<idno type="DOI">10.1126/science.aax2342</idno>
		<ptr target="https://doi.org/10.1126/science.aax2342" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Gender bias perpetuation and mitigation in AI technologies: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-023-01675-4</idno>
		<ptr target="https://doi.org/10.1007/s00146-023-01675-4" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Weapons of math destruction: How big data increases inequality and threatens democracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O'neil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>First edition. Crown</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Competent Men and Warm Women: Gender Stereotypes and Backlash in Image Search Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6620" to="6631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3025453.3025727</idno>
		<ptr target="https://doi.org/10.1145/3025453.3025727" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Gender discrimination comes in many forms for today&apos;s working women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/fact-tank/2017/12/14/gender-discrimination-comes-in-many-forms-for-todays-working-women/" />
	</analytic>
	<monogr>
		<title level="j">Pew Research Center</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Likelihood of Questioning AI-based Recommendations Due to Perceived Racial/Gender Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dennehy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TTS.2021.3120303</idno>
		<ptr target="https://doi.org/10.1109/TTS.2021.3120303" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Technology and Society</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Why benefiting from discrimination is less recognized as discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="825" to="852" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/pspi0000298</idno>
		<ptr target="https://doi.org/10.1037/pspi0000298" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09124</idno>
		<ptr target="http://arxiv.org/abs/1712.09124" />
		<title level="m">Demographics and discussion influence views on algorithmic fairness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Objectivity in the Eye of the Beholder: Divergent Perceptions of Bias in Self Versus Others</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="781" to="799" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0033-295X.111.3.781</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.111.3.781" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Sanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<title level="m">Biden to Issue First Regulations on Artificial Intelligence Systems. The New York Times</title>
		<imprint>
			<date type="published" when="2023-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Gender Discrimination and Job-Related Outcomes: A Cross-Cultural Comparison of Working Women in the United States and China</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R W</forename><surname>Joplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vocational Behavior</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="427" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<idno type="DOI">10.1006/jvbe.1999.1748</idno>
		<ptr target="https://doi.org/10.1006/jvbe.1999.1748" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Attributions of morality and mind to artificial intelligence after real-world moral violations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Shank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.chb.2018.05.014</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2018.05.014" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The attribution of blame: Causality, responsibility, and blameworthiness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Shaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Are Outcome Fairness and Outcome Favorability Distinguishable Psychological Constructs? A Meta-Analytic Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Justice Research</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Gender discrimination in the United States: Experiences of women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Steelfisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Findling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Blendon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Sayde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Services Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<biblScope unit="page" from="1442" to="1453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title/>
		<idno type="DOI">10.1111/1475-6773.13217</idno>
		<ptr target="https://doi.org/10.1111/1475-6773.13217" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Violating Equality in Social Dilemmas: Emotional and Retributive Reactions as a Function of Trust, Attribution, and Honesty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="894" to="906" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0146167206287538</idno>
		<ptr target="https://doi.org/10.1177/0146167206287538" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Microaggressions in everyday life: Race, gender, and sexual orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Sue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Google may send Republicans to spam, but we&apos;re holding Big Tech accountable | Fox News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thune</surname></persName>
		</author>
		<ptr target="https://www.foxnews.com/opinion/google-may-send-republicans-spam-holding-big-tech-accountable" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">My Friends, Editors, Algorithms, and I: Examining audience attitudes to news selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Helberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trilling</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2018.1493936</idno>
		<ptr target="https://doi.org/10.1080/21670811.2018.1493936" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="469" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<title level="m">These robots were trained on AI. They became racist and sexist</title>
		<imprint>
			<date type="published" when="2022-07-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Washington</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">The impact of gender discrimination on a Woman&apos;s Mental Health. EClinicalMedicine, 20, 100311</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Vigod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Rochon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eclinm.2020.100311</idno>
		<ptr target="https://doi.org/10.1016/j.eclinm.2020.100311" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Racist Algorithms or Systemic Problems? Risk Assessments and Racial Disparities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Viljoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Criminal Justice and Behavior</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1576" to="1584" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0093854820954501</idno>
		<ptr target="https://doi.org/10.1177/0093854820954501" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Financial inequality and gender in older people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachantoni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.maturitas.2012.02.015</idno>
		<ptr target="https://doi.org/10.1016/j.maturitas.2012.02.015" />
	</analytic>
	<monogr>
		<title level="j">Maturitas</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="104" to="107" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Propagation of societal gender inequality by internet search algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vlasceanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Amodio</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2204529119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2204529119" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Artificial Intelligence Bias in Health Care: Web-Based Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Vorisek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stellmach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A I</forename><surname>Klopfenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bures</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henningsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thun</surname></persName>
		</author>
		<idno type="DOI">10.2196/41089</idno>
		<ptr target="https://doi.org/10.2196/41089" />
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Keya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="134" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3490099.3511108</idno>
		<ptr target="https://doi.org/10.1145/3490099.3511108" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376813</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376813" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">The Masculinity of Money: Automatic Stereotypes Predict Gender Differences In Estimated Salaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Elizabeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spencer-Rodgers</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1471-6402.2009.01537.x</idno>
		<ptr target="https://doi.org/10.1111/j.1471-6402.2009.01537.x" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Women Quarterly</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="20" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Thumbs Up or Down: Consumer Reactions to Decisions by Algorithms Versus Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Puntoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Van Osselaer</surname></persName>
		</author>
		<idno type="DOI">10.1177/00222437211070016</idno>
		<ptr target="https://doi.org/10.1177/00222437211070016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="696" to="717" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Framing the challenges of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjqs-2018-008551</idno>
		<ptr target="https://doi.org/10.1136/bmjqs-2018-008551" />
	</analytic>
	<monogr>
		<title level="j">BMJ Quality &amp; Safety</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="238" to="241" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Examining perceptions towards hiring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yencha</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.techsoc.2021.101848</idno>
		<ptr target="https://doi.org/10.1016/j.techsoc.2021.101848" />
	</analytic>
	<monogr>
		<title level="j">Technology in Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
