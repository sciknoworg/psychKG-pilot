<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Network of Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-06-24">June 24, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nebraska Omaha</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nebraska Omaha</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dr</roleName><forename type="first">Michael</forename><surname>Cortese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nebraska Omaha</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Network of Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-24">June 24, 2024</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Humans are found to display a bias when they are doing deductive reasoning in the Wason selection task (WST). Specifically, when asked to identify evidence that disproves a rule (e.g., that a card with an even number on one side must contain a vowel on the opposite side), humans perform better when the task uses real-life examples than abstract examples. The mechanism of this reasoning bias remained unclear. One study used a parallel distributed processing (PDP) model to model the human-like bias but failed. The possible reason for the failure was that semantic similarities between words/concepts used in the task could induce the bias, and the semantic similarity among learned concepts was not simulated in the PDP model. Regardless, PDP offers an exciting avenue to examine decision making due to its theoretical and biological plausibility. And so, the current study used a PDP approach to model the deductive reasoning behaviors in the Wason selection task. Different from the previous PDP model study, we trained the PDP model with a WST under two semantic distance conditions: close and far. We found that the PDP model learned to choose &quot;p and not q&quot; under the close semantic distance condition and &quot;p and q&quot; under the long semantic distance condition. However, unlike human participants, the PDP model did not find it more difficult to choose &quot;p and not q&quot; over &quot;p and q&quot; in either condition. Future studies might explore how the model&apos;s cause-effect experiences without proper feedback prior to exposure on the Wason selection task may produce biased outcomes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Parallel Distributed Processing Network of Human Reasoning</head><p>One of the greatest mysteries in psychology is the cognitive mechanism of human reasoning. People display the extraordinary ability of reasoning to solve complicated problems such as math problems and everyday issues like predicting the weather. However, the reasoning ability in humans was questioned by the results of the Wason selection task (WST). The standard WST first presents participants with a conditional rule, such as "If there is a vowel on one side, then there is an even number on the other side." Participants are then required to choose two cards from four cards (e.g., E, K, 4, 7) with one visible side to prove the rule <ref type="bibr" target="#b17">(Wason, 1966)</ref>. The rule and four cards can be logically represented as "if p then q" and "p, q, not p, not q." Instead of choosing the correct answer (p and not q), most participants displayed a preference for cards that matched the rule (p and q), which was called "matching bias" <ref type="bibr" target="#b13">(Ragni et al., 2018;</ref><ref type="bibr" target="#b18">Wason, 1968</ref>). The matching bias raised the question of whether humans are rational, given their poor performance in reasoning tasks <ref type="bibr" target="#b11">(Oberauer et al., 1999)</ref>. Several theories attempted to explain this bias using algorithmic accounts, but many of them fail to explain most empirical data and have specific assumptions <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001;</ref><ref type="bibr" target="#b13">Ragni et al., 2018)</ref>. For example, the information gain model, which argued that matching bias was explained by people interpreting the WST as an inductive reasoning task rather than a deductive reasoning task, fits poorly with several experimental data <ref type="bibr">(Oaksford &amp; Chater, 1995;</ref><ref type="bibr" target="#b11">Oberauer et al., 1999)</ref>. Additionally, the rarity assumption, which assigns high information to p and q, lacks justification for the cognitive meaning and contradicts some experimental results <ref type="bibr" target="#b11">(Oberauer et al., 1999)</ref>.</p><p>Different from most models in the WST, Parallel Distributed Processing (PDP) models have great potential to explain the matching bias due to their theoretical basis and biological plausibility. Firstly, one basic assumption in the PDP framework is that cognitive processes and representations emerge from the interaction of simple units, which may little resemble the apparent behavior of the system, while symbolic models align well with the system-level behavior without capturing the features of the underlying process <ref type="bibr" target="#b16">(Rogers &amp; McClelland, 2014)</ref>.</p><p>Given the contradictions in current theories regarding the matching bias of the WST, which were constructed at the system level, the PDP model might provide a useful tool for understanding human reasoning. Furthermore, the distributed representation in the PDP model could shed light on cognitive neuroscience, such as how the anterior temporal lobe represents semantic knowledge <ref type="bibr" target="#b15">(Rogers &amp; McClelland, 2004)</ref>. Only one study used the PDP model to simulate behavior in the WST and failed to replicate the matching bias <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001)</ref>.</p><p>Although a larger number of hidden units was enough to generate "not q" than "p," which could imply more difficulty in choosing the falsifiable option, the matching bias should emerge from the same neural network since individual differences in the brain were not a key factor in participants' performance. Given that PDP models learn the task through pattern classification, categorizing input data to match the output response, which could explain reasoning behavior because people make use of categorization to reason <ref type="bibr">(Bechtel &amp; Abrahamsen, 1991;</ref><ref type="bibr">Gobet &amp; Simon, 1998;</ref><ref type="bibr">Goldstone &amp; Barsalou, 1998)</ref>. Therefore, a PDP model is theoretically possible to learn the conditional rule within the network when only the cards and the response are provided.</p><p>Additionally, poor performance in reasoning tasks appears more in abstract contexts than in real-world contexts <ref type="bibr">(Cheng and Holyoak, 1985;</ref><ref type="bibr">Cosmides, 1989;</ref><ref type="bibr">Cosmides and Tooby, 1992)</ref>.</p><p>A recent study about the reasoning ability in language models, which used WST as one reasoning task, also performed better in realistic contexts than in abstract contexts <ref type="bibr" target="#b1">(Dasgupta, 2022)</ref>. Together, irrational reasoning trend under abstract contexts implies that semantic information in WST might affect performance. To investigate more about the reason behind the A Network of Reasoning Li 5 matching bias, it is necessary to encode a WST with various semantic contexts, which could be quantified. Semantic distance is a quantitative measurement of the distance between concepts or words, which could be used in determining the semantic similarities of words used in WST.</p><p>Furthermore, the previous PDP model only used WST under abstract contexts to model the reasoning <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001</ref>). Therefore, using WST is essential to model human-like behaviors.</p><p>The current study aims to explain the striking bias from human reasoning behaviors in the Wason selection task by building a PDP model. The study hypothesized that the PDP model would display human-like responses in WST if different semantic information was involved, especially when items used in WST are semantically distant. Besides, we expect to observe different learning curves between the close semantic distance condition and the long semantic distance condition. The PDP model should learn faster under the close semantic distance condition than the long semantic distance condition. Moreover, Analyzing the activities of hidden units could help to know if the model learns certain features from WST and also bridge the representation of the PDP model and the evidence found in cognitive neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Encoding</head><p>In task encoding for the Wason selection task, each trial had four words as the input matching the four cards presented to the human participants. To manipulate the semantic distance in WST, we chose four major kinds of words: animals, plants, inorganic matter, and artificial objects to construct the rule of each WST trial. Since animals and plants are more semantically closed than animal and inorganic matter, "if animals then plant" was considered as the semantically closed rule, and "if animals then inorganic matter" was considered as the semantically distant rule". There were 12 types of rules in total, including four semantically closed rules and eight semantically far rules <ref type="table">(Table 1)</ref>. With ten instances in each kind of word, these rules formulated a set with 32400 unique input patterns. 70% of the input patterns compromised the training set while the left part was the testing set. Input patterns were selected randomly. To balance the number of trials with semantically closed rules and the number of trials with semantically far rules in the set, we reversed the order of semantically closed trials to double the amounts of the semantically closed trials with the same rule and semantic distance.</p><p>For example, if the original trial used "if lion then rose", then the reversed trial would use "if rose then lion". A binary output was used, "0" represented the "p and q" (the biased answer), and "1"</p><p>represented "p and not q" (the correct answer). Correct outputs for each trial in the training set were assigned according to the semantic distances to match the human performance. "0" was assigned to semantically distant trials and "1" was assigned to semantically closed trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Structure</head><p>The PDP model was structured as a three-layer feedforward network, encompassing an input layer, a hidden layer, and an output layer. Weighted connections existed between each unit in the input level and hidden level and between the hidden level and the output level. The model received four words as inputs which represented the specific card instances. At the start of the task, a language model called Glove was used to gain the mathematical representation of words, each represented by a vector with 300 numbers in length <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>. Therefore, the input layer had 1200 units in each trial. After activating the input layer, all vectors flowed to the hidden layer, which contained four units. A bigger hidden unit layer was used here compared to the previous PDP model because the semantic information was included, which made the task A Network of Reasoning Li 9 more complicated. In the output layer, the activity of each unit was summed and computed by the ReLU function. The ReLU function served as the activation function to introduce nonlinearity, and the cross-entropy function was adopted as the loss function for backpropagation <ref type="bibr" target="#b4">(Fukushima, 1975;</ref><ref type="bibr" target="#b5">Gneiting&amp; Raftery, 2007)</ref>. When the loss function converged, the training phase ended, and all weights were frozen. During the testing phase, the generated answers from the PDP model would be compared to the correct answers, and the accuracy rate would be calculated.</p><p>The initial weight and bias were selected randomly from a normal distribution (μ = 0, σ = 0.1). Model construction and training were implemented using the PyTorch 1.9.0 open-source package within the Python 3.0 language (Paszke et al., 2019). The learning rate was set to be .01 in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Representation</head><p>Neural representations of the hidden layer were analyzed by the density jitter plots. Separated ranges of activities from different hidden units formed a "band" in density jitter plots, which implied that different units represented different functions <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training and Testing</head><p>To find if the PDP model successfully learned the WST task, the loss function and the accuracy were recorded in every trial <ref type="figure" target="#fig_0">(Fig. 2 &amp; Fig. 3)</ref>. The x-axis represents the number of epochs, and the y-axis represents the change of loss function. The loss function was below 0.001 after 8 epochs that could be considered as converged under the normal standard. accuracy, which was also the learning curve of the PDP model in WST. Both the training accuracy and the testing accuracy were up to 1.0 around Eight epochs, which highly suggested that the PDP model already learned the structure of the WST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Li 10</head><p>To further determine if the PDP models had different learning curves under close semantic distance condition and long semantic distance condition, the training loss and the accuracy were separated by semantic distance <ref type="figure">(Fig. 4 &amp; Fig. 5</ref>). Blue lines represent the close semantic distance condition and orange lines represent the long semantic distance condition.</p><p>Both the x-axis and y-axis remain the same meaning as the previous figures. <ref type="figure">Fig. 4</ref> Training Loss between Different Semantic Distances Both semantically closed trials and far trials decreased the training loss at a similar rate which indicated that the model converged regardless of different semantic distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5 Training Loss between Different Semantic Distances</head><p>Both accuracies of semantically closed trials and far trials were close to 1 in epoch 5 and remained 1 from epoch 6 to 10. Besides, their accuracies changed at a similar speed. Clearly, the PDP model kept the same learning trend in WST regardless of the semantic distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Representation of Hidden Units</head><p>To figure out if four hidden units of the PDP model learn distinct features about WST, the activities of each hidden unit were analyzed by density jittered plots. The x-axis of the density jittered plots would be the activities of each hidden unit and the y-axis would be a Li 12 random number in the range from 0 to 1. <ref type="figure">Fig. 6</ref> presents the activities of all hidden units of all trials. Blue dots represented Hidden Unit 1, red dots represented Hidden Unit 2, green dots represented Hidden Unit 3, and purple dots represented Hidden Unit 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6 Activities of Hidden Units</head><p>Distributions of hidden unit activities were stable after epoch 5 while the model converged around epoch 5. Even though there were no bands in these plots, Hidden Unit 4 showed a special range of activities after epoch 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This study is about modeling the reasoning bias of the Wason selection task under various semantic contexts by using the parallel distributed processing network. Firstly, the result showed that the PDP model was able to display the human-like bias when it was doing the WST under the semantically far context. The previous PDP model failed to generate the biased response ("p and q") while our PDP model performed closer to humans <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001</ref>). This result was consistent with our first hypothesis. Secondly, this PDP model did not learn faster under the semantically close condition than the semantically far condition, which did not support our second hypothesis. Finally, activities of hidden units did have different A Network of Reasoning Li 13 distribution ranges but were highly overlapped, which suggested that hidden units did not represent task features separately.</p><p>The PDP model learned the WST with the same speed under different semantic distance conditions suggested that choosing the "p and not q" was not harder than choosing "p and q", which was also aligned with the previous study of PDP network but violated the humans result <ref type="bibr" target="#b8">(Leighton &amp; Dawson, 2001</ref>). Humans are found to have difficulty selecting the negate response unless the context is meaningful <ref type="bibr" target="#b3">(Evans et al., 1993)</ref>. One possible reason that our PDP model did not learn difficultly choosing the "p and not q" could be that our semantic conditions were highly semantically related but were not real-life related. Future studies could test the PDP model on WST with semantically related conditions and real-life related conditions and compare the learning curves. The PDP model might make it easier to choose the falsifying answer ("p and not q") when the context is real-life related rather than only semantic-related.</p><p>One possible account for the overlapped representations of hidden units could be the information was highly compressed here. Since previous PDP model used the binary code to encode every word used in the WST while our PDP model used a language model to encode the words into vectors, which could lead to more complex information. This difference might cause the highly overlapped activity ranges of hidden units. Future studies are suggested to use a new mathematical method to analyze the activities of hidden units.</p><p>In conclusion, our study found that the PDP model could be trained to choose the falsifying answer under the semantically closed condition, which is similar to human behaviors. However, our model did not show the extra difficulty in choosing the falsifying answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Training Loss Fig. 3 Training and Testing Accuracy The x-axis represents the number of epochs, and the y-axis represents the change of in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Semantic Rules SemanticSemantic Distances of All Instances</head><label></label><figDesc>distances between all instances were computed by SemDis, an open platform that could compute semantic distance by using semantic models<ref type="bibr" target="#b0">(Beaty &amp; Johnson, 2021)</ref>.Table 2shows the semantic distances of all trials.</figDesc><table><row><cell cols="2">lavender silica</cell><cell>panda cactus</cell><cell>0.96225 0.87546</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">cactus iron</cell><cell>dolphin tulip</cell><cell>0.93933 0.9545</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">tulip carbon</cell><cell>penguin cabbage</cell><cell>0.93237 0.97623</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">cabbage magnet</cell><cell>cheetah sunflower</cell><cell>0.98688 0.89831</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">sunflower aluminum</cell><cell>hippopotamus orchid</cell><cell>0.82658 0.95557</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">orchid car</cell><cell>ostrich rose</cell><cell>0.85275 0.88007</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">car computer</cell><cell>water oak</cell><cell>0.87814 0.98472</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">computer bridge</cell><cell>calcium tomato</cell><cell>0.98077 0.97654</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">bridge piano</cell><cell>sodium bamboo</cell><cell>0.96696 0.86667</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">piano airplane</cell><cell>acid lavender</cell><cell>0.92459 1.01532</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">airplane satellite</cell><cell>potassium cactus</cell><cell>1.00646 0.97731</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">satellite keyboard</cell><cell>silica tulip</cell><cell>0.94825 0.9566</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">keyboard notebook</cell><cell>iron cabbage</cell><cell>0.98913 0.94231</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">notebook ambulance</cell><cell>carbon sunflower</cell><cell>0.94129 0.99888</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">ambulance bed</cell><cell>magnet orchid</cell><cell>0.96931 0.94297</cell><cell></cell><cell>close far</cell></row><row><cell>bed</cell><cell></cell><cell>aluminum Table 2</cell><cell>0.93134</cell><cell></cell><cell>close</cell></row><row><cell cols="2">Rules: if p then q water</cell><cell>lion</cell><cell>p 0.91642</cell><cell>q</cell><cell>far</cell></row><row><cell cols="3">if animals then plants calcium elephant</cell><cell>animals 0.9754</cell><cell cols="2">plants far</cell></row><row><cell cols="3">if plants then animals sodium giraffe</cell><cell>plants 1.00007</cell><cell cols="2">animals far</cell></row><row><cell>acid</cell><cell cols="2">if inorganic matter then artificial objects kangaroo</cell><cell>inorganic matter 0.92398</cell><cell cols="2">artificial objects far</cell></row><row><cell cols="3">if artificial objects then inorganic matter potassium panda</cell><cell>artificial objects 0.99336</cell><cell cols="2">inorganic matter far</cell></row><row><cell cols="3">if animals then artificial objects silica dolphin</cell><cell>animals 0.98085</cell><cell cols="2">artificial objects far</cell></row><row><cell>iron</cell><cell cols="2">if animals then inorganic matter penguin</cell><cell>animals 0.99467</cell><cell cols="2">inorganic matter far</cell></row><row><cell cols="3">if plants then artificial objects carbon cheetah</cell><cell>plants 0.93908</cell><cell cols="2">artificial objects far</cell></row><row><cell cols="3">if plants then inorganic matter magnet hippopotamus</cell><cell>plants 0.94927</cell><cell cols="2">inorganic matter far</cell></row><row><cell cols="3">if artificial objects then animals aluminum ostrich</cell><cell>artificial objects 0.92081</cell><cell cols="2">animals far</cell></row><row><cell>car</cell><cell cols="2">if artificial objects then plants lion</cell><cell>artificial objects 0.94233</cell><cell cols="2">plants far</cell></row><row><cell cols="3">if inorganic matter then animals computer elephant</cell><cell>inorganic matter 0.9745</cell><cell cols="2">animals far</cell></row><row><cell cols="3">if inorganic matter then plants bridge giraffe</cell><cell>inorganic matter 0.94652</cell><cell cols="2">plants far</cell></row><row><cell cols="2">piano</cell><cell>kangaroo</cell><cell>0.95498</cell><cell></cell><cell>far</cell></row><row><cell cols="2">airplane</cell><cell>panda</cell><cell>0.95818</cell><cell></cell><cell>far</cell></row><row><cell cols="2">satellite</cell><cell>dolphin</cell><cell>0.91999</cell><cell></cell><cell>far</cell></row><row><cell cols="2">keyboard</cell><cell>penguin</cell><cell>0.96463</cell><cell></cell><cell>far</cell></row><row><cell cols="2">notebook</cell><cell>cheetah</cell><cell>0.93958</cell><cell></cell><cell>far</cell></row><row><cell cols="2">ambulance</cell><cell>hippopotamus</cell><cell>0.95807</cell><cell></cell><cell>far</cell></row><row><cell>bed</cell><cell></cell><cell>ostrich</cell><cell>0.9414</cell><cell></cell><cell>far</cell></row><row><cell cols="2">p water</cell><cell>q rose</cell><cell>Semantic Distance 0.87907</cell><cell></cell><cell>Trial Type far</cell></row><row><cell cols="2">rose calcium</cell><cell>lion oak</cell><cell>0.89436 0.9542</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">oak sodium</cell><cell>elephant tomato</cell><cell>0.89453 0.87141</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">tomato acid</cell><cell>giraffe bamboo</cell><cell>0.93033 0.91475</cell><cell></cell><cell>close far</cell></row><row><cell cols="2">bamboo potassium</cell><cell>kangaroo lavender</cell><cell>0.85047 0.88707</cell><cell></cell><cell>close far</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automating creativity assessment with SemDis: An open platform for computing semantic distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Beaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="757" to="780" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models show human-like content effects on reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07051</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching bias in the selection task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lynch</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8295.1973.tb01384.x</idno>
		<ptr target="https://doi.org/10.1111/j.2044-8295.1973.tb01384.x" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="397" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human Reasoning: The psychology of deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">B T</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Newstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M J</forename><surname>Byrne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Erlbaum</publisher>
			<pubPlace>Hove</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cognition: A self-organizing multilayered neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00342633</idno>
		<ptr target="https://doi.org/10.1007/BF00342633" />
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Strictly proper scoring rules, prediction, and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">477</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1198/016214506000001437</idno>
		<ptr target="https://doi.org/10.1198/016214506000001437" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/s13428-020-01453-w</idno>
		<ptr target="https://doi.org/10.3758/s13428-020-01453-w" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A parallel distributed processing model of Wason&apos;s selection task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="231" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S1389-0417(01</idno>
		<ptr target="https://doi.org/10.1016/S1389-0417(01" />
		<imprint>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A rational analysis of the selection task as optimal data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oaksford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-</idno>
		<ptr target="https://doi.org/10.1037/0033-" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="631" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian rationality for the Wason selection task? A test of optimal data selection theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.1080/135467899393139</idno>
		<ptr target="https://doi.org/10.1080/135467899393139" />
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="144" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GLOVE: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/d14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On selecting evidence to test hypotheses: A theory of selection tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="779" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/bul0000146</idno>
		<ptr target="https://doi.org/10.1037/bul0000146" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semantic Cognition: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parallel distributed processing at 25: Further explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12148</idno>
		<ptr target="https://doi.org/10.1111/cogs.12148" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1024" to="1077" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Harmondsworth: Penguin Books</title>
		<editor>B. Foss</editor>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
	<note>New Horizons in Psychology</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning about a rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="281" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
