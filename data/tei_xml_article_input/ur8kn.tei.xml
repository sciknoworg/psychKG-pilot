<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A comparison of distributed machine learning methods for the support of &quot;Many Labs&quot; collaborations in computational modelling of decision making</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-28">September 28, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhang</surname></persName>
							<email>lili.zhang27@mail.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Insight SFI Research Centre for Data Analytics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Vashisht</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Totev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Trinh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Ward</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Insight SFI Research Centre for Data Analytics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Glasnevin Campus</addrLine>
									<settlement>Dublin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A comparison of distributed machine learning methods for the support of &quot;Many Labs&quot; collaborations in computational modelling of decision making</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-28">September 28, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Deep learning models, especially RNN models, are potentially powerful tools for representing the complex learning processes and decision-making strategies used by humans. Such neural network models make fewer assumptions about the underlying mechanisms thus providing experimental flexibility in terms of applicability. However this comes at the cost of requiring a larger number of tunable parameters requiring significantly more training and representative data for effective learning. This presents practical challenges given that most computational modelling experiments involve relatively small numbers of subjects, which while adequate for conventional modelling using low dimensional parameter spaces, leads to sub-optimal model training when adopting deeper neural network approaches. Laboratory collaboration is a natural way of increasing data availability however, data sharing barriers among laboratories as necessitated by data protection regulations encourage us to seek alternative methods for collaborative data science. Distributed learning, especially federated learning, which supports the preservation of data privacy, is a promising method for addressing this issue. To verify the reliability and feasibility of applying federated learning to train neural networks models used in the characterisation of human decision making, we conducted experiments on a real-world, many-labs data pool including experimentally significant data-sets from ten independent studies. The performance of single models that were trained on single laboratory data-sets was poor, especially those with small numbers of subjects. This unsurprising finding supports the need for larger and more diverse data-sets to train more generalised and reliable models. To that end we evaluated four collaborative approaches for comparison purposes. The first approach represents conventional centralized data sharing (CL-based) and is the optimal approach but requires complete sharing of data which we wish to avoid. The results however establish a benchmark for the other three distributed approaches; federated learning (FL-based), incremental learning (IL-based), and cyclic incremental learning (CIL-based). We evaluate these approaches in terms of prediction accuracy and capacity to characterise human decision-making strategies in the context of the computational modelling experiments considered here. The results demonstrate that the FL-based model achieves performance most comparable to that of a centralized data sharing approach. This demonstrate that federated learning has value in scaling data science methods to data collected in computational modelling contexts in circumstances where data sharing is not convenient, practical or permissible.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>One of the most important tasks in cognitive psychology and behavioural science is to understand and explain how people think, infer and process information. This is generally achieved through observing their behaviour under experimental settings. In the field of human decision making, where we evaluate how people make decisions based on learning from experience, we can either analyse the process by which information is itself processed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or investigate how the expectation values of options are learned and updated through experience <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In such approaches, we use cognitive models that specify a set of assumptions about the underlying learning processes used by the subjects, thus generally requiring manual engineering with an iterative process to examine the consistency between the hypothesis and the empirical data <ref type="bibr" target="#b4">[5]</ref>. The approach may fail if subjects adopt a completely different strategy during decision making. Deep Learning (DL) models, especially Recurrent Neural Network (RNN) models, present an alternative approach to the characterisation of human decision making behavior. They can automatically capture behavioural trends exhibited by subjects without strong assumptions about the mathematical structure of the underlying process through taking advantage of their data-driven design and higher capacity for representing complex computational processes <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>Unfortunately, this does not come for free since modern DL models involve the learning of a large number of parameters which in turn requires large amounts of diverse data for effective performance. For example, in a recent medical study <ref type="bibr" target="#b7">[8]</ref>, it was found that DL models overfit when trained on small and biased institutional data and generalized poorly on data from institutions whose data were not seen during training. Although the data used in that study comprised of medical images rather than behavioural data, it is a good example and a warning that DL models need to be carefully trained. Efforts must be made to minimize the introduction of confounding factors associated with experimental biases which can dominate the training process and obscure the relationship of predictions to the underlying targeted pathology. Such models may perform with good accuracy when testing against held-out data from the same experiment, but may not generalize well to the same experiments conducted in other studies. A natural way to increase both data size and diversity is through collaborative learning, in which multiple laboratories cooperate and contribute data to train a global model together. It is an effective approach that has the potential to solve the limitation in most cognitive studies where the number of subjects is adequate for the modelling approach originally intended but then becomes inadequate when the same data is used to train new models involving a larger number of parameters. Furthermore, one of the significant challenges in computational psychiatry <ref type="bibr" target="#b8">[9]</ref>, i.e. translating advances in understanding the cognitive biases of people with mental illness into improvements in clinical practice, will also benefit from the pattern of multi-experiment collaboration, given that the scale of current cognitive studies is not sufficient for developing robust models that satisfy clinical needs.</p><p>One of the most conventional options for conducting multi-experimental analysis requires different laboratories to share subjects' data to a centralized location for model training (i.e. centralized training). However, in practice, data sharing is not easy, especially when involving larger number of laboratories from different legal jurisdictions, due to privacy, ethical, and data regulation barriers <ref type="bibr" target="#b9">[10]</ref>. Consequently, information coming from various populations worldwide remains distributed in isolation across multiple laboratories, highlighting the need to seek alternative approaches. The solution for addressing this problem is distributed model training, in other words, sharing model parameters instead of data, as the model itself does not contain any individually-identifiable information. One approach for distributed training is unparallel, or as we will refer to it here -incremental distributed learning (IL) <ref type="bibr" target="#b10">[11]</ref>.</p><p>With this approach each laboratory trains the model and passes the learned model parameters to the next laboratory for training on its data, until all have trained the model once. Another approach is an extension of incremental distributed learning, called cyclical incremental distributed learning (CIL). This involves the repetition of the incremental learning process multiple times, i.e. fixing the number of training iterations at each laboratory and cycling repeatedly through the laboratories. Chang et al. <ref type="bibr" target="#b11">[12]</ref> explored these two distributed training approaches using medical imaging data and found the cyclical approach performed comparably to centralized training, suggesting that sharing data may not always be necessary to build deep learning models for patient imaging data. The incremental distributed learning approaches, however, have been criticized for introducing the problem of 'catastrophic forgetting' <ref type="bibr" target="#b12">[13]</ref>, where the trained model strongly favors the data it has most recently seen. Nevertheless the repeated cycles and limited iterations per institution performed in cyclic incremental learning enable it to make gradual progress, despite the forgetting iissue, resulting in better models than the first distributed training approach.</p><p>Federated learning (FL) can be considered another distributed machine learning paradigm seeking to directly address the problem of data governance and privacy. It was first introduced by Google AI <ref type="bibr" target="#b13">[14]</ref> to allow mobile devices collaboratively learn machine learning models without sharing data from the devices. It was applied very successfully in training Google's autocomplete keyboard application <ref type="bibr" target="#b14">[15]</ref>. Institutions or laboratories can also be viewed as 'devices' in the context of federated learning and therefore FL has emerged as a promising strategy in scenarios where, for example, hospitals operate under strict privacy practices and may face legal, administrative, or ethical constraints that require data to remain local. Different from the unparallel approaches, FL is a data parallel training process, in which multiple collaborators train a DL model simultaneously (each on their own data in parallel) and then send their model parameters to a central sever where these are aggregated into a global model. The central sever then sends the global model to all collaborators for further training.</p><p>Each iteration of this process, i.e. parallel training, parameter update, and distribution of global parameters, is referred to as a federated round. Micah et al. <ref type="bibr" target="#b10">[11]</ref> compared federated learning and incremental learning approaches on imaging datasets and unsurprisingly, the basic incremental learning performed the worst compared to federated learning and cyclic incremental learning. While cyclic incremental learning may seem a simpler alternative, given that implementation of federated learning depends on a set of key challenges <ref type="bibr" target="#b15">[16]</ref>, e.g., communication efficiency, which is outside the scope of this article, it required additional validation steps at the end of each cycle, which are basically as complex as the synchronization logic of FL, to achieve comparable results to federated learning paradigm. Critically, cyclical incremental learning was less stable than federated learning, resulting in an inferior alternative.</p><p>The degree to which the laboratory data-sets used for distributed learning are independent and identically distributed (IID) can have a significant influence on the learning performance compared to centralized learning <ref type="bibr" target="#b16">[17]</ref> as deep learning models rely on Stochastic Gradient Decent (SGD) algorithm and the IID samples can ensure that the stochastic gradient is an unbiased estimate of the full gradient <ref type="bibr" target="#b17">[18]</ref>. However, in practice, it is unrealistic to assume that the local data of each laboratory is always IID, which is also a statistical challenge for applying federated learning. It was found in <ref type="bibr" target="#b18">[19]</ref> that the accuracy of a federated learning model trained for image classification was reduced by up to 55% depending on how much institutional bias or degree of non-IID they introduced while dividing a single dataset into hypothetical institutions. The <ref type="table">September 28, 2021  3/20</ref> institutions and institutional biases in their paper were created artificially by defining hypothetical clients and assigning various number of samples to the clients. However, the results of applying the distributed approach on artificially partitioned hypothetical institutional datasets may fail to account for how real-world clients biases impact distributed learning paradigm, a point that has been argued in <ref type="bibr" target="#b16">[17]</ref>. Consequently this also motivated this paper in which we experiment with real-world institutional datasets.</p><p>In this study, we examined the reliability and feasibility of applying federated learning to RNN models for predicting human behaviour and characterising their decision making processes, importantly, using a diverse data pool coming from 10 independent real studies. Subjects in all studies were healthy participants and completed a computerized version of the Iowa Gambling Task (IGT), which is one of the most widely used tasks measuring human decision making under uncertainty in an experimental context. Although previous papers have demonstrated distributed learning applications in various scenarios, e.g. institutional distributed medical image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, none of them has ever applied distributed learning on trial-to-trial decision making data originating from different laboratories. We first trained single laboratory models for each laboratory in the data pool and then evaluated each of these models against held-out testing datasets from each laboratory defined prior to model training. This experiment is used to demonstrate the need for numerous and diverse data for training a robust deep learning model. Second, we illustrated the benefits of multi-laboratory collaboration and the superiority of federated learning compared to incremental learning and cyclic incremental learning. Thus, one centralized model and several distributed models were trained for this purpose. The centralized model is expected to be the best collaborative model as it is the only model that would not lose any information during collaboration. Although all the distributed learning models are expected to sacrifice accuracy and robustness in return for data privacy, the federated learning model was considered to be the most competitive paradigm to train RNN models for predicting human decision actions in the circumstance where data sharing is not possible and thus distributed learning has to be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Iowa Gambling Task</head><p>The Iowa Gambling Task was originally developed to study the decision making deficits of ventromedial prefrontal cortex patients <ref type="bibr" target="#b19">[20]</ref>. Over two decades, it has been one of the most widely used neuropsychological paradigms for simulating complex and experience-based decision-making <ref type="bibr" target="#b20">[21]</ref>. Participants in the IGT are initially given e2000 virtual money and presented with four decks of cards labelled A, B, C, and D. Each card in these decks can generate rewards, and sometimes cause losses. Participants have to choose one card from these four decks consecutively, until the task terminates automatically after a fixed number of trials have been reached. In each trial, feedback on rewards and losses for their choice and the running tally over all trials so far are provided to the participants, but no information is given regarding how many trials they will play and how many trials they have completed during the task. Participants are instructed that they can choose cards from any deck and they can switch decks at any time. They are also told to make as much money as possible by minimising losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The IGT dataset</head><p>The data pool (N = 617) we used in this study derives from 10 studies assessing performance of healthy participants (i.e. without any known neurological impairments) on the Iowa Gambling Task. It involves a broad range of healthy populations aging from 10 to 88 with various education backgrounds and social status. Participants completed a computerized version of the IGT consisting of 95, 100 or 150 trials. All included studies used (a variant of) the traditional IGT payoff scheme or the payoff scheme introduced by Bechara &amp; Damasio <ref type="bibr" target="#b21">[22]</ref>. Since the RNN model we used for predicting subjects' actions required us to have inputs with the same size, the trial-by-trial decision sequences of all subjects were truncated according to the length of the smallest sequence, i.e. 95 trials. In other words, for those subjects who completer 100 or 150 trials of the IGT, only the first 95 trials were used for training or testing the model. Data from different studies were collected in different environments. The number of subjects in each laboratory, which we will refer to laboratory1-10, are <ref type="bibr">15, 162, 19, 40, 70, 25, 153, 35, 57, and 41</ref> respectively. The proportion of females varied approximately from 0.32 to 0.74.</p><p>This dataset is reused here because, 1) first of all, participants in all studies were healthy subjects and they all completed the same decision making task -IGT, which makes it feasible to train a collaborative model for all laboratories; 2) the subjects from ten studies were coming from different backgrounds with various ages and female proportions, thus, conducting multi-laboratory analysis is a natural operation to increase data size and diversity and improve the performance of the model; 3) it created a perfect real-world scenario where the local data of each lab is potentially non-IID and biased, thus it is suitable for testing out the reliability and effectiveness of distributed learning paradigms, especially federated learning, that are sensitive to the data distribution. Each laboratory can be seen as a 'device' in the context of federated learning and the data assignments for the clients in FL can be matched with the real-world data distributions, such that all subjects from the same study are assigned to the same client; 4) last but not least, we can more thoroughly investigate how healthy subjects learn and behave on the IGT using deep learning models, which as stated previously, are promising in the area of decision making modelling because of their higher capability of learning from data in a less model-constrained manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Architecture of the Recurrent Neural Network model</head><p>The RNN model we used is composed of a GRU layer and an output softmax layer with four nodes corressponding to the four choices for the different decks in the IGT. The inputs to the GRU layer is the previous choice made by the subject along with the rewards and losses they received after making that choice. The softmax output are the probabilities of choosing each deck option and sum to 1. The architecture of the model is shown in 1.</p><p>It is known that the basic RNNs are inefficient in solving problems that require the learning of long-term temporal dependencies because of the gradient vanishing problem <ref type="bibr" target="#b22">[23]</ref>. If a sequence is long enough, which is the case of our choice sequences that include at least 95 trials, they'll fail to convey information (the deck preferences) from earlier trials to later ones. LSTM <ref type="bibr" target="#b23">[24]</ref> and GRU <ref type="bibr" target="#b24">[25]</ref> networks were created as the solution to this short-term memory issue. They have internal mechanisms called gates that can regulate the flow of information. The gates can learn which data in a sequence is important to keep or throw away. By doing this, it can pass relevant information down the long chain of sequences to make predictions. In most cases, the two approaches yield comparable performance. It is often the case that the tuning of hyperparameters may be more important than choosing the networks. We chose the GRU here in this paper since it has fewer parameters and can be trained faster. We believe this to be a suitble choice given that we have a relatively small dataset with moderately long sequences.</p><p>The GRU layer is composed of a set of hidden units (N u ). Each unit is associated with a unit output denoted by h k <ref type="figure">Fig 1.</ref> Architecture of the RNN model. The model has a GRU layer (the left part), which receives the previous action (the action is one-hot coded), loss and reward x t and previous hidden states h t−1 and a softmax layer (the right part), which outputs the probability of selecting each deck on the next trial. The GRU layer is composed of two gates, i.e. an update gate and a reset gate, containing a set of weight parameters (N c is the number of choices, which is 4 for the IGT) is a vector containing inputs to the network at time t, i.e. the action c t coded using one-hot representation and the reward r t and loss l t received after taking action. There are two gates in GRU network, i.e. a reset gate and update gate. The update gate helps the model to determine how much of the past information needs to be passed along to the future. The output of this gate z t is a linear combination of the input vector of the current time step x t and the previous cell output going through the sigmoid function.</p><formula xml:id="formula_0">{W r , U r , b r , W z , U z , b z W, U, b hw , b hU }(b r , b</formula><formula xml:id="formula_1">z t = σ(W z x t + U z h t−1 + b z )<label>(1)</label></formula><p>The reset gate is used to decide how much past information is forgotten. The formula is the same as that used for the update gate. The difference arises from the the weights and the gate's usage, which we will examine later in this section.</p><formula xml:id="formula_2">r t = σ(W r x t + U r h t−1 + b r )<label>(2)</label></formula><p>The output of the reset gate is then passed through the current memory content, in which the reset gate is used to store the relevant information from the past.</p><formula xml:id="formula_3">h t = tanh(W x t + b hW + r t U h t−1 + b hU )<label>(3)</label></formula><p>In the last step, the network needs to calculate the h t vector which holds information for the current unit and passes it down to the network. In order to do this, the update gate is needed. It determines what to collect from the current memory content h t and what to take from previous steps h t−1 . The parameters of the GRU layer include</p><formula xml:id="formula_4">W z , W r , W ∈ R Nu×(Na+2) , U z , U r , U ∈ R Nu×Nu , and b z , b r , b hW , b hU ∈ R Nu . h t = z t h t−1 + (1 − z t ) h t<label>(4)</label></formula><p>September 28, 2021 6/20</p><p>The softmax layer takes outputs from the GRU layer (h t ) as its inputs and outputs the probability of choosing each action. The parameter of the softmax layer is V ∈ R Nu×Na . The parameters of the RNN model will be</p><formula xml:id="formula_5">Θ = {V, W z , W r , W, U z , U r , U, b z , b r , b hW , b hU }</formula><p>The RNN model was trained using the maximum-likehood (ML) method and Categorical Cross-Entropy Loss summed over all subjects on all trials:</p><formula xml:id="formula_6">Loss = − S s=1 T t=1 y st log(ŷ st )<label>(5)</label></formula><p>The model was implemented in TensorFlow <ref type="bibr" target="#b25">[26]</ref> and optimization was based on the use of the Adam optimizer <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>First of all, each laboratory dataset was divided into training and testing datsets with 80% of the data points for the former and the remaining 20% for the latter. Subjects were not mixed across training and testing sets. Two experiments are conducted in this section. The first experiment aimed to establish the need for bigger and more diverse data to train a reliable and robust RNN model that could predict human actions with reasonable accuracy. In this experiment, we trained a single RNN model on the training dataset for each laboratory and evaluated each of these models against testing datasets from each laboratory. In total, we trained 10 independent RNN models, one from each separate laboratory, and evaluated their generalization performance both on the testing dataset of their own laboratory and other data from the other laboratories.</p><p>In the second experiment, we seek to demonstrate the improvements of collaborative models compared to single models and also the comparison between the centralized learning model and distributed learning models. Eight collaborative models are trained. The collaborative models were trained directly or indirectly with bigger and more diverse datasets depending on the collaboration methods imposed between the laboratories, i.e. centralized learning (CL-based), incremental learning (IL-based), cyclic incremental learning (CIL-based), and federated learning (FL-based) paradigms. In incremental and cyclic incremental learning, three different orders of training the model were performed, i.e. smallest sample first, largest sample first and random order, yielding three models for these two learning paradigms. We expect CL-based model will behave the best among the four collaborative models, while the other three distributed learning paradigms will sacrifice competitiveness to different extents in exchange for preservation of data privacy. Thus, CL-based model will serve as the baseline model against which comparisons are made with the distributed models. The architectures of the CL-based and FL-based model paradigms are shown in <ref type="figure" target="#fig_1">Fig 2.</ref> The hyperparameters for training the centralized model are set as follows: batch size 50; maximum epochs 250; learning rate 0.02; number of hidden cells 10 (tuned by 10-fold cross-validation, which will be introduced in the following section). To create an equal playing ground for the centralized learning and distributed learning paradigms, we need to guarantee that all models use the same hyperparameter settings or if it is not applicable due to the structure characteristic of the learning framework, we would at least make sure they see exactly the same number of samples in the whole training process. As a result, the learning rate and number of hidden cells are set as the same values for all models, while the batch size and epochs vary for different learning frameworks based on their structures. The batch size for training the local models in all three distributed learning paradigms is set as the size of the training set of the training laboratory. For the IL-based model, the maximum epochs for each laboratory should be set as 250. For the CL-based model, the epochs for each laboratory is considered together with the number of cycles so that their multiplication is equal to 250. In our experiment, the frequency of weight transfer is every 50 epochs, thus the number of cycles is 5. For federated learning, we sharded the training datasets across 10 laboratories to match the real-world configuration of the 10 contributing laboratories.</p><p>Each laboratory was regarded as a client that has different sizes of data samples, ranging from 12 to 129, for training. It is assumed in our experiment that all clients are always powered (as is the case for a typical computer but not for a mobile device), thus instead of selecting clients via an eligibility criterion from multiple client pools, all clients are involved in each round in our implementation. The number of communication rounds is set as 5 (which is the same as the cycle number in cyclic learning) and the epochs for all clients are 50. Thus, the total exposure to the training sets from all laboratories is the same as for the centralized model. Additionally, the aggregation performed in federated learning is a weighted average of laboratory updates and the weightings are calculated as the ratio of the size of each laboratory training set to the size of the aggregated training set.</p><p>After model training, the weight parameters of the models were frozen and evaluated under multiple aspects. We first examined the prediction accuracy of the eight models on the testing datasets of each laboratory. We then simulated the collaborative models in an on-policy way in the IGT (with the same payoff schemes for the subjects and the same number of trials we used for training the model). On-policy means that the models completed the task on their own by selecting the decks that they predict a representative subject would take in each simulation. Lastly, we simulated the models off-policy. Off-policy means a model uses previous actions and payoffs to make predictions about the next action. However, the actual next actions used to simulate the models are not derived from these predictions, instead they are derived from human choices. In this way, we can control the inputs the model receives and examine how they affect the predictions. Apart from examining the relative performance of distributed learning compared to centralized learning paradigm, these simulation analysis allow us to reveal the underlying learning mechanisms and decision-making strategies used by the healthy subjects in the meantime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tuning the number of hidden units in the GRU layer</head><p>The number of hidden units N u in the GRU layer is a hyperparameter that has significant influence on the performance of the model. Networks with different number of hidden units (N u ∈ 5, 10, 20) were considered and the best model was selected through 10-fold cross-validation before the single and collaborative models were trained. The total number of free parameters in both the GRU layer and softmax layer were 219, 584, and 1764 for the networks with 5, 10, and 20 hidden units of GRU layer, respectively.</p><p>As described in the previous section, each laboratory dataset was split into training and testing dataset with a ratio of 80:20. In the first experiment, 10-fold cross-validation was performed for each laboratory on their training datasets to tune the number of hidden units in the GRU layer of the single models. In the second experiment, 10-fold cross validation was performed on the aggregated training dataset of all laboratories to tune the hyperparameter for the collaborative models. In 10-fold cross validation, the dataset (either the training dataset from a single laboratory or the aggregated training dataset of all laboratories) is randomly split into 10 folds. For each unique fold, we take the fold as a hold out. The remaining folds are then taken as a training dataset. We fit a model with a specific number of hidden cells on the training dataset and evaluate the model on the hold out fold. The evaluation score is retained and the model is discarded then. We repeat this process until every fold served as the hold out set. Finally, we take the average of the evaluation scores and that will be the reported performance metric for this model with a particular value of hidden units. The number of hidden units that yields the best performance is applied to train the single models or the collaborative models then. It is worth noting here that the testing datasets from all laboratories were not used to tune the hyperparameter, instead, they were used to estimate the generalization ability of the models on new, previously unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation</head><p>10-fold cross validation was conducted before training and testing the single and collaborative models to tune the number of hidden units in the GRU layer. The cross-validation results conducted on the aggregated training dataset from all laboratories are reported in 3, in which the average training and validation losses of 10 folds across epochs for centralized models with different number of hidden units are plotted (the plots for the single models can be found in the supplements). The model with 20 hidden units achieved the lowest training loss among the three models (the purple line), however, the validation loss of this model (the bronw line) did not decrease steadily after 250 around epochs, instead it increased indicating overfitting. The lowest mean validation losses was achieved by 10 hidden units in the GRU layer (the green line). This setting was then used for training and simulating the four collaborative models. In a similar way, the number of hidden units for all single models were set as 5 based on their cross-validation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The need for more numerous and diverse data</head><p>In this experiment, ten single models were trained with the tuned hyperparameters using the training datasets from each laboratory respectively. These models were then evaluated using the testing datasets availabe from each laboratory. <ref type="figure" target="#fig_3">Fig. 4</ref> shows ten datasets of other laboratories. The chance probability of predicting the next action taken by the subjects correctly is 25% since there are four options in the IGT. However, the prediction accuracy of several single models on some of the testing datasets was lower than the chance probability, which is obviously not satisfactory performance. It can be seen that most single models performed the best on the testing dataset of laboratory9, except for the three models that trained on the least size of laboratory samples, i.e. laboratory1 (12), laboratory3 (15), and laboratory6 <ref type="bibr" target="#b19">(20)</ref>. These laboratories with small number of training samples did not perform well on their own testing sets nor did they generalize well to any other testing sets from other laboratories. The best average model performances were shown on laboratory2 and laboratory7, which have the largest sizes of training samples, 129 and 122, respectively. These results indicate that more numerous and diverse data are needed for a laboratory site in order to train reliable and generalizable models, at least in the contest of training RNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The benefits of training collaborative models</head><p>In this section, we aim to demonstrate 1) the benefits of laboratory collaboration and 2) the feasibility of conducting distributed learning when data sharing barriers are present. We compared the prediction accuracy of the eight collaborative models with the single models for the first purpose. The results are illustrated in <ref type="figure" target="#fig_4">Fig 5.</ref> As a result, apart from random and descending ordered IL and CL models, the prediction accuracy of the collaborative models were much improved when testing on each laboratory's data.</p><p>Notably, the average performance over all laboratories of the CL-based model were improved to 55%, 6% points higher than the highest accuracy achieved by the single model of laboratory2, and the corresponding increase for FL-based, CIL ascend-based and IL ascend-based model was 4%, 4% and 3% points, respectively. Since random and Two collaborative models, i.e. CL-based model and FL-based model, tested against testing datasets from each of the laboratories. The Y-axis represents the two collaborative models trained on the training datasets from all laboratories and the X-axis represents the testing dataset of each independent laboratory. AVG is the average of each collaborative model performance over all laboratories descending ordered IL and CIL models did not outperform the single models, only the ascending ordered models were selected to conduct further evaluation. We will refer to these as IL-based and CIL-based model instead of CIL ascend-based and IL ascend-based in the following analysis for simplicity of reference. The outperformed IL-based and CIL-based models and the other two collaborative models were evaluated from multiple aspects for the second purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-policy simulation</head><p>After training the four collaborative models, we froze the weight parameters of the models and simulated them on-policy in the task. We fed the model with the same payoff schemes as for the subjects and the models selected the decks autonomously based on what they had learned from human behaviours. Since we trained on 491 experimental subjects from training datasets from all studies, we simulated 491 fake agents for each collaborative model as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.0.1">The average probability of selecting each deck over subjects</head><p>The SUBJ column in <ref type="figure" target="#fig_5">Fig 6</ref> shows the average probability of choosing each deck over all experimental subjects. All subjects selected DeckA the least compared to other Decks, which is not surprising because DeckA is the bad deck that yields negative long-term payoff and with more frequent losses. Consistent with human's choices, the probability of choosing DeckA for both centralized and distributed agents was the lowest. Neither CL-based agents (η = 0.005, SE = 0.004, p = 0.26) nor the FL-based and the IL-based agents (FL-based: η = 0.001, SE = 0.004, p = 0.91, IL-based: η = 0.001, SE = 0.004, p = 0.83) are significantly different for subjects in terms of the probability of selecting DeckA, although the CIL-based selected slightly significant more DeckA than subjects (η = 0.01, SE = 0.005, p = 0.03). FL-based agents had significant whereas CIL-based agents did not learn this characteristic of behavioural pattern, the probability of selecting good decks was not significantly higher than bad decks (η = −0.006, SE = 0.010, p = 0.51).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.0.2">The fluctuation of probability of selecting each deck over trials</head><p>The analysis in the previous section suggested that CL-based, FL-based, and IL-based agents selected more good decks than bad decks, which is more consistent with the human behavioral data compared to CIL-based agents. There are multiple possible strategies that the models could follow to obtain the final proportions of each deck.</p><p>Here we aim to examine whether the models were using similar strategies to that used by subjects. We examined the fluctuation of deck preferences of the subjects and model agents over trials. We divided the 95 trials into 10 blocks of 10 trials for the first 9 blocks and 5 trials for the last block. The proportion of each deck selection in each block and the learning scores (i.e. the difference between the number of good deck selections and the number of bad deck selections) for each subject was calculated. blocks of the experimental subjects and four model agents. The behaviour characteristics of CL-based and FL-based agents were more similar to experimental subjects. In the beginning of the task, subjects, CL-based and FL-based agents selected DeckB most frequently, but its representation as a proportion of choices decreased after approximately 4 blocks for subjects and CL-based agents, 5 blocks for FL-based agents. At this stage the proportion choices which resulted in the selection of the "good" DeckD exceeded that for DeckB and became the highest. The proportion of choices for selecting DeckC was low in the beginning and gradually increased over trials as subjects learned more from the task. The final proportion of DeckC was a little higher than that of DeckB both for subjects, and CL-based agents, although almost equal for FL-based agents. The proportion of selecting DeckA was always the lowest and decreasing throughout the task. For CIL-based and IL-based agents, however, the proportion of choosing DeckB was almost always the highest. Although consisting with the subjects, the proportion of DeckC for CIL-based agents increased over trials, it never became as high as DeckB and DeckD. It was even worse for IL-based agents because the proportion of choosing DeckC did not vary too much over trials. <ref type="bibr">Fig 8</ref> shows the learning scores across ten blocks of the IGT. A learning process was apparent both for experimental subjects and CL-based and FL-based agents, in which the learning score progressively improved over blocks, although there was a clear dip in block 10 for subjects and FL-based agents and block 8 for CL-based agents. To quantify the differences between the subjects and the two kinds of model agents over blocks on learning scores, the repeated measures ANOVA tests were conducted in the form of 10 (blocks) × 2 (groups). The ANOVA test conducted between human and CL-based agents and human and FL-based agents revealed no significant interaction between the two factors (CL-based: F (7.44, 7290) = 1.39, p = 0.20, FL-based: F (7.1, 6954) = 1.82, p = 0.08), however, there was a significant main effect of blocks (CL-based: F (7.44, 7290) = 67.63, p &lt; 0.001, FL-based: F (7.1, 6954) = 60.64, p &lt; 0.001), with the interpretation that the learning scores of both subjects and CL-based and FL-based model agents varied across blocks and the CL-based and FL-based model did not behave significantly different across subjects in the learning process. The ANOVA test performed between human and the other two collaborative agents showed that the effects of blocks and group were both significant on the learning score (CIL-based: F (7.85, 7693) = 47.11, p = 0.03, F (1, 980) = 17.61, p &lt; 0.001; IL-based: F (7.71, 7558) = 22.66, p = 0.02, F (1, 980) = 5.69, p = 0.02) and there was a significant interaction between groups and blocks on the learning score (CIL-based: F (7.85, 7693) = 5.98, p &lt; 0.001, IL-based: F (7.71, 7558) = 15.76, p &lt; 0.001). This result suggested that the learning process of CIL-based and IL-based agents were significantly different from experimental subjects, which can also be seen from the figure, in which the learning scores of CIL-based were lower than the subjects almost over all blocks and IL-based agents did not present apparent learning trend in the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.0.3">The switch probability after receiving losses</head><p>We finally investigated the immediate effect of loss on choice. The reason why reward effect is not considered here is because subjects can always get rewards no matter which deck they choose, 100 for DeckA and DeckB and 50 for DeckC and DeckD, according to the payoff scheme. of avoidance to losses used by the four models was similar to that seen in the subjects' behavior. However, the switch probabilities of CL-based agents are more closer to that of the experimental subjects visually and FL-based agents rank second. The switch probability of IL-based agents was obviously higher than the subjects when there was no loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Off-policy simulation</head><p>In the off-policy simulation analysis, we fed the four collaborative models with a variety of actions and corresponding rewards and losses specified in the payoff schemes, and the same as specified in the payoff scheme of IGT. As mentioned earlier, based on the IGT payoff scheme, the player would always receive rewards no matter which deck they choose and only the rewards magnitude varies depending on which deck they choose, while they only receive losses occasionally. We only marked trials where there were losses with blue dots in the graphs.</p><p>The four collaborative models behaved quite similarly in responding to the losses according to the plots, especially the FL-based model and the CL-based model.</p><p>Receiving a loss from all decks caused a dip in the probability of staying with that deck, which showed a tendency to switch to another deck. This is consistent with the observations we obtained in <ref type="figure" target="#fig_9">Fig 9 that</ref> the probability of switching is higher when receiving a loss compared to no loss. It seems the CIL-based model was more sensitive to the losses given by DeckC as deeper dips were caused compared to other models. The dips caused by DeckB and DeckD were the deepest, which is not surprising, because</p><p>DeckB and DeckD are decks with less frequent but larger losses. It is reasonable that the switch probability should be higher if you received a significant loss from that deck choice. The probability of choosing DeckA, DeckB, DeckC, and DeckD was higher than that of other decks in each simulation. This implies sticking with the previously taken deck was an element of an deck selection strategy of the four models. We can also see that the perseverance with DeckA is not as strong as to the other three decks as the probability of selecting DeckA predicted by the two models is almost never more than 80% and this is even more obvious for IL-based model. This result is consistent with the result reported in <ref type="figure" target="#fig_5">Fig 6 and Fig 7</ref> where the proportion of selecting DeckA was the lowest both across subjects and trials. Interestingly, the perseverance effect to DeckC was smaller and the sensitivity to the losses of DeckC was higher for CIL-based model and relative to other models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results demonstrate, as expected, that models trained on a single laboratory's dataset only, perform poorly both on their own testing sets and testing sets from other laboratories. The average prediction accuracy of the model of laboratory1, which had the smallest number of subjects among studies, on ten testing datasets was 31%, only 6% higher than the chance correct probability. This result highlights the need for larger and more representative datasets and therefore emphasises the relevance of training using collaborative models involving subjects from other studies.</p><p>Given the difficulties of sharing data across studies,institutions and jurisdictions, it is attractive to use distributed learning paradigms to replace traditional centralized data sharing to make laboratory collaboration easier. However the decision to adopt such as an approach is only justifiable so long as the resultant model yields comparable performance with the data pooling approach. While the work presented here cannot be fully comprehensive, it presents for the first time a representative sampling of eight collaborative models, one of which based on centralized data sharing while the other seven were based on distributed learning including one FL-based, three CIL-based and three IL-based, were trained on the training datasets from all laboratories for comparison. It was found that the order of the laboratories was an important factor that influenced the performance of CIL-based and IL-based models. An ascending order based on the laboratory data-set sizes was the best strategy for both of these two paradigms and the corresponding models were selected, along with the FL-based model, to be further evaluated with the centralized model from multiple aspects. When examining their prediction capability on the unseen testing datasets the prediction accuracy was improved by 24%, 24%, 24%, and 23% points compared to the worst single model on the same testing data-sets for CL-based, FL-based model, CIL-based and IL-based model respectively. This highlights the value of involving larger numbers of data samples for training a deep learning model. It is also clear that FL-based and CIL-based approaches can both achieve competitive performance with CL-based model while retaining the benefit of enhanced data privacy. Another interesting finding is that all the collaborative models performed clearly better on the testing sets of laboratory9, laboratory5, and laboratory4 and the CIL-based and IL-based models did not obviously suffer from the 'catastrophic forgetting' problem as reported in the literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. This is apparent because they did not present performance biased to the the data they had mostly recently seen (i.e. laboratory2 for the ascending order case, laboratory1 in the descending order case, and laboratory10 for the random order case).</p><p>We then evaluated the capability of the distributed model in capturing characteristics of human decision-making strategies used on the IGT in comparison with CL-based model through on-policy and off-policy simulations. In the on-policy simulation, 491 fake agents, which is the same as size as the training data-set for training the collaborative models, were generated for the four models. The proportions for choosing each deck averaged over subjects was examined first. The results revealed that CL-based agents behaved more similarly to human subjects exhibiting similar choice proportions on all decks, while the agents generated by the three distributed models had significantly different deck preferences and aversions on one or two particular decks compared to the subjects choices. This characteristic was also reflected in the fluctuation of the probability of selecting each deck over trials except that FL-based agents performed as well as CL-based agents in this comparison. The probability of choosing DeckB was almost always the highest for CIL-based and IL-based agents, whereas the probability of choosing DeckD surpassed DeckB after four blocks for experimental subjects and CL-based agents and five blocks for FL-based agents. The probability of choosing DeckC for CIL-based and IL-based agents never increased as high as that of the subjects and the other two distributed agents by the end of the game. The comparison analysis of the IGT learning scores across trials between the experimental subjects and model agents also suggested CIL-based and IL-based model were weaker in recognizing the good decks as the learning scores of these two agents did not change for IL-based agents or were always lower than the other groups throughout the task, although the overall trend of the learning score was increasing for CIL-based agents. In terms of the strategy of dealing with losses, all model agents adopted the same strategy as human subjects, i.e. switching more often when they received a loss compared to no loss. It is worth noting here that although no statistically significant weakness was identified for CIL-based and IL-based agents, the average switching probability of receiving losses was relatively higher than that of the human subjects visually, while the CL-based and FL-based agents were almost the same as the subjects on this figure. The characteristic of switching more after losses was validated again in the following off-policy simulation, where the actions fed to the model were specified by us and the models were responsible for predicting the next action to be taken for the next trial. According to the model's prediction, the probability of sticking with a deck was decreased whenever there a loss incurred. CL-based and FL-based models reacted almost the same way again in this simulation, but IL-based model demonstrated less perseverance to DeckA and the CIL-based model was more sensitive to losses compared to the other agents.</p><p>In summary, consistent with the findings of Sheller et al. <ref type="bibr" target="#b16">[17]</ref>, where the distributed learning methods were evaluated in the field of medicine, the FL-based model achieved the best prediction accuracy and capability among the distributed learning paradigms in learning and mimicking the decision-making strategies used by the experimental subjects. Although CIL-based and IL-based models also achieved considerable prediction accuracy when compared to the CL-based model, they failed in capturing some features such as subtle deck preferences between DeckB and DeckC and learning curve shapes for the subjects. These findings can help inform researchers when developing multi-laboratory collaborations in the field of cognitive and behavioural science. It suggests simulated datasets may be a useful means for determining to what September 28, 2021</p><p>18/20 extent distributed learning compromises the quality of the models developed.</p><p>Additionally, emerging improvements to FL <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> hold further promise for the narrowing of the gap in performance between centralized and federated learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Deep learning models, especially RNN models, have emerged as useful methods suitable for characterising and even predicting human behaviour in the context of computational modelling tasks. A successfully trained model generalizes well with respect to data it has not been seen before. Successful training in turn requires a large representative dataset. However, typical cognitive and behaviour studies are often relatively small in terms of the numbers of subjects recruited, which limits the application of deep learning models to human behaviour prediction. A natural way of obtaining bigger and more diverse data is sharing data between laboratories. Nevertheless, sharing data is never easy for the purpose of protecting personal privacy, especially when involving data that could potentially identify individuals and institutions that are internationally distributed. Distributed learning is a promising method for addressing this issue through sharing the model parameters instead of data themselves. In this study, the feasibility and reliability of applying several distributed learning paradigms to the field of cognitive studies aiming to characterise human decision-making was examined. We demonstrated that federated learning outperformed the other two distributed learning paradigms in learning the learning features of human subjects in this context. The use of federated learning over centralized learning has the immediate advantage of keeping data confidential. We hope that the development of this techniques and emerging derivations of the approach will allow for data-private collaborative training over various subjects with different ages, education backgrounds, social status collected by different laboratories. Such collaborations may enable the development of more robust models that satisfy clinical needs and thus will be helpful in terms of translating advances in cognitive studies into clinical improvements for patients suffering from psychiatric conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>z , b hW and b hU are not shown for simplicity). The output of the cells of the GRU layer h t are connected to the softmax layer using black lines. The weight parameters in the softmax layer are represented as V . (h t ∈ R Nu ) as a vector containing all the cell outputs at time t. They are initialized with the value of zero and are updated after receiving each time step input. x t ∈ R Nc+2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 .</head><label>2</label><figDesc>Architectures of two collaborative learning methods for multi-laboratory collaboration. The most straightforward method (a) is based on centralized data sharing, i.e. requiring that every laboratory share data to a central node and aggregating to train a model; An alternative method and the primary focus on this work (b) is Federated learning, in which all laboratories train the same model at the same time and update their model parameters to a central sever where it is used to create a global model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 3 .</head><label>3</label><figDesc>10-fold cross-validation results of models with different numbers of hidden units. Each curve is the average loss of 10 folds on training or testing dataset for each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 4 .</head><label>4</label><figDesc>Single lab models tested against testing datasets from each of the laboratories. The vertical axis represents models trained on a single laboratory dataset, and the horizontal-axis represents the testing dataset of each independent laboratory. AVG is the average of each laboratory model performance over all laboratories. single models' testing results both against their own testing datasets and testing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 5 .</head><label>5</label><figDesc>Fig 5. Two collaborative models, i.e. CL-based model and FL-based model, tested against testing datasets from each of the laboratories. The Y-axis represents the two collaborative models trained on the training datasets from all laboratories and the X-axis represents the testing dataset of each independent laboratory. AVG is the average of each collaborative model performance over all laboratories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 6 .</head><label>6</label><figDesc>aversion to DeckD (η = −0.02, SE = 0.010, p = 0.03) and CIL-based and IL-based agents had significant preference to DeckB (CIL-based: η = 0.03, SE = 0.009, p &lt; 0.001, IL-based: η = 0.02, SE = 0.008, p = 0.010) and significant aversion to September 28Probability of selecting each deck (average over subjects). SUBJ refers to the data of the experimental subjects; CL-based, FL-based, CIL-based, and IL-based are the on-policy simulations of the four collaborative models trained on the IGT with the same payoff scheme and same number of trials the subjects completed, respectively.DeckC (CIL-based: η = −0.04, SE = 0.009, p &lt; 0.001, IL-based: η = −0.03, SE = 0.009, p &lt; 0.001) compared to the subjects, while no significant difference was observed between CL-based agents and subjects on all of these decks. This can also be seen from the plot of the overall probabilities of selecting DeckB and DeckC where IL-based and CL-based agents are relatively higher and lower than that of the subjects, respectively and the probabilities of selecting DeckD of FL-based agents are relatively lower.It is worth noting that subjects prefer decks with infrequent losses, DeckB and DeckD, compared to decks with frequent losses, DeckA and DeckC, although they ended up selecting significantly more good decks (DeckC and DeckD) than bad decks (DeckA and DeckB) (η = 0.078, SE = 0.010, p &lt; 0.001). CL-based, FL-based and IL-based agents managed to learn this strategy from the subjects' actions, selecting more good decks than bad decks (CL-based: η = 0.059, SE = 0.010, p &lt; 0.001, FL-based: η = 0.059, SE = 0.011, p &lt; 0.001, IL-based: η = 0.035, SE = 0.008, p &lt; 0.001),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 7 Fig 7 .</head><label>77</label><figDesc>Mean proportion of choices from each deck within 10 blocks of experimental subjects and four simulated model agents. demonstrates the average proportion of choosing each deck as a function of the 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 8 .</head><label>8</label><figDesc>The IGT learning scores across ten different blocks of experimental subjects and simulated model agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig 9 shows the effect of receiving a loss in the previous trial on the probability of switching to the other deck in the next trial. For the experimental subjects, receiving a loss significantly increased the probability of switching to other decks (η = 0.218, SE = 0.010, p &lt; 0.001). As the figure shows, same pattern was established by all collaborative agents (CL-based: η = 0.209, SE = 0.007, p &lt; 0.001; FL-based: η = 0.201, SE = 0.007, p &lt; 0.001; CIL-based: η = 0.246, SE = 0.007, p &lt; 0.001; IL-based: η = 0.179, SE = 0.007, p &lt; 0.001), suggesting that the strategy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 9 .</head><label>9</label><figDesc>Probability of switch to different deck based on whether received losses or not in the previous trial, average over subjects. recorded the predictions of each model in response to each input set. Simulations of the models are shown in Fig 10. Each panel shows a separate simulation across 30 trials ( horizontal axis). For the total of 30 trials, the action that was fed into the model was DeckA (top left), DeckB (top right), DeckC (bottom left), or DeckD (bottom right) for each simulation, respectively. The reward and losses associated with these trials were</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 10 .</head><label>10</label><figDesc>Off-policy simulations of the two collaborative models. Each panel shows a simulation of 30 trials.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 20212/20</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 20214/20</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t for cell k at time t. Let's define h t = [h 1 t , h 2 t , ...h Nu t ] T September 28, 2021 5/20</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 2021 7/20 (a) Collaborative learning through centralized data sharing: CL-based model training (b) Federated Learning to maintain data privacy: FL-based model training</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 202115/20</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 202117/20</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 28, 2021 20/20</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author summary</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The diffusion decision model: theory and data for two-choice decision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building bridges between perceptual and economic decision-making: neural and computational mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforcement learning: the good, the bad and the ugly. Current opinion in neurobiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="185" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. Current research and theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="64" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Models that learn how humans learn: the case of decision-making and its disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dezfouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Balleine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1006903</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dezfouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ashtiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ghattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Disentangled behavioral representations. bioRxiv</title>
		<imprint>
			<biblScope unit="page">658252</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Using Deep Learning to Predict Human Decisions and Cognitive Models to Explain Deep Learning Models. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hertz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Titano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Oermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS medicine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1002683</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational psychiatry as a bridge from neuroscience to clinical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="413" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The eu general data protection regulation (gdpr). A Practical Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Dem Bussche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3152676</biblScope>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>1st Ed</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed deep learning networks among institutions for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="945" to="954" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Federated optimization: Distributed machine learning for on-device intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno>arXiv:161002527</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Federated learning for mobile keyboard prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Augenstein</surname></persName>
		</author>
		<idno>arXiv:181103604</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kotrotsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Making gradient descent optimal for strongly convex stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<idno>arXiv:11095647</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Civin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>arXiv:180600582</idno>
		<title level="m">Federated learning with non-iid data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Insensitivity to future consequences following damage to human prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bechara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Damasio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Damasio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bayesian techniques for analyzing group differences in the Iowa Gambling Task: A case study of intuitive and deliberate decision-makers. Psychonomic bulletin &amp; review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steingroever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pachur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Šmíra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="951" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decision-making and addiction (part I): impaired activation of somatic states in substance dependent individuals when pondering decisions with negative future consequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bechara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Damasio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1675" to="1689" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<idno>arXiv:14061078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<idno>arXiv:160304467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ozdayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<idno>arXiv:201015582. 2020</idno>
		<title level="m">Improving Accuracy of Federated Learning in Non-IID Settings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing federated learning on non-iid data with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE; 2020</title>
		<imprint>
			<biblScope unit="page" from="1698" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
