<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The softmax function: Properties, motivation, and interpretation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Franke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Degen</surname></persName>
						</author>
						<title level="a" type="main">The softmax function: Properties, motivation, and interpretation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The softmax function is a ubiquitous helper function, frequently used as a probabilistic link function for unordered categorical data, in different kinds of models, such as regression, artificial neural networks, or probabilistic cognitive models. To fully understand the models in which the softmax function occurs, different levels of understanding of the softmax function itself are necessary. For input-output oriented models, like regression or neural network models, mathematical properties are crucial. For models with interpretable and meaningful internal representations like probabilistic cognitive models, we also require a thorough conceptual understanding of the motivation for using the softmax function (instead of something else). This tutorial provides an indepth exposition of the informal, mathematical and conceptual properties of the softmax function. It also provides two mathematical derivations (as a stochastic choice model, and as maximum entropy distribution), together with three conceptual interpretations that can serve as rationale for using the softmax function in models that require explainability of modeling choices. * Thanks to Juliane Schwab for support in creating this manuscript, to Georg Schröter for pointing out the connection to thermodynamics and to Britta Grusdt, Xenia Ohmer, Polina Tsvilodub, and Fausto Carcassi for discussion and feedback.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction &amp; overview</head><p>The softmax function is a ubiquitous helper function, frequently used as a probabilistic link function for unordered categorical data. Within the Cognitive Sciences, it is commonly used in the context of neural networks, regression modeling, or probabilistic cognitive models. Despite its prevalence, there is often little concern about the rationale behind the softmax function or its mathematical properties. As a results, the models in which the softmax function occurs may remain partially opaque. This is perhaps less problematic, if unfortunate, for some areas of application (neural network or regression modeling), but insufficient understanding of the softmax function can be a genuine problem in other areas of application (such as cognitive modeling), as we will explain here.</p><p>The goal of this tutorial is to describe the softmax function in increasing level of conceptual and mathematical detail, so as to enable a better understanding of the models in which it occurs. The tutorial is structured so that readers with different demands can exit when their information needs are satisfied. We recommend that all readers start with Section 2, which gives definitions and formal notation for the rest of the paper. It is also highly recommended to at least skim Section 3, which introduces a distinction between input-output (IO) models and internally-meaningful (IM) models. Depending on the kind of model readers are interested in, they may want to focus on different parts of this tutorial. Readers mostly interested in IO models may want to solidify their understanding of the softmax function in applied contexts, like regression modeling or neural networks, and therefore find useful information in Section 4, which illustrates the workings of the softmax function based on input-output relations (using plots and intuitions), and Section 5, which adds mathematical detail by stating and proving key properties of the softmax function. For those readers working with (IO <ref type="figure">Figure 1</ref>: The softmax function takes a score vector s and a value of the softmax-parameter α as input and returns a probability vector. or IM) models which have an explicit and variable parameter, Section 6 provides some guidance on the interpretation of specific numerical values of the softmax parameter, which is also important to choosing priors when working with Bayesian models. Finally, readers who encounter the softmax function as a stochastic choice function in IM models, e.g., as a stochastic choice function in abstract models of decision-making in the context of cognitive models, psycholinguistics, computational sociology, or behavioral economics, may also care about the conceptual motivation for this modeling choice. To address this, this tutorial gives three different conceptual motivations (with accompanying mathematical derivations) for the softmax function. Concretely, Section 7 characterizes the softmax function as modeling sub-optimal, noise-perturbed decision making, where stochasticity enters due to one particular kind of stochastic error. Section 8 motivates the softmax function as an optimal choice of modellers who want to assume that choices are noise-perturbed, but want to remain maximally uncommitted about the nature of the noise. This interpretation is based on a formal result that shows that the softmax distribution is a maximum-entropy distribution. Finally, Section 9 interprets the same mathematical result in a different way, namely as an optimal tradeoff between exploration and exploitation from a decision-making agent's perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Softmax basics: definition, notation &amp; terminology</head><p>Formally, the softmax function is a mapping that takes a vector of scores s = s 1 , . . . , s n and maps it to a vector of corresponding probabilities p = p 1 , . . . , p n , using the softmax (optimality) parameter α to modulate the shape of the output distribution p (see <ref type="figure">Figure 1)</ref>. The common use case for the softmax function is to serve as a link function for unordered, categorical data. Given n discrete outcome categories x = x 1 , . . . , x n , we want a model that predicts how likely each category x i is. In applications, outcome categories could be labels (Does this picture show a dog, a cat or a goldfish? Is this text fiction, law, or science?) or actions of an agent (Will Bubu order pizza, pasta or salad? How likely are participants to select option x i in a forced-choice experiment?). A model would then be used to predict the outcome probabilities p i for each category x i .</p><p>A probability distribution p = p 1 , . . . , p n over n (finite) categories must sum up to one: i p i = 1 and requires that 0 ≤ p i ≤ 1 for all 1 ≤ i ≤ n. Yet, for technical or conceptual reasons, it is often easier for a computational model to create a probabilistic prediction p by first, internally, constructing a less constrained vector of scores s = s 1 , . . . , s n which do not necessarily sum up to one and where s i can be any finite real number (positive or negative). <ref type="bibr">1</ref> The main use case of the softmax function is then to map these non-normalized scores onto well-behaved probabilities. Section 3 will elaborate on the interpretation of scores, which depends on the kind of application or model at hand.</p><p>The definition of the softmax function is: 2 SM(s; α) = p, with:</p><formula xml:id="formula_0">p i = exp (α s i ) j exp (α s j )</formula><p>Authors often use a simpler notation, omitting the normalizing constant Z = j exp (α s j ), to just write:</p><formula xml:id="formula_1">p i ∝ exp (α s i )</formula><p>The softmax function has a softmax parameter α ∈ R, which is sometimes omitted, i.e., implicitly set to 1. <ref type="bibr">3</ref> The softmax parameter α modulates the output probability in systematic ways. Intuitively, the higher α, the more preferred (in terms of higher probability) will be options with higher scores. (Section 4 explores the effects of α on the output of the softmax function, Section 6 focuses on the interpretation of particular values of α.) But whether we need α in the first place, depends on the interpretation of the scores and the kind of model we have, which is the topic of the next Section 3.</p><p>3 Why care about the softmax function?</p><p>The softmax function is only one of infinitely many functions that map non-normalized scores onto probability vectors. Other functions may have other mathematical properties and different conceptual motivation. In order to fully understand the models in which the softmax function occurs, we must understand enough of the softmax function to know how to use it in practice and interpret results that we obtain with models that use it. However, different types of models and different researcher goals may require different levels of understanding of softmax. To capture two prevalent ways in which researchers apply models that use the softmax function, we distinguish between input-output (IO) models and internally-meaningful (IM) models and expand on each in turn. IO models care only about making accurate predictions, e.g., for forecasting or decision making. IO models do not strive to accurately model the data-generating process through their internal parameters and computations. Their inner mechanics need not be interpretable or transparent and they do not aspire to be explanations of phenomena behind the data to be predicted, such as causal mechanisms of mental representations. Examples of IO models are regression models and (deep) neural networks used for engineering purposes (classification, forecasting, prediction etc.). Consequently, for IO models, it is usually not very important to ponder the conceptual interpretation of the softmax function. What matters most are its technical properties. IO models will often generate non-normalized scores s that are not intrinsically meaningful, but constructed, estimated or learned (from training data) to get the right probabilistic predictions via softmax. Since scores s are conceptually unconstrained, possibly freely estimated from the data, there is also no need to vary the softmax parameter α, which is often just clamped to a fixed, arbitrary value (usually: α = 1), thereby dispensing with any need to understand what the softmax parameter does and how its values could be interpreted. In fact, if the scores can vary freely and are just estimated from the data, as in multinomial regression or neural network models for categorization, having α as an additional free parameter during training would make the model overspecified (see Fact 8 in Appendix A). In sum, for IO models, what matters most are the formal properties of softmax covered in Sections 4 and 5.</p><p>IM models, on the other hand, are intended to be evaluated not only based on accurate prediction of (some aspect of) the data, but their inner mechanisms are supposed to be meaningfully interpretable and transparent, so that they may function as explanatory models of a phenomenon of interest. Examples of IM models are probabilistic cognitive models <ref type="bibr" target="#b5">(Lewandowsky &amp; Farrell, 2011;</ref><ref type="bibr" target="#b4">Lee &amp; Wagenmakers, 2015)</ref> or agent-models in multi-agent simulations, game theory or population dynamics <ref type="bibr" target="#b2">(Goeree et al., 2008;</ref><ref type="bibr" target="#b9">Sandholm, 2010)</ref>. Requiring internal interpretability, IM models should care about the conceptual justification of the softmax function: after all, for a particular use case and the phenomena to be modelled or explained, a different mapping from non-normalized scores to probabilities may be more appropriate than softmax. Moreover, IM models may use internal parameters, including the non-normalized scores, which are independently meaningful (e.g., interpretable as expected utilities of a decision maker). If so, they are not unconstrained, so that estimation of the softmax parameter α becomes important, and so does the question of how to interpret values of the softmax parameter. Consequently, Section 6 builds on results from Section 5 to enlarge on the interpretation the softmax parameter. Subsequently, Sections 7, 8, and 9 cover three possible conceptual interpretations of the softmax, based on two mathematical derivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Softmax by I/O</head><p>The softmax function takes two inputs, the scores s and parameter α, and returns a probability vector p (see <ref type="figure">Figure 1</ref>). To better understand what softmax does, let us explore how different inputs change the output.</p><p>Simple I/O. Our first example (see <ref type="figure" target="#fig_0">Figure 2</ref>) assumes that there are ten outcomes x = x 1 , . . . , x 10 (e.g., food items on a menu) associated with a vector of scores s = s 1 , . . . , s 10 (e.g., the agent's preferences for each of these choices). Higher scores s i are associated with a higher preference for the choice x i . <ref type="figure" target="#fig_0">Figure 2</ref> shows that the probability vector returned by the softmax function maps higher numerical scores to higher probabilities. In other words, the mapping from scores to probabilities is monotone increasing (for α &gt; 0). Consequently, the highest probability is assigned to the outcome(s) with the highest score. Moreover, we see that the mapping is non-linear. This is because the scores are input to an exponential function, which is a non-linear operation. We see non-linearity in the example by noticing that the score difference between the first and second highest scoring options is 1, while that between the second and third highest-scoring option is 2. Nevertheless, the probability difference between the first and second option is bigger than that between the second and the third. (We will see later that non-linearity is a result of a conceptual motivation behind softmax, namely that differences between scores should correspond to probability odds, not probability differences.)</p><p>Changing α. The rows in <ref type="figure" target="#fig_1">Figure 3</ref> show the effect of changing the softmax parameter α for fixed scores. Generally speaking, the parameter α shapes the form of the probability distribution p by increasing or decreasing, respectively, the ratios between higher and lower scoring options. Increasing values for α makes higher-scoring options increasingly likely relative to lower-scoring options, ultimately approaching the output of the argmax function as α approaches ∞. In the penultimate row in <ref type="figure" target="#fig_1">Figure 3</ref> we see that already increasing to α = 5 results in a probability vector that places almost all probability mass on the single top-scoring outcome, at least for the score vectors in the left and middle column. At α = 0, any information contained within the scores is lost due to their multiplication with  <ref type="bibr">2, 2.3, 2.7, 3, 7, 9, 10 .</ref> 0, so that the result is a uniform probability distribution over outcome categories, as shown in the middle row in <ref type="figure" target="#fig_1">Figure 3</ref>. Finally, for values of α &lt; 0, the softmax function puts increased emphasis on the outcomes with the lowest score, resulting in a probability distribution p that favors outcomes which minimize the score. An example for α = −1 is given in the last row of <ref type="figure" target="#fig_1">Figure 3</ref>. Negative values of α results in output that puts higher probability on outcomes the lower their scores are, i.e., as if we tried to minimize scores.. As α approaches −∞, the output of the softmax function approaches the output of the argmin function. However, most often the range of α is restricted to non-negative or even positive values for technical or conceptual reasons.</p><p>Changing scores. Let us finally also explore how the output probabilities depend on properties of the input scores when keeping α fixed. The key to understanding how scores affect the softmax output is: the only thing that matters are differences between scores (see Fact 2 in Section 5). This observation has important consequences. First, if we add the same number to all scores, the output of the softmax function remains unchanged (see Fact 6 in Appendix A). This is shown in <ref type="figure" target="#fig_1">Figure 3</ref> in the left and middle column. A consequence of this is that, even if all scores are negative, all probabilities p i in the resulting probability vector p are positive and sum up to one (see Fact 1 in Section 5). Second, the softmax function is not invariant to multiplicative transformations of the scores. This is because multiplication with a positive constant change the relative differences between scores s i and s j (see <ref type="figure" target="#fig_1">Figure 3</ref> left vs. middle column). The latter, in particular, means that the softmax function is not invariant to transformations like the standardization of scores. Multiplicative score transformations can, however, be recovered by dividing the optimality parameter α by the same constant (see Fact 8 in Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Properties of the softmax function</head><p>This section covers important properties of the softmax function in some technical detail. As mentioned in the previous section, the most important observation that will help unlock a better understanding of the softmax operation is that what matters to output probabilities are the differences between input scores (Fact 2). Well-behaved output for well-behaved input. To start with, let's reassure ourselves that the softmax operation is well-behaved in the sense that it yields positive probabilities p i &gt; 0 for all input scores (as long as there are finite outcome categories and all scores are finite).</p><formula xml:id="formula_2">−2 −1 0 1 x 1 x 2 x 3 x 4 s = 〈 −2, −2, 0, 1 〉 3 4 5 6 x 1 x 2 x 3 x 4 s = 〈 3, 3, 5, 6 〉 −1.0 −0.5 0.0 0.5 x 1 x 2 x 3 x 4 s = 〈 −0.5, −0.5, 0, 0.25 〉 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 1 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 1 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 1 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 0 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 0 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 0 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 5 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 5 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = 5 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = −1 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = −1 0.0 0.5 1.0 x 1 x 2 x 3 x 4 α = −1</formula><p>Fact 1. Let s = s 1 , . . . , s n be a finite-length vector where all s i are finite. Then p = SoftMax(s; a) is a probability vector with positive probability for all outcome categories, i.e., i p i &gt; 0 for all and</p><formula xml:id="formula_3">n i=1 p i = 1.</formula><p>A proof of Fact 1 is in Appendix B, as are the proofs for all other formal results to follow.</p><p>Defining probabilities through odds ratios. Any finite probability vector p = p 1 , . . . , p n over n categories can be fully determined in terms of just n − 1 real numbers. Since probabilities must sum up to one, it suffices to give, for example, just the probabilities p 2 , p 3 , . . . , p n , since the missing number p 1 can be retrieved as</p><formula xml:id="formula_4">p 1 = 1 − p 2 − p 3 − . . . − p n .</formula><p>Indeed, an n-place probability vector can also be fully specified in terms of n − 1 odds. The odds in favor of option i over option j are the fraction of probabilities p i/p j . The probability vector p = p 1 , . . . , p n is fully determined, for instance, via the sequence of odds p 1/p 2 , p 1/p 3 , . . . , p 1/p n . For example, in multinomial regression the choice probabilities for n categories are usually determined in terms of n − 1 log-odds with respect to a single fixed reference category. One way to think of the softmax function is to fix how exactly a vector of scores translates into a sequence of odds, like</p><formula xml:id="formula_5">p 1/p 2 , p 1/p 3 , . . . , p 1/p n .</formula><p>Softmax defines odds in terms of score differences. Here is a simple mathematical result that helps understand many conceptual and technical aspects of the softmax function.</p><p>Fact 2. If p = SoftMax(s; α), then the odds p i/p j are a direct function of score differences s i − s j , namely:</p><formula xml:id="formula_6">p i/p j = exp (α (s i − s j )).</formula><p>This observation invites us to look at softmax as a function that determines, first and foremost, how differences between scores map onto probability ratios (odds), which in turn define the whole probability distribution (as explained above). This has implications for how to think of the scores themselves. If we use softmax, the absolute value of scores does not matter, only the differences do. So, adding or subtracting a constant to all scores does not change anything (see Fact 6 in Appendix A), but multiplying all scores with a positive number (other than one) will (Fact 7 in Appendix A). Conceptually, this means that scores are not meaningful in absolute terms, but only relatively, through their differences.</p><p>Moreover, Fact 2 highlights that the odds p i/p j only depend on the scores assigned to options i and j, but not the scores assigned to any other alternatives. This is the sense in which the softmax operation assures that odds of options x i and x j are independent of the presence, absence or score of alternatives x k (k i, k j). 4 This independence property is conceptually and technically important in different ways for different areas of application of the softmax function. For a cognitive model, for example, if scores represent accumulated evidence for different categories, it is intuitive to assume that relative choice probabilities for two options x i and x j do not depend on the accumulated evidence in favor of any third alternative x k . For a data-driven model, like a multinomial regression model or a neural network classifier, if we want to learn a target distribution over categories, then seeing training data that make us adjust the relative probability between x i and x j should ideally involve "local changes" to internal scores of only those two options, without thereby altering the odds of unrelated options x k and x l .</p><p>Example with two outcome options: Relation to logistic function. A simple example for thinking about softmax in terms of odds is the special case where we only have two outcome categories n = 2. In this case, we only need a single number, like the probability p i of a reference category x i , to determine the outcome probability vector. For n = 2, the softmax function "reduces to" the logistic function, so speak. Concretely, with n = 2 the probability p i of option x i is given by the logistic function of the difference in score between x i and x j (i j).</p><p>Fact 3. When n = 2, the softmax probability p i is given by a logistic function of the score difference</p><formula xml:id="formula_7">d = s i − s j : p i = 1 1+exp(−αd)</formula><p>. <ref type="figure" target="#fig_2">Figure 4</ref> shows examples of the choice probability for the case of n = 2. The plot shows the S-shaped curve for choice probabilities as a function of score differences, and the way in which the softmax parameter modulates the steepness of this S-curve.</p><p>Further properties. We refer the more technically minded reader to Appendix A, where we list further properties of the softmax function that would take us too far into the weeds for the purpose of this tutorial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Interpreting values of the optimality parameter</head><p>This section considers different ways of making sense of numerical values of the optimality parameter. This is usually only relevant when dealing with models in which the optimality parameter is free to vary and, ideally, the scores are interpretable (even if other parts of the IO model are not). When modeling goals make it relevant, interpretability of α can be important in at least two ways: (i) for interpretability across different models or data sets, e.g., for interpreting model fits (Bayesian posteriors or point-estimates, like MLE) for different models, or for the same model for data from different populations; and (ii) for interpretability within a single model and data set, e.g., for specification of reasonable constraints on the optimality parameter, such as priors in Bayesian modeling.</p><p>This section suggests two routes towards a better understanding of α. The first route builds on the observation expressed in Fact 2, that it is score differences that matter to softmax predictions. Based on this idea, we consider three questions one can ask about the values of α: (i) how to interpret a single, fixed value of α (e.g., as returned by a point-estimate after parameter estimation); (ii) what difference it makes to either use α 1 or α 2 ; and (iii) how to choose priors for optimality parameters in Bayesian data analysis. The second route to interpreting α is more holistic in that it is based on the average score entailed by or the entropy associated with the softmax distribution.</p><p>Optimality parameter as log-odds ratio under unit score difference. We saw in Section 5 (Fact 2) that what determines softmax probabilities are differences between scores:</p><formula xml:id="formula_8">p i p j = exp(α (s i − s j ))</formula><p>. This observation provides an intuitive interpretation of the optimality parameter α in terms of the log-odds given a unit score difference. A unit score difference is a case where s i − s j = 1. For such a case we have:</p><formula xml:id="formula_9">p i p j = exp α (s i − s j ) = exp α</formula><p>Consequently, α gives the log-odds under a unit score difference:</p><formula xml:id="formula_10">α = log p i p j</formula><p>For example, say that we have options x i and x j whose scores differ by 1, i.e., s i − s j = 1. Say further that we find that a maximum likelihood fit yieldsα = 5. Withα = 5 o i is predicted to be chosen with a probability that is exp(5) ≈ 150 times larger than the probability of o j . Whether such an estimate of α, is to be judged large or small depends on whether a unit difference in scores itself is to be interpreted as large or small, which depends on the application at hand. If score differences are not interpretable, using the holistic strategy of interpretation introduced below might be more informative.</p><p>Interpreting differences between optimality parameters. To understand the difference between two optimality parameter values, we compare SoftMax(s; α) against SoftMax(s; α ) by looking at the factor f = α α by which α differs from α and note that:</p><formula xml:id="formula_11">p i p j = exp α (s i−s j ) p i p j = exp α (s i−s j ) = exp f α (s i−s j ) = exp α (s i−s j ) f = p i p j f</formula><p>In words, if the optimality operator increases by a factor f , the odds of choosing</p><formula xml:id="formula_12">x i over x j (s i &gt; s j )</formula><p>increase by the power of f . For example, suppose you obtain a maximum-likelihood estimate ofα 1 for the data from an online experiment, andα 2 for the data from the same experiment executed in the lab. Assume further that α 2 is twice as large asα 1 . We can interpret this, following Fact 10, as follows. The probability vector p 1 predicted byα 1 has to be scaled by a power-law transformation function with power parameterα 2 α 1 to obtain the vector p 2 which is predicted from softmax with p 2 . Whether this can be interpreted as large or small would again depend on which magnitude of score differences would count as large or small in the current application.</p><p>Prior distributions over α. If we interpret α, in line with Fact 2, as the log odds, log p i p j , for a unit difference in scores s i − s j = 1, a natural choice of family for a prior on the optimality parameter in Bayesian data analysis is a log-normal distribution (or any other log-something distribution). The mean of the log-normal prior distribution would correspond to the modeller's prior assumption about the expected log odds for a unit difference in scores. The variance of the log-normal prior distribution should be chosen based on the modeller's uncertainty about the log odds for a unit difference in scores. When choosing the variance, guidance is provided by the previous result that increasing α by a factor of f increases odds by the power f . The picture becomes more complicated when the scores are not fixed in advance but hinge on other model parameters. In that case, priors should always be chosen "holistically" and in light of the plausibility of the entailed prior predictive functions <ref type="bibr" target="#b10">(Schad et al., 2021)</ref>.</p><p>Holistic interpretations of α. Let us also consider a completely different approach to making sense of values of α. If we fix s and consider p = SoftMax(s; α), the functional role of α is to modulate the softmax probabilities p. Consequently, we can also make sense of α-values by looking at suitable numerical values associated with p, i.e., by some function F(α) ∈ R. This approach is holistic in the sense that it tries to make sense of α based on the whole distribution p, rather than just in terms of comparing the odds of two options. Ideally, the function F has some practically useful properties, like being monotonic, and relates to intuitively meaningful concepts.</p><p>A useful choice is the expected score, c = p • s, and the negative entropy, where entropy is defined as H(p) = − i p i log p i . These metrics are especially useful when scaled so that the zero-value is associated with the probability vector obtained for the case of α = 0, and the one-value associate with the limiting result for α → ∞. In this way, we obtain holistic metrics quantifying the effect of α on a closed scale that can be interpreted to range from, intuitively speaking, zero optimization (totally random choice) to maximum optimization. The top row of <ref type="figure" target="#fig_5">Figure 5</ref> shows the functional relationship between α and the (scaled) expected scores, the bottom row shows the resulting (scaled) negative entropy of the softmax distribution, for two different vectors of scores (which reoccur in the example in <ref type="figure">Figure 8</ref>).</p><p>Based on a holistic interpretation, modellers can interpret a given value of α as a "degree of optimization." For instance, the examples in <ref type="figure" target="#fig_5">Figure 5</ref> show that to approach the fully optimal (argmax) policy, larger values of α are required for Bo (right-hand side of <ref type="figure" target="#fig_5">Figure 5</ref>) than for Alex (lefthand side). Plots of this kind can be used to find which values of α would count as an expected degree of optimization, e.g., to inform the choice of the mean of a Bayesian prior, and we can inspect how changes in α lead to changes on this scale (such as to inform the variance of a Bayesian prior). Unfortunately, while the mapping from α to average score is straightforward to calculate, the inverse does not have a simple mathematical solution. Nevertheless, an arbitrarily close approximation can be obtained with numerical estimation (see example R code in Appendix C). softmax parameter α <ref type="figure" target="#fig_5">Figure 5</ref>: Examples of the relation between softmax parameter α and the (standardized) expected score achieved by the softmax policy for the given α (top row), as well as the (standardized) negative entropy of the softmax policy (bottom row). Values are scaled to be zero for the expected score under α = 0, i.e., when choices are uniform at random. Values are scaled to be one for the softmax policy resulting from α → ∞, i.e., when decisions approximate the arg-max function. The plots show how the interpretation of α depends (holistically) on the scores: for some cases it can be easier to achieve high average scores of negative entropy with lower values of α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Softmax as the outcome of noise-perturbed selection</head><p>When dealing with internally-meaningful (IM) models, the conceptual interpretation of all model ingredients matters, including that of the softmax function. modellers therefore might need to justify why the model contains softmax and not some other function mapping scores onto probabilities. In this and the two following sections, we provide three different conceptual interpretations of the softmax distribution, which yield three different rationales for its use. In this section, we give a mathematical derivation that supports a possible mechanistic interpretation of the softmax function, namely as the distribution resulting from errors in the computation or assessment of the scores. Under this interpretation, the softmax function implements the assumption that a choice process is approximate, sub-optimal, or error-prone. Concretely, the distribution which is returned by the softmax function can be derived as the probability distribution entailed by a stochastic choice mechanism, in which the choice of outcome x i is optimal (an x i with the highest score is chosen), but in which the scores themselves are "wiggled" or "noise-perturbed" each time a decision has to be made <ref type="bibr" target="#b7">(Luce, 1959;</ref><ref type="bibr" target="#b12">Train, 2009)</ref>. Given specific choices about the probability of the "wiggles," we can derive the output of softmax function as the expected choice frequencies. <ref type="figure">Figure 6</ref>, to be unpacked below, illustrates this process.</p><p>Let's think of x 1 , . . . , x n as an agent's available choice options (actions), each with a score s 1 , . . . , s n which tells us how good each choice is. The agent in question could be a human decision maker (making perceptual or economic decisions), but it could also be an abstract system "choosing" a category based on the objective to maximize the score. An optimal (or rational) agent would always only choose x i if s i = max j s j , so an optimal agent would maximize the score perfectly. But let us now assume that there is room for imperfection. Mistakes happen. But it's not the case that all mistakes are equally likely. An agent is more likely to choose a suboptimal option whose score is almost maximal than to choose a suboptimal option which is far worse. Essentially, this leads to the desire to formulate probabilistic choice rules such that the probability p i of choosing x i is higher the higher its relative score is <ref type="bibr" target="#b7">(Luce, 1959;</ref><ref type="bibr" target="#b12">Train, 2009)</ref>. The softmax choice rule can be derived as the probability p i that an agent chooses x i as the best choice from a noise-perturbed representation s 1 , . . . , s n of the scores, where s j = s j + j and all j are independently and identically distributed random noise perturbations of the actual scores of the available options. In other words, each time the agent chooses from x 1 , . . . , x n , they choose x ∈ arg max j (s j + j ) for a vector of errors = 1 , . . . , n which are sampled anew every time a choice is made (see <ref type="figure">Figure 6</ref>).</p><p>To derive the softmax function, we must make specific assumptions about the probability distribution from which each is sampled. Concretely, we assume that the error terms j come from a Gumbel distribution with location µ = 0 and scale parameter β &gt; 0 (see proof of Fact 4 in Appendix B for a definition of the Gumbel distribution), in order to derive the following:</p><p>Fact 4. The softmax distribution p = SoftMax(s; α) is the expected choice probability if outcome categories are selected based on maximization of noise-perturbed scores s , where each s i = s i + i is obtained by adding an iid sample i from a Gumbel distribution with location µ = 0 and scale β = 1 α . <ref type="figure" target="#fig_4">Figure 7</ref> gives examples of the probability density function of Gumbel distributions for different values of α = 1 β . The plots show that the error assumed in this derivation has a certain structure, namely that larger positive values for are more likely than very small negative ones. This means that a model that contains the softmax function and interprets it as the result of stochastic errors, as described here, commits itself to a rather specific sort of error. However, this assumption is necessary to obtain a result like Fact 4; a similar result is not derivable, for example, under the assumption that error terms are normally distributed <ref type="bibr" target="#b12">(Train, 2009)</ref>.</p><p>In sum, the derivation of softmax presented in this section tells us that there exists at least one way of justifying the softmax function in terms of a specific "noisy process model." This derivation must make rather specific assumptions about the nature of the assumed noise perturbation (additive trembles, iid-sampled from a specific distribution, namely a Gumbel distribution). This gives rise to two questions: First, what should we do if we have more specific domain knowledge about a likely noise process that adds "trembles" or stochasticity to the modelled choice process? -Here, the ideal situation would be to model the noise process directly because this makes for a more robust and empirically testable model. Second, what if have no (strong) prior conceptualization of how stochasticity in choice might arise? What if, specifically, we have no a priori reason to believe that the iid-additive Gumbell trembles are a reasonable or salient model of stochasticity in the choice process? -In this case, you might still want to use the softmax function, because it is not only compatible with the iid-additive Gumbell model, it is also, in a sense, the most neutral, or least specific model of stochastic trembles. This is the topic of the next section. <ref type="figure">Figure 6</ref>: Illustration of repeatedly sampling the best option after noise-perturbations (α = 0.8). The first column shows the number of choices the agent made. The second column shows the actual scores of three outcome categories (which are the same for each choice / row). For each choice (row) a new vector of Gumbel-distributed perturbations are sampled (column 3) and added to the original scores (as shown in column 4, where the actual scores are indicated with slimmer black bars). Based on the perturbed scores the best choice is selected (column 5). The final column shows the relative frequency choices so far. In the limit, as shown in approximation for 1000 trials in the last row, this frequency distribution approaches the softmax distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Softmax as optimal choice if error source is unknown</head><p>This section shows that the softmax function is a maximum entropy distribution (for certain constraints), so that it is arguably a natural go-to option in the absence of strong prior knowledge of the noise processes that might affect the probabilities to be modelled. The previous section showed that softmax is a motivated choice if we want to model a noisy decision process, and if we are willing to accept, at least for practical purposes, a specific assumption about the origin of stochasticity. The result presented in this section can be interpreted as suggesting that we can think of the softmax function as the most neutral, or least assuming, model of a stochastic choice process with unknown error source.</p><p>Let us start with some motivating examples. Consider the following score vector for a three-way choice of Alex (see <ref type="figure">Figure 8</ref>):</p><formula xml:id="formula_13">s [A] = 0, 1, 1</formula><p>Alex uses a stochastic choice policy p <ref type="bibr">[A]</ref> , but you do not know which. Suppose that you knew or reasonably assumed for some purpose that the average score under Alex's choice policy, when repeatedly choosing one of the three options, is p <ref type="bibr">[A]</ref> • s <ref type="bibr">[A]</ref> = 0.75. There are infinitely many stochastic policies that would achieve an expected score of 0.75, but they are not all equally "neutral" or "unassuming." For example, two choice policies that yield the average score of 0.75 for Alex are these: p <ref type="bibr">[A,1]</ref> = 0.25, 0.75, 0 p <ref type="bibr">[A,2]</ref> = 0.25, 0.375, 0.375</p><p>In p <ref type="bibr">[A,1]</ref> Alex never chooses option 3, but chooses option 1 with probability 0.25 and option 2 with probability 0.75. In p <ref type="bibr">[A,2]</ref> Alex chooses option 1 with probability 0.25 and options 2 and 3 with probability 0.375 each. A function which yields p <ref type="bibr">[A,1]</ref> for Alex's scores clearly makes rather specific assumptions about Alex choice processes which are not apparent just from the scores. In other words, if the scores are the only thing that we know about what influences Alex's (possibly noisy) choice process, stipulating p <ref type="bibr">[A,1]</ref> seems unwarranted: where should the difference between options 2 and 3 come from when it is not in the scores? On the other hand, the policy p <ref type="bibr">[A,2]</ref> does not seem to make any such additional assumptions about Alex's choice process. In fact, it is easy to see that it is the only three-place probability vector which (i) yields the fixed average score of 0.75 and (ii) <ref type="figure">Figure 8</ref>: Two different stochastic choice policies for each of two score vectors. All pairs of scores and choice policies have an expected score of 0.75. Although there are infinitely many stochastic choice policies that yield a fixed expected score (see the left side on each panel), the softmax rule (on the right of each panel) always yields a "neutral" or "unassuming" choice policy corresponding to the maximum entropy distribution that generates the given average score. (Values for the softmax function that achieve the expected score of 0.75 are α ≈ 0.41 (left) and α ≈ 1.67 (right).)</p><p>assigns probabilities to choice options that respect the underlying scores (higher score yields higher probability; equal score yields equal probability). It is in this sense that p <ref type="bibr">[A,2]</ref> is the more "neutral" or "unassuming" model for Alex's stochastic choice policy. Indeed, as we will see below, p <ref type="bibr">[A,2]</ref> is the maximum entropy distribution satisfying the constraint that average scores equal 0.75 and p <ref type="bibr">[A,2]</ref> is the only distribution that results from the softmax rule that yields the average score of 0.75. Let us also briefly consider a second example. Suppose that Bo's scores are these:</p><formula xml:id="formula_14">s [B] = 0, 0.5, 1</formula><p>Bo uses a stochastic choice policy p <ref type="bibr">[B]</ref> , but you, again, do not know which. Let's assume that Bo, too, has a known average score of p <ref type="bibr">[B]</ref> • s <ref type="bibr">[B]</ref> = 0.75. In this case, there are infinitely many stochastic policies which yield this average score and also satisfy the constraint that they assign probabilities to choice options that respect the underlying scores (higher score yields higher probability; equal score yields equal probability). For example, the following two policies of Bo's meet both requirements; again, see <ref type="figure">Figure 8</ref> for an illustration: <ref type="bibr">B,1]</ref> = 0.15, 0.2, 0.65 p <ref type="bibr">[B,2]</ref> ≈ 0.116, 0.269, 0.616</p><formula xml:id="formula_15">p [</formula><p>The first vector p <ref type="bibr">[B,1]</ref> is an example that was chosen arbitrarily for illustration. The second vector p <ref type="bibr">[B,2]</ref> is the prediction of softmax for α = 1.66823, the unique real-valued α that satisfies SoftMax(s <ref type="bibr">[B]</ref> ; α) • s <ref type="bibr">[B]</ref> = 0.75. Unlike for the case of Alex, it may not be clear which one of these stochastic choice policies is more or less "neutral" or "unassuming." It depends on what we mean by these terms. One salient contender for filling in the blank is information theory. The entropy of a (categorical) distribution p is defined as:</p><formula xml:id="formula_16">H(p) = − j p j log p j</formula><p>In words, entropy measures the expected surprisal for an agent with beliefs p if outcomes indeed occur with probabilities described by p. Therefore, if we -as modellers-would like to minimize surprisal on average in the stochastic policies that we ascribe to a process of decision making, we best select a distribution that maximizes entropy (i.e., minimizes modeller's surprisal on average). The entropy of the two candidate policies in the running example are: <ref type="bibr">,1]</ref> ) ≈ 0.887 H(p <ref type="bibr">[B,2]</ref> ) ≈ 0.901 So, from the two candidate policies that both yield the same average score, the softmax policy has higher entropy and is therefore "more neutral" or "less assuming" in an information-theoretic sense. Indeed, it is provable that, in general, softmax yields the maximum entropy distribution for a given value of the expected score. Assume that we know the scores s = s 1 , . . . , s n and we know the expected utility c = p • s. We don't know anything else about p = p 1 , . . . , p n , except that i p i = 1. Then we can prove: In sum, this result tells us that the softmax operation yields probability distributions which are, in a particular sense, neutral or unassuming. Consequently, we can motivate the choice of softmax in a model by saying that this is what researchers should optimally choose if they assume that choices are stochastic, but have no idea about, or do not want to make any commitment regarding, what the underlying error source could be, and want to otherwise remain as neutral as possible. As with the result presented in Section 7, this interpretation is itself not unassuming, because it requires that we accept that information-theoretic entropy is a good formalization of the relevant notion of "neutrality." This interpretation also puts particular emphasis on the constraint necessary to derive Fact 5, namely that the expected score is meaningful to the researcher. To address these issues, the following section looks at a completely different conceptual interpretation of the mathematical result presented in Fact 5.</p><formula xml:id="formula_17">H(p [B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Softmax as optimal balance between exploration and exploitation</head><p>The previous two interpretations of the softmax function were anchored in the idea that stochasticity, i.e., potential deviance from the α → × prediction, are sub-optimal or errors. This line of interpretation assumes that everything that matters to evaluating the goodness of a choice is captured in the scores, which are immutable. However, several converging lines of research plead for more nuanced pictures of "optimality," to also include representation and computation costs in the form of bounded rationality <ref type="bibr" target="#b11">(Simon, 1959)</ref> or resource-bounded rationality <ref type="bibr" target="#b6">(Lieder &amp; Griffiths, 2019)</ref>. Other work stresses longterm adaptive or ecological rationality, possibly also taking a changing environment into account (e.g., <ref type="bibr" target="#b0">Anderson, 1990;</ref><ref type="bibr" target="#b1">Chater &amp; Oaksford, 2000;</ref><ref type="bibr" target="#b3">Hagen et al., 2012;</ref><ref type="bibr" target="#b8">McNamara, 2013)</ref>. In this general line of thought, we may think of the softmax function as realizing an optimal tradeoff between exploitation and exploration.</p><p>Consider decision-maker Alex in <ref type="figure">Figure 8</ref>, with the scores s = {0, 1, 1} for three choice options. The minimum score that Alex can attain is 0, the maximum is 1. If Alex wants to score minimally, there is only one thing to do: choose the first option. If Alex wants to score maximally, well, there are infinitely many stochastic choice protocols that serve. One of them is the maximum entropy distribution (for the aspired expected score of 1) where Alex chooses options 2 and 3 with equal probability. Is there anything special about this stochastic choice protocol from the point of view of ecological rationality? -Yes! Suppose that Alex makes repeated decisions in a variable environment, where scores can change over time. To detect changes in scores (and therefore exploit them quickly), it is -intuitively speaking-wise to play "maximally random" for a given desired expected outcome. Since similar considerations also apply to cases where Alex aspires to less than an average score of 1, this provides at least an informal, intuitive, rationale for the use of the softmax function.</p><p>In sum, rather than assuming that it is the modellers' expectation for a particular score, and the modellers' uncertainty about the error source that grounds the mathematical result reported in Fact 9, we can also adopt the agent perspective: if an agent wants to realize a fixed expected score, but also wants to hedge their bets maximally in the face of a dynamically changing world, then, if we assume that information-theoretic entropy is a good formalization of "playing as randomly as possible," we can motivate the softmax function as the optimal balance between exploitation (obtaining a fixed expected score) and exploration (using a choice policy that maximizes entropy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Summary &amp; conclusion</head><p>To sum up, the softmax function is frequently used in different kinds of models. To understand the models in which the softmax function occurs, at least an understanding of the input-output behavior of the softmax function is necessary. Knowledge of some mathematical properties helps better understand this input-output behavior, most notably the observation that softmax probabilities are really a function of differences between scores. Formal properties of the softmax function also help to interpret the α parameter, for which this tutorial suggested two perspectives: one in terms of (log) odds for unit differences in scores, and another, more holistic, in terms of either (scaled) expected scores or (scaled) negative entropy, both of which yield metrics that can be intuitively interpreted as "degree of optimization."</p><p>The tutorial introduced a distinction between what we called input-output models and internallymeaningful models, where the latter, but not necessarily the former, require explainability of all model ingredients, including the softmax function. Three different conceptual interpretations of the softmax function were offered, each of which may justify its use in a given situation. Concretely, we characterized the softmax distribution as either: (i) the expected choice distribution if choices are noiseperturbed in a particular way, (ii) the most neutral assumption about stochastic noise-perturbed choice if the specific error distribution is unknown, and (iii) the optimal tradeoff between exploitation and exploration. None of these interpretations are free of controversial assumptions. We must either assume particular error-distributions, or that information-theoretic entropy is the right formalization of intuitive concepts like "neutrality" or "exploration." Whether this is (approximately) correct for any particular modeling situation is something the modellers and the receiving community have to assess critically on a case-by-case basis.</p><p>A More formal properties of softmax Invariance under addition. Since softmax probabilities are a function of the differences between input scores, adding or subtracting the same number from all scores should not change the softmax outcome:</p><p>Fact 6. Softmax is invariant under addition: for all a ∈ R, SoftMax(s; α) = SoftMax(s + a; α).</p><p>Non-invariance under multiplication. If we multiply all scores with the same factor a 1, this does affect the softmax probabilities, expect in special cases.</p><p>Fact 7. Softmax is not invariant under multiplication: if a ∈ R &gt; 0 is a constant, then SoftMax(s; α) = SoftMax(a s; α) only in trivial cases, namely if α = 0, a = 1 or s i = s j for all i and j.</p><p>Adjusting α. Even though, by Fact 7, softmax is not invariant under multiplication of scores, the effect of multiplication by positive factor a &gt; 0 can be compensated with the choice of a different softmax parameter α. This is the content of the following fact, and important to understand that a model that leaves all scores and α as free parameters (to be inferred from or optimized based on some data) is overspecified (unless it adds additional constraints, such as Bayesian priors).</p><p>Fact 8. Multiplicative factors can be recovered by different optimality parameters: if a ∈ R &gt; 0 is a constant, then SoftMax(s; α) = SoftMax(a s; α /a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change of base.</head><p>Although the softmax operation is frequently implemented in terms of the exponential function with base e, this is not a necessity (technically speaking). We can recover any other base by changing the α parameter accordingly.</p><p>Corollary 9. Fact 8 implies that the softmax function need not be expressed in base e, but can equivalently be expressed in terms of any basis b &gt; 0 if we change the softmax parameter accordingly.</p><p>Power-law decomposition. For a given probability vector p, a common transformation function (for scaling odds) is the power law transformation, defined as follows:</p><formula xml:id="formula_18">Pow(p; α) = q, with: q i = p α i j p α j</formula><p>We can think of the softmax operation with parameter α = a as a composition of softmax with α = 1, followed by a power law transformation with α = a.</p><p>Fact 10. Pow(SoftMax(s, 1); α) = SoftMax(s, α)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head><p>Proof of Fact 1. The non-negativity of p i follows directly from the reciprocity of the exponential function exp (x). For x ≥ 0, the exponential function is defined as:</p><formula xml:id="formula_19">exp (x) = 1 + x + x 2 2 + . . . ≥ 1 &gt; 0</formula><p>For x &lt; 0, we have:</p><formula xml:id="formula_20">exp (x) = 1 exp (−x) &gt; 0 Since exp (x) &gt; 0 for all x ∈ R, exp (α s i ) j exp (α s j ) &gt; 0 for all s i , s j , α ∈ R. That n i=1 p i = 1 is assured by normalization in the definition of softmax. Proof of Fact 2. p i p j = exp(α s i ) k exp(αs k ) k exp(αs k ) exp(α s j ) = exp(α s i ) exp(α s j ) = exp(α s i − α s j ) = exp(α (s i − s j ))</formula><p>Proof of Fact 3.</p><formula xml:id="formula_21">p i = exp(α s i ) exp(α s i ) + exp(α s j ) = exp(α s i ) exp(α s i ) + exp(α s j + s i − s i ) = exp(α s i ) exp(α s i ) + exp(α s j ) exp(α − d) = 1 1 + exp(−αd)</formula><p>Proof of Fact 4. In general, the Gumbel distribution has the following cumulative density and probability density functions:</p><formula xml:id="formula_22">F( ; µ, β) = exp − exp − − µ β f ( ; µ, β) = 1 β exp − − µ β exp − exp − − µ β .</formula><p>The variance of this distribution is a function of the scale parameter: π 2 6 β 2 . This is important for interpreting the optimality parameter α of the softmax function, because we will set α = 1 β to obtain:</p><formula xml:id="formula_23">F( ; µ = 0, β = 1 /α) = exp − exp (−α ) f ( ; µ = 0, β = 1 /α) = α exp (−α ) exp − exp (−α ) .</formula><p>The stochastic "wiggles" can be thought of as random errors in the computation of the scores. (We could say that the agent makes rational choices given a momentarily and subjectively distorted representation of actual scores.) The probability that an agent who maximizes (noise-perturbed) utility chooses action a i is therefore:</p><formula xml:id="formula_24">p i = P(∀ j i : s i + i &gt; s j + j ) = P(∀ j i : j &lt; i + s i − s j )<label>(1)</label></formula><p>Given the assumed distribution of noise perturbations , we can spell out the probability p i from (1) as a function P(x i ; α) of α. Let's first assume, unrealistically, that we would know the value of i . From the right-hand side of (1), p i would then be determined by how likely it is to sample a set of j -s all of which are below a given threshold i + s i − s j . Since all j are sampled independently, this is the product of the cumulative densities for all j being smaller than the threshold i + s i − s j :</p><formula xml:id="formula_25">P(x i ; α) | i = j i F( i + s i − s j ; µ = 0, β = 1 /α) = j i exp − exp −α( i + s i − s j )</formula><p>But, of course, we do not know the value of i . We only know its distribution, so that:</p><formula xml:id="formula_26">P(x i ; α) = f ( i ; µ = 0, β = 1 /α) j i exp − exp −α( i + s i − s j ) d i = α exp (−α i ) exp − exp (−α i ) j i exp − exp −α( i + s i − s j ) d i = α exp (−α i ) j exp − exp −α( i + s i − s j ) d i = α exp (−α i ) exp          − j exp −α( i + s i − s j )          d i = α exp (−α i ) exp          − exp (−α i ) j exp −α(s i − s j )          d i = α exp (−α i ) exp −c exp (−α i ) d i          with c = j exp −α(s i − s j )          = exp(−c (−α i )) c ∞ −∞ = lim i →∞ exp(−c exp (−α i )) c − lim i →−∞ exp(−c exp (−α i )) c = 1 c − 0 = 1 j exp −α(s i − s j ) = 1 exp (−αs i ) j exp αs j = exp(α s i ) j exp(α s j )</formula><p>.</p><p>Proof of Fact 5. Let H(p) = − j p j log p j be the entropy of p. Consider further two auxiliary functions, namely f (p) = p • s, and g(p) = i p i . We want to find the critical points of the Lagrangian:</p><formula xml:id="formula_27">L(p, α, β) = H(p) + α ( f (p) − c) + β (g(p) − 1)</formula><p>As usual, the partial derivatives ∂L ∂α and ∂L ∂β reduce to the auxiliary constraints. Suffice it to compute the partial derivatives ∂L ∂p i for an arbitrary 1 ≤ i ≤ n:</p><formula xml:id="formula_28">0 = ∂ ∂p i = − j p j log p j + α j p j s j + β j p j = − log p i − 1 + αs i + β</formula><p>Since the critical point implies ∂L ∂p i = 0, we can solve for p i :</p><formula xml:id="formula_29">p i = exp (αs i + β − 1) = exp (αs i ) exp (β − 1)<label>(2)</label></formula><p>With this, we can expand the second auxiliary constraint:</p><formula xml:id="formula_30">1 = j p j = j exp (αs i ) exp (β − 1)</formula><p>This is equivalent to:</p><formula xml:id="formula_31">exp (β − 1) = 1 j exp (αs i )<label>(3)</label></formula><p>Combining Equations (2) and (3), we get:</p><formula xml:id="formula_32">p i = exp(αs i ) j exp(αs j )</formula><p>Proof of Fact 6. Proof of Fact 7. To begin with, notice that if all scores in s are equal, softmax probabilities will be equal, no matter which α. Also, if α = 0, all softmax probabilities will be equal, no matter the scores. Therefore, fix s with at least two scores s i and s j unequal (s i &gt; s j ) and let p = SoftMax(s; α) for arbitrary α 0. Now consider q = SoftMax(a s; α) for some a. If a = 1, obviously p = q. Otherwise, odds p i/p j are different from odds q i/q j , because from Fact 2, p i/p j = exp(α(s i − s j )) and q i/q j = exp(α(a s i − a s j )), the latter of which is the same as exp(α(s i − s j )) a and further reduces to return ( policy %*% scores -average _ score ) 34 }) 35 } 36 37 # example (and sanity check ) 38 scores &lt;-c(0, 0.5 , 1) 39 alpha _star &lt;-solve _ alpha (scores , 0.75) $root 40 ( alpha _star) 41 (sm_ policy &lt;-softmax (scores , alpha _star)) 42 dot(sm_policy , scores )</p><p>Listing 1: Example R code for estimating the value of α for a given vector of scores and a target value for the expected score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example of the mapping between scores and probabilities under application of the softmax function with α = 1 and a vector of scores s = s 1 , . . . , s 10 = −5, −4, −2,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of the output from the softmax function under different transformations of scores (columns) and different values of softmax parameter α. The three columns are different score vectors. The middle is obtained by adding 5 to each score in the vector on the left. The right score is obtained by multiplying the left scores with 0.25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>−0.5 −0.4 −0.3 −0.2 −0If there are only two outcome categories, the softmax probability of option x i is a logistic function of the score difference s i − s j . The softmax parameter α scales the steepness of the curve, where higher α yields steeper curves. The limiting case of α = 0 yields indifferent choice (ignoring the scores), and that of σ → ∞ a sharp step function (choose the higher scoring option always).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Examples of Gumbel probability density with location µ = 0 and different values for β = 1 /α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fact 5 .</head><label>5</label><figDesc>Softmax(s; α) is the maximum likelihood solution for p under constraints c = p • s and i p i = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>p i = exp(α (s i + a)) j exp(α (s j + a)) = exp(α) s i +a j exp(α) s j +a = exp(α) s i exp(α) a exp(α) a j exp(α) s j = exp(α s i ) j exp(α s j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>C#</head><label></label><figDesc>by assumption s i &gt; s j , we know that exp(α(s i − s j )) &gt; 1, and sop i/s i ) j exp(α s j )Proof of Corollary 9. Since by Fact 8 we have SoftMax(s; α) = p = SoftMax(a s; α /a), setting a = log b for some b &gt; 0, we obtain: of Fact 10. Let C = k exp s k be the normalizing constant for SoftMax(s, 1). Moreover, assume that q = Pow(SoftMax(s, 1); α). Then:−α exp s i α j C −α exp s j α = exp(αs i ) j exp(αs j )C R code for solving for α given expected scores 1 softmax = function (scores , alpha ) { 2 Calculate the softmax probabilities with a temperature parameter . softmax ( scores = c(0, 0.5 , 1) , alpha = 1.0) 13 return (exp( alpha * scores ) / sum(exp( alpha * scores ))) 14 } 15 16 solve _ alpha = function (scores , average _score , upper _ bound = 100) { 17 # Solve for the optimal alpha parameter using the softmax function . vector of scores for each option .21#average _ score : Target average score for the softmax policy .22 # upper _ bound : Upper bound for the alpha parameter ( default is 100). solve _ alpha ( scores = c(0, 0.5 , 1) , average _ score = 0.75)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In some context, such as in machine learning, the scores are sometimes referred to as logits.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We write exp s i to mean e s i , using the same bracketing and association rules as for the log operator. The softmax function usually assumes the base e, but it can be expressed in terms of any base b &gt; 0 if we compensate for this change with adjustments to the α parameter (see Corollary 9 in Section 5).3 In some contexts, this parameter is also sometimes referred to as "optimality parameter" or "rationality parameter." Some authors use the inverse of α, frequently denoted as τ, and refer to it as "temperature."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Notice that other mappings from (positive) scores to probabilities, like a power function or a linear function, do not have this property.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The adaptive character of thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Lawrence Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The rational analysis of mind and behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oaksford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="93" to="131" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantal response equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Goeree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Palfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The new palgrave dictionary of economics</title>
		<editor>S. N. Durlauf &amp; L. E. Blume</editor>
		<imprint>
			<publisher>Palgrave Macmillan</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evolution and the mechanisms of decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Gallistel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalenscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nettle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Stephens</surname></persName>
		</author>
		<editor>P. Hammerstein &amp; J. R. Stevens</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="97" to="126" />
		</imprint>
	</monogr>
	<note>Decision making: What can evolution do for us</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bayesian cognitive modeling: A practical course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computational modelling in cognition: Principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lewandowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Sage Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. The Behavioral and Brain Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Individual choice behavior: A theoretical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards a richer evolutionary game theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Society Interface</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Population games and evolutionary dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Sandholm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward a principled bayesian workflow in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Theories of decision-making in economics and behavioral science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discrete choice methods with simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Train</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
