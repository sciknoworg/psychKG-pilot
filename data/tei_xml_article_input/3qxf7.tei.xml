<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Autocorrelated Bayesian Sampler: A Rational Process for Probability Judgments, Estimates, Confidence Intervals, Choices, Confidence Judgments, and Response Times</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Qiao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Sundh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Spicer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Chater</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Warwick Business School</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Birmingham, TU</orgName>
								<address>
									<settlement>Darmstadt</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<settlement>Central</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">European University</orgName>
								<orgName type="institution" key="instit2">Cambridge University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Autocorrelated Bayesian Sampler: A Rational Process for Probability Judgments, Estimates, Confidence Intervals, Choices, Confidence Judgments, and Response Times</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Behavioral Science</term>
					<term>Sampling</term>
					<term>Bayesian Models of Cognition</term>
					<term>Normative Model</term>
					<term>Rational Analysis</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Normative models of decision-making that optimally transform noisy (sensory) information into categorical decisions qualitatively mismatch human behavior. Indeed, leading computational models have only achieved high empirical corroboration by adding taskspecific assumptions that deviate from normative principles. In response, we offer a Bayesian approach that implicitly produces a posterior distribution of possible answers (hypotheses) in response to sensory information. But we assume that the brain has no direct access to this posterior, but can only sample hypotheses according to their posterior probabilities. Accordingly, we argue that the primary problem of normative concern in decision-making is integrating stochastic hypotheses, rather than stochastic sensory information, to make categorical decisions. This implies that human response variability arises mainly from posterior sampling rather than sensory noise. Because human hypothesis generation is serially correlated, hypothesis samples will be autocorrelated. Guided by this new problem formulation, we develop a new process, the Autocorrelated Bayesian Sampler (ABS), which grounds autocorrelated hypothesis generation in a sophisticated sampling algorithm. The ABS provides a single mechanism qualitatively explains many empirical effects of probability judgments, estimates, confidence intervals, choice, confidence judgments, response times, and their relationships. Our analysis demonstrates the unifying power of a perspective shift in the exploration of normative models. It also exemplifies the proposal that the &quot;Bayesian brain&quot; operates using samples not probabilities, and that variability in human behavior may primarily reflect computational rather than sensory noise.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Autocorrelated Bayesian Sampler: A Rational Process for Probability Judgments, Estimates, Confidence Intervals, Choices, Confidence Judgments, and Response Times</head><p>Human judgment and decision making has been studied using a wide variety of measures. Participants are asked to provide probability judgments (e.g., <ref type="bibr" target="#b160">Sloman et al., 2004;</ref><ref type="bibr" target="#b58">Fox &amp; Rottenstreich, 2003;</ref><ref type="bibr" target="#b28">Costello &amp; Watts, 2014;</ref><ref type="bibr">Dasgupta, Schultz, &amp; Gershman, 2017;</ref>, estimates of physical quantities (e.g., <ref type="bibr" target="#b67">Gilden et al., 1995;</ref><ref type="bibr" target="#b82">Jazayeri &amp; Movshon, 2007)</ref> with associated confidence intervals (e.g., <ref type="bibr" target="#b84">Juslin &amp; Olsson, 1997;</ref><ref type="bibr" target="#b86">Juslin, Winman, &amp; Hansson, 2007)</ref>, and choices (e.g., <ref type="bibr" target="#b179">Tversky &amp; Kahneman, 1974;</ref><ref type="bibr" target="#b181">Usher &amp; McClelland, 2004;</ref><ref type="bibr" target="#b48">Fehr &amp; Rangel, 2011)</ref> with their associated responses times (e.g., <ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b92">Krajbich &amp; Rangel, 2011;</ref><ref type="bibr" target="#b139">Ratcliff &amp; Starns, 2009;</ref><ref type="bibr" target="#b12">Blurton et al., 2020)</ref> and confidence judgments (e.g., <ref type="bibr" target="#b9">Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b86">Juslin et al., 2007;</ref><ref type="bibr" target="#b102">Li &amp; Ma, 2020;</ref><ref type="bibr" target="#b129">Pleskac &amp; Busemeyer, 2010</ref>). Yet while each measurement has been subject to an enormous amount of empirical and modeling work in psychology, a unified theoretical framework that can provide an integrated account of human performance across all six measures (i.e., probability judgments, estimates, confidence intervals, choices, confidence judgments, and response times) is currently lacking.</p><p>Theorists have taken steps towards a unified model from two starting points, normative and descriptive. Existing normative models are elegant, parsimonious and are easily extendable to all six measures. But these models fail to provide a satisfactory account of many qualitative effects that have been observed in empirical data. By contrast, various descriptive models, which systematically deviate from normative assumptions, capture the empirical effects both qualitatively and quantitatively for up to three of these six measures, but no single model makes predictions across all measures.</p><p>Here, we develop a simple and consistent process that can account for a surprising variety of qualitative findings across all six measures. To achieve this goal, we build on a strong normative foundation for all six measures, rooted in a sampling approximation to Bayesian inference. This approach also implies a radical shift in viewpoint concerning the nature of the decision-making process and the origin of variability in human behavior. In the perceptual decision-making literature, existing normative models generally operate on noisy sensory information and evaluate the relative probability that this noisy information is generated by the different hypotheses (corresponding to choice options) (e.g., <ref type="bibr" target="#b70">Green &amp; Swets, 1966;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998)</ref>.</p><p>Our starting point is that exact Bayesian computations are generally intractable; and hence a Bayesian brain can, at best, only approximate these computations. One of the most widely used approach to such approximation in statistics and machine learning assumes that the brain draws samples from posterior probabilities. Inspired by this approach, many theorists in the Bayesian tradition have argued that the cognitive processes thus operate over these samples, rather than representations of probabilities <ref type="bibr" target="#b71">(Griffiths, Vul, &amp; Sanborn, 2012;</ref><ref type="bibr" target="#b190">Vul et al., 2014;</ref><ref type="bibr" target="#b153">Sanborn &amp; Chater, 2016;</ref><ref type="bibr" target="#b103">Lieder et al., 2018;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref>. But this process of sampling will inevitably be noisy-different samples will be drawn on different occasions. Thus, in this type of model, the main source of variability does not arise from sensory noise, but from computational noise caused by the process of sampling. That is, instead of evaluating evidence from the sensory system or memory, we propose that the cognitive system operates on stochastically generated hypotheses.</p><p>Our aim in this paper is to outline a general process that can be applied to a wide variety of measures and tasks, when equipped with a task-specific representation. Our focus is to show that this process provides a unified framework which captures a wide range of qualitative phenomena across measures and tasks, rather than to produce a comprehensive quantitative model of a particular task. The following sections are structured as follows. First, we review the traditional probabilistic view of normative decision making and note its limitations in explaining psychological data. Then we propose an alternative sampling-based approximation approach to alleviate the computational burden associated with the normative models, which in turn suggests a shift in the target problem of normative concern from accumulating sensory data to integrating stochastic hypotheses. We demonstrate the unifying power of the perspective shift by applying a rational process model across the six behavioral measures, emphasizing on qualitative model behaviors. Finally, we explore how to create these complete models after exploring the general judgement and decision-making process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Probabilistic Decision Making</head><p>The idea that human decision-making process is an optimal, perhaps Bayesian, process is attractive because given a problem description it prescribes the best behavior, and can therefore be justified as an ultimate state of evolution and/or learning <ref type="bibr" target="#b70">(Green &amp; Swets, 1966;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b68">Gold &amp; Shadlen, 2002;</ref><ref type="bibr" target="#b129">Pleskac &amp; Busemeyer, 2010;</ref><ref type="bibr" target="#b13">Bogacz et al., 2006;</ref><ref type="bibr" target="#b118">Moran, 2015;</ref><ref type="bibr" target="#b40">Drugowitsch et al., 2019;</ref><ref type="bibr" target="#b75">Hawkins &amp; Heathcote, 2021;</ref><ref type="bibr" target="#b175">Tickle et al., 2021)</ref>. There are also a wide variety of task-specific Bayesian models in psychology, but in the area of cognitive and perceptual decision making, they are often elaborations of the general decision process of Signal Detection Theory (SDT), which describes how sensory evidence can be transformed into optimal behavior <ref type="bibr" target="#b70">(Green &amp; Swets, 1966)</ref>.</p><p>To illustrate our discussion, we shall consider the following trial in a perceptual task as a running example: a cloud of 24 dots briefly appears on-screen (i.e., stimulus, ) 1 . Participants might be asked to report a probability that the number of dots falls within a certain window (i. probability judgments). They may also be asked to estimate the exact number of dots (ii. estimates) or to provide a confidence interval for the estimate (iii. confidence interval). Alternatively, participants could be asked to decide whether the number of dots was greater or smaller than some predefined boundary (iv. choices) and the elapsed times for making such decision (vi. response times), for example, by asking participants whether or not there were greater than 25 dots on the screen. Finally, it is also possible to elicit their confidence rating for either of these responses (v. confidence in the decision). <ref type="figure">Figure 1</ref>. Illustrations of the scope of behavioral measures for a single task. After the presentation of sensory stimulus, people can be asked a wide range of questions and their responses lead to corresponding behavioral measures.</p><p>Importantly, while this choice of example of numerosity judgment is intended to provide a simple and concrete illustration, and one that connects naturally to existing models such as SDT, the general approach applies quite generally. For a wide range of tasks, the six behavioral economics (e.g., <ref type="bibr" target="#b179">Tversky &amp; Kahneman, 1974)</ref>, which has largely developed as a separate tradition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensory stimulus Asking people a question Behaviour</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A Z I m S 7 T r 9 p j i 3 z 2 Z n P z T 0 z + L 6 O U = " &gt; A A A B + X i c b V D L S g N B E J y N r x h f q x 6 9 D A b B U 9 g N Q b 0 I A S 8 e I 5 g H J M s y O + k k Q 2 Y f z P Q G w 5 I / 8 e J B E a / + i T f / x k m y B 0 0 s a C i q u u n u C h I p N D r O t 1 X Y 2 N z a 3 i n u l v b 2 D w 6 P 7 O O T l o 5 T x a H J Y x m r T s A 0 S B F B E w V K 6 C Q K W B h I a A f j u 7 n f n o D S I o 4 e c Z q A F 7 J h J A a C M z S S b 9 v a 7 y E 8 Y Y Y q h d l t t e b b Z a f i L E D X i Z u T M s n R 8 O 2 v X j / m a Q g R c s m 0 7 r p O g l 7 G F A o u Y V b q p R o S x s d s C F 1 D I x a C 9 r L F 5 T N 6 Y Z Q + H c T K V I R 0 o f 6 e y F i o 9 T Q M T G f I c K R X v b n 4 n 9 d N c X D j Z S J K U o S I L x c N U k k x p v M Y a F 8 o 4 C i n h j C u h L m V 8 h F T j K M J q 2 R C c F d f X i e t a s W 9 q t Q e a u V 6 N Y + j S M 7 I O b k k L r k m d X J P G q R J O J m Q Z / J K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 J H 1 i f P 5 I 7 k 5 I = &lt; / l a t e x i t &gt;</p><formula xml:id="formula_0">s true = 24</formula><p>The number of dots exceeds 25. What is the probability that this proposition is correct?</p><p>How many dots appeared onscreen? Give the smallest interval which you are 60% certain includes the number of dots appeared onscreen?</p><p>Is the number of dots greater than 25? How confident are you that your answer is correct?</p><p>Probability Judgments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimates Confidence Intervals</head><p>Choices &amp; Response Times Confidence Judgments behaviors above can be collected and modelled. So, for example, participants might be asked memory-based questions about how much their last grocery bill was (e.g., "Your last grocery bill exceeded £150. What is the probability that this proposition is correct?"), or even asking participants about one-off future events such as how many years they expect to live. Thus, while we use the numerosity example in <ref type="figure">Figure 1</ref> because it is simple and straightforward to relate to SDT, our approach applies to cognitive as well as perceptual tasks, as we will see below.</p><p>More formally, in the SDT, we wish to choose between option A and B, based on a total of units of sensory input ( ! , " , … , # ) typically assumed to be accumulated over time.</p><p>Assuming that both options are equally likely a priori (i.e., ( ) = ( )), the key variable is the summed log-likelihood ratio over the evidence from each individual unit of sensory input:</p><formula xml:id="formula_1">( ) = , log ( $ | ) ( $ | ) # $%!<label>(1)</label></formula><p>where ( $ | ) is the likelihood of sensory evidence $ when option is the correct choice (similarly for ( $ | )). The probability of choosing over should be a function of the summed log likelihood ratios. Provided with imperfect evidence (e.g., detecting a ship on a noisy radar image), SDT is a principled way to filter out irrelevant sensory noise to pick out the useful signal (e.g., whether the image contains ship). The approach can be applied to a wide range of domains in psychology, including memory, perception, and reasoning <ref type="bibr" target="#b88">(Kellen et al., 2021;</ref><ref type="bibr" target="#b148">Rotello, 2018;</ref><ref type="bibr" target="#b177">Trippas et al., 2018;</ref><ref type="bibr" target="#b114">McCarley &amp; Benjamin, 2013)</ref>.</p><p>SDT, however, makes no explicit commitment on the time course of how evidence is generated and/or collected, and so makes no predictions for response time. This issue can be addressed with a dynamic extension of SDT: the sequential probability ratio test (SPRT), which postulates that the stream of sensory evidence arrives steadily and sequentially over time <ref type="bibr" target="#b44">(Edwards, 1965;</ref><ref type="bibr" target="#b100">Laming, 1968;</ref><ref type="bibr" target="#b13">Bogacz et al., 2006)</ref>. To deal optimally with the incoming sensory evidence in, for example, binary choice, the evidence should be continuously integrated into the log-likelihood ratio between the two options until a fixed threshold is reached, and response times are predicted to depend on the amount of evidence accumulated. More formally, the log-likelihood ratio for choosing option over is recursively updated after the arrival of each new piece of sensory evidence ( # ):</p><formula xml:id="formula_2">( ) = ( − 1) + log ( # | ) ( # | )<label>(2)</label></formula><p>Once the log-likelihood ratio reaches a threshold (assuming symmetric thresholds: ( ) &gt; δ or ( ) &lt; −δ), the evidence accumulation process stops and the response depends on whether the positive or negative threshold is reached. Increasing the magnitude of the threshold (δ) produces a slower but more accurate response as more sensory evidence, on average, is accumulated before either threshold is reached. The SPRT is optimal in the sense that the expected amount of evidence (i.e., ) is minimized for any fixed probability of deciding incorrectly <ref type="bibr" target="#b193">(Wald &amp; Wolfowitz, 1948)</ref>. In other words, following the SPRT allows for the fastest response time for a particular error rate. Because the sensory inputs are assumed to be independent of one another, the SPRT is a random walk model whose starting point is (0) = 0 and with two absorbing states: −δ and δ (see <ref type="figure" target="#fig_0">Figure 2A</ref>). A Sequential Probability Ratio Test sample from sensory representation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A Z I m S 7 T r 9 p j i 3 z 2 Z n P z T 0 z + L 6 O U = " &gt; A A A B + X i c b V D L S g N B E J y N r x h f q x 6 9 D A b B U 9 g N Q b 0 I A S 8 e I 5 g H J M s y O + k k Q 2 Y f z P Q G w 5 I / 8 e J B E a / + i T f / x k m y B 0 0 s a C i q u u n u C h I p N D r O t 1 X Y 2 N z a 3 i n u l v b 2 D w 6 P 7 O O T l o 5 T x a H J Y x m r T s A 0 S B F B E w V K 6 C Q K W B h I a A f j u 7 n f n o D S I o 4 e c Z q A F 7 J h J A a C M z S S b 9 v a 7 y E 8 Y Y Y q h d l t t e b b Z a f i L E D X i Z u T M s n R 8 O 2 v X j / m a Q g R c s m 0 7 r p O g l 7 G F A o u Y V b q p R o S x s d s C F 1 D I x a C 9 r L F 5 T N 6 Y Z Q + H c T K V I R 0 o f 6 e y F i o 9 T Q M T G f I c K R X v b n 4 n 9 d N c X D j Z S J K U o S I L x c N U k k x p v M Y a F 8 o 4 C i n h j C u h L m V 8 h F T j K M J q 2 R C c F d f X i e t a s W 9 q t Q e a u V 6 N Y + j S M 7 I O b k k L r k m d X J P G q R J O J m Q Z / J K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 J H 1 i f P 5 I 7 k 5 I = &lt; / l a t e x i t &gt; s true = 24 stimulus B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A Z I m S 7 T r 9 p j i 3 z 2 Z n P z T 0 z + L 6 O U = " &gt; A A A B + X i c b V D L S g N B E J y N r x h f q x 6 9 D A b B U 9 g N Q b 0 I A S 8 e I 5 g H J M s y O + k k Q 2 Y f z P Q G w 5 I / 8 e J B E a / + i T f / x k m y B 0 0 s a C i q u u n u C h I p N D r O t 1 X Y 2 N z a 3 i n u l v b 2 D w 6 P 7 O O T l o 5 T x a H J Y x m r T s A 0 S B F B E w V K 6 C Q K W B h I a A f j u 7 n f n o D S I o 4 e c Z q A F 7 J h J A a C M z S S b 9 v a 7 y E 8 Y Y Y q h d l t t e b b Z a f i L E D X i Z u T M s n R 8 O 2 v X j / m a Q g R c s m 0 7 r p O g l 7 G F A o u Y V b q p R o S x s d s C F 1 D I x a C 9 r L F 5 T N 6 Y Z Q + H c T K V I R 0 o f 6 e y F i o 9 T Q M T G f I c K R X v b n 4 n 9 d N c X D j Z S J K U o S I L x c N U k k x p v M Y a F 8 o 4 C i n h j C u h L m V 8 h F T j K M J q 2 R C c F d f X i e t a s W 9 q t Q e a u V 6 N Y + j S M 7 I O b k k L r k m d X J P G q R J O J m Q Z / J K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 J H 1 i f P 5 I 7 k 5 I = &lt; / l a t e x i t &gt;  <ref type="bibr">Beta(2,</ref><ref type="bibr">1)</ref> Beta <ref type="bibr">(2,</ref><ref type="bibr">2)</ref> Beta <ref type="bibr">(3,</ref><ref type="bibr">2)</ref> Beta <ref type="bibr">(3,</ref><ref type="bibr">3)</ref> Beta <ref type="bibr">(4,</ref><ref type="bibr">3)</ref> Beta <ref type="bibr">(5,</ref><ref type="bibr">3)</ref> Autocorrelated Bayesian Sampler 26 24 27 22 23 autocorrelated samples &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 4 j 4 + s O K T a j a R h v H g + + P x b U o K D c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S L 0 V J J S 1 G P B i 8 c q 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k d u 5 3 n l B p H s t H M 0 3 Q j + h I 8 p A z a q z 0 k F Q G p b J b d R c g 6 8 T L S R l y N A e l r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z s Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n i 0 t n 5 N I q Q x L G y p Y 0 Z K H + n s h o p P U 0 C m x n R M 1 Y r 3 p z 8 T + v l 5 r w x s + 4 T F K D k i 0 X h a k g J i b z t 8 m Q K 2 R G T C 2 h T H F 7 K 2 F j q i g z N p y i D c F b f X m d t G t V 7 6 p a v 6 + X G 7 U 8 j g K c w w V U w I N r a M A d N K E F D E J 4 h l d 4 c y b O i / P u f C x b N 5 x 8 5 g z + w P n 8 A T o 6 j R 8 = &lt; / l a t e x i t &gt; p( &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w J B D 9 U f t o 2 0 h y 4</p><formula xml:id="formula_3">v d v j h H S W 5 f O 6 4 = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B D 0 E n Z D U I 8 R L x 4 j m g c k S 5 i d 9 C Z D Z m e X m V k h x H y C F w + K e P W L v P k 3 T p I 9 a G J B Q 1 H V T X d X k A i u j e t + O y u r a + s b m 7 m t / P b O 7 t 5 + 4 e C w o e N U M a y z W M S q F V C N g k u s G 2 4 E t h K F N A o E N o P h z d R v P q L S P J Y P Z p S g H 9 G + 5 C F n 1 F j p / u n 6 v F s o u i V 3 B r J M v I w U I U O t W / j q 9 G K W R i g N E 1 T r t u c m x h 9 T Z T g T O M l 3 U o 0 J Z U P a x 7 a l k k a o / f H s 1 A k 5 t U q P h L G y J Q 2 Z q b 8 n x j T S e h Q F t j O i Z q A X v a n 4 n 9 d O T X j l j 7 l M U o O S z R e F q S A m J t O / S Y 8 r Z E a M L K F M c X s r Y Q O q K D M 2 n b w N w V t 8 e Z k 0 y i X v o l S 5 q x S r 5 S y O H B z D C Z y B B 5 d Q h V u o Q R 0 Y 9 O E Z X u H N E c 6 L 8 + 5 8 z F t X n G z m C P 7 A + f w B 1 i G N d w = = &lt; / l a t e x i t &gt; |A)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 r</p><formula xml:id="formula_4">S D K Z J C c T m E o w / I U d t n j o K n s y I = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B D 0 E n Z D U I 9 B L x 4 j m g c k S 5 i d 9 C Z D Z m e X m V k h x H y C F w + K e P W L v P k 3 T p I 9 a G J B Q 1 H V T X d X k A i u j e t + O y u r a + s b m 7 m t / P b O 7 t 5 + 4 e C w o e N U M a y z W M S q F V C N g k u s G 2 4 E t h K F N A o E N o P h z d R v P q L S P J Y P Z p S g H 9 G + 5 C F n 1 F j p / u n 6 v F s o u i V 3 B r J M v I w U I U O t W / j q 9 G K W R i g N E 1 T r t u c m x h 9 T Z T g T O M l 3 U o 0 J Z U P a x 7 a l k k a o / f H s 1 A k 5 t U q P h L G y J Q 2 Z q b 8 n x j T S e h Q F t j O i Z q A X v a n 4 n 9 d O T X j l j 7 l M U o O S z R e F q S A m J t O / S Y 8 r Z E a M L K F M c X s r Y Q O q K D M 2 n b w N w V t 8 e Z k 0 y i X v o l S 5 q x S r 5 S y O H B z D C Z y B B 5 d Q h V u o Q R 0 Y 9 O E Z X u H N</formula><p>E c 6 L 8 + 5 8 z F t X n G z m C P 7 A + f w B 1 6 a N e A = = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|B)</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 4 j 4 + s O K T a j a R h v H g +</p><formula xml:id="formula_5">+ P x b U o K D c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S L 0 V J J S 1 G P B i 8 c q 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k d u 5 3 n l B p H s t H M 0 3 Q j + h I 8 p A z a q z 0 k F Q G p b J b d R c g 6 8 T L S R l y N A e l r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z s Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n i 0 t n 5 N I q Q x L G y p Y 0 Z K H + n s h o p P U 0 C m x n R M 1 Y r 3 p z 8 T + v l 5 r w x s + 4 T F K D k i 0 X h a k g J i b z t 8 m Q K 2 R G T C 2 h T H F 7 K 2 F j q i g z N p y i D c F b f X m d t G t V 7 6 p a v 6 + X G 7 U 8 j g K c w w V U w I N r a M A d N K E F D E J 4 h l d 4 c y b O i / P u f C x b N 5 x 8 5 g z + w P n 8 A T o 6 j R 8 = &lt; / l a t e x i t &gt; p( &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g y o Y K M 7 D 7 m p W w 7 E o d K 5 K Y 2 u c x V U = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G E 3 B P U Y 8 O I x A f O A Z A m z k 9 5 k z O z s M j M r h J A v 8 O J B E a 9 + k j f / x k m y B 0 0 s a C i q u u n u C h L B t X H d b y e 3 s b m 1 v Z P f L e z t H x w e F Y 9 P W j p O F c M m i 0 W s O g H V K L j E p u F G Y C d R S K N A Y D s Y 3 8 3 9 9 h M q z W P 5 Y C Y J + h E d S h 5 y R o 2 V G l f 9 Y s k t u w u Q d e J l p A Q Z 6 v 3 i V 2 8 Q s z R C a Z i g W n c 9 N z H + l C r D m c B Z o Z d q T C g b 0 y F 2 L Z U 0 Q u 1 P F 4 f O y I V V B i S M l S 1 p y E L 9 P T G l k d a T K L C d E T U j v e r N x f + 8 b m r C W 3 / K Z Z I a l G y 5 K E w F M T G Z f 0 0 G X C E z Y m I J Z Y r b W w k b U U W Z s d k U b</formula><p>A j e 6 s v r p F U p e 9 f l a q N a q l W y O P J w B u d w C R 7 c Q A 3 u o Q 5 N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F s z T n Z z C n 8 g f P 5 A 2 + j j K g = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 4 j 4 + s O K T a j a R h</p><formula xml:id="formula_6">v H g + + P x b U o K D c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S L 0 V J J S 1 G P B i 8 c q 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k d u 5 3 n l B p H s t H M 0 3 Q j + h I 8 p A z a q z 0 k F Q G p b J b d R c g 6 8 T L S R l y N A e l r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z s Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n i 0 t n 5 N I q Q x L G y p Y 0 Z K H + n s h o p P U 0 C m x n R M 1 Y r 3 p z 8 T + v l 5 r w x s + 4 T F K D k i 0 X h a k g J i b z t 8 m Q K 2 R G T C 2 h T H F 7 K 2 F j q i g z N p y i D c F b f X m d t G t V 7 6 p a v 6 + X G 7 U 8 j g K c w w V U w I N r a M A d N K E F D E J 4 h l d 4 c y b O i / P u f C x b N 5 x 8 5 g z + w P n 8 A T o 6 j R 8 = &lt; / l a t e x i t &gt; p(</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w J B D 9 U f t o 2 0 h y 4</p><formula xml:id="formula_7">v d v j h H S W 5 f O 6 4 = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B D 0 E n Z D U I 8 R L x 4 j m g c k S 5 i d 9 C Z D Z m e X m V k h x H y C F w + K e P W L v P k 3 T p I 9 a G J B Q 1 H V T X d X k A i u j e t + O y u r a + s b m 7 m t / P b O 7 t 5 + 4 e C w o e N U M a y z W M S q F V C N g k u s G 2 4 E t h K F N A o E N o P h z d R v P q L S P J Y P Z p S g H 9 G + 5 C F n 1 F j p / u n 6 v F s o u i V 3 B r J M v I w U I U O t W / j q 9 G K W R i g N E 1 T r t u c m x h 9 T Z T g T O M l 3 U o 0 J Z U P a x 7 a l k k a o / f H s 1 A k 5 t U q P h L G y J Q 2 Z q b 8 n x j T S e h Q F t j O i Z q A X v a n 4 n 9 d O T X j l j 7 l M U o O S z R e F q S A m J t O / S Y 8 r Z E a M L K F M c X s r Y Q O q K D M 2 n b w N w V t 8 e Z k 0 y i X v o l S 5 q x S r 5 S y O H B z D C Z y B B 5 d Q h V u o Q R 0 Y 9 O E Z X u H N E c 6 L 8 + 5 8 z F t X n G z m C P 7 A + f w B 1 i G N d w = = &lt; / l a t e x i t &gt; |A)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 r</p><formula xml:id="formula_8">S D K Z J C c T m E o w / I U d t n j o K n s y I = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B D 0 E n Z D U I 9 B L x 4 j m g c k S 5 i d 9 C Z D Z m e X m V k h x H y C F w + K e P W L v P k 3 T p I 9 a G J B Q 1 H V T X d X k A i u j e t + O y u r a + s b m 7 m t / P b O 7 t 5 + 4 e C w o e N U M a y z W M S q F V C N g k u s G 2 4 E t h K F N A o E N o P h z d R v P q L S P J Y P Z p S g H 9 G + 5 C F n 1 F j p / u n 6 v F s o u i V 3 B r J M v I w U I U O t W / j q 9 G K W R i g N E 1 T r t u c m x h 9 T Z T g T O M l 3 U o 0 J Z U P a x 7 a l k k a o / f H s 1 A k 5 t U q P h L G y J Q 2 Z q b 8 n x j T S e h Q F t j O i Z q A X v a n 4 n 9 d O T X j l j 7 l M U o O S z R e F q S A m J t O / S Y 8 r Z E a M L K F M c X s r Y Q O q K D M 2 n b w N w V t 8 e Z k 0 y i X v o l S 5 q x S r 5 S y O H B z D C Z y B B 5 d Q h V u o Q R 0 Y 9 O E Z X u H N E c 6 L 8 + 5 8 z F t X n G z m C P 7 A + f w B 1 6</formula><p>a N e A = = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|B)</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 4 j 4 + s O K T a j a R h</p><formula xml:id="formula_9">v H g + + P x b U o K D c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S L 0 V J J S 1 G P B i 8 c q 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k d u 5 3 n l B p H s t H M 0 3 Q j + h I 8 p A z a q z 0 k F Q G p b J b d R c g 6 8 T L S R l y N A e l r / 4 w Z m m E 0 j B B t e 5 5 b m L 8 j C r D m c B Z s Z 9 q T C i b 0 B H 2 L J U 0 Q u 1 n i 0 t n 5 N I q Q x L G y p Y 0 Z K H + n s h o p P U 0 C m x n R M 1 Y r 3 p z 8 T + v l 5 r w x s + 4 T F K D k i 0 X h a k g J i b z t 8 m Q K 2 R G T C 2 h T H F 7 K 2 F j q i g z N p y i D c F b f X m d t G t V 7 6 p a v 6 + X G 7 U 8 j g K c w w V U w I N r a M A d N K E F D E J 4 h l d 4 c y b O i / P u f C x b N 5 x 8 5 g z + w P n 8 A T o 6 j R 8 = &lt; / l a t e x i t &gt; p( &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g y o Y K M 7 D 7 m p W w 7 E o d K 5 K Y 2 u c x V U = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G E 3 B P U Y 8 O I x A f O A Z A m z k 9 5 k z O z s M j M r h J A v 8 O J B E a 9 + k j f / x k m y B 0 0 s a C i q u u n u C h L B t X H d b y e 3 s b m 1 v Z P f L e z t H x w e F Y 9 P W j p O F c M m i 0 W s O g H V K L j E p u F G Y C d R S K N A Y D s Y 3 8 3 9 9 h M q z W P 5 Y C Y J + h E d S h 5 y R o 2 V G l f 9 Y s k t u w u Q d e J l p A Q Z 6 v 3 i V 2 8 Q s z R C a Z i g W n c 9 N z H + l C r D m c B Z o Z d q T C g b 0 y F 2 L Z U 0 Q u 1 P F 4 f O y I V V B i S M l S 1 p y E L 9 P T G l k d a T K L C d E T U j v e r N x f + 8 b m r C W 3 / K Z Z I a l G y 5 K E w F M T G Z f 0 0 G X C E z Y m I J Z Y r b W w k b U U W Z s d k U b A j e 6 s v r p F U p e 9 f l a q N a q l W y O P J w B u d w C R 7 c Q A 3 u o Q 5 N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F s z T n Z z C n 8 g f P 5 A 2 + j j K g = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q o I C b 8 8 Y t + 9 Z E r w 3 j q 5 f V b n R 8 A I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 h K U Y 8 F L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o R S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B V c G 8 / 7 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k p Z N M M W y y R C S q E 1 K N g k t s G m 4 E d l K F N A 4 F t s P x 7 d x v P 6 H S P J G P Z p J i E N O h 5 B F n 1 F j p w X X d f r n i u d 4 C Z J 3 4 O a l A j k a / / N U b J C y L U R o m q N Z d 3 0 t N M K X K c C Z w V u p l G l P K x n S I X U s l j V E H 0 8 W p M 3 J h l Q G J E m V L G r J Q f 0 9 M a a z 1 J A 5 t Z 0 z N S K 9 6 c / E / r 5 u Z 6 C a Y c p l m B i V b L o o y Q U x C 5 n + T A V f I j J h Y Q p n i 9 l b C R l R R Z m w 6 J R u C v / r y O m l V X f / K r d 3 X K v V q H k c R z u A c L s G H a 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B K A o 0 b &lt; / l a t e x i t &gt; ...</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 X o Z l 1 K + a R 4 A I t e 6 + + 0 F j u n</p><formula xml:id="formula_10">L d E = " &gt; A A A B 6 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y G o B 4 D X j x G M I m Q L G F 2 M p s M m c c y M y u E J b / g x Y M i X v 0 h b / 6 N s 8 k e N L G g o a j q p r s r S j g z 1 v e / v d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p G p V q Q j t E c a U f I 2 w o Z 5 J 2 L L O c P i a a Y h F x 2 o u m t 7 n f e 6 L a M C U f 7 C y h o c B j y W J G s M 2 l A V f j Y b X m 1 / 0 F 0 D o J C l K D A u 1 h 9 W s w U i Q V V F r C s T H 9 w E 9 s m G F t G e F 0 X h m k h i a Y T P G Y 9 h 2 V W F A T Z o t b 5 + j C K S M U K + 1 K W r R Q f 0 9 k W B g z E 5 H r F N h O z K q X i / 9 5 / d T G N 2 H G Z J J a K s l y U Z x y Z B X K H 0 c j p i m x f O Y I J p q 5 W x G Z Y I 2 J d f F U X A j B 6 s v r p N u o B 1 f 1 5 n 2 z 1 m o U c Z T h D M 7 h E g K 4 h h b c Q R s 6 Q G A C z / A K b 5 7 w X r x 3 7 2 P Z W v K K m V P 4 A + / z B x M p j j k = &lt; / l a t e x i t &gt; log</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 X o Z l 1 K + a R 4 A I t e 6 + + 0 F j u n</p><formula xml:id="formula_11">L d E = " &gt; A A A B 6 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y G o B 4 D X j x G M I m Q L G F 2 M p s M m c c y M y u E J b / g x Y M i X v 0 h b / 6 N s 8 k e N L G g o a j q p r s r S j g z 1 v e / v d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p G p V q Q j t E c a U f I 2 w o Z 5 J 2 L L O c P i a a Y h F x 2 o u m t 7 n f e 6 L a M C U f 7 C y h o c B j y W J G s M 2 l A V f j Y b X m 1 / 0 F 0 D o J C l K D A u 1 h 9 W s w U i Q V V F r C s T H 9 w E 9 s m G F t G e F 0 X h m k h i a Y T P G Y 9 h 2 V W F A T Z o t b 5 + j C K S M U K + 1 K W r R Q f 0 9 k W B g z E 5 H r F N h O z K q X i / 9 5 / d T G N 2 H G Z J J a K s l y U Z x y Z B X K H 0 c j p i m x f O Y I J p q 5 W x G Z Y I 2 J d f F U X A j B 6 s v r p N u o B 1 f 1 5 n 2 z 1 m o U c Z T h D M 7 h E g K 4 h h b c Q R s 6 Q G A C z / A K b 5 7 w X r x 3 7 2 P Z W v K K m V P 4 A + / z B x M p j j k = &lt; / l a t e x i t &gt; log log likelihood ratio 1 log likelihood ratio 2 decision making choice A (less than 25 dots) physical times L(0) L(1) L(2) L(3) L(4) L(5)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y + H a I B 6 s b I 5 z O K e t z R n F N 0 3 J 8 s E = "</p><formula xml:id="formula_12">&gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B g 5 R E i n o s e P F Y w X 5 A G 8 p m s 2 n X b r J h d y K U 0 v / g x Y M i X v 0 / 3 v w 3 b t s c t P X B w O O 9 G W b m B a k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l l G Z Z r z J l F S 6 E 1 D D p U h 4 E w V K 3 k k 1 p 3 E g e T s Y 3 c 7 8 9 h P X R q j k A c c p 9 2 M 6 S E Q k G E U r t X o h l 0 j 7 5 Y p b d e c g q 8 T L S Q V y N P r l r 1 6 o W B b z B J m k x n Q 9 N 0 V / Q j U K J v m 0 1 M s M T y k b 0 Q H v W p r Q m B t / M r 9 2 S s 6 s E p J I a V s J k r n 6 e 2 J C Y 2 P G c W A 7 Y 4 p D s + z N x P + 8 b o b R j T 8 R S Z o h T 9 h i U Z R J g o r M X i e h 0 J y h H F t C m R b 2 V s K G V F O G N q C S D c F b f n m V t C 6 r 3 l W 1 d l + r 1 C / y O I p w A q d w D h 5 c Q</formula><p>x 3 u o A F N Y P A I z / A K b 4 5 y X p x 3 5 2 P R W n D y m W P 4 A + f z B 4 3 c j w 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o t C</p><formula xml:id="formula_13">v / s j l j t j Q B + L k + T i 8 7 F i q / W c = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B g 5 Z E i n o s e P F Y w X 5 A G 8 p m M 2 m X b j Z h d y O U 0 h / h x Y M i X v 0 9 3 v w 3 b t s c t P X B w O O 9 G W b m B a n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L Z 1 k i m G T J S J R n Y B q F F x i 0 3 A j s J M q p H E g s B 2 M 7 m Z + + w m V 5 o l 8 N O M U / Z g O J I 8 4 o 8 Z K 7 c t e i M L Q f r n i V t 0 5 y C r x c l K B H I 1 + + a s X J i y L U R o m q N Z d z 0 2 N P 6 H K c C Z w W u p l G l P K R n S A X U s l j V H 7 k / m 5 U 3 J m l Z B E i b I l D Z m r v y c m N N Z 6 H A e 2 M 6 Z m q J e 9 m f i f 1 8 1 M d O t P u E w z g 5 I t F k W Z I C Y h s 9 9 J y B U y I 8 a W U K a 4 v Z W w I V W U G Z t Q y Y b g L b + 8 S l p X V e + 6 W n u o V e o X e R x F O I F T O A c P b q A O 9 9 C A J j A Y w T O 8 w p u T O i / O u / O</formula><p>x a C 0 4 + c w x / I H z + Q P 4 D I 9 G &lt; / l a t e x i t &gt; estimate confidence interval 23 using the last sample as estimate 60% empirical quantiles as 60% CI <ref type="bibr">[23,</ref><ref type="bibr">26]</ref> choice B (greater or equal than 25 dots) confidence &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A g e</p><formula xml:id="formula_14">v U K P h J E Q w B Q E F / m u v U N a W k w k = " &gt; A A A C F H i c b Z B N S w M x E I a z f l u / V j 1 6 C R Z B q J R d K e p F U H r x W M H a Q r e W b D q r w W x 2 S W b F s v R H e P G v e P G g i F c P 3 v w 3 p r W I W l 8 I P L w z w 2 T e M J X C o O d 9 O B O T U 9 M z s 3 P z h Y X F p e U V d 3 X t 3 C S Z 5 l D n i U x 0 M 2 Q G p F B Q R 4 E S m q k G F o c S G u F 1 d V B v 3 I A 2 I l F n 2 E u h H b N L J S L B G V q r 4 5 Y C h F v M q 4 m K + p 1 j e k i D S D O e w 0 X Q B Y m s n / u l b + 6 4 R a / s D U X H w R 9 B k Y x U 6 7 j v Q T f h W Q w K u W T G t H w v x X b O N A o u o V 8 I M g M p 4 9 f s E l o W F Y v B t P P h U X 2 6 Z Z 0 u j R J t n 0 I 6 d H 9 O 5 C w 2 p h e H t j N m e G X + 1 g b m f 7 V W h t F B O x c q z R A U / 1 o U Z Z J i Q g c J 0 a 7 Q w F H 2 L D C u h f 0 r 5 V f M p o I 2 x 4 I N w f 9 7 8 j i c 7 5 b 9 v X L l t F I 8 2 h n F M U c 2 y C b Z J j 7 Z J 0 f k h N R I n X B y R x 7 I E 3 l 2 7 p 1 H 5 8 V 5 / W q d c E Y z 6 + S X n L d P z 0</formula><p>e e n g = = &lt; / l a t e x i t &gt;</p><formula xml:id="formula_15">Conf A = e 1 + e</formula><p>choice RT choosing A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / / m A j k a B J X T L L A b 3 I w 5 X k + T 6 K 7 U = " &gt; A </p><formula xml:id="formula_16">A A C A X i c b V D L S g M x F M 3 4 r P U 1 6 k Z w M 1 i E F q T M S F G X B T c u R 7 A P a I e S S T O d 0 E w S k o x Q p n X j r 7 h x o Y h b / 8 K d f 2 P a z k J b D w Q O 5 9 z L z T m h o E R p 1 / 2 2 V l b X 1 j c 2 C 1 v F 7 Z 3 d v X 3 7 4 L C p e C o R b i B O u W y H U G F K G G 5 o o i l u C 4 l h E l L c C o c 3 U 7 / 1 g K U i n N 3 r k c B B A g e M R A R B b a S e f e y X 4 7 G q d I X k Q n P H L 6 t x X D F a p W e X 3 K o 7 g 7 N M v J y U Q A 6 / Z 3 9 1 + x y l C W Y a U a h U x 3 O F D j I o N U E U T 4 r d V G E B 0 R A O c M d Q B h O s g m y W Y O K c G a X v R F y a x 7 Q z U 3 9 v Z D B R a p S E Z j K B O l a L 3 l T 8 z + u k O r o O M s J E q j F D 8 0 N R S h 2 T d V q H 0 y c S I 0 1 H h k A k i f m r g 2 I o I d K m t K I p w V u M v E y a F 1 X v s l q 7 q 5 X q 5 3 k d B X A C T k E Z e O A K 1 M E t 8 E E D I P A I n s E</formula><formula xml:id="formula_17">4 = " l o N 7 P L x U f D N A d c v Y w g c Q / J o x h 5 Q = " &gt; A A A B 7 3 i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 S N g N Q b 0 I A T 1 4 j G A e k C x h d j K b D J m d X W d 6 h R D y E 1 4 8 K O L V 3 / H m 3 z h J 9 q C J B Q 1 F V T f d X U E i h U H X / X Z W V t f W N z Z z W / n t n d 2 9 / c L B Y c P E q W a 8 z m I Z 6 1 Z A D Z d C 8 T o K l L y V a E 6 j Q P J m M L y Z + s 0 n r o 2 I 1 Q O O E u 5 H t K 9 E K B h F K 7 U 6 t 1 w i v S 5 3 C 0 W 3 5 M 5 A l o m X k S J k q H U L X 5 1 e z N K I K 2 S S G t P 2 3 A T 9 M d U o m O S T f C c 1 P K F s S P u 8 b a m i E T f + e H b v h J x a p U f C W N t S S G b q 7 4 k x j Y w Z R Y H t j C g O z K I 3 F f / z 2 i m G V / 5 Y q C R F r t h 8 U Z h K g j G Z P k 9 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 o j y N g R v 8 e V l 0 i i X v I t S 5 b 5 S r J 5 n c e T g G E 7 g D D y 4 h C r c Q Q 3 q w E D C M 7 z C m / P o v D j v z s e 8 d c X J Z o 7 g D 5 z P H 1 P O j 3 I = &lt; / l a t e x i t &gt; = 2</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x e J J P e D U y m 0 6 Q r </p><formula xml:id="formula_18">I p I 8 D y V A a p m q w = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 b d U t S L U N C D x w r 2 Q 9 q l Z N N s G 5 p k l y Q r l K W / w o s H R b z 6 c 7 z 5 b 0 z b P W j r g 4 H H e z P M z A t i z r R x 3 W 8 n t 7 K 6 t r 6 R 3 y x s b e / s 7 h X 3 D 5 o 6 S h S h D R L x S L U D r C l n k j Y M M 5 y 2 Y 0 W x C D h t B a O b q d 9 6 o k q z S D 6 Y c U x 9 g Q e S h Y x g Y 6 X H 7 i 3 l B l + f V 3 r F k l t 2 Z 0 D L x M t I C T L U e 8 W v b j 8 i i a D S E I 6 1 7 n h u b P w U K 8 M I p 5 N C N 9 E 0 x m S E B 7 R j q c S C a j + d H T x B J 1 b p o z B S t q R B M / X 3 R I q F 1 m M R 2 E 6 B z V A v e l P x P 6 + T m P D K T 5 m M E 0 M l m S 8 K E 4 5 M h K b f o z 5 T l B g + t g Q T x e y t i A y x w s T Y j A o 2 B G / x 5 W X S r J S 9 i 3 L 1 v l q q n W V x 5 O E I j u E U P L i E G t x B H R p A Q M A z v M K b o 5 w X 5 9 3 5 m L f m n G z m E P 7 A + f w B v</formula><formula xml:id="formula_19">d A M O x n Z q n u 7 x z 4 M d 1 3 5 g Z E = " &gt; A A A B / H i c b V D L S s N A F J 3 4 r P U V 7 d L N Y B H E R U m k P p Y F N y 6 r 9 A V N K J P p t B 0 6 y Y S Z G z G E + i t u X C j i 1 g 9 x 5 9 8 4 b b P Q 1 g M X D u f c y 7 3 3 B L H g G h z n 2 1 p Z X V v f 2 C x s F b d 3 d v f 2 7 Y P D l p a J o q x J p Z C q E x D N B I 9 Y E z g I 1 o k V I 2 E g W D s Y 3 0 z 9 9 g N T m s u o A W n M / J A M I z 7 g l I C R e n b J A / Y I 2 X 1 j g r 1 Y y R g k v u j Z Z a f i z I C X i Z u T M s p R 7 9 l f X l / S J G Q R U E G 0 7 r p O D H 5 G F H A q 2 K T o J Z r F h I 7 J k H U N j U j I t J / N j p / g E 6 P 0 8 U A q U x H g m f p 7 I i O h 1 m k Y m M 6 Q w E g v e l P x P 6 + b w O D a z 3 g U J 8 A i O l 8 0 S A Q 2 L 0 6 T w H 2 u G A W R G k K o 4 u Z W T E d E E Q o m r 6 I J w V 1 8 e Z m 0 z i v u Z a V 6 V y 3 X z v I 4 C u g I H a N T 5 K I r V E O 3 q I 6 a i K I U P a N X 9 G Y 9 W S / W u / U x b 1 2 x 8 p k S + g P r 8 w d t j p S O &lt; / l a t e x i t &gt; RT / 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 u r o Z 2 B W R 6 l c N N 6 d b R f z D Y M t A m 4 = " &gt; A A A C B H i c b V C 7 S g N B F J 2 N r x h f q 5 Z p B o M g F m F X o q Y R I m k s I 5 g H J C H M T m a T I b O z y 8 x d M S x b 2 P g r N h a K 2 P o R d v 6 N k 0 e h i Q c u H M 6 5 l 3 v v 8 S L B N T j O t 5 V Z W V 1 b 3 8 h u 5 r a 2 d 3 b 3 7 P 2 D h g 5 j R V m d h i J U L Y 9 o J r h k d e A g W C t S j A S e Y E 1 v V J 3 4 z X u m N A / l H Y w j 1 g 3 I Q H K f U w J G 6 t n 5 D r A H S K q h 9 N P e N b 7 C H V 8 R m p y n S T n t 2 Q W n 6 E y B l 4 k 7 J w U 0 R 6 1 n f 3 X 6 I Y 0 D J o E K o n X b d S L o J k Q B p 4 K l u U 6 s W U T o i A x Y 2 1 B J A q a 7 y f S J F B 8 b p Y / 9 U J m S g K f q 7 4 m E B F q P A 8 9 0 B g S G e t G b i P 9 5 7 R j 8 c j f h M o q B S T p b 5 M c C Q 4 g n i e A + V 4 y C G B t C q O L m V k y H x K Q A J r e c C c F d f H m Z N M 6 K 7 k W x d F s q V E 7 n c W R R H h 2 h E + S i S 1 R B N 6 i G 6 o i i R / S M X t G b 9 W S 9 W O / W x 6 w 1 Y 8 1 n D t E f W J 8 / L z K X u w = = &lt; / l a t e x i t &gt; Conf A = 5 8</formula><p>probability judgment with a prior on responses of &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " </p><formula xml:id="formula_20">T g E N h W Z k D c 4 V W 5 A r F f X z x v t Z 7 x Y = " &gt; A A A C A H i c b V D L S s N A F J 3 4 r P U V d e H C z W A R x E V J p G o 3 Q s W N y w r 2 A U 0 I k + m k H T p 5 M H M j l J C N v + L G h S J u / Q x 3 / o 3 T N g t t P X D h c M 6 9 3 H u P n w i u w L K + j a X l l d W 1 9 d J G e X N r e 2 f X 3 N t v q z i V l L V o L G L Z 9 Y l i g k e s B R w E 6 y a S k d A X r O O P b i d + 5 5 F J x e P o A c Y J c 0 M y i H j A K Q E t e e a h M y S Q N X P v B l 9 j J 5 C E Z h d 5 V s 8 9 s 2 J V r S n w I r E L U k E F m p 7 5 5 f R j m o Y s A i q I U j 3 b S s D N i A R O B c v L T q p Y Q u i I D F h P 0 4 i E T L n Z 9 I E c n 2 i l j 4 N Y 6 o o A T 9 X f E x k J l R q H v u 4 M C Q z V v D c R / / N 6 K Q R 1 N + N R k g K L 6 G x R k A o M M Z 6 k g f t c M g p i r A m h k u t b M R 0 S n Q L o z M o 6 B H v + 5 U X S P q / a l 9 X a f a 3 S O C v i K K E j d I x O k Y 2 u U A P d o S Z q I Y p y 9 I x e 0 Z v x Z L w Y 7 8 b H r H X J K G Y O 0 B 8 Y n z / L 1 p X V &lt; / l</formula><formula xml:id="formula_21">f u g T 0 T h 7 R B o u 8 = " &gt; A A A B + X i c b V D L S g N B E J y N r x h f q x 6 9 L A Y h i o T d E N R j 0 I v H C O Y B y R J m J 7 3 J k N k H M 7 3 B s O R P v H h Q x K t / 4 s 2 / c Z L s Q R M L G o q q b r q 7 v F h w h b b 9 b e T W 1 j c 2 t / L b h Z 3 d v f 0 D 8 / C o q a J E M m i w S E S y 7 V E F g o f Q Q I 4 C 2 r E E G n g C W t 7 o b u a 3 x i A V j 8 J H n M T g B n Q Q c p 8 z i l r q m W Y X 4 Q n T W 0 A 6 L V U u n f O e W b T L 9 h z W K n E y U i Q Z 6 j 3 z q 9 u P W B J A i E x Q p T q O H a O b U o m c C Z g W u o m C m L I R H U B H 0 5 A G o N x 0 f v n U O t N K 3 / I j q S t E a 6 7 + n k h p o N Q k 8 H R n Q H G o l r 2 Z + J / X S d C / c V M e x g l C y B a L / E R Y G F m z G K w + l 8 B Q T D S h T H J 9 q 8 W G V F K G O q y C D s F Z f n m V N C t l 5</formula><p>6 p c f a g W a x d Z H H l y Q k 5 J i T j k m t T I P a m T B m F k T J 7 J K 3 k z U u P F e D c + F q 0 5 I 5 s 5 J n 9 g f P 4 A Q 2 2 S s Q = = &lt; / l a t e x i t &gt; Beta(2, 1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z r a k p r + u 7 m F C F Q M U e + V c f t 6 8 h 0</p><formula xml:id="formula_22">Q = " &gt; A A A C E H i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W s I m V G 6 m N Z E M F l l b 6 g U 0 o m T d v Q T G Z I 7 o h l 6 C e 4 8 V f c u F D E r U t 3 / o 3 p Y 6 G t B w I n 5 9 x 7 k 3 v 8 S H A N j v N t p R Y W l 5 Z X 0 q u Z t f W N z S 1 7 e 6 e q w 1 h R V q G h C F X d J 5 o J L l k F O A h W j x Q j g S 9 Y z e 9 f j f z a P V O a h 7 I M g 4 g 1 A 9 K V v M M p A S O 1 7 E M P 2 A M k d + U h 9 j Q P 8 O R 6 r Q S R 3 W H u 7 M Q T Z l a b H L X s r J N 3 x s D z x J 2 S L J q i 1 L K / v H Z I 4 4 B J o I J o 3 X C d C J o J U c C p Y M O M F 2 s W E d o n X d Y w V J K A 6 W Y y X m i I D 4 z S x p 1 Q m S M B j 9 X f H Q k J t B 4 E v q k M C P T 0 r D c S / / M a M X Q u m w m X U Q x M 0 s l D n V h g C P E o H d z m i l E Q A 0 M I V d z 8 F d M e U Y S C y T B j Q n B n V 5 4 n 1 d O 8 e 5 4 v 3 B a y x e N p H G m 0 h / Z R D r n o A h X R D S q h C q L o E T 2 j V / R m P V k v 1 r v 1 M S l N W d O e X f Q H 1 u c P R s 6 c p w = = &lt; / l a t e x i t &gt; RT ⇠ Erlang(5, )</formula><p>which 24 dots were briefly presented on-screen as the stimulus. The SPRT draws sequential samples from the noisy sensory representation (e.g., corrupted images), while the ABS draws autocorrelated samples of hypotheses (e.g., numbers of dots).  <ref type="figure" target="#fig_0">Figure   2B</ref>. Here, the sampler was automatically terminated when 5 samples were generated, while the dashed lines denote potential future samples if continued. Samples were compared to a decision boundary of 25 (red dots: evidence for lower than 25 dots, blue dots: evidence for greater than or equal to 25). The 5 samples were then integrated with a prior on responses (here used an asymmetric prior, Beta(2,1)), reaching a posterior of Beta(5,3). The mean of this posterior on responses was then used to generate probability judgments or confidence judgments in decision-making.</p><p>While intuitive and simple, the framework of the SPRT also makes imperfect decisions that take time, as people do, which is an advantage over SDT in modeling empirical data. Indeed, the SPRT can produce human-like speed accuracy tradeoffs: requiring faster decisions reduces accuracy, while requiring more accurate decisions reduces speed. This is captured in the model by assuming that people control the magnitude of the thresholds to suit their objectives. In response to experimental emphasis on speed (accuracy), people are assumed to be able to decrease (increase) the decision threshold; the model's guarantee of optimal performance implies that these two measures will trade off against one another.</p><p>Unfortunately, the SPRT does not easily explain other psychological relationships between choice and response times. In binary choice, for example, the SPRT predicts identical response time distributions for choosing either of the two options (assuming an unbiased starting point, (0) = 0, and symmetric thresholds), contradicting the empirical observation that mean response times differ for correct and incorrect decisions <ref type="bibr" target="#b166">(Stone, 1960;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998)</ref>. This is far from the only issue: <ref type="table" target="#tab_1">Table 1 summarizes several</ref> qualitative effects of choice, response time, and confidence, the majority of which cannot be accommodated by the SPRT. These stylized facts have been used to motivate descriptive models, including the family of models known as Drift Diffusion Models (DDMs) that relaxes the normative SPRT framework to better describe human data, specifically regarding three key measures: choice, response time, and confidence. While such approaches have been highly successful, our focus here remains on approaches closely tied to normative depictions of behavior, though we return to DDMs and other common descriptive models below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Representation for Producing Estimates and Confidence Intervals</head><p>The categorical-hypotheses representations used in SPRT can produce choice, response time, and confidence measures, but are not fine-grained enough to produce probability judgments (e.g., judge the probability that the number of dots was greater than 25), estimates (e.g., how many dots are there on the screen), or confidence intervals (e.g., placing a 95% confidence interval around the estimate). What is needed is an extension of the hypothesis space beyond the categorical hypotheses used when making a choice. In principle, within a Bayesian framework, this is straightforward, although the resulting model looks very different. Instead of simply using two categorical hypotheses (e.g., whether or not there are more than 25 dots on the screen), the model can instead represent the fine-grained hypotheses relevant for estimates (e.g., the exact number of dots on the screen). With such a representation, estimates and confidence intervals can simply be a function (e.g., the mean and quantiles respectively) of this distribution. The probabilities of categorical hypotheses used to produce choices, confidence judgments, and response times can be calculated simply by summing up the posterior probability of the fine-grained hypotheses that are consistent with each choice (e.g., summing the probability of all the hypotheses in which the number of dots is more than 25). This representational change, however, does not allow a probabilistic model to account for many empirical effects found with estimates and confidence intervals. For estimates, anchoring effects demonstrate a dependence on preceding choices even when the choice question transparently contains no information <ref type="bibr" target="#b179">(Tversky &amp; Kahneman, 1974)</ref>.</p><p>Moreover, estimated confidence intervals are empirically far too narrow and are strikingly different depending on whether participants produce or evaluate them <ref type="bibr" target="#b86">(Juslin et al., 2007)</ref>. In addition, a long line of empirical work shows that probability judgments are systematically biased and incoherent (e.g., subadditivity, conjunction fallacies, partition dependence), which argues against all purely probabilistic models (e.g., <ref type="bibr">Tversky &amp; Kahneman, 1983;</ref><ref type="bibr" target="#b180">Tversky &amp; Koehler, 1994;</ref><ref type="bibr" target="#b171">Tentori et al., 2013;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref><ref type="bibr" target="#b203">Zhu et al., 2020)</ref>.</p><p>Exact probabilistic models also show fundamental mismatches with the results of recent investigations into the source of noise in human judgment and decision making. While probabilistic models assume a noise-free inference process using precise probabilities, there is growing empirical evidence suggesting that much, or even most, variability in decision making in fact arises from "computational noise" (i.e., variability in precision and approximation used to perform inference) rather than "sensory noise" (i.e., variability in relevant sensory features) or "decision noise" (i.e., variability associated with action selection) <ref type="bibr" target="#b43">(Drugowitsch et al., 2016;</ref><ref type="bibr" target="#b51">Findling &amp; Wyart, 2021;</ref><ref type="bibr" target="#b165">Stengård &amp; van den Berg, 2019)</ref>. Clearly, then, there are problems with the descriptive adequacy of all probabilistic models, including SDT and the SPRT, which may stem from the psychologically implausible assumption of exact calculation of probabilities and the lack of mechanism to account for the stochasticity arising from the inference process. In the next section, we propose how to address these fundamental problems, before evaluating how far the proposed solution produces a better qualitative match to a wide range of regularities in human behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling-based Approximation Perspective for Rational Decision Making</head><p>Assuming imprecise probabilities does not necessarily mean abandoning probabilistic models. While exact Bayesian computation is often out of reach for real-world computational mechanisms, including the human brain <ref type="bibr" target="#b1">(Anderson, 1991;</ref><ref type="bibr" target="#b3">Aragones et al., 2005;</ref><ref type="bibr" target="#b98">Kwisthout et al., 2011)</ref>, computer scientists and statisticians have devised a number of sophisticated, general-purpose approximations for producing useful answers with a more reasonable amount of computational time and effort. It is therefore interesting to explore whether the brain has hit on similar solutions. One major family of general-purpose approximations in computer science and statistics are sampling techniques 2 .</p><p>Following the Bayesian approach, we propose that people solve cognitive tasks by building an internal model and posterior distribution over fine-grained hypotheses, which can support the responses for all of the aforementioned six behavioral measures. But, because the exact representation of the posterior probabilities of hypotheses is typically computationally intractable, we further hypothesize that the posterior probability distribution is not computed exactly, but is approximated by drawing representative samples from that distribution.</p><p>Sampling-based approximations to the posterior are appealing as a psychological mechanism because (i) some sampling algorithms (e.g., MCMC: Brooks et al., 2011) need only local knowledge of the target posterior distribution and can represent only one or a few hypotheses at a time, lending these algorithms psychological plausibility <ref type="bibr" target="#b1">(Anderson, 1991;</ref><ref type="bibr" target="#b154">Sanborn, Griffiths, &amp; Navarro, 2010)</ref>, (ii) sampling algorithms show much of the same behavioral variability and deviations from ideal probabilistic inference as observed in people across a range of domains <ref type="bibr" target="#b71">(Griffiths, Vul, &amp; Sanborn, 2012;</ref><ref type="bibr" target="#b153">Sanborn &amp; Chater, 2016;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref><ref type="bibr" target="#b103">Lieder et al., 2018;</ref><ref type="bibr" target="#b190">Vul et al., 2014;</ref><ref type="bibr" target="#b206">Zhu et al., 2022b)</ref>, and (iii) the variability of sampling algorithms has been found to match neural variability in the cortex <ref type="bibr">(Hoyer &amp; Hyvarinen, 2003;</ref><ref type="bibr" target="#b72">Haefner et al., 2016;</ref><ref type="bibr" target="#b52">Fiser et al., 2010)</ref>. These suggest that the samplingbased explanations can connect with all three of <ref type="bibr" target="#b113">Marr's (1982)</ref> celebrated explanatory levels: computational (through implementing Bayesian inference), algorithmic (via a tractable <ref type="bibr">2</ref> We do not further consider other general-purpose approximation algorithms, such as variational inference, in which a simpler distribution is used to approximate a more complex one, and the statistical distances between the two distributions are minimized by optimisation algorithms. Such algorithms provide an alternative source of explanations for human behaviour <ref type="bibr" target="#b63">(Gershman &amp; Beck, 2017;</ref><ref type="bibr">Sanborn, 2017)</ref>. computational mechanism), and implementational (through potentially mapping on to neural activity).</p><p>Taking a sampling-based approximation perspective to model choices suggests decision-making should be conceptualized as the problem of integrating a sequence of stochastic hypotheses into categorical decisions. The key distinction with other probabilistic models such as SDT and the SPRT is that we specifically define the 'evidence' as samples of hypotheses, abstracting away from noisy sensory percepts or memory traces (see <ref type="figure" target="#fig_0">Figure 2B)</ref> and implying that it is computational noise in the inference process that is the primary source of variability in behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Autocorrelated Bayesian Sampler</head><p>Here we outline a rational process for producing probability judgments, estimates, confidence intervals, choices, confidence judgments, and response times based on a sampling approximation of the posterior probability of fine-grained hypotheses, which we call the Autocorrelated Bayesian Sampler (ABS). Our key theoretical contributions are links between the sampling process and each of the six behavioral measures. This is possible because samples of the fine-grained hypotheses contain all of the relevant information to produce these (and indeed many other) aspects of behaviors.</p><p>Continuing our numerosity example (see <ref type="figure">Figure 1</ref> and 2), the ABS produces behavior based on the posterior probability of the hypotheses, (ℎ| ), which is calculated using Bayes rule:</p><formula xml:id="formula_23">(ℎ| ) = ( |ℎ) (ℎ) ( )<label>(3)</label></formula><p>where ℎ is a hypothesis, is a stimulus, ( |ℎ) is the likelihood of a stimulus given a hypothesis, (ℎ) is the prior probability of a hypothesis 3 , and ( ) is the overall probability of observing the stimuli across all possible hypotheses included in the internal model. In the numerosity task, for example, the hypothesis space reflects all possible numbers of dots that may have appeared on-screen, while the posterior distribution could be represented as a This general framework applies far beyond our numerosity task illustration. For example, it can be applied to intuitive physics when ℎ is a complete object trajectory and is the initial movement of an object (e.g., <ref type="bibr" target="#b73">Hamrick et al., 2015;</ref><ref type="bibr" target="#b10">Battaglia, Hamrick, &amp; Tenenbaum, 2013;</ref><ref type="bibr" target="#b155">Sanborn, Mansinghka, &amp; Griffiths, 2013)</ref>, language production when ℎ is the next word in a sentence and are the preceding words (e.g., <ref type="bibr" target="#b24">Chater &amp; Manning, 2006;</ref><ref type="bibr" target="#b101">Levy, Reali, &amp; Griffiths, 2008)</ref>, and common-sense reasoning when ℎ is a social goal of other agents and is a sequence of actions performed by those agents (e.g., <ref type="bibr" target="#b7">Baker, Saxe, &amp; Tenenbaum, 2009;</ref><ref type="bibr" target="#b6">Baker, Goodman, &amp; Tenenbaum, 2008)</ref>. Similarly, Bayesian models have also been successfully implemented in explaining effects in other areas of psychology such as vision (e.g., <ref type="bibr" target="#b197">Yuille &amp; Kersten, 2006)</ref>, motor control <ref type="bibr">(Kording &amp; Wolpert, 2004)</ref>, causal reasoning (e.g., <ref type="bibr" target="#b0">Abbott &amp; Griffiths, 2011;</ref><ref type="bibr" target="#b14">Bramley et al., 2017)</ref>, and learning (e.g., <ref type="bibr" target="#b32">Courville &amp; Daw, 2007;</ref><ref type="bibr" target="#b64">Gershman, Blei, &amp; Niv, 2010</ref>).</p><p>3 Note that this prior reflects the prior knowledge of the hypothesis space and therefore should be amenable to feedback and experimental instructions about the hypothesis space. For the running numerosity example, the experimenter could explicitly inform participants, or they could learn through experiences, that the numbers of dots appearing on-screen across trials are uniformly distributed in the range of <ref type="bibr">[21,</ref><ref type="bibr">30]</ref>. Indeed, previous work has shown that participants can quickly acquire an accurate prior from feedback in a numerosity task <ref type="bibr" target="#b152">(Sanborn &amp; Beierholm, 2016)</ref>. In this case, for simplicity, we should use a uniform distribution as the prior for hypotheses.</p><p>Next, a set of hypotheses are sampled from the fine-grained posterior distribution, and these samples directly and straightforwardly support all six of our measures ( <ref type="figure" target="#fig_0">Figure 2B</ref>).</p><p>Probability judgments are based on the relative proportion of the samples (e.g., the number of samples with numerosities greater than 25). Estimates are based on a summary statistic of the samples (e.g., the mean sampled numerosity or the value of the latest sample). Confidence intervals are based on the quantiles of the samples (e.g., ordering five samples and using the numerosities of the 2 nd and 4 th sample as the bounds of a 60% confidence interval). Choices are based on the preponderance of the samples (e.g., depending on whether more than half the samples have numerosities greater than 25). Confidence judgments are (like probability judgments) based on the relatively proportion of the samples that agree with the choice.</p><p>Response times are a function of the number of samples drawn (e.g., on average drawing four samples takes longer than three).</p><p>To generate concrete predictions from the model, and assess the match with human behavior, we need to outline three further aspects of the model: the choices of sampling algorithm, prior on responses, and stopping rule, to which we now turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Sampling Algorithm</head><p>We assume that the mind conducts sampling-based approximations by drawing samples of hypotheses in proportion to the posterior probabilities associated with each hypothesis (e.g., <ref type="bibr" target="#b71">Griffiths, Vul, &amp; Sanborn, 2012;</ref><ref type="bibr" target="#b190">Vul et al., 2014;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref><ref type="bibr" target="#b203">Zhu et al., 2020;</ref><ref type="bibr" target="#b25">Chater et al., 2020)</ref>. Rather than reviewing the extensive literature on sampling algorithms in statistics and computer science (see <ref type="bibr" target="#b2">Andrieu et al., 2003</ref> for an overview), we focus on algorithms that have been previously shown to match human behavior in some domains of psychology.</p><p>The simplest sampling algorithm is direct sampling in which independent and identically distributed (i.i.d.) samples are drawn (e.g., <ref type="bibr" target="#b190">Vul et al., 2014)</ref>. However, a lot must be known about the target distribution to draw i.i.d. samples: people would have to (at least implicitly) know the posterior probability of every hypothesis, which fails to alleviate the intractability problem that motivates the need for sampling approximations. Another difficulty with for direct sampling is descriptive. Human hypothesis generation is not a process of drawing independent samples, as the direct sampling requires. Instead, what comes to mind now depends on what came to mind in the past <ref type="bibr" target="#b67">(Gilden et al., 1995;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref><ref type="bibr" target="#b200">Zhu et al., 2022c)</ref>.</p><p>In light of this, researchers have recently started to explore a family of more sophisticated sampling algorithms called Markov Chain Monte Carlo (MCMC; <ref type="bibr" target="#b144">Robert &amp; Casella, 2004)</ref>. MCMC algorithms explore the hypothesis space using only local knowledge about the probability distribution, greatly reducing the knowledge required to generate samples. The key idea of MCMC is that, in its simplest form, it represents only a single hypothesis at a time, and probabilistically transitions between hypotheses in proportion to their posterior probabilities. The local transitions induce a serial dependence between samples, akin to the local transitions in human hypothesis generation <ref type="bibr" target="#b14">(Bramley et al., 2017;</ref><ref type="bibr">Franken, Theodoropoulos, &amp; Bramley, 2022)</ref>.</p><p>In our own work, we have found that an extension of MCMC, named MC 3 , provides a close match to the dynamics of repeated human judgement, capturing the observed longrange autocorrelations between estimates, as well as the heavy-tailed distribution of changes between responses <ref type="bibr" target="#b204">(Zhu, Sanborn, &amp; Chater, 2019;</ref><ref type="bibr" target="#b200">Zhu et al., 2022c;</ref><ref type="bibr" target="#b205">Zhu et al., 2021;</ref><ref type="bibr" target="#b163">Spicer et al., 2022)</ref>. We therefore use MC 3 as the sampling algorithm in the present model, though the specific mechanics of this algorithm beyond dependencies between samples are not necessary for almost all of the behaviors targeted here (see Appendix A for algorithmic details). That is, with the exception of explaining the cross-trial autocorrelation results which requires quantitative characterizations of the dependence in samples, the key condition for a sampler to reproduce the qualitative model behaviors (e.g., comparing average model behaviors between experimental conditions) is simply that sampling is local and autocorrelated. Thus, most model predictions can be replicated using many other MCMC sampling algorithms, including the widely-used Random Walk Metropolis algorithm, and many others, so long as the generated samples are positively correlated across time.</p><p>Using dependent samples influenced our choice for how the ABS produces estimates.</p><p>In past work, estimates have been based on the most recently generated sample or by averaging over samples <ref type="bibr" target="#b190">(Vul et al., 2014;</ref><ref type="bibr" target="#b103">Lieder et al., 2018)</ref>. While the mean of a set of independent samples is clearly a better estimate of the underlying mean than a single sample, with dependent samples, earlier samples are more likely to be biased by the starting point than later samples. For this reason, we chose to use the last sample as our estimate. However, these two approaches do not predict qualitatively different behavior on aggregate (see Appendix E for details).</p><p>Producing confidence intervals, however, requires more than a single sample, and instead can reflect statistics of the entire set of samples: for example, the 2.5% and 97.5% quantiles of the samples can represent a 95% confidence interval of the target distribution.</p><p>This approach can only be applied directly for large samples. With small samples, we produce more fine-grained intervals by following <ref type="bibr" target="#b86">Juslin et al. (2007)</ref> and use linear interpolation to fill in the gap between the two quantiles of the samples.</p><p>We also assume that sampling takes time. For simplicity, we model the time necessary to produce samples as a Poisson process: while time taken to produce a new sample is random, the samples are generated at a constant rate ( samples per sec). In a Poisson process, the waiting time between samples is exponentially distributed, and the time necessary to generate samples follows an Erlang distribution:</p><formula xml:id="formula_24">( ) ~ Erlang( , )<label>(4)</label></formula><p>The mean and variance of RT for a sample size of are then defined as</p><formula xml:id="formula_25">[ ] = &amp; ' and [ ] = &amp; '</formula><p>! respectively. Using a Poisson process allowed us to more closely link our approach to existing models such as the Poisson random walk model <ref type="bibr" target="#b12">(Blurton et al., 2020;</ref><ref type="bibr">discussed below)</ref>, though the results in this paper would be qualitatively the same under a wide variety of assumptions of how long it takes to generate each additional sample. This is because many empirical results only require assuming the samples were generated sequentially and the time to generate a sample is non-zero. Exponential waiting times are assumed here to explain the shape of RT distributions, particularly the observation that the response times for probability judgments (which we assume to have been produced using a fixed number of samples) have heavy tails (see Appendix F) 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Bayesian Monte Carlo Prior on Responses</head><p>Samples of the fine-grained hypotheses generated from our sampling algorithm can be readily used to make a choice. In our numerosity example, if asked to decide whether the number of dots that appeared on-screen is greater than 25, the hypothesis space should be partitioned into two subspaces with 25 on the boundary line. Samples that indicate greater than 25 dots or not should be categorized as supporting evidence for the corresponding hypotheses. That is, evidence is directly translated from the samples, here taking one of two values. And also, unlike the evidence used in SPRT, there is no inherent uncertainty about which alternative each sample supports. For the numerosity example, the generated sample can denote any number of dots in the hypothesis space, but it can only support one alternative in decisions: if the sample was 23, it only supports the hypothesis that there were less than 25 dots on screen.</p><p>Similarly for M-alternative choices ( &gt; 2), the hypothesis space should be partitioned into M subspaces with hypothesis samples from each subspace supporting the corresponding alternative.</p><p>These samples implicitly carry information about the probability that each choice alternative is correct. For example, when asked about the probability that the number of dots is greater than 25, the relative frequency of evidence in favor of the event should inform the probability estimate. But, as explored in Zhu et al. <ref type="formula">2020</ref>, people should not directly use the relative frequency of the hypotheses as a probability estimate. This is especially true when sample sizes are small because the relative frequency tends to be extreme. Indeed, a single sample would lead to a probability estimate of either 0 or 1. This problem can be solved by incorporating a prior on responses to temper the relative frequency in the estimates of the probability that each choice alternative is correct, an approach that in statistics is called Bayesian Monte Carlo <ref type="bibr" target="#b62">(Gelman et al., 2013;</ref><ref type="bibr" target="#b132">Rasmussen &amp; Ghahramani, 2002)</ref> 5 . The Bayesian Sampler model of Zhu et al. (2020) used a fixed prior on responses, and for mathematical simplicity, this was chosen to be a Beta distribution, because this is the conjugate prior for probability estimates. The Beta distribution is bounded by 0 and 1, and has two parameters, ( and ! , which determine its shape: when both parameters exceed 1, <ref type="bibr">5</ref> The Bayesian Monte Carlo prior on responses is different from the prior on hypothesis in Equation 3. Specifically, the Bayesian Monte Carlo prior should capture full or partial information about the frequencies of the relevant behavioral outcomes in past trials. Consider again the binary choice in the numerosity example where people were asked to judge whether the number of dots is greater than 25. In this case, the prior on responses should reflect, to some extent, prior belief in different probabilities that each response is correct; and this prior knowledge could be acquired through feedback of correctly choosing greater-than-25 and of correctly choosing the alternative lessor-equal-than-25. An additional difference is that when knowledge of the probabilities is precise, even if there is uncertainty in the hypotheses, the effect of the Bayesian Monte Carlo prior reduces. Thus, in Equation 5, as the sample size, N, approaches infinity then " ( ) tends to ( ).</p><p>the Beta distribution is unimodal with a peak in the middle of the range (i.e., at</p><formula xml:id="formula_26">) " *! ) # +) " *" );</formula><p>when both parameters equal 1, it is uniform; and when both parameters are less than 1, it is bimodal with peaks at both 0 and 1. Most critically, using the Beta distribution as the prior enables evidence to act as pseudocounts in the parameters. For ( ) pieces of evidence of event A, (¬ ) of event not-A, and a Beta( ( , ! ) prior, people will have a posterior distribution for probability estimates that is distributed according to Beta( ( + ( ), ! + (¬ )). The Bayesian Sampler model used the expected value of this posterior distribution as its probability estimate, which is also simple to calculate:</p><formula xml:id="formula_27">O ( ) = ( + ( ) ( + ( ) + ! + (¬ ) = ( + ( ) + ( + !<label>(5)</label></formula><p>where = ( ) + (¬ ) denotes the total amount of samples that were generated and translated into evidence. Both the prior parameters (which affect ( , ! ) and the sampling process (which affects ( ) and ) affect the expected value. As the prior parameters are defined to be non-negative (i.e., ( , ! ≥ 0), the Bayesian Sampler's estimated probabilities tend to avoid extreme values and regress to the mean of the prior (i.e., ( /( ( + ! )).</p><p>Here, we generalize the prior on responses used in the original Bayesian Sampler in two ways. The first is to make it multivariate: in many situations, people can be asked to judge a multivariate event where the hypothesis space should be partitioned into many subspaces. For example, when asked "what is the probability that the hottest day of the week will be Sunday?", there are seven comparable events for the seven days in a week ("Sunday hottest", "Monday hottest", and so on). In this case, the Dirichlet distribution, a multivariate generalization of the Beta distribution, is the natural conjugate prior. For an -variate Dirichlet prior, Dir( ), with = ( ( , ! , … , ,*! ), people report the mean posterior distribution as their probability estimates:</p><formula xml:id="formula_28">O ( ) = ( + ( ) + ∑ - ,*! -%(<label>(6)</label></formula><p>This view of probability estimates implies an indifference point (when the underlying probability and the estimated probability matches) that depends on the number of alternatives (see <ref type="figure" target="#fig_6">Figure 4A</ref>). Indifference points were directly reported by the data analyses in <ref type="bibr" target="#b160">Fox &amp; Rottenstreich (2004)</ref>, <ref type="bibr">Bardolet, Fox, &amp; Lovallo (2004)</ref>, <ref type="bibr" target="#b186">and Varey, Mellers, &amp; Birnbaum (1990)</ref>, and were inferred from the regression in <ref type="bibr" target="#b5">Attneave (1953)</ref>.</p><p>The second way in which we generalize the prior on responses of the Bayesian Sampler is to allow it to adapt to experience (e.g., the trial history in an experiment). In Bayesian data analysis, when no prior information is available, a default prior is typically recommended <ref type="bibr" target="#b62">(Gelman et al., 2013)</ref>. However, for many real-world applications and especially for everyday cognitive tasks, historical data (e.g., past experiences of the same A B</p><p>task, data from similar previous tasks or from observing others' performing the task) are available which can help people can construct an appropriate prior. For example, if repeatedly choosing between the same two alternatives, historical choice data should provide useful information such as the base rate, which in turn can help construct a prior on responses to guide future decisions. How to construct an adaptive prior based on historical data is a topic of debate in statistics and computer science because it is difficult to determine how much to generalize previous experience to new situations <ref type="bibr" target="#b80">(Ibrahim et al., 2015;</ref><ref type="bibr" target="#b27">Chen et al., 2000;</ref><ref type="bibr" target="#b35">Diaconis &amp; Ylvisaker, 1979)</ref> 6 . For simplicity, we assume that people only use information from the immediately previous trial to develop their adaptive prior for the present trial: in binary choice, a non-informative, uniform prior (Beta(1,1)) is adjusted to favor the option the feedback indicated was correct, becoming either Beta(2,1) or Beta(1,2).</p><p>The adaptive prior on responses, in conjunction with the generated samples, then determines the model's estimated probability of a categorical alternative being correct. This estimated probability is used both as the model's probability estimate and its confidence judgment in whether a choice is correct 7 . The equivalence between the two is not unique to our model-it has been previously posited as the Bayesian Confidence Hypothesis <ref type="bibr" target="#b111">(Mamassian, 2016;</ref><ref type="bibr" target="#b89">Kepecs &amp; Mainen, 2012;</ref><ref type="bibr" target="#b130">Pouget et al., 2016)</ref>, and has attracted both support (Calder-Travis et al., 2020) and criticism <ref type="bibr" target="#b102">(Li &amp; Ma, 2020</ref>).</p><p>6 Incorporating historical information into new situations is known as power prior in the statistics literature, which is also closely related to the ideas of meta-learning (or learning-to-learn) and hierarchical Bayesian modeling.</p><p>7 Confidence judgments are often made on various ordinal rather than probability scales, though analyses of confidence judgments often just assume that they are monotonically related <ref type="bibr" target="#b102">(Li &amp; Ma, 2020;</ref><ref type="bibr" target="#b156">Shekhar &amp; Rahnev, 2021a</ref>), as we do here. For comparability across different ordinal scales we present all of the model predictions on the probability scale rather than specifying those relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Stopping Rule</head><p>Any model of judgment or decision that depends on the sequential accumulation of evidence needs a rule determining at what point to stop collecting evidence and make a decision. When to stop drawing samples should depend on both the costs (e.g., metabolic, opportunity, etc.) of sampling as well as the task-specific benefits of additional samples for providing a good response. For probability judgments, estimates, and confidence intervals, in the absence of a reason to do otherwise, we make the simplest possible assumption: that a fixed number of samples are drawn to answer each question.</p><p>A fixed number of samples will allow the model to produce indifferent probability judgments (e.g., judging a binary event to have a probability of 0.5) as is often observed in the human data 8 . However, for making decisions a fixed sample is likely to be too simple. If the samples so far leave the evidence finely balanced regarding which decision to make, then it is likely that more data will be collected. While it is possible in principle to derive an optimal stopping rule for the sampling process in this model, unlike with the SPRT, the optimal rule is not analytically tractable and can instead only be computed using dynamic programming (see Appendix C). So, again for simplicity, we use a well-known heuristic stopping rule instead: the max-minus-next rule, which counts the difference in evidence between the top two hypotheses, and terminates the sampling process whenever the accumulated difference exceeds a threshold. This simple heuristic stopping rule has also been shown to approach the performance of an optimal SPRT even in multi-alternative settings <ref type="bibr" target="#b38">(Dragalin et al., 1999;</ref><ref type="bibr" target="#b185">2000)</ref>. For binary choices, this reduces to just the difference in the number of samples in favor of each alternative, which has been proposed in past work <ref type="bibr" target="#b73">(Hamrick et al, 2015;</ref><ref type="bibr" target="#b190">Vul et al, 2014)</ref>. The decision-making panels of <ref type="figure" target="#fig_0">Figure 2B</ref> demonstrates the max-minus-next stopping rule with a threshold value of 2.</p><p>While the choice of stopping rule does not change how samples are used to produce the different measures, it does influence the content of the samples and the variability of the sample size and hence responses times. So, for example, in our model, while the response times for a probability judgment which assumes fixed sample sizes will follow an Erlang distribution (see Appendix F for further justification), response times for a choice (which assumes optional stopping) will follow a mixture of Erlang distributions (see Appendix B for details). </p><p>The initial hypothesis used by the autocorrelated sampler will often be the comparison value used in the decision task (i.e., the anchored hypothesis). Moreover, the samples used to reach the decision are assumed to be reused in the estimation task. People are more confident in conditions in which they take more time to make a choice. <ref type="bibr" target="#b188">Vickers &amp; Packer (1982)</ref> The decision threshold is greater in the accuracy condition than in the speed condition, so choices will take longer while the greater difference in sample counts at threshold leads to higher confidence. We now demonstrate the explanatory power of the ABS. We focus on behavioral results that deviate from the Bayesian ideal embodied in models like the SPRT (see <ref type="table" target="#tab_1">Table 1</ref>), simulating these using a consistent set of parameters (detailed in Appendix A). To facilitate understanding of the active ingredients of the model, we also show results from three restricted variants of the full ABS model. The no-prior variant removes the adaptive prior (i.e., equivalent to fixing the prior to Beta(0,0)) while keeping the remaining components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Intervals</head><p>The direct-sampling variant uses independent samples instead of the autocorrelated samples while keeping the remaining components. The fixed-sample-size variant always uses a fixed number of samples (N=5) to form behaviors while keeping the remaining components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biases in Probability Judgments</head><p>Biases in probability judgments are perhaps the most direct evidence against purely normative probabilistic models. We find that the prior on responses and local sampling algorithm of the ABS, which help to reduce the computational burden of the model by reusing old and useful computations and by using only local knowledge of the posterior distribution respectively, suffice to produce many of these biases. In the ABS, there are no biases in the underlying posterior probabilities; biases arise solely from the algorithmic process by which the posterior is sampled and judgments and decisions are generated.</p><p>Using prior knowledge to temper the probability estimates was the basis of the Bayesian Sampler model <ref type="bibr" target="#b203">(Zhu et al., 2020)</ref>. The ABS works in the same way, except that it uses autocorrelated, rather than independent, samples 9 . As shown in <ref type="figure" target="#fig_6">Figure 4A</ref>, the prior in the Bayesian Sampler produces a linear bias toward conservative judgments <ref type="bibr" target="#b203">(Zhu et al., 2020)</ref> where people avoid the extremes in their probability judgments <ref type="bibr" target="#b50">(Fiedler, 1991;</ref><ref type="bibr" target="#b128">Peterson &amp; Beach, 1967;</ref><ref type="bibr" target="#b46">Erev, Wallsten, &amp; Budescu, 1994)</ref>. This type of conservatism captures the results of a series of probabilistic identities investigated by Costello, Watts, and colleagues <ref type="bibr" target="#b28">(Costello &amp; Watts, 2014;</ref><ref type="bibr" target="#b31">Costello, Watts, &amp; Fisher, 2018)</ref>, which were constructed by adding and subtracting various mean judgments across combinations of events. While all these identities would equal zero if participants reported coherent probabilities (even if just on average), mean judgments were zero for some identities and substantially different from zero for others. The results from the entire set of identities,</p><p>including conditional probability judgments of dependent events, were well fit by the Bayesian Sampler's linear conservatism bias <ref type="bibr" target="#b203">(Zhu et al., 2020)</ref>. As the average behavior of the ABS is approximated by the Bayesian Sampler especially when the effects of local sampling are not strong (e.g., random initializations of the local sampler), the ABS will produce these results as well.</p><p>The sample size and prior on responses of the ABS can be dissociated by examining the mean-variance relationship in probability judgments. When probability judgments are <ref type="bibr">9</ref> The autocorrelation of the samples does not qualitatively alter the overall model predictions for probability judgments (with the exception of implicit subadditivity and implicit superadditivity, discussed below) because autocorrelated sample sizes can be corrected to produce the effective sample size of independent samples using the following equation:</p><formula xml:id="formula_30">!"" = # $%&amp; ∑ ( ! " !#$</formula><p>where ) is the degree of autocorrelation at lag . For the parameters we used in the simulations, the effective sample size is on average 16.80% of the autocorrelated sample size (95% CI, [16.40%, 17.21%]). In addition, in the studies we refer to, there is very rarely any feedback. Without feedback, we assume the ABS prior does not change from trial to trial, making it identical to the fixed prior of the Bayesian Sampler for binary events.</p><p>based on sampled outcomes, the relationship between the mean probability estimates and the variance of the estimates will constitute an inverse U-shaped ("rainbow-shaped") curve (see <ref type="figure" target="#fig_8">Figure 5A</ref>). The prior on responses then constrains the range of possible probability estimates that an agent can produce, thereby lowering the relative variance and pulling the curve both inward and downward (see <ref type="figure" target="#fig_8">Figure 5B</ref>). For example, for a binary event with a uniform prior, if a single sample is drawn, probability judgments will be either 0.33 or 0.67, and total variance will be relatively lower than for the pure proportions of sampled outcomes (taking now account of the prior on responses). Overall, the Bayesian Sampler predicts a shrinkage of the mean-variance curve for probability judgments, and this was empirically validated in four experiments <ref type="bibr" target="#b168">(Sundh et al., 2021)</ref>. For the same reasons as the Bayesian Sampler, the ABS model predicts this shrinkage of the mean-variance curve as well (see <ref type="figure" target="#fig_8">Figure 5B</ref>). Moreover, the phenomenon of explicit subadditivity in probability judgments also occurs as a direct consequence of using the prior on responses. Explicit subadditivity is when the estimated probability of an event ( ( ) is lower than the sum of estimated probabilities for events ( ! , " , … , ,. ) where ( is the disjunction of those ′ mutually exclusive events.</p><p>That is:</p><formula xml:id="formula_31">O ( ( ) &lt; O ( ! ) + O ( " ) + ⋯ + O ( ,. )<label>(7)</label></formula><p>where probability theory requires that these should be equal. An explicit subadditivity bias was observed generally in between-participant designs in which participants were asked explicitly to judge the probability of each of the ′ events and their disjunction, ( , so that a total of ′ + 1 probability estimates were recorded (e.g., <ref type="bibr" target="#b180">Tversky &amp; Koehler, 1994;</ref><ref type="bibr">Tversky &amp; Fox, 1994;</ref><ref type="bibr">Fox et al., 1994)</ref>. According to the sampling account, for each query, because participants do not know the full range of questions that are asked, they will treat the event to be judged as a binary event; that is, participants will sample instances and non-instances of that event (that is, -vs. not--), thus requiring a Beta prior on responses. The resulting estimate of each ( -) will therefore be inflated by regression to the mean. The regressionto-mean effect then applies multiple times on aggregate to the right-hand side of Equation 7</p><p>and only once to the left-hand-side, predicting a subadditivity bias for low probability events.</p><p>As a corollary, more probability judgments queried on the right-hand-side of Equation 7 should associate with a greater degree of subadditivity bias. For M' component hypotheses, the predicted difference between the sum of the M' probability estimates and the probability estimates of the disjunction can be derived as follows:</p><formula xml:id="formula_32">O ( ! ) + O ( " ) + ⋯ + O ( , $ ) − O ( ( ) = , X + 2 ( ( -) + ( + 2 ( Y , $ -%! − X + 2 ( ( ( ) + ( + 2 ( Y = ( . − 1) ( + 2 (</formula><p>where the assumptions were fixed sample size ( ) and symmetric prior on responses, Beta( ( , ( ). Indeed, the empirical findings suggest a positive relationship between M' and the degree of explicit subadditivity bias, and the Bayesian Sampler correctly captures the relationship (see <ref type="figure" target="#fig_9">Figure 6</ref>). Moreover, when the disjunction of M' mutually exclusive events was exactly 1, participants were sometimes asked to only judge the probabilities of M'</p><p>component hypotheses but not their disjunction. In this case, model predictions can be analytically approximated as ( ′ − 2)</p><formula xml:id="formula_33">) # &amp;+") #</formula><p>under the same assumption as before. This prediction also matches the empirical pattern known as the binary complementarity: on average, no subadditivity bias was observed for mutually exhaustive events when . = 2 <ref type="bibr" target="#b180">(Tversky &amp; Koehler, 1994</ref>). The ABS model inherits these predictions from the Bayesian Sampler. Similarly, this regression-to-mean effect predicts the conjunction fallacy <ref type="bibr">(Costello &amp; Watts, 2016;</ref><ref type="bibr" target="#b203">Zhu et al., 2020)</ref>. The conjunction fallacy arises where the estimated probability for a conjunctive event is greater than that for its constituent events O ( ( ⋂ ! ) ≥ O ( ( ), whereas the probability theory requires the probability of conjunctive events to be less or equal with their constituents, ( and ! <ref type="bibr">(Tversky &amp; Kahneman, 1983)</ref>. The conjunction fallacy occurs in the Bayesian Sampler when the regression-to-mean applies more to the conjunctive event than to the constituent events. Specifically, it is assumed that fewer samples of the more-complex conjunctive events can be generated or tallied in a fixed amount of time; and the prior produces a greater regression-to-mean effect for smaller sizes (see Equation 6). This allows the Bayesian Sampler to predict above-chance levels of conjunction fallacies when the conjunction and constituent event both have low probability <ref type="bibr" target="#b203">(Zhu et al., 2020)</ref>, as is often the case in empirical work <ref type="bibr">(Costello &amp; Watts, 2016</ref>). The ABS model also inherits this prediction from the Bayesian Sampler.</p><p>In contrast with the explicit judgments of M'+1 probabilities above, both subadditivity and its opposite effect, superadditivity, have been observed in so-called implicit experimental designs. In implicit designs, only two probability judgments are made: one for the unpacked descriptor (e.g., "baby bottles and other bottles made of glass") and one for the simple disjunctive descriptor (e.g., "bottles made of glass") <ref type="bibr" target="#b33">(Dasgupta et al., 2017;</ref><ref type="bibr" target="#b160">Sloman et al., 2004)</ref>. Unpacking to typical examples (e.g., a baby bottle in the category of bottles made of glass) leads to subadditivity:</p><formula xml:id="formula_34">O ( ( ) ≤ O ( ! ∩ " ∩ … ∩ , )</formula><p>, whereas unpacking to atypical examples (e.g., a shampoo bottle in the category of bottles made of glass) leads to <ref type="bibr" target="#b33">(Dasgupta et al., 2017;</ref><ref type="bibr" target="#b160">Sloman et al., 2004)</ref>.</p><formula xml:id="formula_35">superadditivity: O ( ( ) ≥ O ( ! ∩ " ∩ … ∩ , )</formula><p>Again, since ( was unpacked into mutually exclusive events ( ( = ! ∩ " ∩ … ∩ , ), probability theory requires the two probability estimates to be equal. Previous work with autocorrelated sampling models <ref type="bibr" target="#b33">(Dasgupta et al., 2017;</ref><ref type="bibr" target="#b153">Sanborn &amp; Chater, 2016)</ref> accounted for this effect by assuming that the descriptor influenced the local sampler's starting point:</p><p>typical unpacking initializes the sampler in a high probability region of the hypothesis space, while atypical unpacking initializes it in a low probability region. As a result, the proportion of hypotheses supporting the event's occurrence will be highest for typical unpacking, intermediate for the simple disjunctive descriptor (assuming it results in a random starting point), and lowest for atypical unpacking. We believe that the ABS will inherit this prediction because it produces autocorrelated samples, though we do not reproduce it here because auxiliary assumptions about the locations and probabilities of hypotheses are needed to do so.</p><p>This explanation of implicit subadditivity and superadditivity depends on local sampling.</p><p>They cannot be predicted by the Bayesian Sampler model <ref type="bibr" target="#b203">(Zhu et al., 2020</ref>; see a similar argument against a "regressive model" in <ref type="bibr" target="#b180">Tversky and Koehler, 1994)</ref>, which assumes independent sampling.</p><p>Interestingly, people's probability estimates are also found to exhibit so-called "partition dependence." That is, they regress to ! , where is the number of alternatives that people are encouraged to consider (see <ref type="figure" target="#fig_6">Figure 4B</ref> for a summary; <ref type="bibr">Attneave,1953;</ref><ref type="bibr" target="#b58">Fox &amp; Rottenstreich, 2003;</ref><ref type="bibr">Bardolet et al., 2004;</ref><ref type="bibr" target="#b186">Varey et al., 1990)</ref>. For example, asking</p><p>"what is the probability that Sunday will be hotter than any other day next week?" encouraged participants to treat the event as binary, and their estimates were observed to be biased toward ! " , while asking, "what is the probability that the hottest day of the week will be Sunday?", encouraged participants to consider seven possible outcomes, and estimates were observed to be biased toward ! / <ref type="bibr" target="#b58">(Fox &amp; Rottenstreich, 2003)</ref>. In ABS, framing the probability query as judging an M-variant event invokes a Dirichlet prior with M parameters,</p><formula xml:id="formula_36">Dir(α ( , α ! , … , α ,*! )</formula><p>, which for a binary event reduces to a Beta prior, Beta(α ( , α ! ). Partition dependence effects can be explained by assuming that people have no a priori reason to believe one event occurs more often than another event: ( = ! = ⋯ = ,*! and so probability estimates are predicted to be biased toward</p><formula xml:id="formula_37">) # ∑ ) % &amp;'" ()# = !</formula><p>, (see <ref type="figure" target="#fig_6">Figure 4A</ref>). In the ABS, the impact of this non-informative prior should be more pronounced in situations where people are less knowledgeable about the probability estimation task or less confident in a learning context (reflecting fewer samples), matching the empirical results <ref type="bibr" target="#b57">(Fox &amp; Rottenstreich, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice Accuracy and Response Times</head><p>The Bayesian Monte Carlo process for choice and RT correctly predicts four key relationships between choice and RT. First, and in common with many other evidence accumulation models, the ABS predicts a trade-off between accuracy and speed where increasing decision thresholds lead to, on average, more evidence being accumulated (and thus higher accuracy) as well as longer response times. This trade-off between accuracy and speed has been widely documented in the literature <ref type="bibr" target="#b61">(Garrett, 1922;</ref><ref type="bibr" target="#b83">Johnson, 1939;</ref><ref type="bibr" target="#b127">Pachella, 1974;</ref><ref type="bibr" target="#b194">Wickelgren, 1977;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998)</ref>.</p><p>Second, unlike many models, the ABS predicts that correct and incorrect responses have unequal average response times. The empirical result is that, when accuracy is emphasized (or in difficult tasks), errors are usually slower than correct responses. By contrast, when speed is emphasized (or in easy tasks), errors are usually faster <ref type="bibr" target="#b108">(Luce, 1986;</ref><ref type="bibr" target="#b169">Swensson, 1972;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b142">Ratcliff, Thapar, &amp; McKoon, 2003)</ref>. This empirical pattern is surprisingly difficult to match for models that accumulate relative evidence to symmetric bounds: these models predict that the response time distributions for correct responses and errors will always be the same, regardless of choice accuracy <ref type="bibr" target="#b105">(Link &amp; Heath, 1975;</ref><ref type="bibr" target="#b187">Vickers, 1979)</ref>. To produce slow errors, the usual route is to add variability to the strength of the "signal", or the drift rate in DDMs <ref type="bibr" target="#b137">(Ratcliff &amp; Rouder, 1998)</ref>. While both strong signals and weak signals will produce equal mean response times, weak signals are both more error-prone and slower. So, with an equal mixture of strong and weak signals, there will be more slow errors and more fast correct responses.</p><p>The ABS produces slow errors in a different way. Instead of adding cross-trial variation to the signal strength, or independent within-trial variation to the signal strength <ref type="bibr" target="#b36">(Diederich &amp; Oswald, 2016)</ref>, slow errors result from the local sampling algorithm producing autocorrelated samples. For example, if the sampling algorithm begins far above the decision boundary (e.g., the red subspace in the posterior of hypotheses illustrated in <ref type="figure" target="#fig_0">Figure 2B</ref>), then the initial samples will almost all favor the correct response, while if the sampling algorithm begins far below the decision boundary (e.g., the blue subspace in the posterior of hypotheses) then the proportion of correct samples will almost all favor the incorrect response. Slow errors also require optional stopping, because with a fixed stopping rule the response distribution is itself fixed. This can be seen in the simulation in <ref type="figure" target="#fig_10">Figure 7B</ref>: both autocorrelation and optional stopping (i.e., the no prior variant) are needed to produce errors that are on-average slower than correct responses.</p><p>Fast errors, often found in easy tasks, are produced in a different way. The usual route to producing fast errors is to assume variability in the starting point of the evidence accumulation process <ref type="bibr" target="#b100">(Laming, 1968;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b142">Ratcliff et al., 2003)</ref>. In the ABS, the adaptive prior on responses is assumed to change in response to the outcomes of the preceding trial. This encourages repeating past successes, but also introduces cross-trial variability in the starting point of the accumulator. This is because the accumulator will be biased toward whichever response was correct on the last trial, and assuming that (as is usual in experiments) the correct response randomly varies between trials, it will sometimes be closer to the correct threshold and sometimes closer to the error threshold. For those latter trials, the amount of evidence required to reach the error threshold is reduced, leading to a shortened mean response times for errors. As with slow errors, optional stopping is also necessary: only with both the adaptive prior on responses and optional stopping (i.e., the direct sampling variant) do fast errors appear in the stimulation in <ref type="figure" target="#fig_10">Figure 7D</ref>.</p><p>The differences between the simulations of the 'difficult-accuracy' condition ( <ref type="figure" target="#fig_10">Figure   7B</ref>) and the 'easy-speed' condition ( <ref type="figure" target="#fig_10">Figure 7D</ref>) track the conditions in which slow errors and fast errors are found. We assume that: (i) greater emphasis on accuracy causes the threshold to be higher, and consequentially more pieces of evidence are needed to terminate the sampling algorithms, and (ii) easier stimuli makes the evidence more homogenous (e.g., samples are more likely to point to the same response) 10 . As a result, the 'easy-speed' condition involves integration over homogenous but smaller amounts of evidence than in the 'difficult-accuracy' condition. In other words, the starting point of the accumulator has more influences, while the degree of autocorrelation has less influence, on determining the predicted behavior in the 'easy-speed' condition than in the 'difficult-accuracy'. Across <ref type="figure" target="#fig_10">Figure 7B</ref> and 7D, only the full ABS model matches the empirical observations that slow errors are more common in the 'difficult-accuracy' condition, while fast errors are more common in the 'easy-speed' condition. 10 A reduction in trial difficulty can lead to either less variable posterior of hypotheses (e.g., people are more certain about the number of dots), the decision boundary that partitions the hypothesis space moving to one extreme (e.g., people are asked to judge whether the number of dots are less than 1000 while only 24 dots appeared onscreen), or both. Overall, these model implications of the difficulty reduction all contribute to an increase in proportions of samples that support the correct response. an RT distribution that becomes more positively skewed and spread out with an increase in decision threshold. Empirical data were adapted from <ref type="bibr" target="#b142">Ratcliff, Thapar, and McKoon (2003)</ref>.</p><p>As in other models (e.g., <ref type="bibr" target="#b12">Blurton et al., 2020)</ref>, the assumptions of exponential waiting time between consecutive samples and the optional stopping rule correctly reproduce many distributional properties of RTs including (i) that there tends to be one mode in the distribution and (ii) distributions with higher means more positively skewed. Further regularities in the shapes of RT distributions were stressed by Ratcliff et al. (2015) using quantile-quantile (Q-Q) plots (see <ref type="figure" target="#fig_12">Figure 8A</ref>). Plotting the quantiles of RT from one difficulty condition against the quantiles from another difficulty condition, the empirical Q-Q plots reveals near-linear relationships and a fan shape: increasing task difficulty has its greatest impact on the tails of the distribution with the near linearity suggesting similar RT distribution shapes across difficulty conditions. As shown in <ref type="figure" target="#fig_12">Figure 8B</ref>, the ABS captures the fan shape and near-linear regularity. The direct-sampling variant shows results that are closer to linear, as would be expected if the autocorrelation in samples causes the upper tails in RT distributions to spread out even more in harder tasks. Also of interest is the fixed-sample-size variant, which because it always collects the same number of samples for all difficulty levels, produces identical quantiles between RTs from one level of difficulty and those from another, and thus doesn't match the empirical data.  <ref type="bibr" target="#b142">Ratcliff, Thapar, and McKoon (2003)</ref>. One difficulty level was selected to compute its quantiles and then quantiles of the other four difficulty levels were plotted against the first condition. The rank of a condition depends on its mean RTs. (B) Q-Q plots of RT distributions produced by the ABS model and its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence in Decisions</head><p>From a Bayesian perspective, it is natural to map decision confidence onto the posterior probability that the decision is correct, a mapping which has been called the Bayesian Confidence Hypothesis <ref type="bibr" target="#b111">(Mamassian, 2016;</ref><ref type="bibr" target="#b89">Kepecs &amp; Mainen, 2012;</ref><ref type="bibr" target="#b130">Pouget et al., 2016)</ref>. For the SPRT, posterior probability is updated as sensory samples are observed, and its posterior probability at the time of choice is simple: it is the posterior probability when the threshold is reached, because as evidence collection stops once this occurs (see <ref type="figure" target="#fig_0">Figure 2A</ref> confidence).</p><p>The SPRT thus predicts that decision confidence only relates to the threshold values, because the threshold captures the amount of evidence favoring one hypothesis or the other. Given that the threshold value is fixed prior to, and independent of, the characteristics of a particular trial, this means that confidence will be the same for all trials on which the same hypothesis is chosen 11 .</p><p>Unlike the SPRT, the ABS does not have direct access to its posterior probability that a response is correct (i.e., its confidence). Instead it needs to estimate this probability given a set of samples (see <ref type="figure" target="#fig_0">Figure 2B confidence)</ref>. Fortunately, the form of the adaptive prior on responses (a Beta distribution in the case of binary choice) makes this estimate easy to update as samples are sequentially generated. At the start of the trial, the adaptive prior on responses reflects the prior belief in different probabilities that each response is correct. Using the binary choice example, assume a prior for choice A of Beta( , ) (and a prior for choice B of Beta( , )). When coming to a decision, samples in favor of each response, ( ) and ( ), are sequentially collected until the decision process is terminated by the stopping rule. The confidence after samples (i.e., N= ( ) + ( )) is then</p><formula xml:id="formula_38">Conf 1 = ( ) + + + , in Conf 2 = ( ) + + + , in .<label>(8)</label></formula><p>The max-minus-next heuristic stopping rule terminates the sampling algorithm when the quantity of evidence favoring one choice exceeds a threshold, ∆= | + ( ) − ( + ( ))| &gt; 0. The final decision confidence can then be rewritten as follows:</p><p>Conf 1 = 1 − Conf 2 = + + + ∆ 2( + + ) , if was chosen (9)</p><p>11 While this characterizes an SPRT that stops at the threshold (e.g., as discussed in <ref type="bibr" target="#b129">Pleskac and</ref><ref type="bibr" target="#b129">Busemeyer, 2010, and</ref><ref type="bibr" target="#b187">Vickers, 1979)</ref>, there are variants that would make different predictions. For example, the boundaries do not necessarily have to be symmetric, or they could collapse. Also, it is possible that the SPRT's confidence would reflect not just the threshold but the posterior probability of the all the sensory samples that were observed before stopping, with confidence then being greater than or equal to the threshold confidence. All these variants would produce variable posterior probabilities and hence variable confidence judgments. However, none of these variants would produce estimates, confidence intervals, or other judgments that are generated by the ABS, but which are beyond the scope of the SPRT.</p><p>where the confidence judgments predicted by the ABS are decided by both the threshold values (∆) and the amount of evidence accumulated ( ; this is same as the number of samples generated because evidence is directly mapped from hypothesis samples; for example, a sample of 27 dots will be converted into a piece of evidence for the proposition that the number of dots are greater than 25): the greater the number of samples generated before a decision is reached, the lower the confidence in that decision. This is because the ABS embodies a prior over the strength of signal in the Bayesian Monte Carlo process, and the longer the sampling process continues, the more likely the signal is weak, and so the posterior probability that the decision is correct correspondingly decreases. This is in contrast to the SPRT: in the SPRT, confidence does not change with additional sampling because its confidence is determined by a fixed decision threshold.</p><p>The decreasing decision confidence of the ABS with an increasing number of samples allows it to capture four key empirical phenomena which are not accommodated by the SPRT described above: the positive relationship between confidence and the discriminability of the stimuli <ref type="bibr" target="#b9">(Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b187">Vickers, 1979;</ref><ref type="bibr" target="#b188">Vickers &amp; Packer, 1982,</ref>  <ref type="figure" target="#fig_14">Figure 9A</ref>), the "resolution of confidence" effect <ref type="bibr" target="#b4">(Ariely et al., 2000;</ref><ref type="bibr" target="#b9">Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b187">Vickers, 1979;</ref><ref type="bibr" target="#b61">Garrett, 1922;</ref><ref type="bibr">Vickers, 2014;</ref><ref type="bibr" target="#b188">Vickers &amp; Packer, 1982,</ref>  <ref type="figure" target="#fig_14">Figure 9B</ref>), so-called "metacognitive inefficiency" <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a</ref>; 2021b, <ref type="figure" target="#fig_14">Figure 9C)</ref>, and the complex relationship between RT and confidence <ref type="bibr" target="#b9">(Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b188">Vickers &amp; Packer, 1982,</ref>  <ref type="figure" target="#fig_14">Figure 9D</ref>).  Each dot denotes a level of difficulty. Empirical data adapted from <ref type="bibr" target="#b188">Vickers &amp; Packer (1982)</ref>.</p><p>Error bars denote 95% confidence intervals of the model simulations.</p><p>The first of these effects, the positive relationship between confidence and stimulus discriminability, follows from the strength of the signal in the ABS. More discriminable stimuli will result in more homogenous evidence supporting one alternative (i.e., samples will more consistently support one response alternative over the other), and because decision confidence is a transformation of the proportion of samples that support the chosen response, more discriminable stimuli will on average produce higher confidence judgments (see Equation 8). Conversely, on more difficult trials, the evidence will be more heterogeneous and so the ABS predicts lower average decision confidence. <ref type="figure" target="#fig_14">Figure 9A</ref> shows this qualitative effect arising in ABS model simulations, in which confidence is expressed on a probability scale which is ordinally related to the scale with which the empirical data were collected, and it is produced by all model variants (see <ref type="table" target="#tab_6">Table 2</ref>).</p><p>Second, average confidence ratings tend to be higher for correct responses than for incorrect responses (e.g., <ref type="bibr" target="#b4">Ariely et al., 2000;</ref><ref type="bibr" target="#b9">Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b187">Vickers, 1979;</ref><ref type="bibr">Vickers, 2014;</ref><ref type="bibr" target="#b188">Vickers &amp; Packer, 1982)</ref>. This so-called "resolution-of-confidence" effect also holds true even if stimulus difficulty is held constant <ref type="bibr" target="#b9">(Baranski &amp; Petrusic, 1998)</ref> and even if choice and confidence are simultaneously elicited from participants <ref type="bibr" target="#b139">(Ratcliff &amp; Starns, 2009;</ref><ref type="bibr" target="#b185">Van Zandt, 2000;</ref><ref type="bibr" target="#b90">Kiani et al., 2014)</ref>. Once again, the SPRT cannot properly explain this effect given that its thresholds are fixed prior to, and independently from, the characteristics of particular trials (e.g., it is constant across all trials or randomly drawn from a fixed distribution). However, if we make the assumption that people have the correct generative model of the task (i.e., the probability of generating a sample that supports the correct alternative is the largest among all other alternatives), the ABS predicts that correct responses will on average be made with higher confidence. This is tied to the explanation for slower errors above: autocorrelations cause errors to be slower on average, and slower responses produce lower confidence judgments (see Equation 9). Therefore, the ABS predicts a resolution-of-confidence effect in experimental conditions that produce slow errors (see <ref type="figure" target="#fig_14">Figure 9B</ref>). As this effect requires both optional stopping and autocorrelated samples, as also are required to produce slow errors, only the full model and the no prior variant produce it (see <ref type="table" target="#tab_6">Table 2</ref>).</p><p>Third, studies have shown that the meta-cognitive judgments in confidence ratings generally carry less information about the accuracy of a decision than would be predicted by a purely normative account like the SPRT. Thus, there seems to be a systematic deficit in "metacognitive efficiency" <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a;</ref><ref type="bibr" target="#b157">2021b)</ref>. To give an intuition, imagine a participant is asked to make a decision whether to respond A or B to a stimulus. The participant's ability to discriminate between the alternatives (i.e., . ) can be calculated, based on SDT, by using the percentage of A stimuli that are correctly identified (i.e., hits) and the percentage of B stimuli that are incorrectly identified as A stimuli (i.e., false alarms). This standard . measure can also be extended to metacognition by choosing a confidence criterion and recalculating the hit and false alarm rates from confidence judgments that exceed this criterion to produce a meta_d' 12 . SDT predicts that . equals meta_d' for any confidence criterion and so predicts that metacognitive judgments are always efficient (while 12 Informativeness of choices and confidence ratings are measured as stimulus sensitivity d' and meta_d' respectively <ref type="bibr" target="#b112">(Maniscalco &amp; Lau, 2012;</ref><ref type="bibr" target="#b56">Fleming &amp; Lau, 2014)</ref>. More specifically, * = +$ (hit rate) − +$ (false alarm rate) where +$ is the inverse of the cumulative Gaussian distribution. meta_d' is calculated in the same manner but with the hit rate and the false alarm rate tallied according to a criterion value that partitions confidence ratings. More specifically, the hit rate is the proportion of trials in which participants reported high confidence given a correct response, whereas the false alarm rate is the proportion of trials in which participants reported high confidence given an incorrect response; and the confidence criterion value determines whether a confidence judgment is considered high or low.</p><p>the SPRT predicts constant confidence judgments and so cannot be evaluated using this measure). By contrast, a value of _ ′/ ′ &lt; 1 would indicate that information available for the decision is lost in part of in whole when making confidence judgments. Empirically, metacognition has been found to be inefficient, and moreover meta_d' decreases relative to</p><p>. as the confidence criterion increases, meaning that higher confidence ratings are less informative than lower confidence ratings <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a;</ref><ref type="bibr" target="#b157">2021b)</ref>. Metacognitive inefficiency has been explained by adding additional noise to confidence judgments <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a)</ref>.</p><p>While it would be straightforward to add noise to the ABS confidence judgments, surprisingly this additional noise is not necessary to produce such metacognitive inefficiency;</p><p>in fact, there are multiple routes for the ABS to produce this effect already offered in the current specification. A first route derives from more informed decisions being overall less confident decisions. For example, imagine using a stopping rule with ∆= 2 and a symmetric Beta(1,1) prior on responses. If a decision is made based on only a total of 2 samples, then both will have to be in favor of the chosen response and confidence will be 75% (i.e.,</p><p>plugging these values in Equation 8:</p><p>"+! "+" = 75%). However, if a decision is made based on a total of 100 samples then only 51 can have supported the chosen alternative (because the stopping rule requires ∆= 51 − 49 = 2) and confidence will be about 51% (i.e., plugging these values in Equation 8:</p><p>3!+! !((+" ≈ 51%). Thus, with optional stopping, lower confidence decisions will be based on more samples (and so have longer RTs) and hence will be more informative (see Equation 9). A second route derives from basing confidence judgments on discrete samples of hypothesis counts rather than the Gaussian distributed sensory evidence assumed by SDT; this applies even if samples are independent, a fixed number of samples are generated, and no prior is used (see Appendix D). Therefore, the ABS predicts decreasing metacognitive efficiency for more extreme confidence judgments not only for the full model (see <ref type="figure" target="#fig_14">Figure 9C</ref>) but also for all its variants (see <ref type="table" target="#tab_6">Table 2</ref>).</p><p>Finally, confidence is empirically observed to systematically vary with RTs, with positive (cross-condition) and negative (cross-trial) relationships between confidence and RTs (see <ref type="figure" target="#fig_14">Figure 9D)</ref>. When people are forced to respond more quickly in a particular experimental condition, their confidence reduces, which is in line with the standard speed-accuracy tradeoff, assuming the confidence positively co-varies with accuracy <ref type="bibr" target="#b188">(Vickers &amp; Packer, 1982;</ref><ref type="bibr" target="#b81">Irwin et al., 1956)</ref>. Both the SPRT and the ABS can capture the positive (cross-condition) relationship simply by varying the threshold according to experimental conditions:</p><p>emphasizing accuracy moves the threshold further away from the starting point of the accumulator (and the opposite is true for the speed condition). Higher threshold values in the SPRT lead to more extreme final log odds and therefore higher confidence readouts. Higher threshold values in the ABS (i.e., larger ∆) naturally lead to higher confidence as shown in Equation 9.</p><p>However, within a condition, people are more confident in decisions they reach quickly -intuitively, the "easy" trials are decided quickly and with high confidence <ref type="bibr" target="#b9">(Baranski &amp; Petrusic, 1998;</ref><ref type="bibr">Vickers &amp; Packers, 1982)</ref>. Crucially, the SPRT cannot explain this because the strength of evidence at which a decision is made depends only on the threshold, which is determined prior to, and hence independently from, the characteristics of any particular trial.</p><p>The ABS can explain this negative (cross-trial) relationship because earlier termination (for a fixed threshold ∆) implies that there will be a higher proportion of evidence supporting the chosen alternative. As a result, the ABS predicts that within a condition, faster decisions will be given with higher confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Intervals</head><p>So far, we have considered confidence in decisions. But confidence reports can also be elicited for estimates by asking for confidence intervals. Commonly a participant is given a probability first and then asked to produce an interval (by giving upper and lower bounds) that correspond to the probability (e.g., 'give the smallest interval which you are 60% certain to include the number of dots which appeared onscreen: between ____ and ____ dots').</p><p>However, this procedure can also be reversed: participants can be shown an interval of some quantity of interest and then asked to evaluate the probability of that interval (e.g., 'what is the probability that the number of dots which appeared onscreen falls in the range of 23 to 25?'; <ref type="bibr" target="#b85">Juslin &amp; Persson, 2002)</ref>.</p><p>In the ABS, confidence interval production and evaluation both are driven by very similar mechanisms to those underlying the naïve intuitive statistician model of <ref type="bibr" target="#b86">Juslin, Winman, &amp; Hansson (2007)</ref>. Taking a set of samples, a confidence interval can be produced by using the lower and upper bounds of the sample coverage (i.e., empirical quantiles of the samples) as the lower and upper bounds of the confidence interval. When the values of the quantiles are not explicitly represented in the sample (e.g., deriving a 93% CI based on 5 samples), linear interpolation was assumed to fill in the gap between the samples <ref type="bibr" target="#b86">(Juslin et al., 2007)</ref>. This mechanism correctly predicts the considerable overconfidence in interval production found empirically (see <ref type="figure">Figure 10A</ref> dots; <ref type="bibr" target="#b87">Juslin et al., 2003;</ref><ref type="bibr" target="#b86">Juslin et al., 2007)</ref>. This is because for small sample sizes, the empirical quantile of the sample will have a shorter range than the confidence interval from the posterior because distributional tails tend to be underrepresented within a few samples. Therefore, the proportion generated from the sample will be too small, producing an overconfident interval in our simulations (see <ref type="figure">Figure   10B</ref> dots). One might then question why interval production overconfidence is not corrected in the same manner described for probability judgments above where useful prior knowledge is incorporated -this lack of correction for confidence interval production is what was "naïve" about the naïve intuitive statistician model. Corrections for intervals, however, depend on the functional form of the distribution, so that a general correction process is difficult to establish in the ABS. While the standard computation of a confidence interval assumes a Gaussian distribution, for unknown distributions confidence intervals are usually produced by bootstrapping. For the purposes of producing the confidence interval for a sample, as opposed to producing the confidence interval for a mean, bootstrapping is essentially what the ABS does.</p><p>In contrast to confidence interval production, confidence interval evaluation shows very different empirical results: here there is little to no overconfidence with only a small degree of conservatism at the extremes of the subjective probability (see <ref type="figure">Figure 10A</ref> squares; <ref type="bibr" target="#b87">Juslin et al., 2003)</ref>. This arises in the ABS (see <ref type="figure">Figure 10B</ref> squares), using the simplest possible assumption (and following <ref type="bibr" target="#b86">Juslin et al., 2007</ref>) that people answer this question by generating samples and calculating the proportion that fall within the provided interval. As noted by <ref type="bibr" target="#b86">Juslin et al., (2007)</ref>, this proportion is an unbiased estimator, and hence shows good</p><formula xml:id="formula_39">calibration 13 .</formula><p>13 There is only slight overconfidence predicted by the naïve intuitive statistican model when the internal generative model does not perfectly describe the data generating process <ref type="bibr" target="#b86">(Juslin et al., 2007)</ref>, and the ABS would show the same effect with an imperfect model of the data generating process. <ref type="figure">Figure 10</ref>. (A) Empirical data for interval evaluation (i.e., probability judgment) and interval production, adapted from <ref type="bibr" target="#b87">Juslin et al. (2003)</ref>. Interval evaluations were relatively well calibrated while substantial overconfidence was observed in interval production. The dashed line illustrates perfect calibration. (B) ABS predictions of confidence interval production and evaluation: strong overconfidence in interval production (blue dots) and no overconfidence in interval evaluation (red squares). The horizontal axis indicates either the requested interval coverage (production) or the judged probability of the interval (evaluation), while the vertical axis indicates the empirical proportion of events covered by the interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decisions Affecting Later Estimates</head><p>Besides eliciting confidence judgments after choices, experimenters have often asked participants to provide separate secondary responses to the same stimulus. One example is the decision-estimation task where people were asked to first choose, say, whether the number of dots which appeared on-screen was greater or smaller than 25, and then are asked, immediately following the choice, to estimate the number of dots. In this setting, an estimate can be influenced by the preceding choice (e.g., <ref type="bibr" target="#b179">Tversky &amp; Kahneman, 1974;</ref><ref type="bibr" target="#b82">Jazayeri &amp; Movshon, 2007)</ref>. In cognitive judgments, estimate have often been observed to be pulled towards a preceding arbitrary value-a phenomenon better known as the anchoring bias <ref type="bibr" target="#b179">(Tversky &amp; Kahneman, 1974;</ref><ref type="bibr" target="#b45">Epley &amp; Gilovich, 2006)</ref>. For example, in the famous study of <ref type="bibr" target="#b179">Tversky and Kahneman (1974)</ref>, participants were first asked to choose whether the percentage of African countries in the United Nations was higher or lower than a value (ℎ * ),</p><p>and then give an estimate of that percentage. The comparison value used in the choice, ℎ * , was generated randomly and so should have been irrelevant to the distribution of hypotheses (and thus irrelevant to the estimate too), but estimates were biased toward ℎ * .</p><p>However, in an almost identical paradigm of decision-estimation tasks, perceptual judgments showed the opposite effect: estimates of aspects such as dot orientation or direction of motion have been observed to be pushed away from ℎ * <ref type="bibr" target="#b82">(Jazayeri &amp; Movshon, 2007;</ref><ref type="bibr" target="#b198">Zamboni et al., 2016;</ref><ref type="bibr" target="#b109">Luu &amp; Stocker, 2018)</ref>. The phenomenon is better known as the repulsion effect found in perceptual tasks.</p><p>Existing models of anchoring cannot predict the repulsion effect and vice versa. This is because they only predict one direction of bias (e.g., <ref type="bibr" target="#b179">Tversky &amp; Kahneman, 1974;</ref><ref type="bibr" target="#b167">Strack &amp; Mussweiler, 1997;</ref><ref type="bibr" target="#b82">Jazayeri &amp; Movshon, 2007;</ref><ref type="bibr" target="#b109">Luu &amp; Stocker, 2018)</ref>, and thus fail to capture the co-occurrence of anchoring and repulsion. While this would be tenable if anchoring and repulsion were each specific to their respective (cognitive or perceptual) domains, a recent empirical investigation suggests otherwise <ref type="bibr" target="#b163">(Spicer et al., 2022)</ref>. In this work, we noted that the location of the comparison value, ℎ * , relative to the distribution of hypotheses has not typically been the same across cognitive and perceptual paradigms. Indeed, it was found empirically that the relative location of ℎ * determines whether the subsequent estimates will be pulled toward or pushed away in both cognitive and perceptual tasks. Specifically, estimates are drawn towards distant values of ℎ * (replicating the anchoring effect) but pushed away from nearby values of ℎ * (replicating the repulsion effect; <ref type="bibr" target="#b163">Spicer et al., 2022)</ref>. This finding is consistent with a common general-purpose algorithm underlying decision-making in both cognition and perception.</p><p>The anchoring effect, the repulsion effect, and their dependence on the relative position of ℎ * are captured by the ABS assuming that the set of samples generated to make the choice is then reused to produce the estimate. To explain anchoring, the ABS follows the approach of <ref type="bibr" target="#b103">Lieder et al. (2018)</ref> and assumes that local sampling algorithm uses ℎ * as an initial hypothesis. For a small number of iterations, the local sampler will then be biased toward the initial hypothesis. Anchoring is then produced in our simulations for the full model and all variants except the direct sampling variant (see <ref type="figure" target="#fig_16">Figure 11</ref> and <ref type="table" target="#tab_6">Table 2</ref>).</p><p>To explain repulsion, we first note that in the ABS ℎ * effectively partitions the hypothesis space into two binary response regions. The sampling algorithm is adaptively terminated when a sufficient number of samples support one alternative over the other, with the amount determined by the threshold parameter (i.e., ∆). This adaptive stopping rule produces a repulsion bias if the estimate is also based on the same set of samples <ref type="bibr" target="#b204">(Zhu et al., 2019)</ref>, because the sampling process will terminate when the weight of evidence favors one hypothesis rather than when the evidence is finely balanced: In effect, optional stopping for choice biases the subsequent estimate away from indifference (i.e., the decision boundary).</p><p>Thus, repulsion is produced by the full model and all of the variants except for the fixed sample size variant (see <ref type="figure" target="#fig_16">Figure 11</ref> and <ref type="table" target="#tab_6">Table 2</ref>). sampling with means in the range of <ref type="bibr">[21,</ref><ref type="bibr">30]</ref> were shown in black solid lines, which were vertically rescaled by a factor of 1/4 to aid visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-trial Autocorrelations in Estimates and RTs</head><p>Substantial cross-trial autocorrelations are an important, and often unexplained, aspect of human behavior. For example, long-range dependencies in estimates and in RTs known as 1/f noise have been observed in many cognitive and perceptual tasks and can explain more variance in behavior than the experimental manipulations <ref type="bibr" target="#b67">(Gilden et al., 1995;</ref><ref type="bibr" target="#b66">Gilden, 2001;</ref><ref type="bibr" target="#b191">Wagenmakers et al., 2004)</ref> 14 . In these tasks, participants were instructed to repeatedly</p><p>14 1/f noise goes under various names in different literatures (e.g., pink noise and flicker noise). Besides cognition, diverse complex processes have also been found to exhibit this type of estimate fixed physical quantities (e.g., a 1-sec temporal interval or a 1-inch spatial interval) or to repeatedly choose between two options. The statistical features of the time series produced by participants were analyzed in the frequency domain, with the high-frequency components corresponding to trials that are close together, whereas low-frequency components correspond to trials that are well separated. The power of each of these components for explaining the time series are then calculated (i.e., in a spectral density analysis; <ref type="bibr" target="#b159">Sheu &amp; Ratcliff, 1995;</ref><ref type="bibr" target="#b67">Gilden et al., 1995;</ref><ref type="bibr" target="#b66">Gilden, 2001</ref>). Standard statistical processes show different relationships between frequency and power: in a random walk the power falls off with 1/f 2 noise (i.e., a slope of -2 in log-log power spectra), whereas white noise (also called independent sampling or direct sampling) has a flat power spectrum (i.e.,</p><p>1/f 0 noise and a slope of 0). In a time-series containing long-range serial dependence, as has been found in human data, power spectra typically have a slope between -1.5 and -0.5, and are thus categorized as 1/f noise. The long-range autocorrelations in 1/f noise are not straightforward to produce, generally requiring complex processes to do so <ref type="bibr" target="#b60">(Gardner, 1978)</ref>.</p><p>Further complicating the picture, while time-series of estimates have long-range autocorrelations that are classed as 1/f noise <ref type="bibr" target="#b67">(Gilden et al., 1995;</ref><ref type="bibr" target="#b66">Gilden, 2001;</ref><ref type="bibr" target="#b191">Wagenmakers et al., 2004;</ref><ref type="bibr" target="#b205">Zhu et al., 2021)</ref>, RT time series fluctuate as 1/f noise but with a log-log slope that is shallower than that of estimates <ref type="bibr" target="#b182">(Van Orden et al., 2003;</ref><ref type="bibr" target="#b191">Wagenmakers et al., 2004)</ref>.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, the ABS qualitatively reproduces the observed autocorrelations in time series of RTs and estimates. The cross-trial autocorrelation in estimates is predicted by the cross-trial carryover of the sampler's location in the autocorrelated MC 3 algorithm <ref type="bibr" target="#b201">(Zhu et al., 2018;</ref><ref type="bibr" target="#b200">Zhu et al., 2022c)</ref>: the initial location of the sampler for the present trial is the long-range dependence. For example, MEG and EEG data from human brains, and indeed classical music, all exhibit 1/f-like fluctuations <ref type="bibr" target="#b122">(Novikov et al., 1997;</ref><ref type="bibr" target="#b106">Linkenkaer-Hansen et al., 2001)</ref> last sample for the preceding trial. In comparison, the RT time series is predicted to be less autocorrelated because samples generated by the MC 3 are accumulated to a threshold to produce the RT; this is a non-linear transformation of autocorrelated samples which "whitens" the power spectrum. Simulations of the full model demonstrate both these effects, and as it is driven by the MC 3 algorithm it occurs for all variants except for the direct sampling variant (see <ref type="figure" target="#fig_0">Figure 12</ref> and <ref type="table" target="#tab_6">Table 2</ref>). predicts independent estimates and RTs and thus exhibits a flat line (i.e., the power spectrum of white noise). (Right) The ABS model predicts autocorrelations in estimates and RTs with the latter displaying flatter slopes than the former (i.e., the power spectra of 1/f noise). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Using a consistent set of parameter values, we have shown that the ABS qualitatively captures empirical results and relationships observed across probability judgments, estimates, confidence intervals, choices, confidence judgments, and response times (see <ref type="table" target="#tab_6">Table 2</ref>). The wide range of predicted behaviors is based on an internal probabilistic model using a finegrained set of hypotheses. The process of inferring the posterior probability of the hypotheses is governed by Bayes' rule and approximated using an autocorrelated sampling algorithm.</p><p>While the autocorrelation in the sampling algorithm is motivated as making the sampling process computationally efficient, it is crucial for explaining many empirical effects such as slow errors, anchoring, and cross-trial autocorrelations. Assuming each sample generated is costly, turning these samples into choices relies on an optional stopping rule that trades the benefits of larger samples against the cost of sampling. In turn, the optional stopping rule helps explain empirical effects such as the repulsion effect, the resolution of confidence, and metacognitive inefficiency. The probabilistic model also learns from trial history, using the adaptive prior. This prior helps explain effects such as conservatism, the conjunction fallacy, partition dependence, and fast errors. In sum, this rational process is fruitful for understanding a wide assortment of human behavior. </p><formula xml:id="formula_40">N/A ✓ ✗ ✓ ✓ ✓ Mean-variance relationship N/A ✓ ✗ ✓ ✓ ✓ Explicit subadditivity N/A ✓ ✗ ✓ ✓ ✓ Conjunction fallacy N/A ✓ ✗ ✓ ✓ ✓ Implicit subadditivity in typical unpacking N/A ✗ ✓ ✗ ✓ ✓ Implicit superadditivity in atypical unpacking N/A ✗ ✓ ✗ ✓ ✓ Partition dependence N/A ✓ ✗ ✓ ✓ ✓ Decisions Affecting Later Responses Anchoring N/A N/A ✓ ✗ ✓ ✓ Repulsion N/A N/A ✓ ✓ ✗ ✓ Accuracy and Response Times Speed-accuracy trade- off ✓ N/A ✓ ✓ ✓ ✓ Slow errors ✗ N/A ✓ ✗ ✗ ✓ Fast errors ✗ N/A ✗ ✓ ✗ ✓ Near-linear relationship of RT quantiles with a fan shape ✓ N/A ✓ ✓ ✗ ✓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence in Decisions</head><p>Positive relationship between confidence and stimulus discriminability </p><formula xml:id="formula_41">✓ N/A ✓ ✓ ✓ ✓ Resolution of confidence ✗ N/A ✓ ✗ ✗ ✓ Metacognitive inefficiency ✗ N/A ✓ ✓ ✓ ✓</formula><formula xml:id="formula_42">✗ N/A ✓ ✗ ✗ ✓</formula><p>Note. N/A denotes that the model is non-applicable to the empirical effect because it does not produce the relevant behavioral measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Competing Models</head><p>There are many models that can produce at least a subset of the empirical effects that the ABS does, and many were briefly mentioned in the text above. Here we compare the ABS first to other models of probability judgments and then to drift-diffusion models of choice, response time, and confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models of Probability Judgments</head><p>Intensive modeling efforts have also been directed at explaining human probabilistic reasoning, spurred on by the identification of biases, particularly those summarized in <ref type="table" target="#tab_1">Table   1</ref>, demonstrating that people's judgments do not conform with the rules of probability theory (e.g., <ref type="bibr" target="#b128">Peterson &amp; Beach, 1967;</ref><ref type="bibr" target="#b124">Nilsson et al., 2009;</ref><ref type="bibr" target="#b28">Costello &amp; Watts, 2014;</ref><ref type="bibr" target="#b33">Dasgupta et al., 2017;</ref><ref type="bibr" target="#b203">Zhu et al., 2020;</ref><ref type="bibr" target="#b180">Tversky &amp; Koehler, 1994;</ref><ref type="bibr" target="#b78">Hilbert, 2012)</ref>. Many models have assumed that probability judgments follow a deterministic process, albeit one that violates the rules of probability theory. For example, one type of model, geared toward accounting for conjunction fallacies, assumes that probability estimates of conjunctions are the weighted average of the probabilities of their constituent events, which produces above-chance conjunction fallacy rates and can reproduce several probabilistic identities <ref type="bibr" target="#b47">(Fantino, Kulik, Stolarz-Fantino, &amp; Wright, 1997;</ref><ref type="bibr" target="#b124">Nilsson, Winman, Juslin, &amp; Hansson, 2009;</ref><ref type="bibr" target="#b123">Nilsson, Juslin, &amp; Winman, 2016)</ref>. However, these models require additional mechanisms to match the empirically observed combination of above-chance and below-chance rates of conjunction fallacies that the ABS can produce <ref type="bibr" target="#b124">(Nilsson et al., 2009;</ref><ref type="bibr" target="#b54">Fisk &amp; Pidgeon, 1996)</ref>.</p><p>A different type of deterministic approach that has been used to explain conjunction fallacies is based on quantum probability. Here probabilities are based on projections of event subspaces. If the events are compatible, probability judgments are indistinguishable from classical probability theory, but if the events are incompatible then interference produces probability judgments that deviate from classical probability theory. These deviations are such conjunction and disjunction fallacies will occur at rates above chance, and in this way, quantum probability can produce both above-chance and below-chance conjunction fallacies depending on how the events are represented <ref type="bibr" target="#b21">(Busemeyer et al., 2011)</ref>. Quantum probability has explained a wide range of probabilistic biases, including some not covered here <ref type="bibr" target="#b20">(Pothos &amp; Busemeyer, 2022)</ref>. However, there are also probabilistic identities that quantum probability cannot reproduce, that are predicted by sampling-based models, including the ABS <ref type="bibr" target="#b203">Zhu et al., 2020)</ref>.</p><p>A third deterministic approach is support theory, which was developed to explain subadditivity biases. The core assumption of support theory is that the probability of event descriptions is evaluated rather than the probability of the events themselves and does not incorporate the probabilities of events that are not immediately available (e.g., those not mentioned in the descriptor of events). This approach elegantly explains both a range of implicit subadditivity results, as well as explaining why subadditivity does not occur for mutually exclusive binary events. However, it does not produce the later finding that an atypical unpacking of events produces implicit superadditivity <ref type="bibr" target="#b160">(Sloman et al., 2004)</ref>, and requires additional mechanism such as an "ignorance prior" which pulls probability judgments towards indifference between the available responses <ref type="bibr" target="#b58">(Fox &amp; Rottenstreich, 2003)</ref>.</p><p>These different mechanisms have echoes in the ABS. In the ABS, a hypothesis is "available" only if it has been sampled, and the event description influences the starting point of the local sampler. Further, the prior on responses is a principled version of the ignorance prior, one that is uncertain about the underlying probabilities because often only a small number of samples is available.</p><p>Recent approaches have rejected purely deterministic approaches and explored the alternative possibility that stochastic mechanisms explain the biases in probability judgments.</p><p>For example, simple unbiased response noise has been shown to produce subadditivity <ref type="bibr" target="#b11">(Bearden, Wallsten, &amp; Fox, 2007;</ref><ref type="bibr" target="#b15">Brenner, 2003)</ref>. However, unbiased response noise alone does not explain why subadditivity still occurs for median judgments. A more promising alternative is to consider corruptive noise in memory or evidence accumulation, which can produce stronger biases (e.g., <ref type="bibr" target="#b28">Costello &amp; Watts, 2014;</ref><ref type="bibr" target="#b46">Erev et al., 1994;</ref><ref type="bibr" target="#b78">Hilbert, 2012)</ref>. In a leading stochastic model, Probability Theory plus Noise (PT+N), people are assumed to first draw independent samples from a probabilistic representation, and unbiased "counting noise"</p><p>is added to individual samples to reflect an error-prone cognitive system <ref type="bibr" target="#b28">(Costello &amp; Watts, 2014</ref>). This counting noise pulls probability judgments towards indifference and allows the PT+N to capture empirical results such as explicit subadditivity, the conjunction fallacy, and a wide range of probabilistic identities <ref type="bibr" target="#b28">(Costello &amp; Watts, 2014;</ref>. The PT+N has impressive empirical coverages, although it has recently been argued that it does not fully reproduce all the mean-variance relationship in probability judgments <ref type="bibr" target="#b168">(Sundh et al., 2021)</ref>: while it will produce the inverted U-shaped relationship between the mean and variance of judgments, the curve will not be pulled inward as is with the empirical data and as the ABS predicts. This mean-variance relationship also stands as a challenge to deterministic models because it is not easily produced by simply adding response noise to a deterministic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drift-Diffusion Models</head><p>One family of models that deserves more extensive discussion is the family of Drift-Diffusion Models (DDMs; <ref type="bibr" target="#b141">Ratcliff et al., 2016;</ref><ref type="bibr" target="#b69">Gold &amp; Shadlen, 2007;</ref><ref type="bibr" target="#b92">Krajbich &amp; Rangel, 2011;</ref>. While there are many members of this family, they all describe decision making as a stochastic process similar to that of a biased random walk (or a biased diffusion process, in continuous time) in which the path of the accumulator is, on average, biased by the drift rate <ref type="bibr" target="#b136">(Ratcliff &amp; McKoon, 2008;</ref><ref type="bibr" target="#b13">Bogacz et al., 2006)</ref>. For binary choices, this is determined by the difference in the evidence signals supporting for the two alternatives. In line with the stopping rule of the SPRT, the threshold reached in the DDM decides the choice and the time taken to do so the response time. However, unlike the static summation process of log-likelihood ratios in the SPRT, the accumulator in the DDM also diffuses because the accumulator is corrupted by noise, typically white noise. In perceptual tasks, the drift rate is related to which choice is objectively correct <ref type="bibr" target="#b136">(Ratcliff &amp; McKoon, 2008</ref>), whereas in high-level cognitive tasks where people are choosing their preferred option the drift rate is assumed to be related to the relative appeal of the alternatives <ref type="bibr" target="#b93">(Krajbich, Armel, &amp; Rangel, 2010;</ref><ref type="bibr" target="#b92">Krajbich &amp; Rangel, 2011</ref>).</p><p>There are generally strong theoretical links between these models and the normative framework of the SPRT: in the continuous limit, the SPRT converges on the DDM and the drift rate and the corruptive noise in the DDM can jointly mimic calculation of likelihood ratios in the SPRT <ref type="bibr" target="#b13">(Bogacz et al., 2006)</ref>. However, implementing an optimal statistical decision test in the form of the DDM also generates a number of useful theoretical and empirical insights that were not originally part of the SPRT. First, the psychologically implausible assumption that people are required to have global knowledge of the generative model of the task to calculate the exact likelihoods (e.g., ( $ | ) or ( $ | )) is implicitly relaxed by the DDM because the drift rate and diffusion noise are free parameters that are recovered from fitting to behavioral data. Thus, the DDM does not need to calculate with the exact cumulative differences in evidence as supposed by the SPRT, greatly improving the DDM's computational plausibility, given that the exact likelihood ratios are almost always impractical to compute in real time except in simple toy problems.</p><p>Second, extensions of the DDM can also account for empirical features such as those noted above which are not accounted for by the SPRT. For example, slow and fast errors <ref type="bibr" target="#b137">(Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b176">Townsend &amp; Ashby, 1983)</ref> can be produced by further assuming that model parameters are variable across trials <ref type="bibr" target="#b100">(Laming, 1968;</ref><ref type="bibr" target="#b150">Rouder, 1996;</ref><ref type="bibr" target="#b134">Ratcliff, 1981;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998)</ref>. In particular, varying the drift rates trial-by-trial generates slow errors, while varying the starting point of the accumulator predicts fast errors <ref type="bibr" target="#b100">(Laming, 1968;</ref><ref type="bibr" target="#b137">Ratcliff &amp; Rouder, 1998</ref>). The ABS model works in a similar fashion, as autocorrelation acts like variability in drift rates and a biased prior of evidence acts like variability in the starting point of the accumulator.</p><p>Moreover, the benefits of using the DDM instead of the SPRT also apply to explaining confidence judgments. The SPRT predicts the confidence ratings to be identical between correct and incorrect responses because, for a fixed symmetric threshold, there will always be the same level of evidence difference accumulated favoring the selected option, 15 and the probability that the chosen option is correct is simply read out from the final state of the accumulator. As outlined above, such predictions are contradicted by empirical data in which choice accuracy and confidence are positively related <ref type="bibr" target="#b9">(Baranski &amp; Petrusic, 1998;</ref><ref type="bibr" target="#b37">Dougherty, 2001;</ref><ref type="bibr" target="#b187">Vickers, 1979;</ref><ref type="bibr" target="#b196">Yeung &amp; Summerfield, 2014)</ref>. To reconcile the confidence data with the SPRT, one kind of DDM introduced another assumption in which the same drift-diffusion process continues to run for a period of times after the choice has been made</p><p>15 Regardless of trial-by-trial variability in drift-rates and/or starting points.</p><p>but before the confidence judgments <ref type="bibr" target="#b129">(Pleskac &amp; Busemeyer, 2010)</ref>. As the accumulator has a bias toward the correct choice, this continued accumulation after the choice and before the confidence judgment, no longer bounded by the fixed threshold of the decision, will drive the confidence ratings towards supporting the correct choice. Hence, with this additional assumption, this DDM can correctly predict that people should report higher confidence in correct responses than in errors, and moreover that the resolution of confidence effect grows with the delay between choosing and reporting confidence. However, since the temporal structure supposed by this assumption is that confidence occurs after the choice, this DDM cannot explain why the relationship between choice accuracy and confidence also appears when confidence judgments are given simultaneously with a decision (e.g., <ref type="bibr" target="#b102">Li &amp; Ma, 2020;</ref><ref type="bibr" target="#b90">Kiani et al., 2014)</ref>. In explaining these data, researchers have assumed that confidence decreases with response time <ref type="bibr" target="#b90">(Kiani et al., 2014;</ref><ref type="bibr" target="#b23">Calder-Travis et al., 2020)</ref>, and as occurs in the ABS.</p><p>Alternative versions of the DDM, such as the RTCON model, have been developed to capture no-choice confidence rating <ref type="bibr">(Ratcliff &amp; Starn, 2009)</ref>. RTCON assumes that each confidence rating has an independent diffusion process and the first diffusion process to reach the threshold determines the confidence rating. So, for 7 confidence ratings, there are 7 diffusion processes racing to the threshold. RTCON captures many key empirical relationships between confidence and RT <ref type="bibr">(Ratcliff &amp; Starn, 2009</ref>), but appears to have difficulty capturing the interaction between confidence and choice because the model was not initially designed for tasks involving both choices and confidence judgments <ref type="bibr" target="#b129">(Pleskac &amp; Busemeyer, 2010)</ref>. The approach of RTCON has been generalized to continuous-response paradigms in which there are an infinite number of potential responses: the circular diffusion model <ref type="bibr" target="#b162">(Smith, 2016)</ref> and the spatially continuous diffusion model <ref type="bibr" target="#b135">(Ratcliff, 2018)</ref>. The two models have recently been integrated into a unified framework where geometric similarity among response options is represented <ref type="bibr" target="#b96">(Kvam &amp; Turner, 2021)</ref>.</p><p>Overall, the family of models encapsulated by the DDM effectively describe behavior even in tasks far different from the perceptual tasks for which it was initially developed <ref type="bibr" target="#b141">(Ratcliff et al., 2016;</ref><ref type="bibr" target="#b13">Bogacz et al., 2006)</ref>, including tasks in which there is little to no perceptual noise, such as value-based decisions <ref type="bibr" target="#b22">(Busemeyer &amp; Townsend, 1993;</ref><ref type="bibr" target="#b181">Usher &amp; McClelland, 2004;</ref><ref type="bibr" target="#b117">Milosavljevic et al., 2010)</ref> and recognition memory tasks <ref type="bibr" target="#b164">(Starns &amp; Ratcliff, 2014;</ref><ref type="bibr" target="#b140">Ratcliff, Gomez, &amp; McKoon, 2004;</ref><ref type="bibr" target="#b143">Ratcliff, Thapar, &amp; McKoon, 2011</ref>).</p><p>Because of its strong normative underpinnings in the SPRT, and its psychologically plausible assumptions, the DDM has been widely used in psychology, economics, and neuroscience <ref type="bibr" target="#b141">(Ratcliff et al., 2016;</ref><ref type="bibr" target="#b48">Fehr &amp; Rangel, 2011;</ref><ref type="bibr" target="#b138">Ratcliff &amp; Smith, 2015)</ref>. Indeed, the DDM has become the default framework in decision-making research.</p><p>That being said, the broader scope of behavioral responses including choice, RT, confidence, and estimates captured by the ABS have not yet been united within a single implementation of the DDM. This is partly because the task representation needed for choice is different than that needed for estimates or confidence intervals (in particular, the DDM represents an accumulated value -a summary statistics of the sample -but does not retain the sample itself). Thus, while the ABS and the DDM share similar descriptive capabilities, the ABS arguably has the advantage in terms of parsimony given the breadth of behavior covered within its single framework, capturing relationships, such as the combination of anchoring and repulsion effects described above, not explained by current DDM approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Toward Complete Task-Specific Models</head><p>The ABS so far has explained the generic judgment and decision-making process. But how might this be applied to specific tasks? Fortunately, we can relate our approach to existing models in the literature that take quite a similar approach to that taken here, and indeed have helped inspire our work. Here we describe two successful existing models, <ref type="bibr" target="#b121">Nosofsky &amp; Palmeri's (1997)</ref> exemplar-based random-walk (EBRW) model and <ref type="bibr" target="#b12">Blurton et al.'s (2020)</ref> visual attention model, and we outline how using the ABS would involve only minor modifications (such as adding autocorrelations) to them. The result is that we can view these existing models as complete task-specific models in the ABS framework.</p><p>The EBRW operates in a hypothesis space of exemplars which represent categories <ref type="bibr" target="#b121">(Nosofsky &amp; Palmeri, 1997)</ref>. This representational assumption is inherited from the generalized context model where each individual exemplar is situated as a point in a multidimensional psychological space, and similarity between exemplars decreases with the distance between points in the space <ref type="bibr" target="#b119">(Nosofsky, 1984;</ref><ref type="bibr" target="#b158">Shepard, 1987)</ref>. Building on this representation, EBRW further assumes that exemplars are retrieved sequentially as in a random walk process, predicting the time course of categorization and recognition decisionmaking <ref type="bibr" target="#b121">(Nosofsky &amp; Palmeri, 1997)</ref>. Similar ideas can be found in the Poisson random walk model of visual attention <ref type="bibr" target="#b12">(Blurton et al., 2020;</ref><ref type="bibr" target="#b19">Bundesen, 1990</ref>). In this model, a series of tentative categories (i.e., hypotheses) is proposed and accumulated until one category has accrued enough samples more than any other category <ref type="bibr" target="#b12">(Blurton et al., 2020)</ref>. The generation of tentative categories is governed by the theory of visual attention <ref type="bibr" target="#b19">(Bundesen, 1990)</ref>.</p><p>Across the two models, there is a common mechanism that integrates over hypotheses for response selection. While neither model was originally motivated from normative principles, they can both be seen as special cases within our framework in which decisions are driven by samples of hypotheses (see Appendix B for detailed comparisons), but with the addition of features such as autocorrelated samples. More specifically, the exemplar-based representation of hypotheses of the EBRW can be adopted by the ABS when modeling categorization tasks, suggesting how the ABS could be applied to explain the effects of similarity and practice in categorization and RT, which have been captured by the EBRW.</p><p>When combined with a Bayesian theory of visual attention, the ABS could also be generalized to account for human eye movement and object localization. In addition, the correspondence between the Poisson random walk and the direct sampling variant of the ABS provides a bridging condition that allows the ABS to account for detailed fits in response time distributions. Furthermore, adding autocorrelations and an adaptive prior to both the EBRW and Poisson random walk models generalizes these models to capture a wider range of empirical effects such as those found in confidence judgments. This demonstrates how the ABS can be adapted and work well in specialized tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The Autocorrelated Bayesian Sampler is a step towards a unified rational process of human behavior. Through our analysis, we have identified two key ideas that are necessary for such a unified framework: probabilistic models and approximate inference. Approximate inference via accumulating hypothesis samples means that the ABS views response time in a different way from most existing approaches, as primarily determined by time to mentally sample hypotheses, rather than accumulating more sensory data. We discuss this further below, pointing to possible reconciliations on the view of time as well as possible reconciliations between diffusion processes (and noisy probability judgment models) and the ABS. Next, we explore how the ABS could be extended both to multi-alternative tasks and how complex probabilistic representations could be incorporated. Finally, we discuss and comment on the extent to which the ABS has rational, Bayesian, basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrasting Views on the Role of Time</head><p>The existing dominant view on the role of time in decision making is to collect and integrate sensory evidence. In the binary choice example, the likelihood ratio between the two alternatives is represented exactly at any moment and the odds of the correct response are constantly updated in light of new sensory evidence or newly retrieved memories. This view has been adopted by many models including SDT, the SPRT, and by and large DDM approaches.</p><p>By contrast, the ABS takes a very different view on the role of time because it sees perception and cognition as emerging from probabilistic representations and computations.</p><p>Instead of coding a single value of the sensory input, people are assumed to implicitly encode multiple values of the sensory input with their subjective uncertainty about those values. But this posterior is difficult to represent exactly for virtually all cognitive tasks; thus, approximation is needed to access it. The passage of time is then viewed as being used for generating more samples from the posterior to refine this approximation. Thus, time matters because it allows the computational process of sampling the posterior to unfold, not because additional sensory data must be accumulated. In the limit, the gradual refinement of the posterior belief should lead to a convergence to the optimal choice.</p><p>These contrasting perspectives were also studied in more detail in Lengyel et al.</p><p>(2015) which presents empirical evidence supporting the view of posterior approximation. It is, however, important to note that the two views on the role of time may not be mutually exclusive. One possible reconciliation could be that the brain first conducts evidence integration, and then a posterior based on the sensory evidence can be approximated with sampling, or these processes could be overlapping. Further analyzing the aspects of the two views on time may be an important topic for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion, Noise, and Sampling</head><p>While we have contrasted other models and the ABS, there are also potential links between these approaches. Considering stochastic models of probability judgments, it may be possible to extend a sampling model with noisy counting, such as the PT+N, to explain vast majority of the effects that we explored above. In our previous work, we have shown that the PT+N and the Bayesian Sampler models can mimic one another's predictions of average probability judgments <ref type="bibr" target="#b203">(Zhu et al., 2020)</ref>. Building on this, it is possible to envision generalizing the PT+N in the same way we generalized the Bayesian Sampler to the ABS.</p><p>First, rather than using independent samples, the PT+N could instead use a local, autocorrelated sampler such as MCMC or MC 3 . Indeed,  have begun to explore this possibility by introducing a type of autocorrelation or carry-over effect allowing earlier samples to influence later probability judgments. Second, when making decisions, PT+N could also use optional stopping rather than a fixed number of samples to account for RT data. This could be a promising alternative to the ABS, although more work is required to develop this rough sketch into a formal model and determine how well it reproduces human data.</p><p>For diffusion models, an interesting starting point is the recent interest in continuous diffusion models <ref type="bibr" target="#b135">(Ratcliff, 2018;</ref><ref type="bibr" target="#b97">Kvam, Marley, &amp; Heathcote, 2022;</ref><ref type="bibr" target="#b94">Kvam, 2019;</ref><ref type="bibr" target="#b95">Kvam &amp; Busemeyer, 2020)</ref>. The main focus of the continuous diffusion models was to describe the cognitive processes underlying tasks that involve continuous responses such as orientation estimation <ref type="bibr" target="#b135">(Ratcliff, 2018)</ref> and pricing <ref type="bibr" target="#b95">(Kvam &amp; Busemeyer, 2020)</ref>. The accumulator is typically depicted in a two-dimensional space. Without any bias in starting point, the accumulator initializes in state [0,0] (i.e., the origin). The amount of evidence accumulated is described as the distance from the origin, and there is a directional bias towards the option that is most favored at that moment. A continuous absorbing threshold (e.g., a semicircle whose center is the origin) defines the space of possible trajectories; when the threshold is reached, the diffusion process is terminated and a response is triggered. The size of the threshold regions that correspond to each response has been adjusted to make some responses more or less likely <ref type="bibr" target="#b96">(Kvam &amp; Turner, 2021)</ref>.</p><p>This kind of mechanism could link a continuous diffusion process to the ABS. As long as the size of the threshold associated with each discrete fine-grained hypothesis is proportional to its posterior probability, then the continuous diffusion process will effectively be sampling from the posterior on fine-grained hypotheses 16 . If the fine-grained hypotheses <ref type="bibr">16</ref> Alternatively, without changing the absorbing threshold, we can adjust the drift rate to mimic a sampling algorithm. The idea is that a diffusing particle that moves with a bias in the direction of higher probability can be linked to local sampling algorithms <ref type="bibr" target="#b147">(Rossky, Doll, &amp; Friedman, 1978;</ref><ref type="bibr" target="#b146">Roberts &amp; Stramer, 2002;</ref><ref type="bibr" target="#b110">Ma et al., 2019)</ref>. Indeed, local samplers move around the space stochastically, which can be described as a diffusion process. Therefore, by carefully designing the biasing method for diffusion, the long-term behaviour will converge to as if drawing samples from a that are sampled are then processed to produce judgements and decisions as in the ABS, this would formally link the models, through substituting a diffusion-based sampling algorithm for the local sampling algorithm currently used. Thus, the distinction between diffusion and sampling may not be clear cut, and they may potentially be viewed as parts of a larger framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-alternative ABS</head><p>Asking people to choose among more than two alternatives is often a useful strategy to test the generalizability of computational models developed to explain binary choice data.</p><p>As previously noted, many aspects of the ABS are readily applicable to such choices: the hypothesis space can be divided into as many regions as required by the query, and the Beta prior generalizes to a Dirichlet distribution when considering more than two options. The optimal stopping rule derived from dynamic programming is, however, more difficult to calculate within realistic times when choosing between more than two alternatives (see Appendix C for detail). As we've noted, the max-minus-next heuristic is again often considered as a good approximation to the optimal stopping rule in multi-alternative choices <ref type="bibr" target="#b38">(Dragalin et al., 1999;</ref><ref type="bibr" target="#b185">2000)</ref> and, indeed, there is computational work suggesting that humans adopt the max-minus-next stopping rule to choose among many alternatives <ref type="bibr" target="#b17">(Brown, Steyvers, &amp; Wagenmakers, 2009)</ref>. This heuristic stopping rule can also explain the bestknown empirical result on the relationship between choice and RT, Hick's Law <ref type="bibr" target="#b17">(Brown et al., 2009)</ref>: that RT increases logarithmically with the number of alternatives <ref type="bibr" target="#b77">(Hick, 1952;</ref><ref type="bibr" target="#b131">Proctor &amp; Schneider, 2018)</ref>.</p><p>distribution. For instance, a class of Lengevin diffusions has been designed such that the stationary distribution of the diffusion is in proportion to the target distribution of MCMC sampling <ref type="bibr" target="#b146">(Roberts &amp; Stramer, 2002;</ref><ref type="bibr" target="#b110">Ma et al., 2019)</ref>.</p><p>Multi-alternative choice tasks with confidence have also recently been used to argue against the Bayesian confidence hypothesis-that confidence in a choice is the posterior probability of that choice. <ref type="bibr" target="#b102">Li and Ma (2020)</ref> found that the best-fitting model for confidence ratings in a three-alternative choice task was not the posterior probability of the chosen option, but the difference between the probability of the chosen option and the probability of second most probable option. This result could be reconciled with the Bayesian confidence hypothesis through the ABS, assuming a stopping rule such as the max-minus-next heuristic.</p><p>More formally, consider a three-alternative choice with options A, B, and C, with respective accumulated evidence , , and . Further assuming that &gt; &gt; , then option A is chosen where − = ∆ following the max-minus-next stopping rule. The confidence difference between the best and the second-best is thus, Diff = Conf 1 − Conf 2 = 5*6 5+6+7 , and the total amount of evidence is related to this difference, + + = 5*6 89:: . Given that, we can rewrite the confidence for the chosen option A as follows:</p><p>Conf 1 = + + = Diff − = Diff ∆ (10) Thus, the best-fitting model in <ref type="bibr" target="#b102">Li &amp; Ma (2020)</ref>, which was used to argue against the Bayesian confidence hypothesis, is proportional to the predictions of the ABS using the max-minusnext stopping rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extending to Complex Representations</head><p>The behaviors discussed here are low-dimensional, with responses being situated within one-or two-dimensional spaces. As a result, the transformation from hypothesis samples to behavior is relatively simple, sometimes just with a linear mapping. But the scope and diversity of human behavior is much broader: many complex human behaviors are both high-dimensional and embedded in hierarchically organized spaces. Drawing, for example, or even copying line-drawings, requires sophisticated mental processes that represent a description of a drawing's parts (e.g., lines and circles) and higher-order relations (e.g., <ref type="bibr">repetition and hierarchy;</ref><ref type="bibr" target="#b184">Van Sommers, 1984;</ref><ref type="bibr" target="#b173">Tian et al., 2020</ref>). The motor system that implements routines and trajectories for turning these rich, structured representations into motor commands to produce drawings is also doing a more complex task than in lowdimensional behavior (for example, complex trajectories may to be segmented into discrete, and hierarchically organized actions). Thus, the hypothesis space from which outputs are</p><p>selected can be open-ended, and hierarchically organized at a range of levels of abstraction.</p><p>While it is difficult to see how sensory accumulation models such as the SPRT or the DDM might extend to such cases, it is at least possible in principle to see how a Bayesian, sampling-based approach might operate. For example, many existing models of cognition, perception, and motor control involve Bayesian inference over compositional symbolic representations (e.g., Lake et al., 2017) -and the relevant computations can only be approximated, often using sampling (frequently, using standard MCMC). An interesting direction for future research is how far these Bayesian models can be mapped into finegrained data relating detailed measures of high-dimensional output (including, for example, accuracy and variability) to fine-grained performance features, such as timing, and autocorrelations across trials. In general, where a Bayesian cognitive model can be defined, a sampling approximation to that model can be created, and compared with detailed process data from experiments. Thus, the ABS provides a possible bridge from simple, but intensively-studied, decisions concerning binary choice or one-dimensional estimation, to models of cognition operating at full scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessing the Rationality of the ABS</head><p>Bayesian models of cognition, pitched at Marr's computational level, combines all available trial information (i.e., prior knowledge of hypotheses, p(h), and the likelihood of the data presented in the trial given a hypothesis, p(h|s)) using the rules of probability theory. In doing so, Bayesian models fully extract trial information with 100% efficiency <ref type="bibr" target="#b199">(Zellner, 2002)</ref>.</p><p>We see the ABS as Bayesian in two ways. In the first way, the ABS can provide a sample-based approximation to an underlying Bayesian representation of a task. Combining the ABS with a Bayesian representation produces a rational process model <ref type="bibr" target="#b71">(Griffiths, Vul, &amp; Sanborn, 2012)</ref>. That is, it is an algorithmic approximation of a computational-level model, which transforms it into a process model. A rational process model does not align perfectly with the underlying computational-level model because sampling approximations inevitably lead to loss of information. As a result, sampling models predict mistakes, systematic biases, and variability in behavior that are due to using stochastic samples.</p><p>While we developed the ABS with a Bayesian perspective in mind, the underlying model does not necessarily have to be Bayesian. The probabilistic representation could be simply the relative frequencies of past events, without an associated model (e.g., <ref type="bibr" target="#b28">Costello &amp; Watts, 2014)</ref>. Alternatively, the probabilistic representation might not even be described as optimal or rational <ref type="bibr" target="#b170">(Tauber, Navarro, Perfors, &amp; Steyvers, 2017)</ref>. All that is required is that the representation can be written as a probability distribution, which covers a wide range of representations -any finite set of non-negative numbers can be normalized to become a probability distribution.</p><p>The second way in which the ABS is Bayesian is that Bayesian Monte Carlo is used to interpret the samples that are generated. This allows the ABS to take advantage of contextfree expectations about the underlying probabilities, which improves probability (and confidence) judgments when a small number of samples give only imprecise information about those probabilities. We assume a conjugate prior on responses, making this process computationally very simple. Our analysis suggests that people do incorporate this prior on responses in forming behaviors-as we have seen, this assumption explains many classical empirical effects in probability judgments. Simple adaptivity in constructing the prior on responses (e.g., adapt to immediate feedback from the preceding trial) has also been shown to be useful in explaining human data such as fast errors.</p><p>This raises the question of whether the ABS makes optimal use of the sampling mechanism it has available. This is a different kind of normative concern than just fully extracting trial information. Here the question is whether the ABS is resource rational <ref type="bibr" target="#b104">(Lieder &amp; Griffiths, 2020;</ref><ref type="bibr" target="#b18">Bhui, Lai, &amp; Gershman, 2021)</ref>. Aside from the Bayesian Monte Carlo prior on responses, which could be argued to be resource rational, there is the smallerscale temporal tradeoff between either generating another sample which takes time or stopping and making do with the samples collected so far. The optimal stopping rule relies on solving a difficult dynamic programming problem (detailed in Appendix C). Indeed, given the fact that the optimal stopping rule is generally computationally intractable, we assumed another approximation to the optimal stopping rule with the max-minus-next rule, which has been independently suggested to well-approximate the performance of optimal stopping rule in the information theory literature <ref type="bibr" target="#b38">(Dragalin et al., 1999)</ref>. This optional stopping rule helps explain empirical patterns such as repulsion, slow errors, and resolution of confidence.</p><p>In short, we argue that exploiting algorithmic approximations to the optimal solution is the key feature of the ABS that justifies it as a rational process model. Our proposal, nonetheless, does not address the meta-level theoretical question concerning how the mind allocates cognitive resources across these approximation algorithms (e.g., the local sampler, the Bayesian Monte Carlo process, and the approximate dynamic-programming solution for stopping). A fully resource rational analysis would further specify the balance between the times spent on each approximation algorithm and the incentives from the task. Whether an optimal allocation of the limited cognitive resources is at play merits future investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We have outlined a rational process model of human behavior: the ABS. The ABS is rooted in a Bayesian framework, where the cognitive system is presumed to have an internal probabilistic model, which describes how the sensory data is generated in the real world. But rather than representing and computing with probabilities, we assume that the cognitive system uses a tractable approximation of the posterior which is realized via a local sampling algorithm. Distinct aspects of these posterior samples are relevant for different types of query and when appropriately transformed, they provide a unified explanation of probability judgments, estimates, confidence intervals, choices, confidence judgments, and the time course of the posterior sampling accounts for response times.</p><p>Our framework shifts the locus of explanations for the accumulation of sensory input to computation (through sampling) over internal hypotheses. Thus, in our framework, the time-course, and variability, of behavior is primarily explained in terms of an internal, noisy, computational process (involved in sampling from the hypothesis space), rather than through the gradual accumulation of noisy sensory data. We demonstrate the usefulness of our theory by reproducing key pair-wise relationships and stylized facts for the six behavioral measures and point the way toward extending the approach to the complex probabilistic models required to describe the richness of human behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Algorithm and Simulation Details</head><p>Throughout the paper, we used a consistent set of parameters in simulating the Autocorrelated Bayesian Sampler. All simulations were repeated 2 14 times. The target posterior distributions (i.e., P(h|s) in Equation 3) for sampling were Gaussian distributions: ( = 0, = 1) for choice tasks and ( = $;&lt;=&gt;$ , = 4) for estimation tasks. The stimuli discriminability was manipulated at 5 different values by fully randomizing the decision boundaries between {-0.2,0.2} (for the least discriminable stimuli), {-0.5, 0.5}, {-0.7, 0.7}, {-1, 1}, and {-2, 2} (for the most discriminable stimuli). When accuracy was emphasized, we set a larger value for decision threshold ∆= 6, whereas we set ∆= 2 when speed was emphasized. To qualitatively reflect that increasing visual contrasts reduces task difficulty, as shown in <ref type="figure" target="#fig_14">Figure 9B</ref>, we simulated Gaussian target distributions with means all equal to 0 but standard deviations of 1.5 (contrast 1), 1.3 (contrast 2), and 1 (contrast 3).</p><p>We simulated a local sampling algorithm, MC 3 , which draws samples from the unnormalized posterior <ref type="bibr" target="#b65">(Geyer, 1991)</ref>. MC 3 runs parallel Markov chains at different temperatures and allows individual chains to exchange information. We fixed the number of parallel chains at = 6 and varied the temperature between the chains where the c-th chain has a temperature of ? = ! !+@(?*!) . The higher temperature chains will draw samples from flattened target distributions, thus prefer making long distance moves and assist lower temperature chains to make non-local jumps through the swapping step (see Algorithm for details). This means that the first chain is also the cold chain (temperature of 1) such that its target distribution is not heated up and thus undistorted. The widths of proposal distribution were always in smaller proportions to the width of target distribution, where the ratios were set at 1:100 for choice tasks and 1:1.5 for estimation tasks. The average speed of generating a sample = 0.1 sec *! .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Metropolis-coupled Markov chain Monte Carlo (MC 3 )</head><p>Choose a starting hypothesis ℎ ! , target unnormalized posterior (ℎ) = * (ℎ| ) for n=1 to N ⊳generate N samples for c=1 to C ⊳update all C chains Draw a candidate hypothesis ℎ′~(ℎ C*! ?</p><p>, " ) ⊳Gaussian proposal</p><formula xml:id="formula_43">? = min{1, [ (ℎ . ) (ℎ C*! ? )</formula><p>] !/# * } ⊳ acceptance probability Sample ~[0,1] ⊳ acceptance threshold if &lt; ? then ℎ C ? = ℎ′ else ℎ C ? = ℎ C*! ? end if ⊳ decide acceptance end for repeat floor( /2) times ⊳ chain swapping Randomly select two chains , without repetition</p><formula xml:id="formula_44">EF;G = min{1, (ℎ C 6 ) !/# + (ℎ C 5 ) !/# , (ℎ C 5 ) !/# + (ℎ C 6 ) !/# , } ⊳ swapping probability Sample ~[0,1] ⊳ swapping threshold if &lt; EF;G then swap(ℎ C 5 , ℎ C 6</formula><p>) end if ⊳ decide swapping end repeat end for instances ("exemplars") of events in memory, and evaluate new events by activating stored exemplars according to their similarity <ref type="bibr" target="#b115">(Medin &amp; Schaffer, 1978;</ref><ref type="bibr" target="#b120">Nosofsky, 1986)</ref>. This idea has been formalized in the Generalized Context Model <ref type="bibr" target="#b120">(Nosofsky, 1986)</ref>: individual exemplars are assumed to be represented as points in a multidimensional psychological space where similarity between exemplars decreases with the distance between objects in the space <ref type="bibr" target="#b158">(Shepard, 1987)</ref>. if its indicator function takes a value of 1. In this case, the exemplar-based model provides one way to construct the posterior probability of hypotheses by incorporating similarity-weighted information from stored exemplars. EBRW then supposes that people sequentially draw samples according to † ( ) until a threshold number of samples in favor of a response is reached. The waiting times between samples is exponentially distributed, as in the ABS, though each waiting time also includes an additive constant which is not currently present in our model. Thus, the EBRW is nearly equivalent to the direct sampling variant of the ABS in which there is no interdependence between samples of exemplars. What the full ABS can add to this model are autocorrelations between samples and an adaptive prior, which potentially could allow it to account for a wider range of behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blurton et al.'s (2020) Poisson Random Walk Model</head><p>While closely related to the EBRW, <ref type="bibr" target="#b12">Blurton et al.'s (2020)</ref> Poisson Random Walk (PRW) model instead focuses on assigning confusable visual stimuli to perceptual categories (e.g., whether a Gabor patch is oriented to the left or right of vertical) and is based on the Theory of Visual Attention <ref type="bibr" target="#b19">(Bundesen, 1990)</ref>. The TVA's contribution can be captured by Equation S1, with the additional assumption of PRW being that the inter-step time between two successive tentative categories is exponentially distributed with a rate . Thus, for samples generated in this way, the response time should follow an Erlang distribution (the sum of exponentially distributed intervals):</p><p>( | , ) = Erlang( , ) (S2) where, in the binary case, the average speed of generating a tentative category = 1 + 2 where 1 , 2 represent the strengths of category , respectively. In other words, the probability of a tentative category being is 1 = K . K . +K / and that of being is 2 = 1 −</p><formula xml:id="formula_45">1 = K / K . +K / .</formula><p>If we consider the category as the upper bound and the category as the lower bound, the random walk of the accumulator has an increment of +1 with probability 1 and -1 with probability 2 . Coupling the Poisson process of tentative category generation with an optional stopping rule whose accumulator starts at (0 &lt; &lt; ) and evidence thresholds 1 = , 2 = 0, we can derive the cumulative distribution function of first passage time to state 2 (Equation A5 of <ref type="bibr" target="#b12">Blurton et al., 2020)</ref>:</p><formula xml:id="formula_46">2 ( | 1 , 2 , , ) = , ( | , 1 + 2 ) L C%M 2 ( | 1 1 + 2 , , )<label>(S3)</label></formula><p>where ( | , 1 + 2 ) is the cumulative distribution function of the Erlang distribution above. Because of the fixed thresholds (0, ) and starting point ( ), the number of samples (or number of increments) in the random walk to reach one of the thresholds is a random variable. can be as small as (for reaching threshold ) or − (for reaching threshold )</p><p>if all the tentative categories generated in the process are for the same category. But due to the randomness in generating tentative categories, can also be far greater than those numbers and the probability of has a closed form solution <ref type="bibr" target="#b49">(Feller, 1968;</ref><ref type="bibr">Blurton et al., 2020 equation A1)</ref>. Marginalizing over all possible s produces the CDF of response times.</p><p>This basic version of the PRW is mathematically very close to both the decision process of the EBRW (as noted in <ref type="bibr" target="#b12">Blurton et al., 2020</ref>) and a restricted version of the ABS that uses direct sampling and no adaptive prior. First, we can interpret 1 , 2 as posterior probabilities of category and respectively and they also normalize to 1. Second, the Erlang distribution of response times given samples is also assumed in the ABS (compare Equation 6 and S2). Third, the random walk process with an increment of +/-1 is equivalent to the heuristic stopping rule used in the ABS where the differences in number of samples is tracked. While 1 , 2 , are free parameters for the Poisson random walk model, our reinterpretation will constrain their values. 1 , 2 as the posterior probability should be further constrained by the mental representation of stimuli.</p><p>Beyond this basic formulation, the PRW also includes extensions to improve its fit to the data and its scope. To produce fast or slow average errors (in comparison to average correct responses), the PRW incorporates, following the approach used in DDMs, trial-bytrial variability in category strengths and starting points. To explain the variability in the leading edge of response time distributions, the PRW allows for processing rates to vary within a trial. The full version of the ABS produces fast or slow errors because of an adaptive prior and autocorrelated sampling respectively. While we did not explore the ABS's predictions for variability in the leading edge of response time distributions, it would be interesting to see if autocorrelated sampling -effectively varying processing rates within a trial -produces this effect as well, or whether other mechanisms, such as a variable nondecision time, are needed.</p><p>The PRW also has been extended beyond two-alternative choice tasks to any number of alternatives by assuming that evidence for one response suppresses evidence for all other responses. The suppression is strong enough that only one counter has non-zero counts at any one time. This extension differs from our proposal in the Discussion of using a max-minusnext stopping rule for multi-alternative choice as there is no inhibition, but future work is needed to explore what contrasting predictions these mechanisms make.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Optimal Stopping Rule is Approximated by the Heuristic Stopping Rule</head><p>The optimal solution to terminate the sampling algorithm is to solve the following dynamic programming problem. To illustrate, we demonstrate the optimal stopping rule in binary choice. Let (( denote the prior probability of one alternative being true (i.e., the prior on responses) and 56 the posterior probability after 0s and 1s have been accumulated. The posterior probability that the other alternative is true is simply 1 − 56 . The binary decision task is equivalent to a sequential statistical test on whether 56 &lt; 1/2, and the optimal stopping rule for this test can be formally described using dynamic programming as follows <ref type="bibr" target="#b192">(Wald, 1949;</ref><ref type="bibr" target="#b204">Zhu, Sanborn, &amp; Chater, 2019)</ref>:</p><p>( , ) = min { ( ( , ), + 56 ( , + 1) + Ž1 − 56 • ( + 1, )} (S4) where ( , ) and ( ( , ) are respectively expected cost of sampling and expected cost of termination after samples against and samples in favour have been observed, and is the opportunity cost of generating a sample. Suppose that (( = Beta(0,0), then the posterior of evidence is also Beta distributed 56 = Beta( , ). If the punishment for an incorrect decision is one unit of utility, the expected cost of termination should be the expected cost of incorrectly choosing an alternative when posterior of that alternative is 56 :</p><p>( ( , ) = min { + , + } (S5) The expected cost of drawing another sample is the sum of (i) the cost of generating one sample, , (ii) the expected cost if the new sample turns out to be in favor, 56 ( , + 1), and (iii) the expected cost if the new sample turns out to be against, Ž1 − <ref type="figure" target="#fig_8">56 • ( + 1, )</ref>. The sampling process should stop whenever the expected cost of sampling exceeds that of termination: ( , ) ≥ ( ( , ). We solved the dynamic programming and visualised its stopping rules in <ref type="figure">Figure S1</ref> by setting the cost of collecting one sample, = 0.006, and the prior probability of evidence to Beta(1,1). While the optimal decision threshold is collapsing over time, it can be approximated by a heuristic max-minus-next stopping rule. Indeed, in multi-alternative settings, it has been shown that implementing the max-minus-next rule closely matches the performance of an optimal stopping rule <ref type="bibr" target="#b38">(Dragalin et al., 1999;</ref><ref type="bibr" target="#b185">2000)</ref>. <ref type="figure">Figure S1</ref>. Decision boundaries obtained from solving the dynamic programming problem.</p><p>The sampling algorithm should be terminated once the accumulator reaches the yellow terminating regions. In this illustration, the accumulator started at the top-left corner {i=0,j=0} (white triangle) and terminated at the state {i=3,j=5} (white circle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence From Binomial Sample Counts Can Produce Metacognitive Inefficiency</head><p>In this section, we demonstrate the problem that when confidence judgments are assumed to be tally of discrete sample counts (especially when the sample size is small), the so-called "metacognitive inefficiency" effect demonstrated by <ref type="bibr" target="#b156">Shekhar &amp; Rahnev (2021a)</ref> can also be present even without any loss of information (i.e., noise) in forming confidence judgments. To recap, metacognitive inefficiency describes a relationship between an informativeness measure ( _ ′/ . ) and the confidence criterion which classifies 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Terminating regions confidence judgments as either high or low <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a)</ref>. Both _ ′ and ′ are model-based measures of informativeness that crucially assume Gaussian distributions <ref type="bibr" target="#b112">(Maniscalco &amp; Lau, 2012;</ref><ref type="bibr" target="#b56">Fleming &amp; Lau, 2014)</ref>. More specifically, the inverse Gaussian cumulative distribution function (CDF) is used to calculate _ ′ and ′:</p><p>_ . ( ) = *! (hit rate| ) − *! (false alarm rate| )</p><p>where *! is the inverse Gaussian CDF and is the confidence criterion for which individual confidence judgments that are greater (smaller) than are considered high (low). Note that when = 50% (i.e., when choosing either option always receives the same level of confidence), . = _ . .</p><p>The empirical results uncovered using this approach indicate that as the confidence criterion ( ) increases, _ ′/ ′ decreases <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a)</ref>. As ′ is fixed in this calculation, that means that as moves further into the upper tails, _ ′( ) becomes smaller. This result has been used to argue that a certain amount of information about a decision was lost in the process of forming metacognitive judgments and that more extreme confidence judgments are noisier <ref type="bibr" target="#b156">(Shekhar &amp; Rahnev, 2021a;</ref><ref type="bibr" target="#b157">2021b)</ref>.</p><p>Here, we do not challenge the empirical validity of the effect. Instead, we show that assuming a Gaussian distribution when calculating the informativeness of metacognitive judgment can lead to a decrease in the measured metacognitive efficiency when there is no loss of information but the data-generating distribution is not Gaussian. To illustrate, we present an example when the data is generated by binomial distributions (see <ref type="figure" target="#fig_0">Figure S2)</ref>. In this example, the cumulative Gaussian distributions that respectively intersect with the hit and false alarm distributions (which are cumulative binomial) become closer together as the confidence criterion becomes more extreme: _ . ( ) &lt; _ . ( ) and &gt; ≥ .</p><p>The quantitative difference between the two _ . of 0.16 is small but noticeable in this example (see <ref type="figure" target="#fig_0">Figure S2</ref>). However, this bias becomes smaller when is large because the data-generating binomial distribution is better approximated by a Gaussian distribution. <ref type="figure" target="#fig_0">Figure S2</ref>. An illustration of metacognitive inefficiency arising not from loss of information in confidence judgment, but from incorrectly assuming Gaussian generating distributions.</p><p>Two confidence criteria were shown ( , , where &gt; ). Hit rates were calculated based on the binomial distribution Bin( , ) (black circles) given its intersection with a confidence criterion, whereas similarly false alarm rates were calculated using the symmetric binomial distribution Bin( , 1 − ) (black squares). In this illustrative example, = 12 and = 0.8.</p><p>Using a Gaussian to compute _ . (difference in the horizontal positions of the solid curves) will lead to a decrease in value when the confidence criterion increases: &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 1 a i i 3 C K k a Y C 9 k X q 7 d U F + F c l c N s = " &gt; A A A B + n i c b V B N S 8 N A E N 3 U r 1 q / U r 0 I X h a L U E F L I k U 9 F r 1 4 k g r 2 A 9 p Q N t t t X b r Z h N 2 J W m J / i h c P i n j 1 l 3 j z 3 7 h t c 9 D W B w O P 9 2 a Y m e d H g m t w n G 8 r s 7 C 4 t L y S X c 2 t r W 9 s b t n 5 7 b o O Y 0 V Z j Y Y i V E 2 f a C a 4 Z D X g I F g z U o w E v m A N f 3 A 5 9 h v 3 T G k e y l s Y R s w L S F / y H q c E j N S x 8 2 1 g j 5 B c c D k q X h + 5 x 9 F h x y 4 4 J W c C P E / c l B R Q i m r H / m p 3 Q x o H T A I V R O u W 6 0 T g J U Q B p 4 K N c u 1 Y s 4 j Q A e m z l q G S B E x 7 y e T 0 E T 4 w S h f 3 Q m V K A p 6 o v y c S E m g 9 D H z T G R C 4 0 7 P e W P z P a 8 X Q O / c S L q M Y m K T T R b 1 Y Y A j x O A f c 5 Y p R E E N D C F X c 3 I r p H V G E g k k r Z 0 J w Z 1 + e J / W T k n t a K t + U C 5 X d N I 4 s 2 k P 7 q I h c d I Y q 6 A p V U Q 1 R 9 I C e 0 S t 6 s 5 6 s F + v d + p i 2 Z q x 0 Z g f 9 g f X 5 A + q f k v 8 = &lt; / l a t e x i t &gt; Bin(N, 1 p)</p><formula xml:id="formula_47">_ . ( ) &lt; _ . ( ).</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B w m N J m m g T 6 y N Z j J i M S 8 Y g X O r S M k = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v U i e F k s Q g U p i R T 1 W P T i S S r Y D 2 h D 2 W w 3 7 d L N J u x O x B r 6 S 7 x 4 U M S r P 8 W b / 8 Z t m 4 N W H w w 8 3 p t h Z p 4 f C 6 7 B c b 6 s 3 N L y y u p a f r 2 w s b m 1 X b R 3 d p s 6 S h R l D R q J S L V 9 o p n g k j W A g 2 D t W D E S + o K 1 / N H V 1 G / d M 6 V 5 J O 9 g H D M v J A P J A 0 4 J G K l n F 7 v A H i C 9 5 H J S v j m J j 3 t 2 y a k 4 M + C / x M 1 I C W W o 9 + z P b j + i S c g k U E G 0 7 r h O D F 5 K F H A q 2 K T Q T T S L C R 2 R A e s Y K k n I t J f O D p / g I 6 P 0 c R A p U x L w T P 0 5 k Z J Q 6 3 H o m 8 6 Q w F A v e l P x P 6 + T Q H D h p V z G C T B J 5 4 u C R G C I 8 D Q F 3 O e K U R B j Q w h V 3 N y K 6 Z A o Q s F k V T A h u I s v / y X N 0 4 p 7 V q n e V k u 1 / S y O P D p A h 6 i M X H S O a u g a 1 V E D U Z S g J / S C X q 1 H 6 9 l 6 s 9 7 n r T k r m 9 l D v 2 B 9 f A M H U p K N &lt; / l a t e x i t &gt; Bin(N, p) <ref type="figure" target="#fig_6">Figure S4</ref>. Histograms of response-time data collected from the three experiments of probability judgment shown in the legend. Across the three experiments, RTs for probability judgments are unimodal and positively skewed. Colored lines are best-fitting Gamma distributions. RTs greater than 60 sec were excluded from analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Schematic illustrations of the computational mechanisms and potential behavioral outputs of the SPRT (A) and the ABS (B). A typical trial of a numerosity task is visualized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r e L O e r B f r 3 f q Y j 6 5 Y + c 4 R + A P r 8 w e g d J W n &lt; / l a t e x i t &gt; P (h|s) / P (s|h)P (h) t e x i t s h a 1 _ b a s e 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 S P q Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " c 3 C w y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " l a f 5 r 0 e j O m p k 9 b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Further illustrations of the autocorrelated sampling process (top) and the Bayesian Monte Carlo process (bottom), expanding the illustrative example of numerosity in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Gaussian distribution with mean equal to 24-the correct number of dots. The variance of the distribution may stem from all kinds of uncertainties including, for example, perceptual noise and/or uncertainties associated with the processing of information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Relationship between the underlying probabilities of event A and the average probability judgments of A predicted by the ABS. (A) From left to right, the number of alternatives, M, varies from 2 to 7. Within each panel, simulated sample sizes, N, range from 1 to 9 in increments of 2. While uniform Dirichlet priors were used in the simulation, the indifferent points are always located at (1/M, 1/M) for symmetric Dirichlet priors. (B) As produced by the ABS, the empirical indifference points between mean probability judgments and objective probabilities are related to the inverse of the number of alternatives (M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>(A) Empirical results based on the 4 experiments of<ref type="bibr" target="#b203">Zhu et al. (2020)</ref> and<ref type="bibr" target="#b168">Sundh et al. (2021)</ref>, showing inverted U-shaped relationships between means and variances of probability estimates, such that extreme probability estimates (very near 0 or 1) are ruled out.Solid curves are the mixed-effect regression models fitted on individual-level probability estimates. (B) Analytic approximations of the mean-variance relationship predicted by the model predicts an inverted-U shape with stronger priors on responses moving the curve inward and more samples moving the curves downward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Bias in explicit subadditivity, computed as the sum of the probability estimates of each component hypothesis minus the probability estimates of their disjunction, increases as the number of component hypotheses increases. This empirical effect is correctly captured by the Bayesian Sampler (solid line). The sample sizes were set at N=5, and the prior on responses was Beta(1,1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>(A) Empirical choice outcomes and response-time distributions in the difficultaccuracy condition. (B) Simulated choice outcomes and RT distributions in the difficultaccuracy condition. RTs were fitted with Gamma distributions with the best-fitting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Frequency</head><label></label><figDesc>solid lines for correct (in blue) and error (in red) responses. Overlaid dots and their horizontal error bars denote mean RTs and 95% confidence intervals respectively. Similarly, (C) and (D) are respectively empirical data and model simulations for the easy-speed condition. Across the different variants, only the full ABS model reproduces both slow and fast errors in the correct experimental conditions. All predicted RT distributions were unimodal and positively skewed. The full model also correctly reproduces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Sample quantile-quantile plots of response-times distributions. (A) An example Q-Q plot of empirical RT distributions adapted from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 .</head><label>9</label><figDesc>ABS simulations showing effects of confidence in decisions. (A) Positive relationship between stimulus discriminability and average confidence. Empirical data adapted from Vickers &amp; Packer (1982). (B) Resolution of confidence in which average confidence is higher for correct responses than for incorrect responses. Empirical data replotted from Vickers &amp; Packer (1982). (C) Degree of metacognitive efficiency showing extreme confidence ratings are less informative about the accuracy of a choice. Empirical plot adapted from Shekhar &amp; Rahnev (2021a). (D) Negative (cross-trials) relationship between confidence and RT and positive (cross-conditions) relationship between confidence and RT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 .</head><label>11</label><figDesc>(A) Experimental data on the decision-estimation task. For the region of correct hypotheses in the range of [21, 30], the estimates were pushed away from a nearby comparison value (25.5) used in the preceding decision task (blue bars), while pulled toward a far-off comparison value (75.5; red bars). The data was replotted from Spicer et al. (2022). (B) Simulating the ABS (Right), its direct sampling variant (Left), and fixed sample size variant (Middle) on decision-estimation tasks. The full ABS model predicts both anchoring (for far-away target stimuli; red bars) and repulsion effects (for close-by target stimuli; blue bars), whereas the direct-sampling variant only predicts the repulsion effect and the fixedsample-size variant only predicts the anchoring bias. Target Gaussian distributions for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 .</head><label>12</label><figDesc>Power spectra for time series of estimates and RT. Dashed lines denote power spectra by a simulated participant with solid lines showing the average. RT time series are colored in red, whereas time series of estimates are in blue. (Left) The direct sampling variant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>For a new observation and a set of stored exemplars * = { ! * , " * , … , H * }, all exemplars are activated in proportion to their similarity with the new observation ( , * ). The predictive value for the new observation is:† ( ) = ∑ ( 5 ) ( , 5 )where ( 5 ) is the information associated with the exemplar 5 . This equation is a general expression for many cognitive applications of the exemplar model. For example, to identify within the set of exemplars * (i.e., † ( ) = ( * | )), we can set the function ( 5 ) = 1 if 5 = 0 otherwise ; similarly, to judge the probability of belonging to a category (i.e., ( 5 ) is an indicator function as it was used in the categorization tasks, the resultant † ( ) is a discrete probability distribution within a continuous psychological space. The probability associated with the -th exemplar equals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>s true = 24</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>stimulus</cell></row><row><cell>(number of dots)</cell><cell>hypotheses 20 21 22 23 24 25 26 27 28</cell><cell>the posterior</cell><cell>sample from</cell><cell>posterior of hypotheses</cell></row><row><cell></cell><cell>physical times</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Key Empirical Effects of the Six Behavioral Measures</figDesc><table><row><cell></cell><cell>Empirical</cell><cell>Description</cell><cell>Example</cell><cell>ABS Explanations</cell></row><row><cell></cell><cell>Effects</cell><cell></cell><cell>References</cell><cell></cell></row><row><cell>Probability</cell><cell>Conservatism</cell><cell>Probability</cell><cell>Erev et al.</cell><cell>Incorporating a symmetric</cell></row><row><cell>Judgments</cell><cell>and</cell><cell>judgments show a</cell><cell>(1994);</cell><cell>prior on responses pulls</cell></row><row><cell></cell><cell>probabilistic</cell><cell>linear bias toward</cell><cell>Costello &amp;</cell><cell>probability judgments</cell></row><row><cell></cell><cell>identities</cell><cell>0.5 for binary</cell><cell>Watts</cell><cell>towards 0.5 for binary events.</cell></row><row><cell></cell><cell></cell><cell>events.</cell><cell>(2014)</cell><cell></cell></row><row><cell></cell><cell>Mean-</cell><cell>The variance of</cell><cell>Sundh et al.</cell><cell>Same as above.</cell></row><row><cell></cell><cell>variance</cell><cell>probability</cell><cell>(2021)</cell><cell></cell></row><row><cell></cell><cell>relationship</cell><cell>judgments is</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>highest for</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>moderate</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>probabilities and</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>goes to zero before</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>reaching the limits</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>of the scale.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Explicit</cell><cell>When assessed</cell><cell>Tversky &amp;</cell><cell>Same as above.</cell></row><row><cell></cell><cell>subadditivity</cell><cell>separately, the sum</cell><cell>Koehler</cell><cell></cell></row><row><cell></cell><cell></cell><cell>of the probability</cell><cell>(1994);</cell><cell></cell></row><row><cell></cell><cell></cell><cell>judgments of the</cell><cell>Fischhoff et</cell><cell></cell></row><row><cell></cell><cell></cell><cell>unpacked</cell><cell>al. (1978);</cell><cell></cell></row><row><cell></cell><cell></cell><cell>descriptors exceeds</cell><cell>Olson</cell><cell></cell></row><row><cell></cell><cell></cell><cell>that of the packed</cell><cell>(1976);</cell><cell></cell></row><row><cell></cell><cell></cell><cell>descriptor.</cell><cell>Mehle et al.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>Which Models Reproduce the Key Empirical Targets</figDesc><table><row><cell></cell><cell>Empirical Effects</cell><cell>SPRT Bayesian</cell><cell cols="3">Autocorrelated Bayesian Sampler</cell></row><row><cell></cell><cell></cell><cell>Sampler</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(Zhu et</cell><cell>No</cell><cell>Direct</cell><cell>Fixed</cell><cell>Full</cell></row><row><cell></cell><cell></cell><cell>al., 2020)</cell><cell>prior</cell><cell>sampling</cell><cell>sample</cell><cell>model</cell></row><row><cell></cell><cell></cell><cell></cell><cell>variant</cell><cell>variant</cell><cell>size</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>variant</cell></row><row><cell>Probability</cell><cell>Conservatism and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Judgments</cell><cell>probabilistic identities</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will only occasionally touch on the large and distinct literature on risky decision making based on verbally or numerically problems in the fields of judgment and decision making and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Non-decision time, as is commonly used in models of response time, would be necessary to include to quantitatively reproduce empirical RT distributions, but it is not needed to produce the qualitative effects we evaluate below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">A reanalysis of the four probability judgment experiments presented in<ref type="bibr" target="#b168">Sundh et al. (2021)</ref> shows that participants produced indifferent probability judgments (i.e., exactly 50 on a 0-100 scale) on about 6% of trials.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinterpretations of Nosofsky &amp; Palmeri's (1997) Exemplar-based Random Walk</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Blurton et al.'s (2020) Poisson Random Walk Model</head><p>In this section, we provide detailed reinterpretations of <ref type="bibr" target="#b121">Nosofsky &amp; Palmeri's (1997)</ref> EBRW model and <ref type="bibr" target="#b12">Blurton et al.'s (2020)</ref> Poisson random walk model. This analysis helps shed light on the relationship between the ABS and these existing computational models. In short, the direct sampling variant of the ABS parallels the model mechanisms of both the EBRW and Poisson random walk models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nosofsky &amp; Palmeri's (1997) Exemplar-based Random Walk Model</head><p>The EBRW model, a generalization of Logan's instance-based model <ref type="bibr" target="#b107">(Logan, 1988)</ref>, aims to explain the relationship between categorization and response times by integrating exemplar-based models of categorization and response-time models of evidence accumulation <ref type="bibr" target="#b121">(Nosofsky &amp; Palmeri, 1997)</ref>. Exemplar models assume that people store many</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E ABS Predictions When Estimates are the Sample Average Instead of the Last Sample</head><p>Here we consider an alternative process of making estimates using sample averages instead of taking the last sample as the estimate. As shown in <ref type="figure">Figure S3</ref>, with this new process of producing estimates, the qualitative model behavior of the ABS was reproduced.</p><p>Using the sample average as the estimate, however, does make the anchoring effect stronger, while repulsion and the degree of autocorrelation become weaker. This is because, compared to taking the last sample, averaging of all samples increased the impact of the earlier samples generated by the autocorrelated sampler. <ref type="figure">Figure S3</ref>. Using the sample average, rather than the last sample, as the estimate does not qualitatively change model behaviors. (A) The model predicts co-occurrence of repulsion and anchoring as shown in <ref type="figure">Figure 11. (B)</ref> The model produces 1/f noise as shown in <ref type="figure">Figure 12</ref>.</p><p>The sample size used in the simulations was fixed at 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F The Process and Time for Producing Probability Judgment</head><p>In this section, we provide further justification for the ABS process of probability judgment in which (i) a fixed number of samples is collected and (ii) the waiting time between two consecutive samples are exponentially distributed. The fixed-sample-size rule was assumed both for simplicity and because of the lack of an obvious target for deriving an optimal stopping rule for probability judgments. As shown in the main text, this process should produce Gamma-distributed RTs (i.e., continuous generalization of the Erlang distribution), which are unimodal and positively skewed. In <ref type="figure">Figure S4</ref>, this prediction is confirmed by the RT data collected from three experiments <ref type="bibr" target="#b203">(Zhu et al., 2020;</ref><ref type="bibr" target="#b168">Sundh et al., 2021)</ref>. The best-fitting shape and scale parameters for experiment 1, 2, and 3 are Gamma(2.36, 3.43), Gamma(2.37, 3.97), Gamma(2.22, 3.68) respectively. The RT distributions resemble each other despite being based on different groups of participants and different probability judgments, suggesting that people may indeed use a fixed-sample-size rule to produce probability judgments.</p><p>Alternatively, one could replace the fixed-sample-size rule with an optional stopping rule (e.g., the max-minus-next stopping rule) or a fixed-time stopping rule (e.g., stop after thinking about the task for 10 sec). A max-minus-next stopping rule would predict a mixture of Erlang distributions but would also predict no indifferent probability judgments because sampling continues under this rule until probability judgments are no longer indifferent. This mismatches empirical data as discussed earlier. On the other hand, a fixed-time rule predicts a Gaussian distribution of RTs if we further assume Gaussian noise in the estimation of elapsed time. This prediction should be rejected given the skewness in the RT distributions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring the influence of particle filter parameters on order effects in causal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<editor>L. Carlson, C. Hoelscher, &amp; T. F. Shipley</editor>
		<meeting>the annual meeting of the cognitive science society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The adaptive nature of human categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to MCMC for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fact-free learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aragones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Postlewaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmeidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1355" to="1368" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The effects of averaging subjective probability estimates between and within judges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tung Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Budescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Zauberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Psychological probability as a function of experienced frequency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Attneave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Theory-based social goal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual conference of the cognitive science society</title>
		<editor>B. C. Love, K. McRae, &amp; V. M. Sloutsky</editor>
		<meeting>the thirtieth annual conference of the cognitive science society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1447" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action understanding as inverse planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="349" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Corporate capital allocation: A behavioral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bardolet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lovallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strategic Management Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1465" to="1483" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probing the locus of confidence judgments: Experiments on the time to determine confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Baranski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Petrusic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="929" to="945" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrasting stochastic and support theory accounts of subadditivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Bearden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Wallsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="229" to="241" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Poisson random walk model of response times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Blurton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kyllingsbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bundesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">362</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The physics of optimal decision making: a formal analysis of models of performance in twoalternative forced-choice tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moehlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">700</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Formalizing Neurath&apos;s ship: Approximate algorithms for online causal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Lagnado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">301</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A random support model of the calibration of subjective probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="110" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Handbook of Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
		<editor>X. L.</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Observing evidence accumulation during multi-alternative decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="453" to="462" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Resource-rational decision making. Current Opinion in Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A theory of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bundesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">523</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Quantum cognition. Annual review of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Pothos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="749" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A quantum theoretical explanation for probability judgment errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Pothos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">193</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decision field theory: a dynamic-cognitive approach to decision making in an uncertain environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">432</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian confidence in optimal decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calder-Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PsyArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic models of language processing and acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic biases meet the Bayesian brain</title>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="506" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Power prior distributions for generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Surprisingly rational: probability theory plus noise explains biases in judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">463</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Explaining high conjunction fallacy rates: The probability theory plus noise account</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="321" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Invariants in probabilistic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Surprising rationality in probability judgment: Assessing two competing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="280" to="297" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The rat as particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Platt, D. Koller, Y. Singer, &amp; S. Roweis</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Where do hypotheses come from?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Theoretical neuroscience: computational and mathematical modeling of neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conjugate priors for exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ylvisaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="269" to="281" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-stage sequential sampling models with finite or infinite time horizon and variable boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="128" to="145" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integration of the ecological and error models of overconfidence using a multiple-trace memory model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R P</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="579" to="599" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multihypothesis sequential probability ratio tests. I. Asymptotic optimality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dragalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Tartakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Veeravalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2448" to="2461" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multihypothesis sequential probability ratio tests. II. Accurate asymptotic expansions for the expected sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dragalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Tartakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Veeravalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1366" to="1383" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning optimal decisions with confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">F</forename><surname>Mainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="24872" to="24880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The cost of accumulating evidence in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno-Bote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3612" to="3628" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The cost of accumulating evidence in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno-Bote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3612" to="3628" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Computational precision of mental inference as critical source of human choice suboptimality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Devauchelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1398" to="1411" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimal strategies for seeking information-Models for statistics, choice reaction-times, and human information-processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="312" to="329" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The anchoring-and-adjustment heuristic: Why the adjustments are insufficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simultaneous over-and underconfidence: The role of error in judgment processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Wallsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Budescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">519</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The conjunction fallacy: A test of averaging hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stolarz-Fantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neuroeconomic Foundations of Economic Choice--Recent Advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">An introduction to probability theory and its applications</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1968" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Heuristics and biases in theory formation: On the cognitive processes of those concerned with cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory &amp; Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="430" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Computation noise in human learning and decision-making: origin, impact, function. Current Opinion in Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Findling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Statistically optimal perception and learning: from behavior to neural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orbán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fault trees: Sensitivity of estimated failure probabilities to problem representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fischhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lichtenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Component probabilities and the conjunction fallacy: Resolving signed summation and the low component model in a contingent approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pidgeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How to measure metacognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">443</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Between ignorance and truth: Partition dependence and learning in judgment under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rottenstreich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1385</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Partition priming in judgment under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rottenstreich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Inductive Inference as Adaptive Search using Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Fränken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Theodoropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Bramley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>PsyArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mathematical games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="25" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A study of the relation of accuracy to speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Garrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="105" />
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Complex probabilistic inference. Computational models of brain and behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">453</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Context, learning, and extinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">197</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo maximum likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing science and statistics: Proceedings of 23rd Symposium on the Interface, Interface Foundation</title>
		<meeting><address><addrLine>Fairfax Station</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cognitive emissions of 1/f noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gilden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">1/f noise in human cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gilden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">5205</biblScope>
			<biblScope unit="page" from="1837" to="1839" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Banburismus and the brain: Decoding the relationship between sensory stimuli, decisions, and reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The neural basis of decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Signal detection theory and psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>Wiley</publisher>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bridging levels of analysis for probabilistic models of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="268" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Perceptual decision-making as probabilistic inference by neural sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haefner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Think again? The amount of mental simulation tracks uncertainty in the outcome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<editor>D. C. Noelle, R. Dale, A. S</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warlaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Matlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jennings</surname></persName>
		</author>
		<title level="m">Proceedings of the thirtieth annual conference of the cognitive science society</title>
		<editor>&amp; P. P. Maglio</editor>
		<meeting>the thirtieth annual conference of the cognitive science society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<biblScope unit="page" from="866" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Racing against the clock: Evidence-based vs. timebased decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="263" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The relation of the time of a judgment to its accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A C</forename><surname>Henmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On the rate of gain of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Toward a synthesis of cognitive biases: how noisy information processing can bias human decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Interpreting neural response variability as Monte Carlo sampling of the posterior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The power prior: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="3724" to="3749" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Tests of two theories of decision in an &quot;expanded judgment&quot; situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">261</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A new perceptual illusion reveals mechanisms of sensory decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Movshon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="issue">7138</biblScope>
			<biblScope unit="page" from="912" to="915" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Confidence and speed in the two-category judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Thurstonian and Brunswikian origins of uncertainty in judgment: A sampling model of confidence in sensory discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">344</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">PROBabilities from EXemplars (PROBEX): A &quot;lazy&quot; algorithm for probabilistic inference from generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="563" to="607" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The naïve intuitive statistician: A naïve sampling model of intuitive confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">678</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Calibration, additivity, and source independence of probability judgments in general knowledge and sensory discrimination tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="34" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Testing the foundations of signal detection theory in recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A computational framework for the study of confidence in humans and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kepecs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">F</forename><surname>Mainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="1322" to="1337" />
			<date type="published" when="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Choice certainty is informed by both evidence and decision time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Corthell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1329" to="1342" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Bayesian integration in sensorimotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">6971</biblScope>
			<biblScope unit="page" from="244" to="247" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="13852" to="13857" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Visual fixations and the computation and comparison of value in simple choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Armel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1292" to="1298" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A geometric framework for modeling dynamic decisions among arbitrarily many alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kvam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="14" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A distributional and dynamic theory of pricing and preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Reconciling similarity across models of continuous selections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A unified theory of discrete and continuous responding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Bayesian intractability is not an ailment that approximation can cure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwisthout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wareham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="784" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">253</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Information theory of choice-reaction times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R J</forename><surname>Laming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Academic Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Modeling the effects of memory on human online sentence processing with particle filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, &amp; L. Bottou</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The anchoring bias reflects rational use of cognitive resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="349" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A sequential theory of psychological discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Long-range temporal correlations and scaling behavior in human brain oscillations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Linkenkaer-Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Nikouline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Palva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Ilmoniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1370" to="1377" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Toward an instance theory of automatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">492</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Response times: Their role in inferring elementary mental organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Oxford University Press on Demand</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Post-decision biases reveal a self-consistency principle in perceptual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">33334</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Sampling can be faster than optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="20881" to="20885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Visual confidence. Annual Review of Vision Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="459" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Consciousness and Cognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="422" to="430" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>W. H. Freeman</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Bayesian and signal detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Cognitive Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Context theory of classification learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Medin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">207</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">The availability explanation of excessive plausibility assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Gettys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">The drift diffusion model can account for value-based choice response times under high and low time pressure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="437" to="449" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Optimal decision making in heterogeneous and biased environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="53" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Choice, similarity, and the context theory of classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Attention, similarity, and the identification-categorization relationship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">An exemplar-based random walk model of speeded classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Palmeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">266</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Scalesimilar activity in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shannahoff-Khalsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2387</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Heuristics can produce surprisingly rational probability estimates: Comment on Costello and Watts</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Linda is not a bearded lady: configural weighting and adding as the cause of extension errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">517</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Some apparent violations of the representativeness heuristic in human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">599</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Neural variability and sampling-based probabilistic representations in the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orbán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="530" to="543" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">The interpretation of reaction time in information-processing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Pachella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human information processing: Tutorials in performance and cognition</title>
		<editor>B. H. Kantowitz</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1974" />
			<biblScope unit="page" from="41" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Man as an intuitive statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Beach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Two-stage dynamic signal detection: a theory of choice, decision time, and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pleskac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">864</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Confidence and certainty: distinct probabilistic quantities for different goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kepecs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="374" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Hick&apos;s law for choice reaction time: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1281" to="1299" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Bayesian Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>S. Becker, S. Thrun, &amp; K. Obermayer</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A theory of order relations in perceptual matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="552" to="572" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Decision making on spatially continuous scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">888</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">The diffusion decision model: theory and data for twochoice decision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Modeling response times for two-choice decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Modeling simple decisions and applications using a diffusion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<editor>J. R. Busemeyer, Z. Wang, J. T. Townsend, &amp; A. Eidels</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="35" to="62" />
		</imprint>
	</monogr>
	<note>The Oxford handbook of computational and mathematical psychology</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Modeling confidence and response time in recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Starns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A diffusion model account of the lexical decision task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Diffusion decision model: Current issues and history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="281" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A diffusion model analysis of the effects of aging on brightness discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thapar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="535" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Effects of aging and IQ on item and associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thapar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">464</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Monte Carlo statistical methods</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Langevin diffusions and Metropolis-Hastings algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology and Computing in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Brownian dynamics as smart Monte Carlo simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rossky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4628" to="4633" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Signal detection theories of recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Rotello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning and memory: A comprehensive reference</title>
		<editor>J. H. Byrne &amp; J. T. Wixted</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="226" />
		</imprint>
	</monogr>
	<note>Cognitive Psychology of Memory. 2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">On the hardness of approximate reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="302" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Premature sampling in random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="289" to="296" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Where is the fault in fault trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kolzow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Fast and accurate learning when making discrete numerical estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1004859</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Bayesian brains without probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="883" to="893" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: alternative algorithms for category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1144</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Reconciling intuitive physics and Newtonian mechanics for colliding objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">411</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">The nature of metacognitive inefficiency in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Sources of metacognitive inefficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Toward a universal law of generalization for psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4820</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">The application of Fourier deconvolution to reaction time data: A cautionary note</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Sheu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rottenstreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadjichristidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Typical versus atypical unpacking and superadditive probability judgment</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Diffusion theory of decision making in continuous report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">425</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Perceptual and cognitive judgments show both anchoring and repulsion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Validating the unequal-variance assumption in recognition memory using response time distributions instead of ROC functions: A diffusion model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Starns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Imperfect Bayesian inference in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stengård</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1006465</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Models for choice-reaction time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="260" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Explaining the enigmatic anchoring effect: Mechanisms of selective accessibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mussweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">437</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">The mean-variance signature of Bayesian probability judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/yuhaz</idno>
		<ptr target="https://doi.org/10.31234/osf.io/yuhaz" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">The elusive tradeoff: Speed vs accuracy in visual discrimination tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Swensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="32" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Bayesian models of cognition revisited: Setting optimality aside and letting data drive psychological theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="441" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">On the determinants of the conjunction fallacy: probability versus inductive confirmation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tentori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Crupi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">235</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Overestimation of subjective probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Teigen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="62" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Learning abstract structure for drawing by efficient motor program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kryven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>H. Larochelle, M. Ranzato, R</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Hadsell, M. F. Balcan, &amp; H. Lin</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2686" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Human optional stopping in a heteroscedastic world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tickle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Stochastic modeling of elementary psychological processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Ashby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CUP Archive</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Characterizing belief bias in syllogistic reasoning: A hierarchical-Bayesian meta-analysis of roc data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trippas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fugelsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2141" to="2174" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Belief in the law of small numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: Heuristics and biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Support theory: a nonextensional representation of subjective probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">547</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Loss aversion and inhibition in dynamical models of multialternative choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">757</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Self-organization of cognitive performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Van Orden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Turvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">331</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">The tractable cognition thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="939" to="984" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Drawing and cognition: Descriptive and experimental studies of graphic production processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Sommers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">ROC curves and confidence judgments in recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">582</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Judgments of proportions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Varey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Mellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Birnbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">613</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Decision processes in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Academic Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Effects of alternating set for speed or accuracy on response time, accuracy and confidence in a unidimensional discrimination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Packer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Decision making and memory: A critique of Juslin and Olsson&apos;s (1997) sampling model of sensory discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="789" to="804" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">One and done? Optimal decisions from very few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="637" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Estimation and interpretation of 1/f α noise in human cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="615" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Statistical decision functions. The Annals of Mathematical Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="page" from="165" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Optimum character of the sequential probability ratio test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="326" to="339" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Speed-accuracy tradeoff and information-processing dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Wickelgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="67" to="85" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Choice variability and suboptimality in uncertain environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="109" to="115" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Shared mechanisms for confidence judgements and error detection in human decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Cognitive Neuroscience of Metacognition</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="147" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Vision as Bayesian inference: analysis by synthesis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Do perceptual biases emerge early or late in visual processing? Decision-biases in motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zamboni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ledgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schluppeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<date type="published" when="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Information processing and Bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Understanding the structure of cognitive noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Mental sampling in multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Bianchi, &amp; R. Garnett</editor>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000190</idno>
		<ptr target="http://dx.doi.org/10.1037/rev0000190" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="719" to="748" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Why decisions bias perception: an amortised sequential sampling account</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Cognitive Science Society</title>
		<editor>A. K. Goel, C. M. Seifert, &amp; C. Freksa</editor>
		<meeting>Annual Meeting of the Cognitive Science Society<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Cognitive variability matches speculative price dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/gfjvs</idno>
		<ptr target="https://doi.org/10.31234/osf.io/gfjvs" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Clarifying the relationship between coherence and accuracy in probability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Newall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page">105022</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
