<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum learning in humans and neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Strittmatter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Sarao Mannelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science and AI</orgName>
								<orgName type="department" key="dep2">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Chalmers University of Technology</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ruiz-Garcia</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Departamento de Estructura de la Materia</orgName>
								<orgName type="department" key="dep2">Física Térmica y Electrónica</orgName>
								<orgName type="institution">Universidad Complutense Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Grupo Interdisciplinar de Sistemas Complejos</orgName>
								<orgName type="institution" key="instit2">Universidad Complutense Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Musslick</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Cognitive Science</orgName>
								<orgName type="institution">Osnabrück University</orgName>
								<address>
									<settlement>Osnabrück</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Cognitive and Psychological Sciences</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Wolfgang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Spitzer</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Luther University Halle-Wittenberg</orgName>
								<address>
									<settlement>Martin, Halle</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Spitzer</surname></persName>
							<email>markus.spitzer@psych.uni-halle.de</email>
						</author>
						<title level="a" type="main">Curriculum learning in humans and neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The sequencing of training trials can significantly influence learning outcomes in humans and neural networks. However, studies comparing the effects of training curricula between the two have typically focused on the acquisition of multiple tasks. Here, we investigate curriculum learning in a single perceptual decision-making task, examining whether the behavior of a parsimonious network trained on different curricula would be replicated in human participants. Our results show that progressively increasing task difficulty during training facilitates learning compared to training at a fixed level of difficulty or at random. Furthermore, a sequences designed to hamper learning in a parsimonious neural network network impair learning in humans. As such, our findings indicate strong qualitative similarities between neural networks and humans in curriculum learning for perceptual decision-making, suggesting the former can serve as a viable computational model of the latter.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Curriculum learning is the structured organization of training data or experiences, where the ordering follows a predefined strategy to influence the learning process. The effects of curriculum learning have been studied in both humans and neural networks (for humans, see <ref type="bibr" target="#b0">Ahissar and Hochstein, 1997;</ref><ref type="bibr" target="#b5">Church et al., 2013;</ref><ref type="bibr" target="#b8">Hocker et al., 2024;</ref><ref type="bibr" target="#b14">Liu et al., 2008;</ref><ref type="bibr" target="#b17">McLaren and Suret, 2000;</ref><ref type="bibr" target="#b23">Roads et al., 2018</ref>; for neural networks, see <ref type="bibr" target="#b8">Hocker et al., 2024;</ref><ref type="bibr" target="#b12">Lee et al., 2024;</ref><ref type="bibr" target="#b15">Makino, 2023;</ref><ref type="bibr" target="#b16">Mannelli et al., 2024;</ref><ref type="bibr" target="#b24">Saglietti et al., 2022)</ref>. These studies suggest that progressively increasing task difficulty typically facilitates learning. However, the relationship between curriculum learning in humans and networks remains unclear-mainly whether networks can serve as suitable models for human learning and whether human-inspired curricula can improve network training. Identifying similarities and differences may help provide a deeper understanding of human cognition by examining it through neural network architectures. Conversely, these comparisons can inform the design of networks to more closely mimic human cognition <ref type="bibr" target="#b6">(Dekker et al., 2022;</ref><ref type="bibr" target="#b7">Flesch et al., 2018;</ref><ref type="bibr" target="#b9">Lake &amp; Baroni, 2023)</ref>. However, despite the application of curriculum learning in both (e.g., <ref type="bibr" target="#b1">Anderson et al., 1995;</ref><ref type="bibr" target="#b28">Wang et al., 2021)</ref>, a direct comparison of curriculum learning between humans and networks in qualitatively similar tasks remains largely unexplored-especially in the context of single-task learning. Here, we systematically investigate the effects of different curricula during perceptual learning in humans and a neural network.</p><p>Prior studies comparing human and neural network learning have focused on their similarities and differences concerning sequential acquisition of multiple tasks. For example, <ref type="bibr" target="#b7">Flesch et al. (2018)</ref> investigated continual learning-learning multiple tasks in sequence-in humans and networks. Their behavioral findings suggest that humans learn categorization tasks more effectively when training trials are grouped into task blocks instead of interleaving tasks. In contrast, their simulations indicate that networks perform better when interleaving tasks and that they suffer catastrophic forgetting when training trials are blocked. Similarly, <ref type="bibr" target="#b6">Dekker et al. (2022)</ref> observed that humans learn new tasks remarkably quickly by generalizing knowledge to new settings, while standard networks fail to do so. However, the CURRICULUM LEARNING 4 authors also reported that more advanced neural networks-especially those trained with structured learning approaches-can achieve human-like performance (for another example, see <ref type="bibr" target="#b9">Lake and Baroni, 2023)</ref>. While these studies have mainly highlighted differences in learning between humans and neural networks, they have primarily focused on the sequential acquisition of multiple tasks.</p><p>In contrast, studies considering perceptual learning typically examined training curricula within a single task. For instance, <ref type="bibr" target="#b5">Church et al. (2013)</ref> demonstrated that humans performed better on a hard auditory perceptual discrimination task when previously trained on trials with progressively increasing difficulty compared to trials with decreasing or consistently hard difficulty throughout the training. Similarly, <ref type="bibr" target="#b16">Mannelli et al. (2024)</ref> found that parsimonious neural networks learned a perceptual discrimination task more efficiently when trained with progressively increasing difficulty rather than consistently hard or random difficulty. 1</p><p>Studies on curriculum learning typically use training regimes that include ascending, hard, or random 2 curricula. After training, researchers generally assess the performance in a testing phase consisting exclusively of hard trials. (e.g., <ref type="bibr" target="#b5">Church et al., 2013;</ref><ref type="bibr" target="#b16">Mannelli et al., 2024</ref>). An ascending curriculum structures learning by gradually increasing the task difficulty, while a hard curriculum examines an alternative hypothesis that training on trials with consistently hard difficulty, matching that used in testing, maximizes learning efficiency.</p><p>Random difficulty curricula serve to control for the alternative account that not progressively increasing difficulty but rather variability in difficulty maximizes learning efficiency.</p><p>Comparisons between these conditions help determine whether progressively increasing task difficulty uniquely enhances learning or if alternative approaches yield similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The present study</head><p>In this study, we investigated curriculum learning in a perceptual decision-making task for both humans and parsimonious neural networks. Inspired by the modeling work of <ref type="bibr" target="#b16">Mannelli et al. (2024)</ref>, we designed corresponding tasks for both a neural network and 1 Curriculum learning did not significantly improve the training of a deep neural network in their study.</p><p>2 Matching the difficulty trials of the ascending curriculum, presented in random or descending order. CURRICULUM LEARNING 5 humans, each requiring integration across two feature dimensions, with the added complexity of noisy feature values in both dimensions. Crucially, we structured the tasks so that the features had to be integrated following an XOR rule (see Method sections). This design allowed us to examine curriculum learning by manipulating feature noise (e.g., reducing noise over time in an ascending curriculum). More importantly, this design also allow for a bad curriculum through blocked learning, where feature pairings were systematically grouped, restricting exposure to certain feature combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curriculum Learning in Neural Networks</head><p>To examine the similarity between human and neural network perceptual learning, we conducted numerical experiments to determine whether networks' predictions generalize to human behavior.</p><p>Specifically, we compared how four training curricula affect the accuracy of the same network architecture with different weights initializations. Based on previous theory on the beneficial effects of an ascending curriculum (e.g., <ref type="bibr" target="#b16">Mannelli et al., 2024)</ref>, we expected networks trained on an ascending curriculum to outperform random and hard ordering. This also allowed us to design a "bad curriculum" whose objective was to disrupt performance in the network-see the Stimuli section below.</p><p>In addition to examining performance on test trials, we analyzed the training trajectory.</p><p>We split the neural network population into high-achieving networks exceeding 65% accuracy and low-achieving networks falling below this threshold. This allowed us to examine whether overall performance differences were due to curricula affecting all networks similarly-essentially shifting the entire performance distribution while maintaining its shape-or whether curricula altered the distribution itself, impacting the proportion of high-achieving networks. Finally, as an additional exploratory analysis, we examined the long-term impact of initial training conditions by exposing neural networks to extended training following the initial curriculum training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURRICULUM LEARNING 6</head><p>Method Architecture Similar to <ref type="bibr" target="#b16">Mannelli et al. (2024)</ref>, the neural networks consisted of two layers with K = 4 hidden units and a ReLU activation function. We trained these networks using binary cross-entropy loss and online stochastic gradient descent updates. With K = 4, the network is capable of parsimoniously solving an XOR task in its optimal weight configuration <ref type="bibr" target="#b4">(Ben Arous et al., 2022;</ref><ref type="bibr" target="#b16">Mannelli et al., 2024;</ref><ref type="bibr" target="#b22">Refinetti et al., 2021)</ref>. We accounted for individual variability by initializing the network weights drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>As the perceptual discrimination task, we used an XOR Gaussian-Mixture model <ref type="bibr" target="#b22">(Refinetti et al., 2021)</ref>, where inputs are sampled from four Gaussian distributions whose means (±µ y ) are arranged to form an XOR, and the XOR rule determines the labels. Formally, given a label from y ∼ Bern 1 2 , x ∼ 1 orthogonal unit vectors. Large standard deviation (σ) creates less distinct Gaussians, making the task harder.</p><p>We compared four training curricula-ascending, hard, random, and bad-and assessed accuracy in a testing phase of exclusively hard trials. In the training trials, the standard deviation (and thus the difficulty levels) ranged from σ = 0.1 (easy) to σ = 0.65 (hard), except for the hard curriculum where we fixed σ at 0.65. For the ascending curriculum, σ linearly decreased to increase difficulty. We used the same difficulty levels as the ascending curriculum in the random curriculum but presented them in a randomized order. In the bad curriculum, we presented trials in random order of difficulty, but we manipulated the presentation to show only a subset of labels ±1 at the beginning and the other subset ±1 at the end. Throughout training the probability of observing the initial subset linearly decreased, while the probability of observing the other increased. This design limited early exposure to the complete set of feature pairings, making it harder to generalize the XOR-based response strategy across the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURRICULUM LEARNING 7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Building on the work of previous studies <ref type="bibr">(cf. Ben Arous et al., 2022;</ref><ref type="bibr" target="#b22">Refinetti et al., 2021;</ref><ref type="bibr" target="#b25">Sarao Mannelli et al., 2024)</ref>, we employed a mean-field approach from statistical physics to characterize the learning dynamics. This approach reduced the stochasticity of learning to the initialization phase and allowed us to analytically evaluate the learning trajectory without requiring numerical simulations. This provided a fast and precise characterization of the learning process. After training for 10 time units with a learning rate of 10, where time in the mean-field approach is given by epoch/input dimension, we evaluated the accuracy exactly using the mean-field solution of the problem. We ran 10 ′ 000 simulations for each curriculum to characterize the accuracy distribution. To explore long-term impact, we trained the neural networks on additional hard trials up to 100, 1 ′ 000, and 10 ′ 000 time units after the initial 10 time units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>Given this large sample size of 10 ′ 000 for each condition, we used Cohen's d to assess statistically significant differences between curricula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing</head><p>Figure 1 depicts the accuracy of neural networks in test trials. The ascending curriculum showed on average the highest accuracy, and reported a large and statistically significant effect against the other curricula. Specifically, the absolute value of Cohen's d was 0.98 for the ascending curriculum against the random curriculum, 2.5 against the hard curriculum, and 2.1 against the bad curriculum. The random curriculum also demonstrated a large and statistically significant effect size, with values of 1.5 against the hard curriculum and 1.1 against the bad curriculum, performing better than the hard and bad curriculum but worse than ascending. The smallest, yet significant, effect size of 0.90 was observed between the hard and bad curricula, indicating relatively higher accuracy for the bad curriculum. Accuracy on test trials for neural networks trained on four different training curricula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>The upper panel of <ref type="figure">Figure 2</ref> depicts the accuracy of neural networks in training trials, while the lower panel depicts the training trajectories when splitting the networks into a high-achieving group (above 65% accuracy) and low-achieving group (below 65%) based on the accuracy of the final training time unit. Since trial difficulty varied between curricula in the training but not in the test trials, we also report the proportion of networks above threshold performance: In the ascending curriculum, 11.62% of networks performed above the threshold in the final training trials, and 11.46% did so in the test trials. In contrast, none of the networks trained with the hard curriculum surpassed the threshold in either training or test trials. For the random curriculum, 18.96% of networks exceeded the threshold in the final training trials but only 1.07% in the test trials. In the bad curriculum, 99.97% of networks surpassed the threshold in the final training trials, yet none did so in the test trials.</p><p>Long-term Impact ascending curriculum further improves accuracy, networks initially trained with a hard, random, or bad curriculum do not show accuracy gains. However, while this trend persists for 1, 000 trials, after 10, 000 trials, networks initially trained with a hard or random curriculum eventually catch up and reach similar accuracy levels to networks trained on an ascending curriculum. In contrast, networks initially trained with a bad curriculum continue to underperform, failing to reach accuracy above chance even after receiving 10, 000 trials. Testing accuracy when adding additional hard trials to the initial curriculum training. Note, the first group of violin plots reproduces the results shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion: Neural Networks</head><p>Altogether, the neural network simulations suggest that training with an ascending curriculum is the most effective approach for learning a perceptual decision-making task.</p><p>Our analysis of training trajectories and the split between high-and low-achieving networks in the test phase shows that ascending curricula increase the proportion of high-achieving networks compared to the other conditions. These findings align with prior studies; for instance, Sarao <ref type="bibr" target="#b16">Mannelli et al. (2024)</ref> demonstrated that an ascending curriculum expands the proportion of networks that lead to high performance.</p><p>Furthermore, our analysis identifies discrepancies between the proportion of high-achieving networks at the end of training and in the test phase for the random and bad curricula, whereas such discrepancies are absent in the ascending and hard curricula. This effect is particularly pronounced in the bad curriculum. These findings provide insight into the underlying mechanisms determining whether networks successfully acquire the task.</p><p>The results of the long-term impact analysis indicate that for the chosen difficulty level the network can successfully learn the XOR task when provided with sufficient training data, even under the hard curriculum. However, the detrimental effects of exposure to the bad curriculum appear irreversible, suggesting that the network becomes trapped in an attractor CURRICULUM LEARNING 11 state induced by learning only one subset of the task, preventing successful generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curriculum Learning in Human Participants</head><p>Like the neural network simulations, we conducted a between-group web-based experiment to examine whether the same four training curricula affected participants' accuracy on subsequent hard test trials. The test trials were identical across training curricula.</p><p>Based on the neural network simulations, we expected the ascending group to outperform the hard and random groups, as reflected in significantly higher accuracy. Additionally, we anticipated higher accuracy in the random group compared to the hard group during testing, also based on the network simulations. Regarding the bad curriculum, although previous empirical work suggests that blocked learning benefits learning in humans <ref type="bibr" target="#b7">(Flesch et al., 2018)</ref>, our network simulations predicted lower accuracy for this group, leading us to expect weaker performance compared to the other conditions.</p><p>In addition to examining test performance, we analyzed the training trajectory of human participants, similar to our approach with neural networks. We also categorized participants into high-achieving-above 65% accuracy-and low-achieving-below 65%-groups to investigate whether overall performance differences were primarily driven by curricula influencing all individuals similarly or whether curricula instead affected the distribution of participants who performed well versus poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We collected data from 200 participants (mean age = 23.1) through an online experiment administered on Prolific. We implemented within-counterbalancing using SweetPea <ref type="bibr" target="#b19">(Musslick et al., 2020)</ref> and automated the experiment execution via AutoRA <ref type="bibr" target="#b18">(Musslick et al., 2024)</ref>. All participants were between 18 and 45 years old and consented to participate. The experiment lasted five minutes on average. The data was partially collected at Brown University, Providence, USA, where the study received approval from the Institutional Review Board, and at Martin-Luther University, Halle-Wittenberg, Germany, where the study was also in accordance with ethical guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURRICULUM LEARNING 12</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>We exposed participants to a perceptual decision-making task in which they had to identify the majority motion direction or majority color of dots in a random-dot-motion kinematograms (RDK). The task followed an XOR rule, meaning responses depended on the conjunction of these features-for example, "yellow-up" and "blue-down" required the same response, while "yellow-down" and "blue-up" required a different response. We used 'F' and 'J' as response keys and instructed participants to use the left and right index fingers. The key-response mapping was counterbalanced between participants.</p><p>We administered RDK trials using the rdk-plugin <ref type="bibr" target="#b21">(Rajananda et al., 2017)</ref> and always presented 600 dots during both training and testing. The dot radius was set to 2 pixels, and the moving distance was set to 1 pixel per frame. To manipulate task difficulty during training, motion coherence ranged from 40% to 100%, indicating the percentage of the dots moving coherently in the target direction instead of in random directions. The color coherence ranged from 66% to 100%, indicating the percentage of dots colored with the target color as opposed to the opposite color. Notably, color coherence was not systematically manipulated to create curricula but counterbalanced across trials. We chose this task as coherence decreases typically affect humans' performance in the task, leading to increased error rates and thus increasing the difficulty of the task <ref type="bibr" target="#b2">(Baker et al., 1991;</ref><ref type="bibr" target="#b10">Lankheet &amp; Verstraten, 1995;</ref><ref type="bibr" target="#b27">Strittmatter et al., 2023</ref><ref type="bibr" target="#b26">Strittmatter et al., , 2024</ref>.</p><p>The difference between the four training curricula was the sequence in which the trials were presented. Similar to the noise in the neural networks training, we manipulated the sequence of motion coherence in the ascending, hard, and random curricula. In the ascending curriculum, we decreased the motion coherence linearly from 100% to 40% coherence. For the hard curriculum, we set the coherence to 40% throughout the trials, and in the random curriculum, the motion coherence was the same as in the ascending but randomly ordered. In the bad curriculum, the trials again had the same motion coherence as in the ascending condition, but the feature pairing was blocked. For example, with the response-key mapping "yellow-up" and "blue-down" to 'F' and "yellow-down" and "blue-up" to 'J', the first half of trials contained the pairings "yellow-up" and "yellow-down" (or "yellow-up" and "blue-up"), CURRICULUM LEARNING 13 while the second half contained the pairings "blue-up" and "blue-down" (or "yellow-down" and "blue-down", respectively). Thus keeping one feature dimension constant in each subset parallel to the procedure described for the neural networks. All possible feature subsets and response-key mapping were counterbalanced between participants. Additionally, as in the training of the neural network, the proportion of trials from one subset decreased, while the proportion of trials from the other subset correspondingly increased over the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>We randomly assigned participants to four different training curricula, with 50 participants per curriculum, while all participants responded to the same test trials. We instructed them to perform the task as accurately as possible on each trial and that they could gain a bonus of 0.02$ dollar per trial for accurate performance during the test trials. Next, the participants had to respond to the colored RDKs. Importantly, the experiment comprised two phases: a training phase and a testing phase. We presented 100 training trials and 16 test trials to participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>We conducted the data analysis in R (R Core Team et al., 2013). We first evaluated whether the different training curricula affected participants' accuracy and RT. To estimate differences in accuracy between curricula during testing, we ran a hierarchical logistic regression model using the lme4 package <ref type="bibr" target="#b3">(Bates et al., 2014)</ref>, with the factor curricula as the independent variable and participants' accuracy as the dependent variable. A random intercept was applied to control for the overall variability in accuracy between the participants. We did not apply a random slope term as the model with a random slope term did not converge. We used the emmeans package to evaluate the pairwise comparisons between all curricula <ref type="bibr" target="#b13">(Lenth et al., 2018)</ref>. Finally, and as with the neural network simulations, we illustrated participants' performance trajectory on their training sequences. <ref type="figure">Figure 4</ref> depicts the accuracy of human participants in the test trials. We observed that participants were significantly more accurate during testing if they were previously exposed to an ascending curriculum, compared to a hard, random, or bad curriculum; ascending vs. hard: beta = .65; z = 3.58; p &lt;.001; ascending vs. random: beta = .49; z = 2.69; p = .007; ascending vs. bad: beta = .857; z = 4.72; p &lt;.001). Participants with the hard curriculum did not significantly differ in accuracy during testing compared to the random and to the bad curriculum (hard vs. random: beta = -.16; z = -.89; p = .373; hard vs. bad: beta = .21; z = 1.16; p = .244). Participants with the random curriculum achieved significantly higher accuracies during testing than the bad curriculum (random vs. bad: beta = .37; z = 2.05; p = .040).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURRICULUM LEARNING 14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Accuracy on hard test trials for human participants previously training on four different curricula. illustrates the average accuracy for every bin and for each curriculum. Similar to the procedure for the neural networks, we then split the participants into a high-achieving group (above 65% accuracy) and a low-achieving group (below 65%). The lower panel of <ref type="figure" target="#fig_3">Figure 5</ref> depicts the training trajectories when splitting the groups based on the accuracy of the final 20 training trials. Note, here we also report the proportion of participants that reached above threshold accuracy on test trials. Most participants in the ascending curriculum exceeded the accuracy threshold in the final 20 training trials (72% of the group) and testing (60%). In the hard curriculum, only 24% met the threshold during training and 30% during testing. In the random curriculum, 46% reached the threshold in training and 40% in testing. The bad curriculum had the second-highest training accuracy during the final 20 trials (63%) but the fewest participants who performed above the threshold during testing (16%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion: Human Participants</head><p>The behavioral results from human participants qualitatively align with the findings from the neural network simulations. Notably, we not only replicated the superior performance of participants exposed to the ascending curriculum compared to those in the hard and random curricula but also observed similarly poor performance in the bad curriculum. This similarity extends beyond average trends, as both humans and networks exhibited comparable splits into high-and low-achieving groups.</p><p>As in the network simulations, discrepancies in the proportions of high-achieving participants between the final training trials and the test trials emerged in the bad and random curricula but not in the ascending and hard curricula, demonstrating strikingly similar patterns.</p><p>These findings suggest that the underlying mechanisms driving failure in the bad curriculum were comparable in humans and networks-specifically, participants performed well on one feature subset while failing to retain the other. However, it is important to note that, unlike in networks, we did not assess the long-term impact of training on humans. This remains an open question for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>We conducted this study to evaluate potential similarities and differences between neural networks and humans in curriculum learning when learning on a single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURRICULUM LEARNING 16</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>Participants' average accuracy for each curriculum condition as a function of training trials, with 20 trials per bin (a) and 4 trials per bin (b).</p><p>Specifically, we first trained neural networks on four different curricula-ascending, hard, random, and bad-and assessed their performance during a subsequent hard testing phase. We then applied the same curricula to human participants to investigate whether similar patterns emerged.</p><p>Our results indicate that both humans and networks benefited from an ascending curriculum compared to the hard and random conditions. As expected, the bad curriculum, a blocked design meant to hinder learning for neural networks (cf. <ref type="bibr" target="#b7">Flesch et al., 2018)</ref>, led to poor network performance. Interestingly, contrary to previous studies suggesting that blocked learning benefits human learning (e.g., <ref type="bibr" target="#b7">Flesch et al., 2018)</ref>, in our task design, humans also performed poorly on this curriculum.</p><p>A key distinction between our study and previous research on blocked learning is the nature of the tasks being acquired. Prior studies typically examine the learning of multiple independent tasks or stimulus sets presented in a blocked fashion. In contrast, while our XOR task may superficially appear to involve learning two separate tasks, successful performance requires the integration of both task components rather than treating them as independent units. As a result, the structured benefits of a blocked design, which typically facilitate the acquisition of distinct tasks, might not emerge in this context. Importantly, analyzing training performance in relation to test performance allowed us to generate new hypotheses regarding the factors that contribute to success or failure in both humans and neural networks. First, our findings indicate that differences in learning outcomes are at least partially driven by shifts in the proportion of high-achieving versus low-achieving learners, replicating results from <ref type="bibr">Mannelli et al., 2024. Second, by examining discrepancies</ref> between training and test performance-most pronounced in the bad curriculum-we observed that humans exhibit similar patterns of catastrophic forgetting as neural networks.</p><p>However, while we demonstrated that the detrimental effects of the bad curriculum persist over time in neural networks, we did not conduct a corresponding long-term experiment for human participants. Investigating this further would be a valuable future direction, as it remains possible that introducing even a small number of interleaved trials after blocked training in the XOR task could mitigate or even reverse the negative impact on learning outcomes.</p><p>A notable limitation of this study is that, while we applied neural network-inspired curricula to human participants, we did not explicitly model the neural networks to align with human behavior. Additionally, we do not provide a mechanistic explanation for why either CURRICULUM LEARNING 18 humans or networks exhibit the observed learning patterns. Mannelli et al., 2024 offers a starting point for such an investigation by demonstrating that the effects of curriculum learning emerge only in parsimonious neural networks and not in deep neural networks. Yet, describing the human brain as parsimonious is not biologically plausible, and the only inference we can confidently draw from these comparisons is that the brain must be subject to certain constraints. However, further work is required to shed light into the similarities of parsimonious neural networks and human learning.</p><p>Taken together, our findings reveal striking similarities in curriculum learning between humans and parsimonious neural networks. Our results also highlight promising directions for future research, particularly regarding the long-term implications of different curricula and the underlying mechanisms driving the observed learning effects. A deeper understanding of these mechanisms could have important implications for both educational curricula and the development of optimized training sequences for more efficient neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>depicts the accuracy in additional hard training trials followed by an initial curriculum training phase. While continued training on to 100 trials following an initial Accuracy for the neural networks during training with four different curricula. The upper panel shows violin plots for the four curricula throughout training. The lower panel shows the same curricula split between networks above and below 65% accuracy in the final training time unit. The solid line indicates the average while the shaded region around represents the standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>TrainingFigure 5</head><label>5</label><figDesc>depicts the accuracy of human participants in the training trials. Note, to reduce noise and calculate accuracy estimates on the participant level, we binned the training trials in bins of 20 trials to analyze the accuracy during training. The upper panel in Figure 5 CURRICULUM LEARNING 15</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">N (µ y , σ 2 I d ) + 1 2 N (−µ y , σ 2 I d ), with µ 0 , µ 1 ∈ R d</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The network returns three values: returning one of the two labels or zeroing out and not returning any label.This lowers chance-level accuracy by allowing the network to forgo an answer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task difficulty and the specificity of perceptual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahissar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">387</biblScope>
			<biblScope unit="issue">6631</biblScope>
			<biblScope unit="page" from="401" to="406" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cognitive tutors: Lessons learned. The journal of the learning sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pelletier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="167" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Residual motion perception in a &quot;motion-blind&quot; patient, assessed with limited-lifetime random dot stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zihl</surname></persName>
		</author>
		<idno type="DOI">10.1523/jneurosci.11-02-00454.1991</idno>
		<ptr target="https://doi.org/10.1523/jneurosci.11-02-00454.1991" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="454" to="461" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fitting Linear Mixed-Effects Models using lme4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v067.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v067.i01" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-dimensional limit theorems for sgd: Effective dynamics and critical scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jagannath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25349" to="25362" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal dynamics in auditory perceptual learning: Impact of sequencing and incidental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">270</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum learning for human compositional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">2205582119</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing continual task learning in minds and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="10313" to="10322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Curriculum learning inspired by behavioral shaping trains neural networks to adopt animal-like decision making strategies. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Constantinople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Savin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human-like systematic generalization through a meta-learning neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="issue">7985</biblScope>
			<biblScope unit="page" from="115" to="121" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentional modulation of adaptation to two-component transparent motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lankheet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Verstraten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1401" to="1412" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/0042-6989(95)98720-T</idno>
		<ptr target="https://doi.org/10.1016/0042-6989(95)98720-T" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Why do animals need shaping? A theory of task composition and curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mannelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S0DPCE7tt4" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-07-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emmeans: Estimated marginal means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lenth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buerkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AKA least-squares means</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The easy-to-hard effect in human (homo sapiens) and rat (rattus norvegicus) auditory identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Orduña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Psychology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arithmetic value representation for hierarchical behavior composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Makino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tilting the odds at the lottery: The interplay of overparameterisation and curricula in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mannelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ivashynka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2024</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">114001</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suret</surname></persName>
		</author>
		<title level="m">Transfer along a continuum: Differentiation or association? Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autora: Automated research assistant for closed-loop empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Strittmatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">104</biblScope>
			<biblScope unit="page">6839</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sweetpea: A standard language for factorial experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherkaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Darragh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>R Core Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A random-dot kinematogram for web-based vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajananda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Odegaard</surname></persName>
		</author>
		<idno type="DOI">10.5334/jors.194</idno>
		<ptr target="https://doi.org/DOI:10.5334/jors.194" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Research Software</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Refinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8936" to="8947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The easy-to-hard training advantage with real-world medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Roads</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analytical theory of curriculum learning in teacher-student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21113" to="21127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tilting the odds at the lottery: The interplay of overparameterisation and curricula in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarao Mannelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ivashynka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A jspsych touchscreen extension for behavioral research on touch-enabled interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Strittmatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ging-Jehli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="7814" to="7830" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A random-object-kinematogram plugin for web-based research: Implementing oriented objects enables varying coherence levels and stimulus congruency levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Strittmatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W H</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="883" to="898" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="4555" to="4576" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
