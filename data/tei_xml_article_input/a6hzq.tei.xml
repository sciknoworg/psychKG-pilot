<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pseudo-learning effects in reinforcement learning model-based analysis: A problem of misspecification of initial preference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Katahira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Graduate School of Environment</orgName>
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Graduate School of Environment</orgName>
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of literature and law</orgName>
								<orgName type="institution">Communication University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Nakao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Graduate School of Education</orgName>
								<orgName type="institution">Hiroshima University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pseudo-learning effects in reinforcement learning model-based analysis: A problem of misspecification of initial preference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>reinforcement learning</term>
					<term>model-based analysis</term>
					<term>statistical artifact</term>
					<term>decision-making</term>
					<term>preference</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this study, we investigate a methodological problem of reinforcement-learning (RL) modelbased analysis of choice behavior. We show that misspecification of the initial preference of subjects can significantly affect the parameter estimates, model selection, and conclusions of an analysis. This problem can be considered to be an extension of the methodological flaw in the free-choice paradigm (FCP), which has been controversial in studies of decision making. To illustrate the problem, we conducted simulations of a hypothetical reward-based choice experiment. The simulation shows that the RL model-based analysis reports an apparent preference change if hypothetical subjects prefer one option from the beginning, even when they do not change their preferences (i.e., via learning). We discuss possible solutions for this problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Reinforcement-learning (RL) model-based trial-by-trial analysis is an important tool for analyzing data from decision-making experiments that involve learning <ref type="bibr" target="#b7">(Corrado and Doya, 2007;</ref><ref type="bibr" target="#b8">Daw, 2011;</ref><ref type="bibr" target="#b20">O'Doherty, Hampton, and Kim, 2007)</ref>. One purpose of this type of analysis is to estimate latent variables (e.g., action values and reward prediction error) that underlie computational processes. Estimates of these variables are correlated with neural signals, allowing brain regions that represent variables to be identified. Another purpose is to characterize individual subjects by using model parameter estimates. These parameter estimates are related to physiological/personality characteristics or neuronal/mental deficits in individuals <ref type="bibr" target="#b23">(Yechiam, Busemeyer, Stout, and Bechara, 2005;</ref><ref type="bibr" target="#b16">Kunisato et al., 2012;</ref><ref type="bibr" target="#b9">Huys, Pizzagalli, Bogdan, and Dayan, 2013)</ref>. Originally, RL model-based analysis was developed for the reward-or punishment-based learning paradigm. However, researchers have begun to apply RL model-based analysis to various other contexts, including the cognitive-control paradigm <ref type="bibr">(Mars, Shea, Kolling, and Rushworth, 2012)</ref>, the effect of self-choice on subsequent choices in perceptual decision-making <ref type="bibr">(Akaishi, Umeda, Nagase, and Sakai, 2014)</ref>, and free choice <ref type="bibr" target="#b6">(Cockburn, Collins, and Frank, 2014)</ref>.</p><p>In another context, the validity of the traditional analysis of the free-choice paradigm <ref type="bibr">(FCP)</ref> has become controversial <ref type="bibr" target="#b5">(Chen and Risen, 2010;</ref><ref type="bibr" target="#b11">Izuma and Murayama, 2013;</ref><ref type="bibr" target="#b1">Alós-ferrer and Shi, 2015)</ref>. Researchers in this field have repeatedly demonstrated that choice induces preference change by showing that preference rating increases for chosen items and decreases for items that were not chosen <ref type="bibr" target="#b3">(Brehm, 1956)</ref>. This tendency has been explained by cognitive dissonance theory, which postulates that choice modulates decision-makers' preferences to preserve consistency. A change in preference is quantified by positive spreading, that is, rating the difference between two items is increased after choice, whereas a chosen item is more preferred, and the item that is not chosen is less preferred. However, <ref type="bibr" target="#b5">Chen and Risen (2010)</ref> noted that this positive spread is possible due to certain statistical biases, even if a preference remains stable. This flaw is caused by the misspecification of true preference. When the initial preference of two options was deemed to be neutral despite their difference, subjects tended to choose the more preferred option the second time, which caused an apparent "choice-induced preference change". <ref type="bibr">Izuma and Murayama (2010)</ref> explained this problem by conducting computer simulations. Alós-ferrer and Shi (2010) corrected the proof of <ref type="bibr" target="#b5">Chen and Risen (2010)</ref> and clarified the limitation of the problem.</p><p>The objective of this study is to prove that this methodological flaw can arise in RL modelbased analyses in value-based decision-making studies. To achieve this objective, we conducted simulations that considered a hypothetical and reward-based choice experiment. Standard RL models compute the values of possible actions and do not directly represent preference. However, we can consider that the models represent preference for an option if a higher value is assigned to a given option compared to other options. The simulation shows that the RL model-based analysis reports an apparent preference change even when the hypothetical subject does not have any preference change (i.e., via learning) and even if the actual subject prefers one option. We refer to this effect as "the pseudo-learning effect" and show that it affects the model parameter estimates, model selection, and conclusions of an analysis. We also discuss certain possible solutions for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The objective of the simulations that were performed in this study is to illustrate the situation 5 in which the pseudo-learning effect occurs. When the RL model is fitted to data for the case in which the preference is biased toward one option from the beginning of the experiment, learning appears to occur even though the real preference does not change (i.e., the learning does not occur).</p><p>To clarify the mechanism of this phenomenon, we consider simple simulation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothetical data generation</head><p>We consider a simple probabilistic learning task, in which two options exist. One hundred hypothetical subjects perform 100 choice trials. We assumed that all subjects prefer option 1 without loss of generality. We also assume that the subjects select option 1 with a probability of 80% and select option 2 with a probability of 20% independent of their experiences (e.g., reward and choice histories). Thus, the hypothetical subjects do not change preference throughout the experiment (i.e., learning does not occur).</p><p>The reward schedule was established as follows. In Case 1, option 1 is associated with a reward with a probability of pr during the entire experiment, and option 2 is associated with a reward with a probability of 1 -pr, i.e., reward probabilities are symmetric between two options.</p><p>In Case 2, the reward probability pr is shared between both options. In the simulations, we varied pr from 0.05 to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We primarily consider a typical RL model-the Q-learning model <ref type="bibr" target="#b22">(Watkins and Dayan, 1992</ref>)-which is the most commonly employed model for the model-based analysis of choice behavior [Footnote 1]. The model assigns the value ( ) to each action, where i is the index of the action, and t is the index of the trial. The initial Q values are 1 (1) = 1 0 , and 2 (1) = 2 0 .</p><p>In a common setting, the initial action values are set to zero (i.e., 1 0 = 1 0 = 0), and both options have a neutral value prior to the experiment. Based on the outcome of a decision, the action value for action i is updated as</p><formula xml:id="formula_0">( + 1) = ( ) + ( ( ) − ( )),</formula><p>where is the learning rate that determines how much the model updates the action value depending on the reward prediction error ( ) − ( ). The learning rate is restricted to the range between 0 and 1. In a common RL-model based analysis, the action value of the option that is not chosen is not typically updated; however, an exception and related discussions are noted by <ref type="bibr" target="#b10">Ito and Doya (2009)</ref> and <ref type="bibr" target="#b13">Katahira (2015)</ref>.</p><p>Based on the set of action values, the probability of choosing option i during trial t is given by the softmax function:</p><formula xml:id="formula_1">( ( ) = ) = exp( • ( )) ∑ exp( • ( )) =1 ,</formula><p>where β is the inverse temperature parameter that determines the sensitivity of the choice probabilities to differences in the values, and K represents the number of possible actions. In this study, K = 2.</p><p>The primary model that is considered in this study is the Q-learning model that has initial Q values that are set to zero (i.e., 1 0 = 0, 2 0 = 0). In this model, the free parameters are the learning rate and the inverse temperature . In addition to this model, we consider two null models, which have no learning mechanisms. The first null model, which is the random-choice model, chooses options with equal probabilities for all options (i.e., the model selects both options with a probability of 0.5). This random-choice model does not include a free parameter. We show that the analysis produces an incorrect conclusion due to the pseudo-learning effect if only this random-choice model is compared to the Q-learning model. The second null model, which is the biased choice model, includes the free parameter . This model chooses option 1 with a probability of and option 2 with a probability of 1 − . The second null model (i.e., the biased choice model) can include the "true model" for the proposed hypothetical subject with = 0.8 (i.e., it can have a constant preference for option 1). This null model was included in the simulation to show that the pseudo-learning effect does not manifest if a researcher included the appropriate null model in the model comparison. The first null model corresponds to a special case of the Q-learning model, where = 0, or equivalently, = 0 and 1 0 = 2 0 . The second null model corresponds to a special case in which = 0, 1 0 ≠ 2 0 and has been appropriately scaled.</p><p>To estimate the model parameters, maximum likelihood estimation (MLE) was separately performed for each subject. MLE searches a single parameter set to maximize the log-likelihood of the model for all trials. This maximization was performed using the LBFGS algorithm that was </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model evaluation</head><p>To evaluate the models, we computed three criteria including the log Bayes factor, pseudor 2 , and the p-value of the likelihood ratio test; these parameters have been employed for the modelbased analysis of choice data. First, we compare the primary model with the two null models using the Bayes factor (BF), which is a ratio of the marginal likelihood of two models <ref type="bibr" target="#b12">(Kass and Raftery, 1995)</ref>. The log marginal likelihood of model i is approximated using the Bayesian information criterion (BIC), which is given by = −2 + log ,</p><p>where l is the log likelihood for the parameter set obtained using MLE, k is the number of free parameters of model i, and T is the number of trials (i.e., T = 100). Given the BIC, the Bayes factors of model i and model j can be approximated as</p><formula xml:id="formula_2">= (Data|Model ) (Data|Model ) ≈ exp (− − 2 ) .</formula><p>When BFij is greater than 1, model i is favored; when BFij is less than 1, model j is favored. We applied the natural logarithm of the Bayes factor. A guideline suggests that the log Bayes factor that exceeds three implies "strong" evidence of model i against model j <ref type="bibr" target="#b12">(Kass and Raftery, 1995)</ref>.</p><p>The pseudo-r 2 is a standardized measure of how a model fits a given dataset <ref type="bibr" target="#b4">(Camerer and Ho, 1999;</ref><ref type="bibr" target="#b8">Daw, 2011)</ref> and quantifies the relative degree of improvement in predicting choice over pure chance. The pseudo-r 2 is given by (R − L) / R = 1 − L / R, where R is assumed to be the log data likelihood of chance (i.e., the random choice model; 100 × log(0.5) for the proposed case), and L is assumed to be the log likelihood of the fit model. In the simulation settings in this study, the best model is the biased model, whose bias parameter is = 0.8. The expected pseudor 2 for this "true" model is 1 -(0.8 • log(0.8) + 0.2 • log(0.2)) / log(0.5) = 0.278.</p><p>We also performed a classical hypothesis test (i.e., the likelihood-ratio test). We compared the log likelihood of the null model, which is denoted by , with the log likelihood of the alternative model, which is denoted by . The null hypothesis is that the improvement in the likelihood of more complex models relative to simpler models occurred by chance. Under the null hypothesis, the likelihood-ratio statistic D = − 2 ( − ) is known to obey the chi-square distribution, in which the degree of freedom is the difference in the number of model parameters between the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>First, we illustrate how the pseudo-learning effect arises. <ref type="figure">Figure 1</ref> shows typical choice traces in Case 1. <ref type="figure">Figure 1A</ref> describes the case in which the reward probability associated with option 1 (i.e., pr) is 0.7, and option 2 is associated with the reward probability of 0.3. <ref type="figure">Figure 1B</ref> describes the case in which pr = 0.3. The prediction of the Q-learning model for the propensity to choose option 1 (i.e., bold lines in the upper panels) approaches 0.8 (i.e., the broken line in the figure), which is the true preference bias of the data generation model. This effect is identified as the pseudo-learning effect because the preference (i.e., the tendency to choose option 1) appears to change in the model behavior, even though the real preference did not change over the entire session.</p><p>These results are explained as follows. First, the action values (i.e., Q-values) in the Qlearning model approach the expected reward value from this option after a sufficient number of trials if the learning rate is sufficiently small. Thus, if the Q-value is near the expected reward value, the average of the reward prediction error ( ) − ( ) is near zero, and the average update of the Q-values is near zero. In the case in which the reward probability is greater for the preferred option (i.e., option 1; the expected reward value is 0.7), Q1 becomes larger than Q2</p><p>( <ref type="figure">Figure 1A, bottom panel)</ref>. In this situation, the model can attain the given preference if the inverse temperature is appropriately tuned. The MLE, which maximizes the log data likelihood of the model, performs this tuning.</p><p>Conversely, when the reward probability is smaller for the preferred option compared with the less preferred option (e.g., the condition in <ref type="figure">Figure 1B</ref>), Q1 becomes smaller than Q2 after a sufficient number of trials; this situation appears after a greater number of more trials than shown 10 in <ref type="figure">Figure 1B</ref>. The model cannot easily represent the preference of option 1 in this situation.</p><p>However, during the transient phase, in which the Q-values attain the expected rewards and the expected reward values are 0.3 for option 1 and 0.7 for option 2, as shown in <ref type="figure">Figure 1B</ref>, Q1 may be large because the model experiences the outcome from option 1. Thus, Q1 increases at a faster rate than Q2, and the smaller is the learning rate α, the longer is this transient phase. Thus, with a sufficiently small learning rate and a sufficiently large inverse temperature, the model can accurately represent the preference for option 1 during all trials. This situation is possible with a smaller learning rate, which enables the transient phase to cover all trials; the MLE selects this parameter set ( <ref type="figure">Figure 1B</ref>). <ref type="figure" target="#fig_2">Figure 2</ref> shows the results of Case 1 with the reward probability systematically varied. The log BF quantifies how a model is favored over another model. <ref type="figure" target="#fig_2">Figure 2A</ref> shows the log BF of the Q-learning model over the random choice model (i.e., the null model). When the reward probability is small, the random choice model is favored because it provides a simpler explanation of the data. However, when the reward probability exceeds 0.2, which enables the Q-learning model to represent the preference for option 1 by the mechanisms shown in <ref type="figure">Figure 1</ref>, the log BF exceeds 10 and the posterior probability of the Q-learning model is 20 times larger than the posterior probability of the random choice model. These results may cause a researcher to conclude that the property of the choice behavior can be explained by the Q-learning model better instead of by chance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of reward probabilities in Case 1</head><p>The likelihood-ratio test produces a similar conclusion ( <ref type="figure" target="#fig_2">Figure 2B</ref>) when examining the null hypothesis that the improvement of the likelihood by including model parameters (e.g., the learning rate and the inverse temperature) only occurs by chance. When the reward probability trends away from 0.2, the p-value decreases, and when the reward probability exceeds 0.4, the majority of samples from the simulations become significant (p &lt; .05; <ref type="figure" target="#fig_2">Figure 2A</ref>). Pseudo-r 2 , which is a commonly employed measure for the goodness of fit of RL models, shows similar tendencies ( <ref type="figure" target="#fig_2">Figure 2D</ref>) because pseudo-r 2 quantifies the improvement in the log likelihood compared with the random-choice model. As the reward probability for option 1 increases, the mean pseudo-r 2 approaches the theoretical upper bound of 0.278 (i.e., the broken line; see the Methods section).</p><p>Conversely, when the Q-learning model is compared with the biased choice model, in which the bias parameter can represent an inherent preference that is independent of experience, the log BF favored the biased choice model because the Q-learning model no longer yields a better fit to the data, and the biased choice model has fewer parameters ( <ref type="figure" target="#fig_2">Figure 2C</ref>).</p><p>Parameter estimates are also dependent on the reward probability. When the reward probability for option 1 is small, the Q-learning model represents the bias using the transient phase of learning. In this case, the learning rate tends to be small ( <ref type="figure" target="#fig_2">Figure 2E</ref>). Conversely, when the reward probability exceeds 0.5, the learning rate tends to increase, which enables the Q-values to approach the expected value, as shown in the example in <ref type="figure">Figure 1A</ref>. Because an excessively large learning rate can produce unstable Q-values, which reduces the likelihood, MLE produces estimates of the learning rate at an appropriate point. The estimated inverse temperature parameter shows the opposite pattern ( <ref type="figure" target="#fig_2">Figure 2F</ref>). When the reward probability for the preferred option is small, the difference between the two action values is small. Because the inverse temperature parameter is multiplied by the Q-values when computing the choice preference in the softmax function, a large inverse temperature can leverage a small difference in Q-value; a larger inverse temperature can compensate for a small difference in Q-values and accurately represent the intrinsic preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of reward probabilities in Case 2</head><p>Figure 3 shows the results when both options produce the reward with the same probability (Case 2). Although this situation is less common in reward-learning tasks, we examined this case to clarify the generality of the proposed situation. For this case, the apparent preference due to the pseudo-learning effect does not easily arise because the expected reward is common to both options. However, the pseudo-learning effect can arise in this case because the model can use the transient phase to represent the preference to option 1 by establishing a small learning rate, as shown in the example in <ref type="figure">Figure 1B</ref>. This case is not dependent on the reward probability; for all reward probabilities, the fit of the Q-learning model is better than that of the random-choice model ( <ref type="figure" target="#fig_3">Figure 3A</ref>, B, and D). Based on this explanation, the learning rate is smaller for all reward probabilities ( <ref type="figure" target="#fig_3">Figure 3E)</ref>. Conversely, the inverse temperature was large when the reward probability was small, in which the discrepancy between the difference between two action values is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The RL model-based analysis has been recognized as a useful framework for estimating internal psychological processes using experimental data. However, when the fitted model is misspecified, researchers can attain an incorrect conclusion. This pitfall of model-based analysis has been documented elsewhere <ref type="bibr" target="#b17">(Nassar and Gold, 2013)</ref> with other types of model-13 misspecification in which assumed models have a constant learning rate while the true learning rate dynamically changes. The results of the present study can be considered to be an example of this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How the pseudo-learning effect can occur in realistic situations</head><p>The problem that was investigated in this study can easily occur in real situations. The choice task that is frequently combined with RL model-based analysis employs neutral images that represent the options that subjects should choose. However, such a task is frequently used without examining the neutrality of the images. If the subjects prefer one image to others due to certain personal reasons, their preferences can affect the results of the RL model-based analysis.</p><p>The results of the simulation that was performed in this study indicates that the pseudolearning effect can arise when subjects prefer either option with commonly employed reward probabilities (e.g., 70% for the first option and 30% for the second option). This result indicates that even when the preference of subjects varied and the preferred options are balanced across subjects, the pseudo-learning effect cannot be canceled by averaging across individuals and can appear in the results of a group analysis.</p><p>This problem also arises when subjects shape their preference based on the outcome of their first choice, which describes the effect of "first impression" <ref type="bibr" target="#b21">(Shteingart, Neiman, and Loewenstein, 2013)</ref>. <ref type="bibr" target="#b21">Shteingart et al. (2013)</ref> reported that the subject choice behaviors in certain gambling tasks are best described by the Q-learning model, in which the initial action value is replaced with the reward value of the first outcome from this option. Thus, when one option produces a reasonable outcome while another option produces a poor outcome (e.g., an absence of reward), the effective initial action value of the former may be higher than the later. The problem that was investigated in this study may arise for this situation if the standard Q-learning model is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to a free-choice task</head><p>This pseudo-learning problem is closely related to the methodological flow in the FCP, which was described by <ref type="bibr" target="#b5">Chen and Risen (2010)</ref>, as we discussed in the Introduction of this paper. Studies that apply the FCP have demonstrated that choosing one option increases the preference for the chosen item, whereas subjects tend to lose their preference for the item that is not chosen.</p><p>However, Chen and Risen (2010) noted that this effect (i.e., choice-induced preference change)</p><p>can arise even when the subjects' true preferences remain the same under certain reasonable conditions. Thus, the apparent preference change may be a statistical artifact. Thus, the problem is similar to the problem of the misspecification of the initial value of the RL-model based analysis.</p><p>The preference change in the FCP can be modeled as an RL framework. If the outcome of the chosen item is always assumed to be positive (i.e., the reward probability is 1.0 for the chosen option), the choice data in the FCP can be modeled by an RL model <ref type="bibr" target="#b0">(Akaishi et al., 2013;</ref><ref type="bibr" target="#b18">Nakao et al., 2016)</ref>. Thus, the problem described in this study can be considered to be an extension of the problem described by <ref type="bibr" target="#b5">Chen and Risen (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How the pseudo-learning effect can be problematic in research</head><p>The pseudo-learning effect can be problematic in real situations for at least two reasons. First, certain psychological research questions are translated into the differences among certain model parameters (e.g., how the mood of subjects influence the learning rate; <ref type="bibr" target="#b2">Bakic, Jepma, De Raedt, and Pourtois, 2014)</ref>. However, the degree of initial preference can affect the estimation of the learning rate, as shown in the simulation results in this study. Second, other research questions assess whether subjects can learn via reinforcement and if RL models can better explain other models. When learning does not occur but a subject has a high preference for one option and RL models are only compared to unbiased choice models, as with pseudo-r 2 , the data can appear to be explained by the RL theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Possible solutions</head><p>We discuss several remedies for this problem. The problem arises because only misspecified models are employed. Thus, a principled approach is to include models that capture true intrinsic preferences. These models may include the initial values of each option as free parameters.</p><p>Alternatively, a constant term may be added to an action value to provide a choice bias. By including a bias component in a null model (i.e., the biased choice model, which has been considered in this study) and comparing it with the RL models, researchers may quantify the effect of true learning processes (e.g., <ref type="bibr" target="#b15">Katahira, Fujimura, Okanoya, and Okada, 2011)</ref>.</p><p>Basic analyses that do not rely on model fits are also important. For example, if a researcher wants to confirm that learning occurred, the primary effect of binned blocks should be statistically significant, which can be confirmed by an analysis of variance (ANOVA). In addition, when the reward probabilities are switched during a session (e.g., <ref type="bibr" target="#b15">Katahira et al., 2011;</ref><ref type="bibr" target="#b14">Katahira et al., 2014)</ref>, the choices should be significantly biased to the optimal option in each interval with constant reward probabilities.</p><p>A fundamental solution attempts to minimize the initial preference that is independent of experience; this method includes making the option (e.g., images) as neutral as possible. For example, a solution is achieved using a neutral stimulus after measuring preference via a subjective rating. However, researchers should be careful when interpreting this rating because a subjective rating does not always correspond to the effect of an image on choice behavior <ref type="bibr" target="#b15">(Katahira et al., 2011)</ref>. In addition, this rating can be noisy and cause the methodological flaw of the FCP that was reported by <ref type="bibr">Chen and Risen (2011)</ref>. Thus, careful interpretation is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Computational model-based analysis, which includes RL model-based analysis, is a valuable and powerful tool for analyzing behavioral data and identifying corresponding psychological and biological processes. However, if the model settings fail to capture the underlying characteristics of the behavior, the results may be less meaningful: The parameter estimates and the results of model selection may change due to the statistical artifact. The pseudo-learning effect due to misspecification of the initial preference, which we have discussed in the present study, may be a common scenario in which such problems occur. Researchers should be aware of the limitations of model-based analysis and carefully check the validity of the model assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Footnotes</head><p>1. The RL models considered in the present study are a special case of Q-learning where there is only one state and the delay in reward is not considered. Although they can also be regarded as special cases of SARSA, which is another type of RL, we term the present models as Q-learning according to convention. <ref type="figure">Figure 1</ref>. Examples of model fits to hypothetical biased-choice data (Case 1), in which the reward probability of option 1 is 0.7 and the reward probability of option 2 is 0.3 (A); and the reward probability of option 1 is 0.3 and the reward probability of option 2 is 0.7 (B). For both cases, the upper panels show the simulated sequences of choice and reward and model prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures</head><p>The long vertical lines represent the choice (i.e., the upper line indicates choosing option 1 and the lower indicates choosing option 2) and the outcome (i.e., red represents a rewarded trial, and black represents a non-rewarded trial). Rewarded trials are also denoted by small vertical lines.</p><p>The choice is generated from the biased choice model, in which the probability of choosing option 1 (= 0.8) is depicted by a broken horizontal line. The solid black lines indicate the probability of   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>implemented in Stan (Version 2.8.0, Stan Development Team, 2015), which was accessed from the R programming language (Version 3.2.0, R Core Team, 2015) via RStan (Version 2.8.0, Stan Development Team, 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>choosing option 1, which was computed from the fitted model. The bottom panels show the estimated time course of the action values Q1 and Q2. The estimated parameters include the learning rate ( = 0.080) and the inverse temperature ( = 3.260) for (A); and = 0.017 and = 17.34 for (B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The effect of the reward probabilities on the model evaluation and parameter estimation when the reward probabilities are symmetric between the two options (Case 1). Each dot represents a sample from a single hypothetical subject. The bold line represents the mean for each reward probability. (A): Log Bayes factor (BF) of the Q-learning model for a random unbiased model. A positive log BF value indicates that the Q-learning model is favored, and a negative value indicates that the random model is favored. (B): P-value of the likelihood ratio test, where the null hypothesis is that the improvement of the Q-learning model compared with the random-choice model only occurs by chance. (C): Log Bayes factor of the Q-learning model of the biased choice model. (D): Pseudo-r 2 . The broken line indicates the theoretical upper bound of the average pseudo-r 2 attained by the true model (i.e., the biased choice model). (E): Estimated values of the learning rate α in the Q-learning model. (F): Estimated values of the inverse temperature β in the Q-learning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Effect of reward probabilities on the model evaluation and estimation when the reward probabilities are shared by two options (Case 2). The conventions are the same conventions shown in Figure 2.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autonomous Mechanism of Internal Choice Estimate Underlies Decision Inertia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Akaishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Umeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakai</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2013.10.018</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Choice-induced preference change and the free-choice paradigm : A clarification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alós-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="49" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effects of positive mood on probabilistic learning: Behavioral and electrophysiological correlates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jepma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pourtois</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2014.09.012</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Postdecision changes in the desirability of alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Brehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="389" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experienced-weighted attraction learning in normal form games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="827" to="874" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How choice affects and reflects preferences: revisiting the free-choice paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Risen</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0020217</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="594" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Reinforcement Learning Mechanism Responsible for the Valuation of Free Choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2014.06.035</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="557" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding neural coding through the model-based analysis of decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1590-07.2007</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="8178" to="80" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Trial-by-trial data analysis using computational models. Decision Making, Affect, and Learning: Attention and Performance XXIII</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapping anhedonia onto reinforcement learning: a behavioural meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizzagalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1186/2045-5380-3-12</idno>
	</analytic>
	<monogr>
		<title level="j">Biology of Mood &amp; Anxiety Disorders</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Validation of decision-making models and analysis of decision variables in the rat basal ganglia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.6157-08.2009</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="9861" to="74" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Choice-induced preference change in the free-choice paradigm: a critical methodological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Izuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murayama</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00041</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayes factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The relation between reinforcement learning parameters and the influence of reinforcement history on choice behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Individual differences in heart rate variability are associated with the avoidance of negative emotional events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okanoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okada</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2014.10.007</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision-Making Based on Emotional Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okanoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okada</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2011.00311</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">311</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effects of depression on reward-based decision making and variability of action in probabilistic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kunisato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamawaki</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbtep.2012.05.007</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1088" to="94" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Healthy Fear of the Unknown: Perspectives on the Interpretation of Parameter Fits from Computational Models in Neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003015</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Odani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Northoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Post-response βγ power predicts the degree of choice-based learning in internally guided decision-making</title>
		<idno type="DOI">10.1038/srep32477</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32477</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model-based fMRI and its application to reward learning and decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1196/annals.1390.022</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1104</biblScope>
			<biblScope unit="page" from="35" to="53" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of first impression in operant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shteingart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0029550</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="476" to="88" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Using cognitive models to map relations between neuropsychological disorders and human decision-making deficits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yechiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bechara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<idno type="DOI">10.1111/j.1467-9280.2005.01646.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="973" to="981" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
