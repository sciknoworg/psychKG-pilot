<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Intersectional Fairness in Algorithmic Decision Making Using Intersectional Differential Algorithmic Functioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-06-11">June 11, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmi</forename><surname>Suk</surname></persName>
							<email>ysuk@tc.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Teachers College</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">T</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate Management Admission Council</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Intersectional Fairness in Algorithmic Decision Making Using Intersectional Differential Algorithmic Functioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-11">June 11, 2024</date>
						</imprint>
					</monogr>
					<note type="submission">This article has been accepted for publication in Journal of Educational and Behavioral Statistics,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fairness</term>
					<term>Intersectionality</term>
					<term>Discrimination</term>
					<term>Algorithms</term>
					<term>Machine learning</term>
					<term>Decision analysis</term>
					<term>Differential item functioning</term>
					<term>and Regularized regression</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Ensuring fairness is crucial in developing modern algorithms and tests. To address potential biases and discrimination in algorithmic decision making, researchers have drawn insights from the test fairness literature, notably the work on differential algorithmic functioning (DAF) by Suk and Han (2024). Nevertheless, the exploration of intersectionality in fairness investigations, within both test fairness and algorithmic fairness fields, is still relatively new. In this paper, we propose an extension of the DAF framework to include the concept of intersectionality. Similar to DAF, the proposed notion for intersectionality, which we term &quot;interactive DAF,&quot; leverages ideas from test fairness and algorithmic fairness. We also provide methods based on the generalized Mantel-Haenszel test, generalized logistic regression, and regularized group regression to detect DAF, interactive DAF, or other subtypes of DAF. Specifically, we employ regularized group regression with three different penalties and examine their performance via a simulation study. Finally, we demonstrate our intersectional DAF framework in real-world applications on grade retention and conditional cash transfer programs in education.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation: Test Fairness and Intersectional Fairness</head><p>Over the past six decades, numerous methods for differential item functioning (DIF) have been developed and widely used to assess the test fairness and validity at the item level of test (e.g., <ref type="bibr" target="#b15">Hanson, 1998;</ref><ref type="bibr">Holland &amp; Wainer, 1993;</ref><ref type="bibr" target="#b28">Lim et al., 2022;</ref><ref type="bibr" target="#b29">Magis et al., 2011;</ref><ref type="bibr" target="#b34">Pine, 1977)</ref>. Briefly, a test item is often a single question on an assessment test, and an item is considered to function differently (or DIF for short) if a subgroup of test-takers, such as the male group, perform differently on the item than another subgroup, such as the female group. For example, if the male group is more likely to answer a test item correctly than the female group given the same latent trait level (e.g., ability), the item is considered a DIF item and is typically suggested to be removed from the assessment. There is a long and rich literature on DIF in test development and psychometrics, including characterizing subtypes of DIF and developing powerful tests to detect DIF. Recently, <ref type="bibr" target="#b40">Suk and Han (2024)</ref> extended DIF to create an alternative framework for assessing algorithmic fairness, which they call differential algorithmic functioning (DAF). The DAF framework uses a decision variable, a set of "fair" variables, and a protected variable such as race and gender, and addresses whether a decision functions differently across different subgroups of the protected variable. <ref type="bibr" target="#b40">Suk and Han (2024)</ref> also characterize different subtypes of DAF and provide statistical tests to detect these subtypes of DAF; see the section on "Review: Differential Algorithmic Functioning" for more details. Unfortunately, a major limitation of the current DAF framework is that it does not take into account intersectionality. It is possible to test whether an algorithm is fair with respect to each protected attribute separately, but it is not feasible to test concurrent and compound injustices that arise across intersectional subgroups. The overarching goal of this paper is to extend the DAF framework to include the concept of intersectionality.</p><p>While discussions on test fairness have continued over the past six decades, the concept of algorithmic fairness recently emerged in the 2010s to identify and rectify unfair biases in machine learning and artificial intelligence systems <ref type="bibr" target="#b0">(Barocas et al., 2019;</ref><ref type="bibr" target="#b30">Mitchell et al., 2021;</ref><ref type="bibr" target="#b33">Pessach &amp; Shmueli, 2022)</ref>. Definitions of modern-day algorithmic fairness are often similar or identical to earlier definitions of test fairness <ref type="bibr" target="#b23">(Hutchinson &amp; Mitchell, 2019)</ref>. However, the exploration of intersectionality in fairness investigations within both fields is relatively recent. Russell and his colleagues have recently addressed intersectionality in the DIF literature <ref type="bibr" target="#b37">Russell et al., 2022)</ref>. Additionally, a handful of researchers have examined intersectional fairness in the field of algorithmic fairness <ref type="bibr" target="#b14">(Foulds et al., 2020;</ref><ref type="bibr" target="#b17">Hébert-Johnson et al., 2018;</ref><ref type="bibr" target="#b24">Kearns et al., 2018;</ref><ref type="bibr" target="#b25">M. P. Kim et al., 2019;</ref><ref type="bibr" target="#b46">Yang et al., 2021)</ref>. The concept of intersectionality originally arose from the feminist movement that highlights the importance of considering sexism and racism simultaneously rather than separately <ref type="bibr" target="#b8">(Crenshaw, 1989)</ref>, and it is now understood as an analytical lens for investigating societal unfairness along overlapping dimensions including gender, race, class, and disability <ref type="bibr" target="#b14">(Foulds et al., 2020)</ref>.</p><p>As a tangible example of intersectionality issues, suppose we are interested in designing an algorithm to assist teachers' decisions to retain a student or not. We consider two protected variables, gender and race. In a working retention algorithm, it is possible to be fair for gender and race separately, whereas it can be unfair among intersectional subgroups determined by two protected variables (e.g., the black female group). Specifically, the working algorithm may produce a systematic disadvantage to black female students in ways that are more than the sum of those by being female and being black. Such discriminatory bias will be ignored unless intersectionality is taken into account in fairness investigations.</p><p>In this paper, we propose an extension of the DAF framework to include the concept of intersectionality. To address intersectionality, we introduce the notion of interactive DAF, which assesses whether an algorithm functions differently among subgroups of one protected variable after accounting for another protected variable. We also expand the capabilities of the existing three DIF methods with a multi-categorical protected variable so that they can work properly under the intersectional DAF framework. Specifically, we use the generalized Mantel-Haenszel test and the generalized logistic regression method, using a decision variable as the dependent variable. As the final method, we propose a regularized group regression approach to detect different types of DAF, including interactive DAF. We evaluate the performance of DAF detection methods through a simulation study. We also demonstrate our proposed framework in real-world applications about grade retention and conditional cash transfer programs. To the best of our knowledge, this paper is the first attempt to integrate algorithmic fairness and test fairness in the context of intersectionality.</p><p>The remainder of the paper is organized as follows. We first provide a brief review of the DAF framework and prior works on intersectionality in test fairness and algorithmic fairness. Next, we discuss an extension of the DAF framework to include intersectionality and provide DAF detection methods. We then offer the designs and results of our simulation study to examine the performance of DAF methods. Subsequently, we demonstrate our approach for intersectional DAF in two empirical data in education. Finally, we provide our discussion and conclusions in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review: Differential Algorithmic Functioning</head><p>Consider a classification algorithm that is trained using data with N study units, indexed by i = 1, 2, . . . , N . Data of study unit i is composed of covariates V i ∈ V and a binary outcome Y i . The covariates V i are divided into finite-dimensional protected variables G i and finitedimensional unprotected variables</p><formula xml:id="formula_0">X i , i.e., V i = (G i , X i ). A decision variable D i ∈ D = {0, 1}</formula><p>is determined according to a decision rule based on V i , i.e., δ : V → D. The most common goal of a classification algorithm is to find a decision rule that makes correct decisions,</p><formula xml:id="formula_1">Y i = D i .</formula><p>We review the DAF framework <ref type="bibr" target="#b40">(Suk &amp; Han, 2024)</ref> for evaluating fairness in algorithmic decision making. DAF uses three pieces of information: a decision variable D, a set of "fair" variables W , and a protected variable G. <ref type="bibr" target="#b40">Suk and Han (2024)</ref> define DAF as conditional dependence of algorithmic decision D and protected variable G given fair attribute W . The fair attribute W means a set of justifiable variables that are important and valid in decision-making processes. It is a function of unprotected variables X, i.e., W = h(X), h : R px → R pw , and is determined based on domain knowledge. Formally, DAF is defined with one protected variable G as:</p><formula xml:id="formula_2">Definition 1 P r(D = 1|W, G = g) ̸ = P r(D = 1|W, G = g ′ ), ∃g ̸ = g ′ .</formula><p>In words, an algorithm has DAF if the probability of receiving the treatment decision differs across subgroups of a protected attribute (e.g., male vs. female) after accounting for the fair attribute; otherwise, the algorithm is non-DAF. One possible measure of the statistical relationship between Y and G given W is odds ratios, and the DAF, based on the odds ratio, can be expressed as:</p><formula xml:id="formula_3">∆(W ) = P r(D = 1|W, G = g)/{1 − P r(D = 1|W, G = g)} P r(D = 1|W, G = g ′ )/{1 − P r(D = 1|W, G = g ′ )} ̸ = 1, ∃g ̸ = g ′ .<label>(1)</label></formula><p>Equation <ref type="formula" target="#formula_3">1</ref>is an alternative way of expressing that DAF exists, i.e., Y and G are conditionally dependent given W . Typically, the focal group (G = g) represents the subgroup expected to have a systemic disadvantage by the algorithm, whereas the reference group (G = g ′ ) represents the subgroup expected to have an advantage. Importantly, DAF emphasizes procedural fairness rather than outcome fairness by incorporating the fair attribute and treating individuals with the identical fair attribute similarly. Moreover, the DAF framework provides a detailed description of disparity patterns by defining different subtypes of DAF as:</p><p>Definition 2 In the presence of DAF, uniform DAF exists if the statistical relationship (e.g., odds ratios) between D and G is constant for all levels of W ; otherwise, DAF is classified as nonuniform DAF.</p><p>For example, when a statistical relationship of interest between D and G is odds ratios, uniform DAF is defined to be present if ∆(W ) = c ̸ = 1 for all values of W where c is a constant but not 1. When an algorithm is uniform DAF, the algorithm consistently (dis)favors the focal group over the reference group across all the values of the fair attribute. An algorithm with uniform DAF shows static disparity with respect to decision allocations. On the other hand, nonuniform DAF is a type of DAF that is not uniform DAF. For example, it occurs when the advantage a group receives from an algorithm varies depending on the fair attribute. This can lead an algorithm to provide more favorable decisions to the focal group within certain ranges of the fair attribute, while favoring the reference group within other ranges. Nonuniform DAF displays dynamic disparity in decision allocations. <ref type="bibr">1</ref> For detecting DAF, three DAF methods are available, which are modifications of three well-established DIF methods, namely Mantel-Haenszel test <ref type="bibr" target="#b19">(Holland &amp; Thayer, 1986)</ref>, logistic regression <ref type="bibr" target="#b42">(Swaminathan &amp; Rogers, 1990)</ref>, and residual-based DIF <ref type="bibr" target="#b28">(Lim et al., 2022)</ref>; see <ref type="bibr" target="#b40">Suk and Han (2024)</ref> for more details on DAF detection methods with a binary protected variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Review: Intersectionality in Test Fairness and Algorithmic Fairness</head><p>An intersectional approach to DIF analyses in the field of test fairness was initially addressed in  and further discussed in  and <ref type="bibr" target="#b37">Russell et al. (2022)</ref>. These works emphasize that intersectionality acknowledges the existence of multiple identities that shape individuals' lived experiences, and thus, forms intersectional groups based on multiple protected variables. Specifically, three variables of interest in their works are gender (male and female), racial stratification (white, black, Hispanic, and Asian), and economic status (economically advantaged and economically disadvantaged). One historically advantaged group, such as the "male-white-advantaged" group, is used as the reference group, while others are considered focal groups. Pairwise comparisons are then conducted between the reference group and each of the focal groups, using four DIF detection methods for a binary protected variable, namely the standardized D statistic <ref type="bibr" target="#b10">(Dorans &amp; Kulick, 1986)</ref>, Mantel-Haenszel test, logistic regression, and the simultaneous item bias test <ref type="bibr" target="#b38">(Shealy &amp; Stout, 1993)</ref>. In essence, their framework views intersectionality as the redefinition of multiple protected attributes into a single product category. This aligns with the issue of a multi-categorical protected variable in the DIF literature.</p><p>In the field of algorithmic fairness, intersectionality issues have gained increasing attention and are often discussed with formal and quantifiable definitions <ref type="bibr" target="#b14">(Foulds et al., 2020;</ref><ref type="bibr" target="#b17">Hébert-Johnson et al., 2018;</ref><ref type="bibr" target="#b24">Kearns et al., 2018;</ref><ref type="bibr" target="#b25">M. P. Kim et al., 2019;</ref><ref type="bibr" target="#b46">Yang et al., 2021)</ref>. Among these, we highlight two approaches that extend beyond merely creating intersectional groups: one related to privacy <ref type="bibr" target="#b14">(Foulds et al., 2020)</ref> and another to causality <ref type="bibr" target="#b46">(Yang et al., 2021)</ref>. Specifically, <ref type="bibr" target="#b14">Foulds et al. (2020)</ref> extend the concepts of 80% rule 2 or differential privacy <ref type="bibr" target="#b11">(Dwork &amp; Roth, 2013)</ref> to define intersectional fairness. Their proposed definition of differential fairness (DF) measures the probability ratios across different subgroups determined by all the protected variables, and it aims to make the ratios similar to achieve intersectional fairness. They also introduce two more notions based on DF, namely DF bias amplification and differential fairness with confounders (DFC). DF bias amplification measures a difference in DF between the original dataset and an algorithm based on the same dataset, quantifying the bias induced by the algorithm. DFC evaluates DF with confounders affecting outcome distributions; for more details, see <ref type="bibr" target="#b14">Foulds et al. (2020)</ref>. Moreover, <ref type="bibr" target="#b14">Foulds et al. (2020)</ref> outline five important criteria for formally defining intersectional fairness of fairness in AI. We have applied these criteria to assess the validity of our proposed framework for intersectional fairness in the "Discussion and Conclusions" section.</p><p>In addition, <ref type="bibr" target="#b46">Yang et al. (2021)</ref> introduce a causal modeling approach to intersectional fairness in ranking tasks, considering the intersecting dimensions of multiple protected variables. Their framework builds on counterfactual fairness <ref type="bibr" target="#b27">(Kusner et al., 2017)</ref>, a fairness framework based on causal inference. This framework assumes that a decision is fair towards an individual if it is the same between the actual world and in a counterfactual world where the protected attribute were different. <ref type="bibr" target="#b46">Yang et al. (2021)</ref> adapt the framework to address intersectionality and ranking settings, where a ranking is counterfactually fair if it remains the same when comparing using actual protected variables to one counterfactual, intersectional, reference subgroup. <ref type="bibr" target="#b46">Yang et al. (2021)</ref> further categorize the roles of unprotected, mediator variables into non-resolving, resolving, and semi-resolving types in causal models, and outline a procedure to compute counterfactually fair rankings considering these mediator types. While theoretically compelling, there are difficulties in implementing the framework, as it requires no unmeasured confounding and a correctly specified causal model.</p><p>Overall, these previous studies have explored intersectionality from various angles. Yet, they have not formally distinguished how the effects of intersectionality manifest, whether in additive or non-additive ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Proposal: Intersectional Differential Algorithmic Functioning</head><p>In this section, we have extended the DAF framework to include the concept of intersectionality across protected variables. Our intersectional DAF analysis involves more than two protected variables, creating intersectional subgroups determined by these protected variables. The effect of intersectionality across the combination of protected variables can often be additive or become more complex. To detect interactive effects among intersectional subgroups, we define interactive DAF, which measures whether an algorithm functions differently among subgroups of one protected variable after accounting for another protected variable. With at least two protected variables (e.g., G 1 , G 2 ), interactive DAF is written as:</p><p>Definition 3 Interactive DAF exists if the statistical relationship (e.g., relative risks) between a decision variable (D) and one protected variable varies depending on the levels of other protected variable(s), after accounting for a set of fair variables (W ).</p><p>For example, a statistical relationship of interest can be expressed using relative risks. In this case, interactive DAF is defined to exist if P r[D=1|W,</p><formula xml:id="formula_4">G 1 =g 1 ,G 2 ] P r[D=1|W,G 1 =g ′ 1 ,G 2 ] ̸ = P r[D=1|W,G 1 =g 1 ] P r[D=1|W,G 1 =g ′ 1</formula><p>] . Another statistical relationship of interest can be odds ratios, and interactive DAF is detected if</p><formula xml:id="formula_5">Odds(W,G 1 =g 1 ,G 2 ) Odds(W,G 1 =g ′ 1 ,G 2 ) ̸ = Odds(W,G 1 =g 1 ) Odds(W,G 1 =g ′ 1 ) , where Odds(W, •) := P r(D=1|W,•) 1−P r(D=1|W,•)</formula><p>. Moreover, if a statistical relationship of interest is on the difference scale, interactive DAF is present if P r</p><formula xml:id="formula_6">[D = 1|W, G 1 = g 1 , G 2 ]− P r[D = 1|W, G 1 = g ′ 1 , G 2 ] ̸ = P r[D = 1|W, G 1 = g 1 ] −P r[D = 1|W, G 1 = g ′ 1 ]</formula><p>. Regardless, interactive DAF aims to detect distinctive effects by intersecting protected attributes, and it will be a useful concept to examine non-additive systematic (dis)advantages in algorithms across intersecting dimensions. In the absence of interactive DAF, additive effects across multiple protected attributes can be captured by examining at existing subtypes of DAF, i.e., uniform DAF and nonuniform DAF.</p><p>To characterize different subtypes of DAF in the intersectional framework, we extend the types of DAF discussed in <ref type="bibr" target="#b40">Suk and Han (2024)</ref>, which distinguishes between uniform DAF and nonuniform DAF. Specifically, we add another dimension to include interactive DAF. <ref type="table" target="#tab_0">Table 1</ref> provides a description of the allocation patterns associated with four types of DAF: (1) uniform (and simple) DAF, (2) nonuniform (and simple) DAF, (3) uniform and interactive DAF, and (4) nonuniform and interactive DAF. Simple DAF refers to unfair decision disparities associated with individual protected variables at a time or those not falling under the category of interactive DAF. These classifications facilitate a more informative discussion of intersectionality across protected variables.  <ref type="figure" target="#fig_0">Figure 1</ref> provides illustrations of DAF types with an intersectional variable between gender and race. An intersectional protected variable represents a combination of protected variables, say G * = G 1 × G 2 . For example, if gender (male vs. female) and race (white vs. black) are used as protected variables, intersectional variable G * contains four levels: white male, white female, black male, and black female. In plot A of <ref type="figure" target="#fig_0">Figure 1</ref>, the algorithm shows uniform DAF because the odds ratios of the group advantages in an algorithm are constant across the fair attribute, and it shows static disparity in decision allocations. In contrast, nonuniform DAF exists in plot B because the group advantages vary depending on the fair attribute, showing dynamic disparity. Moreover, when an interactive effect is of interest, we can further inspect the presence of interactive DAF. For example, we observe nonuniform DAF in plot C since group advantages depend on the fair attribute, and in particular, only people who are both black and female are disadvantaged systematically with increasing W . That is, we observe both nonuniform DAF and interactive DAF in plot C, showing dynamic, interactive disparity in decision allocations. To detect the presence of DAF with intersectionality in algorithms, we utilize existing DIF methods that can handle multi-categorical protected variables and modify them properly. Such methods include the generalized Mantel-Haenszel test (GMH; <ref type="bibr" target="#b31">Penfield, 2001;</ref><ref type="bibr" target="#b39">Somes, 1986)</ref>, generalized logistic regression (GLR; <ref type="bibr" target="#b29">Magis et al., 2011)</ref>, generalized Lord's χ 2 test (S.-H. <ref type="bibr" target="#b26">Kim et al., 1995)</ref>, and most recently, regularized regression methods <ref type="bibr" target="#b43">(Tutz &amp; Schauberger, 2015;</ref><ref type="bibr" target="#b45">Wang et al., 2022)</ref>. Although these DIF methods show their effectiveness in detecting DIF in test items for one protected variable at a time, they are not designed for discovering DAF coupled with intersectionality across protected variables, as defined in Definition 3. Therefore, we expand the capabilities of existing DIF methods so that they can work within the DAF framework under the context of intersectionality. In particular, we harness GMH, GLR, and regularized group regression methods because of their direct applicability. For simplicity, we assume two protected variables G = (G 1 , G 2 ) and a one-dimensional fair attribute W in the observed data; see details for each method below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalized Mantel-Haenszel</head><p>The GMH statistic is a multivariate generalization of the Mantel-Haenszel chi-square statistic that is used to analyze a multi-categorical protected variable <ref type="bibr" target="#b19">(Holland &amp; Thayer, 1986)</ref>. We use the GMH statistic to test whether an algorithm has DAF or not across all intersectional subgroups simultaneously. Here, intersectional protected variable G * ∈ {1, 2, . . . , J}, which is either categorical or categorized, is used, and fair attribute W is discretized into K strata (k = 1, 2, . . . , K). It is important to note that the GMH test does not provide separate test statistics to detect different subtypes of DAF (e.g., uniform DAF, interactive DAF), and it can only detect the composite effect of DAF.</p><p>Consider the data shown in <ref type="table" target="#tab_1">Table 2</ref> that contains treatment and control decisions (D = 1 and D = 0) from an algorithm for J categorical, intersectional groups. Let A k = (n 11k , n 21k , . . . , n (J−1)1k ) ⊺ be a (J − 1) × 1 vector containing the sample size of any (J − 1) groups who receive D = 1. E(A k ) and V (A k ) represent the expectation and variance-covariance matrix of A k , respectively, and can be written as:</p><formula xml:id="formula_7">E(A k ) = n •1k n k n ••k , V (A k ) = n •1k n •0k n ••k diag(n k ) − n k n ⊺ k (n ••k − 1)n 2 ••k ,</formula><p>where</p><formula xml:id="formula_8">n k = (n 1•k , n 2•k , . . . , n (J−1)•k )</formula><p>, and diag(n k ) is a (J − 1) × (J − 1) diagonal matrix with elements n k . The GMH statistic is given by</p><formula xml:id="formula_9">χ 2 GM H = (A − E(A)) ⊺ V (A) −1 (A − E(A)),<label>(2)</label></formula><p>where</p><formula xml:id="formula_10">A = K k=1 A k , E(A) = K k=1 E(A k ), V (A) = K k=1 V (A k )</formula><p>. This statistic asymptotically follows a chi-squared null distribution with (J − 1) degree of freedom, under the null hypothesis that an algorithm is non-DAF. If the GMH statistic is significant, it supports that the algorithm has DAF. </p><formula xml:id="formula_11">D = 1) Control Decision (D = 0) Total Group 1 n 11k n 10k n 1•k Group 2 n 21k n 20k n 2•k . . . . . . . . . . . . Group J n J1k n J0k n J•k Total n •1k n •0k n ••k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalized Logistic Regression</head><p>The GLR method is based on <ref type="bibr" target="#b29">Magis et al. (2011)</ref> and <ref type="bibr" target="#b42">Swaminathan and Rogers (1990)</ref> and it requires fitting a logistic regression model that regresses D on fair attribute W and protected variable G. Let us denote π i as the probability that individual i from intersectional group G * i = g receives D = 1, where g = 1, 2, ..., J − 1 for the first, second, . . ., J − 1 focal group, respectively, and g = J for the reference group. Let G * ∈ R n×(J−1) be an n × (J − 1) dummy matrix indicating individuals' intersectional protected membership in one of the (J − 1) focal groups and consisting of (J − 1) dimensional binary vector variables where the g-th element is 1 and the rest are 0. A logistic model for the GLR method with W i and G * i can be written as:</p><formula xml:id="formula_12">logit(π i ) = [1, W i ]β 0 + G * i β 1 + W i G * i β 2 ,<label>(3)</label></formula><p>where β 0 = (β 00 , β 01 ) ⊺ represents the intercept and slope parameters of the reference group g = J; β 1 = (β 11 , β 12 , . . . , β 1,(J−1) ) ⊺ is a coefficient vector that denotes differences in the intercept between each focal group g = 1, . . . , J − 1 and the reference group;</p><formula xml:id="formula_13">β 2 = (β 21 , β 22 , . . . , β 2,(J−1) ) ⊺</formula><p>is a coefficient vector that indicates differences in the slope between each focal group and the reference group. Model (3) serves as the full model, and using (3), we can detect DAF, uniform DAF, and nonuniform DAF. The null and alternative hypotheses for each case are provided in <ref type="table" target="#tab_2">Table 3</ref>. </p><formula xml:id="formula_14">β 2 = 0 β 1 ̸ = 0 or β 2 ̸ = 0 Uniform DAF β 1 = 0 under β 2 = 0 β 1 ̸ = 0 under β 2 = 0 Nonuniform DAF β 2 = 0 β 2 ̸ = 0</formula><p>Note: H 0 and H A represent null and alternative hypotheses, respectively. 0 in the null and alternative hypotheses represents coefficient vectors of zeros.</p><p>We use likelihood-ratio test statistics between the respective null models and respective alternative models to detect DAF, uniform DAF, and nonuniform DAF. We denote the three test statistics as GLR, GLR UNI , and GLR NUNI , respectively. Additionally, let G 1 be an n×(J 1 −1) dummy matrix indicating individuals' protected group membership in one focal level of the first protected variable G 1 ; likewise, G 2 be an n × (J 2 − 1) dummy matrix indicating individuals' protected group membership in one focal level of the second protected variable G 2 . To analyze interactive DAF, we fit the following model:</p><formula xml:id="formula_15">logit(π i ) = [1, W i ]β 0 + [ G 1i , G 2i ]β ′ 1 + W i [ G 1i , G 2i ]β ′ 2 + [ I i , W i I i ]β 3 .<label>(4)</label></formula><p>Here, I i = G 1i ⊗ G 2i is the resulting matrix that captures the interaction terms between G 1i and G 2i where ⊗ represents the Kronecker product;</p><formula xml:id="formula_16">β ′ 1 = (β ′ 11 , β ′ 12 . . . , β ′ 1,(J 1 +J 2 −2) )</formula><p>⊺ is a coefficient vector that indicates intercept differences for each focal level of one protected variable when another protected variable is fixed at its reference level;</p><formula xml:id="formula_17">β ′ 2 = (β ′ 21 , β ′ 22 , . . . , β ′ 2,(J 1 +J 2 −2) )</formula><p>⊺ is a coefficient vector that indicates slope differences for each focal level of one protected variable when another protected variable is fixed at its reference level;</p><formula xml:id="formula_18">β 3 = (β 31 , β 32 , . . . , β 3,2(J 1 −1)(J 2 −1) ) ⊺</formula><p>is a coefficient vector that represents interactive effects across different protected variables. Based on Model (4), we can detect the presence of interactive DAF by testing the null hypothesis that an algorithm has no interactive DAF, i.e., H 0 : β 3 = 0. We use a likelihood-ratio test to detect interactive DAF, and the test statistic is denoted as GLR INT . If GLR INT is significant, it supports the existence of interactive DAF in the algorithm. By using GLR INT , we can examine whether the DAF effect among intersectional subgroups is interactive or not, and it provides another dimension to describe the DAF effect. We remark that alternatively, it is possible to use a contrast matrix based on Model (3) to detect interactive DAF. However, fitting Model (4) allows us to directly test interactive DAF as defined in Definition 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regularized Group Regression</head><p>We propose an approach based on regularized group regression to detect the composite effect of DAF and specific types of DAF, including uniform DAF, nonuniform DAF, and interactive DAF.</p><p>This approach was inspired by the work of Tutz and Schauberger (2015) on group lasso penalty and Wang et al. (2022) on (ungrouped) lasso and adaptive lasso. We refer to our proposed approach as regularized group differential algorithmic functioning (rgDAF). Specifically, the rgDAF method groups regression coefficients responsible for DAF and employs three different penalties: lasso, the smoothly clipped absolute deviation (SCAD; <ref type="bibr" target="#b12">Fan &amp; Li, 2001)</ref>, and the minimax concave penalty (MCP; <ref type="bibr" target="#b48">Zhang, 2010)</ref>. An important distinction between an ungrouped penalty and a group penalty is that a group penalty selects coefficient groups, not individual coefficient parameters within the group; that is, within a group, coefficients will either all be zero or all be non-zero.</p><p>Let β M represent the set of coefficient parameters in a fitting model M, e.g.,</p><formula xml:id="formula_19">β Model 3 = (β 0 , β 1 , β 2 ) in Model (3) and β Model 4 = (β 0 , β ′ 1 , β ′ 2 , β 3 ) in Model (4).</formula><p>The objective function for regularized group regression optimization is written as:</p><formula xml:id="formula_20">Q(β M ; λ) = L(β M ) + S s=1 p(β (s) ; λ),<label>(5)</label></formula><p>where the loss function L(β M ) is the negative log-likelihood of a binomial distribution, i.e.,</p><formula xml:id="formula_21">L(β M ) = − 1 n i logP(y i |η i ), with η i being unit i's linear prediction in model M.</formula><p>The penalty term s p(β (s) ; λ) contains only the parameters responsible for detecting specific types of DAF, forming S different coefficient groups (s = 1, . . . , S). Specifically, the group lasso penalty term is given by:</p><formula xml:id="formula_22">p(β (s) ; λ) = λ d s ∥β (s) ∥.<label>(6)</label></formula><p>Here, the tuning parameter λ ≥ 0 controls the penalty size. The presence of √ d s in the penalty term accounts for different sizes of coefficient groups, and normalizes the penalty impact across groups of different sizes; we denote λ √ d s := λ s . The group lasso penalty specifically applies a lasso penalty to the Euclidean (L 2 ) norm of each coefficient group (denoted as ∥β (s) ∥), and thus, encourages sparsity and variable selection at the group level. That is, the solution of the penalized parameters has the property that if coefficient group s is selected, then all individual coefficients within coefficient group s are included; otherwise, β (s) = 0 for all s. Additionally, the rgDAF method requires choosing the tuning parameter λ. We considered several criteria for selecting the tuning parameter, such as cross-validation, the Akaike information criterion (AIC), and the Bayesian Information Criterion (BIC). Following <ref type="bibr" target="#b43">Tutz and Schauberger (2015)</ref>, <ref type="bibr" target="#b3">Belzak and Bauer (2020)</ref>, and our investigations via simulations, we use BIC to choose the tuning parameter. <ref type="table" target="#tab_3">Table 4</ref> describes the specific groups of penalized coefficients β P M used in our rgDAF method to detect DAF and its specific types. We group the first-order terms together as one coefficient group, while each second-order or higher term constitutes its own respective coefficient group. Specifically, for detecting DAF, we have J coefficient groups, where the first coefficient group β (1) = β 1 with d 1 = J − 1, and the last coefficient group β (J) = β 2,(J−1) with d J−1 = 1. To detect uniform DAF, only one coefficient group of β 1 exists. In contrast, the number of coefficient groups for detecting nonuniform DAF is J − 1, where the first coefficient group β (1) = β 21 and the last coefficient group β (J−1) = β 2,(J−1) , with d s = 1 for all coefficient groups. To detect interactive DAF, we create 2(J 1 − 1)(J 2 − 1) coefficient groups, and each coefficient element in β 3 forms a separate group, as for nonuniform DAF. We note that one could use a different specification for grouping regression coefficients if it is chosen reasonably. 3 </p><formula xml:id="formula_23">M Model M DAF J (β 1 , β 21 , β 22 , . . . , β 2,(J−1) ) (3) Uniform DAF 1 (β 1 ) β 2 = 0 in (3) Nonuniform DAF J − 1 (β 21 , β 22 , . . . , β 2,(J−1) ) (3) Interactive DAF 2(J 1 − 1)(J 2 − 1) (β 31 , β 32 , . . . , β 3,2(J 1 −1)(J 2 −1) ) (4)</formula><p>Note: The comma inside the parenthesis for the group lasso penalty serves as a separator for different groups.</p><p>For estimating β (s) , we use the multivariate soft-thresholding operator within the group coordinate descent algorithm <ref type="bibr" target="#b4">(Breheny &amp; Huang, 2013)</ref>. The soft-thresholding operator <ref type="bibr" target="#b9">(Donoho &amp; Johnstone, 1994</ref>) is a mathematical function that operates on a vector z by reducing its magnitude towards 0 by an amount λ while preserving its direction. Formally, it is written as F (z, λ) = 1 − λ ||z|| + z where z ||z|| is the unit vector in the direction of z <ref type="bibr" target="#b47">(Yuan &amp; Lin, 2005)</ref>. Algorithm 1 summarizes the rgDAF method for detecting the composite effect of DAF based on the coordinate descent algorithm for group lasso logistic regression. In Algorithm 1, Q represents the design matrix in a fitting model, and Q (s) is the portion of the design matrix associated with β (s) . Briefly, the group descent algorithm optimizes the target function with respect to a single coefficient group at a time and iterates through the coefficient groups until convergence. Then, we assess whether the coefficients responsible for DAF are zero or not based on the estimated coefficients. If any of the grouped, penalized coefficients are non-zero, it provides evidence for DAF. Likewise, to detect subtypes of DAF, we use an appropriate set of penalized coefficients and adjust a fitting model in Algorithm 1 as summarized in <ref type="table" target="#tab_3">Table 4</ref>. For example, in the case of interactive DAF, if any coefficient in (β 31 , β 32 , . . . , β 3,2(J 1 −1)(J 2 −1) ) does not shrink to 0, it supports the presence of interactive DAF.</p><p>Algorithm 1 Regularized group differential algorithmic functioning (rgDAF) for detecting DAF based on the group coordinate descent algorithm Input: Decision D i , fair attribute W i , and protected variable G i 1: Set m = 0. Initialize vector of residuals r (m) = (y − π)/v for all individuals, where</p><formula xml:id="formula_24">π i = exp(η i ) 1+exp(η i ) , η i = Q i β (m) , Q i = [1, W i , G * i , W i G * i ], v = max i sup η {▽ 2 L i (η)} ≤ 1/4. 2:</formula><p>For s = 1, 2, . . . , S, carry out the following calculations:</p><formula xml:id="formula_25">(i) Calculate z (s) = Q ⊺ (s) r (m) + β (m) (s) (ii) Update β (m+1) (s) ← F gLasso (vz (s) , λ s )/v (iii) Update r (m+1) ← r (m) − Q ⊺ (s) (β (m+1) (s) − β (m)<label>(s)</label></formula><p>) 3: Update m ← m + 1. 4: Repeat Steps 2 and 3 until convergence to obtain the final estimate,β (s) . 5: Detect the presence of DAF; if any ofβ (s) is not zero, DAF exists. Output:β (s) and the presence of DAF Additionally, we incorporate two additional penalties, SCAD and MCP in (5). These two penalties aim to alleviate lasso bias by reducing the strength of the penalty on large estimates in absolute value <ref type="bibr" target="#b16">(Hastie et al., 2017)</ref>. Between SCAD and MCP, MCP immediately relaxes the penalization rate, while with SCAD, the rate remains constant for a while before decreasing. To obtain the group SCAD and group MCP solutions using the group descent algorithm, we replace F gLasso (vz (s) , λ s )/v in Algorithm 1 with corresponding variations for SCAD and MCP; see Supplemental Appendix A for more details on these penalty functions and solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary of DAF Detection Methods</head><p>We provide a summary of test statistics from three methods-GMH, GLR, and rgDAF-to detect different types of DAF in <ref type="table" target="#tab_4">Table 5</ref>. Based on our simulation study below and the section on "Our Proposal: Intersectional Differential Algorithmic Functioning," we outline the strengths and limitations of DAF detection methods. The GMH test is a non-parametric test that does not require statistical modeling, which ensures valid Type-1 error control even when the relationship between D, G, and W is complex. As we will demonstrate in simulations, however, the GMH test has low power to detect DAF when nonuniform DAF is present, and it is not capable of differentiating different types of DAF: uniform DAF, nonuniform DAF, and interactive DAF. On the other hand, the tests from the GLR method have the power to detect different types of DAF. The asymptotic properties of these GLR tests are only valid if the fitting models are correctly specified, and if misspecified, it can inflate Type-1 error rates. Similar to GLR, the rgDAF method can detect subtypes of DAF but might suffer from model misspecification. Between the GLR and rgDAF methods, the rgDAF method becomes more advantageous when dealing with a larger number of protected attributes, especially if many of the focal groups within these protected attributes exhibit no differences from their respective references. This is because lasso regression is generally more advantageous when dealing with a larger number of predictors, especially when many of them are irrelevant or redundant. Lastly, we emphasize that while technical solutions provide valuable insights into the overall presence of DAF, visual inspection is highly recommended. We specify our strategies using graphical tools in Supplemental Appendix B and present specific examples in the section on "Conditional Cash Transfer Programs." 5 Simulation Study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Designs and Evaluation</head><p>We conduct a simulation study to assess the performance of GMH, GLR, and rgDAF methods with nine test statistics: one from the GMH method (GM H), four from the GLR method (GLR, GLR UNI , GLR NUNI , and GLR INT ), and four from the rgDAF method (rgDAF , rgDAF UNI , rgDAF NUNI , and rgDAF INT ). As a comparison, we include the Pearson chi-square test statistic (denoted as P earson) for the statistical parity criterion. Note that statistical/demographic parity requires that an algorithm's decision be independent of group membership, i.e., P r(D = 1|G = g) = P r(D = 1) <ref type="bibr" target="#b13">(Feldman et al., 2015)</ref>, and it aims to achieve equality of outcome rather than procedural fairness.</p><p>Our simulation study is categorized into five designs; see <ref type="figure">Figure 2</ref> for illustrations of our simulation designs. Design 1 assumes non-DAF (e.g., β 0 ̸ = 0, β ′ 1 = 0, β ′ 2 = 0, and β 3 = 0 in model (4)), and Design 2 assumes uniform DAF (e.g., β 0 ̸ = 0, β ′ 1 ̸ = 0, β ′ 2 = 0, and β 3 = 0 in model (4)). Design 3 assumes "balanced" nonuniform DAF where the advantages of intersectional groups are balanced across the levels of the fair attribute (e.g., β 0 ̸ = 0, β ′ 1 = 0, β ′ 2 ̸ = 0, and β 3 = 0 in model (4)). Design 4 assumes "unbalanced" nonuniform DAF where the group advantages are not balanced across the fair attribute (e.g., β 0 ̸ = 0, β ′ 1 ̸ = 0, β ′ 2 ̸ = 0, and β 3 = 0 in model (4)). Our last design, Design 5, is based on Design 4, but additionally assumes interactive DAF (e.g., β 0 ̸ = 0 and β 3 ̸ = 0 in model (4)). More specifically, our datagenerating model was based on logistic regression model (4) with one fair attribute and two binary protected variables, where non-zero coefficients responsible for specific DAF effects were set to -0.45 on a logit scale. We also varied the sample size, where intersectional subgroup size n g was set to either 500 or 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Simulation Designs</head><p>For all the designs, we evaluated the performance of nine DAF test statistics (as well as the Pearson statistic for statistical parity) by measuring Type-1 error and power rates. GMH and GLR statistics were assessed at the 5% alpha level to determine whether they provide evidence for DAF. The average detection rates represent the power rates when a test statistic is specifically designed for detecting a particular type of DAF; otherwise, they represent the Type-1 error rates. We repeated the simulation with five designs 1,000 times. For the rgDAF method implementation, we used the R package grpreg <ref type="bibr" target="#b4">(Breheny &amp; Huang, 2013)</ref>. <ref type="table" target="#tab_5">Table 6</ref> summarizes the results of the simulation study. In Design 1 that assumes non-DAF with a sample size of n g = 500, the DAF statistics from GMH and GLR methods, as well as the Pearson chi-square statistic (P earson), have well-controlled Type-1 error rates of ≤ 0.06. Test statistics from rgDAF methods exhibit very small Type-1 errors across different penalty terms, which is desirable. In Design 2, with uniform DAF and the sample size of n g = 500, all the DAF methods perform as expected. The DAF statistics that specialize in detecting uniform DAF (i.e., GLR UNI and rgDAF UNI ) or any type of DAF (i.e., GM H, GLR, and rgDAF ) show high power rates (&gt; 0.9). The DAF statistic for nonuniform DAF from the GLR method (GLR NUNI ) has a well-controlled Type-1 error rate of &lt; 0.05, but the one for interactive DAF (GLR INT ) shows a slightly inflated Type-1 error rate of 0.07. Additionally, rgDAF statistics for nonuniform DAF and interactive DAF exhibit very small Type-1 error rates of &lt; 0.02 across different penalty terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>For Design 3, with balanced nonuniform DAF and n g = 500, the GLR and rgDAF methods perform well; the test statistics for detecting nonuniform DAF (GLR NUNI and rgDAF NUNI ) or any type of DAF (GLR and rgDAF ) show high power rates (&gt; 0.9). In contrast, the GMH method fails to detect the presence of DAF because it is not able to accurately detect DAF when the group advantages are canceled out across the levels of the fair attribute. The Pearson statistic for statistical parity also shows low retention rates because it is anticipated to have no marginal difference under the balanced nonuniform design. In Design 4 which assumes unbalanced nonuniform DAF and n g = 500, all test statistics perform as anticipated. That is, test statistics except for GLR INT and rgDAF INT show high detection rates (i.e., &gt; 0.9 power rates), and GLR INT and rgDAF INT with three different penalties show low detection rates (i.e., &lt; 0.05 Type-1 error rates).</p><p>In Design 5 which assumes both unbalanced nonuniform DAF and interactive DAF with n g = 500, all the average detection rates from GMH, GLR, and Pearson statistics are high, ranging from 79.7% to 99.7%. Importantly, GLR INT performs well in detecting the presence of interactive DAF. Regarding rgDAF methods, unlike other designs, we observe noticeable performance differences across different penalty terms. The rgDAF statistic with group lasso has detection rates lower than about 5% compared to group SCAD and MCP. This may be because the (group) lasso tends to overshrink large coefficients in absolute value compared to the other two <ref type="bibr" target="#b12">(Fan &amp; Li, 2001;</ref><ref type="bibr" target="#b21">Huang et al., 2012)</ref>, and it could lead to lower detection rates for rgDAF INT with lasso penalty. Also, we find that all rgDAF INT results have much lower detection rates than GLR INT under the sample size condition of 500. This lower power of rgDAF method may be because sparsity only applies to a small set of coefficients responsible for interactive DAF (i.e., β 3 ), while all the other coefficients remain un-penalized in the current data generating models. This specification could lead the regularized method to readily have the penalized coefficients shrunk to zero, in order to reduce model variance at the expense of a small increase in bias. Additionally, the lower detection rates of rgDAF INT may be explained in part by our simple data-generating model with only one fair attribute and two binary-protected variables.</p><p>When the sample size of n g = 1000 increases, similar patterns are found in most statistics; desirable Type-1 errors and power rates are observed in Designs 1, 2, 3, 4, and 5. A noticeable difference is the increasing power rates of rgDAF INT compared to the sample size of n g = 500, but the detection rates of group MCP or group SCAD for interactive DAF are still higher than that from group lasso. Given these findings, we recommend implementing the rgDAF method with group MCP or group SCAD rather than group lasso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Real Data Analysis</head><p>In this section, we illustrate our DAF framework with two real datasets: one about grade retention and the other about conditional cash transfer programs. The former dataset is used to test the presence of DAF in the development of a new algorithm with the aforementioned test statistics, while the latter is used to diagnose an existing algorithm and highlight the importance of visual inspection for detecting DAF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Grade Retention</head><p>We used data from the Early Childhood Longitudinal Study-Kindergarten cohort (ECLS-K) <ref type="bibr" target="#b44">(Walston &amp; McCarroll, 2010)</ref> to design a retention decision-making algorithm. The ECLS-K is a national longitudinal study that examines school achievement and student experiences from kindergarten to middle school, and it is sponsored by the National Center for Education Statistics. We used students' retention in grade 1 as the outcome of interest Y , where Y = 0 means that they moved onto grade 1 and Y = 1 means that they were retained (i.e., they repeated kindergarten). We selected 60 students' covariates in the kindergarten year (i.e., V ) that are expected to affect whether a student is retained or not to design a decision-making algorithm for retention based on prior works (e.g., <ref type="bibr" target="#b6">Cannon &amp; Lipscomb, 2011;</ref><ref type="bibr" target="#b20">Hong &amp; Raudenbush, 2006)</ref>. We used intersectional groups determined by gender (male vs. female) and race (white vs. black) as protected variables of interest. The sample sizes for each intersectional subgroup are as follows: 3,627 white male students, 670 black male students, 3,391 white female students, and 663 black female students. Fair variables contained prior achievement scores in math, reading, and general knowledge, all measured during the kindergarten year; we chose them as fair variables because grade retention is often determined based on test-based scores according to prior work (e.g., <ref type="bibr" target="#b32">Penfield, 2010)</ref>. We constructed one summary variable using factor analysis with the prior achievement scores to alleviate multicollinearity and effectively capture the latent construct of ability.</p><p>To make algorithm-based decisions, we fitted random forests <ref type="bibr" target="#b5">(Breiman, 2001</ref>) using 60 covariates as predictors, and we made a student's predictions P from the model. Then, we used a set of threshold values that ranged from 0.20 to 0.30 with an increment of 0.02 to decide whether to retain or promote a student; we specifically used a range of threshold values below the average estimated retention probability (i.e., 0.039) in our analysis to account for the small number of retained students reported in prior works (e.g., <ref type="bibr" target="#b20">Hong &amp; Raudenbush, 2006;</ref><ref type="bibr" target="#b40">Suk &amp; Han, 2024;</ref><ref type="bibr" target="#b49">Zill et al., 1997)</ref> and observed in our sample. For instance, we made our decision based on a threshold value of 0.20 as: D = I(P ≥ 0.20). Here, D = 0 indicates promoting them to grade 1, and D = 1 indicates retaining them in kindergarten. Using the aforementioned three DAF methods (i.e., GMH, GLR, and rgDAF with group MCP), we assessed the presence of DAF among intersectional groups in the working retention algorithm. <ref type="figure">Figure 3</ref> summarizes the results of the test statistics, where satisfying a fairness criterion is denoted as 0, and unsatisfying is denoted as 1. Our intersectional DAF analysis reveals that the evidence of DAF depends on the threshold values and DAF test statistics. The P earson statistic detects marginal unfairness across all the threshold values. The GM H, GLR, and rgDAF statistics detect DAF for all the threshold values. When looking at statistics for subtypes of DAF from GLR and rgDAF methods, GLR NUNI detects nonuniform DAF when the threshold value is larger than or equal to 0.26, whereas rgDAF NUNI detects nonuniform DAF when the threshold value is at 0.20 or larger than 0.26; otherwise, uniform DAF is detected via GLR UNI and rgDAF UNI statistics. Note that if there is evidence of nonuniform DAF, the DAF subtype is considered nonuniform DAF, regardless of the presence of uniform DAF. Regarding interactive DAF, we observe no interactive DAF from the results of GLR INT across all the threshold values, while interactive DAF is detected at some of the threshold values from rgDAF INT . Overall, our analysis suggests that the working algorithm may potentially lead to discriminatory bias among intersectional groups while also presenting some conflicting evidence for interactive DAF. Therefore, the algorithm should be revised by adjusting statistical models, altering the algorithm procedure, or modifying the variables used, to ensure it is DAF-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Conditional Cash Transfer Programs</head><p>We demonstrate the application of our intersectional DAF framework on an existing recommendation algorithm developed by <ref type="bibr" target="#b41">Suk and Park (2023)</ref>. The authors developed data-driven, optimal recommendations for individual students regarding conditional cash transfer programs using data from Colombia <ref type="bibr" target="#b1">(Barrera-Osorio et al., 2019;</ref><ref type="bibr" target="#b2">Barrera-Osorio et al., 2011)</ref>, but they did not incorporate fairness-related considerations in their model development. To detect the presence of DAF in Suk and Park (2023)'s model, we used the program recommendation status from one of the best-fitting models as the decision variable of interest (D); D = 1 represents a recommendation for the program and D = 0 represents no recommendation. We also used Colombia's poverty index, known as the SISBEN, where a lower value indicates lower socioeconomic status for individuals. We selected the SISBEN index as the fair attribute because it is widely recognized and utilized in Colombia as a measure for identifying beneficiaries of social programs <ref type="bibr" target="#b7">(Castañeda &amp; Fernandez, 2005)</ref>. Two protected variables of interest are (i) a student's gender and (ii) the age gap between a student's age and the typical age for their grade. The gender variable is binary (male vs. female), and the age-gap variable is measured on two different scales: binary and continuous. The binary age-gap variable indicates whether a student is older than the typical age in their grade (older vs. not older), and the continuous age-gap variable indicates the number of years older the student is for their grade <ref type="bibr">(min = -14, max = 52, mean=0.26)</ref>.</p><p>We first analyze intersectionality with gender and a binary age-gap variable, and in this case, the intersectional subgroups contain 1443 not-older male students, 437 older male students, 1603 not-older female students, and 389 older female students. Before conducting a DAF analysis, we checked the marginal differences in decisions among the four subgroups. We observe that 59.4% of the students in the older female group are recommended to receive the program, but in other groups, about 75% of those are recommended; specifically, 73.9% are recommended among the not-older female group, 75.3% among the older male group, and 74.7% among the not-older male group. <ref type="figure" target="#fig_1">Figure 4</ref> provides decision characteristic curves among the four groups determined by gender and the binary age-gap variable, similar to <ref type="figure" target="#fig_0">Figure 1</ref>. In <ref type="figure" target="#fig_1">Figure 4</ref>, we observe that the decision probabilities for female students who are older than the typical age in their grade are similar to those among other groups around the minimum of the fair attribute, but they rapidly decrease as the fair attribute of SISBEN increases. In contrast, the decision probabilities for the other <ref type="figure">Figure 3</ref>: Results of test statistics from fairness metrics about differential algorithmic functioning (DAF) and statistical parity with an intersectional group variable. three groups are relatively stable over the fair attribute. This indicates that both nonuniform DAF and interactive DAF are present in the existing algorithm. Using the GLR and rgDAF methods, we also find evidence of both nonuniform DAF and interactive DAF, based on the test statistics that specialize in detecting nonuniform DAF (i.e., GLR NUNI and rgDAF NUNI ) and interactive DAF (i.e., GLR INT and rgDAF INT ). To examine the intersectionality between gender and age-gap variables in more detail, we used the continuous age-gap variable that represents the number of years older the child is for their grade (denoted as Years Back). Using the continuous protected variable, we can inspect detailed patterns of intersectionality, compared to its binary version used in <ref type="figure" target="#fig_1">Figure 4</ref>. <ref type="figure" target="#fig_2">Figure 5</ref> describes how differences in decision probabilities between female and male students change over the fair attribute and Years-Back variable through a contour plot. Positive values indicate that female students have higher probabilities than male students, but negative values indicate that male students have higher probabilities than female students. We see that there are no or very small differences (highlighted in white) around the minimum of the fair attribute and around the zero value of Years-Back over the fair attribute. In contrast, when the fair attribute goes to the maximum, differences in decision probabilities diverge depending on the values of Years-Back. When we look at fair-attribute values of 20 or above, differences in the probabilities are the most positive near the minimum of Years-Back (highlighted in light pink), whereas differences are the most negative when Years-Back amounts to about 20 (highlighted in dark blue). These results imply that the algorithm discriminates against students impacted by age and gender in ways that are more than additive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusions</head><p>In this paper, we have extended the DAF framework for algorithmic fairness to incorporate the concept of intersectionality across protected variables. We introduced the notion of interactive DAF to identify how protected variables interact with each other, highlighting the importance of considering their intersecting dimensions in algorithmic decision making. To test the presence of DAF within the intersectional DAF framework, we proposed three DAF detection tools: GMH, GLR, and rgDAF methods. Among them, the GLR and rgDAF methods can identify different types of DAF, including uniform DAF, nonuniform DAF, and interactive DAF. Our simulation study revealed that the performance of the rgDAF method for interactive DAF is underwhelming when the sample size of one intersectional subgroup is 500. However, with an increasing sample size of 1000, its performance improves. When comparing the performance of the rgDAF method with different penalties, we found that group MCP and group SCAD are preferred over group lasso. We believe that the rgDAF method is particularly useful when dealing with a larger number of protected attributes, especially when many of the focal groups within these attributes do not differ significantly from their respective references. Additionally, we emphasized the significance of visual inspection in understanding disparity patterns in decision allocations, and we demonstrated our framework using two real datasets on grade retention and conditional cash transfer programs; the former dataset was evaluated in the development of a new algorithm, while the latter was used to diagnose an existing algorithm.</p><p>Importantly, our framework of intersectional DAF satisfies the intersectional fairness criteria proposed by <ref type="bibr" target="#b14">Foulds et al. (2020)</ref>, which include (i) considering multiple protected variables, (ii) defining and protecting the intersecting values of the protected variables, (iii) protecting individual protected variables, (iv) protecting minority groups, and (v) considering structural oppression among protected groups. Our framework achieves criteria (i) and (ii) by considering intersectional groups across different protected variables. Criterion (iii) is met because any subtypes of DAF should not be present to ensure a DAF-free algorithm, and this guarantees the protection of individual protected variables. We do not introduce any penalty against minority groups, such as the weights of the group sample size, thus satisfying criterion (iv). Additionally, criterion (v) is satisfied because the use of fair attributes aims to consider the important and valid decision-making process and can account for structural oppression among protected groups.</p><p>Moreover, the intersectional DAF framework depends on the choices of fair attributes as discussed in <ref type="bibr" target="#b40">Suk and Han (2024)</ref>. Unlike DIF analysis, where the latent ability score is used as a fair attribute, DAF analysis allows fair attributes to be either manifest or latent. When selecting fair attributes in DAF analysis, researchers should leverage subject matter knowledge about factors behind the decision-making process to identify which variables qualify as fair attributes. For example, in our real-data application of the grade retention algorithm, there are two approaches to determining grade retention: test-based retention and teacher-based retention <ref type="bibr" target="#b22">(Huddleston, 2014)</ref>. In this demonstration, we adopt the test-based approach, where prior achievement scores are considered key and valid variables in the decision-making process for grade retention. Thus, we use prior achievement scores as a fair attribute, given our knowledge about the test-based retention decision-making process and the role of the prior achievement scores. However, in cases where subject matter knowledge is limited, researchers might consider data-driven measures, such as those based on changes in R-squared, Gini index, or classification accuracy, as potential candidates for the fair attributes among unprotected variables. Nonetheless, the validity and reliability of fair attributes should be critically evaluated based on subject matter expertise, involving various stakeholders in the design, implementation, and use of algorithms. Failure to do so may result in the selection of inappropriate fair attributes and/or the use of variables with substantive measurement errors, potentially drawing misleading conclusions in the subsequent DAF analysis.</p><p>Based on the findings of this paper, we provide some suggestions for future research regarding our proposed intersectional DAF framework. First, while we focused on parametric approaches to detect subtypes of DAF, future work would investigate nonparametric techniques for detecting them, particularly interactive DAF. Second, we employed regularized group regression, where penalized coefficients are grouped in specific ways to detect DAF or its subtypes. Future research would examine exploring different specifications for grouping regression coefficients in the method to compare their performance. Third, we did not focus on the impact of measurement bias in variables (e.g., fair attributes or outcomes) on DAF analysis. While this source of bias potentially influences the results of our DAF analysis, more systematic research would be required to investigate its impact in the DAF assessment. Fourth, it is essential to recognize that algorithms flagged as DAF may only have the potential to be unfair. Thus, addressing fairness-related biases in algorithms requires a holistic approach that combines both technical and non-technical solutions. Lastly, our framework of intersectional DAF has the potential to be adapted to DIF settings in test development, and this enables the detection of "interactive DIF" in addition to the existing subtypes of DIF. Future research would explore how to incorporate interactive DIF within the context of test fairness.</p><p>While no universal definition is suitable for all systems and contexts, our intersectional DAF framework focuses on achieving the intersectional fairness of decision allocations and sheds light on different patterns of decision disparities across multiple protected variables. We believe that our intersectional DAF framework will serve as a valuable tool for assessing fairness in algorithmic decision-making and can be viewed as a meaningful integration between the concepts of test fairness and algorithmic fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visual Inspection</head><p>Although tests of statistical significance in the section on "Our Proposal: Intersectional Differential Algorithmic Functioning" offer a useful indicator of the overall presence of DAF, conducting visual inspection is still strongly advised. Visual inspection is the process of examining DAF patterns using graphs/plots for data visualization, without relying on significance tests. This can provide an effective means to extract additional information and critical insights about the nature of DAF that might have been completely undetected or overlooked in the statistical significance testing. To find disparity patterns in algorithms, as seen from <ref type="figure" target="#fig_0">Figure 1</ref>, we can draw decision characteristic curves in a two-dimensional plot, in particular when intersectional subgroups are categorical or categorized. Decision characteristic curves assist researchers in understanding how the decision probabilities (or transformation thereof) change over the fair attribute among intersectional subgroups.</p><p>We can also use heatmaps or contour plots. In DAF analysis, heatmaps or contour plots display the 3-dimensional relationship in two dimensions, where the fair attribute and one protected variable (G 1 ) are plotted on the x-and y-scales and decision probabilities (or transformation thereof) are represented by colors or contours within groups of another protected variable (G 2 ). A contour line is a curve that joins points of equal values in decision probabilities. Or, one can create heatmaps or contour plots that represent differences in decision probabilities (or transformation thereof) between each focal group and the reference group of G 2 by colors or contours.</p><p>Similar to heatmaps and contour plots, surface plots display the 3-dimensional relationship, but they are in three dimensions with decision probabilities represented by a smooth surface on the z-scale. Instead of decision probabilities, one can also put differences in decision probabilities between groups on the z-scale in surface plots. Using these visualizations helps researchers discover discriminatory bias from algorithms and identify disparity patterns without relying solely on significance tests. Even when the significance tests do not indicate any presence of DAF, visual inspection can sometimes reveal localized DAFs (DAF exhibiting only in isolated spaces of fair attributes), which can still be consequential for the corresponding group of people and call for fairness-related attention. Therefore, visual inspection should be routinely used in DAF analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrations of decision characteristic curves for intersectional groups by race (white vs. black) and gender (male vs. female) with different types of differential algorithmic functioning (DAF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Decision characteristic curves for intersectional groups by gender (male vs. female) and age gap (older vs. not older).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Differences in decision probabilities between female and male students.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Types of differential algorithmic functioning (DAF) and allocation patterns in the framework of intersectionality.</figDesc><table><row><cell>DAF Type</cell><cell>Simple DAF</cell><cell>Interactive DAF</cell></row><row><cell>Uniform DAF</cell><cell>Static disparity</cell><cell>Static, interactive disparity</cell></row><row><cell>Nonuniform DAF</cell><cell>Dynamic disparity</cell><cell>Dynamic, interactive disparity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A contingency table by intersectional protected variable G * and decision variable D within the k-th stratum of fair attribute W .</figDesc><table><row><cell>Treatment Decision</cell></row><row><cell>(</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Null and alternative hypotheses in generalized logistic regression</cell></row><row><cell></cell><cell>H 0</cell><cell>H A</cell></row><row><cell>DAF</cell><cell>β 1 = 0 and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Group specification of coefficients responsible for differential algorithmic functioning (DAF) DAF Type Number of Groups, S Group Specification in β P</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test statistics from three methods that detect differential algorithmic functioning (DAF) with intersectionality.</figDesc><table><row><cell>DAF Type</cell><cell>GMH</cell><cell>GLR</cell><cell>rgDAF</cell></row><row><cell>DAF</cell><cell>GM H</cell><cell>GLR</cell><cell>rgDAF</cell></row><row><cell>Uniform DAF</cell><cell></cell><cell>GLR UNI</cell><cell>rgDAF UNI</cell></row><row><cell>Nonuniform DAF</cell><cell></cell><cell>GLR NUNI</cell><cell>rgDAF NUNI</cell></row><row><cell>Interactive DAF</cell><cell></cell><cell>GLR INT</cell><cell>rgDAF INT</cell></row><row><cell cols="4">Note. GMH = generalized Mantel-Haenszel; GLR = generalized logistic regression; rgDAF =</cell></row><row><cell cols="2">regularized group differential algorithmic functioning</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">Average detection rates under Designs 1, 2, 3, 4, and 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Penalty</cell><cell cols="5">Design 1 Design 2 Design 3 Design 4 Design 5</cell></row><row><cell>n g = 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P earson</cell><cell></cell><cell>0.057</cell><cell>1.000</cell><cell>0.036</cell><cell>1.000</cell><cell>0.943</cell></row><row><cell>GM H</cell><cell></cell><cell>0.054</cell><cell>1.000</cell><cell>0.045</cell><cell>1.000</cell><cell>0.960</cell></row><row><cell>GLR</cell><cell></cell><cell>0.060</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>0.997</cell></row><row><cell>GLR UNI</cell><cell></cell><cell>0.060</cell><cell>1.000</cell><cell>0.047</cell><cell>1.000</cell><cell>0.965</cell></row><row><cell>GLR NUNI</cell><cell></cell><cell>0.053</cell><cell>0.047</cell><cell>1.000</cell><cell>1.000</cell><cell>0.859</cell></row><row><cell>GLR INT</cell><cell></cell><cell>0.049</cell><cell>0.070</cell><cell>0.041</cell><cell>0.035</cell><cell>0.797</cell></row><row><cell>rgDAF</cell><cell>Group lasso</cell><cell>0.028</cell><cell>1.000</cell><cell>0.998</cell><cell>1.000</cell><cell>0.975</cell></row><row><cell>rgDAF UNI</cell><cell>Group lasso</cell><cell>0.004</cell><cell>1.000</cell><cell>0.004</cell><cell>1.000</cell><cell>0.800</cell></row><row><cell>rgDAF NUNI</cell><cell>Group lasso</cell><cell>0.027</cell><cell>0.017</cell><cell>0.999</cell><cell>0.992</cell><cell>0.829</cell></row><row><cell>rgDAF INT</cell><cell>Group lasso</cell><cell>0.007</cell><cell>0.011</cell><cell>0.007</cell><cell>0.005</cell><cell>0.437</cell></row><row><cell>rgDAF</cell><cell>Group SCAD</cell><cell>0.030</cell><cell>1.000</cell><cell>0.999</cell><cell>1.000</cell><cell>0.975</cell></row><row><cell>rgDAF UNI</cell><cell>Group SCAD</cell><cell>0.004</cell><cell>1.000</cell><cell>0.004</cell><cell>1.000</cell><cell>0.800</cell></row><row><cell>rgDAF NUNI</cell><cell>Group SCAD</cell><cell>0.027</cell><cell>0.017</cell><cell>0.999</cell><cell>0.992</cell><cell>0.829</cell></row><row><cell>rgDAF INT</cell><cell>Group SCAD</cell><cell>0.007</cell><cell>0.012</cell><cell>0.010</cell><cell>0.004</cell><cell>0.484</cell></row><row><cell>rgDAF</cell><cell>Group MCP</cell><cell>0.031</cell><cell>0.999</cell><cell>0.999</cell><cell>1.000</cell><cell>0.974</cell></row><row><cell>rgDAF UNI</cell><cell>Group MCP</cell><cell>0.003</cell><cell>0.999</cell><cell>0.003</cell><cell>1.000</cell><cell>0.785</cell></row><row><cell>rgDAF NUNI</cell><cell>Group MCP</cell><cell>0.027</cell><cell>0.017</cell><cell>0.999</cell><cell>0.992</cell><cell>0.838</cell></row><row><cell>rgDAF INT</cell><cell>Group MCP</cell><cell>0.007</cell><cell>0.012</cell><cell>0.010</cell><cell>0.004</cell><cell>0.484</cell></row><row><cell>n g = 1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P earson</cell><cell></cell><cell>0.045</cell><cell>1.000</cell><cell>0.043</cell><cell>1.000</cell><cell>0.998</cell></row><row><cell>GM H</cell><cell></cell><cell>0.039</cell><cell>1.000</cell><cell>0.052</cell><cell>1.000</cell><cell>0.999</cell></row><row><cell>GLR</cell><cell></cell><cell>0.039</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>GLR UNI</cell><cell></cell><cell>0.037</cell><cell>1.000</cell><cell>0.049</cell><cell>1.000</cell><cell>0.999</cell></row><row><cell>GLR NUNI</cell><cell></cell><cell>0.052</cell><cell>0.049</cell><cell>1.000</cell><cell>1.000</cell><cell>0.996</cell></row><row><cell>GLR INT</cell><cell></cell><cell>0.049</cell><cell>0.041</cell><cell>0.046</cell><cell>0.043</cell><cell>0.978</cell></row><row><cell>rgDAF</cell><cell>Group lasso</cell><cell>0.019</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>rgDAF UNI</cell><cell>Group lasso</cell><cell>0.000</cell><cell>1.000</cell><cell>0.003</cell><cell>1.000</cell><cell>0.993</cell></row><row><cell>rgDAF NUNI</cell><cell>Group lasso</cell><cell>0.021</cell><cell>0.017</cell><cell>1.000</cell><cell>1.000</cell><cell>0.990</cell></row><row><cell>rgDAF INT</cell><cell>Group lasso</cell><cell>0.009</cell><cell>0.007</cell><cell>0.009</cell><cell>0.006</cell><cell>0.782</cell></row><row><cell>rgDAF</cell><cell>Group SCAD</cell><cell>0.020</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>rgDAF UNI</cell><cell>Group SCAD</cell><cell>0.000</cell><cell>1.000</cell><cell>0.003</cell><cell>1.000</cell><cell>0.993</cell></row><row><cell>rgDAF NUNI</cell><cell>Group SCAD</cell><cell>0.022</cell><cell>0.017</cell><cell>1.000</cell><cell>1.000</cell><cell>0.990</cell></row><row><cell>rgDAF INT</cell><cell>Group SCAD</cell><cell>0.010</cell><cell>0.006</cell><cell>0.011</cell><cell>0.007</cell><cell>0.829</cell></row><row><cell>rgDAF</cell><cell>Group MCP</cell><cell>0.020</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>rgDAF UNI</cell><cell>Group MCP</cell><cell>0.000</cell><cell>1.000</cell><cell>0.003</cell><cell>1.000</cell><cell>0.992</cell></row><row><cell>rgDAF NUNI</cell><cell>Group MCP</cell><cell>0.021</cell><cell>0.018</cell><cell>1.000</cell><cell>1.000</cell><cell>0.990</cell></row><row><cell>rgDAF INT</cell><cell>Group MCP</cell><cell>0.010</cell><cell>0.006</cell><cell>0.010</cell><cell>0.007</cell><cell>0.828</cell></row><row><cell cols="6">Note: Design 1 assumes non-DAF; Design 2 assumes uniform DAF; Design 3 assumes balanced</cell><cell></cell></row><row><cell cols="6">nonuniform DAF; Design 4 assumes unbalanced nonuniform DAF; Design 5 assumes unbalanced</cell><cell></cell></row><row><cell cols="7">nonuniform DAF and interactive DAF. Bold fonts indicate the power for DAF, and regular fonts indicate</cell></row><row><cell>the Type-1 error.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We remark that researchers have the flexibility to select a statistical relationship for the subtypes of DAF. They may choose a measure that is widely accepted in their field or one that best fits their data and hypotheses. It is also possible to utilize multiple relationships that are equally important in the DAF assessment and report the results for each one, if applicable.2 The 80% rule indicates legal evidence of adverse impact when the probability ratio of a favorable outcome between disadvantaged and advantaged groups is below 0.8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Regarding reasonable choices for grouping, we need to consider two key factors. First, only the parameters that are responsible for DAF should be penalized and grouped. Second, if they are grouped, all coefficients associated with a single protected variable should be grouped together within the same degree of polynomials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are grateful to Dubravka Svetina Valdivia and Montserrat Valdivia Medinaceli for their comments on an earlier version of the manuscript, shared during the 2024 National Council for Measurement in Education (NCME) conference. This research was partly funded by the National Science Foundation under Grant No. 2225321. The opinions, findings, conclusions, or recommendations expressed in this work are solely those of the authors and do not necessarily reflect the views of the National Science Foundation. Also, the original collector of the data, ICPSR, and the relevant funding agency bear no responsibility for use of the data or for interpretations or inferences based upon such uses.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Penalty Functions and Solutions for group SCAD and group MCP Among various nonconvex penalties, the smoothly clipped absolute deviations (SCAD; <ref type="bibr" target="#b12">Fan &amp; Li, 2001</ref>) penalty, denoted as p SCAD , is one of the earliest and most important. The SCAD penalty and its derivative are written as:</p><p>Here, tuning parameter λ represents the penalty size, and tuning parameter γ controls the concavity of the penalty, which determines how rapidly the penalty tapers off; for SCAD, γ &gt; 2. SCAD behaves like the lasso until |θ| = λ, and then it smoothly transitions to a quadratic function until |θ| = γλ. Beyond this point, it remains constant for all |θ| &gt; γλ. The SCAD penalty maintains the penalization rate of the lasso for small coefficients but continuously relaxes the rate of penalization as the absolute value of the coefficient increases. The minimax concave penalty (MCP; Zhang, 2010) follows a similar principle. The MCP penalty and its derivative are written as:</p><p>where γ &gt; 1. Similar to SCAD, MCP starts with the same rate of penalization as the lasso and then gradually reduces the penalization rate to zero as the absolute value of the coefficient increases. However, unlike SCAD, MCP immediately relaxes the penalization rate, whereas SCAD maintains it for a while before decreasing. The close-form solutions for updating coefficients under group SCAD and MCP are as follows:</p><p>While Algorithm 1 is presented for the rgDAF method with group lasso, it can be readily adapted to fit group MCP and group SCAD models. Specifically, one can replace F gLasso (vz (s) , λ s , γ)/v with F gSCAD (vz (s) , λ s , γ)/v for SCAD and F gM CP (vz (s) , λ s , γ)/v for MCP, respectively, to implement the rgDAF method with group SCAD and group MCP.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fairness and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="http://www.fairmlbook.org" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Replication data for: Improving the design of conditional transfer programs: Evidence from a randomized education experiment in Colombia (tech. rep.)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barrera-Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perez-Calle</surname></persName>
		</author>
		<idno type="DOI">10.3886/E113783V1</idno>
		<ptr target="https://doi.org/10.3886/E113783V1" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>Inter-university Consortium for Political and Social Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving the design of conditional transfer programs: Evidence from a randomized education experiment in colombia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barrera-Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perez-Calle</surname></persName>
		</author>
		<idno type="DOI">10.1257/app.3.2.167</idno>
		<ptr target="https://doi.org/10.1257/app.3.2.167" />
	</analytic>
	<monogr>
		<title level="j">American Economic Journal: Applied Economics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving the assessment of measurement invariance: Using regularization to select anchor items and identify differential item functioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C M</forename><surname>Belzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000253</idno>
		<ptr target="https://doi.org/10.1037/met0000253" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="673" to="690" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-013-9424-2</idno>
		<ptr target="https://doi.org/10.1007/s11222-013-9424-2" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="187" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/a:1010933404324" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Early grade retention and student success: Evidence from Los Angeles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lipscomb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Public Policy Institute of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Targeting social spending to the poor with proxy-means testing: Colombia&apos;s sisben system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Castañeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fernandez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">529</biblScope>
		</imprint>
	</monogr>
	<note>World bank human Development Network social protection unit discussion paper</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crenshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="139" to="167" />
		</imprint>
		<respStmt>
			<orgName>University of Chicago Legal Forum</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ideal spatial adaptation by wavelet shrinkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/81.3.425</idno>
		<ptr target="https://doi.org/10.1093/biomet/81.3.425" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Demonstrating the utility of the standardization approach to assessing unexpected differential item performance on the Scholastic Aptitude Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Dorans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kulick</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1986.tb00255.x</idno>
		<ptr target="https://doi.org/10.1111/j.1745-3984.1986.tb00255.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1561/0400000042</idno>
		<ptr target="https://doi.org/10.1561/0400000042" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variable selection via nonconcave penalized likelihood and its oracle properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214501753382273</idno>
		<ptr target="https://doi.org/10.1198/016214501753382273" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">456</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783311</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783311" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An intersectional definition of fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Keya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icde48307.2020.00203</idno>
		<ptr target="https://doi.org/10.1109/icde48307.2020.00203" />
	</analytic>
	<monogr>
		<title level="m">IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uniform DIF and DIF defined by differences in item response functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Hanson</surname></persName>
		</author>
		<idno type="DOI">10.2307/1165247</idno>
		<ptr target="https://doi.org/10.2307/1165247" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="253" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Extended comparisons of best subset selection, forward stepwise selection, and the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1707.08692</idno>
		<ptr target="https://doi.org/https://doi.org/10.48550/arXiv.1707.08692" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multicalibration: Calibration for the (computationally-identifiable) masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hébert-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1939" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Differential item functioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780203357811</idno>
		<ptr target="https://doi.org/10.4324/9780203357811" />
		<editor>Wainer, H.</editor>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differential item functioning and the Mantel-Haenszel procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Thayer</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.2330-8516.1986.tb00186.x</idno>
		<idno>i-24</idno>
		<ptr target="https://doi.org/10.1002/j.2330-8516.1986.tb00186.x" />
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating kindergarten retention policy: A case study of causal inference for multilevel observational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Raudenbush</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214506000000447</idno>
		<ptr target="https://doi.org/10.1198/016214506000000447" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">475</biblScope>
			<biblScope unit="page" from="901" to="910" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A selective review of group selection in high-dimensional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1214/12-sts392</idno>
		<ptr target="https://doi.org/10.1214/12-sts392" />
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Achievement at whose expense? a literature review of test-bas ed grade retention policies in us schools. Education Policy Analysis Archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Huddleston</surname></persName>
		</author>
		<idno type="DOI">10.14507/epaa.v22n18.2014</idno>
		<ptr target="https://doi.org/http://dx.doi.org/10.14507/epaa.v22n18.2014" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">50 years of test (un) fairness: Lessons for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287600</idno>
		<ptr target="https://doi.org/10.1145/3287560.3287600" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Preventing fairness gerrymandering: Auditing and learning for subgroup fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2564" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiaccuracy: Black-box post-processing for fairness in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/ACM Conference on AI, Ethics, and Society</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detection of differential item functioning in multiple groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1995.tb00466.x</idno>
		<ptr target="https://doi.org/10.1111/j.1745-3984.1995.tb00466.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="276" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<ptr target="https://doi.org/30" />
		<title level="m">Counterfactual fairness. 2017 Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A residual-based differential item functioning detection framework in item response theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1111/jedm.12313</idno>
		<ptr target="https://doi.org/10.1111/jedm.12313" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="104" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generalized logistic regression procedure to detect differential item functioning among multiple groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raıche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Béland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gérard</surname></persName>
		</author>
		<idno type="DOI">10.1080/15305058.2011.602810</idno>
		<ptr target="https://doi.org/10.1080/15305058.2011.602810" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Testing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithmic fairness: Choices, assumptions, and definitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-042720-125902</idno>
		<ptr target="https://doi.org/10.1146/annurev-statistics-042720-125902" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="141" to="163" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Assessing differential item functioning among multiple groups: A comparison of three Mantel-Haenszel procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Penfield</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15324818ame1403_3</idno>
		<ptr target="https://doi.org/10.1207/s15324818ame14033" />
	</analytic>
	<monogr>
		<title level="j">Applied Measurement in Education</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="259" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Test-based grade retention: Does it stand up to professional standards for fair and appropriate test use? Educational Researcher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Penfield</surname></persName>
		</author>
		<idno type="DOI">10.3102/0013189X10363007</idno>
		<ptr target="https://doi.org/https://doi.org/10.3102/0013189X10363007" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A review on fairness in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pessach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shmueli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3494672</idno>
		<ptr target="https://doi.org/10.1145/3494672" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Applications of item characteristic curve theory to the problem of test bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of computerized adaptive testing: Proceedings of a symposium presented at the 18th annual convention of military testing association</title>
		<editor>D. J. Weiss</editor>
		<imprint>
			<publisher>Psychometric Methods Program</publisher>
			<date type="published" when="1977" />
			<biblScope unit="page" from="37" to="43" />
		</imprint>
		<respStmt>
			<orgName>University of Minnesota, Department of Psychology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An intersectional approach to differential item functioning: Reflecting configurations of inequality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="DOI">10.7275/20614854</idno>
		<ptr target="https://doi.org/https://doi.org/10.7275/20614854" />
	</analytic>
	<monogr>
		<title level="j">Practical Assessment, Research, and Evaluation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An intersectional approach to DIF: Do initial findings hold across tests?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Szendey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="DOI">10.1080/10627197.2021.1965473</idno>
		<ptr target="https://doi.org/10.1080/10627197.2021" />
	</analytic>
	<monogr>
		<title level="j">Educational Assessment</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="284" to="298" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An intersectional approach to DIF: Comparing outcomes across methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Szendey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1080/10627197.2022.2094757</idno>
		<ptr target="https://doi.org/10.1080/10627197.2022.2094757" />
	</analytic>
	<monogr>
		<title level="j">Educational Assessment</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="135" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shealy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stout</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf02294572</idno>
		<ptr target="https://doi.org/10.1007/bf02294572" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="194" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The generalized Mantel-Haenszel statistic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Somes</surname></persName>
		</author>
		<idno type="DOI">10.2307/2684866</idno>
		<ptr target="https://doi.org/10.2307/2684866" />
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="108" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A psychometric framework for evaluating fairness in algorithmic decision making: Differential algorithmic functioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.3102/10769986231171711</idno>
		<ptr target="https://doi.org/10.3102/10769986231171711" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Designing optimal, data-driven policies from multisite randomized trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-023-09937-2</idno>
		<ptr target="https://doi.org/10.1007/s11336-023-09937-2" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1171" to="1196" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting differential item functioning using logistic regression procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1990.tb00754.x</idno>
		<ptr target="https://doi.org/10.1111/j.1745-3984.1990.tb00754.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="370" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A penalty approach to differential item functioning in rasch models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schauberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-013-9377-6</idno>
		<ptr target="https://doi.org/10.1007/s11336-013-9377-6" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="43" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Eighth-grade algebra: Findings from the eighth-grade round of the early childhood longitudinal study, kindergarten class of 1998-99 (ECLS-K). statistics in brief. NCES 2010-016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccarroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>National Center for Education Statistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using lasso and adaptive lasso to identify DIF in multidimensional 2PL models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273171.2021.1985950</idno>
		<ptr target="https://doi.org/10.1080/00273171.2021.1985950" />
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="387" to="407" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoyanovich</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.FORC.2021.7</idno>
		<ptr target="https://doi.org/10.4230/LIPIcs.FORC.2021.7" />
		<title level="m">Causal intersectionality and fair ranking. 2021 Symposium on Foundations of Responsible Computing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9868.2005.00532.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9868.2005.00532.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nearly unbiased variable selection under minimax concave penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-aos729</idno>
		<ptr target="https://doi.org/10.1214/09-aos729" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The elementary school perfor mance and adjustment of children who enter kindergarten late or repeat kindergarten: Findings from national surveys (statistical analysis report NCES 98-097)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>West</surname></persName>
		</author>
		<ptr target="http://www.rand.org/pubs/technicalreports/TR678/" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
