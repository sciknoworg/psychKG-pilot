<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Memory Models as a General Computational Framework for Exploring Language Processing: Bringing the Lexicon to Life</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
							<email>brendan.johns@mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Manitoba</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J C</forename><surname>Crump</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Brooklyn College</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Indiana University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>McGill College Avenue Montreal</addrLine>
									<postCode>2001, H3A 1G1</postCode>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Memory Models as a General Computational Framework for Exploring Language Processing: Bringing the Lexicon to Life</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Instance memory</term>
					<term>Corpus-based modeling</term>
					<term>Computational modeling</term>
					<term>Usage-based theories</term>
					<term>Machine learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Instance models have been successfully applied to a range of problems including memory, language, attention, learning, action, decision making, and categorization (see Jamieson, Johns, Vokey, &amp; Jones, 2022). According to instance theory, the individual experience constitutes the fundamental unit of knowledge and knowledge of the general emerges during parallel retrieval from memory (Brooks, 1978, 1987). Until recently, applications of instance theory to the problem of language were constrained to small and contrived laboratory experiments (e.g., Jamieson &amp; Mewhort, 2009). However, the approach has now been applied at scale to the large and messy problem of natural language (e.g., Jamieson et al., 2018; Johns &amp; Jones, 2015; Johns et al., 2020). With those demonstrations now in hand, we argue that the framework can present an articulate mechanistic underbelly to the usage-based theory of language that highlights the role of specific language experience in general language behavior (Abbot-Smith &amp; Tomasello, 2006; Tomasello, 2003). Overall, this article argues that instance memory models provide an ability to deepen our understanding of language as a dynamic, contextually embedded process, serving to bridge the gap between cognitive psychology and the language sciences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INSTANCE MODELS OF LANGUAGE 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Memory Models as a General Computational Framework for Exploring Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processing: Bringing the Lexicon to Life</head><p>Classic theories of language processing propose that the human ability to use language is a special process operating in parallel to, rather than arising from, basic cognitive and memorial processes <ref type="bibr" target="#b11">(Chomsky, 1957</ref><ref type="bibr" target="#b12">(Chomsky, , 1988</ref><ref type="bibr" target="#b13">(Chomsky, , 1991</ref>. Under this framework, language is a species-specific, innate, special system linking arbitrary symbols (signifiers) to meaning (signified). We can remember language, but the remembering of language is in addition to the processes governing language. The work under this perspective has provided a great deal of insight and explanation, presenting language as a cognitive domain unto itself, separable from the cognitive process that govern cognition in other domains of inquiry (e.g., attention, memory, decision).</p><p>However, recent work by cognitive scientists has arrived at a counter point: language processing is not a special class of cognitive operations but, rather, emerges out of the same cognitive processes that steer and constitute other classes of cognition including learning, remembering, thinking, and knowing <ref type="bibr">(Beckner et al., 2008;</ref><ref type="bibr" target="#b14">Christiansen &amp; Chater, 2008;</ref><ref type="bibr">Tomasello, 2008)</ref>. These proposals point to language functioning as arising from domain-general learning and processing functions, adapted to serve social communicative functions.</p><p>Usage-based theories <ref type="bibr" target="#b52">(Tomasello, 2000</ref><ref type="bibr" target="#b53">(Tomasello, , 2003</ref> provide an example of utilizing cognitive theory to explain language-based phenomena. Under this perspective, it is proposed that language is fundamentally socially oriented and adapted to optimize communicative effectiveness within a group. The cognitive operations underlying this framework include the memory of individual utterances that a person has experienced within a communicative context <ref type="bibr">(Abbott-Smith &amp; Tomasello, 2006;</ref><ref type="bibr" target="#b4">Bannard, Lieven, &amp; Tomasello, 2009)</ref>, with grammar-like language production abilities emerging from abstraction over these individual memories. This theory entails that language is not a system that one has; instead, language is a cognitive skill or behavior that one acquires. Moreover, the skill of language is built from the application of basic cognitive processes to a complex language environment with constraints imposed not only by the language itself (i.e., words and utterances) but the social and cultural language environment in which a person is embedded.</p><p>The usage-based perspective on language acquisition and processing is broadly consistent with the theoretical developments that have taken place in the computational cognitive modeling INSTANCE MODELS OF LANGUAGE 4 of lexical semantics, which itself follows from classic work on the philosophy of language by <ref type="bibr">Wittgenstein (1953)</ref> and <ref type="bibr" target="#b15">Firth (1957)</ref>. This class of model is referred to as distributional, or embedded, semantic models (DSMs), and the models acquire vector representations of word meanings through co-occurrence statistics in large corpora of natural language. The intuition behind these models is summarized compactly by <ref type="bibr" target="#b15">Firth (1957)</ref> who stated that, "You shall know a word by the company it keeps." These approaches exploded in popularity with the introduction of <ref type="bibr" target="#b43">Landauer and Dumais' (1997)</ref> Latent Semantic Analysis (LSA) model, which has inspired a number of different models of distributional modeling (see <ref type="bibr" target="#b17">Günther et al., 2019;</ref><ref type="bibr">Kumar, 2021</ref> for recent reviews).</p><p>Although DSMs have been extremely influential, there is a key missing ingredient in their theoretical toolbox: the ability to be environmentally adaptive. The theories have done very well at reproducing patterns from the semantic structure of words and other lexical behaviors displayed by participants in language-based experiments. However, they assume that the lexicon is a static and dead system once model training has finished. Thus, once the vector representations for words are derived, they do not change (or change very little) in response to environmental and linguistic requirements of context. In contrast, people use language actively.</p><p>Thus, a derivation of lexical representations alone cannot present the discipline with a full account of language processing without some partnership with a system that explains how those representations are used. We need to integrate or at least coordinate our theories of semantic memory with theories of episodic and working memory -where those representations come alive in the act of thinking-through-remembering. Here, we outline and propose that an instance memory-based approach to understanding cognition provides the formal tools and framework that computational models of language need to bring the lexicon to life.</p><p>Instance models are domain general models that explain a variety of phenomena across different areas of study (see <ref type="bibr" target="#b1">Ambridge, 2020, and</ref><ref type="bibr" target="#b29">Jamieson, Johns, Vokey, &amp;</ref>, for recent reviews). According to instance theory, the individual experience constitutes the fundamental unit of knowledge and knowledge of the general emerges during parallel retrieval from memory <ref type="bibr" target="#b7">(Brooks, 1978</ref><ref type="bibr" target="#b8">(Brooks, , 1987</ref>. Until recently, applications of instance theory to the problem of language were constrained to small and contrived laboratory experiments (e.g., <ref type="bibr">Jamieson &amp; Mewhort, 2009)</ref>. However, the approach has now been applied at scale to the large INSTANCE MODELS OF LANGUAGE 5 and messy problem of natural language (e.g., <ref type="bibr" target="#b22">Jamieson, Avery, Johns, &amp; Jones, 2018;</ref><ref type="bibr" target="#b30">Johns &amp; Jones, 2015;</ref><ref type="bibr" target="#b31">Johns, Jamieson, Crump, Mewhort, &amp; Jones, 2020)</ref>.</p><p>In this article, we outline how the MINERVA 2 <ref type="bibr" target="#b19">(Hintzman, 1984</ref><ref type="bibr" target="#b20">(Hintzman, , 1986</ref><ref type="bibr" target="#b21">(Hintzman, , 1988</ref>) instance model of episodic memory can be partnered with the BEAGLE <ref type="bibr" target="#b38">(Jones &amp; Mewhort, 2007)</ref> model of semantic memory to provide an articulate and constructive framework to investigate a usagebased account of language cognition. We also identify challenges that instance theory faces in relation to language and point to next steps for development and investigation of this framework.</p><p>MINERVA. Instance models come in many flavors, with the requirements for a model changing across the domains of interest. One prime example of an instance model, and one that has been applied across multiple domains of cognitive science, is provided by the MINERVA model of <ref type="bibr" target="#b19">Hintzman (1984</ref><ref type="bibr" target="#b20">Hintzman ( , 1986</ref><ref type="bibr" target="#b21">Hintzman ( , 1988</ref>. The attractive feature of this model lies in its generalizability and mechanistic transparency, which has allowed it to be adapted for many different uses.</p><p>As in most instance memory models, the fundamental component of MINERVA's operations lie in the storage and retrieval of individual memory traces. In the original implementation of <ref type="bibr" target="#b20">Hintzman (1986)</ref>, memory traces were vectors of features. During a study phase, it is assumed that each stimuli presented is encoded as a separate memory vector.</p><p>The fundamental retrieval operation that MINERVA employs is trace activation. When the model is presented with a probe, the similarity between that probe and all memory traces is taken. Most current MINERVA models use a vector cosine (normalized dot product) for the model's similarity criterion, which returns a value between -1 and 1. Once similarity is calculated each trace is activated by raising the similarity to an exponent . Trace t, when presented with probe p, is activated with:</p><formula xml:id="formula_0">( , ) = ( , )<label>(1)</label></formula><p>where, is a free scaling parameter, typically set at 3. The activation function is non-linear and preferentially activates traces that are high in similarity, while reducing medium or low similarity trace activation.</p><p>Once traces have been activated, there are two memory retrieval routes: 1) echo intensity and 2) echo content. Echo intensity is used when a decision has to be made about whether a probe is recognized and/or consistent with studied items (i.e., categorization). Echo intensity for probe p is calculated by summing the activation of all traces:</p><formula xml:id="formula_1">INSTANCE MODELS OF LANGUAGE 6 ℎ _ ( ) = ∑ ( , ) =1 (2)</formula><p>where, m is the number of traces in the model's memory store. This provides a scalar value that indexes evidence of the probe being represented in memory.</p><p>Echo content is used to retrieve information about a probe, by summing each memory trace into a composite vector, where each trace's contribution is proportional to its activation.</p><p>The echo content for a probe p is given with:</p><formula xml:id="formula_2">_ ( ) = ∑ ( , ) * =1 (3)</formula><p>where, m is the number of traces in memory. The echo content contains a representation of the associated information for a probe. For example, suppose that each vector is a sentence, with each word in the sentence being encoded in the vector. If one wanted to construct a representation of a word's co-occurrence, you could probe with a word, and the echo content would contain the latent co-occurrence representation for that word (see <ref type="bibr" target="#b22">Jamieson et al., 2018)</ref>.</p><p>Using MINERVA to examine language processing. A prime example of how MINERVA can be used to simulate language-like processing is provided by <ref type="bibr" target="#b24">Jamieson and Mewhort (2005</ref><ref type="bibr" target="#b25">, 2009a</ref><ref type="bibr">,b, 2010</ref><ref type="bibr" target="#b28">, 2011</ref>, who used the model to examine artificial grammar learning. In an artificial grammar task, first explored by <ref type="bibr" target="#b44">Miller (1958)</ref> but perhaps most famously by <ref type="bibr" target="#b48">Reber (1967)</ref>, participants memorize strings (i.e., sentences) of characters (i.e., words) that are probabilistically generated according to a grammar. When tested, participants can successfully discriminate novel unstudied strings generated from the same grammar, from novel unstudied strings generated by another. Crucially, participants are unable to articulate the strategies that they employ while doing this task -that is, the acquired knowledge is implicit. The general theory put forth to explain these results is that participants are able to learn the rules of the grammar through exposure to the strings of that grammar, but do not have conscious access to that knowledge, similar to the proposals of traditional accounts of language processing <ref type="bibr" target="#b48">(Reber, 1967)</ref>.</p><p>This theoretical account has been questioned by episodic memory researchers (see <ref type="bibr" target="#b9">Brooks &amp; Vokey, 1991;</ref><ref type="bibr" target="#b54">Vokey &amp; Brooks, 1992</ref><ref type="bibr" target="#b55">, 1994</ref>. The crux of the argument is that artificial grammar tasks are functionally equivalent to an episodic memory task, where participants are simply store the strings in memory and their decisions are sensitive to summed activation elicited by a probe at retrieval (see Equation 2). If a probe string is grammatical, it is likely that probe will have shared features with the studied items, then the echo intensity of that probe will be INSTANCE MODELS OF LANGUAGE 7 greater than a probe string from a different grammar. Jamieson and Mewhort demonstrated that such a model can account for empirical findings in the artificial grammar literature, providing a far simpler account of people's performance than more traditional accounts. The work suggests that an instance approach is, at least, a plausible and productive framework for thinking about the complexities of language behaviour. However, the model proposed by <ref type="bibr" target="#b24">Jamieson and Mewhort (2005</ref><ref type="bibr" target="#b25">, 2009a</ref><ref type="bibr">,b, 2010</ref>) contains a number of blind spots, particularly in terms of how language instances should be represented to capture the structure in how they are used (see <ref type="bibr" target="#b40">Kinder, 2010)</ref>. To overcome these issues,</p><p>Jamieson and Mewhort (2011) adopted some of the representation assumptions from a popular DSM, BEAGLE <ref type="bibr" target="#b38">(Jones &amp; Mewhort, 2007)</ref>.</p><p>Integrating MINERVA and BEAGLE. BEAGLE is a vector accumulation model that uses the storage assumptions of the popular distributed memory model TODAM <ref type="bibr" target="#b45">(Murdock, 1982</ref><ref type="bibr" target="#b46">(Murdock, , 1993</ref>.</p><p>In vector accumulation models, a semantic representation is formed continuously based on how a word is used in a sentence. The most important operation that BEAGLE adopted from TODAM was the use of circular convolution to encode associative information (e.g., encoding two words as a pair). Circular convolution, denoted ⊛, is a function that takes in two vectors and constructs a new, unique vector that represents the association between the two items <ref type="bibr" target="#b47">(Plate, 1995)</ref>.</p><p>In In the standard form of BEAGLE, each word is represented separately, thus making it a target of the criticisms outlined previously. However, the encoding framework that BEAGLE employs allows for a powerful tool to generate sentence representations. Indeed, the mechanisms that BEAGLE uses to encode both context and order information have been used to drive INSTANCE MODELS OF LANGUAGE 8 instance-based distributional models (see <ref type="bibr" target="#b22">Jamieson et al., 2018;</ref><ref type="bibr" target="#b30">Johns &amp; Jones, 2015;</ref><ref type="bibr" target="#b31">Johns et al., 2020</ref>; see also Ambridge, 2020 for a general discussion of these issues).</p><p>To illustrate how sentence representations can be constructed using BEAGLE, consider the production model of <ref type="bibr" target="#b31">Johns et al. (2020)</ref> in relation to the sentence, "The dog ran away from home." In the model, sentence representations are formed by encoding bigram, trigram, and locative (position of a word in the sentence) information, which is displayed in <ref type="figure" target="#fig_2">Figure 1</ref>. The general form for encoding an instance using this framework would be:</p><formula xml:id="formula_3">_ = ∑ ⊛ + ∑ −1 ⊛ =2 =1 + ∑ −2 ⊛ −1 ⊛ =3 (4)</formula><p>where, w is the environmental vector for word i, l is the environmental vector for location i in the sentence and n is the number of words in a sentence. In this way locative information and forward bigrams and trigrams are encoded for a sentence in a single vector, which can then be placed in memory to be used in conjunction with MINERVA's retrieval operations.</p><p>The strength of this approach comes from its simplicity, as currently the models do not integrate any higher-order information into the instances that are formed. This was a deliberate decision, as it allows for an existence proof that an instance framework can drive models of complex linguistic behavior. However, this does not mean that higher-order information cannot be included in these models. For example, as displayed in <ref type="figure" target="#fig_2">Figure 1</ref>, one could associate grammatical information into an instance by generating environmental vectors for each grammatical marker (e.g., noun, verb, adjective, etc…) and convolve word environmental vectors with the grammatical vectors. If one wanted to generate an instance that contained locative, bigram, and grammatical markers, the following equation could be used:</p><formula xml:id="formula_4">_ = ∑ ⊛ + ∑ ⊛ ( ) + =1 ∑ −1 ⊛ =2 =1 (5)</formula><p>Where ( ) is a function that returns the grammatical category for word i. By integrating higherorder information into instances, it would provide a model testing framework by which the power that different types of linguistic markers provide in accounting for linguistic behaviors could be tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INSTANCE MODELS OF LANGUAGE 9</head><p>Using instance models to account for lexical behavior. Jamieson et al. (2018; see also <ref type="bibr" target="#b42">Kwantes, 2005)</ref> has proposed an instance-based approach to distributional semantics, entitled the Instance Theory of Semantics (ITS). Unlike other DSMs, in the ITS there is no centralized representation of a word's meaning stored in memory. Instead, every linguistic context (whether it is at the sentence or document level) is stored as a single instance in memory, using the storage assumptions of BEAGLE (where each trace is the sum of environment vectors for all words in the linguistic context). To retrieve the meaning of a word, the word is treated as a memory probe, and all of the traces that a word occurred in can be retrieved using the echo content produced by a probe word as a word's representation. One major advantage of this model compared to other DSMs is that multiple words can be used as cues in a joint activation function (e.g., river *  incrementally goes through a sentence word-by-word, using each word as a probe to generate expectations about upcoming structure, in the form of an echo content vector. For example, for the sentence contained in <ref type="figure" target="#fig_2">Figure 1</ref>, each of the, dog, ran, away, etc… would be used as probe (in combination with locative information), to generate an echo content vector and summed to form a holistic representation of the meaning of the sentence. This combined representation allowed for a measure of the expectedness of a word in the sentence to be produced, similar to other model types utilizing prediction in sentence processing <ref type="bibr">(Altmann &amp; Mirkovic, 2009)</ref>. <ref type="bibr" target="#b30">Johns and Jones (2015)</ref> demonstrated that this approach could account for a variety of findings, including sentence reading-time effects, integration of linguistic and perceptual processing, and the cultural evolution of language. <ref type="bibr" target="#b31">Johns et al. (2020)</ref> proposed an instance model of language production, entitled the Instance Production Model (IPM). Similar to the ICS and ITS, the IPM stores sentence representations, but with two components: a context and order vector, concatenated together (a similar approach was taken by Johns and Jones, 2012 to integrate semantic and perceptual information using MINERVA and a DSM). The context vector is simply the sum of the environmental vectors of the words in a sentence, while the order vector is computed with equation (4). The task that the model is given in production is to take a set of unordered words (e.g., howled, wolf, loudly, the) and produce a grammatically correct sentence (i.e., The wolf howled loudly). This is done by probing with a context vector and retrieving an echo content of the order vectors, which contains the latent ordering of the probe words. This echo content vector is then compared against all possible orderings of the set of words, and the most similar is the sentence that is produced. This model was shown to be able to account for a variety of behavioral effects in the syntactic priming literature, as well as to generate syntactically correct sentences at a high rate (e.g., 80% of 6-word sentences tested were produced correctly), even with a rather limited number of instances (the model was tested at a maximum of only one million sentence instances). However, the model had no higher-level information integrated into its functioning (such as what is indicated with grammatical information in equation 5), suggesting that this approach can be built upon to increase its power significantly; in essence, producing syntactic behaviour without knowledge of syntactic structure outside how words are used in prior language experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INSTANCE MODELS OF LANGUAGE 11</head><p>Conversely, there is a growing number of instance models that have integrated linguistic representations into their operations to account for episodic memory phenomena (e.g., <ref type="bibr" target="#b10">Chang &amp; Johns, 2023;</ref><ref type="bibr" target="#b16">Guitard et al., 2025;</ref><ref type="bibr" target="#b50">Reid &amp; Jamieson, 2023)</ref>. This suggests that a stronger integration of theories of language and episodic memory processing would not just advance our understanding of the nature of language, but also how we can better understand the nature of representation in memory.</p><p>Challenges and future directions. As outlined in this article, instance models provide an attractive and articulate theoretical account for examining language processing. The attractiveness of this framework lies in its simplicity, its tie to classic theoretical notions in the cognitive sciences, and its relatedness to popular theories of language acquisition, such as usagebased grammars. However, this simplicity does not extend to implementation -it is both expensive in terms of storage requirements (the need to maintain a very large matrix in main memory) and computation (the need to calculate millions of similarity values per retrieval probe). This makes the approach unwieldly without the technical skills and computer hardware required to run the models.</p><p>There are multiple solutions to this problem. One is the development of alternative storage and retrieval techniques that limit the storage and hence processing requirements for a resulting model. There are different avenues to accomplish this -for example, one could limit the storage of new traces to only those traces that provide unique information (i.e., those experiences that are not redundant) using discrepancy-learning (e.g., <ref type="bibr" target="#b23">Jamieson, Crump, and Hannah, 2012)</ref> or distinctiveness rules (e.g., <ref type="bibr" target="#b39">Jones, Johns, &amp; Recchia, 2012;</ref><ref type="bibr" target="#b33">Johns, 2021a)</ref>.</p><p>Alternatively, one could adopt matrix dimensionality reduction techniques to limit the processing requirements for retrieval operations or utilize graphic processing units which would allow for massive parallelization of the calculation of similarity values. One could also re-express instance models in the language of distributed representation and connectionism to evaluate the potential for faster compute times <ref type="bibr" target="#b1">(Ambridge, 2020;</ref><ref type="bibr" target="#b3">Ashby &amp; Rosedahl, 2017;</ref><ref type="bibr" target="#b49">Reichle, Veldre, Yu, &amp; Andrews, 2022)</ref>. In all likelihood, these proposals will likely all have to be used in conjunction to truly scale instance models to realistic levels of experience and test their overall plausibility.</p><p>As detailed here, the BEAGLE model provides a powerful set of tools to construct instances of language. However, as usage-based theories propose, language is not just language INSTANCE MODELS OF LANGUAGE 12 itself, but rather the usage of language as a social and communicative tool that is important. None of the models outlined above have integrated this type of information into an instance language model. However, recent work by <ref type="bibr" target="#b33">Johns (2021a</ref><ref type="bibr" target="#b36">Johns ( ,b, 2023</ref><ref type="bibr" target="#b37">Johns ( , 2024</ref>; see also  has used socially-based information to explore lexical semantics and lexical organization.</p><p>It has been repeatedly found that basing models around communicative information such as the user who produced an utterance, and in what discourse it is produced in, provides additional power in accounting for lexical behaviors. This type of information could be easily integrated into an instance model, by encoding instances with markers such as who produced an utterance, and/or in what discourse, among other metadata. This would further increase the environmental adaptiveness of the approach by allowing for the model to adapt its functioning to individual social contexts, accounting for lexical phenomena like code-switching. Doing so, would represent an effort to cash in on famous suppositions from ecological cognition on the promise of emergent complex behaviours arising from simple processing mechanisms applied to structured behavioural environments <ref type="bibr" target="#b18">(Hills, Jones, &amp; Todd, 2012;</ref><ref type="bibr" target="#b51">Simon, 1969)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>The goal of this article was to outline and propose that instance memory models can serve as a comprehensive computational framework for understanding language processing.</p><p>By situating language within broader cognitive theory, we move away from the traditional theoretical view that treats language as a distinct and specialized cognitive function and propose that language processing emerges from domain-general cognitive processes, including episodic memory functioning. Through a combination of the retrieval mechanisms of the instance model MINERVA, combined with instance formation utilizing tools from the DSM BEAGLE, we have detailed how instance-based approaches can explain multiple types of linguistic behavior, including lexical semantics, sentence comprehension, and language production. However, the real potential of these approaches lies in their simplicity, as the models make very few assumptions about the nature of language. As discussed, higher-order information about language can be easily integrated into instances of language, if needed to explain empirical phenomena.</p><p>Specifically, we have proposed that language is a usage-based skill that emerges from instance-based memory processes. This proposal challenges the notion that language is necessarily mediated by a specialized system that runs separately from basic cognitive processes.</p><p>Although our proposal does not rule out the possibility of a specialized system for language, a INSTANCE MODELS OF LANGUAGE 13 corollary of our proposal is that instance-based memory processes at least provide necessary and sufficient means for accounting for a wide variety of linguistic skills. In other words, if human memory systems are instance-based in the ways our modeling work describes, then regardless of whether people have a specialized system for language, we argue that it is theoretically necessary that people also have language abilities that emerge from instance-based processes. Stated differently, even if there is a specialized language system, we argue there are also instance-based language skills. Indeed, our framework demands that language skills emerge from instance-based processes. If the instance-based approach to language is ultimately unable to provide a unifying framework, future work is required to separately understand how language skills may jointly emerge from specialized language-based cognition with basic cognitive processes.</p><p>However, challenges remain, particularly regarding the computational demands and storage limitations inherent in instance memory models. Future research must address these issues by exploring efficient storage techniques and incorporating socially relevant information to create models that are theoretically robust, practically applicable, and more accurately embedded in the language environment that better reflects the communicative dynamics of language usage. By advancing our computational frameworks and fostering interdisciplinary collaboration, we can deepen our understanding of language as a dynamic, contextually embedded process, ultimately bridging the gap between cognitive psychology and the language sciences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BEAGLE, words are represented with two types of vectors -environmental and memory vectors. Environmental vectors are assumed to represent perceptual properties of a word, are static across learning, and serve as unique functional word identifiers. Memory vectors are used to accumulate word meanings, including both context and order information. The context representation of a word is derived from the sum of all other words that a target words occurs within the corpus (i.e., co-occurrence information). The order representation is generated by encoding the n-grams of words surrounding a target word using circular convolution. The ability to acquire both co-occurrence and basic syntactic information distinguishes BEAGLE from other distributional models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>bank), allowing for parsimonious explanations of complex linguistic phenomena such as polysemy. Johns and Jones (2015) combined MINERVA and BEAGLE to account for a variety of sentence comprehension effects, entitled the Instance Comprehension Model (ICM). The model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>A list of candidate component representations (n-grams) for the sentence representation of, "The dog ran away from home." To form an instance of a sentence, these convolutions are summed into a single composite vector.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Wittgenstein, L. (1953). Philosophical Investigations. Oxford, UK: Blackwell.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instance-learning and schematization in a usagebased account of syntactic acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abbot-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Linguistic Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="275" to="290" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Against stored abstractions: A radical exemplar model of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ambridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">First Language</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="509" to="559" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incrementality and prediction in human sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mirković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="583" to="609" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural interpretation of exemplar theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosedahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">472</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling children&apos;s early grammatical knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lieven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="17284" to="17289" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bybee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Schoenemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">Language is a complex adaptive system: Position paper. Language Learning</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonanalytic Concept Formation and Memory for Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognition and Categorization</title>
		<editor>E. Rosch &amp; B. Lloyd</editor>
		<meeting><address><addrLine>Hillside, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Elbaum Associates</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page" from="3" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Concepts and conceptual development: Ecological and intellectual factors in categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
		<editor>U. Neisser</editor>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="141" to="174" />
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
	</monogr>
	<note>Decentralized control of categorization: The role of prior processing episodes</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Vokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstract analogies and abstracted grammars: Comments on Reber</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="316" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating distributed semantic models with an instance memory model to explain false recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 45th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Syntactic structures. The Hague: Moutin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language and the problems of knowledge. The Managua Lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linguistics and cognitive science: Problems and mysteries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Chomskyan turn</title>
		<editor>A. Kasher</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language as shaped by the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
		<title level="m">Papers in linguistics 1934-1951</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An embedded computational framework of memory: Accounting for the influence of semantic information in verbal short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guitard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saint-Aubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<date type="published" when="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1006" to="1033" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal foraging in semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="431" to="438" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MINERVA 2: A simulation model of human memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hintzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Schema abstraction&quot; in a multiple-trace memory model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hintzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="411" to="428" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Judgments of frequency and recognition memory in a multiple-trace memory model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hintzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="528" to="551" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An instance theory of semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Avery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain and Behavior</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="119" to="136" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An instance theory of associative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hannah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="61" to="82" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The influence of grammatical, local, and organizational redundancy on implicit learning: An analysis using information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Applying an exemplar model to the serial reaction time task: Anticipating from experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1757" to="1783" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Applying an exemplar model to the artificialgrammar task: Inferring grammaticality from similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="550" to="575" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Applying an instance model to the artificialgrammar task: String-completion and performance for individual items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1014" to="1039" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grammaticality is inferred from global similarity: A reply to Kinder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="209" to="216" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instance theory as a domaingeneral framework for cognitive psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Vokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating structure from experience: A retrieval-based model of language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="233" to="251" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J C</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Production without rules: Using an instance memory model to exploit structure in natural language</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">104165</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling contextual diversity: Communicative need as a lexical organizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="525" to="557" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributional social semantics: Inferring word meanings from communication patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="page">101441</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Content matters: Measures of contextual diversity must consider semantic content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computing word meanings by aggregating individualized distributional models: Wisdom of the crowds in lexical semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="90" to="102" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Determining the relativity of word meanings through the construction of individualized models of semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">13413</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representing word meaning and order information in a composite holographic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The role of semantic diversity in lexical organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Recchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="115" to="124" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Is grammaticality inferred from global similarity?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1049" to="1056" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Comment on Jamieson &amp; Mewhort</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic memory: A review of methods, models, and current challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="40" to="80" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using context to build semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kwantes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="703" to="710" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Free recall of redundant strings of letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="485" to="491" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A theory for the storage and retrieval of item and associative information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Murdock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="609" to="626" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TODAM2: A model for the storage and retrieval of item, associative, and serial-order information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Murdock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="183" to="203" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Implicit learning of synthetic languages: The role of instructional set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Reber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A neural implementation of MINERVA 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veldre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">True and false recognition in MINERVA 2: Extension to sentences and metaphors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The Sciences of the Artificial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Do young children have adult syntactic competence? Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="209" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Constructing a language: A usage-based theory of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Harvard Univ. Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Salience of item knowledge in learning artificial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Vokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="328" to="344" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fragmentary knowledge and the processing-specific control of structural sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Vokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1504" to="1510" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
