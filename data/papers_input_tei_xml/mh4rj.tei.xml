<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mitigating AI Bias in School Psychology: Toward Equitable and Ethical Implementation 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Lockwood</surname></persName>
							<email>alockwo2@kent.edu.jeffrey</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kent State University Jeffery Brown San Diego State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Lifespan Development and Educational Sciences</orgName>
								<orgName type="department" key="dep2">Jeffrey Brown: Department of Counseling and School Psychology</orgName>
								<orgName type="institution" key="instit1">Kent State University</orgName>
								<orgName type="institution" key="instit2">San Diego State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Lifespan Development and Educational Sciences</orgName>
								<orgName type="institution">Kent State University</orgName>
								<address>
									<addrLine>150 Terrace Drive</addrLine>
									<postCode>44242</postCode>
									<settlement>Kent</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mitigating AI Bias in School Psychology: Toward Equitable and Ethical Implementation 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Intelligence</term>
					<term>School Psychology</term>
					<term>Educational Equity</term>
					<term>Systemic Inequalities</term>
					<term>Community-Engaged AI</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The integration of Artificial Intelligence (AI) into school psychology is evolving rapidly, presenting both opportunities and challenges. AI has the potential to enhance educational and mental health services by facilitating data-driven decision-making, streamlining administrative tasks, and offering personalized interventions for students. However, biases inherent in AI systems-reflecting the prejudices of developers and historical data-pose significant risks of exacerbating systemic inequalities. This paper examines AI&apos;s historical context, current applications in education, and specific uses within school psychology. It also discusses the sociopolitical factors contributing to algorithmic biases, data privacy concerns, and language/cultural inequities in AI systems. Recommendations are offered to mitigate AI biases, emphasizing the importance of diverse representation in AI development, comprehensive policy formation, transparency, and community involvement. Addressing these concerns is crucial for ensuring that AI contributes equitably to student success.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head><p>The implementation of Artificial Intelligence (AI) in the field of school psychology is a rapidly evolving development that holds both significant promise and inherent challenges. AI has changed most aspects of our lives and has the potential to revolutionize educational <ref type="bibr" target="#b72">(Perry &amp; Turner Lee, 2019)</ref> and mental health services <ref type="bibr" target="#b28">(Farmer et al., 2024)</ref> by enabling more data-driven decision-making, streamlining administrative processes, and even offering personalized interventions for students. However, the risks of perpetuating existing systemic biases and inequalities make it critical for school psychologists to engage deeply with these technologies.</p><p>School psychologists are currently in a unique position to understand and influence how the field adopts AI equitably. School psychology must grapple with the rapidly changing tech landscape, while updating its ethical principles to continue to protect the most marginalized groups from harm. To fully leverage AI's benefits, we need a thorough understanding of what it means to implement AI in school psychology and the implications it holds for practice, ethics, and social justice. The purpose of this paper is to discuss how structural racism and other biases become embedded into AI algorithms, and the potential impact of this bias on school psychology practice and the field of education more broadly. The paper will outline several recommendations for school psychology researchers and practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Very Brief History of AI and Its Implications for Education</head><p>AI has a rich history that spans decades. AI can be defined as the simulation of human intelligence by machines, particularly computer systems, including processes like learning, reasoning, and self-correction. The concept of AI was first introduced in the 1950s by researchers such as Alan Turing and John McCarthy, often referred to as the fathers of AI. Similar to the birth of school psychology as we know it, McCarthy and his colleagues first used the term "artificial intelligence" in the proposal for a conference at Dartmouth college in 1956, which brought  <ref type="bibr" target="#b2">(Anderson, 2024)</ref>. In the following decades, the development of AI was primarily driven by academic and government research, focusing on logical reasoning, language processing, and basic machine learning <ref type="bibr">(Maryville University, 2023)</ref>.</p><p>Throughout AI's history, the field has been predominantly shaped by white male scientists and researchers, which has led to a narrow focus on certain types of problems, approaches, and ideals <ref type="bibr" target="#b18">(Cave &amp; Dihal, 2020;</ref><ref type="bibr" target="#b31">Gebru, 2020)</ref>. AI teams continue to be disproportionately White and male (e.g. <ref type="bibr" target="#b15">Brown et al., 2022</ref><ref type="bibr" target="#b80">, Roopaei et al., 2021</ref> and do not represent the multicultural educational communities who may use this technology in diverse educational settings. This lack of diversity has had lasting consequences on the development of AI technologies, including the perpetuation of systemic biases. The limited representation of women and people of color in the field has contributed to gaps in understanding the broader implications of AI for various communities, particularly those from marginalized backgrounds <ref type="bibr" target="#b18">(Cave &amp; Dihal, 2020)</ref>. The biases inherent in AI reflect society's biases, in that they may be structural or interpersonal, explicit or implicit. While explicit prejudice and overt discrimination still exist but have become less acceptable today, there are still many examples of policies that disproportionately affect the health, safety and prosperity of marginalized communities (e.g. <ref type="bibr" target="#b45">Kapadia &amp; Borrell, 2023)</ref>. AI systems can also construct structural biases because they replicate the historical exclusion of minoritized communities from seats of power (e.g. <ref type="bibr" target="#b71">Park, 2022)</ref>. AI can also replicate implicit biases such as those against proxy characteristics of minoritized groups such as African Americans. For instance, Hoffman et al. (2024) discussed AI algorithms that included prejudice against users of African American Vernacular English (AAVE). Similarly, AI systems often replicate implicit biases of their human developers as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential Uses of AI in Education</head><p>While the understanding of its use is still in its infancy, AI also holds numerous potential uses that could transform education. One potential use is personalized learning, where AI supports individualized education by adapting instructional content to meet each student's unique needs. Personalized learning allows students to progress at their own pace, engage in activities suited to their learning preferences, and maintain greater control over their educational experiences. Using AI tools for differentiating assignments and creating data-driven, adaptive teaching practices enhances the learning process without significantly increasing the workload for teachers <ref type="bibr">(University of Iowa, 2024)</ref>.</p><p>Teachers can also benefit from AI, which can serve as a co-designer for curriculum and courses. AI tools can assist teachers by identifying key concepts, structuring curriculum plans, and preparing assessments. This application has the potential to make educational content more comprehensive and adaptable to diverse learning environments <ref type="bibr" target="#b40">(Holmes &amp; Miao, 2023)</ref>. Another promising application is the use of AI as a teaching assistant. Generative AI chatbots can provide individualized support to students by answering questions, suggesting relevant learning resources, and reinforcing content across various subject areas. This could lead to the development of teacher assistants that are available to provide 24/7 support <ref type="bibr" target="#b40">(Holmes &amp; Miao, 2023)</ref>. A report by Carnegie <ref type="bibr" target="#b17">Learning (2024)</ref> also suggested that about 62% of educators use AI technology either "sometimes" or "Always". The report suggested that teachers perceived AI most helpful with tasks such as "brainstorm generation" or lesson planning. However, most teachers and administrators responding to the survey "felt uncomfortable" with students using AI, including 81% worrying that students would use AI for "cheating" (Carnegie Learning, p. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head><p>An additional emerging application of AI is its use in co-designing supports for learners with special needs. Conversational AI tools could potentially assist in diagnosing learning difficulties, especially for those learners experiencing challenges related to psychological, social, or emotional issues. Through natural language engagement, these AI-powered tools aim to identify the needs of such learners and provide appropriate support or instruction. This approach could evolve into providing 1:1 advisory services to learners facing social or emotional challenges <ref type="bibr" target="#b40">(Holmes &amp; Miao, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uses of AI in School Psychology</head><p>School psychologists are increasingly incorporating AI tools into their practice, making it essential to understand both current applications and their implications. According to a recent survey by <ref type="bibr" target="#b28">Farmer et al. (2024)</ref>, approximately 64.3% of school psychologists reported using AI in their work, with 19.7% indicated weekly usage for work-related tasks. The primary applications of AI include assisting with report writing, generating recommendations, and answering work-related questions. Furthermore, research suggests that AI may be useful in assisting school psychologists in analyzing data <ref type="bibr" target="#b56">(Lockwood et al., 2024)</ref> and writing papers <ref type="bibr" target="#b56">(Lockwood &amp; Castleberry, 2024)</ref>. These uses indicate that AI is already playing a significant role in reducing the administrative burden on school psychologists and allowing them to focus more on direct student support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Addressing Bias Means Addressing the Biased Use of Technology</head><p>Systemic racism continues to affect educational outcomes for minoritized individuals throughout the U.S. Historically marginalized groups, such as Black, Indigenous, and other people of color (BIPOC), have faced unequal access to quality education, inequitable disciplinary actions <ref type="bibr" target="#b6">(Armour, 2015)</ref>, and biased assessment practices <ref type="bibr" target="#b89">(Williams et al., 1980)</ref>. One Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY of the most troubling manifestations of systemic racism in schools is the school-to-prison pipeline. This refers to policies and practices that disproportionately affect marginalized students-especially Black and Latinx youth-by criminalizing minor infractions, increasing the likelihood of school suspensions, expulsions, and contact with the juvenile justice system. The school-to-prison pipeline perpetuates a cycle of disadvantage, contributing to long-term negative educational and social outcomes for these students <ref type="bibr" target="#b46">(Kim et al., 2010;</ref><ref type="bibr" target="#b59">Mallett, 2016)</ref>. The schoolto-prison pipeline continues to perpetuate disproportional outcomes in these communities which are much more likely to be surveilled by police and punished by the criminal justice system, which disproportionately doles out harsher sentences (e.g. <ref type="bibr" target="#b38">Hinton et al., 2018)</ref>. This has the distal impact of keeping these communities in poverty with relatively less chance to improve their socioeconomic status <ref type="bibr" target="#b41">(Hudson et al., 2019)</ref>.</p><p>In addition to the school-to-prison pipeline, many schools continue to struggle with resource inequities that predominantly impact schools in lower socioeconomic status (SES) communities, which are disproportionately attended by Black and Latinx students <ref type="bibr" target="#b86">(Weathers &amp; Sosina, 2022)</ref>. These schools often have fewer resources, outdated materials, and less access to advanced coursework compared to schools in wealthier districts. Disparities in funding have led to environments where students do not receive the support or opportunities necessary to succeed academically, further widening the gap in educational attainment <ref type="bibr" target="#b26">(Duncombe, 2017)</ref>. Moreover, discriminatory disciplinary policies that disproportionately target Black and Latinx students exacerbate these inequalities, resulting in higher suspension and expulsion rates that remove students from the educational environment, setting them up for future academic failure <ref type="bibr" target="#b74">(Peterson, 2021</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head><p>Additionally, implicit biases among educators and administrators contribute to how students of color are perceived and treated in the classroom. Teachers may have lower expectations for students from marginalized communities, which affects how these students are evaluated, disciplined, and ultimately supported <ref type="bibr" target="#b10">(Beachum &amp; Gullo, 2020)</ref>. While institutions have shifted more of their focus on recognizing implicit bias among their staff, this remains a very difficult attitude to disrupt, and will likely continue to be <ref type="bibr">(Howell et al., 2024)</ref>. These inequities make it critical for any new technologies, including AI, to be thoroughly vetted to ensure they do not reinforce or amplify existing disparities within the education system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Socio-Political Factors</head><p>The socio-political climate also plays a crucial role in shaping educational policies and practices. Recent political movements, including legislation such as "Don't Say Gay" laws, book bans, and efforts to restrict social-emotional learning (SEL), create a challenging environment for education. These socio-political dynamics have resulted in limitations on what teachers are allowed to discuss in classrooms, especially regarding LGBTQ+ issues, racial equity, and critical discussions of history. "Don't Say Gay" laws have restricted the ability of teachers to openly discuss LGBTQ+ topics, which limits the support available to students questioning their sexual orientation or gender identity <ref type="bibr" target="#b35">(Goodrich, 2022)</ref>. Similarly, book bans and challenges have removed or restricted access to books that represent diverse perspectives, hindering students' opportunities to engage with a broad range of ideas <ref type="bibr" target="#b57">(Lowery, 2023)</ref>. As of September 5, 2024 the American Civil Liberties Union (ACLU) was currently tracking 530 anti-LGBTQ bills in the United States. While many of these will likely be defeated, the sheer number of efforts show the attempt to solidify the classroom as a setting for the continued oppression of minoritized (ACLU, 2024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head><p>Efforts to push back on SEL initiatives further limit schools' ability to address students' emotional and social needs-especially in a time when many children are already experiencing mental health crises (APA, 2023). The culmination of these socio-political forces restricts educators from fostering inclusive environments that acknowledge and celebrate diversity, potentially leaving marginalized students without the support they need to thrive (Human Rights Watch, 2024). In short, both historical and current systemic racism in the tech industry, education, and broader society have continued to perpetuate inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic Bias in AI-Assisted Decision-Making</head><p>Algorithmic bias (i.e., the immergence of bias withing a computer program or system;</p><p>Danks &amp; London, 2017) emerges due to bias in the training data biases of those designing these algorithms, or a mixture or both (IBM, n.d.). While a full description of algorithmic bias is beyond the scope of this paper, prominent research has highlighted the problematic assumption that technological tools such as AI are inherently fair. For instance, Buolamwini and Gebru's seminal work showed the inaccuracy with which facial recognition technology detected darker skin tones, especially those of Black women <ref type="bibr" target="#b16">(Buolamwini &amp; Gebru, 2018)</ref>. This algorithmic bias in decision-making has led to several adverse outcomes for marginalized or minoritized communities. The increased reliance on facial recognition for purposes such as "public safety" brings with it increased risks. For instance, an error in facial recognition surveillance technology led to the wrongful arrest of Robert Williams, a Black man in Detroit (American Civil Liberties Union, 2020). Algorithmic bias has also negatively affected decisions in other areas, such as unfairly denying Black people mortgages <ref type="bibr" target="#b36">(Hale, 2021)</ref>, bias against minoritized groups in hiring <ref type="bibr">(Bogen, 2021)</ref>, and inaccurate diagnoses for Black medical patients <ref type="bibr" target="#b69">(Obermeyer et al., 2019)</ref>.</p><p>While there is much nuance when using AI in decision making in terms of fairness, it is crucial Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY to inspect if these algorithms result in harm to minoritized communities and take steps to mitigate these biases (e.g. <ref type="bibr" target="#b68">Nazer et al., 2023)</ref>.</p><p>The integration of AI into educational decision-making processes brings with it the risk of algorithmic bias based on race <ref type="bibr" target="#b47">(Kiritchenko &amp; Mohammad, 2018)</ref>, nationality <ref type="bibr" target="#b67">(Narayana Venkit et al., 2023)</ref>, gender <ref type="bibr" target="#b8">(Basta et al., 2019;</ref><ref type="bibr" target="#b50">Kotek et al., 2023)</ref>, religion <ref type="bibr" target="#b0">(Abid et al., 2021)</ref>, and disability <ref type="bibr" target="#b65">(Narayana Venkit et al., 2022)</ref>. These biases can perpetuate and even exacerbate existing inequalities. Recently, civil society groups have sought the reduction of the use of AI tools throughout school systems and in some cases banning federal funding, citing the potentially harmful effects of AI surveillance on student safety and privacy <ref type="bibr" target="#b48">(Klein, 2024)</ref>. AI systems used in schools might unintentionally result in unfair student assessment practices <ref type="bibr" target="#b20">(Chinta et al., 2024)</ref>, discipline <ref type="bibr">(Maden et al., 2024)</ref>, and resource allocation due to the data they are trained on.</p><p>If AI models are trained on historical data that reflect longstanding disparities in school discipline (Institute of Education Sciences, 2019; <ref type="bibr" target="#b74">Peterson, 2021;</ref><ref type="bibr" target="#b91">Woelfel et al., 2023)</ref>, they may reproduce these biases by disproportionately identifying students of color as being at higher risk for disciplinary actions. For instance, algorithmic model early warning systems that were implemented for a decade in Wisconsin were found to inaccurately predict that Black and Hispanic students would drop out, leading to negative stigmatization and differential treatment by educators <ref type="bibr" target="#b29">(Feathers, 2023;</ref><ref type="bibr" target="#b91">Woelfel et al., 2023)</ref>. Perhaps a sign of things to come, a recent survey of 1,045 U.S. teens suggests that Black teens are twice as likely to be falsely accused of using AI to complete work assignments <ref type="bibr" target="#b58">(Madden et al., 2024)</ref>. This is due, at least in part, to AI detection software that reflects societal biases <ref type="bibr" target="#b48">(Klein, 2024)</ref>.</p><p>In student assessment and grading, AI systems might inadvertently reinforce stereotypes related to student potential based on race, socioeconomic status, or other factors <ref type="bibr">(Chinta et al., Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY 2024;</ref><ref type="bibr">Gonzalez-Calatayud et al., 2021)</ref>. These biases often emerge because the algorithms are trained on datasets that may be skewed by historical prejudices or limited in diversity <ref type="bibr" target="#b37">(Harry, 2023)</ref>. AI-driven academic tracking systems might use historical performance data that is biased against marginalized groups, leading to lower expectations and fewer opportunities for those students. Such systems may categorize students into lower academic tracks without fully considering their potential or providing them with opportunities for growth <ref type="bibr" target="#b72">(Perry, &amp; Turner Lee, 2019</ref>).</p><p>Surveillance tools designed to monitor students for emotional distress often disproportionately impact students with disabilities, escalating situations without proper context and exacerbating existing psychological challenges. AI software may flag mental health-related keywords inappropriately, leading to unwarranted interventions that can further distress these students. Instead of providing support, this lack of nuance can turn preventive measures into harmful actions. Additionally, the continuous monitoring of school-issued devices disproportionately affects Black, Hispanic, and low-income students, subjecting them to excessive scrutiny and increasing the likelihood of being flagged for minor infractions. This reinforces harmful stereotypes and creates a hostile learning environment <ref type="bibr" target="#b91">(Woelfel et al., 2023)</ref>.</p><p>Furthermore, AI's role in resource allocation within schools can also be problematic if the algorithms reflect existing funding disparities. AI systems that determine which schools or students receive additional resources might inadvertently prioritize more privileged groups if the data used includes metrics like historical funding levels or standardized test scores, which are often influenced by socioeconomic factors <ref type="bibr">(Chin et al., 2024;</ref><ref type="bibr" target="#b53">Kumar, 2021)</ref>. This creates a cycle in which under-resourced schools and students continue to receive less support, perpetuating existing educational inequities <ref type="bibr">(Chin et al., 2024)</ref>. Further, using AI systems may compromise Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY schools' ability to comply with the Federal Education Rights Protection Act (FERPA) protections for student data (Public Interest Privacy Center, n.d.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection and Privacy Concerns</head><p>AI systems require vast amounts of student data to function effectively, raising significant concerns about data privacy and security. These concerns are particularly pronounced for marginalized students, who may not be properly represented in training data and may be more vulnerable to data breaches <ref type="bibr" target="#b43">(Imada, 2024)</ref>. Furthermore, apprehension regarding data security may discourage diverse students from using AI educational platforms. This may be especially true for undocumented students who fear their legal status could be exposed. These privacy issues could create barriers to accessing AI tools <ref type="bibr" target="#b43">(Imada, 2024)</ref>. The data collected to train and operate AI systems often includes sensitive information such as academic performance, behavioral records, and socio-emotional indicators. When data collection is extensive, there is a risk it could be used to unfairly profile students, leading to biased or inequitable outcomes <ref type="bibr" target="#b61">(Marachi, 2022)</ref>.</p><p>For instance, predictive analytics might use socioeconomic data to make assumptions about a student's future success, thereby reinforcing systemic inequalities. If AI systems rely on historical data reflecting past biases, these tools may inadvertently label students from lower socioeconomic backgrounds as less capable or at higher risk of academic failure. Such labels can limit students' opportunities and have long-term effects on their educational trajectories. This kind of profiling has the potential to not only impact individual students' educational experiences but also perpetuate broader social inequities <ref type="bibr" target="#b61">(Marachi, 2022;</ref><ref type="bibr" target="#b79">Richardson, &amp; Lener Miller, 2021)</ref>.</p><p>Perhaps most alarming, AI systems designed to monitor student behavior have raised concerns about the disproportionate targeting of marginalized groups. Surveillance technologies, Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY when set to flag keywords like "gay" or "transgender," have been used to target LGBTQ+ students, leading to situations where they were forcibly outed without consent, exacerbating their vulnerabilities and contributing to a hostile learning environment <ref type="bibr" target="#b91">(Woelfel et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language and Cultural Bias</head><p>AI language models are often trained primarily on standard English, which means they may struggle to accurately process and interpret diverse dialects, accents, or non-standard variations of English. This can lead to disenfranchisement and act as a gatekeeping mechanism that favors students from the dominant culture <ref type="bibr">(Ta &amp; Turner Lee,2023)</ref>. English language learners (ELLs), who are already navigating the challenge of learning in a second language, may find AI-powered educational tools ineffective and are more likely to have their work inaccurately flagged as AI-generated (Center for <ref type="bibr">Democracy &amp; Technology, 2023;</ref><ref type="bibr" target="#b91">Woelfel et al., 2023)</ref>.</p><p>For students from culturally diverse backgrounds, the biases inherent in AI models can lead to unfair assessments of their language skills or academic potential. AI tools that are not adequately trained on diverse linguistic data may perceive non-standard grammar, accents, or culturally specific references as errors rather than variations of language. This can negatively impact student evaluations, limit students' confidence, and restrict their opportunities for success.</p><p>For example, an AI-driven assessment tool might inaccurately evaluate a student's oral presentation if it fails to recognize their regional accent, resulting in a lower score that does not accurately reflect the student's understanding of the content.</p><p>The risk of language bias is compounded by the fact that many AI systems are developed and trained with datasets that primarily reflect the language and experiences of dominant groups <ref type="bibr" target="#b37">(Harry, 2023)</ref>. As a result, the nuances of different cultural expressions and linguistic practices may be poorly represented or entirely absent, making AI less effective for students from Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY marginalized communities. Consequently, these biases in AI language models not only undermine the accuracy of assessments but also reinforce systemic inequities by disadvantaging students who do not conform to standardized language norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation in AI Development</head><p>The lack of diversity in AI development teams poses a significant challenge to creating equitable AI systems. When AI tools are developed primarily by homogenous groups, there are often blind spots in understanding the needs, experiences, and cultural contexts of diverse student populations. Worse, these systems may exclude the representation of historically marginalized groups, particularly Black communities, resulting in knowledge systems that are implicitly or explicitly racist <ref type="bibr" target="#b23">(Dancy &amp; Saucier, 2021)</ref>. Furthermore, diverse perspectives in the ecosystem often become exploited or subject to the control of dominant groups, who tokenize rather than empower these communities <ref type="bibr" target="#b71">(Park, 2022)</ref>. For instance, big tech often relies on the work of marginalized communities such as those from the global south as data labelers (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colón-Vargas, 2024).</head><p>Without diverse perspectives in the development process, educational AI tools may overlook cultural nuances or specific needs that are crucial for marginalized students. For example, an AI system developed without input from culturally diverse stakeholders may not adequately address the unique learning needs of students from different backgrounds, leading to less effective educational outcomes. Ensuring that AI tools are inclusive and capable of meeting the needs of all students requires input from various stakeholders, including educators, students, families, and community members from diverse backgrounds. These perspectives help identify potential biases, offer culturally relevant solutions, and ensure that AI tools are designed with equity in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head><p>Additionally, transparency is a key concern in the development of AI tools for education <ref type="bibr" target="#b87">(Wilton et al., 2022)</ref>. Many educational AI offerings currently lack transparency regarding their training datasets and algorithmic processes <ref type="bibr" target="#b4">(Anthis, 2022)</ref>. Based on our examination, the same may hold true for school psychology AI companies, with none providing clear guidance on the algorithms they use, the datasets they were trained on, or their data protection policies. Despite claims by some school psychology companies that they are addressing bias, without transparency, it is impossible for school psychologists to verify these claims or understand the potential impacts of these tools on marginalized communities. The lack of transparency raises questions about accountability and the ethical use of AI in school psychology settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommendations for Addressing Bias and Racism in School Psychology</head><p>To ensure AI systems are implemented equitably in school psychology, it is crucial to develop comprehensive policies that address bias and racism in both technology and practice.</p><p>These recommendations are broken down into several key areas: Developing Comprehensive Policies for AI Use in Education, Transparency and Diverse Stakeholder Engagement, and Funding and Support for Equitable AI Development. Each area is essential for ensuring that AI technologies contribute positively to an equitable and just educational landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Developing Comprehensive Policies for AI Use in Education</head><p>Developing comprehensive policies for AI use in education is a crucial step toward mitigating the risks of bias and inequity. AI developers, school administrators, and policymakers must collaborate to create fairer, more transparent AI models. This includes ensuring that datasets used to train AI systems are representative and inclusive of diverse populations <ref type="bibr" target="#b52">(Kuhlman et al., 2020)</ref>, conducting regular audits to identify and address biases, and publicly acknowledging bias when found <ref type="bibr" target="#b28">(Farmer et al., 2024;</ref><ref type="bibr" target="#b76">Raji &amp; Buolamwini, 2019)</ref>. Involving Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY educators and community members in the development process provides insight into potential blind spots. Additionally, maintaining a human-in-the-loop approach, where non-technical users can identify and fix fairness issues <ref type="bibr" target="#b63">(Nakao et al., 2022)</ref>, may help ensure that AI-driven decisions are equitable and serve the best interests of all students.</p><p>To address privacy concerns, schools and technology developers must adopt a proactive approach to data ethics beyond what is required by FERPA. This includes minimizing data collection to only what is strictly necessary for the AI system to function, implementing robust data encryption and protection measures <ref type="bibr" target="#b55">(Kangwa, 2023)</ref>, and ensuring that students and families are fully informed about how their data will be used. Transparency is key-students and parents must have a clear understanding of what data is being collected, how it is being stored, and for what purposes it will be used <ref type="bibr" target="#b28">(Farmer et al., 2024;</ref><ref type="bibr">Haresamudram et al., 2023)</ref>. Additionally, clear informed consent, including details on data usage and options to opt-out of data collection practices <ref type="bibr" target="#b3">(Andreotta et al., 2022)</ref>, is essential. By prioritizing student privacy and data security, educational institutions can help prevent the misuse of student information and ensure that AI technologies are used equitably and justly.</p><p>Addressing language and cultural biases in AI requires technology developers to ensure training datasets are representative of the linguistic diversity in educational settings <ref type="bibr" target="#b52">(Kuhlman, Jackson, &amp; Chunara, 2020)</ref>. This includes incorporating a broad range of dialects, accents, and cultural references to build more inclusive AI models <ref type="bibr">(Ta &amp; Turner Lee,2023)</ref>. Human oversight should play a critical role in evaluating AI-generated assessments to ensure fairness and equity <ref type="bibr" target="#b63">(Nakao et al., 2022)</ref>. <ref type="bibr" target="#b11">Bender et al. (2021)</ref> previous discussed this within the context of Large Language Models (LLMs) on which generative AI like ChatpGPT is based. To understand how some of these AI tools could yield biased outcomes, educators must become familiar with the Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY function and operation of structural and political bias more generally. Educators, especially those responsible for purchasing AI tools for schools, states, or districts, should be equipped to identify potential biases in AI tools and provide supplementary assessments when AI evaluations fail to capture the full scope of a student's abilities. <ref type="bibr" target="#b84">Tomasev et al. (2021)</ref> also warn against the potential bias against more "invisible" minoritized communities, such as LGBTQ+ individuals, due to the assumptions of heterosexism or heteronormativity within the design of AI algorithms.</p><p>Due to the risks of bias toward historically marginalized groups, input from representatives of these communities must be sought <ref type="bibr" target="#b28">(Farmer et al., 2024)</ref>. By acknowledging and addressing language and cultural biases, schools can better use AI to support all students in achieving their educational goals, regardless of their linguistic or cultural backgrounds.</p><p>Furthermore, states and school districts should adopt AI governance policies specifically designed to address bias in educational settings. These policies need to account for the unique challenges present in different regions and districts and should be crafted with input from educators, parents, and community members to ensure they are contextually relevant.</p><p>Additionally, the National Association of School Psychologists (NASP) and the American Psychological Association (APA) should have governance bodies to oversee emerging technologies. NASP's establishment of an AI Taskforce is a good start, though insufficient to provide proper governance and guidance on the use of these technologies. To a large extent, we believe that the rapid adoption of ChatGPT and similar generative AI technology <ref type="bibr" target="#b13">(Bick et al., 2024)</ref> has caught governing bodies of school psychology research and practice unprepared, necessitating a swift response and adaptation. However, this has not occurred. These organizations as well as educators must stay engaged with the rapid advancement of technology Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY more generally and begin to anticipate how best to evolve based on the future directions of these technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Diverse Stakeholder Engagement</head><p>Tech companies must maintain transparency throughout the development of their AI products, including outlining the training datasets, providing insight into the algorithmic processes used, and substantiating claims about bias mitigation <ref type="bibr" target="#b87">(Wilton et al., 2022)</ref>.</p><p>Transparency enables schools and policymakers to make informed decisions about which AI tools to implement and how to best utilize these technologies to support all students equitably.</p><p>Moreover, companies need to involve diverse stakeholders-such as educators, students, parents, and representatives from marginalized communities-throughout the development, implementation, and ongoing use of AI systems <ref type="bibr" target="#b28">(Farmer et al., 2024)</ref>. This ensures that AI tools are developed to reflect the real-world needs and experiences of those they are designed to serve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Investment in Equitable AI Development</head><p>Investment in equitable AI development is crucial for ensuring that technology evolves in a way that benefits everyone, particularly those historically marginalized. To address biases and systemic racism embedded in AI, targeted funding is needed to create technologies designed with equity at their core. This means prioritizing the development of algorithms and tools that are fair, transparent, and inclusive.</p><p>One key aspect of this investment involves funding initiatives that encourage diversity in the technology workforce <ref type="bibr" target="#b30">(Gangas et al., 2023)</ref>. Without a diverse set of voices contributing to AI development, the risk of perpetuating harmful biases remains high. Programs and scholarships that promote the participation of underrepresented groups in AI, data science, and other tech-related fields are critical for creating a workforce that can bring varied perspectives to Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY the table <ref type="bibr" target="#b30">(Gangas et al., 2023)</ref>. Many organizations already exist which have been doing this work for decades, such as Black Girls Code, Latinas Who Code, Google's CodeNext, and the Kapor Center, among many others <ref type="bibr" target="#b15">(Brown et al., 2022)</ref>. However, diversity in this field goes far beyond recruiting diverse individuals. Tech leaders must also ensure that their workplace culture and climate support diverse individuals, and that there is a pathway to leadership for them <ref type="bibr" target="#b15">(Brown et al., 2022)</ref>. This works to ensure that AI technologies are better suited to the needs of diverse communities.</p><p>Equitable AI development also calls for the direct involvement of marginalized groups in the development process through inclusive and participatory design <ref type="bibr">(Domin et al., 2023;</ref><ref type="bibr" target="#b71">Park, 2022;</ref><ref type="bibr" target="#b28">Farmer et al., 2024)</ref>. Funding should be directed to companies and projects that actively engage these communities, incorporating their insights into the design, testing, and implementation phases of AI systems <ref type="bibr" target="#b52">(Kuhlman et al., 2020)</ref>. This approach ensures that resulting technologies are not only more inclusive but also capable of addressing specific needs that may be overlooked by traditional development models.</p><p>Institutions such as the Institute of Education Sciences (IES), National Institutes of Health (NIH), and private companies have a significant role to play in advancing equitable technology. By prioritizing grants and investments for projects committed to reducing inequities, these organizations can help shift the industry toward more socially just practices. Private tech companies, venture capitalists, and philanthropic organizations also have a responsibility to invest in startups and initiatives led by underrepresented groups, particularly those focused on building technology to solve community-specific challenges.</p><p>Ultimately, achieving equity in AI development requires a long-term commitment to funding and support. The equitable allocation of resources can help foster innovations that not Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY only mitigate bias but also actively promote social good. By making these investments, we can work towards an AI landscape that reflects and respects the diverse fabric of society, leading to fairer outcomes for all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The adoption of AI in school psychology holds substantial promise for enhancing the efficacy and equity of educational and mental health services. However, to harness this potential effectively, it is crucial to address the biases and inequalities embedded within these technologies. The current lack of diversity in AI development teams and the socio-political forces shaping educational policies contribute to algorithmic biases that disproportionately harm marginalized communities. Ensuring fairness in AI-assisted decision-making requires transparency, comprehensive policy development, diverse stakeholder engagement, and sustained investment in equitable AI development. By adopting these measures, AI can support more just educational outcomes and contribute positively to the success of all students, regardless of their background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY together several researchers envisioning what would become this new type of technology</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large language models associate Muslims with violence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-021-00359-2</idno>
		<ptr target="https://doi.org/10.1038/s42256-021-00359-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="461" to="463" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Legislative attacks on LGBTQ rights in 2024</title>
		<ptr target="https://www.aclu.org/legislative-attacks-on-lgbtq-rights-2024" />
	</analytic>
	<monogr>
		<title level="j">American Civil Liberties Union</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AI as Philosophical Ideology: A Critical look back at John McCarthy&apos;s Program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-024-00731-</idno>
		<ptr target="https://doi.org/10.1007/s13347-024-00731-" />
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">AI, big data, and the future of consent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kirkham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rizzi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-021-01262-5</idno>
		<ptr target="https://doi.org/10.1007/s00146-021-01262-5" />
		<imprint>
			<date type="published" when="2022" />
			<publisher>AI &amp; Society</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1715" to="1728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The black-box syndrome: Embracing randomness in machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Anthis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence in Education</title>
		<imprint>
			<date type="published" when="2022-07" />
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-031-11647-6_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-11647-6_1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Restorative practices: Righting the wrongs of exclusionary school discipline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armour</surname></persName>
		</author>
		<ptr target="https://scholarship.richmond.edu/lawreview/vol50/iss3/9" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">999</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Richmond Law Review</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithmic bias in autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Danks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>London</surname></persName>
		</author>
		<ptr target="https://www.cmu.edu/dietrich/philosophy/docs/london/IJCAI17-AlgorithmicBias-Distrib.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the ImageNet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Basta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.07726" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">School leadership: Implicit bias and social justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Beachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Gullo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-14625-2_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-14625-2_66" />
	</analytic>
	<monogr>
		<title level="m">Handbook on promoting social justice in education</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="429" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</title>
		<meeting>the 2021 ACM conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2021-03" />
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The rapid adoption of generative AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deming</surname></persName>
		</author>
		<ptr target="https://www.stlouisfed.org/on-the-economy/2024/sep/rapid-adoption-generative-ai" />
		<imprint>
			<date type="published" when="2024-09-23" />
			<pubPlace>Federal Reserve Bank of St. Louis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">All the ways hiring algorithms can introduce bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bogen</surname></persName>
		</author>
		<ptr target="https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias" />
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attrition of Workers with Minoritized Identities on AI Teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Custis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</title>
		<meeting>the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</meeting>
		<imprint>
			<date type="published" when="2022-10" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The state of AI in education: Artificial intelligence in education 2024 report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carnegie</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="https://discover.carnegielearning.com/hubfs/PDFs/2024-AI-in-Ed-Report.pdf" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The whiteness of AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dihal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-020-00415-6</idno>
		<ptr target="https://doi.org/10.1007/s13347-020-00415-6" />
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="703" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Chinta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Quy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18745</idno>
		<ptr target="https://arxiv.org/abs/2407.18745" />
		<title level="m">FairAIED: Navigating fairness, bias, and ethics in educational AI applications</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting the margin: How capitalism fuels AI at the expense of minoritized groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colón</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1007/s43681-024-00502-w</idno>
		<ptr target="https://doi.org/10.1007/s43681-024-00502-w" />
	</analytic>
	<monogr>
		<title level="j">AI and Ethics</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">All the ways hiring algorithms can introduce bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dell'acqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias" />
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AI and blackness: Toward moving beyond bias and representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Dancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Saucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Technology and Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TTS.2021.3089687</idno>
		<ptr target="https://doi.org/10.1109/TTS.2021.3089687" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Standards for protecting at-risk groups in AI bias auditing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Domin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandodick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
		<ptr target="https://www.ibm.com/downloads/cas/DV4YNKZL" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unequal opportunities: Fewer resources, worse outcomes for students in schools with concentrated poverty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duncombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
			<publisher>The Commonwealth Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Family Educational Rights and Privacy Act</title>
		<editor>20 U.S.C. §</editor>
		<imprint>
			<biblScope unit="page">1232</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Artificial intelligence in practice: Opportunities, challenges, and ethical considerations. Professional Psychology: Research and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lockwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1037/pro0000595</idno>
		<ptr target="https://doi.org/10.1037/pro0000595" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Takeaways from Our Investigation into Wisconsin&apos;s Racially Inequitable Dropout Algorithm. The Markup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Feathers</surname></persName>
		</author>
		<ptr target="https://themarkup.org/the-RunningHead:MITIGATINGAIBIASINSCHOOLPSYCHOLOGYbreakdown/2023/04/27/takeaways-from-our-investigation-into-wisconsins-racially-inequitable-dropout-algorithm" />
		<imprint>
			<date type="published" when="2023-04-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Equitable Tech Policy Initiative. Kapor Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gangas</surname></persName>
		</author>
		<ptr target="https://kaporfoundation.org/equitable-tech-policy-initiative/" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Oxford handbook of ethics of AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<editor>M. Dubber, F. Pasquale, &amp; S. Das</editor>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="251" to="269" />
		</imprint>
	</monogr>
	<note>Race and gender</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predictive policing algorithms are racist. They should be dismantled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gershgorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2020-07-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Artificial intelligence for student assessment: A systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gonalez-Calatayud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prendes-Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roig-Vila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.3390/app11125467</idno>
		<ptr target="https://doi.org/10.3390/app11125467" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Don&apos;t Say Gay&quot; and the visibly invisible heterosexism of the academy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Goodrich</surname></persName>
		</author>
		<idno type="DOI">10.1080/01933922.2022.2030820</idno>
		<ptr target="https://doi.org/10.1080/01933922.2022.2030820" />
	</analytic>
	<monogr>
		<title level="j">The Journal for Specialists in Group Work</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="89" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Groupthink in counselor education</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">AI bias caused 80% of Black mortgage applicants to be denied</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hale</surname></persName>
		</author>
		<ptr target="https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/" />
		<imprint>
			<date type="published" when="2021-09-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Role of AI in education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harry</surname></persName>
		</author>
		<ptr target="https://injurity.pusatpublikasi.id/index.php/inj/article/view/52" />
	</analytic>
	<monogr>
		<title level="j">Interdisciplinary Journal and Humanity (INJURITY)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="268" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An unjust burden: The disparate treatment of Black Americans in the criminal justice system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
		<respStmt>
			<orgName>Vera Institute of Justice</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="https://www.vera.org/downloads/publications/for-the-record-unjust-burden-racial-disparities.pdf" />
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Artificial intelligence in education: Promises and implications for teaching and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Miao</surname></persName>
		</author>
		<ptr target="https://unesdoc.unesco.org/ark:/48223/pf0000376709" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding health inequalities experienced by Black men: Fundamental links between racism, socioeconomic position, and social mobility. Men&apos;s Health Equity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="408" to="432" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Why do they hate us so much? Discriminatory censorship laws harm education in Florida</title>
		<ptr target="https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples" />
	</analytic>
	<monogr>
		<title level="j">Human Rights Watch</title>
		<imprint>
			<date type="published" when="2024-06" />
		</imprint>
	</monogr>
	<note>Shedding light on AI bias with real-world examples</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generative AI&apos;s impact on students of color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Imada</surname></persName>
		</author>
		<ptr target="https://annenberg.usc.edu/research/center-public-relations/usc-annenberg-relevance-report/generative-ais-impact-students-color" />
		<imprint>
			<date type="published" when="2024-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Indicator 15: Retention, suspension, and expulsion</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
		<respStmt>
			<orgName>National Center for Education Statistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structural racism and health inequities: moving from evidence to action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Borrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of public health</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The school-to-prison pipeline: Structuring legal reform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Losen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Hewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint>
			<publisher>NYU Press Running Head</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04508</idno>
		<ptr target="https://arxiv.org/abs/1805.04508" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Black students are more likely to be falsely accused of using AI to cheat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="https://www.edweek.org/technology/black-students-are-more-likely-to-be-falsely-accused-of-using-ai-to-cheat/2024/09" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Racial disparities in automated speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koenecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nudell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quartey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mengesha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rickford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1915768117</idno>
		<ptr target="https://doi.org/10.1073/pnas.1915768117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="7684" to="7689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluating system performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kotek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mieskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="178" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<idno type="DOI">10.1162/coli_a_00443</idno>
		<ptr target="https://doi.org/10.1162/coli_a_00443" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">No computation without representation: Avoiding data and algorithm biases through diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuhlman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chunara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11836</idno>
		<ptr target="https://arxiv.org/abs/2002.11836" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Biases in artificial intelligence applications affecting human life: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Recent Technology and Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<idno type="DOI">10.35940/ijrte.D6558.1110421</idno>
		<ptr target="https://doi.org/10.35940/ijrte.D6558.1110421" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Prevention of personally identifiable information leakage in e-commerce using offline data minimization and online pseudonymization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kangwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Running Head: MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY Innovative Science and Research Technology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Examining the capabilities of GPT-4 to write an APAstyle school psychology paper. Contemporary School Psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lockwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Castleberry</surname></persName>
		</author>
		<idno type="DOI">.org/10.1007/s40688-024-00500-zOSF</idno>
		<ptr target="https://osf.io/v9537" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">But these are our stories! Critical conversations about bans on diverse literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Lowery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="34" to="47" />
		</imprint>
	</monogr>
	<note>Research in the Teaching of English</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The dawn of the AI era: Teens, parents, and the adoption of generative AI at home and school</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
	<note>Common Sense</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The school-to-prison pipeline: A critical review of the punitive paradigm shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Mallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child and Adolescent Social Work Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10560-015-0407-1</idno>
		<ptr target="https://doi.org/10.1007/s10560-015-0407-1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The big business of tracking and profiling students. The Markup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marachi</surname></persName>
		</author>
		<ptr target="https://themarkup.org/newsletter/hello-world/the-big-business-of-tracking-and-profiling-" />
		<imprint>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A history of artificial intelligence</title>
		<ptr target="https://online.maryville.edu/blog/history-of-ai/" />
		<imprint>
			<date type="published" when="2023-05" />
		</imprint>
		<respStmt>
			<orgName>Maryville University.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Toward involving endusers in interactive human-in-the-loop AI fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Strappelli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458762</idno>
		<ptr target="https://doi.org/10.1145/3458762" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Disability and bias: Evaluating machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Narayana Venkit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accessibility in AI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3554605</idno>
		<ptr target="https://doi.org/10.1145/3554605" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">AI and nationality biases: Examining cultural stereotypes in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Narayana Venkit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3554605</idno>
		<ptr target="https://doi.org/10.1145/3554605" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="121" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bias in artificial intelligence algorithms and recommendations for mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Nazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zatarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waldrip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X C</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moukheiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Digital Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">278</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Racial bias found in a major health care risk algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<date type="published" when="2019-10-25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Gender bias perpetuation and mitigation in AI technologies: challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2045" to="2057" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Making AI inclusive: Four guiding principles for ethical engagement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://partnershiponai.org/paper/making-ai-inclusive-4-guiding-principles-for-ethical-engagement/" />
	</analytic>
	<monogr>
		<title level="m">Partnership on AI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">AI is coming to schools, and if we&apos;re not careful, so will its biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner Lee</surname></persName>
		</author>
		<ptr target="https://www.brookings.edu/articles/ai-is-coming-to-schools-and-if-were-not-careful-so-will-its-biases/" />
		<imprint>
			<date type="published" when="2019-09" />
			<pubPlace>Brookings Institute</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Racial inequality in public school discipline for Black students in the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peterson</surname></persName>
		</author>
		<ptr target="https://ballardbrief.byu.edu/issue-briefs/racial-inequality-in-public-school-discipline-for-black-students-in-the-united-states" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
		<ptr target="https://publicinterestprivacy.org/ai-laws/" />
	</analytic>
	<monogr>
		<title level="j">Public Interest Privacy Center. (n.d.). AI laws. Public Interest Privacy Center</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m">AAAI/ACM Conference on AI, Ethics, and Society</title>
		<imprint>
			<biblScope unit="page" from="429" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3306618.3314245</idno>
		<ptr target="https://doi.org/10.1145/3306618.3314245" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Higher education algorithms and student data discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://slate.com/technology/2021/01/higher-education-algorithms-student-data-discrimination.html" />
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Women in AI: Barriers and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roopaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klaas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Salmon-Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grunow</surname></persName>
		</author>
		<idno type="DOI">10.1109/AIIoT52608.2021.9454202</idno>
		<ptr target="https://doi.org/10.1109/AIIoT52608.2021.9454202" />
	</analytic>
	<monogr>
		<title level="j">IEEE World AI IoT Congress</title>
		<imprint>
			<biblScope unit="page" from="497" to="503" />
			<date type="published" when="2021-05" />
			<publisher>AIIoT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Reducing biases toward minoritized populations in medical curricular content via artificial intelligence for fairer health outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salavati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dori-Hacohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI</title>
		<meeting>the AAAI/ACM Conference on AI</meeting>
		<imprint>
			<date type="published" when="2024" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1269" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">How language gaps constrain generative AI development. Brookings Institution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp; Turner</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<ptr target="https://www.brookings.edu/articles/how-language-gaps-constrain-generative-ai-development/" />
		<imprint>
			<date type="published" when="2023-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<title level="m">MITIGATING AI BIAS IN SCHOOL PSYCHOLOGY</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<title level="m">Fairness for unobserved characteristics: Insights from technological impacts on queer communities</title>
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The role of AI in modern education</title>
		<ptr target="https://onlineprograms.education.uiowa.edu/blog/role-of-ai-in-modern-education" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2024" />
			<biblScope unit="page" from="254" to="265" />
		</imprint>
		<respStmt>
			<orgName>University of Iowa.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Separate remains unequal: Contemporary segregation and racial disparities in school district revenue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Weathers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Sosina</surname></persName>
		</author>
		<idno type="DOI">10.3102/00028312221103917</idno>
		<ptr target="https://doi.org/10.3102/00028312221103917" />
	</analytic>
	<monogr>
		<title level="j">American Educational Research Journal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="905" to="938" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<title level="m">Where is the AI? AI literacy for educators. International Conference on Artificial Intelligence in Education</title>
		<imprint>
			<date type="published" when="2022-07" />
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-031-11647-6_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-11647-6_14" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The war against testing: A current status report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dotson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Negro Education</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Late applications: Disproportionate effects of generative AI detectors on English learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woelfel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-12" />
		</imprint>
		<respStmt>
			<orgName>Center for Democracy &amp; Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">CDT Report-Late Applications: Protecting Students&apos; Civil Rights in the Digital Age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Woelfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aboulafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
		<respStmt>
			<orgName>Center for Democracy &amp; Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
