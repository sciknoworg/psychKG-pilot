<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A signal detection perspective on error and unfairness detection as a critical aspect of human oversight of AI-based systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Langer</surname></persName>
							<email>markus.langer@uni-marburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Philipps-Universität Marburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Baum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Schlicker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Philipps-Universität Marburg</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Digitalisierung in psychologischen Handlungsfeldern</orgName>
								<orgName type="institution">Philipps-Universität Marburg</orgName>
								<address>
									<addrLine>Gutenbergstraße 18</addrLine>
									<postCode>35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Philipps-Universität Marburg</orgName>
								<address>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A signal detection perspective on error and unfairness detection as a critical aspect of human oversight of AI-based systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human oversight</term>
					<term>error detection</term>
					<term>signal detection theory</term>
					<term>human-AI decisionmaking</term>
					<term>fairness</term>
					<term>risk mitigation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Legislation and ethical guidelines around the globe call for effective human oversight of the use of AI-based systems in high-risk contextsthat is oversight that reduces the risks associated with the use of AI-based systems. Such risks may relate to the imperfect accuracy of systems (e.g., incorrect classifications) or to ethical concerns (e.g., unfairness of outputs). Given the significant role that human oversight is expected to play in the operation of AI-based systems, it is crucial to gain a better understanding of the conditions for effective oversight. In this paper, we build on the literature on the management of imperfect automation to show that the reliable detection of errors or other deviant behavior is crucial for the effective management of these errors and thus crucial for effective oversight. We then propose that Signal Detection Theory (SDT) offers a promising framework for better understanding what affects people&apos;s sensitivity (i.e., how well they are able to detect errors) and response bias (i.e., the tendency to report errors given a perceived evidence of an error) in detecting errors. To demonstrate the broad applicability of an SDT perspective to the study of error detection when overseeing AIbased systems, we then explicate the specifics for the case of unfairness detection. Additionally, we propose factors (task-, system-, and person-related factors) that may affect the sensitivity and response bias of humans tasked with detecting unfairness associated with the use of AIbased systems. Finally, we discuss implications and future research directions for an SDT perspective on error detection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Legislation (e.g., the European proposal for an AI Act; 2021) and ethical guidelines around the globe <ref type="bibr" target="#b45">(Green, 2022)</ref> call for human oversight to reduce the risks associated with the use of AI-based systems in high-risk contexts. Such risks may relate to imperfect accuracy of systems (e.g., incorrect classification of a disease) or to ethical concerns (e.g., unfairness of outputs). When such issues arise, effective human oversight (i.e., oversight that reduces risk) implies that humans should be able to detect errors or other deviant behavior, correct errors or intervene when errors have happened, and report problems to help developers to improve the systems <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b69">Langer, 2023;</ref><ref type="bibr" target="#b125">Schwartz et al., 2022)</ref>. However, research has cast doubt on whether humans can actually be effective overseers <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>. For example, there are attentional and attitudinal challenges (e.g., automation bias; <ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>, decision-makers find it difficult to distinguish between correct and incorrect outputs generated by large language models <ref type="bibr" target="#b59">(Ji et al., 2023)</ref>, and experts may benefit only slightly from accurate system outputs, but incorrect outputs strongly undermine their decision performance <ref type="bibr" target="#b2">(Alberdi et al., 2004;</ref><ref type="bibr" target="#b41">Gaube et al., 2021;</ref><ref type="bibr" target="#b62">Jussupow et al., 2021)</ref>. Furthermore, people may underestimate that systems may produce ethically problematic outputs <ref type="bibr" target="#b13">(Bigman et al., 2022;</ref><ref type="bibr" target="#b16">Bonezzi &amp; Ostinelli, 2021)</ref> or may themselves contribute to ethical issues <ref type="bibr" target="#b3">(Albright, 2019;</ref><ref type="bibr" target="#b46">Green &amp; Chen, 2019)</ref>. Thus, given the significant role that humans (are supposed to) play in mitigating risk in the operation of AI-based systems, it is crucial to gain a better understanding of the conditions for effective human oversight.</p><p>While the idea of human oversight over AI-based systems has recently attracted increasing attention, understanding the conditions under which humans can effectively oversee imperfect automated systems has a long tradition <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b126">Sheridan, 2008)</ref>. The literature has shown that humans must first be effective at detecting errors before they can adequately intervene and learn to prevent future errors <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>. This can be seamlessly applied to the area of oversight of AI-based systems where the reliable detection of errors and other deviant behavior 1 will also be important for effectively dealing with and learning from these issues (i.e., is a crucial aspect of effective human oversight).</p><p>A promising framework for investigating the conditions that influence effective error detection is provided by signal detection theory (SDT). The SDT is a widely applied psychological theory of human judgment that describes how people distinguish noise from signal under conditions of uncertainty <ref type="bibr" target="#b65">(Kellen &amp; Klauer, 2018;</ref><ref type="bibr" target="#b82">Lynn &amp; Barrett, 2014)</ref>. A classic example is trying to detect a signal in noise during a hearing test. SDT proposes that decision-makers' decisions about whether a signal is present are influenced by their sensitivity (i.e., their ability to distinguish signal from noise) and their response bias (i.e., an internal threshold that determines how much evidence people need before deciding that a signal is present). Applied to error detection, SDT makes it possible to identify people's sensitivity (i.e. how well they are able to detect errors) and response bias (i.e. the tendency to judge that there is an error given a certain amount of evidence for an error) in error detection.</p><p>We propose that using an SDT perspective to investigate the conditions that influence sensitivity and response bias will advance our understanding of human decision-making in the detection of errors. However, this requires explaining the specificities associated with error detection of AI-based systems. In particular, SDT requires defining what constitutes a signal/error and what constitutes noise/normal system behavior. In classical areas of application of automated systems (e.g., automated warning systems; <ref type="bibr" target="#b108">Parasuraman et al., 2000;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>, automation errors are almost exclusively related to the accuracy of the system, that is, its ability to accurately sense, interpret, and act upon true states of the environment. For example, automation errors are associated with systems failing to detect changing environmental conditions (e.g. planes flying too close together) or recommending or taking inappropriate actions (e.g. robots manipulating the wrong parts on an assembly line). While similar issues are important for AI-based systems, what constitutes an error or deviant behavior may be less clearly defined, as what overseers should detect goes beyond accuracy-related errors, or as Article 14 of the European AI Act puts it: "Human oversight shall aim at preventing or minimizing the risks to health, safety or fundamental rights that may emerge when a highrisk AI system is used". One such risk to fundamental rights that human oversight should mitigate is possible unfairness associated with AI-based systems <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b125">Schwartz et al., 2022)</ref>. To fully exploit the potential of an SDT perspective, it is necessary to explain what an SDT perspective means for the detection of ethical issues such as unfairness.</p><p>In this paper, we build on the literature on dealing with imperfect automation to first describe that error detection is crucial for effectively dealing with these errors, and thus also crucial for effective human oversight. We then introduce the SDT and apply it in the context of error detection in AI-based systems. To demonstrate its broad applicability, we then explicate the specifics of an SDT perspective for the case of unfairness detection as an important case for dealing with ethical concerns. In the respective section, we also propose possible influences on the sensitivity and response bias of humans tasked with detecting unfairness. Finally, we discuss implications and future research directions for an SDT perspective on effective error detection. In summary, the contributions of this paper are fourfold: a) we highlight the importance of error detection for effective human oversight, b) describe that SDT offers a promising framework for studying error detection in AI-based systems, c) extend the SDT perspective by explaining its specificities for unfairness detection, and d) emphasize that research can contribute to a better understanding of decision-maker sensitivity and response bias in unfairness detection by examining the proposed influences of task-, system-, and person-related factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effective Human Oversight Requires Effective Error Detection</head><p>AI-based systems are being used for high-risk tasks ranging from autonomous driving <ref type="bibr" target="#b67">(Kraus et al., 2020)</ref>, student assessment <ref type="bibr" target="#b84">(Marcinkowski et al., 2020)</ref>, human resources <ref type="bibr" target="#b70">(Langer et al., 2022)</ref>, judicial <ref type="bibr" target="#b47">(Grgić-Hlača et al., 2019)</ref>, and medical decision-making <ref type="bibr" target="#b41">(Gaube et al., 2021)</ref>. Although they offer various advantages, AI-based systems will be imperfect <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b125">Schwartz et al., 2022)</ref>; there will always be the possibility of errors. System errors could be defined as a system producing an output that does not meet implicit or explicit criteria <ref type="bibr" target="#b126">(Sheridan, 2008)</ref>, or as a system producing an output that is inconsistent with the true state of the world <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. Such errors can include false alarms (e.g., misdiagnosing a tumor), misses (e.g., failing to detect that objects are on collision course), inaccurate decisionsupport (e.g., a large language model producing incorrect outputs), or implementing inappropriate actions (e.g., an autonomous car speeding up at a stop sign) <ref type="bibr" target="#b108">(Parasuraman et al., 2000)</ref>. The consequences of errors can range from minor annoyances to risks to safety and fundamental rights.</p><p>Policymakers around the world are highlighting the central role of effective human oversight in the use of AI-based systems for high-risk tasks <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b60">Jobin et al., 2019)</ref>, that is oversight that reduces risk in the use of AI-based systems. On the one hand, effective human oversight can be broadly understood as oversight by different stakeholders throughout the lifecycle of an AI-based system <ref type="bibr" target="#b131">(Shneiderman, 2016</ref><ref type="bibr" target="#b133">(Shneiderman, , 2020</ref>. For instance, this can mean oversight during planning, development, and design, during operation, and for a retrospective analysis of (adverse) events <ref type="bibr" target="#b131">(Shneiderman, 2016)</ref>. On the other hand, in this paper we focus on oversight at runtime or inspection time, that is, contexts in which a human is either supposed to control and oversee an algorithmic system during operation or post-hoc, where the overseer is supposed to evaluate AI-assisted processes <ref type="bibr" target="#b45">(Green, 2022)</ref>.</p><p>While the notion of human oversight has received increasing attention recently, research has long been concerned with situations in which humans work with imperfect automated systems <ref type="bibr" target="#b27">(Dixon et al., 2007;</ref><ref type="bibr" target="#b86">McBride et al., 2014;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>. For example, in supervisory control, humans monitor automated systems and check that they are functioning correctly, sometimes requiring them to take control when they detect errors <ref type="bibr" target="#b130">(Sheridan, 2021)</ref>. In aviation, pilots and air traffic controllers monitor automated systems, must detect errors, and must respond appropriately <ref type="bibr" target="#b33">(Endsley, 1995)</ref>. In the area of automated decision support, decision-makers in power plants deal with automated warning systems and evaluate whether system warnings are correct <ref type="bibr" target="#b109">(Parasuraman &amp; Wickens, 2008)</ref>, or medical doctors have to assess the quality of system recommendations <ref type="bibr" target="#b2">(Alberdi et al., 2004)</ref>.</p><p>Drawing on this broad literature can help to better understand the concept of effective human oversight. In their literature review, <ref type="bibr" target="#b86">McBride et al. (2014)</ref> highlight the importance of four processes in error management for imperfect automation: detection, explanation, correction, and integration. In their model, detection means that a human operator has to monitor an automated system as well as the world and its states in order to detect errors.</p><p>Explanation refers to trying to understand why an error has occurred. This can be done by looking for familiar patterns that may relate to similar errors in the past, by examining mental representations and knowledge about the system, or by trying to integrate information, consider options, and generate hypotheses about possible causes of the error <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>.</p><p>Correction involves taking corrective action. This may involve building on insights from the explanation process (e.g., using stored rules to correct the error), trying different options and observing the system for feedback on progress, or disabling the system and taking manual control. Integration then means that the operator can optimize future interactions with the system by learning from errors, adjusting its monitoring strategies (e.g. paying more attention to certain parts of the system), and refining its mental representation of the system (e.g.</p><p>updating known system constraints). A similar conceptualization can be found in Sheridan (2021), who describes the roles that humans play in supervisory control of automation: planning, teaching, monitoring automation, intervening, learning, with the last three roles in particular overlapping with McBride et al.'s model. Taken together, these sources show that accurate error detection is a critical aspect in effectively dealing with those errors; effective error detection is a critical aspect for effective oversight.</p><p>AI-based systems are being applied in a broader context of applications than has traditionally been the focus of automation research, but accurate detection of errors is also critical for effective oversight of AI-based systems <ref type="bibr" target="#b125">(Schwartz et al., 2022)</ref>. Accurate detection of errors is critical for operators of autonomous cars who need to detect when to take over <ref type="bibr" target="#b67">(Kraus et al., 2020)</ref>, for doctors who need to detect erroneous diagnoses before correcting them <ref type="bibr" target="#b41">(Gaube et al., 2021)</ref>, and for human resource managers or judges who need to detect whether AI-based decisions were unfair in order to override such decisions <ref type="bibr" target="#b46">(Green &amp; Chen, 2019;</ref><ref type="bibr" target="#b70">Langer et al., 2022)</ref>. The critical role of detection in dealing with undesirable system behaviors is also emphasized by various institutions. For example, the US National Institute of Standards and Technology emphasizes that the detection of unfairness reflected in system behavior is a crucial first step in dealing with such unfairness <ref type="bibr" target="#b125">(Schwartz et al., 2022)</ref>. UNESCO, in its "Recommendation on the ethics of artificial intelligence" (2022), emphasizes the relevance of transparency by pointing out that this should enable the detection of negative human rights impacts. Also the European Commission's High-Level Expert Group on AI in their "Ethics guidelines for trustworthy AI" (2019) explicitly calls for the establishment of "detection and response mechanisms" in the context of human oversight.</p><p>To avoid conflating ineffective human oversight with ineffective automation, we emphasize here that even imperfect automation can lead to better overall human-system performance than manual performance <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. For instance, <ref type="bibr" target="#b138">Skitka et al. (1999)</ref> conducted a monitoring task in a flight simulator with multiple opportunities for omission errors (failing to respond to events when automated devices fail to detect or indicate them;</p><p>Mosier <ref type="bibr">&amp; Skitka, 1999 p. 344)</ref> and commission errors (i.e., following an incorrect system recommendation, without verifying it, or despite contradictions from other sources; <ref type="bibr">Mosier &amp; Skitka, 1999 p. 344)</ref>. Out of 6 possible omission and 6 possible commission trials over 100 trials, participants missed an average of 2.44 omission trials (compared to 0.18 in the no automated support condition) and followed incorrect system advice on an average of 3.92 trials.</p><p>This suggests that participants were ineffective at distinguishing errors from appropriate system behavior. However, in the correct advice conditions, participants in the automated support group outperformed those in the manual condition. In addition, participants who received automated support performed better overall than participants in the manual condition (for similar effect see e.g., <ref type="bibr" target="#b27">Dixon et al., 2007;</ref><ref type="bibr" target="#b95">Moray et al., 2000;</ref><ref type="bibr" target="#b116">Rovira et al., 2007)</ref>. However, the issue of effective error detection remains crucial, as ineffective detection limits the full potential of overall human-system performance. This is the case, for example, when operators overlook errors or when they overwrite actually accurate system output.</p><p>Effective error detection is challenging. Research on imperfect automation has documented several psychological challenges that undermine accurate error detection. In what they describe as facets of a "control problem" in managing imperfect algorithmic systems, <ref type="bibr" target="#b152">Zerilli et al. (2019)</ref> identified challenges related to human capabilities (e.g., can humans even effectively manage systems that typically perform better than humans), attention (e.g., it is difficult for humans to maintain constant attention when monitoring mostly reliable systems), attitudes (e.g., attitudes that systems perform consistently near to perfection; <ref type="bibr" target="#b83">Madhavan &amp; Wiegmann, 2007)</ref>, and skill degradation. Relatedly, research has documented complacency effects that are associated with suboptimal levels of monitoring behavior when systems perform generally reliable <ref type="bibr" target="#b106">(Parasuraman &amp; Manzey, 2010)</ref>. Automation bias describes a bias that results from using the outputs of automated systems "as a heuristic replacement for vigilant information seeking and processing" <ref type="bibr">(Mosier et al., 1996 p. 204)</ref>. More recently, concepts such as algorithm aversion <ref type="bibr" target="#b24">(Dietvorst et al., 2015)</ref> and algorithm appreciation <ref type="bibr" target="#b80">(Logg et al., 2019)</ref> continue the tradition of exploring effects that can undermine effective human-system performance and also pose a challenge to the effective error detection. For example, algorithm aversion may lead to being overly critical of system outputs or reporting errors where none existed. Conversely, algorithm appreciation can lead people to overlook system errors. Recent studies of AI-based decision-making in different contexts also suggest that effective error detection is challenging. For example, clinicians' decisions seem to benefit slightly from accurate system outputs but suffer greatly from inaccurate ones <ref type="bibr" target="#b41">(Gaube et al., 2021;</ref><ref type="bibr" target="#b62">Jussupow et al., 2021)</ref>. Other research has argued that it may be difficult for people to realize incorrect outputs that large language models such as ChatGPT produce <ref type="bibr" target="#b59">(Ji et al., 2023)</ref>. Furthermore, research has shown that people may overwrite actually accurate system outputs <ref type="bibr" target="#b46">(Green &amp; Chen, 2019)</ref>, may not be able to judge the quality of AI-based decisions (Grgić-Hlača et al., 2019), may not expect problematic system outputs <ref type="bibr" target="#b70">(Langer et al., 2022)</ref> or may over-or undertrust system outputs <ref type="bibr" target="#b22">(de Visser et al., 2018;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010</ref>). All of this shows that we still need to better understand the conditions for effective error detection as a crucial aspect of effective human oversight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal Detection Theory and the Detection of Errors</head><p>Signal Detection Theory (SDT) is a widely accepted theory of human decision-making in ambiguous and uncertain situations. It is often applied to situations where people have to decide whether what they perceive is noise or signal. For example, SDT has been used to understand human decision-making when people discriminate between words they have heard before and new words <ref type="bibr" target="#b111">(Ratcliff et al., 1994)</ref>, between the presence and absence of a tumor <ref type="bibr" target="#b88">(McFall &amp; Treat, 1999)</ref>, between innocent and guilty defendants at court <ref type="bibr" target="#b6">(Arkes &amp; Mellers, 2002)</ref>, or between true and fake news <ref type="bibr" target="#b11">(Batailler et al., 2022)</ref>.</p><p>Detecting errors is a critical first step in responding effectively to those errors <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. This involves deciding whether system behavior is indicative of appropriate system functioning or whether it reflects errors or other deviant behavior <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b128">Sheridan, 2019</ref><ref type="bibr" target="#b130">Sheridan, , 2021</ref>. This decision often involves ambiguity and uncertainty, as it may not be clear whether system behavior is evidence of error, or because system behavior may be intransparent (which may particularly be true for modern AI-based systems; Burrell, 2016).</p><p>Consequently, deciding whether a system is functioning appropriately (noise) or whether a system has produced an error (signal) can be understood from an SDT perspective.</p><p>A similar idea of applying SDT to judging system behavior has been proposed by <ref type="bibr" target="#b128">Sheridan (2019)</ref> who suggested that the decision to trust or not trust systems can be understood from an SDT perspective. Specifically, he proposed that situations in which one should not trust a system (i.e., situations in which a system produces an error) can be viewed as situations in which there is noise, as opposed to situations in which one can trust a system (i.e., situations in which a system produces correct outputs), which can be viewed as situations in which there is signal. Unfortunately, to the best of our knowledge this idea has not yet found its way into empirical research. Furthermore, <ref type="bibr" target="#b128">Sheridan (2019)</ref> only briefly describes what the SDT perspective would look like for the case of trust without explicitly connecting it to the idea of error detection as a crucial aspect of human oversight, and without extending it to the case of unfairness detection.</p><p>Also, there are examples in the classical human factors literature where error detection can be interpreted from an SDT perspective. Note, however, that in most of these studies, participants not only detected automation errors, but also dealt with these errors and took corrective actions. For instance, in <ref type="bibr" target="#b27">Dixon et al. (2007)</ref> participants performed a monitoring task in which they had to detect when a gauge display showed values outside a predefined range and then act to solve this issue. An automated system was designed to alert when the values were outside the predefined range, but it occasionally produced misses and false alarms. The study did not explicitly analyze error detection from an SDT perspective, but if participants acted when the system did not alert them, this implies that they recognized that the system was missing something. Similarly, if participants did not act even though the system generated a warning, this implies that they detected that a system was in error (for other examples in similar tasks see, e.g., <ref type="bibr" target="#b93">Molloy &amp; Parasuraman, 1996;</ref><ref type="bibr" target="#b95">Moray et al., 2000;</ref><ref type="bibr" target="#b110">Prinzel III et al., 2005)</ref>. With this kind of research, the literature on dealing with imperfect automation provides an inspiring basis for hypotheses about factors that affect the effectiveness of detecting errors (for such inspiration see the overview papers by <ref type="bibr" target="#b86">McBride et al., 2014;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>. However, we also believe that it is necessary to explicitly distinguish the process of error detection from the process of intervention and corrective action, as there may be different factors that affect the effective performance of the former versus the latter.</p><p>We are not aware of research in AI-based decision-making that has explicitly considered an SDT perspective for the detection of AI-based system errors although there are studies that could be analyzed and interpreted from an SDT perspective (studies with several rounds of AI-supported decision-making, rounds with correct and incorrect outputs, as well as decisions regarding whether to follow or reject system outputs; see e.g., <ref type="bibr" target="#b19">Cecil et al., 2023;</ref><ref type="bibr" target="#b41">Gaube et al., 2021;</ref><ref type="bibr" target="#b46">Green &amp; Chen, 2019;</ref><ref type="bibr" target="#b124">Schoeffer et al., 2023)</ref>. We propose that SDT provides a promising general framework for studying human error detection in the context of AI-based systems, as it a) provides measures to quantify the effectiveness of error detection b) distinguishes sensitivity and response bias as two crucial aspects of human decision-making in error detection, and c) can be used to study the factors that influence sensitivity and response bias. Since many other fields have used an SDT perspective to study human decision-making, there is inspiration that research on error detection can draw on regarding possible factors that influence sensitivity and response bias. In the following, we introduce the basics of SDT using the example of error detection when working with an AI-based system in medical diagnosis (for more detailed introductions into SDT see, e.g., <ref type="bibr" target="#b65">Kellen &amp; Klauer, 2018;</ref><ref type="bibr" target="#b142">Stanislaw &amp; Todorov, 1999)</ref>. We then focus on the implications that an SDT perspective can have for research on error detection for AI-based systems, with a particular focus on the detection of unfairness.</p><p>Suppose a clinician uses an AI-based system to detect abnormal heart sounds. The system analyses audio data and classifies the audio into 'normal heartbeat' and 'abnormal heartbeat' (see <ref type="bibr">Melms et al.</ref>, 2023 for such a system). For each output of the system, the clinician has to decide whether the system is working adequately or whether the experience is evidence that the outputs may not be valid. This reflects a decision-making task with uncertainty, as there are many system behaviors to observe that may or may not indicate a problem (e.g. time to produce outputs, certainty values). In some cases, the clinician will correctly identify errors, in some cases they will incorrectly assume that there has been an error, sometimes they will overlook that there has been an error, and sometimes they will correctly assume that everything is working adequately. Depending on various factors (e.g., time pressure), the clinician may sometimes be more or less inclined to say that there was an error.</p><p>Additionally, some clinician will have a greater ability to detect errors and some will require to see more evidence to say that there has been an error than others.</p><p>Let us translate this into SDT terms. The SDT proposes that the clinician translates what they experience with the AI-based system into a specific value for a subjective decision variable (which we could call 'error evidence' in the example case). If this value is above a subjective response criterion (i.e., a threshold on the 'error evidence scale'), the clinician will decide that there was a signal (i.e., that there was an error), if this value is below this criterion the doctor will assume that what they have experienced reflects noise (i.e., appropriate system behavior). Some decisions will reflect hits (saying that there was signal when there actually was signal), some will reflect false alarms (saying that there was signal when there was actually noise), some will reflect misses (saying that there is noise when there actually was signal), and some will reflect correct rejections (saying that there was noise when there actually was noise).</p><p>Depending on various factors, the response criterion of the clinician may differ. In addition, the sensitivity of different clinician to detect signal (i.e. to detect error) will vary, as will their response criterion (e.g., some will have a liberal response bias, i.e., lower response criterion, others a conservative response bias, i.e., higher response criterion). <ref type="figure" target="#fig_0">Figure 1</ref> shows the simplest version of how this situation can be represented in terms of the SDT: the equal variance SDT for a binary decision. It shows two equal variance Gaussian distributions: the noise distribution (in our case with a mean of 0 on the error evidence scale) and the signal distribution (in our case with a mean of 2 on the error evidence scale). The presence of Gaussian distributions means that certain events reflecting system errors will feel more or less like errors than others and will lead to higher values on the internal decision variable. The same applies to events that reflect appropriate system behavior, where some events may feel more (un)usual than others. Furthermore, due to human internal variability, similar events reflecting noise (or signal) may be judged differently by human observers on different occasions <ref type="bibr" target="#b65">(Kellen &amp; Klauer, 2018)</ref>. In our example, similar system behavior reflecting an error (or appropriate system behavior) may result in different values of the internal decision variable on different occasions. Each event experienced by the clinician in their interaction with the AI-based system could now originate from the noise distribution (appropriate system behavior) or the signal distribution (system error). The overlap of the distributions means that events that actually reflect appropriate system behavior can sometimes lead to stronger perceived evidence of system error than events that actually reflect system error.</p><p>In SDT, the decision as to whether something reflects noise or signal depends on a person's sensitivity and response bias. Sensitivity is often referred to as d' and reflects the decision-maker's ability to discriminate between noise and signal. In <ref type="figure" target="#fig_0">Figure 1</ref>, sensitivity is the distance between the two distributions. The further apart the two distributions are, the easier it is for a human to decide whether an event reflects noise or signal. In equal variance SDT, where there are only binary decisions to be made (noise vs. signal), d' can be calculated using the following formula <ref type="bibr" target="#b142">(Stanislaw &amp; Todorov, 1999)</ref>:</p><formula xml:id="formula_0">d'= Φ -1 (H) -Φ -1 (FA) (1)</formula><p>where and H and FA are Hit and False Alarm rates.</p><p>The response bias is often described with c, a measure that reflects where the decisionmaker's response criterion lies in comparison to the crossing point of the two distributions (i.e., the point of equal likelihood of noise and signal, where the height of the distributions is equal; in equal variance SDT this point equals d´/2; <ref type="bibr" target="#b65">Kellen &amp; Klauer, 2018)</ref>. In the equal variance SDT, c can be calculated using the following formula:</p><formula xml:id="formula_1">c= -0.5*(Φ -1 (H) + Φ -1 (FA)) (2)</formula><p>If c were directly at the crossing point of the two distributions, its value would be 0, indicating no response bias towards noise or signal. Positive values of c indicate a more conservative response bias, requiring higher values of the internal decision variable to say that there was a signal. Negative values indicate a more liberal response bias, requiring lower values on the internal decision variable to say that there was a signal.</p><p>The main contribution of SDT is that it disentangles sensitivity and response bias as two aspects that affect human decisions in ambiguous situations. First, this allows for a nuanced analysis of decision behavior. To illustrate, note that in <ref type="figure" target="#fig_0">Figure 1</ref>, the area under the noise distribution to the right of the decision criterion reflects the false alarm rate, whereas the area under the signal distribution to the right of the decision criterion reflects the hit rate. Assuming constant d', moving the decision criterion to the right (left) will decrease (increase) the hit and false alarm rates. This shows the monotonous relationship between hits and false alarms when sensitivity remains constant. Ignoring this, for example by using the hit rate as a measure of the decision-maker's ability to detect the signal, means ignoring the fact that a higher hit rate is associated with a higher false alarm rate. In contrast, using d' and c helps to better test whether decision-makers actually have a higher sensitivity to detect signal or whether differences in hit rates are due to a different response bias.</p><p>Another way of thinking about the relationship between hits and false alarms is to use Receiver Operating Characteristics (ROC) curves. They show the relationship between hits and false alarms for different values of the response criterion. <ref type="figure" target="#fig_1">Figure 2</ref> shows an ROC curve, where moving up the curve means having a more liberal response bias, which simultaneously increases Hits and False Alarms. The only way to get closer to a more favorable ratio of hits to false alarms is with a higher sensitivity d', which moves the curve closer to the upper left corner (see <ref type="figure" target="#fig_2">Figure 3</ref>). This also implies that the area under the ROC (AUC) can be used as a measure of sensitivity, as this area increases with increasing sensitivity <ref type="bibr" target="#b142">(Stanislaw &amp; Todorov, 1999)</ref>.   <ref type="bibr" target="#b65">Kellen and Klauer (2018)</ref>.</p><p>Secondly, disentangling sensitivity and response bias allows to investigate the factors influencing these aspects. A higher sensitivity is desirable because it improves the hit rate while reducing the false alarm rate. For example, sensitivity may differ due to individual differences (e.g., cognitive ability; Alexander III, 2022) or may be affected by training <ref type="bibr" target="#b8">(Bahner et al., 2008)</ref>, or task-related aspect (e.g., varying base rates; <ref type="bibr" target="#b65">Kellen &amp; Klauer, 2018)</ref>. With respect to response bias, the value that can be considered optimal depends on the desired rate of hits and false alarms. Decision-makers could actively influence their response bias, for example, by instructing themselves to say that if in doubt, an event will be judged as noise <ref type="bibr" target="#b6">(Arkes &amp; Mellers, 2002;</ref><ref type="bibr" target="#b145">Thomas &amp; Hogue, 1976)</ref>. Furthermore, response bias can be influenced by factors such as the base rate of events <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>, the cost associated with false alarms versus misses <ref type="bibr" target="#b95">(Moray et al., 2000)</ref> or situational states (e.g., workload; <ref type="bibr" target="#b27">Dixon et al., 2007)</ref>. We later return to the influences on sensitivity and response bias for the case of unfairness detection.</p><p>There are other versions of SDT reflecting different assumptions (e.g., unequal variance of the signal and noise distributions, alternative distribution assumptions; <ref type="bibr" target="#b65">Kellen &amp; Klauer, 2018</ref>) and the SDT can be applied not only to binary decision tasks but also to forced-choice or rating tasks <ref type="bibr" target="#b65">(Kellen &amp; Klauer, 2018;</ref><ref type="bibr" target="#b142">Stanislaw &amp; Todorov, 1999)</ref>. A detailed discussion of the variants of SDT is beyond the scope of this paper. At this point, we want to emphasize that there are various possible adaptations of SDT for specific empirical questions or theoretical assumptions that can provide valuable insights, for example, regarding the psychological processes involved (for a comprehensive guide to SDT see <ref type="bibr" target="#b54">Hautus et al., 2022)</ref>.</p><p>We will focus on binary decisions in the remainder of this paper, but now briefly mention SDT for rating tasks because it may be of interest for error detection, and in particular for the case of unfairness detection. SDT can easily be applied to situations in which decisionmakers give confidence ratings for decisions, or where they are asked to rate how confident they are that an event reflects a signal. For example, the clinician in our example could be asked to respond on a scale from 1 'very confident that this reflects appropriate system behavior' to 6 'very confident that this reflects an error'. In this case, SDT suggests that moving from 1 to 6 on the rating scale reflects an ordered range of response criteria: the response criterion (i.e. the threshold on the internal decision variable, in our case the 'error evidence scale') to say 2 as opposed to 1 will be lower than the response criterion to say 3 as opposed to 2, or 6 as opposed to 5. These judgments can then be plotted as ROC curves, with each point reflecting one of these response criteria (e.g. between response 6 and 5). The formula for calculating sensitivity can be adapted accordingly (e.g., area under the ROC as a measure for sensitivity; see <ref type="bibr" target="#b142">Stanislaw &amp; Todorov, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDT for the case of unfairness detection</head><p>Regardless of the more specific case of detecting unfairness for AI-based systems and their outputs (which reflects a more specific case of detecting errors or other deviant behavior associated with AI-based systems), deciding whether a situation was (un)fair reflects a context in which SDT can be applied <ref type="bibr" target="#b5">(Alexander III, 2022)</ref>. Deciding between fairness and unfairness often reflects an ambiguous situation where there is a range of evidence that must be integrated to make this decision. The decision-maker transforms what they experience into a value on a subjective decision variable (a value on what we could call the 'unfairness evidence scale'). If this value is above a subjective response criterion, the decision-maker will decide that the situation was unfair (i.e. was signal); if this value is below this criterion, the person will assume that what they experienced was fair (i.e. was noise). In some cases what the person judges to be unfair (fair) will actually be unfair (fair), in other cases unfair will be judged to be fair, or vice versa. Some people will be more sensitive to unfairness and may need less evidence to judge an event as unfair (i.e. have a more liberal response bias) than others.</p><p>While an SDT perspective on error detection is straightforward for accuracy-related errors, deviant behavior of AI-based systems in high-risk contexts also involves ethical concerns. In these contexts, the application of an SDT perspective may initially appear less straightforward. Since human oversight should also address ethical concerns such as unfairness, we need to clarify some specifics to ensure a broad applicability of an SDT perspective for error detection. For unfairness detection, these specificities relate to the level of fairness judgment and the ground truth for (un)fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level of fairness judgment. At what level should decision-makers judge unfairness:</head><p>at an aggregate level or at the level of individual decisions? At an aggregate level, decisionmakers would evaluate entire decision processes consisting of many system outputs to decide whether a system is producing fair outputs. Although not in the context of detecting unfairness in AI-based systems, the studies by <ref type="bibr" target="#b4">Alexander III (2021</ref><ref type="bibr" target="#b45">, 2022</ref> are excellent examples of such contexts. In Alexander III (2022) participants had to assess 57 trials each reflecting a different hiring process. For each trial, participants received information about the total number of applicants, the total number of male and female applicants, and the number of male and female applicants selected. For each trial, participants had to judge whether the process was fair or unfair. With respect to the ground truth that Alexander III (2022) chose (see below the introduction to the issue of defining a ground truth for fairness), the authors calculated the participants' sensitivity and response bias in judging whether the presented processes were unfair. In a similar way, it would be possible to analyze unfairness detection for AI-based systems. Here, decision-makers could analyze several past decision processes of an AI-based system to decide whether the system has made (un)fair decisions for each process. For each process, participants could receive information about the number of decisions the system has produced, the number of (un)favorable decisions for people with protected attributes (e.g. race, age, gender), or other information available about the decision process (e.g. whether there were more complaints from a particular group of people that might reflect unfairness).</p><p>At the level of individual decisions, decision-makers could judge the fairness of several individual system outputs. For example, in a hiring context, study materials could be information about single applicants who receive an evaluation by an AI-based job interview evaluation system. For each applicant, available data could be the evaluation that the applicant received (e.g., on a scale from 1 to 10), demographic information, raw video material (e.g., the recorded responses), system handbook information (e.g., information about training data, validation information), information about evaluations that other applicants have received, as well as information about what features influenced the applicants' evaluation. For the study, it is then necessary to define the ground truth for unfairness at the individual level (see below).</p><p>For every applicant, participants could then be instructed to decide whether the system evaluation is adequate or unfair. Perhaps the decision-maker knows that the tool has problems with accurately transcribing non-native speakers' responses, which can lead to lower scores; perhaps the decision-maker notices, when watching videos of an older candidate, that the audio quality was suboptimal, possibly because the candidate struggled with technical peculiarities of the recording system, which leads to lower scores for that candidate. All this may be evidence for the decision-maker to decide that individual system decisions were unfair.</p><p>A ground truth for fairness. Computing SDT measures requires a ground truth to determine whether decision-makers' decisions reflect hits or false alarms. 2 For example, in a hearing test, there is a ground truth for whether a sound was played. However, there are contexts where the ground truth is less obvious. For example, in judicial decisions (where SDT has been used e.g. in <ref type="bibr" target="#b6">Arkes &amp; Mellers, 2002;</ref><ref type="bibr" target="#b145">Thomas &amp; Hogue, 1976</ref>), there may not be an immediately obvious ground truth for whether a defendant is guilty. In this context, the ground truth may be defined by whether the evidence actually reflects guilt, given legal standards.</p><p>In order to use SDT for unfairness detection, it is also necessary to have a ground truth.</p><p>As in the case of judicial decisions, this ground truth may vary depending on the standards of (un)fairness applied. For example, in one of the few empirical studies of unfairness detection, Alexander III (Alexander III, 2022) used the 80% rule-of-thumb of adverse impact proposed in the "Uniform guidelines on employee selection procedures" by the Equal Employment Opportunity Commission (1978) as a ground truth for determining whether their participants were correct in claiming that a selection process was (un)fair. The 80% rule-of-thumb states that if the selection rate for a minority group is less than 80% of that of the group with the highest selection rate, there is a substantial difference in the selection rates between the groups indicating an adverse impact on the minority group. For instance, if the selection rate of female applicants is .30 and that of male applicants is .40, the ratio is .75. Given the 80% rule, this would be a case where the correct decision would be to say that the selection process was unfair. Although it is not a legal definition, the 80% rule provides a numerical value that serves as a practical means of deciding when selection rates differ substantially.</p><p>However, even if there is a numerical definition of what reflects unfairness, it is often left to the discretion of decision-makers to decide whether something reflects unfairness <ref type="bibr" target="#b5">(Alexander III, 2022</ref>). In the above example, it remains an open question whether decisionmakers will decide that .75 really is substantially lower than what the 80% rule suggests <ref type="bibr" target="#b5">(Alexander III, 2022)</ref>. For instance, if there are 10 male and 10 female applicants, hiring 3 female and 4 male applicants would be unfair according to the 80% rulewhether decisionmakers will judge this situation to be unfair is another question. They may start searching for additional evidence of unfairness and in the end some decision-makers may judge that there is enough evidence of unfairness (i.e., they have a more liberal response bias) whereas others may judge that there was a fair process (i.e., they have a more conservative response bias).</p><p>Moreover, the 80% rule-of-thumb reflects only one of many possible ground truths of fairness <ref type="bibr" target="#b90">(Mehrabi et al., 2021;</ref><ref type="bibr" target="#b125">Schwartz et al., 2022)</ref>. When using an SDT lens to examine decisions about unfairness, it matters which ground truth is chosen, as this will affect the results for sensitivity and response bias evaluations. This is similar to any other situation independent of an SDT perspective, where society tries to define whether an event was (un)fair. Although the issue of trying to define what reflects (un)fairness has been a research topic for decades, mathematical notions that define fairness have become prominent in recent years with attempts to operationalize algorithmic fairness (for an overview see <ref type="bibr" target="#b90">Mehrabi et al., 2021;</ref><ref type="bibr" target="#b125">Schwartz et al., 2022)</ref>. Such mathematical notions could serve as ground truths for an SDT perspective on unfairness detection.</p><p>The beauty of an SDT perspective may now be to compare different ground truths to see if decision-maker sensitivity and response bias differ. For example, simple rules such as the 80% rule-of-thumb may be easy for decision-makers to apply, leading to high sensitivity in detecting unfairness. However, the 80% rule may not always be the best option, so it may be necessary to apply more complex ground truths. For these, it may be more difficult for humans (without additional support, e.g. technical support) to detect unfairness. In addition, different ground truths may affect the response bias of decision-makers. For example, disagreement with a definition of fairness may also lead to a more conservative response bias (i.e., requiring more evidence to say that an AI-based decision was unfair). We return to possible factors influencing sensitivity and response bias for unfairness detection in the next section.</p><p>So far, we have discussed ground truth considerations for unfairness decisions at an aggregate level. At the level of individual decisions, there are also several options for a ground truth for unfairness. Approaches to individual fairness (e.g., as described by <ref type="bibr" target="#b32">Dwork et al., 2012)</ref>, define fairness as requiring that individuals who are similar in relevant aspects be sufficiently similarly assessed. However, in terms of what makes two individuals similar, various ground truths for individual fairness may diverge. In this respect, there are a variety of possible ground truth specifications for decisions about fairness at the individual level, as well as for decisions at the aggregate level. In addition, it may be possible to build on the law to determine whether cases actually reflect unfairness on an individual level. In Europe, for example, the European Union's anti-discrimination directives have been put into law in the member states such as in the General Act on Equal Treatment (2013) in Germany that defines what constitutes direct discrimination and indirect discrimination. Such definitions could be used as a ground truth for fairness for individual decisions. It may also be possible to define a ground truth based on facets of justice such as the ones defined by <ref type="bibr" target="#b20">Colquitt (2001)</ref>. Here it may be possible to define that if certain (procedural, distributive, informational) justice rules have not been met for an individual, then the decision should be judged as unfair. For instance, this could include checking whether all necessary data were collected for the individual, whether the individual received the same information during the process as others, or whether the same decision rules were applied to the individual as to others <ref type="bibr" target="#b20">(Colquitt, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of an SDT Perspective on Error Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using SDT measures to quantify and understand error detection</head><p>Legislation around the globe calls for effective human oversight of AI-based systems for high-risk tasks. A prerequisite for effective human oversight is that decision-makers can reliably detect system errors and deviant behavior of AI-based systems <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>. Thus, understanding the conditions for effective error detection has the potential to improve our understanding of the conditions for effective human oversight.</p><p>An SDT perspective on error detection provides measures for testing the effectiveness of detection. Specifically, SDT provides measures of sensitivity and response bias in the detection of errors in AI-based systems. Decision-maker sensitivity d' can be a measure of the ability of human overseers to detect errors. In the case of unfairness detection, sensitivity means that decision-makers can accurately detect unfairness issues, and is thus a key measure for investigating their effectiveness. Understanding the response bias of decision-makers may allow research and practice to derive interventions and investigate conditions for optimal response criteria for error detection. Furthermore, only by explicitly distinguishing between sensitivity and response bias will it be possible to understand the factors that influence sensitivity and response bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factors that affect sensitivity and response bias in unfairness detection</head><p>Sensitivity and response bias determine whether decision-makers can effectively identify problems with AI-based systems, and they determine whether human-system collaboration will be effective. It is therefore crucial to understand the factors that influence sensitivity and response bias. As SDT has been applied to different decision situations across domains <ref type="bibr" target="#b65">(Kellen &amp; Klauer, 2018)</ref>, it is possible to derive propositions about what factors may affect sensitivity and response bias in the area of error detection of AI-based systems.</p><p>In the case of accuracy errors, it may be straightforward to define what reflects an error and what reflects appropriate system behavior. Also, the factors that may affect effective error detection may be similar to those reported in the literature on dealing with imperfect automation (for an overview of these factors see <ref type="bibr" target="#b86">McBride et al., 2014;</ref><ref type="bibr" target="#b130">Sheridan, 2021)</ref>. However, there may also be specificities in the case of AI-based systems (e.g. related to the lack of transparency of certain machine learning approaches; <ref type="bibr" target="#b125">Schwartz et al., 2022)</ref> or in application contexts that were traditionally not the focus of the literature on imperfect automation (e.g. judicial decisionmaking, hiring, credit approval). Since there is limited empirical evidence that can be directly interpreted from an SDT perspective on error detection for AI-based systems, this calls for empirical investigation of whether insights based on the classical automation literature generalize to the context of AI-based systems.</p><p>In the case of identifying ethical concerns, what reflects an ethical concern and what reflects appropriate system behavior may be less straightforward. In addition, there is less previous research to build on, as imperfect automation has mainly been studied in terms of system failure or accuracy-related issues. Thus, there is a need for future research, particularly on ethical concerns as risks associated with AI. Nevertheless, the literature on dealing with imperfect automation can inspire consideration of possible factors that influence human sensitivity and response bias. In what follows, we will focus on the case of unfairness detection as a particularly important example of an ethical concern, and draw on the literature on dealing with imperfect automation to provide an overview of task-related, system-related, and personrelated factors that future research could investigate for their influence on sensitivity and response bias in unfairness detection. Note that for many of the factors, what we hypothesize will likely be similar for the more general case of error detection. <ref type="table">Table 1</ref> provides an overview of these factors. This list is not intended to be exhaustive, but rather to illustrate the many possibilities that future research could explore in relation to unfairness detection for AI-based systems. <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factors that may affect sensitivity and response bias in unfairness detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity</head><p>Response bias <ref type="table">Task</ref> Notes. + = for sensitivity, this reflects that we propose that this factor may positively affect sensitivity; for response bias, this reflects that we propose that the response bias may become more conservative. -= for sensitivity, this reflects that we propose that this factor may negatively affect sensitivity; for response bias, this reflects that we propose that the response bias may become more liberal. +/-= for sensitivity, this reflects that the effect of this factor could both increase or decrease sensitivity; for response bias this reflects that that this factor could both make the response bias more conservative or more liberal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity</head><p>Task-related factors. First, the simplicity or explicitness of the standards for fairness may increase decision-maker sensitivity to detect unfairness. As mentioned above, simpler standards such as the 80% rule may make it easier to detect unfairness and thus increase sensitivity compared to more complex standards. Similarly, explicit standards should make it easier to decide whether a situation was unfair as they make it clearer what evidence to look for. <ref type="bibr" target="#b5">In Alexander III (2022)</ref>, the authors instructed participants that there should not be substantial differences in selection rates between female and male applicants. On a continuum from less to more explicit standards, this is somewhat explicit, as the instruction refers to "selection rates" in terms of "female and male" applicants as a standard for fairness. Less explicit standards may simply state that the decision-making process should be fair. More explicit standards may explain what is meant by "substantial". For example, Alexander III (2022) could have explained the 80% rule, which would have simplified the task for participants, who could simply calculate this selection ratio to decide whether the process was unfair. In the absence of a third party providing explicit standards (e.g. legislation, ethical standards in an organization), decision-makers could develop such standards for themselves.</p><p>Having such explicit standards could help to make consistent decisions about whether a system has produced unfair decisions, as opposed to subjectively changing fairness standards for each decision. Such consistency in decision-making has been shown to improve decision quality <ref type="bibr" target="#b21">(Dawes et al., 1989;</ref><ref type="bibr" target="#b50">Grove et al., 2000;</ref><ref type="bibr" target="#b68">Kuncel et al., 2013)</ref> and could therefore also increase sensitivity in detecting unfairness.</p><p>Second, time pressure may decrease sensitivity in detecting unfairness. In other domains, time pressure has been shown to decrease sensitivity, for instance due to a reduction of vigilance <ref type="bibr" target="#b119">(Sarter &amp; Schroeder, 2001)</ref> or due to negative effects on situation awareness <ref type="bibr" target="#b35">(Endsley, 2021)</ref>. Additionally, time pressure is associated with increased costs in terms of the time needed to search for and evaluate evidence of whether an event reflects signal <ref type="bibr" target="#b38">(Ezer et al., 2008)</ref>. This can undermine an effective search and evaluation of evidence <ref type="bibr" target="#b38">(Ezer et al., 2008)</ref>.</p><p>Also, <ref type="bibr" target="#b17">Buçinca et al. (2021)</ref> showed that forcing decision-makers to take more time for reflection (which can be interpreted as actively reducing time pressure) can lead to better detection of whether the system's recommendation is flawed.</p><p>Third, decision-maker accountability may affect sensitivity, that is, whether, how much, and for what kinds of outcomes decision-makers are held accountable. The general accountability literature has shown that making people feel responsible for outcomes affects human behavior (e.g. by telling them that their behavior is being monitored, that they have to justify their behavior to a third party; <ref type="bibr" target="#b53">Hall et al., 2017)</ref>. For example, accountability could influence the attention and monitoring behavior of decision-makers <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b137">Skitka et al., 2000)</ref>. In line with this, there is evidence in the imperfect automation literature that accountability can improve the sensitivity of error detection <ref type="bibr" target="#b101">(Mosier et al., 1998;</ref><ref type="bibr" target="#b137">Skitka et al., 2000)</ref>. However, high accountability may also be a demand that could increase stress in supervisory jobs, and this kind of stress may negatively affect the sensitivity to detect errors <ref type="bibr" target="#b53">(Hall et al., 2017)</ref>. Thus, it remains an empirical question how accountability affects the sensitivity of decision-makers in unfairness detection <ref type="bibr" target="#b152">(Zerilli et al., 2019)</ref>.</p><p>System-related factors. First, the reliability of the system to oversee could affect sensitivity. In the case of unfairness, the reliability of the system could be expressed as the base rate of unfair events. High system reliability combined with a low base rate of unfair events could increase human sensitivity. For example, if few events are unfair, this could make those events more salient. However, a low base rate implies that the system works adequately most of the time, which could reduce human vigilance and lead to complacent behavior <ref type="bibr" target="#b106">(Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>. In line with this, empirical findings suggest that it is rather lower system reliability that leads to higher decision-maker sensitivity <ref type="bibr" target="#b78">(Lochner &amp; Smilek, 2023;</ref><ref type="bibr" target="#b141">Sorkin &amp; Woods, 1985;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>.</p><p>In terms of system reliability, it may remain an "irony of automation" as described by <ref type="bibr" target="#b10">Bainbridge (1983)</ref> who suggested that "...the most successful automated systems, with rare need for manual intervention (...) may need the greatest investment in human operator training" <ref type="bibr">(Bainbridge, 1983, p. 777</ref>). In the case of unfairness detection, if the system produces a small number of unfair events, it may be particularly difficult to detect those events. Thus, if we want effective error detection, it may even be a (possibly controversial) option to deliberately use a less sophisticated system that makes it easier for decision-makers to detect unfairness and that may keep humans to be more vigilant <ref type="bibr" target="#b106">(Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b141">Sorkin &amp; Woods, 1985;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>. This highlights that effective error detection depends on optimizing humansystem cooperation <ref type="bibr" target="#b125">(Schwartz et al., 2022)</ref>. Optimizing the system alone may render it unmanageable, and humans will not contribute to effective error detection and management <ref type="bibr" target="#b45">(Green, 2022;</ref><ref type="bibr" target="#b141">Sorkin &amp; Woods, 1985)</ref>. This also means that we need to investigate the performance of systems alone and of human-system collaboration under different conditions.</p><p>For example, optimizing a system may result in a system with 90% accuracy, but since there are situations where it may not be allowed to fully automate a task, the interaction with a human may result in a human-system performance of only 85% (e.g. because the human may miss errors or incorrectly assume that the system behavior was incorrect). Optimizing for accuracy in human-system collaboration may mean that the system alone achieves only 80%</p><p>performance, but the human-system collaboration achieves 87% <ref type="bibr" target="#b141">(Sorkin &amp; Woods, 1985)</ref>. It is up to empirical research to describe what defines such situations, and up to designers of systems and human-system collaboration to decide what would be optimal in practice.</p><p>The level of automation may negatively affect sensitivity to detect unfairness. Higher levels of automation imply that a system takes over more aspects of a task (e.g., information analysis, decision-making, action implementation; <ref type="bibr" target="#b108">Parasuraman et al., 2000)</ref> or achieves a higher automation for individual aspects of a task (e.g., no human input required in action implementation; <ref type="bibr" target="#b64">Kaber &amp; Endsley, 2004)</ref>. Higher levels of automation also imply less human involvement and control <ref type="bibr" target="#b64">(Kaber &amp; Endsley, 2004)</ref>. Research has shown that high levels of automation can reduce people's situation awareness, which may undermine their ability to detect system errors and intervene effectively when a system fails <ref type="bibr" target="#b34">(Endsley, 2017;</ref><ref type="bibr" target="#b105">Onnasch et al., 2014;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010)</ref>. The same can be said for detecting unfairness, as high levels of automation can remove people from the decision-making situation, making it harder to judge when things are going wrong <ref type="bibr" target="#b106">(Parasuraman &amp; Manzey, 2010)</ref>. For example, in a hiring scenario, decision-makers may have a higher sensitivity to judge whether a decision was fair if they themselves have analyzed information from applicants, rather than just seeing the output of a system that scores those applicants <ref type="bibr" target="#b71">(Langer, König, et al., 2021)</ref>.</p><p>Third, the transparency and understandability of the system can affect sensitivity.</p><p>Transparency and understandability may involve having a system where it is easy to understand decision processes, as opposed to an opaque system where this may be more difficult <ref type="bibr" target="#b18">(Burrell, 2016;</ref><ref type="bibr" target="#b74">Langer, Oster, et al., 2021)</ref>. Higher levels of understandability may also include providing additional explanations employing approaches from the field of explainable artificial intelligence (XAI) <ref type="bibr" target="#b0">(Adadi &amp; Berrada, 2018;</ref><ref type="bibr" target="#b7">Arrieta et al., 2020)</ref>. For example, research has</p><p>shown that graphical and verbal information can lead to a better and quicker error detection <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b118">Sarter et al., 2007;</ref><ref type="bibr" target="#b140">Skjerve &amp; Skraaning, 2004)</ref>. For unfairness detection, increasing transparency and understandability may have similar effects, as it may make it easier to scrutinize system decisions for evidence of unfairness.</p><p>Notably, research in the area of XAI often argues that simplifying unfairness detection is one of the central reasons for optimizing system explainability <ref type="bibr" target="#b58">(Hoffman et al., 2018;</ref><ref type="bibr" target="#b74">Langer, Oster, et al., 2021;</ref><ref type="bibr" target="#b124">Schoeffer et al., 2023)</ref>. However, to date there is limited evidence that explanations can actually benefit unfairness detection. For instance, <ref type="bibr" target="#b124">Schoeffer et al. (2023)</ref> describe a study on unfairness detection and on the influence of explainability that could be interpreted from an SDT perspective. Participants were given text excerpts from the biographies of teachers and professors. Participants then received AI-based recommendations that classified the biographies as either 'teacher' or 'professor'. There was a baseline condition in which participants received only this AI-based classification, and two further conditions in which participants received different explanations. In the task-relevant explanation condition, parts of the biographies that appeared task-relevant (e.g., students, school, research) were highlighted. In the gendered explanation condition, parts of the biographies were highlighted that referred to the gender of the person behind the biography (e.g., she, he). The participants' task was to decide whether the person was actually a teacher or a professor.</p><p>The results showed that there was no effect of the explanation condition on the overall accuracy of participants' decisions. An SDT perspective now might provide additional insights.</p><p>For example, since the descriptive statistics presented in the study showed that overall there were slightly fewer corrective overrides (rejecting a wrong AI recommendation; in our words: correctly detecting unfairness, or hits) in the explanation condition than in the baseline condition, but more detrimental overrides (rejecting correct AI recommendations; in our words:</p><p>incorrectly deciding that there was unfairness, or false alarms), this suggests that participants' sensitivity may actually have been negatively affected by the explanation conditions.</p><p>Furthermore, the results may imply that the explanations may have shifted people's response bias. Specifically, the rates of corrective and detrimental overrides were higher for the condition that received an explanation highlighting that gendered words contributed to the decision of an AI-based system. This suggests that the explanations did not make people more sensitive to unfairness, but simply made them more likely to report that a decision was unfair (more on this in the section on system-related factors that may affect response bias).</p><p>Another way to increase transparency that could improve sensitivity to the detection of unfairness would be technology-based oversight aids <ref type="bibr" target="#b125">(Schwartz et al., 2022)</ref>. Technologies, in a publication <ref type="bibr" target="#b125">(Schwartz et al., 2022)</ref> on how to mitigate the risk of unfairness in system decisions, also describes that technology-based oversight aids should assist humans in dealing with potential unfairness issues. In terms of SDT, such aids attempt to amplify the signal to make it easier to detect. Similarly, they could simplify monitoring of certain aspects of the task that are difficult for humans, and could help integrate human and system detection capabilities <ref type="bibr" target="#b141">(Sorkin &amp; Woods, 1985;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>.</p><p>However, while such aids may improve sensitivity, they also come with all the challenges associated with automated decision aids such as automation bias or deskilling <ref type="bibr" target="#b31">(Dwivedi et al., 2021;</ref><ref type="bibr" target="#b96">Mosier &amp; Manzey, 2019;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010)</ref>. In terms of automation bias, people may assume that systems are working as intended <ref type="bibr" target="#b83">(Madhavan &amp; Wiegmann, 2007;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010)</ref>. In reality, supervisory assistance will always be imperfect, and decision-makers' sensitivity may suffer if they rely too heavily on potentially incorrect system recommendations <ref type="bibr" target="#b106">(Parasuraman &amp; Manzey, 2010)</ref>. In terms of deskilling, a technology-based oversight aid could initially improve human-system decisions, but if the decision-maker's sensitivity to detecting fairness issues declines over time, sensitivity benefit may only be temporary <ref type="bibr" target="#b31">(Dwivedi et al., 2021)</ref>.</p><p>Person-related factors. Other domains report detrimental effects on sensitivity for decision-makers' workload <ref type="bibr" target="#b25">(Dixon &amp; Wickens, 2006)</ref>. Error detection will be a task that involves periods of varying workload. Although it may reflect a supervisory task with periods of low engagement <ref type="bibr" target="#b130">(Sheridan, 2021)</ref>, there will be situations where workload will be high, for</p><p>instance because human overseers will be working in multi-task environments where oversight over the AI-based system will only part of the job. Similar to effects in other domains, high workload may then reduce sensitivity to detect unfairness <ref type="bibr" target="#b25">(Dixon &amp; Wickens, 2006;</ref><ref type="bibr" target="#b85">McBride et al., 2011)</ref>.</p><p>Furthermore, training could positively affect the sensitivity of decision-makers. For example, when dealing with imperfect automation, <ref type="bibr" target="#b8">Bahner et al. (2008)</ref> showed that exposing people with system errors during training can enhance their awareness of such errors and may reduce complacent behavior <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. However, research also indicates that there are some effects that may be challenging to prevent even with training (e.g., automation bias effects; <ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref>. Furthermore, such training may only improve local sensitivity to detecting such errors, but may not affect the detection of errors not experienced during training <ref type="bibr" target="#b8">(Bahner et al., 2008;</ref><ref type="bibr" target="#b86">McBride et al., 2014)</ref>. Similarly, for unfairness detection, it may be difficult to expose decision-makers to all possible unfairness events during training. Nevertheless, such training could at least improve the detection of unfairness for certain events.</p><p>In addition, decision-makers may differ with respect to their knowledge of the task and of the system they are using which could affect their sensitivity. Knowledge has been shown to affect sensitivity in several domains <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. In the imperfect automation literature, there is evidence for both positive and negative effects of knowledge on sensitivity <ref type="bibr" target="#b86">(McBride et al., 2014)</ref>. For instance, people with a higher knowledge may be better at judging whether a system produces errors due to a better knowledge of system limitations <ref type="bibr" target="#b151">(Wilkison et al., 2007)</ref>. However, without corrective feedback on assumptions about a system, knowledge may also be detrimental if wrong assumptions prevail <ref type="bibr" target="#b121">(Sarter &amp; Woods, 1994)</ref>. For example, research has shown that people may have strong initial expectations about the consistency and fairness of system decisions <ref type="bibr" target="#b13">(Bigman et al., 2022;</ref><ref type="bibr" target="#b16">Bonezzi &amp; Ostinelli, 2021</ref>) that may persist and may undermine sensitivity. In the case of unfairness detection, people may also have differing knowledge with respect to detection tasks or with the task of identifying unfairness.</p><p>This may have a positive effect on sensitivity in the detection of unfairness because decisionmakers may know better what they are looking for, may have more explicit fairness standards, or may be more aware of situations where a system may produce unfair results <ref type="bibr" target="#b5">(Alexander III, 2022</ref>).</p><p>Finally <ref type="bibr" target="#b5">, Alexander III (2022)</ref> found that participants with higher cognitive ability were more sensitive to detecting unfair situations in a hiring context. In particular, their study primarily involved numerical information related to the 80% rule (e.g., base rates, selection rates, selection ratios) and a higher cognitive ability helped participants to process this information which can provide evidence of unfairness. In contexts where most evidence relates to numerical evidence about (un)fairness, we expect cognitive ability to have a similar effect.</p><p>Whether cognitive ability has a positive effect on sensitivity to detect unfairness is less clear for decisions where a different ground truth for fairness is used (e.g., based on justice rules; <ref type="bibr" target="#b20">Colquitt, 2001)</ref> or where there is (un)fairness evidence beyond numerical information.</p><p>Nevertheless, decisions about (un)fairness may still involve the appropriate integration of complex information, which may be easier for people with higher cognitive abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response bias</head><p>Task-related factors. Research has shown that the costs and benefits associated with (in)correct decisions influence the response bias of decision-makers <ref type="bibr" target="#b54">(Hautus et al., 2022;</ref><ref type="bibr" target="#b82">Lynn &amp; Barrett, 2014)</ref>. When the costs associated with misses are high, decision-makers will adopt a more liberal response bias; when the costs associated with false alarms are high, decisionmakers will adopt a more conservative response bias. For example, in medical decisionmaking, the cost of a miss may sometimes be higher (e.g., failure to detect evidence of cancer)</p><p>than the cost of a false alarm, whereas in a court of law, the cost of a false alarm (e.g., false convictions) may be higher than the cost of a miss. In the former case, the response bias will be more liberal and in the latter case more conservative.</p><p>For unfairness detection, there are various costs that could be associated with false alarms and misses <ref type="bibr" target="#b5">(Alexander III, 2022)</ref>. For example, a false alarm could result in actual costs if a decision process has to be restarted to ensure a fair process. In addition, a false alarm could cause outrage about a process that was actually fair. This outrage could affect an organization using an AI-based system, a provider of an AI-based system, or an individual overseer. In contrast, a miss could involve unfavorable decisions for people who have been adversely affected by an unfair decision-making process. In addition, a miss could have long-term costs, as unfair decision processes remain undetected, and with each missed unfair decision, inequalities may become more entrenched. Which costs are perceived to be higher and/or more salient depends on the context of application. Research can anticipate costs in trying to predict people's response bias in detecting unfairness. For example, in a hiring situation at an aggregate level, misses may initially seem more problematic than false alarms, as misses may inadequately validate an unfair selection process. This would imply that decision-makers have a more liberal response bias. However, in a concrete decision situation, false alarms may be more costly to decision-makers because the costs be more immediate <ref type="bibr" target="#b152">(Zerilli et al., 2019)</ref>: the selection process has to be stopped, repeated, the system redesigned, and applicants may have to be informed. This would imply that decision-makers might adopt a more conservative response bias. Such considerations about the costs of decisions allow for exciting research opportunities with hypotheses about the response bias of decision-makers in the given circumstances <ref type="bibr" target="#b82">(Lynn &amp; Barrett, 2014)</ref>.</p><p>As another task-related factor, perceived accountability for decisions may influence response biases. In particular, accountability may make the cost of consequences particularly salient <ref type="bibr" target="#b53">(Hall et al., 2017)</ref>. For example, there may be instructions that make supervisors more accountable for misses than for false alarms. Examining the effects of accountability will also be important in the light of legislation such as the AI Act, since such legislation, which requires human oversight, also implies that a person who fails to fulfil their duty as an overseer may be responsible for negative consequences. The specifics of accountability (e.g., what exactly is the person's job) may then influence people's response bias.</p><p>System-related factors. System reliability affects the base rate of events, and with a higher reliability people's response bias for judging unfairness may become more conservative <ref type="bibr" target="#b54">(Hautus et al., 2022;</ref><ref type="bibr" target="#b82">Lynn &amp; Barrett, 2014)</ref>. When events have a higher base rate, this usually means that decision-makers will have a more liberal response bias <ref type="bibr" target="#b82">(Lynn &amp; Barrett, 2014)</ref>. In the case of a high base rate for unfairness, there may be a rationale for being more liberal in reporting that there was unfairness because hits by chance become more likely. However, effects may not necessarily be caused by the actual base rate, but by expected base rates <ref type="bibr" target="#b5">(Alexander III, 2022;</ref><ref type="bibr" target="#b147">Tversky &amp; Kahneman, 1973)</ref>. For example, if decision-makers work in an environment where the issue of diversity and fairness is more salient (e.g. has been made salient in ethical statements), or where there have recently been reports of unfair decisions, this may affect expected base rates of unfairness. In addition, if decision-makers have experienced unfairness in the past (e.g. first-hand experience of sexism or racism), this may increase the perceived base rate of unfairness, possibly particularly for the aspect in which they experienced unfairness <ref type="bibr" target="#b4">(Alexander III, 2021</ref><ref type="bibr" target="#b45">, 2022</ref>.</p><p>Design choices that affect transparency and comprehensibility may also affect response bias. For example, we previously presented the study by <ref type="bibr" target="#b124">Schoeffer et al. (2023)</ref> where the results showed that there were more overrides (changes to the AI-based decision) in the gendered explanation condition (where the explanations highlighted that words such as she or he affected system outputs) than in the other conditions. From an SDT perspective, this result can be interpreted as suggesting that the gendered condition led to a more liberal response bias participants were more likely to judge the system's decision as incorrect and override it, regardless of whether it was actually incorrect. Looking more closely at the specifics of Schoeffer et al.'s study, there may also have been an effect of the gender of the person whose biography participants were asked to judge. For the biographies of men, task-relevant explanations led to fewer corrective and detrimental overrides, whereas for the biographies of women, gender-related explanations led to more corrective and detrimental overrides. It seems that different explanations led to different changes in participants' response bias: for men's biographies, task-relevant explanations led to a more conservative response bias, whereas for women's biographies, gendered explanations led to a more liberal response bias. Note that our conclusions should be interpreted with caution, as our second-hand analyses from an SDT perspective are based only on the interpretation of descriptive information presented in <ref type="bibr" target="#b124">Schoeffer et al. (2023)</ref>.</p><p>Person-related factors. Higher workload may render response bias in judging unfairness more conservative. For example, in the case of workload, it may be hypothesized that a high workload may reduce the likelihood of making a decision that further increases that workload <ref type="bibr" target="#b152">(Zerilli et al., 2019)</ref> which could be supported, for example, by the cognitive miser hypothesis <ref type="bibr" target="#b40">(Fiske &amp; Taylor, 1991)</ref>. In the case of unfairness detection, judging a situation to be unfair may lead to additional effort being required (e.g., resolving the problem, reporting the problem), which may make the response bias more conservative if the decision-maker wants to avoid additional workload.</p><p>Another factor related to people's response bias may be decision-makers' perceived fairness with respect to a system or with respect to specific decisions. For example <ref type="bibr" target="#b124">, Schoeffer et al. (2023)</ref> reported that with increasing perceived fairness participants less likely overrode the system decision. This could be interpreted in a way that perceived fairness may reflect people's response bias, as high perceived fairness corresponded to a conservative response bias.</p><p>Given the significant number of studies that have examined the perceived fairness of AI-based decisions (for overviews see <ref type="bibr" target="#b73">Langer &amp; Landers, 2021;</ref><ref type="bibr" target="#b144">Starke et al., 2022)</ref>, these studies could also provide intuition as to what factors might influence people's perceived fairness and thus their response bias in detecting unfairness. To name just a few, the context of use <ref type="bibr" target="#b75">(Lee, 2018)</ref>, the stakes of the decisions <ref type="bibr" target="#b72">(Langer et al., 2019)</ref>, feature properties (e.g., their relevance, reliability; Grgić-Hlača et al., 2018; how sensitivity features are; <ref type="bibr" target="#b104">Nyarko et al., 2021)</ref>, the possibility to appeal algorithmic decisions <ref type="bibr" target="#b55">(Hellwig et al., 2023)</ref>, explanations <ref type="bibr" target="#b76">(Lee et al., 2019;</ref><ref type="bibr" target="#b103">Newman et al., 2020;</ref><ref type="bibr" target="#b123">Schlicker et al., 2021)</ref>, explanation styles <ref type="bibr" target="#b14">(Binns et al., 2018;</ref><ref type="bibr" target="#b29">Dodge et al., 2019)</ref>, and visualization techniques <ref type="bibr" target="#b149">(Van Berkel et al., 2021)</ref> have been shown to affect the perceived fairness of system decisions and future research could test whether these effects translate into a change in response bias. It may also be interesting to examine whether perceived fairness mainly affects people's response bias or whether there is also a relationship with people's sensitivity. Although we expect stronger relationships between perceived fairness and people's response bias (as high perceived fairness is likely to correspond to a more conservative response bias), we could also imagine that perceived fairness may relate to people's sensitivity.</p><p>Furthermore, decision-maker demographics may interact with perceived cost to produce different response biases. For example <ref type="bibr" target="#b4">, Alexander III (2021</ref><ref type="bibr" target="#b45">, 2022</ref> showed that in some contexts, male decision-makers may anticipate higher costs of overlooking unfairness (e.g., social outrage), whereas in other contexts, female decision-makers may anticipate higher costs (e.g., perpetuation of existing unfairness). Here, the expected costs differ according to the gender of the decision-maker, which influences their response bias.</p><p>Another person-related factor that may influence unfairness detection are individual values related to fairness. For example, research has shown that stronger beliefs in a just world are associated with less perceived discrimination <ref type="bibr" target="#b52">(Hafer &amp; Choma, 2009)</ref> and may thus correspond with a more conservative response bias. Another example could be the political orientation of the decision-maker, which could influence response bias through different perceived social norms. For decision-makers who identify more strongly with political conservatism <ref type="bibr" target="#b61">(Jost et al., 2003)</ref>, they may be more likely to accept inequalities, may require more perceived evidence to judge a situation as unfair <ref type="bibr" target="#b61">(Jost et al., 2003)</ref>, and thus may also have a more conservative response bias. For more decision-makers who identify less with political conservatism, existing inequalities may be more salient or seem more problematic, and they may therefore require less evidence to judge events as unfair <ref type="bibr" target="#b61">(Jost et al., 2003)</ref>.</p><p>Another example could be ambivalent sexism <ref type="bibr" target="#b43">(Glick &amp; Fiske, 1996)</ref> which can affect response bias in the detection of unfairness. For example, Alexander III (2022) showed that hostile sexism (i.e., prejudice and negative attitudes towards women) was associated with a more conservative response bias. In contrast, benevolent sexism (i.e., positive stereotypical views of women combined with a belief in restrictive traditional roles for women) was associated with a more liberal response bias.</p><p>Finally, the decision-maker's propensity to trust automated systems may render their response bias in unfairness detection more conservative. Propensity to trust in automation has been defined as "an individual's overall tendency to trust automation, independent of context or a specific system" <ref type="bibr">(Hoff &amp; Bashir, 2015 p. 413)</ref>. With a higher propensity to trust, people may be less likely to believe that there may be problematic system behavior. Therefore, they may need more evidence before deciding that there have been unfair outcomes. This should ultimately lead to a more conservative response bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future work</head><p>First, there is a need for empirical research that examines unfairness detection, and more generally error detection from an SDT perspective. What we have presented in this manuscript has been inspired by other domains applying SDT with a limited number of empirical studies that we are aware of applying an SDT perspective to unfairness detection or to the detection of errors for AI-based systems. Such research could examine any of the factors that may affect sensitivity or response bias presented in <ref type="table">Table 1</ref>, and will provide valuable insights into human decision processes related to unfairness and error detection.</p><p>Second, in such empirical studies, it would be possible to extend the SDT perspective on error and unfairness detection beyond equal variance SDT for binary decisions (e.g., unequal variance, rating tasks). This may be particularly necessary if research finds that the assumptions of equal variance SDT do not fit the empirical data (e.g., the empirical data may not lie on a symmetric ROC curve). In such cases, it is important to consider other versions of SDT, as conclusions of differential sensitivity may be inappropriate if assumptions for specific variants of SDT are not met (e.g., what may reflect a significant difference in sensitivity under equal variance SDT assumptions may not reflect a difference in sensitivity for unequal variance SDT; <ref type="bibr" target="#b65">Kellen &amp; Klauer, 2018)</ref>.</p><p>Furthermore, broadening the SDT perspective could also mean examining the decisionmaking of stakeholders other than the users of AI-based systems. For example, sensitivity and response bias may also be relevant factors in understanding affected people's reactions to AIbased decisions <ref type="bibr" target="#b73">(Langer &amp; Landers, 2021;</ref><ref type="bibr" target="#b144">Starke et al., 2022)</ref>. They may have different sensitivities to detect unfairness, which may help them to report such issues to organizations or act as whistleblowers to detect unfair AI-based processes. Response bias means that affected people may also be more or less liberal in claiming that a decision was unfair. For example, in the case of unfavorable outcomes (e.g. rejection for a job), people may be more motivated to look for signs of unfairness, which could increase their sensitivity but also render their response bias more liberal <ref type="bibr" target="#b135">(Skitka, 2003)</ref>.</p><p>Broadening the SDT perspective can also mean using drift-diffusion models <ref type="bibr">(DDMs)</ref> to complement the SDT perspective for studying human decision processes <ref type="bibr" target="#b112">(Rieger et al., 2022)</ref>. DDMs capitalize on the response time of decision-makers and interpret this response time as an indication of human decision-making processes. For example, longer response times could indicate that people have searched for more evidence to make their decision. The application of DDMs to the detection of error and unfairness may provide interesting research opportunities and capitalizes on response times an often available dependent variable (for introductions to the application of DDMs see <ref type="bibr" target="#b102">Myers et al., 2022;</ref><ref type="bibr" target="#b112">Rieger et al., 2022)</ref>.</p><p>Third, an SDT perspective could be used to integrate various effects associated with response biases into a single framework. For instance, SDT could provide a promising framework for effects that may have a similar psychological basis, such as algorithm aversion <ref type="bibr" target="#b24">(Dietvorst et al., 2015)</ref> (conservative response bias after seeing a system err), algorithm appreciation <ref type="bibr" target="#b80">(Logg et al., 2019</ref>) (liberal response bias in certain application contexts), or automation bias <ref type="bibr" target="#b138">(Skitka et al., 1999</ref>) (liberal response bias due to high initial expectations about system performance).</p><p>Finally, an SDT perspective could have implications for policy-making. For example, since the European AI Act calls for effective human oversight (Article 14), as well as effective support for humans to fulfil this role (Article 13), an SDT perspective could help to explain what helps human decision-makers to become effective overseers. For unfairness detection, we have shown that this could mean, for example, providing explicit standards for unfairness, user training, or technology-based oversight aids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Research has shown that there are challenges to achieving effective human oversight (e.g., automation bias, complacency effects) which has raised doubts about whether humans can be effective overseers. Such challenges may even question developing legislation around the world that has high hopes for human oversight. Rather than throwing the baby out with the bathwater, we propose to build on research from the field of dealing with imperfect automation <ref type="bibr" target="#b86">(McBride et al., 2014;</ref><ref type="bibr" target="#b106">Parasuraman &amp; Manzey, 2010;</ref><ref type="bibr" target="#b130">Sheridan, 2021;</ref><ref type="bibr" target="#b152">Zerilli et al., 2019)</ref> and apply an SDT perspective as a framework for better understanding error detection as a crucial part of effective oversight, whether for detecting unfairness or any other type of deviant system behavior. We hope that this paper will stimulate research using SDT to investigate the conditions for effective error detection in AI-based systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Decision variable distribution for noise and signal events showing d', c, and response criterion location for a decision-maker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example of a theoretical ROC curve with corresponding values of the decision criterion. This figure is inspired by a figure by<ref type="bibr" target="#b65">Kellen and Klauer (2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example ROC curves for different levels of sensitivity d'. This figure is inspired by a figure by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For instance, such aids could analyze whether decisions reflect evidence of unfairness and could alert decisionmakers when there is such evidence. For example, Baum et al. (2023) describe a fairness monitor that compares input and output values for decisions about different pairs of people and warns if there is a discrepancy between input similarity and output similarity (which could indicate that decisions may have been unfair). The US National Institute for Standards and</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the remainder of this paper, we will refer to error detection when we mean the detection of errors and other deviant behavior.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Sometimes participants may produce no false alarms or misses which makes it impossible to calculate SDT measures. There are several alternatives to deal with such issues in the literature<ref type="bibr" target="#b54">(Hautus et al., 2022</ref>) but a common one is to simply add half a count to hit, miss, false alarm, and correct rejection frequencies<ref type="bibr" target="#b114">(Rouder et al., 2007)</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ACCESS.2018.2870052</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2870052" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effects of incorrect computer-aided detection (CAD) output on human decision-making in mammography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alberdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Povyakalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Strigini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ayton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acra.2004.05.012</idno>
		<ptr target="https://doi.org/10.1016/j.acra.2004.05.012" />
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="909" to="918" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">If you give a judge a risk score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Albright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evidence from Kentucky bail decisions. Harvard John M. Olin Fellow&apos;s Discussion Paper</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rater sensitivity and bias in adverse impact decision-making: A signal detection theory approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Making decisions about adverse impact: The influence of individual and situational differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Do juries meet our expectations? Law and Human Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Arkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Mellers</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1020929517312</idno>
		<ptr target="https://doi.org/10.1023/A:1020929517312" />
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="625" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gil-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2019.12.012</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2019.12.012" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Misuse of diagnostic aids in process control: The effects of automation misses on complacency and automation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Bahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Elepfandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/154193120805201906</idno>
		<ptr target="https://doi.org/10.1177/154193120805201906" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ironies of Automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bainbridge</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-1098</idno>
		<ptr target="https://doi.org/10.1016/0005-1098" />
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="90046" to="90054" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A signal detection approach to understanding the identification of fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Batailler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Teas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gawronski</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691620986135</idno>
		<ptr target="https://doi.org/10.1177/1745691620986135" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="98" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Promoting effective human oversight with fairness monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sterz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hetmank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lauber-Rönsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithmic discrimination causes less moral outrage than human discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Arnestad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0001250</idno>
		<ptr target="https://doi.org/10.1037/xge0001250" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General. Advance Online Publication</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">It&apos;s reducing a human being to a percentage&quot;; Perceptions of justice in algorithmic decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lyngs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3173574.3173951</idno>
		<ptr target="https://doi.org/10.1145/3173574.3173951" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can algorithms legitimize discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostinelli</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000294</idno>
		<ptr target="https://doi.org/10.1037/xap0000294" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="459" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Malaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<idno type="DOI">10.1145/3449287</idno>
		<ptr target="https://doi.org/10.1145/3449287" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">CSCW1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How the machine &apos;thinks&apos;: Understanding opacity in machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burrell</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951715622512</idno>
		<ptr target="https://doi.org/10.1177/2053951715622512" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">205395171562251</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The effect of AI-generated advice on decision-making in personnel selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cecil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lermer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F C</forename><surname>Hudecek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaube</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/349xe</idno>
		<ptr target="https://doi.org/10.31219/osf.io/349xe" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the dimensionality of organizational justice: A construct validation of a measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Colquitt</surname></persName>
		</author>
		<idno type="DOI">10.1037/0021-9010.86.3.386</idno>
		<ptr target="https://doi.org/10.1037/0021-9010.86.3.386" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="386" to="400" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clinical versus actuarial judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Meehl</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.2648573</idno>
		<ptr target="https://doi.org/10.1126/science.2648573" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">4899</biblScope>
			<biblScope unit="page" from="1668" to="1674" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From &apos;automation&apos; to &apos;autonomy&apos;: The importance of trust repair in human-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>De Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1409" to="1427" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/00140139.2018.1457725</idno>
		<ptr target="https://doi.org/10.1080/00140139.2018.1457725" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automation reliability in unmanned aerial vehicle control: A reliance-compliance model of automation dependence in high workload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="474" to="486" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872006778606822</idno>
		<ptr target="https://doi.org/10.1518/001872006778606822" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the independence of compliance and reliance: Are automation false alarms worse than misses? Human Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872007X215656</idno>
		<ptr target="https://doi.org/10.1518/001872007X215656" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Explaining models: An empirical study of how explanations impact fairness judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
		<meeting>the 24th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="275" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3301275.3302310</idno>
		<ptr target="https://doi.org/10.1145/3301275.3302310" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ismagilova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coombs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Crick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eirug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Galanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Ilavarasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kizgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kronemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijinfomgt.2019.08.002</idno>
		<ptr target="https://doi.org/10.1016/j.ijinfomgt.2019.08.002" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2090236.2090255</idno>
		<ptr target="https://doi.org/10.1145/2090236.2090255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</title>
		<meeting>the 3rd Innovations in Theoretical Computer Science Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward a theory of situation awareness in dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872095779049543</idno>
		<ptr target="https://doi.org/10.1518/001872095779049543" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="64" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From here to autonomy: Lessons learned from human-automation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720816681350</idno>
		<ptr target="https://doi.org/10.1177/0018720816681350" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="27" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Situation awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Human Factors and Ergonomics</title>
		<editor>G. Salvendy &amp; W. Karowski</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="434" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uniform guidelines on employee selection procedures</title>
		<ptr target="https://www.govinfo.gov/content/pkg/CFR-2011-title29-vol4/xml/CFR-2011-title29-vol4-part1607.xml" />
	</analytic>
	<monogr>
		<title level="j">Equal Employment Opportunity Commission</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence Act) and amending certain union legislative acts</title>
		<idno>e0649735-a372-11eb-9585- 01aa75ed71a1.0001.02/DOC_1&amp;format=PDF</idno>
		<ptr target="https://eur-lex.europa.eu/resource.html?uri=cellar" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>European Commission</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Age-Related Differences in Reliance Behavior Attributable to Costs Within a Human-Decision Aid System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="863" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872008X375018</idno>
		<ptr target="https://doi.org/10.1518/001872008X375018" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Social cognition (2. Aufl.)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Fiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>MacGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Do as AI say: Susceptibility in deployment of clinical decision-aids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lermer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Coughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-021-00385-9</idno>
		<ptr target="https://doi.org/10.1038/s41746-021-00385-9" />
	</analytic>
	<monogr>
		<title level="j">Npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">General act on equal treatment</title>
		<ptr target="https://www.gesetze-im-internet.de/englisch_agg/englisch_agg.html" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The ambivalent sexism inventory: Differentiating hostile and benevolent sexism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Fiske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="512" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0022-3514.70.3.491</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.70.3.491" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The flaws of policies requiring human oversight of government algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.clsr.2022.105681</idno>
		<ptr target="https://doi.org/10.1016/j.clsr.2022.105681" />
	</analytic>
	<monogr>
		<title level="j">Computer Law &amp; Security Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">105681</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Disparate interactions: An Algorithm-in-the-loop analysis of fairness in risk assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287563</idno>
		<ptr target="https://doi.org/10.1145/3287560.3287563" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human decision making with machine assistance: An experiment on bailing and jailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grgić-Hlača</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359280</idno>
		<ptr target="https://doi.org/10.1145/3359280" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CSCW Conference on Human-Computer Interaction</title>
		<meeting>the 2019 CSCW Conference on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grgić-Hlača</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web -WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web -WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="903" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3178876.3186138</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186138" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Clinical versus mechanical prediction: A meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Zald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Lebow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Snitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<idno type="DOI">10.1037//1040-3590.12.1.19</idno>
		<ptr target="https://doi.org/10.1037//1040-3590.12.1.19" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Belief in a just world, perceived fairness, and justification of the status quo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Hafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Choma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social and psychological bases of ideology and system justification (S. 107-125)</title>
		<editor>J. T. Jost, A. C. Kay, &amp; H. Thorisdottir</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An accountability account: A review and synthesis of the theoretical and empirical research on felt accountability: Accountability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Frink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">10.1002/job.2052</idno>
		<ptr target="https://doi.org/10.1002/job.2052" />
	</analytic>
	<monogr>
		<title level="j">Journal of Organizational Behavior</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="204" to="224" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Detection theory a user&apos;s guide (3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hautus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Macimillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Creelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Let the user have a say-Voice in automated decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hellwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Maier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2022.107446</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2022.107446" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ethics guidelines for trustworthy AI</title>
		<ptr target="https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60419" />
	</analytic>
	<monogr>
		<title level="j">High Level Expert Group on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Trust in automation: Integrating empirical evidence on factors that influence trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bashir</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720814547570</idno>
		<ptr target="https://doi.org/10.1177/0018720814547570" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Metrics for explainable AI: Challenges and prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Litman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1812.04608</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1812.04608" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3571730</idno>
		<ptr target="https://doi.org/10.1145/3571730" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The global landscape of AI ethics guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ienca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vayena</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0088-2</idno>
		<ptr target="https://doi.org/10.1038/s42256-019-0088-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Political conservatism as motivated social cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Kruglanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Sulloway</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.129.3.339</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.129.3.339" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="375" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Augmenting medical diagnosis decisions? An investigation into Physicians&apos; Decision-making process with artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jussupow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Spohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gawlitza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="713" to="735" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<idno type="DOI">10.1287/isre.2020.0980</idno>
		<ptr target="https://doi.org/10.1287/isre.2020.0980" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The effects of level of automation and adaptive automation on human performance, situation awareness and workload in a dynamic control task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Kaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
		<idno type="DOI">10.1080/1463922021000054335</idno>
		<ptr target="https://doi.org/10.1080/1463922021000054335" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="153" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Elementary signal detection and threshold theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Klauer</surname></persName>
		</author>
		<editor>E.-J</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Stevens&apos; Handbook of Experimental Psychology and Cognitive Neuroscience (4. Aufl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119170174.epcn505</idno>
		<imprint>
			<publisher>Wiley</publisher>
			<biblScope unit="page" from="161" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The more you know: Trust dynamics and calibration in highly automated driving and the effects of take-overs, system malfunction, and system transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stiegemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baumann</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720819853686</idno>
		<ptr target="https://doi.org/10.1177/0018720819853686" />
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="718" to="736" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Mechanical versus clinical data combination in selection and admissions decisions: A meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Kuncel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Klieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Connelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ones</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0034156</idno>
		<ptr target="https://doi.org/10.1037/a0034156" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1060" to="1072" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fehlgeleitete Hoffnungen?: Grenzen menschlicher Aufsicht beim Einsatz algorithmusbasierter Systeme am Beispiel Personalauswahl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<idno type="DOI">10.1026/0033-3042/a000626</idno>
		<ptr target="https://doi.org/10.1026/0033-3042/a000626" />
	</analytic>
	<monogr>
		<title level="j">Psychologische Rundschau</title>
		<imprint>
			<biblScope unit="page" from="33" to="3042" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Trust in artificial intelligence: Comparing trust processes between human and automated trustees in light of unfair bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hemsing</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10869-022-09829-9</idno>
		<ptr target="https://doi.org/10.1007/s10869-022-09829-9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Psychology</title>
		<imprint>
			<date type="published" when="2022" />
			<publisher>Advance Online Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Changing the means of managerial work: Effects of automated decision-support systems on personnel selection tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Busch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10869-020-09711-6</idno>
		<ptr target="https://doi.org/10.1007/s10869-020-09711-6" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="751" to="769" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Highly-automated job interviews: Acceptance under the influence of stakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papathanasiou</surname></persName>
		</author>
		<idno type="DOI">10.1111/ijsa.12246</idno>
		<ptr target="https://doi.org/10.1111/ijsa.12246" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Selection and Assessment</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="234" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Landers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.106878</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.106878" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What do we want from Explainable artificial intelligence (XAI)? A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Speith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2021.103473</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2021.103473" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page">103473</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951718756684</idno>
		<ptr target="https://doi.org/10.1177/2053951718756684" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">205395171875668</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kusbit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2019" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3359284</idno>
		<ptr target="https://doi.org/10.1145/3359284" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The uncertain advisor: Trust, accuracy, and self-correction in an automated decision support system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lochner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10339-022-01113-1</idno>
		<ptr target="https://doi.org/10.1007/s10339-022-01113-1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2018.12.005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Utilizing&quot; signal detection theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797614541991</idno>
		<ptr target="https://doi.org/10.1177/0956797614541991" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1663" to="1673" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Similarities and differences between human-human and human-automation trust: An integrative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wiegmann</surname></persName>
		</author>
		<idno type="DOI">10.1080/14639220500337708</idno>
		<ptr target="https://doi.org/10.1080/14639220500337708" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="301" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Implications of AI (un-)fairness in higher education admissions: The effects of perceived AI (un-)fairness on exit, voice and organizational reputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marcinkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kieslich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lünich</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372867</idno>
		<ptr target="https://doi.org/10.1145/3351095.3372867" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 FAT* Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 FAT* Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="122" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Understanding the effect of workload on automation use for younger and older adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Fisk</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720811421909</idno>
		<ptr target="https://doi.org/10.1177/0018720811421909" />
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="672" to="686" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Understanding human management of automation errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Fisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="545" to="577" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/1463922X.2013.817625</idno>
		<ptr target="https://doi.org/10.1080/1463922X.2013.817625" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Quantifying the information value of clinical assessments with signal detection theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Mcfall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Treat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="241" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<idno type="DOI">10.1146/annurev.psych.50.1.215</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.50.1.215" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3457607</idno>
		<ptr target="https://doi.org/10.1145/3457607" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Training one model to detect heart and lung sound events from single point auscultations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Melms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Ilesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Atila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schieffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Obergassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schlicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Hirsch</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.06078</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.06078" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Monitoring an automated system for a single failure: Vigilance and task complexity effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="322" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872096779048093</idno>
		<ptr target="https://doi.org/10.1518/001872096779048093" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Adaptive automation, trust, and self-confidence in fault management of time-critical tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Inagaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Itoh</surname></persName>
		</author>
		<idno type="DOI">10.1037//0278-7393.6.1.44</idno>
		<ptr target="https://doi.org/10.1037//0278-7393.6.1.44" />
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Human Performance in Automated and Autonomous Systems (1. Aufl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manzey</surname></persName>
		</author>
		<editor>M. Mouloua, P. A. Hancock, &amp; J. Ferraro</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CRC Press</publisher>
			<biblScope unit="page" from="19" to="42" />
		</imprint>
	</monogr>
	<note>Humans and automated decision aids: A match made in heaven</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<idno type="DOI">10.1201/9780429458330-2</idno>
		<ptr target="https://doi.org/10.1201/9780429458330-2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Automation use and automation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="344" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/154193129904300346</idno>
		<ptr target="https://doi.org/10.1177/154193129904300346" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Automation bias, accountability, and verification behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Heers</surname></persName>
		</author>
		<idno type="DOI">10.1177/154193129604000413</idno>
		<ptr target="https://doi.org/10.1177/154193129604000413" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="204" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Automation bias: Decision making and performance in high-tech cockpits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Heers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burdick</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327108ijap0801_3</idno>
		<ptr target="https://doi.org/10.1207/s15327108ijap0801_3" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Aviation Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A practical introduction to using the drift diffusion model of decision-making in cognitive psychology, neuroscience, and health sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Interian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Moustafa</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2022.1039172</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2022.1039172" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1039172</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">When eliminating bias isn&apos;t fair: Algorithmic reductionism and procedural justice in human resource decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Harmon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2020.03.008</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2020.03.008" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="149" to="167" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Breaking taboos in fair machine learning: An experimental study. Equity and Access in Algorithms, Mechanisms, and Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1145/3465416.3483291</idno>
		<ptr target="https://doi.org/10.1145/3465416.3483291" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Human performance consequences of stages and levels of automation: An integrated meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Onnasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manzey</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720813501549</idno>
		<ptr target="https://doi.org/10.1177/0018720813501549" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Complacency and bias in human use of automation: An attentional integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Manzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="410" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0018720810376055</idno>
		<ptr target="https://doi.org/10.1177/0018720810376055" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A model for types and levels of human interaction with automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<idno type="DOI">10.1109/3468.844354</idno>
		<ptr target="https://doi.org/10.1109/3468.844354" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics -Part A: Systems and Humans</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Humans: Still vital after all these years of automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872008x312198</idno>
		<ptr target="https://doi.org/10.1518/001872008x312198" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="520" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Individual differences in complacency and monitoring for automation failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Prinzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Prinzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Individual Differences Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Empirical generality of data from recognition memory receiver-operating characteristic functions and implications for the global memory models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tindall</surname></persName>
		</author>
		<idno type="DOI">10.1037/0278-7393.20.4.763</idno>
		<ptr target="https://doi.org/10.1037/0278-7393.20.4.763" />
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="763" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Modeling responses to alarm systems: A drift diffusion model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parnassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manzey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="711" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/1071181322661389</idno>
		<ptr target="https://doi.org/10.1177/1071181322661389" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Signal detection models with random participant and item effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naveh-Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="621" to="642" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s11336-005-1350-6</idno>
		<ptr target="https://doi.org/10.1007/s11336-005-1350-6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Effects of Imperfect Automation on Decision Making in a Simulated Command and Control Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rovira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcgarry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872007779598082</idno>
		<ptr target="https://doi.org/10.1518/001872007779598082" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Pilots&apos; monitoring dtrategies and performance on automated flight decks: An empirical study combining behavioral and eye-tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Sarter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mumaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872007X196685</idno>
		<ptr target="https://doi.org/10.1518/001872007X196685" />
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="357" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Supporting decision making and action selection under time pressure and uncertainty: The case of in-flight icing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Sarter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872001775870403</idno>
		<ptr target="https://doi.org/10.1518/001872001775870403" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Pilot interaction with cockpit automation II: An experimental study of pilots&apos; model and awareness of the flight management and guidance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Sarter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Aviation Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title/>
		<idno type="DOI">10.1207/s15327108ijap0401_1</idno>
		<ptr target="https://doi.org/10.1207/s15327108ijap0401_1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">What to expect from opening &quot;Black Boxes&quot;? Comparing perceptions of justice between human and automated agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schlicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ötting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.106837</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.106837" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">On explanations, fairness, and appropriate reliance in human-AI decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schoeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kuehl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11812</idno>
		<ptr target="http://arxiv.org/abs/2209.11812" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">NIST special publication 1270: Towards a standard for identifying and managing bias in artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vassilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
		<respStmt>
			<orgName>US Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Risk, human error, and system resilience: Fundamental ideas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="426" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title/>
		<idno type="DOI">10.1518/001872008X250773</idno>
		<ptr target="https://doi.org/10.1518/001872008X250773" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Extending three existing models to analysis of trust in automation: Signal detection, statistical parameter estimation, and model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1162" to="1170" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0018720819829951</idno>
		<ptr target="https://doi.org/10.1177/0018720819829951" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Supervisory Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Human Factors and Ergonomics</title>
		<editor>G. Salvendy &amp; W. Karowski</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="736" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The dangers of faulty, biased, or malicious algorithms requires independent oversight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="13538" to="13540" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1618211113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1618211113" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Human-Centered Artificial Intelligence: Reliable, Safe &amp; Trustworthy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="495" to="504" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/10447318.2020.1741118</idno>
		<ptr target="https://doi.org/10.1080/10447318.2020.1741118" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Of different minds: An accessible identity model of justice reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="286" to="297" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title/>
		<idno type="DOI">10.1207/S15327957PSPR0704_02</idno>
		<ptr target="https://doi.org/10.1207/S15327957PSPR0704_02" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Accountability and automation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Burdick</surname></persName>
		</author>
		<idno type="DOI">10.1006/ijhc.1999.0349</idno>
		<ptr target="https://doi.org/10.1006/ijhc.1999.0349" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="717" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Does automation bias decision-making?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="991" to="1006" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title/>
		<idno type="DOI">10.1006/ijhc.1999.0252</idno>
		<ptr target="https://doi.org/10.1006/ijhc.1999.0252" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">The quality of human-automation cooperation in humansystem interface for nuclear power plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B M</forename><surname>Skjerve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Skraaning</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2004.06.001</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2004.06.001" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="649" to="677" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Systems with human monitors: A signal detection analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Sorkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Woods</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327051hci0101_2</idno>
		<ptr target="https://doi.org/10.1207/s15327051hci0101_2" />
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="75" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Calculation of signal detection theory measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="149" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/BF03207704</idno>
		<ptr target="https://doi.org/10.3758/BF03207704" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baleis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marcinkowski</surname></persName>
		</author>
		<idno type="DOI">10.1177/20539517221115189</idno>
		<ptr target="https://doi.org/10.1177/20539517221115189" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205395172211151</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Apparent weight of evidence, decision criteria, and confidence ratings in juror decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hogue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="442" to="465" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0033-295x.83.6.442</idno>
		<ptr target="https://doi.org/10.1037/0033-295x.83.6.442" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Availability: A heuristic for judging frequency and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0285(73</idno>
		<ptr target="https://doi.org/10.1016/0010-0285(73" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="90033" to="90042" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Recommendation on the ethics of artificial intelligence</title>
		<ptr target="https://unesdoc.unesco.org/ark:/48223/pf0000381137" />
	</analytic>
	<monogr>
		<title level="j">UNESCO</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Effect of information presentation on fairness perceptions of machine learning predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Berkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3411764.3445365</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445365" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Effects of mental model quality on collaborative system performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Wilkison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.1177/154193120705102208</idno>
		<ptr target="https://doi.org/10.1177/154193120705102208" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1506" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Algorithmic decision-making and the control problem. Minds and Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gavaghan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-019-09513-7</idno>
		<ptr target="https://doi.org/10.1007/s11023-019-09513-7" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="555" to="578" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
