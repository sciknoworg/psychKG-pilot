<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Collection in Multimodal Language and Communication Research: A Flexible Decision Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Bauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Section for General Linguistics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">C</forename><surname>Trettenbrein</surname></persName>
							<email>trettenbrein@cbs.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neuropsychology</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Human Cognitive &amp; Brain Sciences</orgName>
								<address>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of German Philology</orgName>
								<orgName type="laboratory">Experimental Sign Language Laboratory (SignLab)</orgName>
								<orgName type="institution">University of Göttingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Amici</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Biology</orgName>
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Ćwiek</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Leibniz-Zentrum Allgemeine Sprachwissenschaft</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Fuchs</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Leibniz-Zentrum Allgemeine Sprachwissenschaft</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Institute for Advanced Studies (IMéRA)</orgName>
								<orgName type="institution">ILCB at Aix-Marseille University</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa-Marie</forename><surname>Krause</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Julius-Maximilians-Universität Würzburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kuder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Section for General Linguistics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silva</forename><surname>Ladewig</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of German Philology</orgName>
								<orgName type="laboratory">Experimental Sign Language Laboratory (SignLab)</orgName>
								<orgName type="institution">University of Göttingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schulder</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of German Sign Language and Communication of the Deaf</orgName>
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><forename type="middle">B</forename><surname>Schumacher</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of German Language &amp; Literature I, Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Department of Rehabilitation and Special Education</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Zulberti</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Biology</orgName>
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schulte-Rüther</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Department of Child and Adolescent Psychiatry</orgName>
								<orgName type="institution">University Medical Center Göttingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff11">
								<orgName type="department">Department of Child and Adolescent psychiatry</orgName>
								<orgName type="institution">University Hospital Heidelberg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="department" key="dep1">Department of Neuropsychology</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Human Cognitive and Brain Sciences</orgName>
								<address>
									<addrLine>Stephanstraße 1a</addrLine>
									<postCode>04103</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Collection in Multimodal Language and Communication Research: A Flexible Decision Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multimodality</term>
					<term>language</term>
					<term>multimodal communication</term>
					<term>corpus</term>
					<term>elicitation</term>
					<term>fieldwork</term>
					<term>data collection</term>
					<term>sign language</term>
					<term>gesture</term>
					<term>methods</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The contemporary study of human language and communication has expanded beyond its traditional focus on spoken and written forms to incorporate gestures, facial expressions, and sign languages. This shift has been accompanied by methodological advancements that extend beyond classical tools such as tape recorders or video cameras and include motion-tracking systems, depth cameras, and multimodal data fusion techniques. While these tools enable richer empirical insights, they also introduce significant conceptual and practical challenges, particularly for researchers new to multimodal data collection. This paper provides a structured exploration of the methodological workflow essential to multimodal language and communication research. We present a flexible decision-making framework that guides researchers through key considerations, from data selection and its alignment with research questions, to data collection methods, technical requirements, and data management, including ethical considerations and data sharing. We also address critical factors such as equipment choice, data synchronization, and ethical concerns (e.g., privacy and data protection) while illustrating these processes with examples from different research contexts (i.e., lab-based experiments, large-scale annotated corpora, field studies including non-human primates). Rather than advocating a one-size-fits-all approach, our discussion emphasizes key decision points, trade-offs and real-world examples to help researchers navigate the complexities of multimodal data collection. By integrating perspectives from different disciplines, our flexible decisionmaking framework is intended as a practical tool for newcomers to address common conceptual and methodological challenges in the rapidly developing area of multimodal data collection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>communication are fundamentally multimodal and not simply accompanied by optional nonvocal cues <ref type="bibr" target="#b0">(Abner et al., 2015;</ref><ref type="bibr" target="#b7">Bavelas, 1990;</ref><ref type="bibr" target="#b28">Goodwin, 1986;</ref><ref type="bibr" target="#b31">Hagoort &amp; Özyürek, 2024;</ref><ref type="bibr" target="#b38">Holler &amp; Levinson, 2019;</ref><ref type="bibr" target="#b41">Keevallik, 2018;</ref><ref type="bibr" target="#b45">Kendon, 2004;</ref><ref type="bibr" target="#b48">Kendrick et al., 2023;</ref><ref type="bibr" target="#b74">Mondada, 2016;</ref><ref type="bibr" target="#b90">Özyürek, 2021;</ref><ref type="bibr" target="#b93">Perniss, 2018;</ref><ref type="bibr" target="#b96">Rasenberg et al., 2022;</ref><ref type="bibr" target="#b97">Sandler, 2022</ref><ref type="bibr" target="#b98">Sandler, , 2024</ref><ref type="bibr" target="#b108">Trettenbrein et al., 2025;</ref><ref type="bibr" target="#b110">Vigliocco et al., 2014;</ref><ref type="bibr" target="#b119">Wilkinson et al., 2016)</ref>. Rather than relying solely on speech, manual signs, or text, real-world language use and communicative interactions involve gestures, facial expressions, body posture, gaze, or tactile and haptic cues <ref type="bibr" target="#b13">(Checchetto et al., 2018;</ref><ref type="bibr" target="#b20">Edwards &amp; Brentari, 2020;</ref><ref type="bibr" target="#b38">Holler &amp; Levinson, 2019;</ref><ref type="bibr" target="#b90">Özyürek, 2021)</ref>. While research in the language sciences has traditionally focused on humans, recent studies have explored parallel phenomena in non-human primates, who appear to also frequently integrate communicative signals across modalities to enhance meaning, improve signal effectiveness, and adapt to diverse social and environmental contexts <ref type="bibr" target="#b2">(Aychet et al., 2021;</ref><ref type="bibr" target="#b25">Fröhlich et al., 2019;</ref><ref type="bibr" target="#b23">Fröhlich et al., 2021;</ref><ref type="bibr" target="#b37">Hobaiter et al., 2017;</ref><ref type="bibr" target="#b61">Liebal et al., 2013;</ref><ref type="bibr" target="#b70">Mine et al., 2024;</ref><ref type="bibr" target="#b117">Wilke et al., 2017)</ref>. This increasing cross-disciplinary interest in multimodality is directly reflected in the increasing number of publications on the topic over the past decades <ref type="figure">(Figure 1</ref>), presumably driven by both theoretical advancements and technological progress in capturing and analyzing multimodal phenomena. However, the concept of multimodality remains diffuse, encompassing a range of interpretations and definitions <ref type="bibr" target="#b23">(Fröhlich et al., 2021;</ref><ref type="bibr" target="#b97">Sandler, 2022)</ref>. Despite growing attention to multimodality and the multimodal nature of communication, current guidelines and standards for multimodal data collection and interpretation remain discipline-specific and fragmented. Previous works offer valuable insights, and practical recommendations for specific fields, such as co-speech gesture studies (e.g., <ref type="bibr" target="#b71">Mittelberg, 2007)</ref> and sign language research (e.g., <ref type="bibr" target="#b22">Fenlon &amp; Hochgesang, 2022;</ref><ref type="bibr" target="#b88">Orfanidou et al., 2015)</ref>, including specific guidance for video data quality and annotation <ref type="bibr" target="#b33">(Hanke &amp; Fenlon, 2022;</ref><ref type="bibr" target="#b92">Perniss, 2015)</ref>. However, researchers investigating multimodal language and communication would benefit from a broader approach incorporating more diverse populations (e.g., children, clinical groups, non-human primates) and specifically focusing on the combination of multiple data streams (e.g., motion capture, physiological sensors, eye-tracking, audio and video streams) and their integration to effectively address interdisciplinary research questions. Furthermore, existing guidelines <ref type="bibr" target="#b22">(Fenlon &amp; Hochgesang, 2022;</ref><ref type="bibr" target="#b71">Mittelberg, 2007;</ref><ref type="bibr" target="#b88">Orfanidou et al., 2015;</ref><ref type="bibr" target="#b33">Hanke &amp; Fenlon, 2022;</ref><ref type="bibr" target="#b92">Perniss, 2015)</ref> do not fully reflect recent technological advancements, such as semi-automatic annotation tools, computer-vision based, markerless motion tracking methods, and techniques for synchronizing and integrating multimodal data sources. These developments create new methodological opportunities and challenges highlighting the need for a coherent decision-making framework for researchers across disciplines that supports comprehensible, reasonable, and replicable practices for multimodal data collection.</p><p>An extensive review on the state of the art of existing research tools is available in a previous publication <ref type="bibr">(Gregori et al., 2023)</ref>. The goal of this paper is to outline a decisionmaking framework for multimodal data collection based on practical experiences. The framework is intended to apply to multiple scenarios across disciplines rather than prescribing a single one-size-fits-all protocol. We introduce this approach to help researchers navigate the methodological complexities inherent in multimodal data collection involving both human and non-human communicative behavior. To bridge interdisciplinary gaps and refine best practices, we illustrate updated recommendations through case studies to highlight key aspects. First, we offer structured guidelines for integrating cross-disciplinary methods, combining approaches A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 7 from linguistics, psychology, and computer science. The case studies demonstrate how these different fields can collaborate effectively. Second, we address the challenges posed by new technologies, ensuring the ethical, transparent, and reproducible use of existing tools, as illustrated through our case examples. Finally, we emphasize the importance of standardizing data collection, improving metadata documentation, and refining annotation practices, all of which are demonstrated in the case studies to enhance research interoperability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is "Multimodality" in the Cognitive Sciences?</head><p>A fundamental challenge in multimodal research is the precise definition of the term multimodality, which can be understood from various interrelated perspectives and does not necessarily have a clear meaning <ref type="bibr" target="#b97">(Sandler, 2022)</ref>. Different researchers coming from different fields tackling at times fundamentally different research questions have employed the terms "multimodality" and "multimodal", but not necessarily with the same intended meaning. These different uses of terminology go beyond theoretical disagreements and seem to us to be rooted in differences with regard to whether researchers attempt to describe language or communication as multimodal, or whether they are referring to the kind of data that they are collecting as being multimodal.</p><p>One perspective considers multimodality as a property of language and communication, viewing language use as an inherently multimodal phenomenon that spans multiple or different physical channels of transmission rather than being confined to a single modality <ref type="bibr">(Cohn &amp; Schilperrord, 2024;</ref><ref type="bibr">Gregori et al., 2023;</ref><ref type="bibr">Hagort &amp; Özyürek, 2024)</ref>. Spoken languages primarily rely on the vocal-auditory channel, while gestures rely on the visualkinematic channel. In sign languages, where the vocal-auditory channel is not available, the linguistic signal is mainly conveyed through the hands, the head and parts of the face, but can also include structural elements, such as mouthings, which are influenced by other channels (e.g. written language) and are considered a part of the natural signing <ref type="bibr" target="#b4">(Bauer, 2019;</ref><ref type="bibr" target="#b6">Bauer &amp; Kyuseva, 2022;</ref><ref type="bibr" target="#b9">Boyes-Braem &amp; Sutton-Spence, 2001;</ref><ref type="bibr" target="#b72">Mohr, 2012)</ref>. Language can also take the form of a tactile sign language <ref type="bibr" target="#b13">(Checchetto et al., 2018;</ref><ref type="bibr" target="#b20">Edwards &amp; Brentari, 2020;</ref><ref type="bibr" target="#b66">Mesch &amp; Raanes, 2023)</ref> or span multiple modalities at once (e.g., vocal signals and gestures complementing each other in spoken interaction <ref type="bibr" target="#b90">(Özyürek, 2021;</ref><ref type="bibr" target="#b112">Wagner et al., 2014)</ref>.</p><p>Another perspective approaches multimodality as a property of the data, referring to the integration of multiple data streams, multiple instruments, measurement devices, or acquisition techniques that capture the same phenomenon. This might include modalities such as audio, video, breathing patterns, electroencephalography (EEG), magnetoencephalography (MEG), eyetracking, or motion capture to analyse visual signals like co-speech gestures. In this framework, each acquisition method is referred to as a modality and corresponds to a distinct dataset. The entire setup, in which data from multiple modalities are available, is described as multimodal. A key characteristic of multimodality is complementarity-each modality contributes unique information that cannot straightforwardly be inferred from any other modality within the setup <ref type="bibr" target="#b58">(Lahat et al., 2015)</ref>. At the same time, multimodal data may be codependent on each other such that data of one source be considered as the trigger of processes observed in another modality (e.g. fixation-related EEG effects; <ref type="bibr" target="#b19">Dimigen &amp; Ehinger, 2021)</ref>.</p><p>Thus, to study the relation of co-speech gesture to speech, audiovisual recordings may be combined with data from an eye-tracker and/or a motion-capture device. Whereas the former multimodal understanding of language and communication was brought about primarily by insights from the study of the world's different sign languages <ref type="bibr" target="#b94">(Pfau et al., 2012;</ref><ref type="bibr" target="#b99">Sandler &amp; Lillo-Martin, 2006)</ref> and from linguistic research on gestures <ref type="bibr" target="#b38">(Holler &amp; Levinson, 2019;</ref><ref type="bibr" target="#b43">Kendon, 1980</ref><ref type="bibr" target="#b45">Kendon, , 2004</ref><ref type="bibr" target="#b64">McNeill, 1992;</ref><ref type="bibr" target="#b79">Müller et al., , 2014</ref>, the latter understanding of multimodality is of major importance to anyone planning to collect data documenting multimodal linguistic phenomena in the former sense.</p><p>In practice, these two perspectives interact: Once we acknowledge (1) the inherently multimodal nature of language and communicative behavior, we are compelled (2) to gather richer, multifaceted datasets to capture the full complexity and richness of (non-)human communication. In turn, this approach may reveal even more aspects of the multimodality of communication. To fully embrace this shift from single-channel to multi-channel recording setups it is essential to understand the various types of data and their characteristics-where A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 9 they come from, how they were collected, how they can be analysed, how they should be stored. In other words, such expanded data collection introduces several methodological and ethical challenges. In this paper, we delineate the primary types of data that emerge in multimodal communication research, highlighting their significance to address key research questions, as well as the methodological considerations related to their characteristics, interactions, and analytical challenges. While many of the points we address apply to general study planning and data-collection protocols, we focus here on the additional layers of complexity introduced by the simultaneous capture of multi-channel data, where best-practice guidelines may be either limited or overly specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three Illustrative Case Studies</head><p>Throughout this paper, we illustrate the complexities of multimodal data collection by referring to examples from our own distinct studies: eliciting multimodal data in controlled laboratory settings, constructing annotated corpora for large-scale analysis, and gathering naturalistic data in field environments.</p><p>As an exemplification of eliciting multimodal data in the lab (Example 1), we examine a gesture elicitation task conducted to inventory the gestural repertoire of non-signing speakers of <ref type="bibr">German (Spruijit et al., 2025)</ref>. This experiment is part of a larger project investigating the influence of the gestural repertoire on the acquisition of individual parameters of a signhandshape, location, and movement-by second language learners of a language in a different modality than their native language. For the elicitation study, silent gestures were elicited from 18 native non-signing speakers of German to investigate the systematicity of gestural representations for particular concepts. None of the participants had prior exposure to a signed language. Participants were presented with a concept in written form and were asked to express this concept gesturally and without pointing to objects in their surroundings.</p><p>As a second example (Example 2), we refer to the DGS Corpus, an annotated corpus of German Sign Language (DGS), which is a collection of signed conversations that have been enriched with additional information (e.g. annotations of manual and non-manual features) to facilitate linguistic analysis. It is part of the DGS-Korpus project, a long-term initiative aimed at A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 10 developing both a linguistic reference corpus and a corpus-based dictionary of DGS <ref type="bibr" target="#b95">(Prillwitz et al., 2008)</ref>. In its first data collection campaign (2010-2012) it collected 560 hours of discourse from 330 participants. A second campaign is currently ongoing <ref type="bibr" target="#b49">(Konrad et al., 2024)</ref>.</p><p>Recordings cover participants from all regions of Germany and all participants, moderators and technicians present during recording sessions use DGS as their primary language of daily life.</p><p>Participants were grouped into pairs and engaged in various conversational tasks, including discussions on assigned topics or historical events, free dialogue, and story retelling <ref type="bibr" target="#b85">(Nishio et al., 2010)</ref>. For further details on corpus curation and demographic composition, see <ref type="bibr" target="#b100">Schulder et al. (2021)</ref> and <ref type="bibr" target="#b102">Schulder &amp; Hanke (2022)</ref>. 50 hours of the DGS were made available publicly as the Public DGS Corpus, which includes the research dataset "MY DGS-Annotated" that provides full sign annotations and translations in German and English <ref type="bibr" target="#b35">(Hanke et al., 2020;</ref><ref type="bibr" target="#b52">Konrad et al., 2020)</ref>.</p><p>While lab studies have a variety of advantages over the control of the environment, not all studies are possible or ideal in the lab. As an example of gathering naturalistic data outside of the lab, we refer to the collection of multimodal data from non-human primates (Example 3).</p><p>Here, instead of focusing on a specific case study, we will refer to a group of existing empirical studies on one of our closest living relative, the chimpanzee <ref type="bibr" target="#b37">(Hobaiter et al., 2017;</ref><ref type="bibr" target="#b70">Mine et al., 2024;</ref><ref type="bibr" target="#b117">Wilke et al., 2017)</ref>. All studies of Example 3 involve video observations of naturalistic interactions in wild or captive settings and annotation of signal production in the different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Types of Data in Relation to Research Questions and Data Collection Methods</head><p>The first step is always to clarify what kind of data is needed to answer a specific research question, and what type of conclusions one would like to be able to draw from the data. Examining short, highly specific phenomena (e.g., individual signs or phonemes, isolated gestures, specific facial expressions, etc.) will necessitate a different experimental design and laboratory setup than studying broader continuous interactions (e.g., dialogues, group interactions, social behaviour, etc.). Specifically, while the former may require an experimental design in which specific conditions, later to be contrasted during analysis, can be elicited by the experimenter from participants (as in Example 1), the latter setup could, in principle, also be implemented using an observational design, which is more suitable for drawing looser correlative conclusions (as in Example 2 from the DGS Corpus). Decisions about which dependent variables and their measurements not only reflect the research question and hypothesis, but also shape the laboratory setup and choice of recording devices.</p><p>For instance, in Example 3, early decisions on data collection protocols, such as whether to perform focal sampling or ad libitum video recordings, can significantly influence the observed range of behaviors (i.e. the repertoire), underscoring the importance of careful protocol selection. Moreover, given that repertoires are often established using top-down classification, it is important to ensure the consistency of applied criteria and suitable granularity to ensure that all signals and modalities are equally captured. Bottom-up coding schemes are a good solution to this problem and enhance transparency and reliability (e.g.</p><p>ChimpFACS, <ref type="bibr" target="#b109">Vick et al., 2007;</ref><ref type="bibr">ChimpLASG, Zulberti et al., 2024)</ref>. Overall, decisions about the type, scope, granularity, and modality of data should align closely with the target phenomenon to support robust and meaningful conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phenomena of Interest and How They Can be Captured</head><p>The distinction between single signs or gestures, short sequences, and continuous, longer-duration conversational data reflects divergent methodological approaches in multimodal analysis. Short sequences, encompassing isolated gestures, signs, words, or phrases (e.g., in sign language lexicon studies or imitation experiments), enable granular examination of specific signals, such as the kinematic or acoustic properties of gestures, signs, utterances, or even individual phonemes. In contrast, continuous conversational data-spanning extended interactions like conversations, interviews, or debates-provide contextualized insights into language pragmatics and discourse dynamics. Such naturalistic datasets facilitate multilevel analyses: At the micro level, the tight coupling of gesture and speech can be scrutinized; at the meso level, recurring multimodal themes structuring entire interactions emerge; and at the macro level, broader discourse stretches reveal convergent multimodal patterns that form larger thematic units <ref type="bibr" target="#b56">(Ladewig &amp; Horst, 2024;</ref><ref type="bibr" target="#b81">Müller &amp; Kappelhoff, 2018;</ref>.</p><p>This hierarchical framework underscores the necessity of aligning data granularity with research objectives, whether focused on discrete signal properties or the interplay of modalities across discourse scales.</p><p>The collection of multimodal data can thus be approached in various ways, depending primarily on the phenomenon or phenomena of interest as well as population being studied. For example, some studies may capture free-flowing interactions, while others may incorporate mild structuring, such as tasks or prompts, to ensure coverage of specific behaviors such as monologues, dialogues, or multilogues. Data from the latter approach allows for studying phenomena such as feedback, turn-taking behavior, interactive dynamics, or collaborative communication. In these contexts, gestures emerge as a means of organizing social interaction which are specialized in fulfilling pragmatic functions ("recurrent gestures", <ref type="bibr" target="#b54">Ladewig 2014</ref><ref type="bibr" target="#b55">Ladewig , 2024</ref><ref type="bibr" target="#b75">Müller, 2017)</ref>. These include modal, meta-communicative, performative, and interactive/interpersonal functions (e.g., <ref type="bibr" target="#b10">Bressem &amp; Müller, 2014;</ref><ref type="bibr" target="#b45">Kendon, 2004</ref><ref type="bibr" target="#b46">Kendon, , 2017</ref><ref type="bibr" target="#b54">Ladewig, 2014)</ref>. A more controlled setting, as in our Example 1, involve a clear task that limited the participants options for responding in a way that would make production of the desired phenomenon (i.e., a gesture describing a given written word or concept in the context of Example 1) by the participants more likely. In non-human primate studies, multimodal communication is often best observed by recording spontaneous interactions between individuals (as in Example 3). In fact, although some studies have successfully triggered the production of multimodal signals from apes interacting with human experimenters (e.g., <ref type="bibr" target="#b59">Leavens et al., 2010)</ref>, the ecological validity of such experiments has been questioned (e.g., de  <ref type="bibr" target="#b17">Waal et al., 2008;</ref><ref type="bibr" target="#b115">Whiten, 2022)</ref>. A similar problem arises for the study of multimodal perception: while vocal signals can be readily played back to test for individuals' reactions in controlled environments (e.g., <ref type="bibr" target="#b65">Mehon &amp; Stephan, 2021)</ref>, the controlled reproduction and manipulation of gestures and facial expressions is still quite limited (but see <ref type="bibr" target="#b113">Waller et al., 2016</ref>).</p><p>The two main types of data collected in multimodal research are typically classified as elicited or naturalistic. Elicited data is collected in a controlled setting like a stationary or mobile laboratory, where participants are prompted to produce specific responses, and is useful for studying specific linguistic features or behaviors under controlled conditions. Both, Example 1 and Example 2, employed such an approach although there are obvious differences:</p><p>Whereas, for Example 1, participants were recorded in a lab located at the university where the researchers carrying out the study were based, data that became part of the DGS Corpus (Example 2) was collected at various sites throughout Germany to avoid participants adjusting their dialectal language use to that of the recording region, so the data collection team would travel instead and bring along the necessary equipment. In contrast, naturalistic data is collected in participants' natural environments without the intervention of experimenters, enabling, as in studies from Example 3, the investigation of multimodal signal use across a variety of contexts and therefore providing a comprehensive understanding of the communication system under investigation. This is particularly relevant in the field of animal communication, as methods for the inference of signals' meanings and functions -two common topics of investigation -can only be derived by contextual information given by the naturalistic settings of interactions.</p><p>Moreover, it should also be mentioned here that multimodal data may also be used as stimuli in case of elicitation or any other kind of experiment, which may cause researchers to face similar obstacles during stimulus presentation that may be encountered during data collection. That is, some variables in the design of such stimuli can be more tightly controlled A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 14 than in data collection, while others are also subject to limitations in the collection of multimodal data. For example, in recording video stimuli of a signer or a person gesturing, there may be subtle differences in movements of the torso, head, or a similar subtle difference that could systematically bias participants for or against a certain experimental condition. One way of dealing with this is to repurpose the same methods that can be used to extract relevant information about multimodal data from such as, for example, computer-vision approaches for pose-estimation of people visible in a video and use these methods for controlling stimuli (e.g., to ensure that there are no systematic differences in movement patterns across different conditions of a controlled factorial experiment in the context of sign language research; see e.g., <ref type="bibr" target="#b107">Trettenbrein &amp; Zaccarella, 2021;</ref><ref type="bibr" target="#b106">Trettenbrein et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Where to Collect Data? Laboratory vs. Online vs. On-site</head><p>Laboratory data is collected in a controlled environment, providing stable conditions throughout the entire duration of the experiment. However, participants must come in person, requiring each slot to be arranged individually or depending on the research question coordinating two or more participants to come to the same appointment. However, an online participant management tool can facilitate the organisation and recruitment of participants.</p><p>Running a lab may also require a lab manager, making it the most effortful yet quality-assured environment. This can influence the smaller participant numbers typically seen in lab studies.</p><p>In contrast, online data is collected via the individual devices of each participant, thus limiting quality control. Quality can be ensured by including so-called catch trials where participants have to respond in a particular way that differs from the actual task, as well as by asking questions about the input device (laptop, tablet, mobile phone) and the output device (especially relevant for audio, such as the type of headphones 1 ) to include this information as covariate in the analysis. In perception experiments, control is higher than in production experiments conducted online due to high variance in cameras and microphones, and there is no control over the participant's position relative to the devices. The advantages include easy accessibility to large and diverse participant groups and generally a fast data collection completion (a few hours to some days).</p><p>Lastly, on-site data is collected outside the lab at the participant's location, making it most commonly used in fieldwork. The data has limited quality control during recording because it depends on the environmental conditions of the site (soundproofing, background noise, general isolation, etc.). However, the data is more naturalistic due to the familiar environmental setting and in many cases can be easier to collect from large participant numbers because the researcher travels to the participants, rather than the other way around.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From whom to collect Data? Population(s) of Interest</head><p>Depending on the actual research question, data can be collected from specific demographic or social groups (e.g., children, adults, elderly, bilingual speakers, individuals with communication disorders) or even species (e.g., non-human primates). This approach allows for comparison across different groups, enhancing our understanding of language acquisition, variation, and use in diverse populations. However, collecting data from specific groups may require more effort due to accessibility and special ethical considerations. Working with clinical populations is most effective if an institution has an established collaboration with a clinic. It is also important to consider the appropriate environmental setting for working with a given population, determining whether a lab experiment is feasible or if only an on-site experiment is possible. For the study of non-human primates, additional considerations must be made regarding the rearing conditions of the population under study, i.e. whether captive, semi-wild or wild, and decisions met based on the influence that this factor might have on the research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to Collect Data? Data Collection Methods</head><p>In naturalistic settings, data collection typically involves recording participants in realworld interactions, such as everyday conversations, interviews, or group discussions, under minimal researcher intervention. Video and audio equipment is set up to capture both speech, gesture, facial expressions, and other body movements without disrupting the natural flow of communication. Multiple camera angles and microphones help ensure that subtler gestures and simultaneous speakers are documented clearly, although this is not always possible in naturalistic settings like fieldwork. Ethnographic methods, such as participant observation, can also be employed, where researchers spend extended periods immersed in the group or community of interest. This approach allows for deeper insights into how gestures emerge within specific cultural or social routines. Ethical considerations, notably, informed consent, confidentiality, and the right to withdraw, are crucial, as these recordings often contain sensitive or personal content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical Considerations</head><p>Recent technological advancements have made it possible to capture high-quality multimodal data by integrating video, audio, and other sensor-based streams such as depth sensors, motion tracking, eye tracking, and physiological sensors (see also <ref type="bibr">Gregori et al., 2023)</ref>.</p><p>However, the successful implementation of such data collection requires careful planning regarding equipment selection, synchronization, and data management <ref type="bibr" target="#b87">(Offrede et al., 2021)</ref>.</p><p>Selecting appropriate recording equipment is crucial for ensuring data quality and usability. High-definition (HD) cameras with a minimum resolution of 1920×1080 pixels are generally recommended, although higher resolutions such as 4K or even 6K may be beneficial for datasets intended for long-term reuse or detailed analyses (for example facial details for analysis of emotions). For its second data collection campaign, DGS Corpus upgraded from HD to 6K. For precise motion analysis, particularly involving rapid hand movements such as those observed in sign languages, gestures, or finger spelling, high frame rates (e.g. 100 fps and higher) and fast shutter speeds are essential to minimize motion blur. Adequate, evenly distributed lighting is crucial, particularly at high frame rates, to avoid blurring of rapid movements. Image blur can be a major hindrance for both human annotation and for computer vision applications. Video recorded indoors at 50 frames per second is typically insufficient to capture such rapid movements. Therefore, fast shutter speeds are recommended and can either be set explicitly or achieved by using high frame rates and ensuring good lighting conditions <ref type="bibr">(Crasborn &amp; Morgan, 2021)</ref>. The setup includes five cameras: two HD cameras provide frontal views of the informants, two stereo cameras (not visible in the picture) capture the signing from above for 3D reconstruction, and one HD camera records the overall scene, including the moderator (seated between the informants, in the chair that is empty in the picture). Elicitation materials are presented on screens placed low between the signers to avoid obstructing their views. The studio arrangement ensures separate filming of each informant while also offering a comprehensive view of interactions, assisting with subsequent transcription and analysis <ref type="bibr" target="#b34">(Hanke et al., 2010)</ref>.</p><p>In studies involving dyadic interactions, a common practice is to use at least three cameras: a frontal view for each participant and a side view capturing both interactants simultaneously (see <ref type="figure">Figure 2</ref>). For sign language recordings, occlusion and the lack of depth perception in 2D recordings pose significant challenges. A top-down bird's-eye camera can be valuable for translation, annotation, or gloss transcription <ref type="bibr" target="#b34">(Hanke et al., 2010)</ref>. When collecting data for computer vision applications (e.g. full 3-D reconstruction), additional camera angles, such as 45-degree side views ore often needed. For example, each public recording in the DGS Corpus includes pose estimation data . This data, generated using OpenPose <ref type="bibr" target="#b11">(Cao et al., 2021)</ref>  Background color selection is a crucial factor in recording. While blue and green screens are commonly used, any uniform background that provides clear contrast with participants' clothing is preferable. Ideally, participants should wear a unicolored shirt, preferably black, to ensure optimal contrast with the background screen as in <ref type="figure">Figure 2</ref>. Blue screens offer a good balance, functioning as a neutral background when watching the video unaltered, while also working well for chroma key operations (i.e., when the background is virtually replaced). Green screens are a typical choice for chroma key operations, but some people report feeling uncomfortable while watching them. However, depending on the research question, unnatural or overly intense background colors and an overly simplified background setting might impede naturalistic and comfortable social interactions and thus needs to be weighed against the technical advantages. Synchronization is a critical aspect of multimodal data collection, particularly when combining different data streams such as audio, video, and sensor inputs (see also <ref type="bibr">Gregori et al., 2023)</ref>. Where technically feasible, timecode coordination should be implemented during recording to enable automatic alignment of different video and audio streams (i.e. synchronizing the internal timer generators of all recording devices, and using data recording formats with integrated timestamps). When this is not possible, external synchronization tools should be employed. A common approach is to use a clapperboard or an LED flash at the start A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 20 of each recording session, ensuring a clear visual and auditory marker on all recording devices.</p><p>Some devices can or need to be synchronized via external hardware trigger signals or remote synchronization protocols that either ensure synced hardware timing, or reliable recording of the external signal in the recorded data stream to allow for post-hoc alignment. For devices outside the auditory or visual domain (e.g. physiological data, motion sensors, thermal cameras) this is often the only option for synchronization. It is important to consult the documentation of respective devices how precise the achieved synchronization can be (a few ms or a few 100 ms?), e.g. if an experimental effect is expected in the range of ~ 200-400 ms, potential errors from synchronization should be an order of magnitude lower.</p><p>Two primary synchronization strategies can be implemented. A centralized recording setup involves a single computer managing all data streams in real time, using tools like Lab Streaming Layer (LSL, https://github.com/sccn/labstreaminglayer). LSL is a software tool and protocol designed for real-time data streaming and synchronization in experimental environments, which facilitates the seamless integration of various data sources and devices by assigning a unique timestamp to each data source. Alternatively, a decentralized approach relies on multiple independent recording devices that have a mechanism for precisely synchronizing their inter time generators prior to a recording, receive external triggers, or integrate a shared timecode signal. In such cases, post-hoc synchronization methods are required (see also <ref type="bibr" target="#b84">Nalbantoğlu, H. &amp; Kadavá, Š., 2024)</ref>. Regardless of the chosen strategy, synchronization protocols should always be tested with pilot data before full-scale data collection begins. If no synchronization strategy has been chosen prior to data collection, a last ressort in the case of audiovisual data could be to synchronize via the audio tracks, or to identify clear visual signals visible in all data streams and use an annotation tool like ELAN for determining relative offsets between the signals and control for these <ref type="bibr" target="#b120">(Wittenburg et al., 2006)</ref>.</p><p>An example of a controlled multimodal data collection setup is the gesture elicitation study (Example 1). Participants were seated against a solid blue background to enhance contrast. The experimental environment was illuminated by four studio lights to ensure consistent lighting conditions. A laptop monitor was placed at a 45-degree angle on the dominant side of the participant, positioned outside their reach to avoid interference with natural gesturing. During the experiment, after being shown a fixation cross accompanied by an auditory signal indicating the upcoming item, written stimuli were presented on the screen for four seconds. Participants produced their gestures during presentation, and were recorded from two camera angles: one directly in front of the participant and another at a 45-degree angle from the non-dominant side. To ensure precise synchronization of data streams-including the two video recordings and the randomly ordered stimulus presentation-button presses were logged in each modality. The experiment presentation software (PsychoPy) automatically recorded these events, while the cameras were positioned to capture the laptop keyboard, allowing for later alignment in ELAN.</p><p>In contrast to controlled laboratory settings, naturalistic data collection focuses on capturing real-world interactions with minimal researcher intervention. This approach is often used to study spontaneous communication in everyday conversations, interviews, or group discussions. To ensure that subtle gestures, facial expressions, and body movements are adequately documented, multiple camera angles and microphones are typically used. In addition to video and audio recordings, ethnographic methods, such as participant observation, can be employed. By immersing themselves in a particular community, researchers gain deeper insights into the role of multimodal communication in specific social and cultural contexts. To this day, the best method for studying multimodal communication in non-human species is to observe spontaneous communicative behaviors in naturalistic settings. This does not necessarily need to be in the wild; it can also occur in captive conditions that closely resemble the species' natural habitat (e.g., sanctuaries) or in zoos, depending on the research question, provided that the species can interact freely with one another. It is worth noting that if the goal is to capture the natural socio-ecological variation in which communication occurswhich may be necessary to reliably identify the functions and drivers of multimodal communicationit is preferable to conduct the study in the wild. However, researchers must be aware that naturalistic data collection raises important ethical concerns. Ethical considerations in work with animals and informed consent, confidentiality, and the right to withdraw from the study in work with humans must be strictly upheld, as such recordings often contain sensitive or personally identifiable information.</p><p>By addressing these technical considerations, researchers can enhance the quality, reliability, and reproducibility of multimodal data collection, whether conducted in controlled laboratory settings or naturalistic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Management, Data Sharing, and Ethics</head><p>Data management is of particular importance for multimodal research, as data organization, storage and accessibility get increasingly complex with the amount and diversity of data modalities. Managing multiple video streams, sensors, audio and associated metadata requires thorough planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identify Data Sensitivity and Sharing Goals</head><p>A first decision concerns the degree to which the data can or should be shared. Factors include the presence of potentially identifying audio-video recordings and the extent of anonymity participants expect or require. Many researchers are particularly concerned with ensuring participant anonymity and may decide against using video data due to the additional workload involved in safeguarding privacy.</p><p>In our Example 1 it was deemed sufficient to share only parts of the data in the form of selected short clips openly, while restricting access to the full dataset. While this is a valid strategy for specific lab experiments, in the case of a public corpus like the DGS, more exhaustive sharing of a curated dataset is the primary goal. In the DGS Corpus, all participants provided informed consent for the use of their video materials in linguistic research and publications <ref type="bibr" target="#b100">(Schulder et al., 2021;</ref><ref type="bibr" target="#b102">Schulder &amp; Hanke, 2022)</ref>. The corpus team decided to release the full data set, but as a curated set of annotated videos under a specific license, while more exhaustive sensitive raw data was stored, but is only made accessible to trusted researchers under an additional agreement. In field work with non-human primates, humans may appear accidentally on some of the video material, and it is feasible to anonymize these if full data sharing is planned. On the other hand, fieldwork involving human participants requires significantly more effort in terms of obtaining consent and ensuring anonymization, and is often not feasible. Clinical data associated with individual level health information comes with its own challenges that need to be considered for each study and dataset individually <ref type="bibr" target="#b40">(Kaur &amp; Cheah, 2024)</ref>. In such cases of (potentially) very high data sensitivity, fine-grained layered access strategies can be a solution <ref type="bibr" target="#b63">(Marcotte et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Determine Level of Anonymization and Data Access</head><p>A related decision needs to be made with respect to the balance of privacy protection and research transparency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal vs. extensive anonymization</head><p>In a sign language corpus, or a lab experiments with elicited gestures, face blurring or voice distortion would remove critical information for data analysis. Researchers need to decide whether partial anonymization is feasible, or if restricted sharing is a better compromise. A further compromise could be to process data up to the point where anonymity is given (e.g. documented extraction of higher level features from voice or face) and provide these data for sharing along with the raw data of blurred faces and voice distortion, or with audio cut away from the raw video signal. One solution is the masked-piper tool <ref type="bibr" target="#b89">(Owoyele et al., 2022)</ref> which masks video while overlaying facial, hand, and arm kinematics. This tool uses body tracking to mask the body while preserving background and kinematic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open vs. Controlled-Access Repositories</head><p>When anonymization is incomplete, depositing data in controlled-access repositories can protect participants while still allowing for scientific collaboration, review, and re-use of acquired data. For particularly vulnerable populations such as children and clinical patients, researchers typically only share derived data (e.g. transcriptions carefully screened for sensitive information, extracted motion capture kinematics, extracted binary facial expression time courses, etc.) in open repositories, and place raw video/audio under a secure credential-based access system of the institution where the data was collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated Anonymization tools</head><p>Large-scale datasets may benefit from semi-automated anonymization tools (e.g. simple face blurring or more sophisticated approaches <ref type="bibr" target="#b89">(Owoyele et al., 2022)</ref>) though researchers should still remain aware of re-identification risks via advanced computer vision or other emerging technologies. Consequently, reevaluating anonymization protocols over time is warranted, especially as analytical techniques evolve <ref type="bibr" target="#b26">(Gadotti et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plan Informed Consent and Ethical Review</head><p>Multimodal research often requires more elaborate consent materials because of the increased complexities of privacy, potential for re-identification, and associated sharing policies. Study aims, types of data collected, foreseeable risks, and potential future use of the data need to be described in detail to obtain true informed consent, and in accordance with established guidelines of the field, community stakeholders involved in the research, and ethics committees. In particular with respect to data sharing, careful planning is required with respect to ethical issues involved <ref type="bibr" target="#b67">(Meyer, 2018)</ref>. When collaborating across multiple institutions or countries, ensure that the data-collection protocol satisfies all relevant institutional review boards (IRBs), national data protection regulations (e.g., GDPR, HIPAA), and disciplinespecific ethics guidelines. Also clarify how participants (or guardians, in the case of children) can request data removal if they later withdraw consent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Establish Storage, Backup and Long-Term Archiving</head><p>Given the typically large file sizes of multimodal data (e.g. up to 4K video recordings, sensor data with high sampling rates), reliable storage solutions and backup protocols are essential. You should always make sure to take care of redundancy (at least two physically separate storage locations), keeping logs of structured metadata (e.g. sensor calibration, environment conditions, participant demographics, and other recording session details). For example, in the lab based study, each camera feed was backed up daily to external drives, session logs were carefully time-stamped to facilitate subsequent annotation in software tools such as ELAN. For field work, data redundancy and safety is at the same time more difficult to achieve and more critical: In the lab, data backup and redundant storage can often be achieved using automated scripts and simple procedures, while more planning is involved in field work (e.g. changing, backup of SD cards). Equipment is typically more safe in the lab and protected against environmental influences and data loss than "in the wild".</p><p>In addition, researchers may choose to adopt FAIR (Findable, Accessible, Interoperable, Reusable, see <ref type="bibr" target="#b102">Schulder &amp; Hanke, 2022;</ref><ref type="bibr" target="#b119">Wilkinson et al., 2016)</ref> data principles, which promote robust documentation and standardized, open metadata. Creating a formal data management plan (DMP) helps ensure consistent practices for backup, versioning, and archiving <ref type="bibr">(Michener, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repository choice for publication and sharing</head><p>For vulnerable participant groups and data with only partial anonymization only secure institutional hosting and storage of the data may be feasible. Such repositories may offer only limited data and access managing features that differ substantially across institutions. On the other hand numerous publicly accessible repositories are available that offer flexible features such as layered data access restrictions, custom licenses and advanced data management and version history (see https://www.re3data.org/ <ref type="bibr" target="#b91">(Pampel et al., 2013)</ref> for available repositories and their features). Usage licensing and repository choice is highly dependent on anonymization constraints, needs for access restrictions, and ultimately needs to be in accordance with legal regulations and the explicit consent obtained from the participants. Thus, the ultimate check before publishing a multimodal dataset always needs to be whether the storage and sharing strategies and policies align with the written informed consent. Whenever possible, persistent identifiers (e.g. DOIs) should be attached to archived datasets to allow for citation and tracking.</p><p>Tools and web services like OSF, Zenodo, or other specialized repositories often support versioning and fine-grained access controls. It is also advisable to clarify the procedure for "take-down requests", i.e. how participants or their guardians may request partial or complete removal of their data at a later point.</p><p>These five decision points-data sensitivity and sharing goals, anonymization and access, informed consent, storage and backup, and repository choice-are essential for a professional data management strategy in multimodal research. As illustrated, there are always trade-offs involved (e.g. open science vs. participant confidentiality, anonymization vs. data completeness), but addressing each decision point optimizes data collection and the opportunities for re-use and sharing for the benefit of scientific advancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this paper, we have addressed core methodological, ethical, and practical considerations in collecting multimodal data across diverse contexts, using three examples (structured gesture elicitation, a large-scale sign language corpus, and field-based primate studies). All three examples underscore the tension between control and ecological validity. In a tightly controlled lab elicitation task (Example 1), researchers can target specific phenomena such as silent gestures, but questions arise about generalizability to spontaneous interaction.</p><p>Conversely, corpus-based efforts (Example 2) capture rich, large-scale data that span demographic diversity and everyday contexts, yet demand more elaborate synchronization, metadata documentation, and annotation protocols. Field studies (Example 3) maximize ecological validity but face logistical challenges, including minimal control over lighting, participant movement, environmental disturbances, and special requirements for fail-safe equipment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Workflow for Multimodal Data Collection: A Flexible Decision Framework</head><p>This flexible decision framework for multimodal data collection, presented here as a diagram <ref type="figure">(Figure 3)</ref>, represents a key outcome of our contribution, highlighting the important factors involved in the process. While the list of factors is not exhaustive, and other similarly significant decisions may arise, we believe these are the most crucial and warrant attention.</p><p>Considering recent technical advances and the interconnectedness of these factors, as depicted in the diagram, this framework aims to guide researchers in making informed decisions for the collection of multimodal data in language sciences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Considerations</head><p>We acknowledge that many aspects remain unexamined and additional challenges must still be considered. In this section, we touch upon some of these factors.</p><p>Collecting multimodal data from children or clinical populations entails unique methodological, ethical, and practical challenges. Protocols must be adapted to accommodate each group's specific capabilities and constraints. For instance, sensors typically designed for use with healthy adults may require different calibration procedures, hardware modifications, or adaptation protocols when used with children or clinical populations. Additionally, individual differences in motor and cognitive development over time can impact the reliability of certain data readings. This is particularly important when setups integrate multiple data channels, requiring careful control to ensure data accuracy and consistency.</p><p>Sensor placement (e.g., for motion capture) and camera framing should be adjusted to account for frequent postural shifts or smaller body sizes. Stimuli and instructions must also be tailored to the developmental stage of participants to elicit behaviors across multiple modalities (e.g., speech and gesture). Shorter tasks, game-like interactions, and visually engaging stimuli can improve participant engagement, particularly in multi-sensor setups or when motiontracking equipment is used. Additionally, real-time monitoring of children's fatigue levels is crucial, as tired or distracted participants may produce incomplete or lower-quality multimodal data.</p><p>Individuals with motor or cognitive impairments may face difficulties following specific instructions required for certain recording setups. To minimize participant burden, researchers should adapt the experimental setup including, for example, task instructions and sensor configurations where possible to the needs of the target population (e.g., using remote eye-tracking instead of head-mounted devices) and consider collecting fewer but higher-quality data streams. Reducing complexity in the experimental setup can improve data reliability while ensuring a more inclusive research approach.</p><p>Furthermore, identifiable audio-video recordings pose significant privacy concerns, particularly when linked to clinical diagnoses or vulnerable populations such as children.</p><p>Researchers must carefully balance the benefits of sharing rich multimodal datasets with the need for strict de-identification protocols and robust consent procedures. Ensuring that participants and their guardians fully understand the implications of data sharing is essential for ethical data collection and management.</p><p>Finally, researchers working with small or marginalized language communities, such as Deaf Ukrainians who have migrated to Germany-a minority within a minority-should be particularly mindful of the ethical considerations involved in data collection. Language data from these groups should be gathered by individuals with established relationships within the community. It is crucial to avoid overburdening language communities with repeated data elicitation requests, as they may be navigating other challenges and priorities. Instead, researchers should seek ways to contribute meaningfully to the community, such as supporting the integration of Deaf 2 participants or engaging in initiatives that directly benefit the group.</p><p>By addressing these methodological and ethical challenges, researchers can ensure more robust and responsible multimodal data collection practices. Future work should continue refining protocols to improve inclusivity, reliability, and ethical standards in multimodal research across diverse populations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Publications Matching "Multimodal Language" in PubMed Note. The increasing interest in multimodality in the language sciences is evidenced in the number of papers published referring to the concept. The histogram shows the total number of papers matching a targeted search on PubMed using the search term "((language) OR (communication)) AND ((multimodal) OR (kinaesthetic))". Because the search produced no relevant findings prior to the 1970s, the figure only displays data from that date until the end of 2024, excluding partial data for the current year 2025. A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Technical Setup of a DGS Corpus Recording Session (Screenshot from dgskorpus_koe_01_free conversation)Note. The informants are seated facing each other at a distance of approximately three meters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and post-processed to correct errors, addresses issues such as the misrecognition of nonexistent persons, the incorrect segmentation of a single individual into multiple entities, and the accidental inclusion of the other participant's hand in the opposite front-facing recording. Additionally, frames lost during recognition are recovered. The availability of such refined pose estimation data enables detailed research into the kinematic characteristics of multimodal signals, as demonstrated by Bauer et al. (2024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Microphone selection is equally important. High-fidelity audio recording ensures that speech and other vocalizations are accurately captured. The placement of microphones must be carefully considered to maximize proximity to the speaker, while minimizing interference with visual data recording. Attention is warranted with respect to ambient noise, reverberation, and other sources of interference. If automatic transcription of speech with speaker diarization is needed, specific microphones for each speaker are advisable.Giventhe large file sizes associated with multimodal data collection, efficient data storage and management strategies are essential. Original recordings should use lossless compression formats (e.g. FFV1, FLAC, WAV) to preserve data quality. However, for longterm storage, accessibility, and sharing, widely supported formats such as MP4 with H.264 or H.265 compression are recommended. Large datasets should be stored in secure repositories with proper metadata documentation to facilitate reproducibility. Technical metadata should include detailed information about the recording devices used, specifying exact models, firmware/software versions, and hardware configurations (e.g., camera sensor specifications, frame rates, shutter speeds, microphone sensitivity, sampling rates). Additionally, precise settings for each device, such as resolution (e.g., HD, 4K, or 6K), compression format (e.g., lossless FFV1 or H.264/H.265 compression), audio encoding standards, and synchronization methods (e.g., embedded timestamps, external trigger signals) should be documented. Information about the physical environment, such as lighting setup, background color and material (particularly for chroma keying), camera and microphone placement, as well as distances and angles relative to participants, should also be included. Further, metadata should clearly describe data management procedures, including file naming conventions, storage formats, version histories, and post-processing steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FigureA</head><label></label><figDesc>Flexible Decision Framework for Multimodal Data Collection Note. The figure illustrates how researchers can move from defining their research scope and hypotheses to selection of participants and methods, thinking about technical setup, and ultimately deciding about a data management plan. The framework highlights key decision points including equipment choice, feasibility checks, synchronization, and ethical considerations. The arrows illustrate interdependence, that is a choice in one domain may affect the available options in other domains. For example, for the analysis of subtle facial expressions high-resolution cameras with higher frame rates are needed. This choice increases the demand for data storage and sophisticated synchronization protocols (especially when multiple cameras are used), while also necessitating enhanced privacy measures and more detailed informed consent procedures.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The proper use of headphones can further be pretested by playing sounds to the left and right ear respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Deaf is capitalized here to recognize the Deaf community as a distinct cultural and linguistic group, while deaf refers to the general medical condition of hearing loss.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This collaboration was supported by the German Research Foundation (DFG) priority program SPP 2392 on "Visual Communication" (ViCom).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gesture for Linguists: A Handy Primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cooperrider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldin-Meadow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="437" to="451" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1111/lnc3.12168</idno>
		<ptr target="https://doi.org/10.1111/lnc3.12168" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential and network analyses to describe multiple signal use in captive mangabeys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aychet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blois-Heulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lemasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.anbehav.2021.09.005</idno>
		<ptr target="https://doi.org/10.1016/j.anbehav.2021.09.005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When words meet signs: A corpus-based study on variation of mouthing in Russian Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistische Beiträge zur Slavistik</title>
		<editor>A. Bauer &amp; D. Bunčić</editor>
		<imprint>
			<publisher>Peter Lang</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9" to="35" />
		</imprint>
	</monogr>
	<note>Specimina philologiae Slavicae</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phonetic differences between affirmative and feedback head nods in German Sign Language (DGS): A pose estimation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schepens</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0304040</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0304040" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New Insights Into Mouthings: Evidence From a Corpus-Based Study of Russian Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kyuseva</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.779958</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.779958" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonverbal and social aspects of discourse in face-to-face interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Bavelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text -Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="8" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1515/text.1.1990.10.1-2.5</idno>
		<ptr target="https://doi.org/10.1515/text.1.1990.10.1-2.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Hands Are The Head of The Mouth. The Mouth as Articulator in Sign Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boyes Braem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton-Spence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Signum Press</publisher>
			<pubPlace>Hamburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A repertoire of German recurrent gestures with pragmatic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bressem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110302028.1575</idno>
		<ptr target="https://doi.org/10.1515/9783110302028.1575" />
	</analytic>
	<monogr>
		<title level="m">Body-Language-Communication: An International Handbook on Multimodality in Human Interaction</title>
		<editor>C. Müller, A. Cienki, E. Fricke, S. Ladewig, D. McNeill, &amp; J. Bressem</editor>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1575" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2019.2929257</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2929257" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The language instinct in extreme circumstances: The transition to tactile Italian Sign Language (LISt) by Deafblind signers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Checchetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cecchetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zucchi</surname></persName>
		</author>
		<idno type="DOI">10.5334/gjgl.357</idno>
		<ptr target="https://doi.org/10.5334/gjgl.357" />
	</analytic>
	<monogr>
		<title level="j">Glossa: A Journal of General Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Multimodal Language Faculty: A Cognitive Framework for Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schilperoord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
			<publisher>Bloomsbury Academic</publisher>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.5040/9781350404861</idno>
		<ptr target="https://doi.org/10.5040/9781350404861" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Definition of minimal contents of dataset for participation (&apos;Intelligent Automatic Sign Language Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Crasborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing Social Skills of Children and Apes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B M</forename><surname>De Waal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Horner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Whiten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5863</biblScope>
			<biblScope unit="page" from="569" to="569" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.1126/science.319.5863.569c</idno>
		<ptr target="https://doi.org/10.1126/science.319.5863.569c" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regression-based analysis of combined EEG and eyetracking data: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ehinger</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.21.1.3</idno>
		<ptr target="https://doi.org/10.1167/jov.21.1.3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feeling phonology: The conventionalization of phonology in protactile communities in the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brentari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="840" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1353/lan.2020.0063</idno>
		<ptr target="https://doi.org/10.1353/lan.2020.0063" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Signed Language Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fenlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochgesang</surname></persName>
		</author>
		<idno type="DOI">10.2307/j.ctv2rcnfhc</idno>
		<ptr target="https://doi.org/doi.org/10.2307/j.ctv2rcnfhc" />
		<editor>J. A.</editor>
		<imprint>
			<date type="published" when="2022" />
			<publisher>Gallaudet University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multicomponent and multisensory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bartolotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fryns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Momon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaffrezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitra Setia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Noordwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Van Schaik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<idno type="DOI">10.1038/s42003-021-02429-y</idno>
		<ptr target="https://doi.org/10.1038/s42003-021-02429-y" />
	</analytic>
	<monogr>
		<title level="j">Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal communication and language origins: Integrating gestures and vocalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sievers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="DOI">10.1111/brv.12535</idno>
		<ptr target="https://doi.org/10.1111/brv.12535" />
	</analytic>
	<monogr>
		<title level="j">Biological Reviews</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1809" to="1829" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gadotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Houssiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Creţu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>De Montjoye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anonymization: The imperfect science of using data while preserving privacy | Science Advances</title>
		<idno type="DOI">10.1126/sciadv.adn7053</idno>
		<ptr target="https://doi.org/10.1126/sciadv.adn7053" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gestures as a resource for the organization of mutual orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goodwin</surname></persName>
		</author>
		<idno type="DOI">10.1515/semi.1986.62.1-2.29</idno>
		<ptr target="https://doi.org/10.1515/semi.1986.62.1-2.29" />
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="29" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A roadmap for technological innovation in multimodal communication research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gregori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ćwiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fritzsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Henlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Herbort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kügler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liebal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lücking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Sánchez-Ramón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulte-Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Eiff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science (LNCS)</title>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="page" from="402" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-031-35748-0_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-35748-0_30" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extending the Architecture of Language From a Multimodal Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">10.1111/tops.12728</idno>
		<ptr target="https://doi.org/10.1111/tops.12728" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Creating Corpora: Data Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fenlon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signed Language Corpora</title>
		<editor>J. Fenlon &amp; J. A. Hochgesang</editor>
		<imprint>
			<publisher>Gallaudet University Press</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="18" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DGS Corpus &amp; Dicta-Sign: The Hamburg Studio Setup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthes</surname></persName>
		</author>
		<ptr target="https://www.sign-lang.uni-hamburg.de/lrec/pub/10047.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies</title>
		<meeting>the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="106" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Extending the Public DGS Corpus in Size and Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahn</surname></persName>
		</author>
		<editor>E. Efthimiou, S.-E. Fotinea, T. Hanke, J. A. Hochgesang, J</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m">Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</title>
		<editor>Kristoffersen, &amp; J. Mesch</editor>
		<meeting>the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</meeting>
		<imprint>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wild chimpanzees&apos; use of single and combined vocal and gestural signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hobaiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuberbühler</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00265-017-2325-1</idno>
		<ptr target="https://doi.org/10.1007/s00265-017-2325-1" />
	</analytic>
	<monogr>
		<title level="j">Behavioral Ecology and Sociobiology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal Language Processing in Human Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="652" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tics.2019.05.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2019.05.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ethical Issues Associated with Managing and Sharing Individual-Level Health Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Cheah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Muthuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-41804-4_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-41804-4_7" />
	</analytic>
	<monogr>
		<title level="m">Research Ethics in Epidemics and Pandemics: A Casebook</title>
		<editor>J. de Vries</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2024" />
			<biblScope unit="page" from="131" to="152" />
		</imprint>
	</monogr>
	<note>S. Bull,</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What Does Embodied Interaction Tell Us About Grammar? Research on Language and Social Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keevallik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/08351813.2018.1413887</idno>
		<ptr target="https://doi.org/10.1080/08351813.2018.1413887" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Gesticulation and Speech: Two Aspects of the Process of Utterance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<editor>M</editor>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<idno type="DOI">10.1515/9783110813098.207</idno>
		<ptr target="https://doi.org/10.1515/9783110813098.207" />
		<title level="m">The Relationship of Verbal and Nonverbal Communication</title>
		<editor>R. Key</editor>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="207" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gesture: Visible Action as Utterance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511807572</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511807572" />
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Reflections on the &quot;gesture-first&quot; hypothesis of language origins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<idno type="DOI">10.3758/s13423-016-1117-3</idno>
		<ptr target="https://doi.org/10.3758/s13423-016-1117-3" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Turn-taking in human face-to-face interaction is multimodal: Gaze direction and manual gestures aid the coordination of A FRAMEWORK FOR MULTIMODAL DATA COLLECTION 36 turn transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2021.0473</idno>
		<ptr target="https://doi.org/10.1098/rstb.2021.0473" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page">20210473</biblScope>
			<date type="published" when="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bleicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Böse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Corpus à la carte -Improving Access to the Public DGS Corpus</title>
		<editor>E. Efthimiou, S.-E</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fotinea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochgesang</surname></persName>
		</author>
		<ptr target="https://www.sign-lang.uni-hamburg.de/lrec/pub/24050.pdf" />
		<title level="m">Proceedings of the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources</title>
		<editor>J. Mesch, &amp; M. Schulder</editor>
		<meeting>the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources</meeting>
		<imprint>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
	<note>{ELRA Language Resources Association (ELRA) and the International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">MY DGS -annotated. Public Corpus of German Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blanck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bleicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jeziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Regen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Salden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Worseck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>3rd release (Version 3.0) [Dataset</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dgs-Korpus Project</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamburg</forename><surname>Idgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>University</surname></persName>
		</author>
		<idno type="DOI">10.25592/dgs.corpus-3.0</idno>
		<ptr target="https://doi.org/10.25592/dgs.corpus-3.0" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recurrent gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Body-Language-Communication: An International Handbook on Multimodality in Human Interaction</title>
		<editor>C. Müller, A. Cienki, E. Fricke, S. Ladewig, D. McNeill, &amp; J. Bressem</editor>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1558" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Recurrent Gestures: Cultural, Individual, and Linguistic Dimensions of Meaning-Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ladewig</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108638869.003</idno>
		<ptr target="https://doi.org/10.1017/9781108638869.003" />
		<editor>A. Cienki</editor>
		<imprint>
			<date type="published" when="2024" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="32" to="55" />
		</imprint>
	</monogr>
	<note>The Cambridge Handbook of Gesture Studies</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How a yoga pose in an online tutorial takes on meaning as felt sensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ladewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horst</surname></persName>
		</author>
		<idno type="DOI">10.1075/pbns.348.07lad</idno>
		<ptr target="https://doi.org/10.1075/pbns.348.07lad" />
	</analytic>
	<monogr>
		<title level="m">Media as Procedures of Communication</title>
		<editor>M. Luginbühl &amp; J. G. Schneider</editor>
		<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
			<date type="published" when="2024" />
			<biblScope unit="page" from="158" to="187" />
		</imprint>
	</monogr>
	<note>Media as processes of doing and perceiving</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Framework</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multimodal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collection</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2015.2460697</idno>
		<ptr target="https://doi.org/10.1109/JPROC.2015.2460697" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="1449" to="1477" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multimodal communication by captive chimpanzees (Pan troglodytes)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Leavens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Cognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="40" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10071-009-0242-z</idno>
		<ptr target="https://doi.org/10.1007/s10071-009-0242-z" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Primate Communication: A Multimodal Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liebal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Slocombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<idno type="DOI">10.1017/CBO9781139018111</idno>
		<ptr target="https://doi.org/10.1017/CBO9781139018111" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tiered Access to Research Data for Secondary Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogden-Schuette</surname></persName>
		</author>
		<idno type="DOI">10.29012/jpc.825</idno>
		<ptr target="https://doi.org/10.29012/jpc.825" />
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Hand and mind: What gestures reveal about thought (pp. xi, 416)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Female putty-nosed monkeys (Cercopithecus nictitans) vocally recruit males for predator defence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Mehon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephan</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.202135</idno>
		<ptr target="https://doi.org/10.1098/rsos.202135" />
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Meaning-making in tactile cross-signing context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Raanes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pragma.2022.12.018</idno>
		<ptr target="https://doi.org/10.1016/j.pragma.2022.12.018" />
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Practical Tips for Ethical Data Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/2515245917747656</idno>
		<ptr target="https://doi.org/10.1177/2515245917747656" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ten Simple Rules for Creating a Good Data Management Plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Michener</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004525</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1004525" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
	<note>C.E.).</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Vocal-visual combinations in wild chimpanzees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Mine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wilke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zulberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behjati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Bosshard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">P</forename><surname>Machanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Slocombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Townsend</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00265-024-03523-x</idno>
		<ptr target="https://doi.org/10.1007/s00265-024-03523-x" />
	</analytic>
	<monogr>
		<title level="j">Behavioral Ecology and Sociobiology</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">One way of working with speech and gesture data: Methodology for multimodality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mittelberg</surname></persName>
		</author>
		<idno type="DOI">10.1075/hcp.18.16mit</idno>
		<ptr target="https://doi.org/10.1075/hcp.18.16mit" />
	</analytic>
	<monogr>
		<title level="m">Methods in Cognitive Linguistics</title>
		<editor>M. Gonzalez-Marquez, I. Mittelberg, S. Coulson, &amp; M. J. Spivey</editor>
		<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="225" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The visual-gestural modality and beyond: Mouthings as a language contact phenomenon in Irish Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sign Language &amp; Linguistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="211" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<idno type="DOI">10.1075/sll.15.2.01moh</idno>
		<ptr target="https://doi.org/10.1075/sll.15.2.01moh" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Challenges of multimodality: Language and the body in social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mondada</surname></persName>
		</author>
		<idno type="DOI">10.1111/josl.1_12177</idno>
		<ptr target="https://doi.org/10.1111/josl.1_12177" />
	</analytic>
	<monogr>
		<title level="j">Journal of Sociolinguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="336" to="366" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">How recurrent gestures mean: Conventionalized contexts-of-use and embodied motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1075/gest.16.2.05mul</idno>
		<ptr target="https://doi.org/10.1075/gest.16.2.05mul" />
	</analytic>
	<monogr>
		<title level="j">Gesture</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cienki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fricke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<editor>&amp; Teßendorf, S.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Body-Language</surname></persName>
		</author>
		<title level="m">An International Handbook on Multimodality in Human Interaction</title>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<idno type="DOI">10.1515/9783110261318</idno>
		<ptr target="https://doi.org/10.1515/9783110261318" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cienki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fricke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<editor>&amp; Teßendorf, S.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Body-Language</surname></persName>
		</author>
		<title level="m">An International Handbook on Multimodality in Human Interaction</title>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Cinematic Metaphor: Experience -Affectivity -Temporality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kappelhoff</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110580785</idno>
		<ptr target="https://doi.org/10.1515/9783110580785" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>De Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Metaphors for sensorimotor experiences. Gestures as embodied and dynamic conceptualizations of balance in dance lessons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language and the Creative Mind</title>
		<editor>M. Borkent, B. Dancygier, &amp; J. Hinnell</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="295" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Framework</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multimodal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collection</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Aligning and Synchronizing Multi-Source Video and Audio Recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nalbantoğlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Š</forename><surname>Kadavá</surname></persName>
		</author>
		<ptr target="https://envisionbox.org/embedded_synchronize_and_cut_script.html" />
		<imprint>
			<date type="published" when="2024-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rathmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<ptr target="https://www.sign-lang.uni-hamburg.de/lrec/pub/10026.pdf" />
		<title level="m">Proceedings of the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies</title>
		<editor>P. Dreuw, E. Efthimiou, T. Hanke, T. Johnston, G. Martínez Ruiz, &amp; A. Schembri</editor>
		<meeting>the LREC2010 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Multi-speaker experimental designs: Methodological considerations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Offrede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mooshammer</surname></persName>
		</author>
		<idno type="DOI">10.1111/lnc3.12443</idno>
		<ptr target="https://doi.org/10.1111/lnc3.12443" />
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Orfanidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Woll</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118346013</idno>
		<ptr target="https://doi.org/10.1002/9781118346013" />
		<title level="m">Research Methods in Sign Language Studies: A Practical Guide</title>
		<editor>Morgan, G.</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Owoyele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pouw</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.softx.2022.101236</idno>
		<ptr target="https://doi.org/10.1016/j.softx.2022.101236" />
	</analytic>
	<monogr>
		<title level="j">SoftwareX</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Considering the nature of multimodal language from a crosslinguistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<idno type="DOI">10.5334/joc.165</idno>
		<ptr target="https://doi.org/10.5334/joc.165" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognition</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Making Research Data Repositories Visible: The re3data.org Registry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pampel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vierkant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scholze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kindling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Goebelbecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gundlach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schirmbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dierolf</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0078080</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0078080" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Collecting and Analyzing Sign Language Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118346013.ch4</idno>
		<ptr target="https://doi.org/10.1002/9781118346013.ch4" />
	</analytic>
	<monogr>
		<title level="m">Research Methods in Sign Language Studies</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="53" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Why We Should Study Multimodal Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2018.01109</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2018.01109" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Sign Language: An International Handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woll</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110261325</idno>
		<ptr target="https://doi.org/10.1515/9783110261325" />
		<editor>B.</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>De Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">DGS Corpus Project -Development of a Corpus Based Electronic Dictionary German Sign Language / German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prillwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwarz</surname></persName>
		</author>
		<ptr target="https://www.sign-lang.uni-hamburg.de/lrec/pub/08018.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC2008 3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora</title>
		<meeting>the LREC2008 3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The multimodal nature of communicative efficiency in social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dingemanse</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-022-22883-w</idno>
		<ptr target="https://doi.org/10.1038/s41598-022-22883-w" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Redefining Multimodality. Frontiers in Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="DOI">10.3389/fcomm.2021.758993</idno>
		<ptr target="https://doi.org/10.3389/fcomm.2021.758993" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Speech and sign: The whole human language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="DOI">10.1515/tl-2024-2008</idno>
		<ptr target="https://doi.org/10.1515/tl-2024-2008" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Linguistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="124" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Sign Language and Linguistic Universals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lillo-Martin</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9781139163910</idno>
		<ptr target="https://doi.org/10.1017/CBO9781139163910" />
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blanck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jeziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rathmann</surname></persName>
		</author>
		<idno type="DOI">10.25592/uhhfdm.14231</idno>
		<ptr target="https://doi.org/10.25592/uhhfdm.14231" />
		<title level="m">Data Statement for the Public DGS Corpus</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<idno type="DOI">10.25592/uhhfdm.1866</idno>
		<ptr target="https://doi.org/10.25592/uhhfdm.1866" />
		<title level="m">OpenPose in the Public DGS Corpus (Project Note AP06-2019-01; Project Notes of the DGS Korpus Project</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>IDGS, Hamburg University</orgName>
		</respStmt>
	</monogr>
	<note>DGS-Korpus project</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">How to be FAIR when you CARE: The DGS Corpus as a Case Study of Open Science Resources for Minority Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanke</surname></persName>
		</author>
		<editor>N. Calzolari, F</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Béchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Framework For Multimodal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazo</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.18/" />
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<editor>J. Odijk, &amp; S. Piperidis</editor>
		<meeting>the Thirteenth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A database for lexical signs of DGS (German Sign Language) and corresponding gestures: transparency, iconicity and gesture-sign overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spruijt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Science Framework</title>
		<imprint>
			<date type="published" when="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Modality-Independent Core Brain Network for Language as Proved by Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trettenbrein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-K</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Slivac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Finkbeiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Friederici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zaccarella</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/hnzgp</idno>
		<ptr target="https://doi.org/10.31234/osf.io/hnzgp" />
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Controlling Video Stimuli in Sign Language and Gesture Research: The OpenPoseR Package for Analyzing OpenPose Motion-Tracking Data in R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trettenbrein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zaccarella</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.628728</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.628728" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Functional and structural brain asymmetries in sign language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trettenbrein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zaccarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Friederici</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-443-15646-5.00021-X</idno>
		<ptr target="https://doi.org/10.1016/B978-0-443-15646-5.00021-X" />
	</analytic>
	<monogr>
		<title level="j">Handbook of Clinical Neurology</title>
		<editor>C. Papagno &amp; P. Corballis</editor>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page" from="327" to="350" />
			<date type="published" when="2025" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A Crossspecies Comparison of Facial Morphology and Movement in Humans and Chimpanzees Using the Facial Action Coding System (FACS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Vick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Smith Pasqualini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Bard</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10919-006-0017-z</idno>
		<ptr target="https://doi.org/10.1007/s10919-006-0017-z" />
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">The Neural Representation of Abstract Words: The Role of Emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Kousta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Della Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tettamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1767" to="1777" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/cercor/bht025</idno>
		<ptr target="https://doi.org/10.1093/cercor/bht025" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Gesture and speech in interaction: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Malisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.specom.2013.09.008</idno>
		<ptr target="https://doi.org/10.1016/j.specom.2013.09.008" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Macaques can predict social outcomes from facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Micheletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1031" to="1036" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10071-016-0992-3</idno>
		<ptr target="https://doi.org/10.1007/s10071-016-0992-3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Blind alleys and fruitful pathways in the comparative study of cultural cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Whiten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Life Reviews</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="211" to="238" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.plrev.2022.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.plrev.2022.10.003" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Production of and responses to unimodal and multimodal signals in wild chimpanzees, Pan troglodytes schweinfurthii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wilke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kavanagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donnellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">P</forename><surname>Machanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Slocombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.anbehav.2016.10.024</idno>
		<ptr target="https://doi.org/doi.org/10.1016/j.anbehav.2016.10.024" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">The FAIR Guiding Principles for scientific data management and stewardship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ij</forename><forename type="middle">J</forename><surname>Aalbersberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Appleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Axton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blomberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Boiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Da Silva Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bouwman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Brookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dumon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Evelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Finkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mons</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.18</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.18" />
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">ELAN: A Professional Framework for Multimodality Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wittenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klassmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sloetjes</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2006/pdf/153_pdf.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1556" to="1559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">ChimpLASG: a form-based approach to the classification of chimpanzee gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zulberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bressem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liebal</surname></persName>
		</author>
		<idno type="DOI">10.1163/14219980-950101AB</idno>
		<ptr target="https://doi.org/10.1163/14219980-950101AB" />
	</analytic>
	<monogr>
		<title level="j">Folia Primatologica</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
