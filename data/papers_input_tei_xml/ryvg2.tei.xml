<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Running head: HOLOGRAPHIC DECLARATIVE MEMORY 1 Holographic Declarative Memory: Distributional Semantics as the Architecture of Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dr</roleName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Arora</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>West</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Information Sciences and Computing</orgName>
								<orgName type="institution">Bucknell University</orgName>
								<address>
									<settlement>Lewisburg</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Cognitive Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carleton University</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Cognitive Science</orgName>
								<orgName type="department" key="dep2">David Reitter Google Research</orgName>
								<orgName type="department" key="dep3">College of Information Sciences and Computing</orgName>
								<orgName type="institution">Carleton University</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<region>New York City, NY</region>
									<country>Canada, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Bucknell University</orgName>
								<address>
									<addrLine>336 Dana Engineering Building, One Dent Drive</addrLine>
									<postCode>17837</postCode>
									<settlement>Lewisburg</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Running head: HOLOGRAPHIC DECLARATIVE MEMORY 1 Holographic Declarative Memory: Distributional Semantics as the Architecture of Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cognitive architectures</term>
					<term>vector symbolic architectures</term>
					<term>common model of cognition</term>
					<term>distributional semantics</term>
					<term>embeddings</term>
					<term>holographic memory</term>
					<term>declarative memory</term>
					<term>fan effect</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We demonstrate that the key components of cognitive architectures-declarative and procedural memory-and their key capabilities-learning, memory retrieval, probability judgement, and utility estimation-can be implemented as algebraic operations on vectors and tensors in a high-dimensional space using a distributional semantics model. High-dimensional vector spaces underlie the success of modern machine learning techniques based on deep learning. However, while neural networks have an impressive ability to process data to find patterns, they do not typically model high-level cognition, and it is often unclear how they work. Symbolic cognitive architectures can capture the complexities of high-level cognition and provide human-readable, explainable models, but scale poorly to naturalistic, non-symbolic, or big data. Vector-symbolic architectures, where symbols are represented as vectors, bridge the gap between the two approaches. We posit that cognitive architectures, if implemented in a vector-space model, represent a useful, explanatory model of the internal representations of otherwise opaque neural architectures. Our proposed model, Holographic Declarative Memory (HDM), is a vector-space model based on distributional semantics. HDM accounts for primacy and recency effects in free recall, the fan effect in recognition, probability judgements, and human performance on an iterated decision task. HDM provides a flexible, scalable alternative to symbolic cognitive architectures at a level of description that bridges symbolic, quantum, and neural models of cognition.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Holographic Declarative Memory: Distributional Semantics as the Architecture of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory 1 Introduction</head><p>Modern machine learning techniques based on neural networks and deep learning are implemented through algebraic manipulations of vectors, matrices, and tensors in high-dimensional spaces. Neural networks have an impressive ability to process data to find patterns, but they do not typically model high-level cognition and it is often unclear how they work. Symbolic cognitive architectures, such as the widely used ACT-R <ref type="bibr" target="#b2">(Anderson, 2009;</ref><ref type="bibr" target="#b69">Ritter, Tehranchi, &amp; Oury, 2019)</ref> can capture the complexities of high-level cognition and provide human-readable, explainable models of behavioural phenomena. But symbolic models scale poorly to naturalistic, non-symbolic data (such as images) or big data (e.g., corpora with hundreds of millions of words).</p><p>Are symbolic and machine learning approaches compatible? Can they be unified?</p><p>Symbolic and neural models can be understood as theories of cognition operating at different levels of description or analysis (see <ref type="bibr" target="#b48">Kersten, West, &amp; Brook, 2016</ref>, for a discussion of levels in computational cognitive models). Is it possible to provide a theory that bridges these two levels, a reduction of the symbolic to the neural, while retaining the strengths and capabilities of each?</p><p>Distributional semantics models, such as word embeddings, represent concepts as points in a high-dimensional space. Similarity between concepts is distance in that space. Distributional models can process millions of data points to infer semantic similarities from language data (e.g., <ref type="bibr" target="#b13">Burgess &amp; Lund, 1997;</ref><ref type="bibr" target="#b31">Griffiths, Steyvers, &amp; Tenenbaum, 2007;</ref><ref type="bibr" target="#b41">Jones &amp; Mewhort, 2007;</ref><ref type="bibr" target="#b53">Landauer &amp; Dumais, 1997;</ref><ref type="bibr" target="#b58">Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013;</ref><ref type="bibr" target="#b64">Pennington, Socher, &amp; Manning, 2014)</ref>, to infer product recommendations from patterns of user preferences (e.g., <ref type="bibr" target="#b72">Rutledge-Taylor, Vellino, &amp; West, 2008)</ref>, to predict human probability judgements <ref type="bibr" target="#b8">(Bhatia, 2017)</ref>, or to predict racial and gender biases <ref type="bibr" target="#b15">(Caliskan, Bryson, &amp; Narayanan, 2017)</ref>. Thus it has been argued that distributional semantics models are a potentially appropriate basis for knowledge representation in general-purpose cognitive models <ref type="bibr" target="#b9">(Bhatia, Richie, &amp; Zou, 2019)</ref>.</p><p>We posit that cognitive architectures, if implemented in a vector-space model, represent a useful, explanatory model of the internal representations of otherwise opaque neural architectures. We demonstrate that a distributional semantics model can be integrated into a cognitive architecture. Specifically, we substitute the ACT-R Declarative Memory (DM) for our model, Holographic Declarative Memory (HDM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HDM is a variant of Dynamically Structured Holographic Memory (DSHM;</head><p>Rutledge-Taylor, <ref type="bibr" target="#b71">Kelly, West, &amp; Pyke, 2014)</ref>. DSHM, in turn, is a variant of the BEAGLE model of distributional semantics <ref type="bibr" target="#b41">(Jones &amp; Mewhort, 2007)</ref> generalized to non-linguistic tasks. HDM is a model of learning for both declarative and procedural tasks. In ACT-R's DM, the association strengths between items in memory are typically set by the modeller by hand. Conversely, in HDM, the representations in memory are learned estimates of the association strengths and conditional probabilities of stimuli in the environment. DM scales poorly to large data sets (as we discuss in §4.1), whereas HDM can scale from learning small, experimental data sets up to corpora representative of a lifetime of experience. In what follows, we illustrate how memory retrieval response time, interference between memories, probability estimation, motivation, and surprise can be implemented by simple mechanisms in a vector space. Using these mechanisms, we demonstrate that HDM can account for primacy and recency effects in free recall, the fan effect in recognition, human probability judgements, and human performance on learning an iterated decision task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cognitive Architectures and the Common Model of Cognition</head><p>Since <ref type="bibr" target="#b61">Newell (1973)</ref>   <ref type="bibr" target="#b50">(Kotseruba &amp; Tsotsos, 2018)</ref>. The similarities suggest an emerging consensus on the basic principles of cognition. <ref type="bibr" target="#b52">Laird, Lebiere, and Rosenbloom (2017)</ref> find commonalities between three cognitive architectures, namely ACT-R <ref type="bibr" target="#b3">(Anderson &amp; Lebiere, 1998)</ref>, Soar <ref type="bibr" target="#b51">(Laird, 2012)</ref>, and SIGMA <ref type="bibr" target="#b70">(Rosenbloom, Demski, &amp; Ustun, 2016)</ref>. On the basis of the commonalities, <ref type="bibr" target="#b52">Laird et al. (2017)</ref> propose a Common Model of Cognition. The Common Model of Cognition is a high-level theory of the modules of the mind and how these modules interact (see <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception sensory cortices</head><p>Working Memory lateral prefrontal cortex</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedural Memory basal ganglia</head><p>Declarative / Long Term Memory hippocampus Action motor cortex <ref type="figure">Figure 1</ref> . The Common Model of Cognition <ref type="bibr" target="#b52">(Laird et al., 2017)</ref> and associated brain areas <ref type="bibr" target="#b75">(Steine-Hanson, Koh, &amp; Stocco, 2018;</ref><ref type="bibr" target="#b78">Stocco, Laird, Lebiere, &amp; Rosenbloom, 2018)</ref>. Solid arrows indicate connections that pass data between modules. Dashed arrows indicate modulation of connections.</p><p>The Common Model of Cognition consists of perceptual and motor modules that interact with the agent's environment, working memory buffers which hold the active data in the agent's mind, a declarative or long-term memory module that holds the agent's world knowledge, and a procedural memory module that controls the flow of information and evaluates possible actions <ref type="bibr" target="#b52">(Laird et al., 2017)</ref>. A large-scale evaluation of fMRI data, collected from over a thousand participants across diverse tasks, found correlations in patterns of activity across brain areas consistent with the Common Model of Cognition's description of modules and their interactions <ref type="bibr" target="#b75">(Steine-Hanson et al., 2018)</ref>. To evaluate our model, Holographic Declarative Memory (HDM), we compare to the ACT-R cognitive architecture in particular. But HDM could be used as a model of long-term memory in any cognitive architecture described by the Common Model of Cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACT-R Declarative Memory</head><p>ACT-R's declarative memory (DM) consists of items of knowledge, called chunks, weighted by an estimate of the probability that chunk is useful in the agent's current context. Each chunk is either an unordered list of slot:value pairs (e.g., "name:tiger type:animal has:stripes") or an ordered list of values (e.g., tiger animal striped).</p><p>In ACT-R, the agent's current context serves as a cue to the memory system. Chunks in memory are activated according to a combination of the chunk's base-level activation and spreading activation. Base level activation reflects how frequently and recently that chunk has been accessed. Spreading activation reflects the strength of the association between the cue and the chunk. More active memories are retrieved more easily and quickly.</p><p>For a chunk i, the activation of that chunk, A i , is:</p><p>(1)</p><formula xml:id="formula_0">A i = B i + n j=1 W j S ji</formula><p>where B i is the baseline activation of the chunk, n is the number of slot-value pairs in the cue, W j is the attention paid to slot-value pair j of the cue, and each S ji is an association strength: a measure of the probability that chunk i is relevant given that the cue contains slot-value pair j.</p><p>DM can be understood by analogy to a hydraulic system. Activation flows like water through connections between concepts like pipes. Activation spreads from the cue to the chunks in DM. Chunks with stronger associations to the cue (wider pipes) receive more activation. The chunk that receives the most activation floats to the surface of consciousness and is selected and retrieved from memory. The time, T , to retrieve a chunk, i, is a function of the chunk's activation, A i , and two fitting parameters I and F ,</p><p>(2)</p><formula xml:id="formula_1">T = I + F e −A i</formula><p>The higher the activation, the shorter the retrieval time.</p><p>ACT-R's equations for activation (Eq. 1) and retrieval time (Eq. 2) form the backbone of ACT-R's declarative memory. They can successfully model a wide range of behavioural phenomena, including practice and forgetting effects in learning new words <ref type="bibr" target="#b63">(Pavlik Jr. &amp; Anderson, 2005)</ref>, the cognitive availability of words <ref type="bibr" target="#b18">(Cole &amp; Reitter, 2018)</ref> and syntactic structures <ref type="bibr" target="#b66">(Reitter, Keller, &amp; Moore, 2011)</ref> when producing language, and the spread of neologisms through online communities <ref type="bibr" target="#b17">(Cole, Ghafurian, &amp; Reitter, 2017)</ref>. ACT-R's activation function has been related to the neurophysiology of long-term potentiation in the hippocampus <ref type="bibr" target="#b63">(Pavlik Jr. &amp; Anderson, 2005)</ref> and the firing patterns of neurons that control saccadic eye movements <ref type="bibr">(Anderson, 2009, p. 131-134)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representing Declarative Memory in a Vector Space</head><p>Holographic Declarative Memory (HDM) is a model of long-term memory that can be integrated within the ACT-R cognitive architecture 1 . HDM uses holographic reduced representations <ref type="bibr" target="#b65">(Plate, 1995)</ref>, a method of representing arbitrarily complex concepts using high-dimensional vectors. Holographic reduced representations belong to a family of methods know as vector-symbolic architectures <ref type="bibr" target="#b27">(Gayler, 2003)</ref> or hyper-dimensional computing <ref type="bibr" target="#b43">(Kanerva, 2009)</ref> and are closely related to the (low-dimensional) conceptual spaces <ref type="bibr" target="#b57">(Lieto, Chella, &amp; Frixione, 2017)</ref>. As such, HDM operates at a level of description that is the lingua franca of cognitive modeling <ref type="bibr" target="#b57">(Lieto et al., 2017)</ref>, bridging symbolic (e.g., ACT-R), quantum (e.g., <ref type="bibr" target="#b12">Bruza, Wang, &amp; Busemeyer, 2015)</ref>, and neural (e.g., Eliasmith, 2013) models.</p><p>Unlike DM, HDM does not store chunks as discrete data structures. Instead, HDM stores a pair of vectors for each unique value associated with a feature (e.g., black or square, see <ref type="figure">Fig. 2</ref>). As chunks are added to HDM, the information from each successive chunk is distributed across the relevant vectors and superimposed onto the 1 Python ACT-R <ref type="bibr" target="#b77">(Stewart &amp; West, 2007)</ref> can be installed from https://github.com/tcstewar/ccmsuite. Python ACT-R with HDM can be downloaded from https://github.com/ecphory/ccmsuite. Example HDM models can be downloaded from https://github.com/ecphory/HDM information from prior chunks, such that the vectors in the high-dimensional space shift to better represent the relationships described by all chunks stored.</p><p>In what follows, we describe first HDM's ability to scale to big data, then the elementary operations of HDM, and finally how those processes can be combined to account for human memory and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scalability</head><p>HDM is more tractable than DM for scaling to large data sets in three ways: (1) ease of training, (2) space complexity, and (3) time complexity. HDM gains scalability by sacrificing DM's perfect storage of each and every chunk. Given that human memory also sacrifices precision for efficiency, we believe the tradeoff made by HDM is reasonable as a model of human memory. However, HDM may be more appropriate for modelling semantic or cortical memory, as opposed to episodic or hippocampal memory.</p><p>as we discuss in §9.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Ease of training. When using DM, association strengths between</head><p>chunks are typically set by hand, which is not feasible for large data sets. Conversely, HDM automatically acquires the associations between all chunks stored in memory by representing the relationships between values as distance in the high-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Space complexity.</head><p>HDM is based on distributional semantics, and so the original application is language. When representing the information in a corpus of hundreds of millions of sentences, HDM needs only to store a representation for each of the tens of thousands of unique words in the corpus's vocabulary. More generally, the size of HDM scales with the number of atomic components of experience (i.e., the number of unique values) rather than with the number of experiences stored.</p><p>Conversely, when using DM with spreading activation, it is necessary to track each of the association strengths S ji between chunks and values. As such, the size of DM scales with the product of the number of chunks and the number of unique values. It may even be possible to implement HDM as a model that is invariant in size, as we discuss in §11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Time complexity.</head><p>The compute time to add a chunk to memory in HDM scales linearly with the number of slot-value pairs in the chunk, as we discuss in the next section ( §4.2). The compute time for retrieval from HDM (not to be confused with predicted response times) scales linearly with the number of unique values in memory. Conversely, the compute time for retrieval from DM scales as a function of the number of chunks. If the number of unique values (i.e., atomic elements of experience)</p><p>is less than the number of chunks (i.e., experiences stored), as is likely to be the case for big data applications, HDM will be the more efficient model in terms of compute time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Add a chunk with slots</head><p>HDM represents each slot by a permutation P slot . Each value is represented by an environment vector e value and a memory vector m value . The process of encoding is summarized in <ref type="figure">Fig. 2</ref>  <ref type="figure">Figure 2</ref> . Example of the process of encoding a stimulus in HDM.</p><p>The permutation for a given slot, P slot , is initially generated randomly and then used consistently thereafter. A permutation can be represented as a permutation matrix, a k × k matrix of zeros with a single, randomly placed one in each row and  <ref type="bibr" target="#b65">(Plate, 1995)</ref>.</p><p>Each m value is a sum of questions to which the given value is an answer. For example, as illustrated in <ref type="figure">Fig. 2</ref>, when a chunk representing a large black square, "color:black shape:square size:large", is added to HDM, m black , m square , and m large are updated. To update m black , the four questions "What color is it?", "What color is the square?", "What color is the large thing?" and "What color is the large square?" are summed together:</p><p>(3) q black = q color:? + q color:? shape:square + q color:? size:large + q color:? shape:square size:large Then to we add the queries q black to the memory m black :</p><formula xml:id="formula_2">(4) m black,t = αm black,t−1 + q black</formula><p>where t is the current time step and α is the forgetting rate.</p><p>Each question is represented by a cue vector, q, constructed by permuting each e value by the corresponding P slot and then convolving. We use "?" to denote the placeholder, as it functions much like a question mark. The placeholder vector is generated randomly like an environment vector. Using the placeholder vector, Φ, the question "What color is the large square?" is constructed as:</p><p>(5) q color :? shape:square size:large = (P color Φ) * (P shape e square ) * (P size e large )</p><p>The process of computing q black (Step 4 in <ref type="figure">Fig. 2</ref> The question-based encoding used by HDM allows the model to be structured around the atomic items of experience-values or concepts-rather than the experiences-chunks or episodes-themselves. The encoding technique used by HDM has proven effective as a method of modelling the semantic <ref type="bibr" target="#b41">(Jones &amp; Mewhort, 2007)</ref> and syntactic <ref type="bibr" target="#b46">(Kelly, Ghafurian, West, &amp; Reitter, 2020)</ref> knowledge stored in the mental lexion, but here we explore its utility as a general purpose scheme for declarative memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Add a chunk without slots</head><p>To encode chunks without slots, that is, as ordered sequences of values (e.g., "large black square"), HDM uses the permutation P before recursively to create nested permutations. The cue vector for "What came after large and before square?" or "large ? square" is: (6) q large ? square = (P before ((P before e large ) * Φ)) * e square HDM uses skip-grams such that values in a chunk do not need to be consecutive to be associated with each other. For example, in "large black square", "? square" (i.e., "What came before square?") is added to both m large and m black .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recall: Exact matching and partial Matching</head><p>To recall something, the request to HDM must provide a chunk with exactly one unknown (e.g., "color:? shape:square size:large"). In exact matching, the chunk is represented by a cue vector corresponding to a single question (e.g., "What color is the large square?"). In partial matching, the chunk is decomposed into all sub-questions (e.g., "What color is it?", "What color is the square?", "What color is the large thing?", and "What color is the large square?") and represented as the sum of those questions.</p><p>The cue vector can be constructed in the same amount of time irrespective of the number of questions, such that exact and partial matching are equally efficient.</p><p>The memory vector that has the highest similarity to the cue vector is selected as the response. Similarity between vectors is measured by the vector cosine, the cosine of the angle between vectors, which is equivalent to a normalized dot product:</p><formula xml:id="formula_3">(7) cosine(q, m value ) = k i=1 q i m i k i=1 q 2 i k i=1 m 2 i</formula><p>where q is the cue and m value is a memory vector, each of k dimensions. HDM measures the cosine similarity between the cue and the memory vector for each possible value.</p><p>Once the most similar memory vector is selected, the placeholder is replaced with the corresponding value and the modified chunk is returned to procedural memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Recognition: Request with no unknowns</head><p>To recognize something, the request to HDM must provide a chunk with no unknowns. HDM then computes the coherence of the chunk. Coherence is calculated as the mean cosine between the memory vector for each value in the chunk and the cue vectors for a chunk with that value substituted for an unknown. For example, the coherence of "color:black shape:square" is:</p><p>(cosine(q color:? + q color:? shape:square , m black ) + cosine(q shape:? + q color:black shape:? , m square )) 2 (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decay and the Serial Position Curve</head><p>In DM, each chunk i in memory has a base level activation B i that decays as a power function of the time since the chunk was last added to memory,</p><formula xml:id="formula_4">B i = ln( n j=1 t −d j )<label>(9)</label></formula><p>where n is the number of times chunk i is presented and t j is the time since the jth presentation. The rate of decay, d, is such that with d = 0 the activation of a chunk does not decay with time, whereas higher d yields faster decay.</p><p>Conversely, HDM has association strengths, but no base level activation. The activation of a value is a function of the distance in the high-dimensional space between that value and a given cue. The structure of HDM commits us to representing the availability of information in memory as entirely contingent on associations.</p><p>The serial position effect <ref type="bibr" target="#b21">(Ebbinghaus, 1885)</ref> is the finding that when people study a list, the items at the beginning and end of the list are remembered best. The serial position effect is an oft-studied key finding that has shaped the development of models and theories of human memory. The recall advantage for items at the beginning of the list (the primacy effect) is generally attributed to participants having longer to process and rehearse those items. There are, however, several competing theories of the recall advantage for items at the end of list (the recency effect).</p><p>In DM, the recency effect is due to decay. In distributed processing models, such as neural networks or holographic memory models, a recency effect can be modeled as interference from more recently acquired information partially overwriting older information (retroactive interference). Older information also interferes with the encoding and retrieval of newer information (proactive interference). To account for the recency effect, it is necessary to postulate a mechanism such that retroactive interference is stronger than proactive interference.</p><p>The holographic memory model TODAM <ref type="bibr" target="#b60">(Murdock, 1982)</ref> uses a forgetting coefficient α to update memory,</p><formula xml:id="formula_5">m i = αm i−1 + v i<label>(10)</label></formula><p>where v i is the vector for a memory trace, which is equivalent to a chunk in ACT-R, and m i−1 and m i are the memory vector before and after storage. The forgetting coefficient α ranges from 0 to 1. Multiplying the memory store by α privileges new information over old information, allowing retroactive interference to be stronger than proactive interference, which produces a recency effect. If α = 0, the model has complete amnesia, and if α = 1, the model does not privilege more recent information over older information.</p><p>In comparison to ACT-R, the forgetting coefficient α is inverse to the decay rate d, such that α = 0 is equivalent to d = ∞ and α = 1 is equivalent to d = 0. However, the decay of activation over time t in ACT-R is a power function, t −d , whereas the decay of activation in TODAM is an exponential function, α t . While on average, human learning tends to mimic a power function <ref type="bibr" target="#b68">(Ritter &amp; Schooler, 2001</ref>), this may be an artifact of aggregation. A survey by <ref type="bibr" target="#b32">Heathcote, Brown, and Mewhort (2000)</ref> finds that individual learning curves tend to more closely resemble exponential functions than power functions. Additionally, while decay in ACT-R is over time in milliseconds, decay in TODAM is properly speaking the result of interference from the addition of new information to memory.</p><p>We use α to control forgetting in HDM. D. R. J. Franklin and Mewhort <ref type="formula">2015</ref> Conversely, HDM has one memory vector per item, such that interference occurs only between different associations for a given item. As a result, there is less interference in HDM than in a single vector model. This property allows HDM to better handle big data, such as modeling language learning, but at the cost of making HDM a less accurate model of small scale memory tasks such as list learning.</p><p>To compensate, we add a time-based decay function to HDM. Whether memories decay with time <ref type="bibr" target="#b67">(Ricker, Spiegel, &amp; Cowan, 2014)</ref> or strictly due to interference <ref type="bibr" target="#b62">(Oberauer &amp; Lewandowsky, 2013)</ref> is controversial. The time-based decay function in HDM may be merely a proxy for sources of interference that have not been explicitly modeled. Decay is implemented by adding noise over time to all memory vectors,</p><formula xml:id="formula_6">(11) m t = m t−1 + ηn</formula><p>where m is a memory vector, t is the time in seconds, n is a random vector, and η is the noise coefficient. If η is zero, no noise is added and there is no decay over time. For larger η, more noise is added per second and decay is steeper.  Activation in DM and vector cosine in HDM are both estimates of the relevance of a given chunk in memory to the current situation. Specifically, in DM, activation is an estimate of the log-odds of a chunk's relevance:</p><formula xml:id="formula_7">(12) A ≈ ln( p 1 − p )</formula><p>where A is the activation of the given chunk and p is the probability that the chunk is relevant. Similarly, vector cosine in HDM is an estimate of the square root of p (see §6</p><p>and Eq. 17 and Eq. 18):</p><formula xml:id="formula_8">(13) C ≈ √ p</formula><p>where C is the vector cosine between the given chunk and the cue. Accordingly, for <ref type="figure" target="#fig_4">Fig. 3</ref>, we convert vector cosine to activation as follows:</p><formula xml:id="formula_9">(14) A = ln( C 2 1 − C 2 )</formula><p>We fit the HDM model to the DM model using a systematic grid search of the parameters η and α in the ranges to η = 1 to 100 and α = 0.0 to 1.0. We average over 10 runs of the HDM model. At η = 3 and α = 0.7, HDM and DM produce comparable activation values for the chunk over time (r = 0.59). The decay over time in HDM is more linear than in DM because noise is added to the vectors as a linear function of time (Eq. 11). A better fit to DM could likely be achieved by adding noise non-linearly in a manner that mimics the DM decay function. The simplest approach to adding noise non-linearly would be to incorporate the α parameter from Eq. 10 into the memory update in Eq. 11, but we leave this modification to HDM for future work.</p><p>To demonstrate HDM's forgetting parameters on human data, we model the serial position effect. In <ref type="figure" target="#fig_6">Fig. 4</ref>, we compare two ACT-R models of the serial position effect to human data from <ref type="bibr" target="#b59">Murdock (1962)</ref>. The two models are identical except that one uses DM and the other uses HDM. Participants and models were presented 20 words at a rate of one word every 2 seconds. After, participants reported back the list in any order (free recall). For simplicity, the models did not report back the list and instead we use the state of memory as a proxy for recall probability.</p><p>To study the list, the models stores chunks that contain pairs of items: the current item and the previous item. There are 22 items in total: the 20 words of the list, the start list cue, and the end list cue. There are 21 chunks, one for each pair of items:</p><p>(START 1, 1 2, ... 9 10, 10 END). In <ref type="figure" target="#fig_6">Fig. 4</ref>, the activation of a given item is calculated as the mean of the activations of the two chunks that the item appears in (e.g., for word 6, the activation is the mean of the chunks 5 6 and 6 7 ). We divide DM activation by 4 to convert to predicted recall probability. Error bars indicate standard deviation.  To capture the primacy effect, the models use the following rehearsal strategy: rehearse the chunk for the current and previous item of the list, then return to the start of the list and rehearse forward as far as can be recalled. DM and HDM use the same equation to determine time to recall a chunk.</p><p>Retrieval time T is an exponential function of A i , the activation of chunk i,</p><formula xml:id="formula_10">(15) T = F e −A i</formula><p>where F is the latency factor. For the free recall task, to prevent the models from rehearsing through the entire 20 item list every 2 seconds during the study phase, we use long retrieval latencies for both models.</p><p>We fit the DM model to human data by adjusting only the latency factor, while keeping the decay rate fixed to the standard value of d = 0.5 <ref type="bibr">(Anderson, 2009, p. 110)</ref>, achieving a best fit with latency F = 8.0. For HDM, we fix the dimensionality to k = 64, as dimensionality does not change average performance (more dimensions increases information storage capacity, which is necessary for scaling to bigger tasks).</p><p>We use a grid search to fit the parameters α, η, and F, achieving a best fit with α = 0.9, η = 1, and F = 0.5. While α and η serve a similar role in HDM as DM's decay rate d, determining if there is appropriate standard values for α and/or η, as there is for d, is a matter for future work.</p><p>Both models strongly correlate with the human data (r = .95 for DM, r = .90 for HDM). The results in <ref type="figure" target="#fig_6">Fig. 4</ref> demonstrate that HDM is able to account for primacy and recency effects, providing approximately as good a fit as DM to the serial position curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Interference and the Fan Effect</head><p>In the fan effect task <ref type="bibr" target="#b1">(Anderson, 1974)</ref>, participants study words pairs, such as person-location pairs (e.g., hippy-park or lawyer-bank). At test, participants are presented pairs that are either studied (targets, e.g., hippy-park) or novel (foils, e.g., lawyer-park) and must quickly discriminate.</p><p>The fan of a word is the number of pairs in the study set that contain that word.</p><p>For example, if there are three pairs in the study set that contain hippy (hippy-park, hippy-bank, and hippy-store), then hippy has a fan of three. The fan effect is the finding that, at test, participants are slower to make judgments about words with a higher fan.</p><p>If lawyer has a fan of 1 (i.e., is only in the pair lawyer-bank), then participants are faster to make judgments about pairs that contain lawyer than they are about pairs that contain hippy (fan of 3).</p><p>The fan effect illustrates a fundamental principle of human memory: the availability of information in memory is an estimate of the probability that the information is useful in the current situation. If a participant has studied 3 pairs with the word hippy, each pair has only a 1 in 3 chance of being useful for judging a test pair that contains hippy. Conversely, if the participant has studied only one pair with the word lawyer, that pair has a 100% chance of being useful for judging a test pair that contains lawyer. DM models the fan effect by setting the association strengths between the words in the cue and the pairs in memory to a function of the fan of each word <ref type="bibr" target="#b4">(Anderson &amp; Reder, 1999)</ref>. Simplifying the ACT-R equations, we find that the DM model produces a response time T in seconds that is a function of the fans f person and f place , (16) T (f person , f place ) = 0.233(f person f place ) 1/3 + 0.845 which correlates well with human data (r = .95, see <ref type="figure" target="#fig_7">Fig. 5</ref>  We model the fan effect task using HDM. Without changing any parameters in <ref type="bibr" target="#b4">Anderson and Reder (1999)</ref>'s model of the fan effect, the HDM model provides a good fit to the data (r = .91, <ref type="figure" target="#fig_7">Fig. 5</ref>). Both the DM and HDM models use a latency of F = 0.63 and no decay (i.e., for HDM, α = 1 and η = 0). HDM uses a dimensionality of k = 256. As demonstrated by Rutledge-Taylor et al. <ref type="figure" target="#fig_6">(2014, p. 18, Fig. 4</ref>), the fan effect is robust across variations in vector dimensionality, but reaction times are more stochastic when using vectors with fewer dimensions (we choose k = 256 for stability).</p><p>The fan effect arises from the geometry of the vector space, as is illustrated in <ref type="figure">Fig. 6</ref>. The memory vector for hippy, m hippy , is constructed as a sum of cues. For a fan of 2, those cues are "Who is in the park?" and "Who is in the bank?", respectively represented by the chunks "? park" and "? bank" and the corresponding vectors q ? park values for the fitting s (1999) ACT-R fan efcompute activation as a previous section. el and the HDM model hile there are slight difhe two models, both the e range of human vari-These results show that odel the fan effect, but t way: by measuring the ensional space. fan effect so well? The onditional probabilities ) fan effect model uses he memory vector for a e number of times that ther concept. Taking the ry vector gives you an ich that cue has been is, the number of times cue have occurred with duct normalized by the this case, is a frequency instances, that is to say, DM as points on a n-DM fan effect model, e sake of visualization, Suppose the model has y, namely, the "hippy is , the memory vector for park. The model is later rk" during the recogniwe take the cosine of )*epark. As mhippy = q?park memory vector is zero, surface of the hyper-. model knows "hippy is ank", then mhippy is the k cue q?bank, Pbefore Φ)*ebank ly chosen vectors are ther. Let us assume that ly orthogonal. As illusn the surface of the hytween the two cues at a e model knows that the re, mhippy will be at an e between the cues for three, mhippy is further approximates f -1/2 for the random vectors used by HDM. Thus HDM predicts that as the fan increases, the cosine decreases, but by diminishing amounts with each increase in fan. As the fan approaches infinity, the cosine approaches zero. HDM makes the intuitive prediction that increases in the fan has a steadily diminishing effect on reaction time, such that knowing 100 facts about the hippy is not appreciably different from knowing 101. The cosine in HDM approximates the square-root of the probability only when the events are equiprobable. For n events with frequencies v1 to vn, the cosine of event i is <ref type="formula">3</ref>When given events of unequal probabilities, HDM will behave as if the most frequent events are disproportionately likely and the least frequent events are disproportionately unlikely. This is a testable and possibly erroneous prediction of HDM. The quantum probability model of human judgements <ref type="bibr" target="#b14">(Busemeyer, Pothos, Franco, &amp; Trueblood, 2011</ref>) also uses vector algebra to calculate probabilities, but uses the square-roots of the frequencies, then squares the cosine, such that Equation 3 is equal to classical probability. Using the square-roots of the frequencies is not possible for HDM as it would require HDM to know a priori how frequently each event will occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work and Applications of HDM</head><p>We have presented in this paper an HDM model of the fan effect and compared it to <ref type="bibr" target="#b4">Anderson and Reder's (1999)</ref> DM model of the fan effect. However, we have only discussed fitting to the reaction time of targets, sentences presented at the recognition phase that occurred in the study set. Anderson and Reder's (1999) model for foils, sentences that were not in the study set, fails on a variant of the fan effect task (West, Pyke, <ref type="bibr">Rutledge-Taylor, &amp; Lang, 2010)</ref>. As the foil is difficult to model, we leave developing an HDM model of the foil for future research.</p><p>At present, HDM does not model recency effects, that is, more recent information is not recalled better than less re-   As the fan increases, the angle between memory and the cue increases. For a fan of f and perfectly orthogonal vectors, the cosine is f −1/2 , i.e., the square root of the probability of the item conditional on the cue. In the fan effect task, all pairs of words in the study set are equiprobable. For events with unequal probabilities, we note that for n events with frequencies v 1 to v n , the cosine of event i is:</p><formula xml:id="formula_11">cosine = v i v 1 2 +... + v i 2 +...v</formula><formula xml:id="formula_12">(17) cosine = v i v 2 1 + ... + v 2 i + ... + v 2 n</formula><p>Whereas the probability of event i is:</p><formula xml:id="formula_13">(18) probability = v i v 1 + ... + v i + ... + v n</formula><p>As a result, the cosine underestimates the probability of low frequency events. When making decisions from experience, people tend to underestimate the probability of rare events <ref type="bibr" target="#b33">(Hertwig, Barron, Weber, &amp; Erev, 2004)</ref>. Thus, the cosine's biased estimate of probability may support HDM's validity as a cognitive model. Like HDM, quantum probability models <ref type="bibr" target="#b14">(Busemeyer, Pothos, Franco, &amp; Trueblood, 2011)</ref> use the cosine between vectors to model human probability judgments.</p><p>Thus HDM can be understood as a realization of the quantum probability model of human judgment within a cognitive architecture, as we discuss in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Quantum Models and Probability Judgement</head><p>The conjunction rule of classical probability theory dictates that the probability of the conjunction of two events A and B is always less than, or equal to, the probability of the individual events:</p><formula xml:id="formula_14">(19) P (A ∧ B) ≤ P (A) P (A ∧ B) ≤ P (B)</formula><p>Famously, <ref type="bibr" target="#b80">Tversky and Kahneman (1983)</ref> showed that human reasoning around probability often violates the conjunction rule. In an experiment, Tversky and Kahneman presented participants with a story about a hypothetical person, Linda, whose characteristics aligned her towards certain interests (like feminism) more than others (like financial systems), such that: (20) P (Linda is a bank teller) &lt; P (Linda is a feminist) Participants were then asked to evaluate the probability of various statements about Linda. The critical comparison concerned the probability estimates of statements "Linda is a bank teller" and "Linda is a bank teller and a feminist." Most participants judged the latter as more probable than the former, implying that for them: (21) P (Linda is a bank teller) &lt; P (Linda is a bank teller ∧ Linda is a feminist) Tversky and Kahneman's experiment illustrates the conjunction fallacy, the belief that a conjunction is more probable than the constituent events. While the conjunction fallacy is not compatible with the classical probability theory, <ref type="bibr" target="#b14">Busemeyer et al. (2011)</ref> argue that the fallacy is explicable through quantum probability theory. The primary difference between the two theories is that while classical probability specifies outcomes in set-theoretic relationships, quantum probability theory is a geometric theory which specifies outcomes as sub-spaces of a vector space (for a general introduction to quantum probability theory, see <ref type="bibr" target="#b14">Busemeyer et al., 2011)</ref>.</p><p>Using quantum probability theory, a person's knowledge of Linda can be expressed as a vector, m Linda , and the probability of Linda being in a particular state can be understood as the size of the projection of m Linda onto the subspace representing a particular state. The formulation of probability in terms of vectors allows for the conjunction fallacy to emerge.</p><p>For example <ref type="figure" target="#fig_11">(Fig. 7)</ref>, if we take a projection of m Linda onto the vector for bank teller, m bankteller , we would expect to get a very small shadow (p Linda→bankteller ) indicating a low probability of Linda being a bank teller. The projection of m Linda onto the vector representing feminist, m feminist , though is much bigger (p Linda→feminist ). To get a vector for "Linda is a feminist and a bank teller", we take a projection p Linda→feminist onto m bankteller to get p Linda→feminist→bankteller . As we can see in the diagram, p Linda→feminist→bankteller is greater than p Linda→bankteller indicating a higher probability of Linda being a bank teller and a feminist compared to Linda being a bank teller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A" B" A B "</head><p>A B "="Projec+on"of"A"on"B"  2. Quantum probability models use the square root of the frequencies and the square of the cosine to compute the exact probability, whereas HDM computes an approximate probability (compare Eq. 17 and Eq. 18).</p><p>HDM is unable to use the square root (2) because its learns from experience (1). HDM learns the frequencies of events as an accumulating sum. To incrementally accumulate the square root of the frequency as a sum, rather than the frequency itself, would require dividing the weight of each event as it is added to memory by the square root of the frequency, and thus would require knowing the frequencies a priori.</p><p>To demonstrate the conjunction fallacy in HDM, we construct two different models. The first model is based on Busemeyer et al. <ref type="formula">2011</ref> respectively, the sets need to satisfy the following conditions to conform to the geometric arrangement in <ref type="figure" target="#fig_11">Fig. 7</ref>:</p><formula xml:id="formula_15">i |S L ∩ S F | &gt; |S L ∩ S B | ii |S B ∩ S F | &gt; |S B ∩ S L |</formula><p>To realize these inequalities, the vectors are summed as illustrated in <ref type="table" target="#tab_4">Table 1</ref>. For example, m Linda is a sum of four unique environment vectors as well as ten environment vectors that m Linda shares with m feminist and three environment vectors it shares with m bankteller , such that m Linda is more similar to m feminist than m bankteller and non-identical to either. We validated the relationships by measuring the cosines of the generated vectors <ref type="figure" target="#fig_14">(Fig. 8a)</ref>. Finally, projections were computed to reflect the probability of Linda being a bank teller and Linda being both a feminist and a bank teller <ref type="figure" target="#fig_14">(Fig. 8b)</ref>. We reproduce the conjunction fallacy reported by <ref type="bibr" target="#b80">Tversky and Kahneman (1983)</ref>. The magnitude of the projection of Linda onto bank teller is less than the magnitude of the projection onto feminist and bank teller.</p><p>However, this model of the conjunction fallacy has two major problems: (1) it uses arbitrary rather than learned vector representations for the concepts, and (2) the model assumes that the conjunction fallacy emerges from question order effects.  <ref type="bibr" target="#b82">(Wang, Solloway, Shiffrin, &amp; Busemeyer, 2014)</ref>. This question order effect can be modelled by projecting from the participant's initial belief state onto the vector representing Bill Clinton and then onto Al Gore, or vice versa <ref type="bibr" target="#b82">(Wang et al., 2014)</ref>.</p><p>If the projection model is an appropriate model of the conjunction fallacy, we would expect that the conjunction fallacy to be contingent on the order of the questions, "Is Linda a feminist?", "Is Linda a bank teller?" and "Is Linda a feminist and a bank teller?". Boyer-Kassem, Duchêne, and Guerci (2016) experimentally  Thus, while the quantum projection model may be an appropriate model for question order effects, another model is needed to account for the conjunction fallacy. <ref type="bibr" target="#b0">Aerts et al. (2017)</ref> proposes an alternative quantum model of the conjunction fallacy that relies on the "emergence of new meanings when concepts are combined" rather than on question order effects.</p><p>Similarly, <ref type="bibr" target="#b8">Bhatia (2017)</ref> proposes a model where judgements about the probability of Linda being both a bank teller and a feminist is computed as the similarity of Linda to the sum of bank teller and feminist. By training distributional semantics models such as word2vec <ref type="bibr" target="#b58">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b64">(Pennington et al., 2014</ref>) on a large corpus and then constructing the Linda vector using the description of Linda given by <ref type="bibr" target="#b80">Tversky and Kahneman (1983)</ref>, <ref type="bibr" target="#b8">Bhatia (2017)</ref> finds that the vectors for Linda, feminist, and bank teller have the necessary arrangement for the conjunction fallacy to emerge.</p><p>Our second model of the conjunction fallacy is similar to Bhatia (2017)'s model. We train HDM, or more specifically, HDM's language-based precursor, BEAGLE <ref type="bibr" target="#b41">(Jones &amp; Mewhort, 2007)</ref>, on either the novels corpus (145 million words; <ref type="bibr" target="#b37">Johns, Jones, &amp; Mewhort, 2016)</ref> or the British National Corpus (100 million words; The British National <ref type="bibr">Corpus, 2007)</ref>. We construct a vector for Linda as a sum of the memory vectors for the words in the description of Linda and a vector for bank teller as the sum of the memory vectors for bank and teller. For feminist, we use the memory vector for feminist. Representing a conjunction as a sum of vectors, as Bhatia does, we find that that for both corpora:</p><formula xml:id="formula_16">(22) cosine(m Linda , m feminist + m bankteller ) &gt; cosine(m Linda , m bankteller )</formula><p>Thus, a conjunction fallacy is predicted by the model (see <ref type="table">Table 2</ref> for cosines).</p><p>Conversely, to predict a conjunction fallacy using projection, it is necessary for the projection of feminist onto bank teller be larger than the projection of Linda onto bank teller, such that cosine(m feminist , m bankteller ) &gt; cosine(m Linda , m bankteller ). For BEAGLE using either corpora, as well as for GloVe trained on English Wikipedia and the Gigaword corpus (six billion words) 2 , we find the reverse: Linda is more similar to bank teller than feminist is to bank teller, and thus no conjunction fallacy is predicted by the projection model (see <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Cosine similarity between vector representations generated by BEAGLE, on either the novels corpus or the British National Corpus (BNC), or by GloVe. We thus see that HDM can model the specific ways in which human probability judgements depart from classical probability theory, both in question order effects, which can be modelled using vector projection, and conjunction fallacies, which can be modelled using vector addition. Furthermore, distributional semantics models are able to model human judgements in a range of paradigms, including predicting gender and racial bias in the implicit association task <ref type="bibr" target="#b15">(Caliskan et al., 2017)</ref>, and answering trivia questions <ref type="bibr" target="#b8">(Bhatia, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Procedural Learning and an Iterated Decision Task</head><p>Procedural and declarative memory are often characterized as memory of how and what, respectively. Procedural memory consists of "if condition then action" production rules weighted by utilities that estimate how good it is to do action. Declarative memory consists of information weighted by how useful it is to remember that information given a cue.</p><p>At a high level of description these two systems are the same: "if cue then information" is not much different from "if condition then action". Chunks and production rules are both weighted by probability estimates. Activation estimates the probability that a chunk is useful to know and utility estimates the probability that a production rule is useful to do.</p><p>The usefulness of knowing and doing are distinct. For example, knowing that touching a sharp object will hurt you is useful. Touching a sharp object is not.</p><p>Nevertheless, the functional similarity between procedural and declarative memory suggest that the same model of memory could be used to implement both systems.</p><p>While ACT-R traditionally uses the procedural memory system to model decision-making, ACT-R can, instead, rely on declarative memory. For example,</p><p>ACT-R's declarative memory has been used to model how humans learn to make sequential decisions when playing simple games, such as backgammon <ref type="bibr" target="#b73">(Sanner, Anderson, Lebiere, &amp; Lovett, 2000)</ref>, rock-paper-scissors <ref type="bibr" target="#b55">(Lebiere &amp; West, 1999)</ref>, and prisoner's dilemma <ref type="bibr" target="#b7">(Ben-Asher, Dutt, &amp; Gonzalez, 2013;</ref><ref type="bibr" target="#b28">Gonzalez &amp; Ben-Asher, 2014;</ref><ref type="bibr" target="#b54">Lebiere, Wallach, &amp; West, 2000)</ref>. One approach to using ACT-R DM to make decisions is codified in Instance-Based Learning Theory.</p><p>Instance-Based Learning Theory <ref type="bibr" target="#b30">(Gonzalez, Lerch, &amp; Lebiere, 2003;</ref><ref type="bibr" target="#b56">Lejarraga, Dutt, &amp; Gonzalez, 2012</ref>) is a theory of choice based on ACT-R. In Instance-Based Learning Theory, instances are stored in declarative memory. Instances are specialized chunks consisting of a choice made, the context of the choice, and the outcome. To make decisions, an Instance-Based Learning Theory model evaluates each possible choice by recalling from declarative memory an aggregate of the outcomes of that choice in contexts similar to the current situation. Instance-Based Learning Theory departs from standard ACT-R DM in two ways:</p><p>1. Instance-Based Learning Theory uses graded, partial matching of remembered contexts to the current situation, and 2. Instance-Based Learning Theory retrieves an aggregate across stored chunks rather than an exact stored chunk.</p><p>In this manner, Instance-Based Learning Theory is able to use declarative memory to estimate expected utility in a wide range of choice tasks. We note that both (1) graded similarity and (2) aggregation across instances arise inherently from the architecture of vector-space models, such as HDM.</p><p>To illustrate how HDM can implement instance-based learning, we model a task from <ref type="bibr" target="#b81">Walsh and Anderson (2011)</ref>. Walsh and Anderson have human participants perform an iterated binary decision task with initially unknown payoffs (see <ref type="figure">Fig. 9</ref>).</p><p>The tasks consists of a first choice, made by pressing one of two letter keys (represented by R and J in <ref type="figure">Fig. 9</ref>). Then an abstract cue is displayed (one of two geometric shapes, represented by S and N in <ref type="figure">Fig. 9</ref>). After the cue, a second choice is made by pressing one of two different letter keys (represented by T and V in <ref type="figure">Fig. 9</ref>). After the second choice, participants receive either positive or negative feedback (represented in the experiment by an asterisk * or hash # symbol respectively). This completes a single trial of the task. The probability of positive feedback is contingent on the first and second choice as well as the cue. The task is difficult to learn as optimal choices yield only a 50% chance of positive feedback.</p><p>As shown in <ref type="figure">Fig. 10a</ref>, over 400 trials, participants gradually learn to perform the task well. Results are from <ref type="bibr" target="#b81">Walsh and Anderson (2011)</ref>. Data is averaged over 26 participants. Error bars indicate standard error.</p><formula xml:id="formula_17">R J T V T V   good bad good 50% 50%</formula><p>20% 80% 20% 80% <ref type="figure">Figure 9</ref> . Iterated decision task. Optimism is standard in Instance-Based Learning Theory models <ref type="bibr">(Lejarraga et al., 2012, p. 3)</ref>. Each possible decision in the task, decision i , is initially associated with positive feedback, good, by adding the chunk "decision i good" to memory 30 times.</p><p>Rutledge-Taylor et al. (2014) find that optimism motivates the model to explore the decision space. This is consistent with the broaden-and-build <ref type="bibr" target="#b26">(Fredrickson, 2001)</ref> theory of positive emotions, which holds that positive emotions broaden the repertoire of actions considered when making decisions.</p><p>Each completed trial is represented sequentially as a chunk of the form "start decision1 cue decision2 feedback" and added to memory. The first decision is made by querying memory with "start ? good" and the second is made by querying with "start decision1 cue ? good". Both decisions use a recall with partial matching. The HDM model learns to perform the task well at a rate similar to humans (see <ref type="figure">Fig. 10b</ref>). We  <ref type="figure">Figure 10</ref> . Rate at which humans and HDM make optimal decisions. <ref type="figure">Fig. (a)</ref> adapted from <ref type="bibr" target="#b81">Walsh and Anderson (2011)</ref>. Error bars indicate standard error.</p><p>learning to correctly choose V is the easiest (p &lt; 0.0001, repeated measures permutation test), learning to correctly choose T is the hardest (p &lt; 0.0001), and overall performance improves from the first block of 200 trials to the second (p &lt; 0.0001).</p><p>However, while HDM learns to correctly choose T at greater than chance (p &lt; 0.0001), HDM correctly chooses T in the second block of trials much less often than human participants (64% for HDM vs. 79% for humans). <ref type="figure" target="#fig_18">Fig. 11</ref> shows the rate at which HDM makes optimal decisions over the 400 trials, averaged across 100 runs of the model. <ref type="figure" target="#fig_18">Fig. 11a</ref> illustrates that after the first 50 trials, HDM's ability to correctly choose T does not improve further, despite performance well below 100% correct.</p><p>HDM struggles to learn to correctly choose T because, in absolute terms, given the choice between T and V, V is more likely to yield positive feedback. Choosing T is only correct conditional on having seen a particular cue (N, see <ref type="figure">Fig. 9</ref>). As discussed in §6, and unlike an instance-based learning model implemented using DM, HDM underestimates low probability events. As such, the probability of receiving positive feedback from T will be underestimated by the HDM model.</p><p>A critical difference between procedural memory and declarative memory is that procedural memory uses reinforcement learning. In reinforcement learning, an association is learned as a function of how surprising it is. How surprising an observation is can be measured by the magnitude of the difference between prediction and observation. <ref type="bibr" target="#b81">Walsh and Anderson (2011)</ref> find that human performance on the task is consistent with temporal-difference reinforcement learning models. We hypothesize that the performance of Rutledge-Taylor et al.'s (2014) model could be improved by implementing surprise-driven reinforcement learning in HDM.</p><p>To implement surprise in HDM, we borrow from MINERVA-AL <ref type="bibr" target="#b36">(Jamieson, Crump, &amp; Hannah, 2012)</ref>, an instance-based model of associative learning.</p><p>MINERVA-AL implements surprise as discrepancy encoding: the model learns the difference between the observed and expected outcome. To use discrepancy encoding, we have HDM predict the next symbol at each step of the task, conditional on the symbols seen so far. At the end of each round, the difference between the observed sequence and the predicted sequence is added to memory. For example, if the predicted sequence is start J S V good but the observed sequence is start J N V bad, then the sequence of vectors added to memory is:</p><p>(23) e start , e J , (e N − e S ), e V , (e bad − e good )</p><p>With discrepancy encoding, HDM's ability to correctly choose T improves over the 400 trials (see <ref type="figure" target="#fig_18">Fig. 11b</ref> compared to <ref type="figure" target="#fig_18">Fig. 11a</ref>) such that by the end, HDM correctly chooses T as often as humans (80% for HDM vs. 79% for humans; see <ref type="figure">Fig. 12</ref>).  <ref type="figure">Figure 12</ref> . Rate at which HDM with discrepancy encoding makes optimal decisions.</p><p>Error bars indicate standard error. As in <ref type="figure">Fig. 10b</ref>, HDM uses 256 dimensional vectors and results are averaged across 100 runs of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>In the preceding discussion, we demonstrate that HDM can account for interference effects in declarative memory, probability judgements, and learning an iterated decision task. HDM is, however, part of a wider family of related models, with broader applications to modelling language and math cognition, game-playing, and knowledge-representation and inference. HDM is also not the only approach to integrating modern machine learning techniques with cognitive architectures. In what follows, we discuss models closely related to HDM and other approaches to re-expressing cognitive architectures in a vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Related Models</head><p>HDM is closely related to the following models, and as such, can replicate human performance on the same tasks. 9.1.1 BEAGLE. BEAGLE can capture semantic similarity between words <ref type="bibr" target="#b41">(Jones &amp; Mewhort, 2007)</ref> and semantic priming effects in lexical decision tasks <ref type="bibr" target="#b40">(Jones, Kintsch, &amp; Mewhort, 2006)</ref>. For example, people are faster to recognize the word pepper when primed with salt. Semantic priming is predicted by the distance between BEAGLE's memory vectors in the high-dimensional space. BEAGLE has also been used for early detection of Alzheimer's disease by identifying abnormal language deficits <ref type="bibr">(Johns et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">Hierarchical Holographic Model. The Hierarchical Holographic</head><p>Model, a recursive variant of BEAGLE with multiple levels of representations, is able to learn arbitrarily abstract relationships <ref type="bibr" target="#b46">(Kelly et al., 2020)</ref>. Sensitivity to abstract relationships is useful for capturing sytactic similarity between words, for ordering words into grammatical sentences, and for being able to distinguish between grammatical and ungrammatical word orderings, even in the case of nonsensical sentences that lack semantics. For example, both humans and the Hierarchical Holographic Model show a preference for "Colorless green ideas sleep furiously" over "Furiously sleep ideas green colorless".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.3">Dynamically Structured Holographic Memory. Dynamically</head><p>Structured Holographic Memory (DSHM; Rutledge-Taylor et al., 2014) can account for the problem size effect: the finding that smaller sums (e.g., 2 + 3 = 5) are easier to remember than larger sums (e.g., 5 + 7 = 12). DSHM is also able to account for human decisions when playing rock-paper-scissors, demonstrating the model's ability to adapt quickly to rapidly changing patterns in behaviour. 9.1.4 K-HDM. K-HDM <ref type="bibr" target="#b6">(Arora, West, Brook, &amp; Kelly, 2018</ref>) is a variant of HDM that incorporates an ontology proposed by <ref type="bibr" target="#b44">Kant (1781)</ref> as a method of representing knowledge of the world and making inferences using that knowledge.</p><p>K-HDM uses an ontology in order to better support modelling general intelligence.</p><p>Taken together with our findings, the aforementioned research demonstrates that HDM's algorithm, and, more broadly, distributional semantics models and vector-symbolic architectures, are capable and versatile techniques for modelling human learning, memory, and knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Episodic Memory</head><p>Declarative memory in ACT-R is understood as comprising both episodic and semantic memory. Episodic memory is typically associated with the hippocampus and semantic memory with cortical regions, such as the temporal lobe. If there is a computational distinction to be made between semantic and episodic memories, it is the degree of aggregation. Recalling a specific episode or event relies on narrow focused retrieval from a relatively small set of memories (i.e., the memories of that event), whereas recalling a fact or meaning relies on broad focused retrieval across a large set of memories (i.e., all memories in the agent's lifetime related to that fact).</p><p>HDM is a modified model of the mental lexicon, though, as we demonstrate in this paper, the approach used by HDM has broad applicability to modelling memory in general. However, HDM may be more appropriately understood as a model of semantic memory. A good candidate for an episode memory model is the MINERVA class of memory models (e.g., <ref type="bibr" target="#b34">Hintzman, 1986;</ref><ref type="bibr" target="#b35">Jamieson, Avery, Johns, &amp; Jones, 2018)</ref>, a vector-based model of human memory that stores one vector for each memory trace (i.e., ACT-R chunk) and has strong commonalities with ACT-R DM <ref type="bibr" target="#b20">(Dimov, 2016)</ref>.</p><p>To account for the full capabilities of human memory across all time scales of learning, it may be necessary to adopt a content-addressable auto-associative memory model to represent episodic memory (such as MINERVA) and a distributional semantics model to represent semantic memory (such as HDM). The practical difference between these two types of memory model is primarily the granularity of the stored representations: an episodic memory system stores episodes, specific events that occur in the life of the agent, whereas a semantic memory system stores concepts, which can be understood as collections of events interrelated by a shared pattern. SDM also encodes information quite differently from HDM. While SDM simply stores the item itself in the word vector, HDM stores the item's associations in the memory vector. As a result, HDM can compactly encode a network of associations between items, allowing it to model, for example, interference in the fan effect and the semantics of words in natural language. That said, we believe that HDM could be re-implemented as an SDM and that SDM is a sufficiently versatile architecture that it could be used to account for many of the same effects as HDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Cognitive Architectures</head><p>LIDA is a symbolic architecture and thus has to translate between symbols and vectors when storing information in SDM. Re-implementing the entirety of LIDA as a vector-symbolic architecture (e.g., using holographic vectors) has been proposed <ref type="bibr" target="#b74">(Snaider &amp; Franklin, 2014)</ref>   <ref type="bibr" target="#b42">Kanerva, 1988)</ref> or a memory tesseract <ref type="bibr" target="#b47">(Kelly, Mewhort, &amp; West, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>Our model, Holographic Declarative Memory (HDM), realizes a fundamental principle of human memory identified by the rational analysis of cognition. Namely, the availability of information in memory is an estimate of the probability that the information is useful in the current situation <ref type="bibr" target="#b5">(Anderson &amp; Schooler, 1991;</ref><ref type="bibr" target="#b16">Chater &amp; Oaksford, 1999)</ref>. HDM realizes this principle through the geometries of the high-dimensional vector space. This geometric property allows HDM to model both the interference effects between related stimuli in the fan effect task and quantum probability effects in probability judgements, such as the conjunction fallacy.</p><p>HDM is able to learn to perform an iterated decision-making task with initially unknown payoffs at a rate comparable to human learners. To do so effectively, HDM makes use of two mechanisms: sensitivity to prediction error and a motivation to explore the decision space. When making choices, HDM selects the option that it estimates as having the highest probability of positive feedback. To motivate HDM to try unexplored options, each option is initialized to estimate a high probability of positive feedback (i.e., HDM is optimistic).</p><p>Sensitivity to prediction error (i.e., surprise) is implemented in HDM by weighting events more strongly in memory when the model fails to predict them. The weighting helps HDM learn and make decisions about low probability events, improving the model's ability to do the decision task and improving the fit to human data.</p><p>Furthermore, HDM is able to approximate the primacy and recency effect in free recall, demonstrating an ability to simulate forgetting caused by interference effects from the temporal order of stimuli.</p><p>HDM's mechanisms of encoding, retrieval, surprise, and forgetting are intended as architectural features of the model. Conversely, motivation (e.g., optimism) exists outside of HDM and is implemented by the procedural memory system.</p><p>HDM is highly scalable, as the memory system is an N × 2k matrix, where N is the total number of unique slots and values, and k is the vector dimensionality. When training HDM on very large data sets (i.e., millions of chunks), we recommend using thousands of dimensions to maintain encoding fidelity (e.g., k = 1024 or 2048).</p><p>However, for domains where the number of possible slots and values N that an input can take is very large or even infinite (such as in vision), it may be necessary to adopt a different implementation of HDM, as we discussion in §11.</p><p>An integrated theory of cognition should have both a symbolic (e.g., a description in terms of features and values) and a sub-symbolic (e.g., a description in terms of vector algebra) component to provide satisfying explanations. We find that distributional semantic representations instantiated using a vector-symbolic architecture are a natural candidate for an account of declarative memory and its learning processes, as well as aspects of what is commonly considered procedural memory and learning.</p><p>Our intent is to advance toward a cognitive architecture that is capable of modeling human performance at all scales of learning, from the half-hour lab experiment to skills acquired gradually over a lifetime. By re-implementing ACT-R's declarative memory using distributional semantics, we create a system that can be integrated with modern machine learning techniques in deep learning while retaining long-term memory, single-trial learning, judgement, and other cognitive capacities associated with high-level cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Future Work</head><p>Since <ref type="bibr" target="#b61">Newell (1973)</ref> first argued for the necessity of unified theories of cognition, there has been a great deal of work in developing computational, cognitive architectures. However, few of these architectures yet make use of modern, powerful, machine learning techniques. The Common Model of Cognition <ref type="bibr" target="#b52">(Laird et al., 2017)</ref> describes a blueprint for realizing a cognitive architecture, consisting of declarative memory, procedural memory, working memory, perceptual systems, and a motor module. HDM is a candidate model for realizing declarative memory, and aspects of procedural memory, in a manner that can be integrated with other vector-based approaches, such as deep-learning models of perception and action.</p><p>HDM, however, has two limitations as a model of cognition, both arising from the fact that HDM stores information across a large number of discrete addresses (i.e., memory vectors):</p><p>1. HDM is not a good model of proactive and retroactive interference effects, as it cannot exhibit such interference between memories that are stored in separate memory vectors. We approximate such interference effects using a decay mechanism, as ACT-R does, in §5. However, there is good reason to believe that decay is merely an approximation, as experiments have shown that the magnitude of forgetting is a function of interference rather than time (see D. R. J. <ref type="bibr" target="#b23">Franklin &amp; Mewhort, 2015</ref>, for discussion and an interference-based model of the serial position curve).</p><p>2. HDM assumes that the perceptual environment can be tokenized into a finite set of items, each of which is assigned a memory vector. While this assumption works well for language, which can be tokenized into words without difficulty, the requirement limits the ability of HDM to model other, more complex environments with continuous-valued inputs.</p><p>Given these limitations, HDM is best understood as a high-level model of how declarative memory is implemented in the brain. HDM could be realized in a lower-level model that does not make the simplifying assumption of one memory vector per item, such as a Sparse Distributed Memory (SDM; <ref type="bibr" target="#b42">Kanerva, 1988)</ref> or a memory tesseract <ref type="bibr" target="#b47">(Kelly et al., 2017)</ref>. Such a lower-level re-implementation of HDM would allow proactive and retroactive interference to emerge between unrelated items, causing temporal order and serial position effects. Furthermore, relaxing the assumption of one item per memory vector would allow HDM to operate in complex environments that cannot be easily tokenized into items. Thus, both of the limitations of HDM can potentially be addressed using a more complex, lower-level architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>first argued that good empirical work and piecemeal theoretical work are insufficient to achieve the goal of understanding the mind, researchers in cognitive science have sought to develop functional, testable theories of cognition as a whole. Cognitive architectures serve as both unified theories of cognition and as computational frameworks for implementing models of specific experimental tasks. Hundreds of cognitive architectures have been developed over the past 40 years and many have strong similarities to each other</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) may seem complex, but the cue is efficiently constructed by HDM in O(nk log(k)) time, scaling linearly with the number of slot-value pairs n and scaling in O(k log(k)) time with respect to the vector dimensionality k. Linear computation with respect to the number of slot-value pairs is achieved by taking advantage of the fact that convolution distributes over addition (i.e., a * (b + c + d) = a * b + a * c + a * d), such that all four of the questions that make up q black are computed in a single pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>'s holographic model accounts for both primacy and recency effects in terms of rehearsal and interference without needing a time-based memory decay function. D. R. J. Franklin and Mewhort's model is a single holographic memory vector that stores all list items. As such, all new items stored interfere with all previous items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Example activation of a chunk over time in DM (with noise turned off) and HDM (averaged over 10 runs). To compare DM and HDM, we treat the coherence of a chunk in HDM as analogous to base level activation in DM. In Fig. 3, we compare activation of a chunk over 30 seconds in DM and HDM. The DM model has a decay rate of d = 0.5. The random noise component of DM's activation function is set to zero for ease of comparison with HDM, as turning DM's random noise on would introduce an additional uncorrelated source of variation. HDM has dimensionality k = 64, forgetting α = 0.7, and noise η = 3.0. The chunk is repeatedly stored in memory at random intervals indicated in Fig. 3 by vertical lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Performance as a function of list position. Error bars indicate standard error for models. Results averaged across 10 runs of each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Response time for targets in the fan effect task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>mhippy with a fan of 2 (left) or 3 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Computing geometric probabilities using projections p of memory vectors m onto other memory vectors. Human probability judgements are the squared magnitudes of the projections, e.g., |p Linda→feminist→bankteller | 2 = P (feminist ∧ bankteller|Linda).Because of the geometric nature of quantum probability, it is trivial to implement in vector-symbolic architectures, allowing models such as HDM to account for the conjunction fallacy. However, HDM differs from Busemeyer et al. (2011)'s quantum probability models in two important ways: 1. HDM can learn the strengths of associations between items from experience whereas in quantum models the vector similarities are set by hand;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>'s model of the conjunction fallacy, and like in Busemeyer et al.'s model, the vectors are set by hand. The second model is similar to Bhatia (2017)'s model of the conjunction fallacy, and like in Bhatia's model, the vectors are learned from experience. For the first model, we conduct 100 simulations, each with a different set of randomly generated environment vectors. The vectors m Linda , m feminist , and m bankteller are each constructed as a sum of environment vectors. So, if S L , S F , and S B are the sets of environment vectors summed to construct m Linda , m feminist , and m bankteller</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Magnitude of the projection of Linda onto bank teller and both feminist and bank teller</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 .</head><label>8</label><figDesc>HDM model of the conjunction fallacy using vectors artificially constructed according toTable 1. Error bars indicate standard error over 100 runs.manipulate the question order for the Linda story, and other scenarios that elicit a conjunction or disjunction fallacy, and while Boyer-Kassem et al. replicate the fallacy, they find no question order effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>vector cosines BEAGLE (novels) BEAGLE (BNC) GloVe cosine(m Linda , m feminist + m bankteller ) 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure adaptedfrom Walsh and Anderson (2011). Rutledge-Taylor et al. (2014) apply HDM to Walsh and Anderson's task. HDM is initialized to a state of optimism, the belief that all choices are potentially rewarding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>replicate Rutledge-Taylor et al.'s model here. The HDM model uses 256 dimensional vectors. Results are averaged across 100 runs of the model. HDM captures the general pattern of results from human participants in this task:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 .</head><label>11</label><figDesc>HDM response accuracy across trials, with and without discrepancy encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>column. Multiplying e value by P slot reorders the elements of e value to create a unique representation of the slot-value pair.An environment vector e value stands for the perceptual features of a stimulus. We</figDesc><table><row><cell>do not simulate the details of the perceptual features (but see Cox, Kachergis, Recchia,</cell></row><row><cell>&amp; Jones, 2011; Kelly, Blostein, &amp; Mewhort, 2013; Kievit-Kylar &amp; Jones, 2011, for</cell></row><row><cell>models that do). Instead, environment vectors are generated by randomly sampling</cell></row><row><cell>from a Gaussian distribution with a mean of zero and a variance of 1/k, where k is the</cell></row><row><cell>dimensionality. In HDM, the dimensions are meaningless, only the relationships</cell></row><row><cell>between vectors are meaningful. The number of dimensions, k, determines the fidelity</cell></row><row><cell>with which HDM stores information, such that smaller k yields poorer encoding.</cell></row><row><cell>A memory vector m value represents the associations a stimulus has with other</cell></row><row><cell>stimuli in the environment. Memory vectors are constructed as HDM learns about the</cell></row><row><cell>world. Memory vectors are holographic in that they use circular convolution (denoted</cell></row><row><cell>by  * ) to compactly encode associations between values</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>Unique and shared environment vectors summed into m Linda , m feminist , and m bankteller</figDesc><table><row><cell></cell><cell cols="3">Linda feminist bank-teller</cell></row><row><cell>Linda</cell><cell>4</cell><cell>-</cell><cell>-</cell></row><row><cell>feminist</cell><cell>10</cell><cell>4</cell><cell>-</cell></row><row><cell cols="2">bank-teller 3</cell><cell>6</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>When modelling question order effects, there is a clear order of operations: one question is asked first and how the participant answers that question affects how they answer the second question. For example, participants may be asked about honesty and trustworthiness of the American politicians Bill Clinton and Al Gore. If asked about Bill Clinton first, people rate Al Gore as less trustworthy than if asked about Al Gore first</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>While modern machine learning techniques have, for the most part, not yet been integrated into cognitive architectures<ref type="bibr" target="#b50">(Kotseruba &amp; Tsotsos, 2018)</ref>, there are some notable exceptions. The cognitive architectures LIDA and SPAUN both take an approach similar to HDM to model cognitive functions.Franklin et al., 2016) uses a high-dimensional vector-symbolic model of long-term memory in order to capture the dynamics of interference and forgetting. Specifically, LIDA uses Sparse Distributed Memory (SDM;<ref type="bibr" target="#b42">Kanerva, 1988)</ref>, which, like HDM, uses paired environment and memory vectors, but the vectors in SDM are referred to as addresses and words respectively, by analogy to the Random Access Memory. However, unlike HDM, there is not a one-to-one correspondence in SDM between addresses and the items in the environment. Instead items are represented distributed across multiple addresses and the corresponding words. The advantage of this is that SDM can in principle handle environments where the items are initially unknown, but the disadvantage is that SDM is less well optimized than HDM for a given set of items.</figDesc><table /><note>9.3.1 LIDA. The LIDA cognitive architecture (Learning Intelligent Distribution Agent; S.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>but remains a matter of future work. The SPAUN cognitive architecture (Semantic Pointer Architecture Unified Network; Eliasmith, 2013), like HDM, uses holographic vectors, but is implemented as a biologically realistic spiking neural model using the NENGO neural modelling platform. SPAUN is a lower level cognitive architecture, concerned with how, exactly, cognitive functions are realized by the brain. As such, SPAUN is computationally intensive to simulate on conventional computers and is best run on neuromorphic hardware.While SPAUN has a robust model of procedural memory (i.e., the basal ganglia;<ref type="bibr" target="#b76">Stewart, Bekolay, &amp; Eliasmith, 2012)</ref>, SPAUN has yet to standardize on a model of declarative memory. HDM could potentially be implemented in NENGO and integrated into SPAUN as a declarative memory system. Re-implementing HDM in NENGO would likely require re-expressing HDM in terms of a lower-level model (see §11 for discussion), such as a Sparse Distributed Memory (SDM;</figDesc><table><row><cell>9.3.2 SPAUN.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Pre-trained GloVe downloaded from: https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors gratefully acknowledge funding from the National Science Foundation </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Testing quantum models of conjunction fallacy on the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Arguëlles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>De Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veloz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10773-017-3288-8</idno>
		<ptr target="https://doi.org/10.1007/s10773-017-3288-8" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Theoretical Physics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3744" to="3756" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrieval of propositional information from long-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0285(74)90021-8</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="451" to="474" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How can the human mind occur in the physical universe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The atomic components of thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Lawrence Erlbaum Associates</publisher>
			<pubPlace>Mahwah, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fan effect: New results and new theories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Reder</surname></persName>
		</author>
		<idno>doi: 10.1.1.139.8243</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reflections of the environment in memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schooler</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.1991.tb00174.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="408" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why the common model of the mind needs holographic a-priori categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.11.060</idno>
	</analytic>
	<monogr>
		<title level="m">Papers on the Common Model of Cognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accounting for the integration of descriptive and experiential information in a repeated prisoner&apos;s dilemma using an instance-based learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ben-Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://www.researchgate.net/publication/273259835" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference on Behavior Representation in Modeling and Simulation</title>
		<editor>B. Kennedy, D. Reitter, &amp; R. St. Amant</editor>
		<meeting>the 22nd Annual Conference on Behavior Representation in Modeling and Simulation<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>BRIMS Society</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Associative judgment and vector space semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000047</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed semantic representations for modeling human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2019.01.020</idno>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantum-like models cannot account for the conjunction fallacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boyer-Kassem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duchêne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guerci</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11238-016-9549-9</idno>
	</analytic>
	<monogr>
		<title level="j">Theory and Decision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="510" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bodleian Libraries, University of Oxford, on behalf of the BNC Consortium</title>
		<ptr target="http://www.natcorp.ox.ac.uk/" />
	</analytic>
	<monogr>
		<title level="m">The British National Corpus</title>
		<editor>3, BNC XML ed.).</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantum cognition: A new theoretical approach to psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2015.05.001</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="383" to="393" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelling parsing constraints with high-dimensional context space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<idno type="DOI">10.1080/016909697386844</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A quantum theoretical explanation for probability judgement errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Pothos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trueblood</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0022542</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aal4230</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ten years of the rational analysis of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oaksford</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1364-6613(98)01273-X</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linking memory activation and word adoption in social language use via rational analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<ptr target="https://core.ac.uk/download/pdf/148333135.pdf#page=214" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Cognitive Modeling</title>
		<editor>M. K. van Vugt, A. P. Banks, &amp; W. G. Kennedy</editor>
		<meeting>the 15th International Conference on Cognitive Modeling<address><addrLine>Coventry, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
		<respStmt>
			<orgName>University of Warwick</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The timing of lexical memory retrievals in language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N18-1183" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2017" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards a scalable holographic representation of word form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kachergis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Recchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-011-0125-5</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="602" to="615" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connections between act-r&apos;s declarative memory system and minerva2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Dimov</surname></persName>
		</author>
		<ptr target="https://cogsci.mindmodeling.org/2016/papers/0096/paper0096.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Cognitive Science Society</title>
		<editor>A. Papafragou, D. Grodner, D. Mirman, &amp; J. C. Trueswell</editor>
		<meeting>the 38th Annual Meeting of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="492" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Memory: A contribution to experimental psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ebbinghaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1885" />
			<publisher>Dover</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to build a brain: A neural architecture for biological cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memory as a hologram: An analysis of learning and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.1037/cep0000035</idno>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="115" to="135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Madl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Faghihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kugele</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A LIDA cognitive model tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bica.2016.04.003</idno>
	</analytic>
	<monogr>
		<title level="j">Biologically Inspired Cognitive Architectures</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="130" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of positive emotions in positive psychology: The broaden-and-build theory of positive psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Fredrickson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.56.3.218</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="218" to="226" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vector symbolic architectures answer Jackendoff&apos;s challenges for cognitive neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Gayler</surname></persName>
		</author>
		<ptr target="http://cogprints.org/3983/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint International Conference on Cognitive Science</title>
		<editor>P. Slezak</editor>
		<meeting>the Joint International Conference on Cognitive Science<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
		<respStmt>
			<orgName>University of New South Wales</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to cooperate in the prisoner&apos;s dilemma: Robustness of predictions of an instance-based learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ben-Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th</title>
		<editor>P. Bello, M. Guarini, M. McShane, &amp; B. Scassellati</editor>
		<meeting>the 36th</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<ptr target="https://escholarship.org/uc/item/8n6437tp" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Cognitive Science Society</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2287" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instance-based learning in dynamic decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lerch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0364-0213(03)00031-4</idno>
		<ptr target="https://doi.org/10.1016/S0364-0213(03)00031-4" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="635" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Topics in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.114.2.211</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The power law repealed: The case for an exponential law of practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03212979</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decisions from experience and the effect of rare events in risky choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">U</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.0956-7976.2004.00715.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="534" to="539" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Schema abstraction&quot; in multiple-trace memory models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hintzman</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.95.4.528</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="411" to="428" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An instance theory of semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Avery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42113-018-0008-2</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="136" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An instance theory of associative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J C</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hannah</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13420-011-0046-2</idno>
	</analytic>
	<monogr>
		<title level="j">Learning &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="61" to="82" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Experience as a free parameter in the cognitive modeling of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Cognitive Science Society</title>
		<editor>A. Papafragou, D. Grodner, D. Mirman, &amp; J. C. Trueswell</editor>
		<meeting>the 38th Annual Meeting of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1325" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Farlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Kareken</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using cognitive models to investigate the temporal dynamics of semantic memory impairments in the development of Alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="http://iccm-conference.org/2013-proceedings/papers/0004/index.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Cognitive Modeling</title>
		<editor>R. West &amp; T. Stewart</editor>
		<meeting>the 12th International Conference on Cognitive Modeling<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
		<respStmt>
			<orgName>Carleton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-dimensional semantic space accounts of priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kintsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2006.07.003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="534" to="552" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representing word meaning and order information in a composite holographic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.114.1.1</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sparse distributed memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-009-9009-8</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Critique of pure reason</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kant</surname></persName>
		</author>
		<editor>Werner S. Pluhar</editor>
		<imprint>
			<date type="published" when="1781" />
			<publisher>Hackett Publishing Co</publisher>
			<pubPlace>Indianapolis, IN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Encoding structure in holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030301</idno>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Indirect associations in learning semantic and syntactic lexical relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2020.104153</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">104153</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The memory tesseract: Mathematical equivalence between composite and separate storage memory models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2016.10.006</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="142" to="155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Leveling the field: Talking levels in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brook</surname></persName>
		</author>
		<ptr target="https://cogsci.mindmodeling.org/2016/papers/0415/paper0415.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual meeting of the cognitive science society</title>
		<editor>A. Papafragou, D. Grodner, D. Mirman, &amp; J. C. Trueswell</editor>
		<meeting>the 38th annual meeting of the cognitive science society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2399" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The semantic pictionary project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kievit-Kylar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://mindmodeling.org/cogsci2011/papers/0520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference of the Cognitive Science Society</title>
		<editor>L. Carlson, C. Hoelscher, &amp; T. Shipley</editor>
		<meeting>the 33rd Annual Conference of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2229" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">40 years of cognitive architectures: Core cognitive abilities and practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotseruba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-018-9646-y</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2018-07-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The Soar cognitive architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v38i4.2744</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.104.2.211</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A memory-based account of the prisoner&apos;s dilemma and other 2x2 games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Cognitive Modeling</title>
		<editor>N. Taatgen &amp; J. Aasman</editor>
		<meeting>the Third International Conference on Cognitive Modeling<address><addrLine>Groningen, the Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Universal Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A dynamic ACT-R model of simple games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<ptr target="https://cognitivesciencesociety.org/wp-content/uploads/2019/01/cogsci_21_1999_Vancouver.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty First Annual Meeting of the Cognitive Science Society</title>
		<editor>M. Hahn &amp; S. C. Stoness</editor>
		<meeting>the Twenty First Annual Meeting of the Cognitive Science Society<address><addrLine>Mahwah, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Instance-based learning: A general model of repeated binary choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lejarraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.722</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="153" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frixione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologically Inspired Cognitive Architectures</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, &amp; K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The serial position effect of free recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Murdock</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0045106</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="482" to="488" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A theory for the storage and retrieval of item and associative information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Murdock</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.89.6.609</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="609" to="626" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">You can&apos;t play 20 questions with nature and win</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual information processing</title>
		<editor>W. G. Chase</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evidence against decay in verbal working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lewandowsky</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0029588</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="380" to="411" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Practice and forgetting effects on vocabulary memory: An activation-based model of the spacing effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Pavlik</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog0000\_14</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="586" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.377968</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A computational cognitive model of syntactic priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2010.01165.x</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="587" to="637" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Time-based loss in visual short-term memory is from trace decay, not temporal distinctiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Ricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000018</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1523" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning curve, the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schooler</surname></persName>
		</author>
		<idno type="DOI">10.1016/B0-08-043076-7/01480-7</idno>
	</analytic>
	<monogr>
		<title level="m">International encyclopedia of the social and behavioral sciences</title>
		<editor>N. J. Smelser &amp; P. B. Baltes</editor>
		<meeting><address><addrLine>Oxford: Pergamon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="8602" to="8605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ACT-R: A cognitive architecture for modeling cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tehranchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Oury</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1488</idno>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The Sigma cognitive architecture and system: Towards functionally elegant grand unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ustun</surname></persName>
		</author>
		<idno type="DOI">10.1515/jagi-2016-0001</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial General Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dynamically structured holographic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Rutledge-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Pyke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bica.2014.06.001</idno>
	</analytic>
	<monogr>
		<title level="j">Biologically Inspired Cognitive Architectures</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="9" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A holographic associative memory recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Rutledge-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vellino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDIM.2008.4746700</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Digital Information Management</title>
		<meeting>the 3rd International Conference on Digital Information Management<address><addrLine>London, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Achieving efficient and cognitively plausible learning in backgammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lovett</surname></persName>
		</author>
		<idno type="DOI">10.1184/R1/6613298.v1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning (ICML-2000</title>
		<meeting>the Seventeenth International Conference on Machine Learning (ICML-2000<address><addrLine>Stanford, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07" />
			<biblScope unit="page" from="823" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snaider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2014.11.103</idno>
	</analytic>
	<monogr>
		<title level="m">5th Annual International Conference on Biologically Inspired Cognitive Architectures</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="188" to="203" />
		</imprint>
	</monogr>
	<note>Vector LIDA. Procedia Computer Science</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Refining the common model of cognition through large neuroscience data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Steine-Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stocco</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.11.026</idno>
	</analytic>
	<monogr>
		<title level="m">Papers on the Common Model of Cognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="813" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to select actions with spiking neurons in the basal ganglia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2012.00002</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deconstructing and reconstructing ACT-R: Exploring the architectural space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogsys.2007.06.006</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Empirical evidence from neuroimaging data for a standard model of the mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosenbloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Conference of the</title>
		<editor>T. T. Rogers, M. Rau, X. Zhu, &amp; C. W. Kalish</editor>
		<meeting>the 40th Annual Conference of the</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title/>
		<ptr target="https://mindmodeling.org/cogsci2018/papers/0216/" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science Society</title>
		<imprint>
			<biblScope unit="page" from="1094" to="1099" />
		</imprint>
	</monogr>
	<note>Cognitive Science Society</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.90.4.293</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="315" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning from delayed feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13415-011-0027-0</idno>
	</analytic>
	<monogr>
		<title level="m">Neural responses in temporal credit assignment. Cognitive, Affective, and Behavioral Neuroscience</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Context effects produced by question orders reveal quantum nature of human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Solloway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Shiffrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1407756111</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="9431" to="9436" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
