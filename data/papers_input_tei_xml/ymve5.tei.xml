<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot compositional reasoning in a reinforcement learning setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><forename type="middle">K</forename><surname>Jagadish</surname></persName>
							<email>akshay.jagadish@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<orgName type="institution">MPRG Computational Principles of Intelligence</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Human-Centered AI</orgName>
								<orgName type="institution">Helmholtz Computational Health Center</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Binz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<orgName type="institution">MPRG Computational Principles of Intelligence</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Human-Centered AI</orgName>
								<orgName type="institution">Helmholtz Computational Health Center</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tankred</forename><surname>Saanum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<orgName type="institution">MPRG Computational Principles of Intelligence</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Deepmind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Schulz</surname></persName>
							<email>eric.schulz@helmholtz-munich.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
								<orgName type="institution">MPRG Computational Principles of Intelligence</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Human-Centered AI</orgName>
								<orgName type="institution">Helmholtz Computational Health Center</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot compositional reasoning in a reinforcement learning setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>People can easily evoke previously learned concepts, compose them, and apply the result to solve novel tasks on the first attempt. The aim of this paper is to improve our understanding of how people make such zero-shot compositional inferences in a reinforcement learning setting. To achieve this, we introduce an experimental paradigm where people learn two latent reward functions and need to compose them correctly to solve a novel task. We find that people have the capability to engage in zero-shot compositional reinforcement learning but deviate systematically from optimality. However, their mistakes are structured and can be explained by their performance in the sub-tasks leading up to the composition. Through extensive model-based analyses, we found that a meta-learned neural network model that accounts for limited computational resources best captures participants&apos; behaviour. Moreover, the amount of computational resources this model identified reliably quantifies how good individual participants are at zero-shot compositional reasoning. Taken together, our work takes a considerable step towards studying compositional reasoning in agents-both natural and artificial-with limited computational resources. Trials 1-5 Linear sub-task Trials 6-10 Periodic sub-task Experiment 1 Experiment 2 Trials 11-15 Time Latent reward functions Linear functions Periodic functions Add Change-point Sample reward functions Results To investigate compositional reinforcement learning in humans, we developed a novel multi-armed bandit paradigm based on previous works 20, 29, 30 as illustrated in Fig. 1. Each task consists of three multi-armed bandit sub-tasks in which rewards follow a latent function that is dependent on the spatial position of the arms. The reward functions for the first two sub-tasks are sampled from either the linear or the periodic family of functions. In the final sub-task, reward functions are constructed by composing the reward functions encountered in the two earlier sub-tasks. We conducted two experiments with different 2/18</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>People have an impressive ability to learn from sparse data <ref type="bibr" target="#b0">1</ref> . We can acquire a new word from only one encounter <ref type="bibr" target="#b1">2</ref> . We can achieve near-perfect classification rates from only one labelled observation <ref type="bibr" target="#b2">3</ref> . We can even ask questions such as "how likely is it that a newly invented machine could transform a man into a vase?" <ref type="bibr" target="#b3">4</ref> , even though we are unlikely to ever encounter such a machine. Many researchers have proposed that the ability to generalize from sparse data is a hallmark of human intelligence <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> .</p><p>What are the mechanisms that underlie this ability? One mechanism that enables strong generalizations is compositionality <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> , which is the idea that complex entities can be constructed through the combination of primitive elements. People are generally considered to excel at reasoning compositionally <ref type="bibr" target="#b10">11</ref> . They can, for example, combine parts of objects into novel objects <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12</ref> or compose previously learned actions to explore in novel contexts <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13</ref> . It has thus been argued that compositionality equips us with the ability to "make infinite use of finite means" <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> , allowing us to generalize to novel situations by reusing and combining past experiences <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> .</p><p>Empirical studies have demonstrated that people have an inherent predisposition towards compositional patterns <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> . For example, utilizing the function learning paradigm, which involves the learning, completion, and prediction of functional patterns, Schulz and colleagues <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> have demonstrated that humans find it easier to learn about compositional than non-compositional patterns. Furthermore, they showed that humans exhibit superior abilities to complete and predict compositional functions, as well as an enhanced capacity for remembering such functions <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> . These findings extend beyond function learning to other domains such as spatial structure learning <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24</ref> , concept learning <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25</ref> , shape perception <ref type="bibr" target="#b25">26</ref> , and auditory sequence learning <ref type="bibr" target="#b26">27</ref> . Taken together, there is strong evidence for the presence of compositional inductive biases in humans.</p><p>While the preference for compositional patterns has received significant attention, how people compose two already learned functions and act on them in a zero-shot manner remains less well-understood. We attempt to close this gap in the present paper by studying human compositional reasoning in a reinforcement learning setting. More specifically, we are interested in how people apply a compositional rule instructed to them on learned latent reward functions. To study this question, we propose a novel experimental paradigm in which people interact with a sequence of three structured multi-armed bandit tasks <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> as illustrated in <ref type="figure">Fig. 1</ref>. The rewards for the first two sub-tasks are sampled from differently structured functions. They are followed by a third sub-task in which the rewards are set to a composition of the previously encountered functions. The structure of our <ref type="figure">Figure 1</ref>. Overview of our experimental paradigm. Each task consists of three multi-armed bandit sub-tasks, with participants performing five trials per sub-task. Each multi-armed bandit task has six arms, each corresponding to different letters on the keyboard (starting from S to L). Rewards for the arms in our bandit tasks follow a latent function that is dependent on the spatial position of the arms. The reward functions for the first two sub-tasks are sampled from either the linear or the periodic family. The rewards for the final sub-task are constructed by composing the reward functions sampled in the two earlier sub-tasks. We conducted experiments with two different composition rules: an additive rule (experiment 1) and a change-point rule (experiment 2). In experiment 1, rewards for the final sub-task are constructed by performing an element-wise summation of sampled rewards from the first two sub-tasks. Whereas in experiment 2, they are constructed by combining segments of the sampled rewards such that the rewards change from being from one family to another after a certain number of options. Specifically, rewards in the first segment (i.e. options S-D-F) come from one family -either a linear or periodic family -and the alternative family in the second segment (i.e. options J-K-L). Note that the order in which these reward segments are combined in the change-point rule is randomized. If participants learn the latent reward functions in the first two sub-tasks and apply the compositional rule correctly, they can -in principle -perform zero-shot compositional inference, meaning that they choose the optimal arm (arm D in the example shown for experiment 1 and arm S in the example shown for experiment 2) on the first trial of the last sub-task.</p><p>task induces a learning curriculum that allows participants to rapidly solve the final sub-task in a zero-shot manner -assuming that they apply the instructed compositional rule.</p><p>In two experimental studies, we find that people can learn the underlying latent functions via reinforcement learning and apply the compositional rule over the learned functions in zero-shot. They do so rapidly right from the first task onward, a propensity studied under the paradigm of rapid instructed task learning <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33</ref> . However, our analyses also indicate that their behaviour deviates systematically from a fully-normative account. Extensive model-based analyses furthermore reveal that human compositional reasoning is overall best explained by a resource-rational account <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> . Taken together, our results suggest that people can make zero-shot compositional inferences but that their performance is constrained by cognitive demands. composition rules: an additive rule and a change-point rule. Our task induces a learning curriculum, which enables us to probe whether people are able to reason compositionally in a reinforcement learning setting. In particular, we expect people to solve the final sub-task in a zero-shot manner, selecting the best option on the first trial. To have a comparison, we also consider a condition without a curriculum. In this non-curriculum condition, people do not interact with the first two sub-tasks and instead directly observe the composite function from the final sub-task. We set the length of each sub-task to five, leading to 15 trials per task in the curriculum and five trials per task in the non-curriculum condition. Note that the number of trials per sub-task is less than the number of available options, thereby preventing participants from exhaustively trying out all options and forcing them to generalize based on the underlying function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Additive rule</head><p>We conducted an online behavioural study, on the Prolific platform, following the structure of the just outlined task to test the underlying mechanisms behind how people compose. Participants played a game under a cover story that they were interacting with slot machines produced by two manufacturers (Blue Lagoon and Green Geeks). They were told that all slot machines from the same manufacturer behaved similarly. However, participants were not told which manufacturer each slot machine belonged to, but had to figure this out through trial and error. In the curriculum condition, participants played with a slot machine from each manufacturer before playing a compositional slot machine that combined the two. In the non-curriculum condition, participants only played with the compositional slot machine. The study involved 20 tasks per participant, leading to 300 trials in total for the curriculum condition and 100 for the non-curriculum condition. We provide further details about the experiment and participants in the Materials and Methods section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioural analysis</head><p>First, we wanted to establish that people successfully composed reward functions in our task. For the corresponding analyses, we considered two behavioural measures: regrets and the probability of making optimal choices. <ref type="figure" target="#fig_0">Fig. 2(a)</ref> shows the mean regret of participants in the compositional sub-task. The regret is computed by taking the difference between the highest reward in the given sub-task and the reward for the action selected by the participant. Participants are said to have successfully performed zero-shot compositional inference if they choose the optimal arm on the first trial of the compositional sub-task (or have a regret measure of 0 on the first trial). We see from the regrets that people performed better than chance right from the outset for the curriculum condition (Mean (M) = 2.163, Standard Error (SE) = 0.116; t 1 = −19.57, p &lt; 0.001) whereas they start at chance-level for the non-curriculum condition (M = 4.055, SE = 0.059). To further quantify the effects of zero-shot compositional inference, we performed a mixed-effects linear regression on the regrets in the final sub-task with trials, conditions, and their interaction as fixed effects (with random slopes and intercepts per participant for all of these factors). This analysis revealed that participants in the curriculum condition had a significantly lower regret on the first trial of the final sub-task than participants in the non-curriculum condition (β = −1.18 ± 0.115; z = −10.24, p &lt; 0.001). In addition, a comparison of the probability for making an optimal choice on the first trial between curriculum and non-curriculum conditions, shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, confirmed that people in the curriculum condition (M = 0.382, SE = 0.02) made optimal choices more frequently than in the non-curriculum condition (M = 0.192, SE = 0.07; t = −9.242, p &lt; 0.001). Regret performance on the first trial of the curriculum condition was even better than performance on the last trial of the non-curriculum condition (M = 2.40, SE = 0.13; t = 2.72, p &lt; 0.01), suggesting that learning within the last sub-task cannot match the performance boost gained from compositional inference.</p><p>While people were able to compose in a zero-shot manner, they did not do so perfectly. Their initial regrets in the final sub-task (M = 2.163, SE = 0.116; t = 34.60, p &lt; 0.001) deviated significantly from ideal compositional reasoning. Further evidence of people's suboptimality comes from the observation that they continued learning during the final sub-task in the curriculum condition (which would not be needed if they were to engage in perfect zero-shot compositional inference). To quantify this effect, we fitted a mixed-effects linear regression model using per-trial regret in the last sub-tasks as the dependent variable, and the corresponding trial number as both fixed effects and random effects over participants. The results of this model showed a significant fixed effect of trial number (β = −0.32 ± 0.02; z = −13.88, p &lt; 0.001) onto regret, confirming that the performance of participants improved with additional interactions. The observed improvement in the curriculum condition (β = −0.21 ± 0.02; z = −12.26, p &lt; 0.001) was generally weaker than that in the non-curriculum condition (β = −0.42 ± 0.02; z = −21.80, p &lt; 0.001).</p><p>We also inspected the marginal action distribution of participants on the first trial of the final sub-task shown in <ref type="figure" target="#fig_0">Fig.  2(c)</ref>. We see that the mode of the participants' action distribution matches the optimal choice, but that human behaviour also systematically deviates from optimal behaviour. Particularly, one interesting feature is that people seem to pick corner armsespecially the left-most one -frequently. This could reflect a bias that has been observed in other studies of people exploring different options starting from left to right <ref type="bibr" target="#b28">29</ref> . However, probability for making an optimal choice on the first trial was still Marginal distribution of choices on the first trial of the final sub-task for the two conditions. The bar in gold shows the marginal distribution of choices for the optimal policy. (d) Explaining choices of participants in the last sub-task based on their task performance in the first two sub-tasks. We classified the choices on the first trial into four categories: optimal arm (top-left), non-optimal corner arms (top-right), non-optimal phasic arm (bottom-left), and neither of the above (bottom-right). Then, we fit a Bayesian logistic regression model from the total regrets (summed over all trials) in the first two sub-tasks onto each of these categories. The sub-plots show the histogram of the posterior regression coefficients of linear and periodic sub-tasks for all four choice categories.</p><p>better in curriculum condition than in the non-curriculum condition even when only compositions where non-corner arms were optimal was considered (for further details, see SI). This indicates that despite their bias towards choosing the corner arms, participants are still able to perform zero-shot compositional inference.</p><p>To better understand the mistakes that people make during compositional reasoning, we looked at how participants' performance in the first two sub-tasks can explain their behaviour on the final compositional sub-task. We first classified choices on the first trial of the compositional sub-task into four categories: first, picking the optimal arm as predicted by compositional inference; second, a non-optimal corner arms category which includes trials where people picked the corner arms despite them not being the optimal choice; third, a non-optimal phasic arms category which includes trials where arms belonging to the same phase as the periodic sub-task were picked even when it was not the optimal choice; and fourth, a category which includes all trials where choices did not fall into any of the three categories mentioned above. We then fitted a separate Bayesian logistic regression model in PYMC3 from the total regrets (summed over all trials) in the first two sub-tasks onto each of these four choice categories coded as a binary variable. <ref type="figure" target="#fig_0">Fig. 2(d)</ref> visualized the posterior regression coefficients from these fitted models with each category shown in a separate sub-plot. When the dependent variable was picking the optimal choice, the posterior regression coefficients were negative with similar means for both linear (M = −0.038, SE = 7.319e − 05) and periodic (M = −0.035, SE = 4.925e − 05; t = −40.728, p &lt; 0.001) sub-tasks. This suggests that participants pick the optimal arm when they learn both the sub-tasks well. The coefficients for regrets of the linear sub-tasks were more negative (M = −0.089, SE = 8.107e − 05) than that of periodic regrets (M = −0.015, SE = 5.040e − 05; t = −779.036, p &lt; 0.001) when the dependent variable was non-optimal corner arms. This result suggests that people tend to pick non-optimal corners arms when they learned linear functions better than periodic functions. When the dependent variable was non-optimal phasic arms, the posterior regression coefficients for regrets from the periodic sub-task were lower (M = −0.0352, SE = 0.477e − 04) than that of the linear sub-task (M = −0.0048, SE = 7.0762e − 05; t = 356.072, p &lt; 0.001). This result indicates that people pick one of the phasic arms from the periodic sub-task on the first trial of the compositional sub-task when they perform better in the periodic sub-task. Lastly, the regression coefficients were positive for both the regrets from the linear sub-task (M = 0.1498, SE = 1.167e − 05; t = 450.07, p &lt; 0.001) and those from the periodic sub-task (M = 0.0874, SE = 7.502e − 05) when the dependent variable was neither of the categories above. This result suggests that people pick neither the optimal, the corner nor the phasic arm when they have not learned the underlying functions in either of the two sub-tasks well.</p><p>Taken together, behavioural results from experiment 1 suggest that people can compose in a zero-shot fashion but are not perfect. However, their mistakes are highly structured and can be predicted based on how well they have learned the different components of the first two sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-based analysis</head><p>In our compositional bandit task, people can -in principle -perform near-perfect if they manage to compose. However, the behavioural analysis above revealed that people (despite generally managing to compose) systematically deviated from optimal behaviour. To get a better understanding of these deviations and the cognitive processes behind them, we investigated people's behaviour using computational models.</p><p>We considered eight different computational models for explaining participants' choices in our task as summarised in <ref type="table" target="#tab_0">Table  1</ref>. The models considered span five key axes in terms of the strategies people might be using: (1) from learning values for options independently to generalisation across options (BMT-based models versus others), (2) from value-driven learning to uncertainty-guided exploration (mean-only models versus uncertainty guided exploration), (3) from learning sub-tasks independently to composing learned reward functions over sub-tasks (BMT and GPR versus their compositional variants), (4) from hand-designed priors to learned priors (all other models versus RL 2 and RR-RL 2 ) and (5) from unbounded compositional reasoning to resource-constrained reasoning (RL 2 versus RR-RL 2 ).</p><p>Six of these are Bayesian models that vary along three dimensions: first, whether or not they can generalize learned values from one option to the other, second, whether or not they use uncertainty-guided exploration strategy, and third, whether or not they can compose the learned values from the first two sub-task to reason on the final sub-task. The Bayesian models include a Bayesian mean-tracker (BMT) <ref type="bibr" target="#b35">36</ref> which is a model that does not learn about the underlying functional structure but instead updates its beliefs about rewards for each option independently, as well as a model that learns functions by generalizing across options within a sub-task based on the idea of Gaussian Process regression (GPR) <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> . For each of these two models, we considered one variant that cannot compose and instead learns separate reward functions for each sub-task, another that does not perform uncertainty-guided exploration, and lastly, one that initializes its predictions in the final sub-task to the composition of the learned means from the first two sub-tasks.</p><p>In addition, we also considered two recurrent neural network models that were trained via meta-reinforcement learning <ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40</ref> . Unlike the Bayesian models from above, these models learn inductive biases about latent reward functions via trial-and-error, without requiring an explicit specification of priors <ref type="bibr" target="#b40">41</ref> . The first of these models is RL 2 -a model that is known to approximate the Bayes-optimal policy for the distribution of tasks it was trained on <ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43</ref> , which thereby allows us to test whether people compose optimally. The second is a resource-rational extension of RL 2 referred to as RR-RL 244 . The particular resource constraint considered by RR-RL 2 is the description length of the meta-learned recurrent neural network, which is defined as the number of bits required to store its parameters. RR-RL 2 captures the hypothesis that people attempt to achieve optimal performance but that they are subject to the constraint of relying on an algorithm with limited computational complexity. We fitted RR-RL 2 's description length on a participant-by-participant basis reflecting the assumption that different participants use different amounts of computational resources.</p><p>We simulated all models on our compositional bandit task and measured their performance in the final sub-task. We found that performance on the first trial was near-optimal for the five models that can compose (the compositional BMT and GPR, as well as the two meta-learning agents), indicating that they can re-use the earlier learned functions to compose new functions in </p><formula xml:id="formula_0">✗ ✗ ✗ ✗ ✗ Bayesian Mean-Tracker ✗ ✓ ✗ ✗ ✗ Compositional BMT (mean only) ✗ ✗ ✓ ✗ ✗ Compositional BMT ✗ ✓ ✓ ✗ ✗ Gaussian Process Regression ✓ ✓ ✗ ✗ ✗ Compositional GPR ✓ ✓ ✓ ✗ ✗ RL 2 ✓ ✓ ✓ ✓ ✗ RR-RL 2 ✓ ✓ ✓ ✓ ✓</formula><p>a zero-shot manner; for detailed visualizations, see Supplementary Information (SI).</p><p>To obtain a quantitative measure of the goodness-of-fit of models to human choices, we conducted a Bayesian model comparison of all previously outlined models. We measured the fit to human choices based on two metrics: posterior model frequency and exceedance probability <ref type="bibr" target="#b44">45</ref> . The posterior model frequency measures how often a model offers the best explanation in the population, while the exceedance probability measures how likely it is that a given model is the most frequent explanation. Further details about this model comparison procedure can be found in the Materials and Methods section.</p><p>This model comparison revealed that RR-RL 2 captures how people behave on the first trial of the compositional sub-task the best according to both metrics, with exceedance probability amounting to 0.99, while its posterior model frequency was 0.704 ± 0.002. The compositional BMT is the second-best model with 4.33e − 09 and 0.288 ± 0.002 on exceedance and posterior model frequency respectively. Interestingly, we found that the two models that performed best in our model simulations (compositional GPR and RL 2 ) did not predict human behaviour well. Taken together, these results support the hypothesis that people do not compose in a fully optimal way, but that their ability to reason compositionally is driven by principles of resource rationality. We conducted additional model-based analyses to test alternative explanations for sub-optimal zero-shot compositional inference displayed by humans. For instance, we included the corner-arm heuristic model, which picks only the two corner options, and the mean-only variant of the compositional BMT model, which posits people mentally add previously chosen maximally rewarding options. However, the RR-RL 2 model still offered the best explanation for human choices. see the section on "Ruling out alternative hypothesis" in SI for details.</p><p>To further support the model comparison results, we simulated behaviour from the two best-fitting models and compared them against human behaviour. With regards to the probability of making the optimal choice on the first trial, we found  Next, we examined whether the fitted description lengths of RR-RL 2 could capture task performance. To do this, we correlated the probability of humans making the optimal choice on the first trial against the fitted description lengths for each participant. We found that description length correlated significantly with optimality (r = 0.359, p &lt; 0.001) as illustrated in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. Likewise, we also observed a significant negative correlation (r = −0.36, p &lt; 0.001) between fitted description lengths and mean regrets on the first trial.</p><p>We then analysed whether fitted description lengths can be used to explain the types of choices participants make. For this, we grouped participants based on their fitted description lengths into two groups. In the first group, we included participants whose fitted description lengths were in the range of 1000 to 10000 (N = 39), and in the second group, we considered those whose fitted description lengths were in the range of 10 to 100 (N = 17). We then compared the probability of making the optimal choice on the first trial of the final sub-task between the two groups. We found that participants in the first group performed near-perfect composition, whereas the choices from participants in the second group were far away from optimality (for further details, see SI). Thus, fitted description lengths can be used to cluster participants into those who can perform near-optimal zero-shot compositional inference and those who cannot.</p><p>Additionally, we looked at how regrets on the first two sub-tasks influence behaviour on the first trial in RR-RL 2 , just like how we did it in people. We found that the posterior regression coefficients of the model match human behaviour qualitatively with slight deviations. The model picks the non-optimal corner arms and non-optimal phasic arms when it learns one sub-task better than the other (with performance in periodic sub-task having a greater influence on the linear in both cases) and follows a completely different strategy from the ones above when it does not learn both the sub-tasks well. An interesting deviation was that, unlike humans who make more optimal choices when they learn both sub-tasks equally well, the model does so when they learn the linear sub-task better than the periodic sub-task (for detailed results and visualizations, see SI).</p><p>Taken together, results from behavioural and model-based analyses suggest that people can perform zero-shot compositional inference but still deviate systematically from optimal behaviour. Their choices are best explained by a meta-reinforcement learning model (RR-RL 2 ) that learns a solution with limited computational resources. Furthermore, we find the simulated behaviour from RR-RL 2 matches human behaviour well and that description length -the parameter that controls computational resources of RR-RL 2 -correlates with the probability of making an optimal choice on the first trial of the compositional sub-task. <ref type="bibr" target="#b1">2</ref> Experiment 2: Change-point rule Next, we wanted to verify that the results obtained in the previous section transfer to another compositional rule. We, therefore, conducted a second experiment using a change-point rule. The experimental procedure followed the same structure as the additive rule but with participants now being tested on a slot machine whose rewards were composed based on the change-point rule (see <ref type="figure">Fig. 1</ref> for an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioural analysis</head><p>Like in the additive rule experiment, we see that people perform much better than chance right from the outset for the curriculum condition (M = 1.937, SE = 0.081; t = −15.105, p &lt; 0.001) while starting at chance-level for the non-curriculum condition (M = 3.096, SE = 0.060) as shown in <ref type="figure">Fig. 4(a)</ref>. We also performed a mixed-effects linear regression analysis as we did in the first experiment which confirmed that participants in the curriculum condition had a significantly lower regret on the first trial of the final sub-task than participants in the non-curriculum condition (β = −1.18 ± 0.115; z = −10.24, p &lt; 0.001). When looking at the probability of picking the optimal arm in <ref type="figure">Fig. 4(b)</ref>, we also find that people make better choices in the curriculum condition (M = 0.337, SE = 0.016) than in the non-curriculum condition (M = 0.168, SE = 0.008, t = −9.347, p &lt; 0.001). The performance in the curriculum condition is better than in the non-curriculum condition for all trials, which was also the case in the additive rule. Thus, similar to the additive rule experiment, people are able to perform approximate zero-shot compositional inferences.</p><p>Even though people were able to compose in a zero-shot manner, they were again not flawless. Their initial regrets in the final sub-task (M = 1.937, SE = 0.081; t = 38.25, p &lt; 0.001) deviated significantly from ideal compositional reasoning, thereby corroborating our results from the previous experiment. In addition, people's suboptimality was underlined by the persistent presence of learning effects in the curriculum condition (β = −0.216 ± 0.013; z = −16.112p &lt; 0.001).</p><p>We also inspected participants' marginal action distribution on the final sub-tasks first trial in <ref type="figure">Fig. 4(c)</ref>. We see that the mode of the participants' action distribution lies at the optimal choice. However, human behaviour systematically deviates from optimal behaviour as people pick sub-optimal options frequently. People also tend to pick the corner options -especially the left-most option -quite frequently as they did in the additive rule.</p><p>Finally, we repeated the regret analysis we did for the additive rule to better understand which kind of mistakes people make. The results of this analysis are summarized in <ref type="figure">Fig. 4(d)</ref>. We see that posterior regression coefficients for regrets of both linear (M = −0.0423, SE = 7.518e − 05) and periodic (M = −0.0464, SE = 5.038e − 05) sub-tasks are negative and have overlapping distributions in cases where people picked the optimal option (t = 45.874, p &lt; 0.001). This suggests that learning both linear and periodic sub-tasks equally well predicts good performance on the first trial. When people pick non-optimal corner arms, their performance in the linear sub-task (M = −0.0607, SE = 7.663e − 05) seems to be driving their behaviour more than their performance in the periodic sub-task (M = −0.0305, SE = 5.0042e − 05). However, on the contrary, picking the non-optimal phasic arm is not driven strongly by periodic sub-task performance with regression coefficients for both linear (M = −0.0259, SE = 7.0662e − 05) and periodic (M = −0.0203, SE = 4.672e − 05) sub-tasks overlapping (t = −66.174, p &lt; 0.001). Lastly, the posterior regression coefficients are distributed on the positive axis for both linear (M = 0.114, SE = 9.796e − 05) and periodic (M = 0.0747, SE = 6.368e − 05) regrets when predicting non-optimal choice belonging to neither to corner or phasic arms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-based analysis</head><p>The results for model-based analyses with the change-point rule mirrored that of the additive rule. We find again that RR-RL 2 captures best how people compose according to both metrics with exceedance probability amounting to 0.99, while its posterior model frequency was 0.741 ± 1.729e − 03. The compositional BMT is the second-best model with an exceedance probability of close to 0 and a posterior model frequency of 0.253 ± 1.702e − 03 as visualized in <ref type="figure">Fig. 5(a)</ref>. These results thus show again that people do not compose in a fully optimal way, but that their ability to reason compositionally is instead impeded by computational constraints.</p><p>We simulated behaviour from the two best-fitting models with their parameters fitted to participant behaviour. Looking at the probability of selecting the optimal choice, we found that simulations from RR-RL 2 (M = 0.2922, SE = 0.0132) matched human behaviour (M = 0.3372, SE = 0.0158) more closely than the compositional BMT (M = 0.2862, SE = 0.0069; t = 2.9560, p &lt; 0.01) as shown in <ref type="figure">Fig. 5(b)</ref>.</p><p>We found that fitted description lengths correlated significantly with the probability of participants making an optimal choice for the change-point rule as well, showing a correlation coefficient of r = 0.487 (p &lt; 0.001) as shown in <ref type="figure">Fig. 5 (c)</ref>. The result also holds when we use regrets as a performance measure (r = −0.448, p &lt; 0.001).</p><p>Following our earlier analysis, we grouped the participants based on their fitted description lengths into two groups with the first group including participants with fitted description lengths between 1000 to 10000 (N = 58) and the second group including participants with fitted description lengths between 10 to 100 (N = 25). We compared zero-shot compositional inference between the two groups (for detailed analysis, see SI) and found again that participants in the first group performed near-perfect zero-shot compositional inferences, whereas the choices from participants in the second group were far away from optimality.</p><p>Finally, we inspected how performance on the first two sub-tasks influenced behaviour on the first trial of the compositional sub-task in RR-RL <ref type="bibr" target="#b1">2</ref> . We found that overall the posterior regression coefficients of the model's behaviour in linear and periodic sub-tasks match human behaviour quite well. They pick the non-optimal corner arms and non-optimal phasic arms when they learn one sub-task better than the other and they follow a completely different strategy from the ones above when they do not learn both the sub-tasks well. However, as in experiment 1 but to a smaller extent, the model makes optimal choices when they learn the linear sub-task better than the periodic sub-task. The results of these analyses are summarised in the SI.</p><p>Taken together, these results mirror those we had for the additive rule with similar factors affecting human behaviour in the compositional sub-task. This suggests that people's ability to do compositional inference is robust with regard to the specific way in which functions are composed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Compositionality is at the core of people's ability to generalize from sparse data. It has even been argued to be an essential component of intelligence more generally <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11</ref> . However, how people use this ability to make decisions is less well understood. To address this question, we have proposed a novel experimental paradigm where people first need to learn about latent reward functions in two sub-tasks to be able to pick the most rewarding option on the first trial of the third sub-task. We found that people indeed perform this kind of zero-shot compositional inference, but they deviate systematically from ideal behaviour. Even so, their mistakes were not random but instead highly structured. Extensive model-based analyses revealed that RR-RL 2 -a meta-learned neural network model that accounts for limited computational resources -captures participants' behaviour the best. Mistakes made by this model were also systematic and predicted by similar factors that predicted human suboptimal choices. This result indicates that people seem to follow resource-rational principles when making compositional inferences, thereby expanding on earlier results from other cognitive domains such as decision-making <ref type="bibr" target="#b45">46</ref> , planning <ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48</ref> , and problem-solving <ref type="bibr" target="#b48">49</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to empirical works in compositional reinforcement learning</head><p>Previous work has investigated how people structure prior knowledge and compose specific components of this learned knowledge to generalize efficiently in reinforcement learning tasks. Xia and Collins <ref type="bibr" target="#b12">13</ref> have used the options framework from hierarchical reinforcement learning to show that humans can learn hierarchical options and do so such that the temporal ordering of the learned options remains intact. They further showed that people, akin to their model, can compose the learned options to explore novel contexts, thereby speeding up learning. Looking into how learned knowledge guides generalisation, Franklin and Frank 8 showed that human learners decompose learned task structures into distinct components such as rewards and state transitions. They devised a meta-learning agent <ref type="bibr" target="#b49">50</ref> that trades off re-using these components jointly or compositionally and showed that, similar to this agent, humans too act adaptively on these components depending on the statistics of the task environment. In comparison to our work, these previous studies focused on how people generalize in a sample-efficient way to novel tasks and not on zero-shot compositional inference. For example, Collins and colleagues <ref type="bibr" target="#b9">10</ref> showed that subjects can transfer previously learned task clusters to a novel context on the first trial, they did not exclusively test for zero-shot compositional inference of previously learned latent concepts as we do. Therefore, our work complements these earlier investigations by addressing a distinct dimension of compositional reasoning. It is worth noting that adapting the proposed models to our experimental paradigm is not straightforward. However, our compositional GPR shares a similar flavour to the hierarchical models proposed by Xia and Collins, indicating potential conceptual connections between different approaches to compositional reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model complexity and compositional inference</head><p>The relationship between description length and human performance has received considerable attention in previous research <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> . However, researchers have typically investigated how the description length of the task influences performance. For instance, Amalric and colleagues <ref type="bibr" target="#b23">24</ref> asked participants to predict and repeat sequences displayed on a clock-like display, varying the complexity of the sequences by changing the length of the generating program. They found a correlation between the difficulty of predicting (and repeating) a given sequence and its complexity. In contrast, we investigated how the description length of strategies applied by individual participants influenced their performance. We found that different participants apply strategies with different description lengths, implying that they use varying amounts of cognitive resources to perform the task. Thus, task performance is influenced not only by external factors such as task difficulty but also by internal factors such as individual differences in cognitive resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural networks and compositionality</head><p>In contrast to the prevailing notion that neural network models struggle with compositional reasoning <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52</ref> , we found that the model that captured human behaviour best in our experiment was a neural network model, suggesting that these models are not inherently unable to reason compositionally. Instead, it matters how they are set up and how they are trained. This result is supported by other recent studies demonstrating that neural network models can be good models of human compositionally <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref> .</p><p>For example, Lake <ref type="bibr" target="#b52">53</ref> demonstrated that meta-learning can be used to train sequence-to-sequence networks that generalize compositionally in human-like ways on the SCAN data set <ref type="bibr" target="#b50">51</ref> , while Kumar and colleagues <ref type="bibr" target="#b53">54</ref> found that augmenting metareinforcement learning agents with an auxiliary objective to reproduce task descriptions aligns them with human behaviour in a setting that requires reasoning about compositionally-generated patterns. Finally, Dekker and colleagues <ref type="bibr" target="#b54">55</ref> designed neural network architecture with an inductive bias for compositional reasoning using a Hebbian gating process, and demonstrated that the resulting model learns composable functions similar to how these functions are learned by people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Possible neural and cognitive processes</head><p>To perform optimal zero-shot compositional reasoning in our task, one must learn the latent reward functions, hold them in working memory until the end of the first two subtasks, and then reason compositionally over these learned latent functions. It is hypothesized that the basal ganglia-striatal circuits, along with the hippocampus, medial temporal cortex, and ventral prefrontal cortex, may be involved in the spatial representation and learning of the latent reward functions underlying the options <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> . Maintaining a representation of acquired potential reward functions in memory can involve working memory via prefrontal cortex <ref type="bibr" target="#b60">61</ref> . In a recent review article, Frankland and colleagues <ref type="bibr" target="#b5">6</ref> have suggested that -across multiple studies -the default mode network, particularly, the lateral mid-anterior temporal cortex, is engaged in compositional reasoning of learned structures. This suggestion is consistent with previous research that has implicated fronto-cortical and striatal interactions in compositional generalization <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">62</ref> . However, additional research is still necessary to investigate the neural processes that can connect function learning, the maintenance of learned functions in working memory, and their composition. It would be interesting to connect our computational models, in particular RR-RL 2 , to some of these results from the neuroscience literature in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While one might argue that the proposed compositional bandit task is too simplistic, we found that human behaviour systematically deviated from optimality, implying that the task complexity was appropriate for the investigated research question. Furthermore, our task has two main advantages compared to those previously used to study compositional reasoning. First, it is directly inspired by experiments used to study human learning in structured environments <ref type="bibr" target="#b28">29</ref> , allowing us to connect our findings to previous work on human cognition. The second advantage is its simplistic design. This simplicity allowed us to build computational models that solve the task near-optimally, picking the best option in a zero-shot fashion. Nevertheless, it might be interesting to develop more naturalistic compositional reasoning tasks in future work to test if our model-based predictions still hold. There are additional variants of our paradigm that could be considered. For example, one could test whether increasing the length of the first two sub-tasks causes people to make better compositional inferences. People's performance could furthermore be boosted by relying on a purely observational setting in which options in the first two sub-tasks are presented in a structured manner (for example from left to right).</p><p>There are several reasons why participants might be deviating from selecting the optimal arm in the third sub-task. We have investigated factors such as the performance on the first two sub-tasks, learning the values of the options independently without any generalisation, the inability to learn the true underlying basis functions, the inability to explore and compose, and the computational resources to learn in the main text. We furthermore considered several additional explanations such as the corner arm heuristic, mentally adding maximal rewards, etc, in the "Ruling out alternative hypothesis" section placed in the SI. However, there are still factors, for example, forgetting values for certain options, and persisting with function learning strategies from the first two sub-tasks, that could not be taken into account which we leave for future work.</p><p>We also note three shortcomings on the modelling side. First, we did not consider resource-rational Bayesian models in our model-based analyses, as building such models is not straightforward. In contrast to this, limited resources are easy to account for in the meta-learning setting <ref type="bibr" target="#b42">43</ref> which is why we relied on such models instead. Second, our models assume each task to be independent of each other. Learning-to-learn effects could be incorporated into both the Bayesian and the meta-learned models. For the Bayesian models, one could build on the work of Schulz and colleagues <ref type="bibr" target="#b28">29</ref> who used a simple clustering algorithm to capture the learning-to-learn behaviour across tasks. For meta-learning agents, it would in principle be possible to train over samples of entire experiments (i.e., 20 successive tasks) where each experiment is sampled from a parameterized distribution of experiments. However, training such agents is challenging in practice especially since gradients need to be propagated over longer horizons. Third, the distribution of problems used to train the meta-learning agents in this work does not reflect the statistics of the real world. Recently, Jagadish and colleagues <ref type="bibr" target="#b62">63</ref> have shown that injecting ecologically valid priors into these agents enables them to explain human category learning better than seven other cognitive models. In future work, it is worth investigating how to design reinforcement learning tasks that capture real-world reward statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduced a novel experimental paradigm and two complementary computational approaches for studying zero-shot compositional reinforcement learning in people. We showed that while people can perform zero-shot compositional inference in our task, their choices were better explained by a resource-constrained model than by optimal zero-shot compositional similarly (i.e. rewards are sampled from the same underlying function, Equation 2 or 3), but were not told which reward function belonged to which company. They had to figure this out via trial and error during the experiment. However, they were shown two canonical examples from each reward function in the task instruction phase to get an idea of how samples from these reward functions could look like. In each casino, participants had five trials per slot machine, with the goal of winning as many coins as possible. In the curriculum condition, they would first interact with a slot machine from each of the two manufacturers. Following their interactions with the two slot machines, participants were tested on a new slot machine, which was a composition of the two previously played machines. Thus, in the curriculum condition, participants interacted with bandits for a total of 300 trials (breakup: 5 trials per sub-task × 3 sub-tasks × 20 tasks). Participants assigned to the non-curriculum condition followed the same task structure as the curriculum condition but with minor changes. In this condition, participants were told that the manufacturers only allowed them to play against the compositional slot machine. As a result, participants only interacted with one slot machine with rewards coming from the additive composition which results in a total of 100 trials (breakup: 5 trials per sub-task × 1 sub-task × 20 tasks)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational models</head><p>In this section, we describe the models that can perform the task with each model making a different assumption on how people could be approaching our task. A complete description of the models can be found in the SI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian models</head><p>Under Gaussian assumptions, a Bayesian Mean-Tracker is often used to track a time-varying reward function f t (a) for option a on trial t. The mean is assumed to change over trials according to a Gaussian random walk:</p><formula xml:id="formula_1">f t+1 (a) ∼ N f t (a), σ 2 ζ (6)</formula><p>where σ ζ = 0.001. We also considered a variant of BMT model that can compose learned rewards called compositional BMT. This model follows the same setup as BMT but has its prior mean for the last sub-task initialized to the composition of learned means from the first two sub-tasks. We provide additional details about model learning and inference in the SI.</p><p>Gaussian Process Regression models learn a distribution over functions f (a) defined by a mean function µ(a) and a covariance, or kernel function k(a, a ′ ), where a and a ′ are arms of the bandit. The mean function defines the expected function value, while the covariance function controls the dependence between the function values for different inputs:</p><formula xml:id="formula_2">f (a) ∼ G P(µ(a), k(a, a ′ )) (7) µ(a) = E[ f (a)] (8) k a, a ′ = E ( f (a) − µ(a)) f a ′ − µ a ′<label>(9)</label></formula><p>For the GP-based agent, we considered the GPR model with radial basis function (RBF) as the kernel. k RBF allows the GPR model to generalize its learned value estimates depending on how (spatially) similar the options are to each other.</p><formula xml:id="formula_3">k RBF a, a ′ = exp − 1 2 a − a ′ ⊤ Θ −2 a − a ′<label>(10)</label></formula><p>where Θ is the length scale hyperparameter. Like BMT, we assume that our GPR agent maintains a separate GP for each sub-task and by design, also cannot do any compositional inference. As GPRs with appropriate priors can approximate the true generative model used for sampling the reward distributions in our task. We constructed a GPR model with compositionality built in, called compositional GPR. Such a model can compose the learned reward estimates from the first two sub-tasks and hence, reason compositionally on the third sub-task. We again assume the agent maintains a separate GP for each sub-task. We set the prior mean of the GPs corresponding to the first two sub-tasks to zero. The covariance function for the first sub-task is defined through a linear kernel k linear defined as The means and kernels for the final sub-task are obtained by composing the means and kernels from the first two sub-tasks <ref type="bibr" target="#b63">64</ref> . For the first trial of the additive composition, the kernel is set to the mean of the learned linear and the periodic kernel from the two sub-tasks. The compositional additive kernel k additive is defined as:</p><p>k additive a, a ′ = 0.5(k linear a, a ′ + k periodic a, a ′ )</p><p>While the prior mean for additive composition is set to the mean of the previously learned mean functions from the linear and periodic sub-tasks. For the first trial of the change-point compositions, the kernel entries are set to that of the linear kernel if both arms belong to the linear function, the periodic kernel if both belong to the periodic function, and zero otherwise. The compositional change-point kernel k change-point is defined as:</p><p>k change-point a, a ′ = k linear a, a ′ α linear a, a ′ + k periodic a, a ′ α periodic a, a ′</p><p>where α linear a, a ′ = 1 if a, a ′ ∈ {0, 1, 2} 0 otherwise <ref type="bibr" target="#b14">(15)</ref> α periodic is defined analogously, giving a value of 1 whenever both arms' a, a ′ ∈ {3, 4, 5}. Note that we randomized whether the first three arms would belong to the linear or the periodic function. The prior mean in the change-point composition is set to the means learned in linear and periodic sub-tasks for the corresponding arms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-reinforcement learning</head><p>The version of RL 2 we use consists of a recurrent neural network (RNN) network followed by two linear networks that output a policy and a value estimate respectively <ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40</ref> . We denote the joint vector of parameters of this model with W. The network receives task-relevant observations o t along with the action a t−1 and reward from the previous time step r t−1 as input and outputs a policy π(a t |h t , W) and a value estimate conditioned on the updated hidden state of the RNN. RL 2 is trained on samples from a task distribution p(ω) to find the policy that maximizes the sum of rewards in an episode of finite horizon H. The full objective function being optimized is shown in Equation 16:</p><formula xml:id="formula_6">max W E p(ω) ∏ p(r t ,o t+1 |a t ,ω)π(a t |h t ,W) H ∑ t=1 r t<label>(16)</label></formula><p>The particular resource constraint considered in RR-RL 2 is the description length of the meta-learned RL algorithm, which is defined as the number of bits required to store its parameters. Mathematically, this can be accomplished through a simple modification of Equation 16:</p><formula xml:id="formula_7">max Λ E q(W|Λ)p(ω) ∏ p(r t ,o t+1 |a t ,ω)π(a t |h t ,W) H ∑ t=1 r t (17) s.t. KL[q(W|Λ)∥p(W)] ≤ C</formula><p>RR-RL 2 differs from RL 2 in two important ways. First, it uses a stochastic parameter encoding over neural network weights q(W|Λ) instead of a point estimate. Second, it places a constraint on the Kullback-Leibler (KL) divergence between q(W|Λ) and a prior p(W), effectively limiting the number of bits that are needed to store the network's parameters and therefore the emerging reinforcement learning algorithm <ref type="bibr" target="#b64">65</ref> .</p><p>The network architecture of RL 2 and RR-RL 2 agents consisted of a gated-recurrent unit of size 128 66 followed by two linear layers that map hidden state-to-value function and policy respectively. The model implementations closely followed the implementation of Binz and Schulz <ref type="bibr" target="#b43">44</ref> . We used a variational dropout prior 67 for RR-RL 2 and assumed that the encoding distribution factorizes into a set of independent normal distributions with learnable means and log-variances. Models were trained using a standard actor-critic loss at the end of each episode <ref type="bibr" target="#b67">68</ref> . We used the ADAM optimizer <ref type="bibr" target="#b68">69</ref> with a learning rate of 0.001 and trained for a total of 10 6 episodes with batches of size 32. RR-RL 2 relied on a dual gradient ascent procedure to enforce the constraint on the KL divergence <ref type="bibr" target="#b69">70</ref> . We obtained gradients w.r.t. the parameters of the encoding distribution λ using the reparametrization trick <ref type="bibr" target="#b70">71</ref> . We trained RR-RL 2 with description lengths between 10 and 10000 nats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling fitting and comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian models</head><p>The parameters of the Bayesian models were optimised endogenously for each participant, i.e., parameters of the model are chosen to maximise the likelihood of the data observed so far as in Schulz and colleagues <ref type="bibr" target="#b28">29</ref> . We feed in the choice taken and reward received by participants from the previous trial and predict the expected reward and its uncertainty measure for all six options for the given trial. Note that the predictions are made after each trial conditioned on all data points up to that trial in the given task. The kernel parameters of these models are learned via gradient descent using the ADAM optimiser <ref type="bibr" target="#b68">69</ref> for 100 iterations. The initial prior noise of these models was set to 0.001.</p><p>Following prior work <ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73</ref> , we use a variant of upper confidence bound sampling with an additional stickiness component as an action selection policy for both Bayesian models: z(a t |β , τ, λ ) = β µ(a t ) + τσ (a t ) + λ δ (a t , a t−1 )</p><p>where δ (a t , a t−1 ) takes the value of 1 if a t = a t−1 and 0 otherwise. This formulation includes uncertainty estimates for the learned values as an additional term to guide exploration. It has been shown to capture human behaviour well in function learning tasks <ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b72">73</ref> and also comes with performance guarantees <ref type="bibr" target="#b73">74</ref> . The policy p Bayesian (a t |β , τ, λ ) is then derived from these values using the softmax function:</p><p>p Bayesian (i) = e z(a t )</p><p>∑ K j=1 e z(a t ) f or a t = 0, 1, . . . , 5</p><p>The free parameters β , τ, and λ were fitted to human choices using a Bayesian model fitting procedure for each participant separately with the priors for parameters set to N (0, 5). Model fitting was performed using the probabilistic programming toolbox PYMC3 <ref type="bibr" target="#b74">75</ref> . We used the marginal likelihood on the first trial of the compositional sub-task for model comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-reinforcement learning</head><p>For modelling human choices, we assumed a mixture policy of the policy provided by the meta-reinforcement learning agent, a random policy, and a stickiness term:</p><formula xml:id="formula_10">p RL 2 (a t | ε, λ ) = (1 − ε − λ )π (a t | h t ) + ε|A | −1 + λ δ (a t , a t−1 )<label>(20)</label></formula><formula xml:id="formula_11">p RR-RL 2 (a t | ε, λ ,C) = (1 − ε − λ )π (a t | h t ,C) + ε|A | −1 + λ δ (a t , a t−1 )<label>(21)</label></formula><p>where C, ε and λ are free parameters, |A | denotes the number of available actions, and δ (a t , a t−1 ) takes the value of 1 if a t = a t−1 and 0 otherwise. The marginal distribution π (a t | h t ) was approximated with 10 samples from the encoding distribution.</p><p>We performed a grid search over the free parameters ε, λ and C and obtained a log-likelihood estimate for all pairs of parameters. ε and λ could take values between 0 and 1 with increments of 0.02, subject to the constraint that their sum is less than or equal to 1. The description length C could take values from 10 to 10, 000 in steps of 10. We assumed a uniform prior probability over these discretized parameter values, which allows us to compute the marginal log-likelihood for the first trial of the compositional sub-task as follows:</p><formula xml:id="formula_12">log ∑ ε ∑ λ ∑ C exp N ∑ n=1 p RR-RL 2 (a 11,n,i | ε, λ ,C) − log(N C • N ε • N λ )<label>(22)</label></formula><p>where N C , N ε , and N λ correspond to the number of considered values for each parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparison</head><p>To obtain a quantitative measure of the goodness-of-fit to human choices, we conducted a Bayesian model comparison of all previously outlined models. We provide the full list of fitted parameters for each model in the SI. We measured the fit to human choices based on two metrics: posterior model frequency and exceedance probability <ref type="bibr" target="#b44">45</ref> . The posterior model frequency measures how often a model offers the best explanation in the population, while the exceedance probability measures how likely it is that a given model is the most frequent explanation. We compute the metrics for model comparison using a Python implementation of the Variational Bayesian Analysis (VBA) toolbox <ref type="bibr">[URL]</ref>. The toolbox requires us to provide log evidencethe marginal log-likelihood from the model fitting procedure in our case -for each model and participant, which we compute as previously described. For further details about this model comparison procedure see Rigoux and colleagues <ref type="bibr" target="#b44">45</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Behavioural results of experiment 1. (a) Mean regrets for participants in the final sub-task for the two conditions: curriculum and non-curriculum. The dotted lines in black indicate mean regret from a random policy. (b) Probability of participants making the optimal choice on the first trial of the final sub-task for the curriculum and non-curriculum condition. Error bars in (a) and (b) represent standard errors computed over participants.(c)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Modelling results of experiment 1. (a) The posterior model frequency of participant choices on the first trial of the last sub-task. (b) Comparison of the probability of making the optimal choice on the first trial of the last sub-task between people and simulations from the best-fitting models: RR-RL 2 and compositional BMT. (c) Correlation between fitted description length of RR-RL 2 (plotted in log-scale) and the probability of making an optimal choice on the first trial of the last sub-task. The fitted regression line is shown in green, with the shaded portion showing the 95% confidence interval.that simulations from RR-RL 2 (M = 0.366, SE = 0.0154) matched human behaviour (M = 0.382, SE = 0.021) whereas the compositional BMT differed significantly (M = 0.459, SE = 0.016; t = −2.938, p &lt; 0.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Behavioural results of experiment 2. (a) Mean regrets for participants in the final sub-task for the two conditions: curriculum and non-curriculum. The dotted lines in black indicate mean regret from a random policy. (b) Probability of participants making the optimal choice on the first trial of the final sub-task for the curriculum and non-curriculum condition. Error bars in (a) and (b) represent standard errors computed over participants. (c) Marginal distribution of choices of participants on the first trial of the final sub-task for the two conditions. The bar in gold shows the marginal distribution of choices for the optimal policy. (d) Explaining choices of participants in the last sub-task based on their task performance in the first two sub-tasks. The choices on the first trial were classified into four categories: optimal arm (top-left), non-optimal corner arms (top-right), non-optimal phasic arm (bottom-left), and neither of the above (bottom-right). Then, we fit a Bayesian logistic regression model from the total regrets (summed over all trials) in the linear and periodic sub-tasks onto each of these categories. The sub-plots show the histogram of the posterior regression coefficients of linear and periodic sub-tasks for all four choice categories. Modelling results of experiment 2. (a) The posterior model frequency of participant choices on the first trial of the last sub-task. (b) Comparison of the probability of making the optimal choice on the first trial of the last sub-task between people and simulations from the best-fitting models: RR-RL 2 and compositional BMT. (c) Correlation between fitted description length of RR-RL 2 (plotted in log-scale) and the probability of making an optimal choice on the first trial of the last sub-task. The fitted regression line is shown in green, with the shaded portion showing the 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The five key axes spanned by computational models of zero-shot compositional inferenceModel Generalization Exploration Composition Learned priors BoundedBayesian Mean-Tracker (mean only)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">t-values are reported from a non-parametric independent two-sample t-test (two-tailed) using 1000 random permutations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that in the main text, we have focused on model-comparison results for the first trial of the compositional sub-task as we are mostly interested in zero-shot compositional inference. However, we also evaluated our models over all trials of the compositional sub-task. The results of these analyses are summarised in the SI.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Max Planck Society, the Volkswagen Foundation, the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A, and funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy-EXC2064/1-390727645.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>inference. Thus, our results provide a new perspective to the understanding of human compositional inference by considering the influence of cognitive resources. Taken together, our work takes a considerable step towards understanding compositional reinforcement learning in humans, symbolic, and sub-symbolic agents under computational constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>In this section, we provide details of the experimental methods and computational models used to analyse compositional reinforcement learning in humans. In the experimental methods subsection, we describe the task parameters, experimental design, participants, and ethics approval. In the computational methods subsection, we expand on the computational models and explain the methods used to fit these models to human behaviour along with the model comparison procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 200 participants (103 female, M age = 28.90) through the Prolific platform for experiment 1. The study was approved by the local ethics committee. Participants were randomly assigned to the curriculum or non-curriculum condition. All participants had an approval rate of 95% or more, were fluent English speakers from the United States, and were 18 years of age or older. Participants were rewarded a base payment of £2 and a performance-dependent bonus payment up to £2.5. The bonus payment was computed by multiplying the total points earned by the participant with 1e-4. For experiment 2, we recruited 211 participants (96 females, M age = 27.58) through the Prolific platform. The rest of the study parameters remained the same as the additive rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Each task consisted of three multi-armed bandit sub-tasks in which rewards follow a function that is dependent on the spatial position of arms:</p><p>where t denotes the time-step, a t ∈ {0, . . . , 5} the arm selected in time-step t and ε t is an additive noise term. Reward functions f linear and f periodic for the first two sub-tasks are sampled from either the linear or the periodic family as shown below:</p><p>where U (a, b) is a uniform distribution on the interval [a,b] and ζ ∼ N (0, 0.2) is an additive noise term. The parameters were chosen after several rounds of piloting to make it easy for participants to perform the task well on average. For example, we found that excluding linear functions with very low slopes and periodic functions with very small amplitudes helped them learn the periodic and linear sub-tasks more easily and hence, improved their performance on the task overall. Reward functions in the final sub-task were constructed by composing the reward functions encountered in the two earlier sub-tasks. We considered two different composition rules, an additive rule and a change-point rule:</p><p>The order of composition in the change-point function was randomized. We set the length of each sub-task to 5 trials, leading to 15 overall trials per task. Note that the number of trials per sub-task was less than the number of available options. This prevents an agent from exhaustively trying out all options and forces it to generalize based on the underlying function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental design</head><p>We conducted an online behavioural study following the structure of the compositional bandit task outlined earlier to test the underlying mechanisms behind how people compose. Participants were told that they were gamblers visiting the fictional town of "Bandit City". They visited multiple casinos (20 in total) in which they played different sets of slot machines. Each casino had two slot machines made by two different companies, called Blue Lagoon and Green Geeks, with their colour (included in the name) indicating the manufacturer. Participants were informed that all slot machines from the same company behaved Author contributions statement A.J., M.B., and E.S. conceived the experiments, A.J. conducted the experiments, A.J. and T.S. analysed the results under M.B.'s, J.W.'s, and E.S.'s supervision. A.J. and M.B. wrote the main draft of the manuscript and J.W., T.S., and E.S. provided comments and reviewed the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information</head><p>The authors declare no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to grow a mind: Statistics, structure, and abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Acquiring a single new word</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reports on Child Lang. Dev</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meaning and compositionality as statistical induction of categories and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revealing ontological commitments by magic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00289</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Concepts and compositionality: in search of the brain&apos;s language of thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Frankland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. review psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="273" to="303" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalizing to generalize: Humans flexibly switch between compositional and conjunctive structures during reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1007720</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural signature of hierarchically structured expectations predicts clustering and transfer of rule sets in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="160" to="169" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Principles Of Psychology Volume II By William James (1890) (Henry Holt and company</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1890" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring the conceptual universe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">685</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal and state abstractions for efficient learning, transfer, and composition in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">643</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluss auf die geistigen Entwickelung des Menschengeschlechts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Von Humboldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Königliche Akademie der Wissenschaften</title>
		<imprint>
			<date type="published" when="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aspects of the Theory of Syntax</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2014" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Theory-based causal induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">661</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Formalizing prior knowledge in causal induction. The Oxf. Handb. Causal Reason</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards a unifying theory of generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>UCL (University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probing the compositionality of intuitive functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for Brains, Minds and Machines (CBMM)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compositional inductive biases in function learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">file://localhost/opt/grobid/grobid-home/tmp/10.1016/j.cogpsych.2017.11.002</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="44" to="79" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo with people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure discovery in nonparametric regression through compositional kernel search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1166" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning of structured task distributions in humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amalric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1005273</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The logical primitives of thought: Empirical foundations for compositional cognitive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">392</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A language of thought for the mental representation of geometric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sablé-Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">101527</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theory of memory for binary sequences: Evidence for a mental compression algorithm in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Planton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">1008598</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Putting bandits into context: How function learning supports decision making. J. experimental psychology: learning, memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">927</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding structure in multi-armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">101261</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compositional generalization in multi-armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saanum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probing compositional inference in natural and artificial agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saanum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Multidisciplinary Conference on Reinforcement Learning and Decision Making</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="275" to="279" />
		</imprint>
	</monogr>
	<note>RLDM 2022</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rapid instructed task learning: A new window into the human brain&apos;s unique capacity for flexible cognitive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Affect. &amp; Behav. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The biological basis of rapid instructed task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Cole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertainty and exploration in a restless bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Top. cognitive science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="351" to="367" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer school on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A tutorial on gaussian process regression: Modelling, exploring, and exploiting functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Psychol</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to reinforcement learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05763</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03969</idno>
		<title level="m">-context learning agents are asymmetric belief updaters</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03030</idno>
		<title level="m">Meta-learning of sequential strategies</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Meta-learned models of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06729</idno>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling human exploration through resource-rational reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31755" to="31768" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian model selection for group studies-revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="971" to="985" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Resource-rational decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Behav. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.03890</idno>
		<title level="m">Humans decompose tasks by trading off utility and computational cost</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rational use of cognitive resources in human planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1112" to="1125" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reconstructing the einstellung effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Brain &amp; Behav</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Compositional clustering in task structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1006116</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2873" to="2882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Compositionality decomposed: How do neural networks generalise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dankers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="757" to="795" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Compositional generalization through meta sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using natural language and program abstractions to instill human inductive biases in machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="167" to="180" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Determinants of human compositional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">PsyArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning composable hierarchical control with multiplicative compositional policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning with latent language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00482</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Organizing conceptual knowledge in humans with a gridlike code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="1464" to="1468" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The hippocampus as a predictive map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1643" to="1653" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Orbitofrontal cortex as a cognitive map of task space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schoenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How much of reinforcement learning is working memory, not reinforcement learning? a behavioral, computational, and neurogenetic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1024" to="1035" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cognitive control over learning: creating, clustering, and generalizing task-set structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">190</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Ecologically rational meta-learned inference explains human category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coda-Forno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01821</idno>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Structure discovery in nonparametric regression through compositional kernel search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zoubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Soft actor-critic algorithms and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bayesian optimization explains human active search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generalization guides human exploration in vast decision spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. human behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="915" to="924" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Information-theoretic regret bounds for gaussian process optimization in the bandit setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="3250" to="3265" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Probabilistic programming in python using pymc3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Wiecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fonnesbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
