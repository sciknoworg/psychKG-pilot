<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamical Driving Interactions between Human and Mentalizing-designed Autonomous Vehicle</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Zhang</surname></persName>
							<email>yikangzhang@umass.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>St</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th</forename><surname>Hanran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luna</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
							<email>haiyanwu@um.edu.mo</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanying</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Biomedical Engineering SUSTech Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Biomedical Engineering SUSTech Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Biomedical Engineering SUSTech Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Centre for Cognitive and Brain Sciences and Department of Psychology University of Macau Macau</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Centre for Cognitive and Brain Sciences and Department of Psychology University of Macau Macau</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Biomedical Engineering SUSTech Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamical Driving Interactions between Human and Mentalizing-designed Autonomous Vehicle</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Autonomous vehicle (AV) is progressing rapidly, but there are still many shortcomings when interacting with humans. To address this problem, it is necessary to study the human behaviors in human-AV interactions, and build a predictive model of human decision-making in the interaction. In turn, modelling human behavior in human-AV interaction can help us better understand human perception of AVs and human driving strategies. In this work, we first train multi-level AV agents using reinforcement learning (RL) models to imitate three mentalizing levels (i.e., level-0, level-1, and level-2), and then design a human-AV driving task that subjects interact with each level of AV agents in a two-lane merging scenario. Both human and AV driving behaviors are recorded. We found that conservative subjects obtain more rewards because of the randomness of the RL agents. Our results indicate that (i) human driving strategies are flexible and changeable, which allows to quickly adjust the strategy to maximize the reward when gaming against AV; (ii) human driving strategies are related to mentalizing ability, and subjects with higher mentalizing scores drive more conservatively. Our study shed lights on the relationship between human driving policy and mentalizing in human-AV interactions, and it can inspire the next-generation AV. Index Terms-Autonomous vehicle (AV), human-AV interaction, reinforcement learning, human behavior, decision making</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, autonomous driving has made great progress in becoming safer, more efficient, and less time-consuming. Many studies have been focused on designing algorithms for autonomous vehicles (AVs) in terms of trajectory prediction, planning and decision making, such as DIM <ref type="bibr" target="#b0">[1]</ref>, TNT <ref type="bibr" target="#b1">[2]</ref>, LaneGCN <ref type="bibr" target="#b2">[3]</ref>, achieving impressive results. Few studies have focused on AV-human interactions. Many Motion Prediction and Planning methods are proposed for autonomous vehicles Decision Making, for example, <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref>. However, traditional rule-based methods usually suffer from unpredictable interaction. Reinforcement learning (RL) is a machine learning method to enable agents learning for taking actions in an environment in order to maximize the notion of cumulative reward. RL, especially the rewards represented by deep neural network (deep RL), has been largely used in AVs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this study, we train AV agents with deep RL to investigate the interactions between humans and autonomous vehicles.</p><p>In traffic scenarios where humans and AVs coexist, merging is a frequent but challenging task that is critical for safe and efficient driving. Completing a merge requires understanding the surrounding environment, modeling the driving strategy of the opponent, and learning to make actions corresponding to the driving behavior of opponents. Therefore, merging is a commonly used task for training and testing AVs. <ref type="bibr" target="#b7">[8]</ref> proposed a probabilistic and interactive prediction approach on ramp merging scenario. <ref type="bibr" target="#b8">[9]</ref> analyses the significance of different <ref type="figure">Fig. 1</ref>: The experimental design of human-AV interactive task and human-human interactive task. (a) PvE (i.e., Player versus Environment) condition. The human subject is asked to interact with the AV agent to arrive the destination as quickly as possible without collision. The human subjects can receive a training block before the formal experiment. We use the block design, including three blocks corresponding to 3 levels of AV agents. A block begins with an introduction and then 40 trials with a 1âˆ¼3-second jitter in inter-trial interval (ITI). Human subjects press the number buttons to imitate driving scenario and interact with an AV in each block. The reward is informed in the end of each trial. (b) PvP (i.e., Player versus Player) condition. Two human subjects are asked to interact with each other with the goal to arrive the destination as quickly as possible without collision. In PvP condition, The monitor is divided into two windows separated with a black screen. We instruct the human subjects to only focus on the window on their own side. The bottom panel of (a) and (b) are examples of real driving moments. The color of car is black when maintaining a constant speed, red when accelerating, and blue when slowing down. The gray shadowed area is the position to arrive in the next 1 second, indicating the speed of the car. Note that both human and AV can only see the color and the gray area of the car controlled by themselves and cannot see the the accelerating and speed information of the opponent. variables for driving style classification when merging. <ref type="bibr" target="#b9">[10]</ref> defined merging behavior as a multi-agent dynamic game, and proposed an iterative algorithm to learn the interactive costs.</p><p>It is unclear whether AV can deal with humans with different personality, and in return, whether humans can learn to safely and efficiently interact with AV agents with different driving styles in a short period of time.</p><p>To operate safely, drivers need to observe the other vehicles and infer the mental state or driving style of the opponents in the traffic flow, which inevitably needs mentalizing. Mentalizing is a fundamental process in which we infer the inner thoughts and intentions of others. Wu et al proposed interactive mentalizing theory (IMT) <ref type="bibr" target="#b10">[11]</ref> and its related interactive mentalizing questionnaire (IMQ) <ref type="bibr" target="#b11">[12]</ref> to measure different mentalizing abilities in social interaction. Specifically, it measures self-self mentalization (SS), self-other(SO) and otherself mentalization (OS). The SO score quantifies the ability to assess the mental state of others from the perspective of the self. The SS score quantifies the ability to assess selfgenerated mental states from the perspective of the self. The OS score quantifies the ability to evaluate mentalization of self-generated mental states from the perspective of others. Meanwhile, a k-ToM model (e.g., level-0, level-1, level-2) is proposed, which predicts that the performance of agents engaged in competitive repeated interactions <ref type="bibr" target="#b12">[13]</ref>. In the social interaction like merging, mentalizing other drivers requires both prediction of other's operations (SO), and also inference of how the others think about oneself's intention (OS). Unpredictable reactions during driving (e.g., speeding, slowing down and right-/left-turn in the merging) may cause catastrophic accidents. Although large amounts of efforts have been made on human's mentalizing process in the in-lab experiments in the psychology community <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, and the decision-making algorithms in the merging task in AV community <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, few studies have combined the two to investigate mentalizing on AVs and human subjects, and the trade-off in time-saving and safety considerations.</p><p>Recent work has integrated the social value orientation (SVO) of drivers into AV algorithms to quantify an agent's degree of selfishness or altruism, resulting in reduced errors in predictions for algorithms incorporated with SVO <ref type="bibr" target="#b19">[20]</ref>. It brings a potential direction to AV algorithms, that is, taking personality traits and mentalized predictions into account in the model. In the current work, we designed a task under the framework of mentalizing level, with the incorporation of mentalizing in the human-human and human-AV interactions (see <ref type="figure">Fig. 1</ref>). That is, with consideration of mentalizing in AV and humans, we trained AV agents with different mentalizing levels and measure the first (SO) and higher-order mentalizing (OS) abilities in human drivers. Here, we aimed to test the following two hypotheses: (i) humans have more diverse driving policy compared with the RL-based AV, and can flexibly adjust driving policy to maximize rewards (Hypo 1); (ii) in human-AV game, human subjects with higher IMQ scores can earn more rewards than the lower ones since subjects with higher IMQ scores can better predict the policy of the opponent AV resulting in more rewards (Hypo 2). Testing these two hypotheses would help us understand the relationship between human driving policy and mentalizing in human-AV interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulator Design</head><p>The environment is designed as shown in <ref type="figure">Figure 1</ref>. Two cars start in two lanes and end up merging in one lane. The opponent car in the left is controlled by the trained RL agents while the ego car in the right is controlled by subjects. Different colors on the cars indicate acceleration (red), constant speed (black) and deceleration (blue) respectively. Shadow indicates the future trajectory of the ego car. The State Space for each agent is defined as S = {dx, dy, dv, x, v}, where dx, dy are the relative longitude and latitude distances of opponent, dv means the velocity difference, and x, v are the global position and velocity of ego car.</p><p>There are five discrete actions in the Action Space A to keep the speed of the vehicle at 0, 10, 20, 30 and 40. To avoid discontinuous state change, instead of setting constant acceleration values directly, we use Model Predictive Controller to Control the speed change of the cars in the next 3 seconds. The one-step control input is optimized with a minimum jerk.</p><p>In our study, four reward terms are considered: R 1 = 2/R 2 = 1, the reward for the first/second car that reaches the merge point; R col = âˆ’10, the punishment for both cars if a collision happens; and R spd = Î±|v âˆ’ v standard |, the punishment for speed deviation that We set a standard speed 20 and the car receives punishments proportional to the deviation of the current speed from the standard speed. The weights are carefully designed so that the strategy to keep full speed for R 1 would gain lower reward than those who keep standard speed and get R 2 . As a result, the best strategy is to stay at standard speed for most time, while overtake a little bit if the opponent takes no aggressive actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Three levels of Agents trained with Deep Q-Learning</head><p>In our study, we design the deep Q-learning network for training the multi-level intelligent autonomous vehicles with mentality. To overcome the requirement of the huge memory storage in the traditional Q-function table of model-free reinforcement learning, the introduced deep Q-learning network includes the neural network module (three fully connected layers followed with the Rectified Linear Unit activation function) for learning a function that characterizes the relationship between the state of the vehicle and its Q-value with different actions. The main objective of the deep Q-Learning Network tries to minimize the mean square error of the predicted Qvalue and the target Q-value Q * (the optimal action-value function) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In other words, the autonomous agent tries to maximize the cumulative future reward with the action selection. In our study, we iteratively train three mentalizing levels (level-0, level-1, level-2) of autonomous vehicles by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment</head><p>We recruited 40 human participants to take part in the experiment, 30 (8 females) for the PvE condition and 10 (2 females) for the PvP condition(aged 19-33 years (22.75Â±2.99). There are no significant difference between age and genders of the participants in two conditions (Age: F=6.280, p=0.017; Gender: F=0.1696, p=0.683). The experiment was divided into two parts: in the first part, the participants needed to complete the Interactive Mentalizing Questionnaire (IMQ) <ref type="bibr" target="#b11">[12]</ref> which evaluates the mentalizing ability of the participants; the second part was the interactive experiment. In our study, we randomly assigned participants to two conditions (PvE or PvP). All participants signed the informed consent before the formal experiment, and the experimental protocol was approved by the Institutional Review Board of the Southern University of Science and Technology.</p><p>In PvE condition, the human participants are informed to play with self-play AV agents. They can have a training block with 20 trials before the formal interactive experiment to ensure understanding the whole experiment. The formal experiment consists of three blocks, corresponding to the AV agents with 3 different mentalizing levels (i.e., level-0, level-1, level-2). The blocks are randomized across subjects, and each block consists of 40 trials. The participant is instructed to maximize the reward by controlling his/her own car. Participants can press buttons 0, 1, 2, 3, and 4 to reach the desired speed. The larger the number of keys, the faster of the speed. The speed of car is indicated by the gray area in front of the car in the monitor <ref type="figure">(Fig. 1(a)</ref>). The ego car controlled by the human driver is in the right lane, while the op car controlled by AV is in the left lane. Each trial started with a 1-3s jittered ITI, and then the participant can control the car by pressing buttons. The speed of the ego car and the current reward are displayed on the right-side, as described in II-A. The trial is ended when the ego car reached the end, and the reward of the current trial is shown for 3 seconds <ref type="figure">(Fig. 1 (a)</ref>). In the end of each block, the total rewards of the participant is presented. The participant's final payment is proportional to the total rewards in three blocks.</p><p>In PvP condition, we designed 4 blocks with 30 trials in each block (the first 10 trials for training, and the rest 20 trials for the formal experiment) and a 30s break after each block <ref type="figure">(Fig. 1 (b)</ref>). Two human participants are involved in the task. In the training trials, participant 1 is asked to free driving for 5 trials and participant 2 has to drive his/her car at a constant speed. The free driving car alternates after 5 trials. The formal experiment in PvP condition is similar to that in PvE condition. The only difference is the monitor is split into two windows with a black screen in the centre. Subjects are instructed to only look at his/her own side of the window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>In the PvE condition, the three agents which control the opponent car had very different driving policy that level-0 agent and level-2 agent drove aggressively and level-1 agent drove conservatively. The participants needed to learn the driving policy of the agent and change their own driving policy correspondingly to maximize rewards. The best driving policy for level-0 agent and level-2 agent is to press 2 the whole time and the best driving policy for level-1 agent is to press 3 or 4 to stay a safe distance in front of the opponent car and then press 2 till the end. In the PvP condition, two participants drove against each other in their own policy to maximize their rewards. Different from the PvE condition, subjects' driving policy could change much across trials and one participant could consider what would the other participant do to maximize their rewards. The PvP condition was similar to the Chicken Game and the best policy for the two participants was taking turns showing weakness maximizing group rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Changes in the ego car speed and the op car speed</head><p>We computed the standard deviation of the car speed to capture the change in speed within a single trial. The results are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. We found that the speed of ego car varied in a larger range than that of the opponent car, suggesting human flexibly utilizes a diversity of driving policies. The comparisons between PvP and PvE conditions show great similarity (Pearson Correlation: R = 0.7563, p = 2.45 Ã— 10 âˆ’75 ) between the behavior in person-to-person interactions and the game behavior when the policy of AV agent maintains while the policy of human flexibly varies. The cross shows the change of the driving policy that location of the cross converges to a region within 20 trials in the PvE condition and do not converge after 40 trials in the PvP condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relationship between rewards and mentalizing ability</head><p>We investigate the relationship between the policy in the game (e.g., total rewards) and the mentalizing ability of (3) The R 2 are 0.079, 0.068, 0.451, respectively. For the OS score, it is with regression coefficient 0.0269/0.0069/0.0344 for three different levels of AV, and a significant negative correlation between the averaged final rewards and SO score (with regression coefficient âˆ’0.0533/ âˆ’ 0.0213/ âˆ’ 0.0852).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relationship between policy switching and mentalizing ability</head><p>We perform a linear regression on the state variable of each trial in the Level-2 agent block and the subject's action:</p><formula xml:id="formula_0">ACT ION = a i * ST AT E i + b<label>(4)</label></formula><p>State: Î´x, the longitudinal distance between the two cars; Î´y, the lateral distance between the two cars; Î´v, the speed difference between the two cars; s, the distance traveled by the ego car; v 1 , the speed of the ego car; v 2 , the speed of the opponent car. </p><formula xml:id="formula_1">ACT ION = a i * ST AT E i + b, a in = Î± i * T rial n + b'</formula><p>The fitted parameter a can describe the importance of different state variables to the subject's decision-making in each trial. In the experiment, the subjects will learn the driving policy of the opponent car in order to maximize the reward, and then change their own driving strategy according to the driving strategy of the opponent car. We perform linear regression on the parameter a and the number of trials to further characterize the strategy switching between subjects:</p><formula xml:id="formula_2">a in = Î± i * T rial n + b'<label>(5)</label></formula><p>The absolute value of the fitted parameter |Î±| can depict how fast the participant's policy changes between trials. In <ref type="figure" target="#fig_2">Figure 4</ref>, we can see that the SO and OS scores are negatively correlated with the |Î±| corresponding to almost every state. Under the assumption that subjects adopt near-optimal driving strategies at the end of the Level-2 agent block, the subjects with higher OS and SO scores can better predict the driving policy of the opponent car, and the driving policies of the subjects in the early trials are closer to the optimal driving policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONCLUSION</head><p>Although early AV models of decision-making largely ignored mentalizing, the last few decades have detailed the central role mentalizing play in guiding social interaction choices. Take the humans' and AV's mentalizing level into account, the current work investigate the driving patterns in both the human-av interaction with different mentalizing level AVs, and human-human interaction with mentalizing ability measures. Our findings first confirm the diverse strategy of humans in human-human interactions, compared to human-AV interactions (Hypo 1). Meanwhile, we observe the interesting prediction effect of human mentalizing ability in policy of the opponent AV or human (Hypo 2).</p><p>A. Different driving policies in human-human interactions and human-AV interactions Through experimental study <ref type="bibr" target="#b22">[23]</ref>, it indicated a difference between human-human and human-AV interactions on the road. For example, human drivers felt more comfortable following the AV than human driver <ref type="bibr" target="#b23">[24]</ref>.AV that can predict risk level (Driver's Risk Field (DRF) model <ref type="bibr" target="#b24">[25]</ref>) can be more reliable and human-like.Under an aggressive driving scenario that using a professional racing simulator <ref type="bibr" target="#b25">[26]</ref>,they found different features are more important between human drivers and AV. Echos to previous study indicate the human-human interaction is more complex than human-AV interaction <ref type="bibr" target="#b26">[27]</ref>, our findings show similar pattern in the policy or strategy of merge. <ref type="figure" target="#fig_1">Figure 3</ref> showed the speed of human-driving car varied within a larger range in the PvE condition, indicating that human subjects' driving strategies were more diverse and flexible. By comparing PvE and PvP conditions, we uncover the differences in human-human interactions and human-AV interactions. We found that the variance of the speed of the two subjects in PvP condition has a linear correlation, indicating that human subjects will adjust their strategies according to each other's strategies, while in the PvE condition the strategy of AV agent was relatively monotonous and did not rely on the change of the subject's strategy. More specifically, in the PvE condition, when the human subject adopt the no-press strategy, the velocity variance of the AV agent continued varying, indicating that the AV agent has greater randomness. We found that the averaged variance converged within the first 20 trials in PvE condition, indicating that human subjects can quickly learn the AV agent's strategy and adjust their own strategies to maximize rewards. In the PvP condition, the averaged variance did not converge even after 40 trials, implying the high complexity of human-human interactions. The constant adjustment of the driving strategy during humanhuman game leads to the instability of the environment, which requires more learning to achieve the optimal strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From mentalizing to driving policy: impacts of mentalizing abilities</head><p>As we predicted, beliefs about our own mentalizing and other's intention is critical in social interaction, including human-human and human-AV scenarios <ref type="bibr" target="#b27">[28]</ref>. We investigate the relationship between the human driving policy and link it to one's mentalizing ability. Our study provide evidence to confirm that the mentalizing ability affect the driving policy both when one interacts with AV or interacts with another human subject. Our results show that subjects with higher mentalizing scores drive more conservatively and thereby resulting in more total rewards <ref type="figure" target="#fig_3">(Fig. 5</ref>). According to these findings, an initial summary is that all three sub-components play role in predicting human interaction with other agents from level-0 to level-2. However, it shows that the higherorder ability (OS score) <ref type="bibr" target="#b11">[12]</ref> may be with stronger influence in this prediction effect to both the optimal strategy and the gained reward. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Future work</head><p>The three agents in PvE condition were not very 'smart'. We may expect an uncertainty module to regular the agents' actions and train more high-level agents with different initial conditions and construct a recurrent neural network to capture temporal information of each trial. In addition to behavioral experiments, we may do some ERP experiments in the same driving environment. Future work with brain signal recording can further provide evidence of which brain regions will be engaged in different interactions during driving scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of 3 types of interactions in merging task and record results. Each car starts driving in its own line (a), then merges in the merging points (b) and eventually drive in the straight line (c). (d) Payoff matrix. The recorded behavioral data include the trajectory (e) and the driving speed (f) of individual human subject and AV. setting the opponent vehicle with the lower mentalizing level. The trained autonomous agents have iterative driving policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The behavioral results of the game. (a-c) In PvE condition, games between the human subject (i.e., ego car) and the 3 levels of AV opponent (op car), i.e., level-0 (a), level-1 (b) and level-2 (c), respectively. (d) In PvP condition, the game between two human subjects. The x-axis and y-axis are the variance of the speed of the ego car and op car in a single trial, with each dot representing a trial. The four gradient colors represent the 1-10, 11-20, 21-30, 31-40 trials respectively, and the symbol Ã— represents the average of speed variance across 10 trials.human subjects (e.g., IMQ scores) in PvE condition. We first applied the regression analysis with all IMQ scores and further breakdown with correlation analysis on the total rewards and the IMQ scores.For multiple linear regression, the result shows different predictive effect to average final results from IMQ scores. The fitted equations are as below : R(level-0) = âˆ’0.0003SS âˆ’ 0.0533SO + 0.0269OS + 0.7844 (1) R(level-1) = 0.0011SSâˆ’0.0213SO+0.0069OS+1.7818 (2) R(level-2) = âˆ’0.0373SS âˆ’ 0.0852SO + 0.0344OS + 2.1183</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Double regression parameters Î± and IMQ scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Reward as a function of mentalizing score. From left to right, the relationship between the accumulated rewards in the block and the SS score (left), SO score (middle) and OS score (right). From top to bottom, it present the results with level-0 agent (top), level-1 agent (middle), and level-2 agent (bottom).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep imitative models for flexible inference, planning, and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06544</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on motion prediction and risk assessment for intelligent vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">StÃ©phanie</forename><surname>LefÃ¨vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dizan</forename><surname>Vasquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ROBOMECH journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A review of motion planning techniques for automated vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JoshuÃ©</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>MilanÃ©s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1135" to="1145" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust deep reinforcement learning for security and safety in autonomous vehicle systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidin</forename><surname>Ferdowsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandayam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic prediction of interactive driving behavior via hierarchical inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On social interactions of merging behaviors at highway on-ramps in congested traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Maximum-entropy multi-agent dynamic games: Forward and inverse solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mac</forename><surname>Schwager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mentalizing during social InterAction: A four component model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><forename type="middle">C</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Mobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mentalizing During Social Interaction: The Development and Validation of the Interactive Mentalizing Questionnaire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The social bayesian brain: does mentalizing make a difference when we learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Devaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Hollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Daunizeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1003992</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural correlates of mentalizing-related computations during strategic interactions in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">N</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bossaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John P O'</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="6741" to="6746" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dissociation between social mentalizing and general reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Van Overwalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1589" to="1599" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mentalizing the body: spatial and social cognition in anosognosia for hemiplegia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahba</forename><surname>Besharati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kopelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aikaterini</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fotopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="971" to="985" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling and analysis of merging behavior at expressway on-ramp bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">2421</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rear-end crash potential estimation in the work zone merging areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Transportation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="238" to="249" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling vehicle merging behavior in work zone merging areas during the merging implementation period</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="925" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social behavior for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Alonso-Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="24972" to="24978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards a collaborative connected, automated driving environment: A game theory based decision framework for unprotected left turn maneuvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalda</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Talebpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1316" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Influence of autonomous vehicles on car-following behavior of human drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalda</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khajeh</forename><surname>Mohammadreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Talebpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation research record</title>
		<imprint>
			<biblScope unit="volume">2673</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="367" to="379" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-like driving behaviour emerges from a risk-based driver model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvesh</forename><surname>Kolekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Joost De Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparing driving behavior of humans and autonomous driving in a professional racing simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Remonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Veas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Granit</forename><surname>Luzhnica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS one</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245320</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attributions of social interactions: Driving among self-driving vs. conventional vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ching</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Momen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Lafreniere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology in Society</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101631</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating mentalization during driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Giorgio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Grasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lucifora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Perconti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEHITS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
