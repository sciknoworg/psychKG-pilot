<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How do people predict a random walk? Lessons for models of human cognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Spicer</surname></persName>
							<email>jake.spicer@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Qiao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Chater</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Warwick Business School</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How do people predict a random walk? Lessons for models of human cognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prediction</term>
					<term>Forecasting</term>
					<term>Cognitive Modelling</term>
					<term>Sampling</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Repeated forecasts of changing values are a key aspect of many everyday tasks, from predicting the weather to financial markets. A particularly simple and informative instance of such fluctuating values are random walks: sequences in which each point is a random movement from only its preceding value, unaffected by any previous points. Moreover, random walks often yield basic rational forecasting solutions in which predictions of new values should repeat the most recent value, and hence replicate the properties of the original series. In previous experiments, however, we have found that human forecasters do not adhere to this standard, showing systematic deviations from the properties of a random walk such as excessive volatility and extreme movements between subsequent predictions. We suggest that such deviations reflect general statistical signatures of human cognition displayed across multiple tasks, offering a window into underlying cognitive mechanisms. Using these deviations as new criteria, we here explore several cognitive models of forecasting drawn from various approaches developed in the existing literature, including Bayesian, error-based learning, autoregressive and sampling mechanisms. These models are contrasted with human data from two experiments to determine which best accounts for the particular statistical features displayed by participants. We find support for sampling models in both aggregate and individual fits, suggesting that these variations are attributable to the use of inherently stochastic prediction systems. We thus argue that variability in predictions is primarily driven by computational noise within the decision making process, rather than &quot;late&quot; noise at the output stage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How do people predict a random walk? Lessons for models of human cognition Introduction Many aspects of our world are not static but changeable, requiring us to regularly update our expectations as time passes: will last week's rain continue this week? Will tomorrow's traffic be better or worse? Will food prices increase or remain stable? We refer to such cases here as iterative predictions, reflecting scenarios in which forecasts are made repeatedly over time as the target value being predicted evolves. A particularly widely studied case of this is financial markets: prices of assets and commodities can fluctuate substantially, and predictive inaccuracy has immediate and potentially severe financial consequences. Understanding how people approach such tasks is therefore of vital importance, particularly whether generated forecasts are optimal given available information, or whether there are systematic biases in our predictions. The current paper thus seeks to offer insight into the cognitive systems supporting iterative predictions, focusing on a comparison of several previously proposed computational models in their fit to behaviour in such tasks.</p><p>The first challenge for any such examination, however, is deciding the target series being predicted, as this will determine both the actual behaviour under investigation and its idealised standard. One possibility is to investigate real world series, though in such cases the underlying generative process is often highly complex, or indeed might be entirely unknown, making definition of optimal responses difficult. Conversely, artificial series used in laboratory experiments offer greater certainty and control, though this comes at the cost of potential realism. In addition, use of artificial series raises further questions on how to select a specific target, offering the freedom to choose data of any form which could naturally advantage particular models suited to that structure; to provide an extreme example, if target data approximately follows a straight line, simple linear models will provide better matches than those using sine waves, whereas oscillating data would reverse this pattern.</p><p>The ideal test case is thus a series which provides a compromise between experimental control and external validity, as well as a neutral test-bed for various different approaches to forecasting. While the perfect target may remain elusive, one particularly appealing option is a random walk process: a random walk is a sequence of values in which each new point represents a random movement from its immediate predecessor with no further influence from previous data. Here, we focus specifically on a log-normal random walk where steps are randomly drawn from a Gaussian distribution in log space (illustrated in <ref type="figure">Figure 1A</ref>) as this naturally allows movements to scale according to the magnitude of the most recent value.</p><p>There are several reasons to focus on the random walk. First, this provides perhaps the most basic form of a 'randomly moving target', with unpredictable movements that cannot be adequately captured by fixed predictions, as would be the case in white noise processes with a consistent mean value. Second, as values are dependent only on the directly previous point, there is a definitive delineation between relevant and irrelevant past information. Third, the random walk has clearly defined statistical properties which can be compared with the patterns of human forecasts, and has established measures to do so. Fourth, the process offers an easily derived rational solution for predicting the next value in the sequence which minimises error via the expected value of its step distribution, which could be learned through experience.</p><p>Fifth, random walks present idealised market processes in finance theory, where any available information is immediately incorporated into price <ref type="bibr" target="#b88">(Samuelson, 1965;</ref><ref type="bibr" target="#b69">Mandelbrot, 1966;</ref><ref type="bibr" target="#b31">Fama, 1970)</ref>. More broadly, the random walk arguably provides one of the simplest forms within the limitless space of possible iterative prediction problems, so offering a basic foundation for testing cognitive models of forecasting.</p><p>A particular advantage of this random walk target is that the rational solution for predictions of subsequent values is clearly defined, providing a simple guideline for optimal behaviour: forecasters should predict the most likely change given by the step distribution (in this case, the mean of the Gaussian distribution). This is especially true if the expected movement is zero as forecasts need only directly repeat the preceding value, though non-zero means also provide reasonably simple trends to follow; for this reason, we shall initially focus on the zero-mean case, though we return to such alternate definitions later. This places optimal behaviour well within human capabilities, in contrast to other potential targets which might require more extensive computation. Indeed, there have been a number of theories which suggest forecasters will adhere to the rational solution, particularly in economics <ref type="bibr" target="#b68">(Lucas, 1972;</ref><ref type="bibr" target="#b71">Muth, 1961)</ref>; in <ref type="figure">Figure 1</ref>: A) Illustration of the log-normal random walk target: each movement in price represents a draw from a Gaussian distribution in log space centred on the immediately preceding price, shown by the red curve. The expected movement is thus 0, meaning the rational prediction on each trial is to repeat the previous price. B) Trial structure for the price prediction task as performed by participants, adapted from <ref type="bibr" target="#b116">Zhu et al. (2021)</ref>: after viewing the most recent price (shown in green), participants entered their prediction of the subsequent price (shown in red), before receiving feedback on the true next price and the potential reward given the accuracy of their prediction. such cases, forecasters are assumed to know that the true data generating process follows a random walk and act accordingly in their predictions, though similar behaviour could also be achieved by sufficiently advanced learning systems which are able to determine the random walk structure based on observation, such as complex connectionist networks or Bayesian models with appropriate prior beliefs. Moreover, merely repeating the current value provides a simple "fast and frugal" heuristic <ref type="bibr" target="#b38">(Gigerenzer &amp; Todd, 1999</ref>) that, while optimal for the random walk, is also likely to provide a good default strategy when the data deviates from a random walk in some unknown way. These types of approaches imply that human-generated predictions should mirror the properties of the original target series, in this case inheriting the features of the random walk.</p><p>In reality, however, actual behaviour does not appear to follow this standard: in a recent set of experiments, we observed that participant predictions of random walk series deviated from the properties of their target <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>. Such deviation is perhaps to be expected given that human judgements have been widely observed to be noisy, generally resulting in suboptimal performance <ref type="bibr" target="#b55">(Kahneman, Sibony, &amp; Sunstein, 2021)</ref>. What is notable, however, is that this noise appears to produce consistent patterns, including extreme changes between subsequent predictions and long-range dependencies in forecast values. Thus, people seem to be adding statistical structure into their forecasts which is not present in the target series being predicted. These patterns link to a wider literature on the features of repeated human estimation beyond random walk targets which has found evidence of specific statistical signatures such as long-range dependencies in both estimates and response times, and power-law distributions of movements between estimates <ref type="bibr" target="#b40">(Gilden, Thornton, &amp; Mallon, 1995;</ref><ref type="bibr" target="#b39">Gilden, 2001;</ref>). The 'noise' observed in these predictions thus appears to be an inherent aspect of human judgment, and so could speak to the nature of the underlying cognitive mechanisms.</p><p>How then might we explain these deviations from random walk structures? What is it about forecasters that leads their predictions to stray from the properties of their target?</p><p>Fortunately, there exist a myriad of theories and models in the existing psychological literature offering descriptive accounts of behaviour, using a variety of different approaches and techniques, which can be applied to model such predictions. The particular pattern of deviation from a pure random walk produced by forecasts from these models then offers a method to distinguish between these accounts, suggesting which (if any) mirrors the patterns produced by human forecasters. Of course, this is not the first study to compare models using fits to human forecasting data, though this does have a distinctive focus: previous studies of forecasting have often concentrated on whether models match the accuracy of human forecasters, or display similar reactions to trends <ref type="bibr" target="#b111">(Wagenaar &amp; Sagaria, 1975;</ref><ref type="bibr" target="#b28">Eggleton, 1982;</ref><ref type="bibr" target="#b63">Lawrence &amp; Makridakis, 1989;</ref><ref type="bibr" target="#b98">Sanders, 1992;</ref><ref type="bibr" target="#b85">Reimers &amp; Harvey, 2011;</ref><ref type="bibr" target="#b58">Kusev, Van Schaik, Tsaneva-Atanasova, Juliusson, &amp; Chater, 2018)</ref>. In contrast, we focus here on general statistical features displayed by participants in our random walk prediction task, testing whether models produce similar patterns of deviation in the higher-level properties of their forecasts. This follows the examination of such features to assess deviations from random walk structures in studies of financial markets when comparing actual price fluctuations against idealised standards <ref type="bibr" target="#b101">(Shiller, 1981;</ref><ref type="bibr" target="#b65">LeRoy &amp; Porter, 1981;</ref><ref type="bibr" target="#b25">Cont, Potters, &amp; Bouchaud, 1997;</ref><ref type="bibr" target="#b20">Campbell, Lo, &amp; McKinlay, 1999;</ref><ref type="bibr" target="#b70">Mantegna &amp; Stanley, 1999;</ref><ref type="bibr" target="#b24">Cont, 2001)</ref>, here being applied to the behaviour of individuals rather than macroeconomic systems. The use of such criteria thus offers a new metric of model performance, allowing for novel insights into the psychology of prediction.</p><p>The present paper thus seeks to compare several computational models of human forecasting drawn from various branches of psychology, assessing their ability to generate the specific statistical properties seen in individuals. The structure of the paper is as follows.</p><p>We begin with a brief review of the prior literature on this subject, and then introduce the empirical properties targeted in our comparison. We then define the set of candidate models to be considered, and contrast their ability to generate the specific properties seen in individuals.</p><p>Results from this comparison show support for a model in which predictions are generated by a local mental sampling process, a type of model with wide applicability beyond forecasting, and for which there is other independent support. We next assess the generality of these findings by examining the ability of these models to produce other prominent effects in the forecasting literature. We then extend our comparison to a new experimental data set to test the reliability of these results, finding similar evidence with alternate target series with positive or negative trends. We close with a discussion of the implications of these findings for depictions of human predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models of Iterative Predictions</head><p>As a fundamental aspect of human behaviour, the mechanisms underlying predictions have been a key focus across a wide range of psychological approaches, with many models specifically targeting how expectations are updated with further experience. While an exhaustive review of all existing approaches to human predictions is certainly beyond the scope of this paper, we here provide a brief summary of some of the more prominent methods that have been developed, focusing on models which can be applied to iterative numerical predictions of a random walk target.</p><p>One of the most notable approaches to iterative predictions is provided by associative learning methods, where expectations are captured psychologically via mental associations between relevant stimuli: these connections are updated with experience according to errors in anticipated events, strengthening where unexpected outcomes occur and weakening when expected outcomes fail to appear <ref type="bibr" target="#b19">(Bush &amp; Mosteller, 1951;</ref><ref type="bibr" target="#b86">Rescorla &amp; Wagner, 1972;</ref><ref type="bibr" target="#b78">Pearce &amp; Hall, 1980;</ref><ref type="bibr" target="#b77">Pearce &amp; Bouton, 2001</ref>). In the case of basic numerical prediction tasks such as that considered here, such techniques can be directly applied to continuous estimates, using anticipation errors to update predicted values themselves, for example in estimates of probabilities <ref type="bibr" target="#b12">(Behrens, Woolrich, Walton, &amp; Rushworth, 2007;</ref><ref type="bibr" target="#b33">Forsgren, Juslin, &amp; Van Den Berg, 2023)</ref>.</p><p>Similar error-based learning techniques also appear in depictions of market predictions in economics, in this case being described as adaptive expectations models <ref type="bibr" target="#b51">(Hey, 1994;</ref><ref type="bibr" target="#b52">Hommes, 2011;</ref><ref type="bibr" target="#b1">Afrouzi, Kwon, Landier, Ma, &amp; Thesmar, 2020)</ref>. Alternatively, multiple associative links can be combined into complex networks of interconnected nodes to provide much more detailed representations, allowing for greater abstraction between stimuli and responses <ref type="bibr" target="#b87">(Rumelhart, McClelland, &amp; the PDP Research Group, 1986)</ref>. Such advanced networks have shown success in matching human performance in complex tasks <ref type="bibr" target="#b61">(Lake, Zaremba, Fergus, &amp; Gureckis, 2015;</ref><ref type="bibr" target="#b104">Testolin &amp; Zorzi, 2016)</ref>, though for the present random walk case, such complexity may be unwarranted.</p><p>In contrast to the abstraction of connectionist methods, an alternate approach to iterative predictions is given by exemplar models, in which experiences are stored in memory and aggregated to provide expectations <ref type="bibr" target="#b74">(Nosofsky, 1992;</ref><ref type="bibr" target="#b54">Jäkel, Schölkopf, &amp; Wichmann, 2007)</ref>.</p><p>The weighting of memories in the aggregate can be adjusted to place greater focus on certain events over others, thus capturing observed behavioural biases. For example, many models define similarity functions which weight events according to correspondences in other trial features <ref type="bibr" target="#b100">(Shepard, 1987;</ref><ref type="bibr" target="#b29">Elwin, Juslin, Olsson, &amp; Enkvist, 2007;</ref><ref type="bibr" target="#b50">Henriksson, Elwin, &amp; Juslin, 2010)</ref>, while others place higher weights on more recent examples to reflect recency biases in human memory <ref type="bibr" target="#b6">(Anderson, Bothell, Lebiere, &amp; Matessa, 1998)</ref>. This does however come at potentially substantial memory cost where experience is extensive, possibly suggesting a need for limits on exemplar representations to save cognitive resources.</p><p>Other models offer a more rule-based approach to predictions, depicting behaviour using higher-level strategies to direct expectations <ref type="bibr" target="#b53">(Hommes &amp; Wagener, 2009)</ref>. Key examples of this are extrapolative models which seek to capture the apparent tendency for trend-chasing in human forecasts; this can be achieved in a number of ways, from basic statistical models such as limited window autoregressive processes which base expectations on recent events <ref type="bibr" target="#b52">(Hommes, 2011)</ref>, to exponential smoothing models which apply the previously described error-based learning to trends <ref type="bibr" target="#b35">(Gardner Jr, 1985;</ref><ref type="bibr" target="#b36">Gardner Jr &amp; McKenzie, 1985)</ref>, to complex integrations across the complete history of the series <ref type="bibr" target="#b8">(Barberis, Greenwood, Jin, &amp; Shleifer, 2015;</ref><ref type="bibr" target="#b1">Afrouzi et al., 2020)</ref>. Such strategies are often contrasted with mean-reverting or fundamentalist approaches where trends are instead expected to dissipate, in keeping with the rational solution described above <ref type="bibr" target="#b34">(Frankel &amp; Froot, 1990;</ref><ref type="bibr" target="#b17">Brock &amp; Hommes, 1998;</ref><ref type="bibr" target="#b9">Barberis, Shleifer, &amp; Vishny, 1998;</ref><ref type="bibr" target="#b14">Boswijk, Hommes, &amp; Manzan, 2007)</ref>. This has led to the development of strategy switching models where agents can select between rules across trials based on their performance, so adapting their behaviour as more information is received <ref type="bibr" target="#b9">(Barberis et al., 1998;</ref><ref type="bibr" target="#b7">Anufriev &amp; Hommes, 2012;</ref><ref type="bibr" target="#b52">Hommes, 2011)</ref>. Another suggested prediction strategy is anchoring and adjustment, in which predictions are initially based on a previously experienced value which is then slightly adjusted to provide a forecast <ref type="bibr" target="#b107">(Tversky &amp; Kahneman, 1974;</ref><ref type="bibr" target="#b64">Lawrence &amp; O'Connor, 1992;</ref><ref type="bibr" target="#b13">Bolger &amp; Harvey, 1993;</ref><ref type="bibr" target="#b58">Kusev et al., 2018)</ref>. The strength of these rule-based models is their simplicity, while the cost is their potential suboptimality: basic rules may be easy to use, but may not offer the best solution. For the present case, however, such suboptimality may itself be beneficial in capturing the deviations observed in actual behaviour.</p><p>Other work has drawn on statistical processes to describe behaviour, with a particularly notable example being the autoregressive integrated moving average (ARIMA) family of models; ARIMA combines autoregressive mechanisms to capture the previously noted extrapolative nature of predictions with moving averages as an adaptive baseline <ref type="bibr" target="#b16">(Box, Jenkins, Reinsel, &amp; Ljung, 2015)</ref>. These models have often been used as a standard against which human forecast performance can be compared <ref type="bibr" target="#b62">(Lawrence, Edmundson, &amp; O'Connor, 1985;</ref><ref type="bibr" target="#b64">Lawrence &amp; O'Connor, 1992;</ref><ref type="bibr" target="#b58">Kusev et al., 2018)</ref>, though this has been primarily focused on measures of accuracy rather than the specific statistical features targeted here. Such a model is particularly well-suited to the present random walk case as a simple autoregressive process with no return to the average matches with the rational solution which repeats the most recent value; indeed, the current random walk target itself essentially represents an ARIMA(1,0,0) process with the addition of random noise on each step. This being said, this correspondence may not be beneficial in the current context, where the aim is to capture human behaviour that strays from the random walk.</p><p>An alternate framework for iterative predictions suggests that forecasters may not have sufficient information regarding the task to achieve rational solutions immediately, but may attempt to learn such solutions as they gain experience. A key example of this is given by Bayesian approaches which seek to determine optimal responses given uncertainty: these models operate by defining probability distributions over potential outcomes which can be iteratively updated with new information according to Bayes' rule to produce normative expectations.</p><p>Bayesian models have shown strong correspondence with human behaviour in a range of tasks, including predicting category structure <ref type="bibr" target="#b5">(Anderson, 1991;</ref><ref type="bibr" target="#b60">Lake, Salakhutdinov, &amp; Tenenbaum, 2015)</ref>, intuitive physics <ref type="bibr" target="#b10">(Battaglia, Hamrick, &amp; Tenenbaum, 2013;</ref><ref type="bibr" target="#b93">Sanborn, Mansinghka, &amp; Griffiths, 2013)</ref>, magnitudes <ref type="bibr" target="#b43">(Griffiths &amp; Tenenbaum, 2011;</ref><ref type="bibr" target="#b79">Petzschner, Glasauer, &amp; Stephan, 2015)</ref>, language <ref type="bibr" target="#b21">(Chater &amp; Manning, 2006)</ref>, and perceptual features <ref type="bibr" target="#b89">(Sanborn &amp; Beierholm, 2016;</ref><ref type="bibr" target="#b102">Spicer, Sanborn, &amp; Beierholm, 2020)</ref>. As noted above, by offering rational learning systems, Bayesian models offer one potential method by which forecasters may obtain rational solutions through aggregated experience, eventually settling on optimal predictions. This is, however, restricted by the pre-existing beliefs of the model: Bayesian solutions are only optimal where prior assumptions hold true, meaning substantial deviations could arise if these beliefs are mis-specified. A further caveat to Bayesian approaches is their complexity: these models require decision makers to work with what may be highly complex probability distributions, requiring substantial computation <ref type="bibr" target="#b92">(Sanborn, Griffiths, &amp; Navarro, 2010;</ref><ref type="bibr" target="#b90">Sanborn &amp; Chater, 2016</ref>).</p><p>Fortunately, another class of models provides a solution to this complexity issue: in place of full distributions, decision makers can instead use a limited number of samples from the target as an approximation, drastically reducing computational costs <ref type="bibr" target="#b91">(Sanborn &amp; Chater, 2017;</ref><ref type="bibr" target="#b22">Chater et al., 2020;</ref><ref type="bibr" target="#b96">Sanborn et al., 2021)</ref>. While large sample counts will produce more accurate approximations, the limits of human cognitive resources mean that responses will often be based on only a few samples, so rarely meeting with the true optimum and thus leading to potential biases <ref type="bibr" target="#b26">(Dasgupta, Schulz, &amp; Gershman, 2017;</ref><ref type="bibr" target="#b67">Lieder, Griffiths, Huys, &amp; Goodman, 2018;</ref><ref type="bibr" target="#b117">Zhu, Sundh, Spicer, Chater, &amp; Sanborn, 2023)</ref>. In addition, the inherent stochasticity of sampling algorithms offers explanations for the noisiness of human behaviour: samples can vary even in response to the same query, potentially resulting in different behaviours. Furthermore, the method by which samples are generated will also lead to patterns in responses that may not be present in the target sequence being predicted, such as autocorrelations between sequential responses <ref type="bibr" target="#b114">(Zhu et al., 2022</ref><ref type="bibr" target="#b117">(Zhu et al., , 2023</ref>. Indeed, in previous work, we have suggested that use of particular sampling algorithms can explain the specific patterns shown by human subjects in repeated estimation tasks <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>, though this model has not yet been directly compared with competitors such as those listed above.</p><p>Note that these modelling approaches to iterative predictions are not necessarily mutually exclusive: in fact, some proposed models use a mixture of these elements to better match actual behaviour, such as connectionist networks with external memory stores <ref type="bibr">(Graves et al., 2016)</ref> or error-based learning within Bayesian frameworks <ref type="bibr" target="#b80">(Piray &amp; Daw, 2021)</ref>. Such hybrids naturally combine the strengths of their component elements, though this does come at the cost of increased complexity. While these hybrids are interesting candidates, we restrict our focus here to base models rather than potential combinations both for simplicity as well as to characterise the individual behaviour of each element, leaving comparisons of these advanced models to future work.</p><p>While this is only a cross-section of the many models that can be adapted to iterative predictions, this should illustrate the breadth of available approaches that could be applied, and that a full and comprehensive comparison is beyond the scope of any one paper. We thus seek here to offer a high-level comparison, evaluating a set of representative key models selected from across these branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioural Targets</head><p>We next outline the properties of human behaviour which will provide the criteria for the subsequent model comparison. These properties are assessed using data from a previous experiment in which we contrasted individual predictions with the movements of financial markets, finding notable correspondence in several features <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>. The present comparison then takes these measures as empirical standards of human behaviour, providing targets for model assessment. We first summarise the details of this experiment, before discussing the key measures taken from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Details</head><p>The task involved trial-by-trial predictions of the price of a fictional financial asset: on each trial t, participants were asked to predict the immediately subsequent price (p t+1 ) of the asset given only their experience with its price history up to that point (p 1:t ). <ref type="figure">Figure   1B</ref> provides an illustration of the task structure. The true price series was predefined using a random walk process taking Gaussian steps in log space (shown in <ref type="figure">Figure 1A)</ref>:</p><formula xml:id="formula_0">ln(p t ) = ln(p t−1 ) + ε t (1) ε t ∼ N(0, σ 2 )<label>(2)</label></formula><p>though these values were exponentiated to prices, multiplied by 100 and rounded to the nearest integer for presentation in the task 1 . Participants were incentivised to minimise error in their predictions by offering them a monetary reward at the end of the task based on their prediction error in a randomly selected trial. This used an exponential reward function based on absolute logarithmic difference between predicted and actual price:</p><formula xml:id="formula_1">reward = £2 + 18 * exp(−9(|ln(p i ) − ln(p e i )|))<label>(3)</label></formula><p>where p e i is the prediction of p i and i is the randomly selected trial. The use of absolute error in the reward function means that participants maximise expected reward by focusing on the median of the step distribution, which is 0; this then means that the rational prediction strategy on any trial is to simply repeat the preceding price, leading the optimal prediction series to mirror the target random walk at a one-step lag.</p><p>After each prediction, the task advanced to the next trial, and the true price for that period was revealed, as well as the error in the preceding prediction and the potential reward resulting from that error should it be selected. Participants continued predicting prices until reaching the maximum experimental duration of 1 hour, with participants completing 449 trials (±16.2 95% CI) on average. Once the task was complete, participants were asked whether they had been interrupted during the task to act as an exclusion criterion. Finally, one of the participant's trials was randomly selected and their error in that trial was used to determine their reward value using Equation 3, paid in British pounds. Data were only accepted for analysis if the participant completed a minimum of 350 trials, and reported no interruptions.</p><p>Full details on this experiment are given by <ref type="bibr" target="#b116">Zhu et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exclusions</head><p>As participant responses in this task represent a series of potentially interconnected predictions, we sought to avoid editing this data as much as possible to preserve its integrity.</p><p>This being said, there are of course concerns that any errors in response could pollute the data and so confound measures taken from it. This is most notable when examining extreme movements between predictions: in such cases, does this truly reflect an extreme change in beliefs, or simply a mistake in the submitted response? We therefore applied an exclusion criterion within the present analysis (in addition to those used in <ref type="bibr" target="#b116">Zhu et al., 2021)</ref> as a precaution for such cases: responses which differed from both the preceding and subsequent predictions by a factor of 10 were treated as errors and replaced with a non-number filler to preserve series spacing, removing an average of 0.37% of responses across participants. This criterion was not used in our previous paper <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>, but was deemed necessary for the more intensive quantitative analyses performed here. We revisit results without this exclusion in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>We next describe the key properties of the human data which will be used as criteria for model comparison. These properties reflect particular statistical features which have been previously used to assess deviations from random walk patterns in financial studies <ref type="bibr" target="#b101">(Shiller, 1981;</ref><ref type="bibr" target="#b65">LeRoy &amp; Porter, 1981;</ref><ref type="bibr" target="#b25">Cont et al., 1997;</ref><ref type="bibr" target="#b20">Campbell et al., 1999;</ref><ref type="bibr" target="#b70">Mantegna &amp; Stanley, 1999;</ref><ref type="bibr" target="#b24">Cont, 2001)</ref>, following the suggestion that random walks provide idealised market structures <ref type="bibr" target="#b88">(Samuelson, 1965;</ref><ref type="bibr" target="#b69">Mandelbrot, 1966;</ref><ref type="bibr" target="#b31">Fama, 1970)</ref>. We used these measures in our prior work to examine the correspondence between cognitive and market fluctuations <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>, though here our focus is on the values shown by human forecasters themselves rather than their similarity with market measures.</p><p>Target properties can broadly be divided into three key aspects: autocorrelations, excess volatility and heavy tails. Multiple measures are used for some features to both provide greater confidence in the assessment of these properties and control for any issues which may be raised with any individual measure. The following provides a brief introduction to each of these measures, as well as their distributions in the experimental sample, as summarised in <ref type="figure" target="#fig_0">Figure 2</ref>. More detailed definitions of these measures are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autocorrelations</head><p>Financial markets display little evidence of autocorrelations in their changes (i.e., an absence of correlation in returns) beyond short time-lags, leading to the appearance of a firstorder random walk <ref type="bibr" target="#b25">(Cont et al., 1997;</ref><ref type="bibr" target="#b24">Cont, 2001)</ref>. We use two measures of this property given by existing studies: the autocorrelation function in changes between subsequent log predictions and the power spectral density of the log prediction series. The autocorrelation function reflects the relationship between trial-to-trial changes in log predictions when compared across longer time lags, and is expected to fall to approximately 0 after the first lag; we therefore assess this property by taking the proportion of autocorrelation coefficients across the first 100 lags which are found to fall outside of 95% confidence intervals around zero, which for ease of presentation will here be termed the significant rate. The mean significant rate across participants was 0.069 (±0.011), suggesting such correlations were indeed rare in this sample 2 .</p><p>The power spectral density is used to examine long-range autocorrelations in the logarithmic prediction series according to the relation between frequency and density following a Fourier analysis, with random walks typically showing linear slopes of approximately -2 in log-log space <ref type="bibr" target="#b70">(Mantegna &amp; Stanley, 1999)</ref>; indeed, the present random walk target itself shows a slope of -1.90, with the difference from the theoretical standard value likely being attributable to the limited length of the experimental target. Participant results fell slightly above this value, with a mean slope of -1.71 (±0.04), potentially suggesting a degree of long-range autocorrelation in the data, as have been observed in other repeated estimation tasks <ref type="bibr" target="#b40">(Gilden et al., 1995;</ref><ref type="bibr" target="#b39">Gilden, 2001;</ref><ref type="bibr" target="#b114">Zhu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Excess Volatility</head><p>Markets have been found to demonstrate higher levels of volatility than is suggested by underlying fundamentals, leading to greater movements in price than predicted by rational approaches <ref type="bibr" target="#b101">(Shiller, 1981;</ref><ref type="bibr" target="#b65">LeRoy &amp; Porter, 1981)</ref>. We measure this property here according to the variance in the distribution of changes between subsequent log predictions (i.e., Var[ln(p e t )− ln(p e t−1 )]); following this property, this variance should exceed that of the true distribution of price changes in the underlying random walk (from Equation 2), though we here directly use this variance to provide a continuous measure rather than a binary comparison. Mean variance in this distribution was 0.188 (±0.032) across participants, with almost all individuals exceeding the true variance of the target series (0.063).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heavy Tails</head><p>Distributions of price changes show heavier tails than Gaussian standards, demonstrating substantial increases and decreases between subsequent points <ref type="bibr" target="#b25">(Cont et al., 1997;</ref><ref type="bibr" target="#b24">Cont, 2001</ref>).</p><p>We again use multiple measures to assess this features based on suggestions from past research:</p><p>first, we estimate the tail index of the distribution of changes between log predictions by fitting a Generalised Pareto distribution to the top 5% of these changes and taking the resulting shape parameter <ref type="bibr" target="#b30">(Embrechts, Klüppelberg, &amp; Mikosch, 1997;</ref><ref type="bibr" target="#b57">Kotz &amp; Nadarajah, 2000)</ref>, where an index of 0 implies Gaussian tails while positive values imply heavier tails. Participant values tend towards this on average, with a mean index of 0.34 (±0.20), though there are some participants displaying thin tails according to this measure. Second, we take the kurtosis (i.e., the fourth standardised moment) of the distribution of changes between log predictions, where kurtosis values above 3 imply heavier tails than Gaussian. Kurtosis was similarly high in participant predictions, with a mean of 14.6 (±1.96), though in this case measures consistently indicated heavy tails for all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Properties</head><p>While the above measures are the key focus of the present study, there is one other property which deserves special mention: volatility clustering, which refers to the finding that market volatility is not even across time but appears in concentrated bursts. This was a key property in our previous comparison of the fluctuations in market prices and human predictions <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>, but is not assessed here as measures of volatility clustering are substantially noisier in individuals than groups, meaning this may not offer a stable behavioural criterion. We do however revisit volatility clustering in Appendix D, further detailing the prevalence of this feature in the human data, and outlining that inclusion of this property does not meaningfully alter model comparison results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Definitions</head><p>We next define the models that will be evaluated in this task. Rather than defining independent candidate models, we here use a factorial modelling comparison procedure in which models reflect combinations of separate factors; this is used as model behaviour is here dependent not only on the method by which predictions are generated but also the representation used 3 . Models are thus composed of four key factors: the prediction mechanism, the target format, the target domain, and response noise, where the prediction mechanism captures the method and the format and domain capture the representation, with response noise reflecting potential errors in expression of expectations. We therefore first define these representational factors as this will determine some aspects of the prediction mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Format</head><p>Target format refers to the specific aspect which is being predicted, here reflecting two potential methods by which price predictions may be generated: the decision maker may directly estimate the subsequent price, or may simply estimate the expected change from the most recent price. This is an important distinction as predictions of price change are naturally anchored to the most recent price point, considering only the movement from that value and so allowing predictions to remain closer to the target series. In contrast, predictions of price do not hold such an attachment and so may show greater deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Domain</head><p>Target domain refers to the scale in which the target format is considered, here divided between standard and log-scale; while participants in the actual task were presented with standard prices, many economic theories would suggest a focus on logarithmic prices to account for differences in scale between movements, particularly where the target series itself is a random walk in log space. We therefore consider both domains within this comparison to determine which better reflects behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Mechanisms</head><p>The prediction mechanism refers to the method by which a prediction may be generated within the considered format and domain. We examine nine potential prediction mechanisms taken from existing approaches to cognitive predictions suggested by previous research, each described in detail below. For ease of interpretation, throughout the following, X t refers to the target at time t in the considered format and domain, being either price (p t ), log price (ln(p t )), price change (p t − p t−1 ) or log price change (ln(p t ) − ln(p t−1 )), in all cases being available to the mechanism at time t. X e t+1 meanwhile reflects the predicted value of the target X t+1 made by that mechanism for the immediately subsequent trial (i.e., the prediction made on trial t for trial t + 1). Given the task considered here is restricted to one-trial ahead predictions only, we do not consider mechanisms using a longer prediction horizon; while such methods have been used in previous examinations of the revision of predictions with further information <ref type="bibr" target="#b1">(Afrouzi et al., 2020)</ref>, these do not offer meaningful predictions within the present framework. Such revisions are not however a key focus of the present study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Information Rational Expectations (FIRE)</head><p>The Full Information Rational Expectations mechanism, or FIRE, represents the optimal prediction method given knowledge of the true data generating process, and is included as an idealised standard for behaviour. As previously noted, this is dependent on the step distribution ε t , here being a normal distribution in log space centred on 0; as such, the rational expectation on each trial is to repeat the preceding price point, regardless of target format and domain:</p><formula xml:id="formula_2">p e t+1 = p t<label>(4)</label></formula><p>This operates on the assumption of an absolute loss function in prediction errors, leading to a focus on the median of the change distribution, being 0 in both standard and log domains. This is reinforced by the reward scheme in the experiment, which was based on absolute log error between predictions and targets, meaning expected reward is maximised where predictions match the present price. FIRE thus directly reflects the target series, and so includes no free parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direct Sampling (DS)</head><p>In contrast to the FIRE predictor which reduces the step distribution to a single rational expectation for all trials, we next consider a range of predictors which sample from this distribution in various ways, leading to greater variation in forecasts. These mechanisms are in fact free to use any distribution for this process, reflecting subjective expectations regarding future values, though we here limit these to the true distribution to both simplify and constrain these predictors. Direct sampling represents the simplest form of this process, with values being drawn directly from the step distribution according to their probability:</p><formula xml:id="formula_3">ln(p e t+1 ) = ln(p t ) + η t (5) η t ∼ N(0, σ 2 )<label>(6)</label></formula><p>where σ 2 is the true variance of the random walk step distribution given by Equation 2. Direct sampling is therefore here functionally equivalent to a noisy form of FIRE in which the true step distribution of the target series is known but not maximised, leading to variations in beliefs across trials, though this equivalence would be diminished if using alternate subjective target distributions. This predictor then also has no free parameters, with all elements being dependent on the true data generating process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markov Chain Monte Carlo (MCMC)</head><p>Markov Chain Monte Carlo, or MCMC, is an alternate sampling method commonly used to approximate a target distribution using only sequential samples: a Markov chain explores the distribution by iteratively proposing new states within that distribution, comparing the current and proposed positions, and either accepting the transition or remaining in place. With a sufficient number of steps, the proportion of trials spent in each state reflects its probability.</p><p>As with Direct sampling, MCMC is here assumed to explore the true distribution of potential price changes given by the random walk data generating process, though in this case, samples</p><p>are not independent of one another but autocorrelated, meaning samples closer in time will be more similar.</p><p>In detail, the mechanism begins by sampling a proposed state from nearby space in the given representation, simulated using a Gaussian distribution around the most recent prediction with a preset variance term, with the first prediction (X e 2 ) being a free parameter:</p><formula xml:id="formula_4">Y e t ∼ N(X e t , σ 2 s )<label>(7)</label></formula><p>where σ s represents the step-size of the sampler. The probability of this sample is then compared to the probability of the preceding sample, here again defined by the true distribution given the domain and format of the proposed value:</p><formula xml:id="formula_5">p(Y e t ) ∼                        logN(ln(p t ), σ 2 ) for standard price N(ln(p t ), σ 2 ) for log price logN(0, σ 2 ) for standard price change N(0, σ 2 )</formula><p>for log price change <ref type="formula">8</ref>We here focus on the Metropolis Hastings algorithm to determine state transitions <ref type="bibr" target="#b46">(Hastings, 1970)</ref>: if the probability of the proposed state exceeds that of the current state, it is immediately accepted. Conversely, if the probability of the proposed state is less than that of the current state, the transition probability is defined by the relative ratio of these probabilities:</p><formula xml:id="formula_6">p(X e t+1 = Y e t ) = min 1, p(Y e t ) p(X e t )<label>(9)</label></formula><p>where the probabilities of both the proposed value Y e t and the previous sample X e t are defined according to Equation 8. The chain is then more likely to move to a lower probability state if the proposed state is more similar to the current state, and vice versa. If the proposed state is accepted, it becomes the prediction for that trial, while if the state is rejected, the previous prediction is repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metropolis Coupled Markov Chain Monte Carlo (MC 3 )</head><p>Metropolis Coupled Markov Chain Monte Carlo, or MC 3 , is another sampling algorithm which extends standard MCMC procedures to include multiple chains exploring the space simultaneously, but at different "temperatures": higher temperatures distort the target distribution, making it more uniform and therefore increasing the probability of transitions to lower probability regions <ref type="bibr" target="#b37">(Geyer, 1991;</ref><ref type="bibr" target="#b2">Altekar, Dwarkadas, Huelsenbeck, &amp; Ronquist, 2004)</ref>. This is achieved by raising the target probability distribution (that is, the true step distribution for the relevant representation defined in Equation 8) to an individual exponent for each chain: p(Y e i,t ) τ i , where Y e i,t is the proposed state now specific to chain i and τ i is the temperature parameter of that chain, defined as:</p><formula xml:id="formula_7">τ i = 1 1 + φ(i − 1) (10)</formula><p>where φ controls the rate of difference in temperature between chains, and the first chain remains undistorted with temperature 1.</p><p>Each chain then proceeds through the same procedure described above for the MCMC predictor to determine state transitions, again using the Metropolis Hastings algorithm. Once these transitions are decided, however, chains are also compared with one another to allow potential switches in temperature: following state selections, chains are randomly paired (without replacement) and the joint probability of their respective positions is compared between their current temperatures and their switched temperatures. The probability of switching is then determined by the ratio of these products using the same transition rule used to determine state transitions:</p><formula xml:id="formula_8">p(switch(τ a , τ b )) = min 1, p(Y e a,t ) τ b p(Y e b,t ) τ a p(Y e a,t ) τ a p(Y e b,t ) τ b (11)</formula><p>where these state probabilities are again defined by Equation 8. Once these switches are decided, the final prediction for each trial is then taken from the undistorted chain. The parameters of MC 3 match those of MCMC with the addition of the number of parallel chains c and the temperature parameter φ; with a single chain, MC 3 reduces to MCMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Updating (BU)</head><p>Whereas the previous mechanisms know the true distribution used by the data generating process (but differed in their access to it), the Bayesian prediction mechanism instead seeks to infer this distribution based on both observed data and prior beliefs, updating this distribution with each new observation. For simplicity, we here assume the Bayesian predictor infers a stationary distribution within its considered format and domain; in the case of log change representations, this is accurate to the true random walk process, whereas price formats will show greater deviations as no stable price distribution exists. The definition of this mechanism follows that given by <ref type="bibr" target="#b5">Anderson (1991)</ref> for inference of continuous variables in which the statistical properties observed in the target to that point are combined with prior parameters to form a posterior distribution from which a prediction can be drawn. The prior distribution is defined by four parameters: the prior mean µ 0 , the prior variance σ 0 2 , and the respective confidence in these values λ 0 and β 0 . Prior variance is estimated using an inverse chi-squared distribution:</p><formula xml:id="formula_9">σ 2 0 ∼ β 0 σ 0 2 χ β 0 2<label>(12)</label></formula><p>while prior mean is estimated using a Gaussian distribution:</p><formula xml:id="formula_10">µ 0 |σ ∼ N µ 0 , σ √ λ 0<label>(13)</label></formula><p>Note that the second parameter of this distribution is the standard deviation rather than the variance. These two distributions can then be combined to produce a t-distribution across potential values for the next point of the target series (again, the second parameter of this tdistribution is the standard deviation rather than the variance):</p><formula xml:id="formula_11">p(X e t+1 |X 1:t ) = t β t (µ t , σ t 1 + 1/λ t )<label>(14)</label></formula><p>where the parameters of this distribution are based on combinations of the prior mean µ 0 and variance σ 0 2 with the observed meanx and variance s 2 taken from the series at time t, using the confidence values β 0 and λ 0 :</p><formula xml:id="formula_12">β t = β 0 + n t (15) λ t = λ 0 + n t (16) µ t = λ 0 µ 0 + n tx λ 0 + n t (17) σ 2 t = β 0 σ 2 0 + (n t − 1)s 2 + λ 0 n t λ 0 +n t (µ 0 −x) 2 β 0 + n t (18)</formula><p>where n t is the number of observations at time t. Predictions can then be taken from the posterior distribution using a number of methods, though for simplicity we here focus solely on the mean of this distribution µ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Delta Rule Learning (DR)</head><p>The Delta Rule Learning mechanism represents a simple form of the associative learning methods described above; like the Bayesian predictor, this also estimates the next value in the target series with iterative adjustments trial-to-trial, but in this case these adjustments are based only on errors in the immediately previous prediction, controlled by the learning rate α:</p><formula xml:id="formula_13">X e t+1 = X e t − α(X e t − X t )<label>(19)</label></formula><p>This mechanism therefore requires definition of the first prediction (i.e. X e 2 ) to begin this process, meaning this value is included as an additional free parameter. This implementation of Delta</p><p>Rule Learning also matches with Adaptive Expectations approaches used in economic models in which predictions are updated on each trial using a weighted average of the most recent prediction and target <ref type="bibr" target="#b1">(Afrouzi et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential Smoothing (ES)</head><p>The Exponential Smoothing predictor is a generalisation of the Delta Rule predictor allowing for similar error-based learning in trends as well as values; while the target random walk does not include such trends, we include this predictor to allow for the possibility that forecasters may mistakenly assume them nonetheless. There are multiple potential definitions of Exponential Smoothing, though for the present purposes we use the definition given by <ref type="bibr" target="#b35">Gardner Jr (1985)</ref> for damped trends without seasonality (pg. 5, Model 7-1). As with the Delta rule predictor, this adjusts predictions according to previous error, though in this case this is separated into a point term S and a trend term T :</p><formula xml:id="formula_14">S e t+1 = X e t − α(X e t − X t ) (20) T e t+1 = δT e t − αζ(X e t − X t )<label>(21)</label></formula><p>X e t+1 = S e t+1 + δT e t+1</p><p>where α is again the learning rate for the point term, while δ and ζ control learning of the trend term; where these trend parameters are set to 0, Exponential Smoothing matches the Delta Rule predictor defined above. As with the Delta Rule predictor, this also requires definition of a first prediction X e 2 to begin error based learning, leading to four total parameters for the Exponential Smoothing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First-Order Autoregressive Process (AR1)</head><p>The AR1 mechanism represents a statistical approach to forecasting, using a subform of the ARIMA family of models (specifically ARIMA(1,0,0)) which predicts prices using a first-order autoregressive process where expectations are based only on the most recent observation of the target with no moving average. This takes the form of an anchor value γ plus the current deviation from that constant modified by coefficient ρ to suit all potential representations:</p><formula xml:id="formula_16">X e t+1 = γ + ρ(X t − γ)<label>(23)</label></formula><p>A ρ value of 0 then leads to constant predictions of γ, while a ρ value of 1 repeats the previous observation; in price formats, this is equivalent to the FIRE model above, matching the original time series, while in change formats, this will produce more trend-chasing behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory-Weighted (MW)</head><p>The Memory-Weighted mechanism represents an exemplar-based approach to predictions, making estimates of future values based on a weighted average of previous observations to that point:</p><formula xml:id="formula_17">X e t+1 = t ∑ i=1 w i X i (24)</formula><p>Note that in change formats, this mechanism is unable to generate a prediction in the first trial as no previous change observations are available at this point; unlike the previous models, this is not included as an additional parameter as this would necessarily differ between formats. To reflect decay in memory over time, this uses an exponential weighting system placing greater focus on more recent events:</p><formula xml:id="formula_18">w i = κ t−i ∑ t j=1 κ t− j (25)</formula><p>where κ is a variable base parameter controlling the speed of decay of the weighting function for older observations. We here constrain the range of this parameter between 0 and 1 to focus on instances of recency bias, with 1 reflecting uniform weighting and 0 reflecting focus on only the most recent observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response Noise</head><p>With the exception of the sampling mechanisms (Direct Sampling, MCMC and MC 3 ), all the above mechanisms offer deterministic predictions, producing constant forecasts for a given input series and set of parameter values. Human behaviour rarely offers such consistency,</p><p>however, suggesting some form of stochasticity is necessary; the sampling models could therefore be seen as holding an advantage in this comparison, as such variation is inherent to their operation.</p><p>To counter this, we include a noise factor in our combined models (including those using the sampling predictors) to capture potential imprecision in the expression of model predictions as actual responses, reflecting factors such as rounding or typing errors. While there are many possible definitions for such noise, in order to limit the current contrasts, we here focus on the simple addition of independent and identically distributed (IID) log-normal noise to each forecast value, allowing errors to scale with the magnitude of predictions:</p><formula xml:id="formula_19">ln(R t ) = ln(p e t+1 ) + δ t (26) δ t ∼ N(0, σ 2 n )<label>(27)</label></formula><p>where R t is the response ultimately given on trial t, being the next value predicted by the candidate mechanism plus a randomly sampled noise term in log space, with σ n being an additional free parameter. In order to determine the contribution of such noise to model fit, however, we evaluate model performance both before and after the addition of noise, treating this as a binary factor in the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head><p>To assess the correspondence of the above models with behaviour, we use a form of Approximate Bayesian Computation, or ABC <ref type="bibr" target="#b105">(Turner &amp; Van Zandt, 2012</ref><ref type="bibr" target="#b11">Beaumont, 2019)</ref>, examining the similarity between model-simulated data and that produced by human participants. ABC is used here for two key reasons: first, traditional model fitting via likelihood estimation is challenging for long prediction series such as those targeted here as each data point is related to its predecessors, meaning a series of predictions must be examined as a high dimensional object rather than many independent samples. ABC circumvents this issue by comparing model simulations with human observations on selected summary statistics, using the difference in these measures between models and participants to quantify model fit.</p><p>Second, ABC allows for a particular focus on the key features of human predictions targeted in this study by selecting these measures as the summary statistics, meaning models are evaluated on their ability to produce these specific properties rather than the raw match between human and model price predictions. While these features are not necessarily the sufficient statistics of the fitted data, it is highly difficult to define the true sufficient statistics for the complex human-generated series targeted here, if such statistics even exist. We therefore use the previously described empirical measures (see <ref type="figure" target="#fig_0">Figure 2</ref>) as a proxy for these values, evaluating model performance according to the expression of these aspects.</p><p>We use two forms of ABC to compare empirical data against the candidate models.</p><p>First, we use an approximate likelihood method to estimate the fit of each simulation by placing a kernel around its generated summary statistics and using this to calculate approximate likelihoods for the participant measures <ref type="bibr" target="#b105">(Turner &amp; Van Zandt, 2012</ref>. While these are not traditional likelihoods, they can be used in a similar fashion to compare the ability of the considered models to capture the target behaviours, allowing for designation of best-fitting models at both the individual and aggregate levels, as well as approximate distributions across parameters and measures. Second, we use a random forest (RF) analysis in which machine learning techniques are trained on the distribution of statistics from each model in order to predict the most likely assignment for each participant given their individual measures <ref type="bibr" target="#b82">(Pudlo et al., 2016)</ref>. RFs are advantageous as the use of machine learning allows for determination of the most useful measures for model discrimination from the data itself rather than requiring this to be defined a priori.</p><p>This also allows RFs to better scale when the number of summary statistics is high, whereas</p><p>the likelihood kernel method suffers as its dimensions increase (the so-called 'curse of dimensionality').</p><p>At the same time, however, RFs do not provide the same level of detail as the likelihood method in their fits, offering only the most common assignments for each participant (or a single posterior probability of that model versus all others), as opposed to distributions across all models or parameters. We therefore utilise both methods here to offer parallel perspectives on model performance, using the strengths of each approach to support the limitations of the other.</p><p>Further details on the implementation of ABC are given in Appendix B, while code used in this analysis is available on our OSF page at: https://osf.io/h57a8/?view only= d62f758c0e0b45018fa9045d2002744d.</p><p>Model comparisons involved all possible combinations of the two formats, two domains, nine prediction mechanisms and two response noise types, though as the FIRE and Direct Sampling predictors are insensitive to representation, these models were collapsed across format and domain, leading to 60 complete models. For ease of presentation, however, we here aggregate results across response noise types as these can be treated as differing values on a consistent dimension: models without response noise essentially have a noise variance parameter of 0, meaning trials of both noise types can be treated as samples from a common 'spike-and-slab' prior according to their relative trial counts (here being equal). The following results therefore also marginalise across this prior to simplify reporting, providing 30 total candidates. We do however separate the noise types when discussing the performance of the best scoring models below to assess the impact of this factor, though note that this does alter some best fitting models at the participant level for the approximate likelihood method due to changes in the marginal likelihoods between models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Recovery</head><p>Before proceeding to results of the comparison, we should first examine the ability of these methods to identify and separate the candidate models. This can be assessed using model recovery exercises to determine the discriminability of the models within both the <ref type="figure">Figure 3</ref>: Model assignment rates for the simulated data using the approximate likelihood analysis. approximate likelihood and random forest methodologies, though the precise procedure of these exercises does slightly differ between the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Likelihood</head><p>For the approximate likelihood method, 100 simulated data sets were generated for each model using uniform sampling across their respective parameter spaces. Each artificial data set was then fit back to other model simulations using a similarity kernel around their generated statistics (outlined in Appendix B), providing marginal likelihood values for each data set from each model. The proportions of simulations best fitting each candidate model were then recorded, collapsing across response noise by treating these assignments as equivalent to reduce the size of the model space to 30 candidates.</p><p>Assignment rates for each generating model are summarised in <ref type="figure">Figure 3</ref>; model recovery rates were generally moderate, with a mean of 0.387 (±0.079 95% CI), though some such as the Bayesian and MC 3 predictors had higher accuracy. This also finds certain models were better able to capture the data of other generative processes: Direct Sampling and FIRE in particular show moderate fits to several other model's data, as does AR1 in log change. While this could raise some concerns with the accuracy of this methodology, it is notable that these rates are often still considerably higher than might be expected given the breadth of the present model space: even with the current reduction to 30 candidate models, most rates are above chance level assignment, with 23 models being significantly above this criterion at the 5% level (based on a binomial test with n = 100 and p = 1/30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forest</head><p>For the random forest analysis, a measure of discriminability is provided by the accuracy of this method is assigning the simulations used as training data to their true generating process (so-called 'out-of-bag' error). <ref type="figure">Figure 4</ref> shows the accuracy rate for the candidate models in these assignments, finding similarly moderate recovery rates overall (mean = 0.536 ±0.088 95% CI), again partially due to the breadth of the model space, though these were notably higher than those of the likelihood method. In addition, all but one model were best matched by their true generating process (with the exception being Exponential Smoothing in standard change), and far above chance level. As with the approximate likelihood method, there was some confusion between certain models, in this case particularly between the MCMC and MC 3 predictors, and between the Delta Rule and Exponential Smoothing predictors, though such confusion is understandable given the similarity of these mechanisms (MCMC matches MC 3 with one chain, and Delta Rule matches Exponential Smoothing with no trend terms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fitting Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Likelihood</head><p>Each model was evaluated using an ABC process to estimate its marginal likelihood for each participant, with the product of these individual likelihoods providing an aggregate <ref type="figure">Figure 4</ref>: Model assignment rates for the simulated data using the random forest analysis. score across subjects. <ref type="table" target="#tab_0">Table 1</ref> gives results for the five best scoring models by aggregate marginal likelihood, with results from all models being listed in Appendix C. For concision, we discuss only the three best scoring models in detail; features of these models are illustrated in <ref type="figure" target="#fig_1">Figure   5</ref>, while <ref type="figure">Figure 6</ref> shows average likelihood distributions across parameter values. Complete simulation results are also available on our OSF page at the link given above.</p><p>The MC 3 predictor in log price held the best fit by aggregated likelihood, and best accounted for the largest proportion of participants at the individual level with a rate of 46.3%.</p><p>The predictions of this model also show a strong correspondence to participant behaviour: measures showed close adherence to the target behaviours for autocorrelations and heavy tails, though the spectral density line does appear to stray from the fit line for higher frequencies.</p><p>As shown in <ref type="figure">Figure 6</ref>, marginal distributions over model parameters show reasonably stable performance across sampled values, though particularly small step sizes show a deficit in fit.</p><p>It is also notable that the distribution across chain count suggests little impact of the number of chains on fit, though the advantage of MC 3 over MCMC does suggest that the additional chains of this predictor were helpful in matching the target properties in this comparison.</p><p>Separating the samples with and without response noise, simulations without additional noise both performed better by aggregate marginal log likelihood (noisy: -64.46; non-noisy: -54.76) and best captured a higher proportion of participants at the individual level (31.7% vs. 4.88%), offering strong evidence that additive noise was not the reason for this model's success.</p><p>The second-best scoring model was the MCMC predictor in log price, which best accounted for 26.8% of participants. Predictions of this model are also fairly consistent with the target behaviours, while marginal parameter distributions again show stable likelihood across starting points, though here moderate step sizes show a clearer advantage. This may reflect a stronger dependence in fit on the step size parameter for the MCMC mechanism compared to MC 3 above: movements in MCMC are solely driven by the proposal distribution whose width is captured by the step size, whereas movements in MC 3 are also driven by swapping of chains, leading to less focus on particular step sizes. Again separating noisy and non-noisy samples, in this case model simulations including response noise showed higher aggregate fit values (noisy: -55.66; non-noisy: -82.57), but the non-noisy version of this model best fit a higher proportion of participants than its noisy equivalent (24.4% vs. 2.44%). The noisy version's advantage in aggregate likelihood thus appears to result from improved fit to those not best fit by this model initially, whereas those who were best fit are better captured by the non-noisy version. This could indicate that additional noise leads to less variation in fit between  subjects, thus producing a better aggregate measure of fit, but leading individual fits to be less distinct.</p><p>Finally, the third-best model by aggregate likelihood was the Delta Rule predictor in standard change, though this held a substantially lower likelihood than the preceding models, and only accounted for 2.44% of participants. Similar to the other top scoring models, this showed reasonable correspondence to participant behaviour in our key criteria, though unlike the previous models, changes between predictions here appear to show greater variability, with higher absolute log changes. There is also higher variability in the autocorrelation function in changes, though this predominantly remains within the bounds around zero, while the empirical quantiles show less deviation at the centre of the range. As with the previous models, this model showed stable performance across starting point, though moderate learning rates show a clear advantage, with an additional cut-off for high learning rates as these tended to produce negative price predictions in the standard change representation. Simulations including response noise held a higher marginal log likelihood value (noisy: -95.59; non-noisy: -102.50), though the non-noisy version again best fit a higher proportion of participants at the individual level (2.44% vs. 0%).</p><p>Autocorrelated sampling models thus place first and second in this comparison, accounting for a combined 73.2% of participants, with a further 9.76% being captured by the MC 3 predictor in standard price, though this placed ninth by aggregate likelihood. Delta Rule predictors also score well by aggregate likelihood, but hold much lower fit rates at the individual level, with a total of 4.88% across representations. The remaining participants meanwhile are divided between the Direct Sampling (2.44%), AR1 (4.88%) and Memory Weighted (4.88%) predictors, though these models did not perform as well according to aggregated likelihood. Sampling models thus appear to dominate this comparison, suggesting strong evidence that these mechanisms offer the best accounts for the behaviour observed in our experimental data. What is more, the above model recovery exercise shows that these results are unlikely to be attributable to the sampling models merely better capturing other generative processes, as both MCMC and MC 3 in log price show relatively little confusion with other models.</p><p>It is also valuable to examine model fit not just in relative terms as in the above comparisons, <ref type="figure">Figure 6</ref>: Binned mean likelihoods across both simulations and participants for each parameter of the three best scoring models by aggregate likelihood.</p><p>but also in absolute terms: a model may perform best within a given set of candidates, but does this offer an accurate depiction more generally? To assess this, we also looked at the posterior predictive distributions across the five key measures from the top scoring models, contrasting these with the equivalent distribution from participants, as illustrated in <ref type="figure" target="#fig_2">Figure 7</ref>.</p><p>Such distributions demonstrate reasonable correspondence between participants and models, though participant values do show greater variation; this is particularly notable for the tail index measure, which shows a stronger tendency towards zero than demonstrated by participants.</p><p>In addition, kurtosis values are lower for the models than the participants, with the exception of the MCMC predictor in log price. This might suggest that these models may benefit from alterations to further encourage heavy tails in order to match the higher levels seen in real forecasters. In the case of the sampling models, this could be achieved by simply sampling from a heavy tailed distribution rather than the present Gaussian, though this would of course mean that these mechanisms would no longer be drawing from the true change distribution of the random walk target. This being said, there is no guarantee within these models that forecasters have an accurate mental representation of the true distribution, meaning such distorted mental distributions are possible. We do not however consider such extensions further here in order to maintain focus on behaviour arising from the operations of the sampling method rather than aspects of the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forest</head><p>A random forest was trained on the same set of simulations used in the approximate likelihood method, using the collected distributions of the five key measures from each model to produce predicted assignments for each participant. As before, we collapsed the FIRE and Direct Sampling models across format and domain given that these predictors are insensitive to representation, providing a total of 60 models, though sample counts for all models remained equal to ensure a uniform prior. We also again collapse the results here over response noise to simplify reporting, though the RF was trained on this distinction.</p><p>Much like the likelihood method, results from the RF analysis showed strong support for the sampling models: 53.7% of participants were classified as MCMC in log price (39.0% with response noise, 14.6% without), 22.0% as MC 3 in log price (all without response noise), 12.2% as MCMC in standard change (4.88% with response noise, 7.32% without), 9.76% as MC 3 in standard price (all without response noise), and 2.44% as MCMC in log change (all without response noise), meaning all participants were assigned to sampling methods. The RF analysis thus conforms with the likelihood comparison above, suggesting the particular patterns shown by human learners best correspond with use of cognitive sampling methods, further bolstering support for these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternate Forecasting Properties</head><p>Thus far, our analyses have focused on a particular set of characteristics used to investigate deviations from idealised random walk structures in financial research <ref type="bibr" target="#b101">(Shiller, 1981;</ref><ref type="bibr" target="#b65">LeRoy &amp; Porter, 1981;</ref><ref type="bibr" target="#b70">Mantegna &amp; Stanley, 1999;</ref><ref type="bibr" target="#b24">Cont, 2001)</ref>. While the use of these features does offer a novel lens into forecasting behaviour, we next examine some other measures used in previous studies, namely the psychophysical kernel of predictions, the deviation of forecasts from rational solutions, and potential optimistic biases in expectations. In addition, we also use this as an opportunity to further investigate an aspect raised in our preceding analyses which may deserve particular attention: the first lag of the autocorrelation function. The following section introduces these properties in more detail and examines their patterns in both the empirical and simulated data. For this purpose, we present posterior predictive measures for these features when considering the correspondence between human and simulated predictions on both these new properties and the original measures defined above. We then apply these measures as additional model fitting criteria to examine the impact on model performance. <ref type="figure">Figure 8</ref>: Illustrations of the novel features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations). Column A shows the psychophysical kernel of predictions over the previous 5 price points; column B shows the mean absolute log deviation from the rational prediction; column C shows mean predicted log change as a measure of optimism; and column D shows the autocorrelation in the changes between predictions, focusing on lag 1. Shaded regions indicate 95% confidence intervals, and black dashed lines indicate equality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psychophysical Kernel</head><p>The psychophysical kernel is a statistical tool used to examine the dependence of responses on previous observations <ref type="bibr" target="#b72">(Neri &amp; Heeger, 2002;</ref><ref type="bibr" target="#b73">Nienborg &amp; Cumming, 2009;</ref><ref type="bibr" target="#b76">Okazawa, Sha, Purcell, &amp; Kiani, 2018;</ref><ref type="bibr" target="#b112">Waskom, Okazawa, &amp; Kiani, 2019</ref>). In the present task, this can be used to capture the relation of new predictions to previous price points; while new values in the random walk target by definition depend only on their immediate predecessors, this may not apply to human forecasts of this series given the deviations from rationality described previously. The kernel is captured here via a regression of predictions over the recent history of prices, examining the influence of past values on subsequent forecasts. We therefore performed linear regressions on the prediction series of each participant and model simulation using the five preceding price values as predictors for each forecast. The kernel window was limited to the five previous points (that is, p t−4:t ) to simplify the use of these factors in model comparison, detailed further below.</p><p>Results from this analysis are illustrated in column A of <ref type="figure">Figure 8</ref>, showing that participants appear to closely follow the dependence structure of the true series: predictions strongly depend on the most recent price value, but earlier values have no substantive influence. This may again reflect the lack of autocorrelations in responses, mirroring the spectral density results examined in the previous analysis: by this metric, participant predictions do follow the dependency structure of the series, whereas it is the distribution of movements which differs. The topscoring models meanwhile capture this pattern reasonably well, though MCMC in log price shows a notably high coefficient for the earlier points, while Delta Rule in standard change shows a negative coefficient for the preceding price value, potentially indicating some oscillation in predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deviation from Rational Predictions</head><p>A factor often examined in previous forecasting studies is the deviation of predictions from rational solutions <ref type="bibr" target="#b111">(Wagenaar &amp; Sagaria, 1975;</ref><ref type="bibr" target="#b85">Reimers &amp; Harvey, 2011;</ref><ref type="bibr" target="#b58">Kusev et al., 2018</ref>). As previously noted, because the rational prediction strategy for the present random walk target is to simply repeat the latest value, the rational forecast series should mirror the properties of the target; as such, the deviation in measures between the target and participant responses noted above implies behaviour strays from optimality in this task. Indeed, as shown in Appendix D, the rate of precise repetitions in participant forecasts is very low, with a mean of 6.64%, reinforcing that behaviour here deviated from this rational solution. This is however a somewhat blunt measure of rationality, giving no consideration as to the degree of optimality of individual predictions. As such, we here consider this aspect more directly by examining the precise numerical deviation of each prediction from its particular rational value (that is, the preceding price).</p><p>To provide a quantitative measure of deviation from rationality, we used the mean absolute difference between each log prediction and its preceding log price for each participant and simulation, where perfectly rational responses should produce a value of zero. These deviations are illustrated in column B of <ref type="figure">Figure 8</ref>: participant scores showed moderate deviation on this measure, with a mean of 0.121 (±0.016), suggesting participants did consistently deviate from optimal forecasts in their individual responses. Furthermore, a supplementary analysis of this aspect (summarised in Appendix D) finds this measure was also stable across trials, implying participants also showed no learning of rational solutions with experience.</p><p>Deviation measures are, however, substantially higher for the models, particularly those using sampling mechanisms, as demonstrated when comparing the empirical values against their posterior predictive equivalents: MC 3 in log price shows a mean deviation of 0.250 (±0.007),</p><p>while MCMC in log price shows a mean deviation of 0.327 (±0.012). The Delta Rule predictor in standard change meanwhile shows a closer match to participant levels, though this is again slightly higher than the empirical values (mean = 0.161 ±0.012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimism</head><p>Optimism refers to a bias observed in past studies of prediction in which forecasters expect new values to be higher than their true expected value, particularly for targets such as prices or sales where higher values are more desirable <ref type="bibr" target="#b63">(Lawrence &amp; Makridakis, 1989;</ref><ref type="bibr" target="#b44">Harvey &amp; Bolger, 1996;</ref><ref type="bibr" target="#b75">O'Connor, Remus, &amp; Griggs, 1997;</ref><ref type="bibr" target="#b85">Reimers &amp; Harvey, 2011)</ref>. As the present random walk target has no consistent trend, this would thus be expressed here as a bias towards positive movements in price; we therefore measured optimism according to the mean expected movement from the most recent price point, again using log space to account for differences in scale. This is in fact a very similar measure to that used for deviation from rational expectations above, though here the sign of the predicted change matters: positive values imply optimism, while negative values imply pessimism.</p><p>Empirical measures of optimism are illustrated in column C of <ref type="figure">Figure 8</ref>, showing participant expectations were not in fact optimistic in this task in general: mean predicted log movement across participants was -0.005 (±0.009), suggesting expectations were generally unbiased, though this varied somewhat between individuals. The current data thus does not appear to replicate previously observed optimism effects, placing our data in line with recent evidence suggesting optimism may not be as reliable as previously assumed <ref type="bibr" target="#b99">(Shah, Harris, Bird, Catmur, &amp; Hahn, 2016;</ref><ref type="bibr" target="#b18">Burton, Harris, Shah, &amp; Hahn, 2022;</ref><ref type="bibr" target="#b81">Powell, 2022)</ref>, though this does come from belief updating tasks rather than forecasting, so caution should be taken when generalising between these tasks. Model predictions meanwhile also tend to fall around zero, though the range of simulated values is notably narrower, while the Delta Rule predictor in standard change shows a slight pessimistic bias (-0.022 ±0.004) compared to the MC 3 (0.004 ±0.003) and MCMC (0.014 ±0.004) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autocorrelation Function at Lag 1</head><p>While not a key measure in previous forecasting work, the autocorrelation at lag 1 captures an effect observed in our existing measures which may deserve further attention:</p><p>as previously noted, human predictions display little evidence of autocorrelations in their changes, as demonstrated in the low rate of autocorrelation coefficients differing significantly from zero in our preceding comparison. There is however a notable exception to this in the first lag, which tends to show substantial and significant negative correlations: participants displayed a mean coefficient of -0.325 (±0.024) for this value. This might reflect a particular reaction to feedback, with forecasters having to adjust their predictions in light of new information, and so could offer an additional signature of human behaviour not captured in the overall significant rate used above; while <ref type="figure" target="#fig_1">Figure 5</ref> does show the top scoring models in the previous comparison also tend to display such a negative autocorrelation, this was not assessed quantitatively, being only one lag within the significant rate. We therefore take this value as its own distinct measure in this expanded comparison. Model measures show close correspondence with this effect, though each of the top three scoring models does tend slightly closer to zero (MC 3 : -0.298 ±0.013; MCMC: -0.288 ±0.030; Delta Rule: -0.286 ±0.015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanding the Model Comparison</head><p>Having introduced these new behavioural measures, we next apply these as additional criteria for model fitting. Such an expansion is unfortunately somewhat difficult within the approximate likelihood methodology as this relies on increasing the dimensions of the likelihood kernel, and risks overweighting certain features reflected by multiple measures (such as the multiple coefficients of the psychophysical kernel). In contrast, the random forest methodology is better able to scale with higher counts of summary statistics, as this analysis infers the most useful criteria directly from the simulated data. We therefore restrict this expanded analysis to the random forest procedure, calculating the psychophysical kernel coefficients, mean deviation from rational predictions, level of optimism and lag 1 ACF value for the existing simulations and including these as additional potential classification criteria for a new random forest. The equivalent empirical measures were then passed to this new forest to predict model assignments for each participant.</p><p>Results from this analysis are summarised in <ref type="table">Table 2</ref>. Assignments are slightly more divided here than the previous RF analysis, though this still shows strong support for sampling models overall: a total of 70.7% of participants were assigned to either MCMC or MC 3 predictors, though change formats do perform better in this case. In addition, the AR1 predictor in standard change notably performs better here, capturing the third-most participants, whereas this model was assigned to no participants previously, potentially suggesting this model is particularly well suited to these new criteria. It is also notable that models including response noise perform substantially better in this analysis, being assigned to 80.5% of the sample (compared to 43.9% previously).  <ref type="table">Table 2</ref>: Model assignment rates from the random forest analysis when using the psychophysical kernel, deviation from rationality, optimism and lag 1 ACF as additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalising to New Series: Experiment 1</head><p>The above sections indicate that autocorrelated sampling mechanisms provide good fits to human behavioural patterns when predicting a random walk series, but have so far been restricted to a single instance of this function. This may lead to concerns with the generality of these findings, as results may differ when considering alternative target series. We thus next seek to address this concern by examining predictions of a new target series with different statistical properties, though our focus remains on random walk functions for the reasons stated previously. This also offers an opportunity to examine additional features that were not possible in the previous series.</p><p>We therefore ran a new experiment to provide human prediction data for comparison with model simulations. This experiment was highly similar to that in our previous paper <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>, though two key changes were made to the design to allow for examination of novel aspects. First, while the target series is again a Gaussian random walk, the parameters of this series were altered, leading to distinct statistical properties. In particular, the mean of the change distribution is no longer zero, allowing for overarching trends in price:</p><formula xml:id="formula_20">ln(p t ) = ln(p t−1 ) + ε t (28) ε t ∼ N(µ, σ 2 )<label>(29)</label></formula><p>where positive values of µ make upward movements more likely, and vice versa. This change was made to allow for examination of reactions to trends in the target series, adding this as a new manipulation. Second, rather than predicting a single series, each participant predicted two distinct series in separate blocks, allowing for assessment of reactions to differing trends, as well as model performance across different targets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>As with the experiment of <ref type="bibr" target="#b116">Zhu et al. (2021)</ref>, this experiment involved repeated predictions of the price of a fictional financial asset (as previously illustrated in <ref type="figure">Figure 1B)</ref>. A key difference from the previous experiment however was that each participant completed two blocks of this task, each focused on a distinct series differing in their underlying trend: one block used a positive trend (µ = 0.01), while the other block used a negative trend (µ = -0.01) 4 . To make these trends apparent whilst also maintaining random movements, the standard deviation of the step distribution σ was reduced to 0.05 for both trend conditions. Each block lasted 25 minutes to fit a total experimental duration of 1 hour; this naturally limits the number of trials in each condition, but allows for examinations of model fits across trend types for each participant.</p><p>Procedure Experiment 1 followed a highly similar procedure to that of <ref type="bibr" target="#b116">Zhu et al. (2021)</ref> as described above, with a few alterations. First, participants completed two 25 minute blocks (one positive trend, one negative trend, randomly ordered) rather than a single 1 hour task, with a 1 minute interval between the blocks. Second, a time limit was placed on each trial to encourage participants to respond quickly in order provide reasonable response counts for analysis, though the limit was set to 60 seconds to avoid missing responses. Third, the underlying log price series were each exponentiated and multiplied by 500 (rather than 100) for presentation to participants to allow more space for downward movements in the negative trend case. Fourth, the reward function was altered to provide a higher base payment and lower maximum:</p><formula xml:id="formula_21">reward = £8 + 2 * exp(−9(|ln(p i ) − ln(p e i )|))<label>(30)</label></formula><p>where i is again a randomly selected trial, here sampled across all trials from both blocks. This again means the rational predicted movement on any trial is equal to the median of the step distribution, though here this is equal to the trend term µ in that block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>Data from this experiment and code for the subsequent analysis is available on our OSF page (https://osf.io/h57a8/?view only=d62f758c0e0b45018fa9045d2002744d).</p><p>Definition of sample size is given above, all data exclusions are described below, and we report all manipulations and measures used in this study. This experiment was not preregistered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Similar exclusion criteria to those of <ref type="bibr" target="#b116">Zhu et al. (2021)</ref> were applied to the data, with slight alterations to suit the design changes: data were only accepted if participants completed a minimum of 175 trials in each block (half the previous limit), and reported no interruptions during the task. 43 participants met these criteria for further analysis. Mean response counts were 246 (±11.0) for the positive trend condition and 272 (±13.9) for the negative trend condition.</p><p>As with the previous data set, predictions which differed from both their preceding and subsequent responses by a factor of 10 were treated as errors and replaced with a non-number filler; this removed 0.17% of responses across participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Measures</head><p>We shall first examine the empirical distributions of the 5 key measures which were the initial focus of this paper in the new data set as these offer our primary assessment of deviations from random walk structures. <ref type="figure">Figure 9</ref> shows measure distributions from predictions of both the positive and negative series, demonstrating reasonably similar properties between conditions. As with the previous data set, the rate of significant autocorrelations was again low, though in this case more participants fell below the true levels of the series (positive mean: 0.033 ±0.009, negative mean: 0.039 ±0.008). Spectral density slopes were again generally flatter than those of the target series, but were closer in the negative trend case, while both showed wider ranges than the previous task (positive mean: -1.76 ±0.09, negative mean: -1.81 ±0.07). Variance is naturally lower than in the previous data set given the reduction in the true variance for both conditions, though this was still predominantly above the actual level of the targets (positive mean: 0.058 ± 0.019, negative mean: 0.035 ±0.012). Tail indices were again mostly above 0, implying heavy tails in both conditions, though as with the previous data set some participants did also fall below this level (positive mean: 0.80 ±0.30, negative mean:</p><p>1.67 ±1.78) 5 . Finally, kurtosis again tended above Gaussian levels, though here this is more <ref type="figure">Figure 9</ref>: Histograms for the five key measures from the two trend conditions of Experiment 1. Dashed lines indicate values from the target series. skewed with some participants having particularly high values (positive mean: 44.8 ±12.3, negative mean: 38.4 ±12.2). Comparisons of measures between conditions using paired t-tests found only one significant difference in variance, being higher for the positive series (t(84) = 2.09, p = .040).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Recovery</head><p>Given the difference in the structure of this experiment, we repeated the model recovery exercise described above to re-examine the ability of these methods to separate the candidate models, in this case considering fit across the two trend conditions simultaneously. <ref type="figure">Figure 10</ref>: Model assignment rates for the simulated data from the trend experiment using the approximate likelihood analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Likelihood</head><p>As with the previous data set, 100 simulations were generated for each model, here predicting both trend conditions using a common parameter set. These were then fit back to the candidate models to produce assignment rates between each true and fitted model, as summarised in <ref type="figure">Figure 10</ref>. Rates were broadly similar to those of the previous data set: the mean recovery rate was 0.442 (±0.074), falling slightly above the mean rate found above, though there was again a fair amount of variation between models. All models held recovery rates significantly above chance levels (1/30), while 28 were best matched by themselves, with the exceptions both being Exponential Smoothing models due to their confusion with the Delta Rule mechanism. The Direct Sampling and FIRE mechanisms again show a tendency to capture the data of other models, though this is not as prevalent for AR1 in log change here.</p><p>Discriminative ability thus appears very similar between this experiment and the previous task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forest</head><p>Recovery rates for the random forest analysis were again determined according to the assignment of simulations used as training data by the decision trees to their true generating process, as summarised in <ref type="figure">Figure 11</ref>. As with the previous data set, recovery rates for the random forests were higher than the approximate likelihood method, with a mean of 0.516 (±0.069), though the margin of this advantage is slightly narrower here. All but two models were best matched by their true generating process, with the exceptions again being the Exponential Smoothing mechanism in change formats due to its continued confusion with the Delta Rule predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head><p>Data from the new experiment were passed through the same model comparison procedure used for the previous data set, calculating both approximate marginal likelihoods for each model and predicted assignments for each participant from a random forest trained on the simulated data. As the task used a within-participants design, we fit both trend conditions together using common parameters under the assumption that these were stable aspects of each individual, meaning likelihoods reflect performance across series. Model definitions were the same as those given above, though as the FIRE, Direct Sampling, MCMC and MC 3 mechanisms make use of the true change distribution, this naturally differed both from the previous data set and between trend conditions. <ref type="table" target="#tab_4">Table 3</ref> lists results for the five best scoring models by aggregate marginal likelihood across participants for the trended series experiment, while <ref type="figure" target="#fig_0">Figure 12</ref> provides a comparison of the properties of the top scoring models against participant behaviour. As in the previous data set, autocorrelated sampling algorithms in price formats performed well by this metric, <ref type="figure">Figure 11</ref>: Model assignment rates for the simulated data from the trend experiment using the random forest analysis. placing first, second and third, though these do differ in domain here. In contrast to the previous results, however, rational models also performed well, with FIRE placing fourth, followed closely by Bayesian models seeking to infer the appropriate change distribution. Delta Rule mechanisms meanwhile perform substantially worse here, with the previous third place model falling to fourteenth. There is also greater division in the individual fits, with no model best capturing more than 18.6% of participants. We shall again focus discussion here on the three best scoring models for concision, with full results given in Appendix C. Parameter distributions for these models are shown in <ref type="figure" target="#fig_4">Figure 13</ref>, while <ref type="figure">Figure 14</ref> compares posterior predictive measure values against their empirical equivalents, here separated by trend condition. MCMC in standard price held the highest aggregate likelihood and fit rate for this data set, in contrast to its lower placement in the previous comparison. Model predictions showed similar statistical features to the empirical data in the ACF and PSD slope, as well as heavy tails in the change distribution, though kurtosis was still generally lower than empirical levels, and demonstrated some differences between trend conditions. Parameter distributions show better fits with more central starting points, but were fairly stable across step size. This is likely due to the model here having to simultaneously account for both upward and downward trends: while intermediate starting points offer a compromise between the trend conditions, because step size does not scale with price in the standard domain, no parameter value holds an advantage as any benefit in one condition may be counteracted by a deficit in the other due to differences in price magnitude. Separating by response noise, simulations without additional noise performed better by both aggregate likelihood <ref type="bibr">(−253.90 vs. −305.60</ref>) and fit rate (18.6% vs. 0%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Likelihood</head><p>The second best scoring model was MC 3 in log price, which ranked first in the previous comparison, though its individual fit rate was substantially lower here. Much like the above MCMC model, simulations again showed similar statistical features to the empirical data in the considered measures, though here deviations from Gaussian standards in changes between predictions appear to persist further towards the centre of the distribution. Parameter distributions were also consistent with those described above, showing a preference for central starting points but reasonably uniform weighting on step size. Temperature and chain count parameters were also fairly stable, though there does appear to be a very slight deficit for the lowest potential chain count. Simulations without response noise performed better by marginal likelihood <ref type="bibr">(−251.25 vs. −307.34</ref>) and fit rate (2.33% vs. 0%), though rates were low for both versions.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forest</head><p>A random forest was again trained on the collected statistics from all model simulations, in this case collecting separate measures for each trend condition. Results from this analysis are summarised in <ref type="table" target="#tab_6">Table 4</ref>; much like the previous analyses, this found strong support for sampling models, with a total of 88.4% of participants being classified to either MCMC or MC 3 mechanisms, though assignments are again more divided than those found in the untrended experiment. It is also notable that these results do not show similarly strong support for rational models to the above likelihood results, with no participants being assigned to either FIRE or Bayesian change models. This may due to the RF focusing more on certain features which specifically discriminate these models, whereas the approximate likelihood method considers  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Features</head><p>As with the previous data set, we next expanded the model comparison to consider additional prediction features used in previous studies of forecasting. This used the same four aspects noted above, calculating the psychophysical kernel, deviation from rational predictions, optimistic bias and lag 1 ACF value for both participant responses and model simulations, in this case separated by trend condition. These were determined using the same methods described above, with the only minor difference being that the rational prediction series now considers the trend of that condition. We shall first give some brief descriptions of these features in both the empirical and simulated data, before investigating how inclusion of these properties as additional criteria impacts model fits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psychophysical Kernel</head><p>Psychophysical kernel results are shown in column A of <ref type="figure" target="#fig_1">Figure 15</ref>. Much like the previous data set, participant predictions showed similar dependence structures to the random walk target for both trend cases, with a strong dependency on the most recent observation but far weaker focus on earlier points, though there does appear to be more variation here between individuals. Results from the models meanwhile are more varied: while MC 3 and MCMC in log price are fairly similar to the empirical results, MCMC in standard price shows substantial deviations, placing greater focus on earlier points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deviation from Rational Predictions</head><p>We again measured deviation from rational predictions by calculating the mean absolute log difference between each prediction and its optimal response, in this case being based on the change distribution of that condition. Distributions of this measure are shown in column B of <ref type="figure" target="#fig_1">Figure 15</ref>; participants again showed consistent deviation from rational responses, with a mean deviation of 0.039 (±0.006) in the positive trend condition and 0.046 (±0.005) in the negative trend condition. Deviation was also found to be significantly higher in the negative condition (t(42) = 2.37, p = .023), most likely because the negative series included more low price values where any deviation represents a larger proportional difference. Model simulations similarly showed persistent deviations from rational values, though this was substantially higher for MCMC in standard price (positive mean = 0.074 ±0.001, negative mean = 0.092 ±0.001) compared to MC 3 in log price (positive = 0.055 ±0.001, negative = 0.055 ±0.001)</p><p>or MCMC in log price (positive = 0.066 ±0.001, negative = 0.070 ±0.001), while posterior predictive measures show narrower ranges than the empirical distributions for all three models.</p><p>In addition, predicted deviations were significantly higher in the negative condition for MCMC in standard price (t(42) = 58.8, p &lt; .001) and log price (t(42) = 5.31, p &lt; .001), mirroring the <ref type="figure" target="#fig_1">Figure 15</ref>: Illustrations of the novel features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations). Column A shows the psychophysical kernel of predictions over the previous 5 price points; column B shows the mean absolute log deviation from the rational prediction; column C shows mean predicted log change as a measure of optimism and trend damping; and column D shows the autocorrelation in the changes between predictions, focusing on lag 1. Colours indicate trend conditions, shaded regions indicate 95% confidence intervals, and black dashed lines indicate equality. empirical data, whereas MC 3 generated no significant difference (t(42) = 1.39, p = 0.172).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimism &amp; Trend Damping</head><p>Optimism was again measured according to mean signed deviation from rational predictions given the true change distribution of each of the two trend conditions. This also allows for assessment of another feature considered in past studies of forecasting, 'trend damping', in which forecasters seemingly mute actual trends in observed series when predicting future data points: compared to their expected value, new values are underestimated for series with positive trends but overestimated for series with negative trends. This behaviour has been observed across a range of target functions, including linear <ref type="bibr" target="#b63">(Lawrence &amp; Makridakis, 1989;</ref><ref type="bibr" target="#b58">Kusev et al., 2018)</ref>, power law <ref type="bibr" target="#b45">(Harvey &amp; Reimers, 2013)</ref> and exponential series <ref type="bibr" target="#b111">(Wagenaar &amp; Sagaria, 1975)</ref>, finding errors of approximately 5-10% of the target depending on series type, presentation method or noise level. Trend damping did not apply to the previous data set as that target series had no trend to damp, but can be investigated in this case by considering how predictions relate to the true trend: positive mean deviation values thus suggest damping for negative trends, and negative mean deviation values suggest damping of positive trends.</p><p>Mean signed deviation values are illustrated in column C of <ref type="figure" target="#fig_1">Figure 15</ref>; much like the previous data set, participant responses did not show evidence of optimism, in this case in fact suggesting trend damping, with predicted movements being significantly below the trend term in the positive condition (M = −0.011 ± 0.007,t(42) = 3.11, p = .003) but significantly above the trend term in the negative condition (M = 0.012 ± 0.006,t(42) = 4.15, p &lt; .001). Such a pattern is also shown in the predictions of the sampling models (MCMC in standard price: positive = −0.031 ± 0.001, negative = 0.037 ± 0.002; MC 3 in log price: positive = −0.017 ± 0.001, negative = 0.017 ± 0.001; MCMC in log price: positive = −0.027 ± 0.002, negative = 0.024 ± 0.002), though in all three cases the level of difference between conditions is notably higher than the empirical results, indicating stronger trend damping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autocorrelation Function at Lag 1</head><p>We again isolated the first lag of the autocorrelation function as its own measure as participant responses again demonstrated substantial negative coefficients at this lag in both trend conditions: mean values were −0.354 (±0.053) in the positive case and −0.326 (±0.049) in the negative case. Similar behaviour was also demonstrated by the top scoring models, though at a slightly lower magnitude: all three models showed initially negative coefficients at the first lag (MCMC in standard price: positive = −0.066 ± 0.006, negative = −0.078 ± 0.008; MC 3 in log price: positive = −0.185 ± 0.011, negative = −0.214 ± 0.016; MCMC in log price: positive = −0.117 ± 0.011, negative = −0.113 ± 0.008), though this effect appears to persist through subsequent lags for the MCMC mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanding the Model Comparison</head><p>As with the previous data set, we restricted analysis of these additional properties to the random forest method to avoid overloading the likelihood kernel with a high count of dimensions. A new random forest was trained on the expanded set of statistics from all simulations across models, again taking separate measures for each trend condition. Predicted assignments from this analysis are given in <ref type="table">Table 5</ref>. Rates are again more divided when including these additional features, though sampling mechanisms continue to perform well: a collected 79.1% of participants were assigned to these models, with the majority being MC 3 . Much like the previous data set, the expanded measures also increase support for samplers using change formats, suggesting these representations may better suit these additional properties.</p><p>In addition, rational models again perform substantially worse here than by the likelihood metrics above, capturing no participants, further suggesting these models are unable to capture the distinctive elements of human behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The collected model comparison results given above provide evidence that the MC 3 prediction mechanism is best able to account for the specific features of human forecasts examined here, performing well across fitting methods, considered features and data sets. This was closely followed by MCMC models, which are themselves subforms of MC 3 with reduced exploratory ability, offering further evidence in support of autocorrelated sampling algorithms.</p><p>In contrast, rational models performed relatively poorly here: while the FIRE and Bayesian learning predictors had some success in fitting predictions of trended series, these methods  <ref type="table">Table 5</ref>: Assignment rates from the random forest analysis for Experiment 1 when using the psychophysical kernel, deviation from rationality, optimism and lag 1 ACF as additional features. rank lower when aggregating evidence across experiments, and do not appear to capture the distinctive features of human behaviour. This reinforces the observation that behaviour in this task specifically deviates from rational principles, naturally implying that rational methods, whether based on perfect knowledge of the generating process or learning through observation, will offer limited descriptions.</p><p>Such results suggest strong support for sampling models of cognition: the particular deviations from rational behaviour observed in our empirical data are best explained by the use of stochastic mechanisms which introduce additional variability into judgments. Indeed, sampling explanations are dominant in these results: collecting across the two data sets, the three sampling mechanisms considered in this comparison best account for a combined 68% of participants by approximate likelihood and 94% by random forest classification (75% with the expanded set of measures), providing substantial evidence for their use. Moreover, these methods are here shown to provide an equivalent or even better match to behaviour than a number of other established learning mechanisms, including Bayesian, error-based learning, and memory-weighting systems. This support for sampling corresponds with findings in other areas of psychology: recent work has found sampling methods provide explanations for various biases in judgments and estimates beyond iterative predictions <ref type="bibr" target="#b26">(Dasgupta et al., 2017;</ref><ref type="bibr" target="#b67">Lieder et al., 2018;</ref><ref type="bibr" target="#b115">Zhu et al., 2020;</ref><ref type="bibr" target="#b96">Sanborn et al., 2021;</ref><ref type="bibr" target="#b103">Spicer et al., 2022;</ref><ref type="bibr" target="#b117">Zhu et al., 2023)</ref>. The present results then contribute to a growing body of evidence for a general sampling approach to cognition used across many tasks and scenarios in which the limitations of human cognitive resources lead to deviations from what would traditionally be considered optimal behaviours.</p><p>Sampling models of cognition also raise an important distinction regarding the role of noise in human predictions: in contrast to sensory noise where external signals are not perfectly captured by the mind or response noise where agents make errors in translating their internal beliefs into actions, sampling methods include computational noise within the judgment process itself. This is an important distinction as this noise is thus inherent to cognition, and so influences the signatures of this process; for example, sequential dependencies between mental samples may lead to apparent autocorrelations in responses <ref type="bibr" target="#b114">(Zhu et al., 2022)</ref>. Additionally, this noise may not be detrimental to performance, but may assist in exploring alternative hypotheses <ref type="bibr" target="#b95">(Sanborn et al., 2022)</ref>. Recent work has suggested that such computational noise is in fact the key determinant of variability in behaviour, as opposed to response or sensory noise <ref type="bibr" target="#b27">(Drugowitsch, Wyart, Devauchelle, &amp; Koechlin, 2016;</ref><ref type="bibr" target="#b32">Findling &amp; Wyart, 2021)</ref>.</p><p>The current results also offer some support for this suggestion, not just in the success of sampling models over deterministic methods, but also through comparisons between models with and without additional response noise: 84.5% of participants across the two tasks were better fit by models without this component by approximate likelihood, though this rate was considerably lower when using the machine learning method (36.9% by the base measures, 53.6% by the expanded measures). Moreover, even where these sampling models include response noise, computational noise remains the dominant source of stochasticity in their forecasts: the current simulations predict 80-90% of variability when forecasts are repeated arises from computational sources versus response errors in the best scoring models (detailed further in Appendix E), though this form of stochasticity was not directly assessed in the empirical data. Further contrasts of these noise types are therefore a key avenue for future work, clarifying the contribution of these aspects to behaviour in prediction tasks. This will likely require more targeted experimental designs to isolate these components, for example having forecasters predict the same sequence multiple times and examining the evolution of their variability across repetitions, or new model comparisons specifically contrasting differing noise definitions. This is not to say that sampling models provide perfect explanations for behaviour, of course: even in these comparisons, there are some features of human predictions which the sampling mechanisms do not exactly reproduce, such as the specific dependency patterns of the psychophysical kernel or the scale of deviations from rational responses. This being said, sampling models do also hold a particular flexibility which was not utilised in these comparisons that may allow for closer matches to behaviour: as the distribution being sampled is psychological, this can be adjusted to capture the particular beliefs of a given individual, for example increasing focus on recent values or making certain movements more likely. The current comparisons restricted the sampling mechanisms to using the true change distribution of each target series to reduce this flexibility and so simplify fitting, but alterations to capture differing subjective assumptions between individuals could improve model performance, offering a valuable line of investigation for future work. Alternatively, rather than assuming the internal distribution a priori, samplers could be combined with other learning mechanisms to acquire these distributions through experience, such as the present Bayesian predictor, which may provide greater variety in the statistical properties of the resulting forecasts.</p><p>On a similar note, the success of sampling methods in these results also does not invalidate other potential cognitive approaches to prediction: the current comparisons selected representative algorithms to reflect key modelling techniques used in the existing forecasting literature, but these are themselves each extensive frameworks, offering many potential methods to capture behaviour beyond the specific mechanisms contrasted here. The observed support for particular sampling models in these data should thus not be used to refute the broader concepts underlying the other candidates (e.g., Bayesian updating, associative learning, memory weighting etc.), simply showing that these sampling algorithms offer a closer match to the features of participant responses than the considered competitors. Such results do however provide an indication of the components needed to capture the distinctive signatures of human behaviour; in this regard, the current findings point to the value of stochasticity in cognition exemplified by sampling mechanisms, suggesting other frameworks may benefit from incorporating such variability in their own systems. Indeed, as noted when initially introducing these approaches above, existing hybrid models demonstrate the possibility of integrating components from differing branches to produce better (if more complex) depictions. It is therefore advisable to consider the present findings within these differing frameworks rather than purely as evidence for a single framework to provide a more complete understanding of the systems underlying human prediction.</p><p>A natural question given these results then is how such findings might generalise to other sequential prediction or production tasks: are these similarly well-explained by the use of sampling methods? Fortunately, the current methodology can be readily applied to these cases, examining noise patterns in such judgments to gain insight into underlying cognitive processes and so distinguish between potential models. Indeed, there has been much work suggesting that consistent noise patterns are exhibited across a wide range of human judgments, leading to the suggestion that this might be a signature of human cognition: for example, research has found evidence of long-range sequential dependencies in estimates of fixed physical quantities such as lengths and time intervals <ref type="bibr" target="#b40">(Gilden et al., 1995;</ref><ref type="bibr" target="#b39">Gilden, 2001;</ref><ref type="bibr" target="#b114">Zhu et al., 2022)</ref>, as well as in the inter-response intervals in generation of category members <ref type="bibr" target="#b114">(Zhu et al., 2022)</ref>. Similarly, real-world series including speech and music have also shown evidence of such dependencies using the same spectral density measures used here <ref type="bibr" target="#b109">(Voss &amp; Clarke, 1975</ref><ref type="bibr" target="#b4">, 1978</ref><ref type="bibr" target="#b56">Kello, Anderson, Holden, &amp; Van Orden, 2008;</ref><ref type="bibr">Hennig et al., 2011)</ref>. Given the current finding that human noise patterns are best explained by the use of mental sampling methods, this might then imply that similar mechanisms also underlie these cases, with agents using sampling algorithms to explore their own mental representations. While the present random walk target is a valuable test bed for models of prediction, these alternate settings will offer a crucial test of the generality of the current findings.</p><p>These results also raise important questions on the rationality of behaviour: given both the observed deviations from rational properties and the relatively poor overall performance of rational methods in this comparison, the current data could be used to argue that human predictions should not be considered optimal. At the same time, however, the success of sampling methods here depicts behaviour from a resource-rational standpoint <ref type="bibr" target="#b66">(Lieder &amp; Griffiths, 2020;</ref>, with participants using sampling techniques to approximate full Bayesian inference with limited cognitive resources. We therefore avoid labelling behaviour in this case as 'irrational', instead offering a different lens through which behaviour should be understood: rationality should be considered within the constraints of our own systems rather than purely objective terms.</p><p>Finally, we should note some limitations of the particular task used here. First, within the current task, predictions are restricted to only the immediately subsequent trial, which contrasts with many real-life decisions in which forecasts can reflect events some distance in the future, or even an evolving series. As such, behaviour may differ when the forecast horizon is extended, meaning further comparisons are required to assess whether the current findings extend to such scenarios. This being said, such an examination will require substantial revisions to the present models to suit alternate tasks, though some existing research does offer definitions for such cases <ref type="bibr" target="#b1">(Afrouzi et al., 2020)</ref>. Related to this, the time scale of the current task is limited to that of an hour-long experiment as oppose to the considerable scales of real predictions which could be over days or months; this is of course a necessary abstraction for laboratory studies, but future work may wish to contrast the present results with longer scale forecasts, potentially using real-world data. A final note is that the current focus on random walk targets is itself a limitation, and may not reflect the more complex series seen in everyday life, though as noted in the Introduction to this paper, the random walk target is advantageous given that its properties can be more easily defined. Nonetheless, extensions to other series will be crucial to test the generality of these findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Iterative predictions of random walk targets provide a valuable lens into how our forecasts are formed, and how they may deviate from optimal answers. In an extensive comparison of potential models, we find sampling approaches offer a particularly compelling candidate for explaining forecasts in such a task, in keeping with a growing literature describing human behaviour using sampling mechanisms. Our results thus fall in line with a general sampling approach to behaviour used across a range of tasks, though of course further work will be required to determine the extent to which such findings generalise to other scenarios. <ref type="bibr">(i.e., Var[ln(p e</ref> t ) − ln(p e t−1 )]). Tail indices were calculated by fitting a Generalised Pareto distribution to the top 5% of changes between log predictions and taking the resulting shape parameter, following the procedure of <ref type="bibr" target="#b30">Embrechts et al. (1997)</ref> and <ref type="bibr" target="#b57">Kotz and Nadarajah (2000)</ref>.</p><p>Kurtosis was taken from the distribution of changes between subsequent log predictions using the kurtosis function of MATLAB R2019b. To be clear, we do not use excess kurtosis, meaning the expected kurtosis value for a Gaussian distribution is 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Measures Between Series</head><p>Both experiments described in this paper used multiple instances of each random walk target to control for any potential issues with any particular instance, but collapsed across these series when reporting empirical measures under the assumption that behaviour can be treated as equivalent between these cases when reduced to the noted summary statistics. We here further assess this assumption by contrasting empirical measures from each series to test for any differences. <ref type="table">Table 6</ref> shows mean measures separated between the two potential series (labelled 'A'</p><p>and 'B') from each data set, with a further separation by trend condition for Experiment 1.</p><p>We performed independent t-tests to compare the measures between the subsets of participants viewing each series, with Bonferroni corrections on the significance criterion given the number of comparisons (α = .05/15 = .003). No significant differences were observed between series in any of the measures at this level, indicating behaviour can be considered as reasonably equivalent across random walk cases. It is notable however that spectral density slopes did</p><p>show near-significant difference for both trend conditions of Experiment 1; this is somewhat surprising given that the only differences between these cases here were in their random movements, with no association between trend conditions. This could then be attributable to individual differences in participants rather than reactions to the series, with participants assigned these series happening to generally show flatter PSD slopes. Further contrasts with a wider range of series are therefore advisable to confirm how robust these results are to differing targets.  <ref type="table">Table 6</ref>: Comparisons of the key measures between alternate series in each data set, including mean values for each series and results from independent t-tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B -Modelling Procedure</head><p>As noted in the main text, model evaluation involved an Approximate Bayesian Computation (ABC) process for each model to determine the correspondence between its predictions and human responses using two parallel methodologies: approximate model likelihood and random forest assignment. To provide data for both analyses, each model was simulated at 10,000 independent parameter sets sampled randomly from the model's prior parameter distribution.</p><p>We used IID sampling rather than the MCMC-like processes of previous ABC methods (e.g. <ref type="bibr" target="#b106">Turner &amp; Van Zandt, 2018;</ref><ref type="bibr" target="#b11">Beaumont, 2019)</ref> to allow for evaluation of likelihoods for all participants simultaneously. To avoid strong assumptions on likely parameter values, all parameters for all models were assumed to have uniform prior distributions across predefined ranges (detailed below). Models predicted the same random walk series given to participants in the experiments, though to simplify fitting, only one series was used in each comparison (with separate series for the positive and negative trend conditions in Experiment 1). Each simulation ran for 512 prediction steps in the first comparison and 256 in each trend condition of the second comparison to roughly correspond with the length of the human data in each task, as well as to give a reasonable count for analysis. Any parameter sets which produced negative predictions of price were discarded and replaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Likelihood</head><p>The collected simulations were used to approximate the marginal likelihood of each model when focusing on the key qualitative features observed in our experiments; while these are not true likelihood scores in the traditional sense, these fulfil the same role, and so are referred to as likelihood here for ease of interpretation. In particular, they can be considered approximations to the likelihood of the statistics of the true generating model, assuming that each participant follows the same model and set of parameters.</p><p>Each simulated prediction series was assessed on the 5 measures detailed in the main text, calculating the raw difference in these measures between simulated and actual data. For the different trend conditions of Experiment 1, these measures were calculated separately in each condition and treated as distinct dimensions. These differences were then passed through a similarity kernel function to return a probability of the observed data given the model prediction, representing an approximate likelihood for that simulation. For simplicity, we used a multivariate Gaussian kernel to fit all measures simultaneously, with mean 0 for each dimension and covariance equal to that of the participant scores across measures in that experiment to account for any relationships between features. Ideally, this kernel should capture the noise in the measures for that parameter set if the simulation were repeated, though in practice this is difficult to specify in advance (without extensive additional simulations). In addition, many of the candidate models are deterministic, meaning no such noise function applies; the Gaussian kernel could thus be seen as a concession to these models, as these would otherwise hold only infinite or zero likelihoods depending on whether or not they achieved a perfect match.</p><p>Likelihoods were calculated for each participant separately to produce individual distributions across the sampled parameter values rather than assuming common parameters between subjects.</p><p>After all simulations for a model were complete, marginal likelihoods were approximated for each participant by taking the mean likelihood across the randomly sampled parameter values. Aggregate scores across subjects were then calculated by taking the product of all participants' marginal likelihoods. To provide the model measures shown in the figures of the main text, simulations were reweighted using a form of importance sampling based on the discrepancy ratio between the prior and posterior distributions, which reduces to the likelihood normalised by its own sum. These weights were then applied to each simulation during aggregation to provide weightedaverage measures for each participant, as summarised in <ref type="figure" target="#fig_1">Figures 5, 7</ref>, 12 and 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forests</head><p>Random forests were used to predict most likely model assignments for each participant by training a set of 500 classification and regression trees (CARTs) to distinguish the candidate models based on their particular distribution of statistics, following the procedure of <ref type="bibr" target="#b82">Pudlo et al. (2016)</ref>. The same simulations used in the approximate likelihood analysis were compiled into a 'reference table' of summary statistics for all models and passed to the forest for training, with each tree using a randomly sampled subset of the simulations and measures. Simulation counts were equal between models to ensure uniform prior probability. Once the forest was trained, empirical measures were passed to the trees to provide predicted classifications for each participant, with the majority vote across trees determining assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Ranges</head><p>Parameter ranges were defined according to the role of that parameter in the given representation and any relevant information available in the target price series, with some parameters from different models sharing bounds due to the similarity of their role in their respective models. The starting point of the MCMC and MC 3 predictors X e 2 , the prior mean of the Bayesian predictor µ 0 , the first prediction of the Delta Rule predictor X e 2 and the constant of the AR1 predictor γ shared common bounds, being defined by the first value in the target series (139 for the first comparison, 500 for both trend conditions of the second comparison)</p><p>due to the impact of format on these parameters: for price formats, these parameters were restricted to fall within 50% of the first value in the price series, while for change formats, changes were restricted to ±50% of the first price value. Ranges were matched between standard and log domains. The step size of the MCMC and MC 3 predictors σ s was restricted between 0 and 50% of the upper limit of the first prediction X e 2 . The temperature parameter φ of the MC 3 predictor was fixed between 1 and 10, while the number of chains c was fixed between 2 and 10 (as 1 chain reduces to MCMC), though note that unlike other parameters c is also restricted to discrete values. The prior variance of the Bayesian predictor σ 2 0 was restricted to be between 0 and 20% of the upper bound of the prior mean µ 0 , while the confidence values β 0 and λ 0 were restricted between 0 and 30. Finally, the Delta Rule learning rate α, the Exponential Smoothing learning rates α, δ and ζ, the AR1 persistence rate ρ and the Memory Weighted base parameter κ were all restricted to fall between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C -Model Scores</head><p>Models were primarily scored according to the aggregate marginal likelihood across participants, though best-fitting models were also determined for each individual according to their maximum marginal likelihood across models as an alternate metric. <ref type="table">Table 7</ref> shows the aggregate scores for each of the considered model combinations as well as the percentage of participants best fit by that model at the individual level for both the untrended experiment of <ref type="bibr" target="#b116">Zhu et al. (2021)</ref> and the trended experiment of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collapsing Factors</head><p>As noted in the main text, the models examined in this comparison are composed of distinct factors, separating each prediction mechanism from its representation. This might naturally lead to questions of how certain factors perform when collapsing across others, such as how each predictor fares when disregarding format and domain. We avoided such comparisons in the main text as we believe it is best to view the models as packages of components due to interactions between elements; for example, the behaviour predicted by the MCMC predictor can be very different between price and change formats. In case this is of interest, however, we here report simplified comparisons reducing the above results for each factor.  <ref type="table">Table 7</ref>: Approximate marginal log likelihood (MLL) results for all model combinations for both data sets. Note that these measures aggregate across noisy and nonnoisy versions of each model, while the FIRE and Direct Sampling predictors are insensitive to format and domain.</p><p>when marginalising across all other factors (assuming all levels of each factor are equally likely) from both experiments. First examining the predictors, this again finds support for sampling models: MC 3 holds both the highest score and largest proportion in the untrended task, followed by MCMC and Direct Sampling. Samplers also score well in the untrended task, though the margins are narrower here, while FIRE has the highest fit rate, again demonstrating the better performance of the rational models in this experiment Format shows a fairly even split in the untrended task, but change holds a substantial advantage in fit rate in the trended task. Domains show stronger support for log representations over standard in both likelihood and fit rates for both experiments, suggesting an advantage for representations able to scale with price magnitude. Finally, response noise shows distinct effects between tasks, with models with additional response noise performing better than those without in the untrended task, but vice versa in the trended task. As referenced in the main text, however, the best fitting sampling models did tend to show better fits without such noise, again suggesting caution should be taken when considering the fit of each factor separately rather than the full model packages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relaxing Exclusions</head><p>As noted in the main text, exclusions were applied to our empirical data to remove responses with extreme deviations from both directly preceding and subsequent estimates to filter potential errors. Questions may then be asked as to how the above results might differ if these exclusions are relaxed, treating each participant's sequence as an accurate reflection of their beliefs regarding future values. We therefore examined this by recalculating participant measures without response exclusions, and re-evaluating likelihood scores from the model simulations with these as the fitting criteria using the above method.</p><p>Results from the top five scoring models of each data set from this analysis are given in <ref type="table">Table 9</ref>, showing this not only generally reduced the fit of the models in both tasks, but also led to some differences in ranks and fit rates. For the untrended task, MC 3 in log price remains first by aggregate likelihood, though its proportion best fit falls from 46.3% to 9.76%, while the previously second place MCMC in log price falls to sixth, with a similar reduction in fit <ref type="bibr" target="#b116">Zhu et al. (2021)</ref> Experiment  Differences are however seen in the fourth and fifth place rankings, with the FIRE and Bayesian models being replaced by other sampling mechanisms, suggesting reduced support for rational models.</p><p>Relaxing these exclusions thus appears to alter results somewhat, particularly for the untrended experiment which shows greater division in fit rate across models. Even so, these results continue to offer support for sampling models, still placing highly in both comparisons, and seemingly dominating in the trended task. It is also interesting that this change substantially  <ref type="table">Table 9</ref>: Model scores for the top five scoring models of each data set by aggregate likelihood when extreme responses are not excluded from participant data.</p><p>increases evidence for the AR1 model in the untrended comparison, here ranking second by both aggregate likelihood and fit rate; this might suggest that this predictor has a particular ability to capture extreme fluctuations. Once again, however, caution should be taken when interpreting these results, as part of the behaviour captured by these models may simply reflect basic errors in response rather than stable signatures of the underlying cognitive mechanism.</p><p>Our focus therefore remains on the comparison including exclusions featured in the main text as this should be more robust to such influences, though this is a valuable avenue for future assessment of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D -Additional Empirical Measures Volatility Clustering</head><p>In addition to the features considered in the main text, another key aspect examined in our previous comparison of individual predictions and market movements is volatility clustering: periods of volatility tend to be clustered together in time with intervening periods of relative calm. This can be measured according to a slow decay in the autocorrelation function in absolute log change between predictions over increasing lags <ref type="bibr">(i.e., Corr[|ln(p e</ref> t ) − ln(p e t−1 )|, |ln(p e t+k ) − ln(p e t+k−1 )|], where k is the lag), with markets being described by power-law decays with exponents between -0.2 and -0.4 <ref type="bibr" target="#b24">(Cont, 2001)</ref>. In our previous paper, we found evidence of similar volatility clustering in the aggregated predictions of small groups of participants, though results from individuals showed notably faster rates of decay <ref type="bibr" target="#b116">(Zhu et al., 2021)</ref>. Examining each participant's volatility autocorrelation function separately, however, demonstrates substantial variation in observed autocorrelation values, perhaps suggesting power-law fits provide a poor reflection of such patterns in this case. We therefore did not consider volatility clustering as one of our criteria for model fitting in the above comparison as this measure may not be sufficiently reliable to distinguish between candidate models. We do however here provide a supplementary investigation of the impact of volatility clustering on the present results, determining whether inclusion of this property substantially alters findings.</p><p>To examine the impact of volatility clustering on model fits, we performed a supplementary model comparison including this feature alongside the five key measures: power-laws were fit to the autocorrelation function in absolute log change for each simulated prediction series where the resulting exponent acted as the key statistic. This measure was then used along with the five principle measures to recalculate likelihoods for each simulation for each participant using the same method described above, producing new marginal likelihood values. Results of this analysis are summarised in <ref type="table" target="#tab_0">Table 10</ref>, focusing on the top scoring models for concision.</p><p>While the inclusion of an additional criterion naturally lowers likelihood for all models, these results are highly consistent with those of the main text: MC 3 and MCMC in price formats continue to score well with only slight differences in the proportion of participants best fit.</p><p>The inclusion of volatility clustering as an additional model criterion thus does not diminish the strong support in these results for sampling models, suggesting these methods are also able to account for such behaviour, though again caution is advised given the potential imprecision in this measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forecast Optimality</head><p>While the deviation in the properties of participant predictions from those of the random walk target suggests behaviour did not follow rational solutions in this task, this is based on statistical features measured across the entire prediction series rather than individual responses, Log Change -682.05 2.33% <ref type="table" target="#tab_0">Table 10</ref>: Model scores for the top 5 models of each data set by aggregate marginal likelihood with volatility clustering included as an additional criterion. Marginal log likelihood values here are summed individual participant marginal log likelihood values, while fit rate reflects the proportion of participants best fit by that model at the individual level.</p><p>which could show different patterns. For example, one question that may be asked is whether participants showed any evidence of learning these solutions as the experiment progressed: even if forecasters deviate from rational standards in higher level measures, they may still move towards such responses at the trial level as they gain further experience with the task.</p><p>Indeed, this is a particular principle of the Bayesian model, with forecasters refining their understanding of the target distribution through observation until eventually reaching correspondence. Such learning is unfortunately not reflected in the particular set of empirical features that are the main focus of this study, as these assess general patterns across the full time series.</p><p>We therefore performed additional analyses on the empirical data to investigate whether participants showed any evidence of such learning. First, we examined the deviation of each response from its rational value, as used in the main text, here considering the change in this measure across each task. Mean absolute log prediction deviations are shown in the left panel of <ref type="figure">Figure 16</ref>, remaining predominantly above 0 throughout the task. This was assessed using a mixed effects linear regression using absolute log prediction deviation (i.e., |ln(p e t+1 ) − ln(p t )|) as a measure and trial as a predictor, with participant as a random effect to allow for different slopes between subjects. To avoid issues with differing trial counts between participants, all series were cut to the shortest length across subjects (365 trials for the untrended task, 184 trials for the trended task). Deviations showed no significant change over the course of the untrended experiment (|β| &lt; 0.001,t(14916) = 0.36, p = .716), or the positive (|β| &lt; 0.001,t(7891) = 1.57, p = .117) or negative conditions of the trended experiment (|β| &lt; 0.001,t(7903) = 0.27, p = .790), suggesting no learning of rational solutions in either data set.</p><p>As an alternate test, we also examined the rate of specifically rational predictions across trials for the untrended experiment (that is, responses which precisely repeated the most recent price point). The mean rate of rational responses from this task is shown in the right panel of <ref type="figure">Figure 16</ref>, showing rational responses were generally rare throughout the task, with an apparent downward trend. As this is a binary measure, we used a mixed effects logistic regression, here examining the rate of rational responses against trials, again with participant as a random</p><p>effect. This in fact demonstrated a significant decrease in optimal responses over the task, β = −0.009,t(14916) = 5.38, p &lt; .001, with predictions of 0 change becoming less frequent in later trials. This analysis was not performed for predictions of the trended series of Experiment 1 as no participants produced any perfectly rational responses in either trend condition. Both of these tests then indicate no shift towards rational forecasts in these data, suggesting that predictions not only deviated from rational standards in overarching measures, but also that participants demonstrated no learning of such strategies as they gained experience with the task either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E -Comparing Computational and Response Noise</head><p>The success of sampling models in these comparisons raises a notable distinction between differing sources of behavioural variability: sampling mechanisms include computational noise within their processes, meaning predictions can differ if repeated, whereas the other mechanisms are deterministic, only varying through the addition of response noise. The extent of the contribution of these noise types to variability in responses however remains unclear:</p><p>sampling models generally fit better without response noise in these comparisons by approximate likelihood, suggesting computational noise alone is sufficient to match behaviour, but assignments <ref type="figure">Figure 16</ref>: Measures of response optimality across trials from the untrended experiment. The left panel shows mean absolute log difference between current price and next predicted price across participants. The right panel show mean rate of rational responses (predicted change = 0) across participants. Shaded regions indicate 95% confidence intervals, while red lines indicate best fitting slopes. from the random forests show a stronger preference for response noise, leading to conflicting conclusions. While the present tasks were not designed to separate these noise types, as a preliminary assessment, we performed an additional analysis comparing the proportion of stochasticity attributable to these elements. This was assessed by examining the variability in model simulations when repeated both before and after the addition of response noise and contrasting their values. As only the sampling models include computational noise, this comparison was only applied to these models, with any stochasticity in the other models being entirely attributable to response noise.</p><p>A subset of 100 parameter samples for each of the sampling models were randomly selected and repeated 10 times, with the mean standard deviation in log predictions across these repetitions being used to assess computational variability: 1 n n ∑ t=1 SD(ln(p e t,1:i ))</p><p>where p e t,1:i is the set of predictions in trial t across repetitions 1 to i, and n is the number of trials. Response noise was then added to each simulation as defined in the main text, and mean standard deviations across repetitions were recalculated. The proportion of computational noise in these simulations was then determined by dividing the mean deviation score without response noise by the score with response noise. Proportions were calculated for samples both before and after the addition of response noise to account for differences in fit between these versions of the model, with simulations without response noise defaulting to a score of 1 as all variability is computational in this case. The approximate likelihood for each simulation was also calculated using the same methodology described above and averaged across repetitions; mean likelihoods were then normalised by their sum across parameter samples to act as posterior weights when aggregating these measures. <ref type="table" target="#tab_0">Table 11</ref> shows the mean posterior predicted proportion of computational noise across participants in each experiment. The share of computational noise was generally high, especially</p><p>for the best scoring models in each comparison (MC 3 predictor in log price for the untrended task, MCMC in standard price for the trended task), implying computational noise is expected to be the dominant source of variability compared to errors in responses. It should be noted however that this division is only inferred from the posterior across simulations because the current tasks do not provide an equivalent empirical measure of stochasticity over repetitions.</p><p>We therefore offer this result as a suggestion to be verified more directly in future work using more specialised experimental designs.  <ref type="table" target="#tab_0">Table 11</ref>: Predicted proportion of computational stochasticity for the sampling models in each data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Components</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Histograms for the five key measures from experimental data. Dashed lines indicate values from the target random walk series.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Illustrations of the key features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations). Column A shows the series of log changes between predictions; column B shows autocorrelation functions in these changes (where blue lines indicate 95% confidence bounds around 0); column C shows power spectral density estimates across predicted price (where red lines indicate average fitted slopes); and column D shows quantile-quantile plots of the prediction change distribution relative to Gaussian standards (shown by the dashed line), where positive slopes indicate heavy tails. Shaded regions indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Posterior predictive distributions of the five key measures for the top scoring models plotted against their equivalent empirical values for that participant. Dashed lines indicate equality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 12 :</head><label>12</label><figDesc>Illustrations of the key features from the empirical data (averaged across participants) and the three best scoring prediction models by aggregate likelihood (averaged across simulations) from Experiment 1, separated by trend condition. Column A shows the series of log changes between predictions; column B shows autocorrelation functions in these changes (where black lines indicate 95% confidence bounds around 0); column C shows power spectral density estimates across predicted price (where bold lines indicate average fitted slopes); and column D shows quantile-quantile plots of the prediction change distribution relative to Gaussian standards (shown by the dashed line), where positive slopes indicate heavy tails. Colours indicate trend conditions, and shaded regions indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 13 :</head><label>13</label><figDesc>Binned mean likelihoods across both simulations and participants for each parameter of the three best scoring models by aggregate likelihood.Finally, MCMC in log price placed third by aggregate likelihood and second by fit rate, in keeping with its strong performance in the previous comparison. As with the other sampling models, this showed similar measures of autocorrelations and heavy tails to the participants' responses, though variance and kurtosis in the change distribution again fell slightly below empirical levels. Parameter distributions again showed a mild advantage for intermediate starting points, while step size showed a strong preference for lower values, in contrast to the other sampling mechanisms; this is attributable to the scaling of this parameter in the log domain allowing more consistent performance across conditions, as well as the greater reliance on step size for MCMC compared to MC 3 noted in the previous comparison.Simulations including response noise again performed better by both aggregate likelihoodFigure 14: Posterior predictive distributions of the five key measures for the top scoring models plotted against their equivalent empirical values for that participant in each trend condition. Dashed lines indicate equality. (−269.59 vs. −296.39) and fit rate (14.0% vs. 2.33%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model scores for the top 5 models by aggregate marginal likelihood. Marginal log likelihood values here are summed individual participant marginal log likelihood values, while fit rate reflects the percentage of participants best fit by that model by likelihood at the individual level.</figDesc><table><row><cell cols="2">Model Components</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell cols="4">Domain Format Marginal Log Likelihood Fit Rate</cell></row><row><cell>MC 3</cell><cell>Log</cell><cell>Price</cell><cell>-53.85</cell><cell>46.3%</cell></row><row><cell>MCMC</cell><cell>Log</cell><cell>Price</cell><cell>-56.74</cell><cell>26.8%</cell></row><row><cell>Delta Rule</cell><cell cols="2">Standard Change</cell><cell>-93.33</cell><cell>2.44%</cell></row><row><cell cols="3">Exponential Smoothing Standard Change</cell><cell>-94.35</cell><cell>0%</cell></row><row><cell>Direct Sampling</cell><cell>-</cell><cell>-</cell><cell>-97.43</cell><cell>2.44%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model scores for the top 5 models by aggregate marginal likelihood. Marginal log likelihood values here are summed individual participant marginal log likelihood values, while fit rate reflects the percentage of participants best fit by that model by likelihood at the individual level.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Assignment rates from the random forest analysis for Experiment 1. general fit across all included measures: rational models may provide stable fits across statistics, but sampling models may be better suited to capturing the particularly distinctive aspects of human predictions, such as the heavy tails in the change distribution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>gives aggregate log likelihoods and participant fit rates for each factor level</figDesc><table><row><cell cols="2">Model Components</cell><cell></cell><cell cols="2">Zhu et al. (2021)</cell><cell cols="2">Experiment 1</cell></row><row><cell>Predictor</cell><cell cols="2">Domain Format</cell><cell cols="2">MLL Fit Rate</cell><cell cols="2">MLL Fit Rate</cell></row><row><cell>FIRE</cell><cell>-</cell><cell>-</cell><cell>-139.51</cell><cell>0%</cell><cell cols="2">-275.12 11.6%</cell></row><row><cell>Direct Sampling</cell><cell>-</cell><cell>-</cell><cell>-97.43</cell><cell cols="3">2.44% -289.30 2.33%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-126.45</cell><cell>0%</cell><cell cols="2">-266.69 18.6%</cell></row><row><cell>MCMC</cell><cell cols="6">Log Standard Change -110.95 2.44% -275.41 4.65% Price -56.74 26.8% -274.59 11.6%</cell></row><row><cell></cell><cell>Log</cell><cell cols="2">Change -132.21</cell><cell>0%</cell><cell cols="2">-277.43 4.65%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell cols="3">-104.29 9.76% -280.02</cell><cell>0%</cell></row><row><cell>MC 3</cell><cell cols="3">Log Standard Change -120.92 Price -53.85</cell><cell cols="3">46.3% -267.28 6.98% 0% -282.16 0%</cell></row><row><cell></cell><cell>Log</cell><cell cols="2">Change -101.46</cell><cell>0%</cell><cell>-280.03</cell><cell>0%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-146.16</cell><cell>0%</cell><cell cols="2">-407.61 2.33%</cell></row><row><cell>Bayesian</cell><cell cols="3">Log Standard Change -137.56 Price -159.14</cell><cell>0% 0%</cell><cell cols="2">-403.42 -275.54 4.65% 0%</cell></row><row><cell></cell><cell>Log</cell><cell cols="2">Change -138.88</cell><cell>0%</cell><cell cols="2">-275.31 4.65%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-143.34</cell><cell>0%</cell><cell>-312.31</cell><cell>0%</cell></row><row><cell>Delta Rule</cell><cell cols="3">Log Standard Change -93.33 Price -142.51</cell><cell cols="2">0% 2.44% -281.41 -314.90</cell><cell>0% 0%</cell></row><row><cell></cell><cell>Log</cell><cell cols="2">Change -113.26</cell><cell>0%</cell><cell cols="2">-278.47 2.33%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-131.79</cell><cell>0%</cell><cell>-307.14</cell><cell>0%</cell></row><row><cell>Exponential</cell><cell>Log</cell><cell>Price</cell><cell>-137.81</cell><cell>0%</cell><cell>-311.02</cell><cell>0%</cell></row><row><cell>Smoothing</cell><cell cols="3">Standard Change -94.35</cell><cell>0%</cell><cell>-281.43</cell><cell>0%</cell></row><row><cell></cell><cell>Log</cell><cell cols="2">Change -113.29</cell><cell>0%</cell><cell cols="2">-278.15 2.33%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-131.90</cell><cell>0%</cell><cell cols="2">-306.85 6.98%</cell></row><row><cell>AR1</cell><cell cols="3">Log Standard Change -97.92 Price -163.23</cell><cell cols="3">0% 2.44% -302.82 2.33% -280.71 6.98%</cell></row><row><cell></cell><cell>Log</cell><cell cols="5">Change -109.96 2.44% -292.54 4.65%</cell></row><row><cell></cell><cell cols="2">Standard Price</cell><cell>-144.38</cell><cell>0%</cell><cell>-307.78</cell><cell>0%</cell></row><row><cell>Memory-</cell><cell>Log</cell><cell>Price</cell><cell>-143.64</cell><cell>0%</cell><cell>-309.03</cell><cell>0%</cell></row><row><cell>Weighted</cell><cell cols="3">Standard Change -99.68</cell><cell cols="3">2.44% -351.69 2.33%</cell></row><row><cell></cell><cell>Log</cell><cell cols="4">Change -121.68 2.44% -349.14</cell><cell>0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Scores for each factor level when marginalising across all other factors. rate from 26.8% to 2.44%. Second place is now taken by AR1 in standard change, previously ranking sixth, whose fit rate increases to 22.0%, the second highest in this comparison. The Direct Sampling predictor also advances one rank from fifth to fourth, with a substantial increase in fit rate from 2.44% to 29.3%, making this the most common model at the individual level, while the Exponential Smoothing predictor in standard change falls to fifth place. Finally, the Delta Rule predictor in standard change remains third, with a slight increase in fit rate from 2.44% to 4.88%. For the trended series, model rankings are more stable, with the same top three models by aggregate likelihood, and an increase in fit rate for the MCMC models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In case of issues with any single instance of the random walk, two potential target series were used in the experiment, randomly assigned between participants. The two series differed in step size σ (0.25 and 0.2) and random movements, though as our focus is on the statistical signatures of predictions rather than their individual points, we here treat behaviour in these cases as equivalent to aid subsequent modelling. This is further assessed in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These autocorrelation functions do show a notable negative coefficient at the first lag, likely as a result of forecasters adjusting responses to fall in line with feedback values. We shall revisit this phenomenon in more detail later.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In fact, interactions between algorithms and representations observed in previous cognitive research argue that these aspects cannot be assessed in isolation and must be treated as inseparable packages<ref type="bibr" target="#b84">(Pylyshyn, 1973;</ref><ref type="bibr" target="#b4">Anderson, 1978;</ref><ref type="bibr" target="#b0">Abbott, Austerweil, &amp; Griffiths, 2015)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Two instances of each series (both positive and negative) were again used to avoid issues with any particular sequence, randomly assigned to each participant. Series here held identical definitions, differing only in their random movements. Again, we treat these cases as equivalent for modelling purposes, assessed further in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Two participants also showed particularly high tail indices in the negative condition (24.2 and 30.0); upon further examination, this appeared to be due to a few extreme responses which did not meet the criterion for exclusion. Data from these participants were retained for analysis and model fitting to keep consistent exclusion criteria between the experiments, though figures of the tail indices do not include these values to avoid issues with visualising the other points.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A -Measure Definitions</head><p>We here provide more precise mathematical definitions for the five key measures taken from both our empirical and simulated time series. These measures focused on the distribution of changes between log predictions (i.e., ln(p e t ) − ln(p e t−1 )), with the exception of the power spectral density, which instead used the series of log predictions itself. where k is the lag). We examined the ACF of each prediction series across the first 100 lags, measuring the proportion that differed significantly from zero. Significance was defined according to whether the estimated autocorrelation coefficient fell outside of 95% confidence bounds around 0 if the series is assumed to be completely random, calculated using the method of <ref type="bibr" target="#b15">Box, Jenkins, and Reinsel (1994)</ref> via the autocorr function of MATLAB R2019b:</p><p>where n is the number of observations supplied to the ACF.</p><p>To calculate the power spectral density slope, the series of log predictions was first mean-subtracted, then spectral density estimates were calculated using a Fourier transform of the series via the periodogram function of MATLAB R2019b with a sampling rate of one sample per trial. Following the procedure of <ref type="bibr" target="#b40">Gilden et al. (1995)</ref>, the set of spectral density values was smoothed by averaging densities within a set of 20 overlapping windows spaced equally across the log frequency range. A linear line was then fit between the log frequencies and log spectral densities to estimate the decay slope, representing a power-law decay in standard space, in keeping with the measure used by <ref type="bibr" target="#b70">Mantegna and Stanley (1999)</ref>. While the precision of this estimate is somewhat limited by the relatively short length of the considered series, slope estimates for 1000 independently generated series of similar length (512 points) found close approximation of the theoretical standards for both random walk (-1.98 ±0.01 95% CI vs.</p><p>-2) and white noise processes (0.00 ±0.01 95% CI vs. 0).</p><p>Variance was taken from the distribution of changes between subsequent log predictions</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random walks on semantic networks can resemble optimal foraging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0038693</idno>
		<ptr target="https://doi.org/10.1037/a0038693" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="569" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overreaction and working memory (Working Paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Afrouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Landier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thesmar</surname></persName>
		</author>
		<idno type="DOI">10.3386/w27947</idno>
		<ptr target="https://doi.org/10.3386/w27947" />
	</analytic>
	<monogr>
		<title level="j">National Bureau of Economic Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Parallel Metropolis coupled Markov chain Monte Carlo for Bayesian phylogenetic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Altekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Huelsenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ronquist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<idno type="DOI">10.1093/bioinformatics/btg427</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btg427" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Arguments concerning representations for mental imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.85.4.249</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.85.4.249" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">249</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The adaptive nature of human categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.98.3.409</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.98.3.409" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="409" to="429" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An integrated theory of list memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bothell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matessa</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmla.1997.2553</idno>
		<ptr target="https://doi.org/10.1006/jmla.1997.2553" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="380" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evolution of market heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anufriev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hommes</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0269888912000161</idno>
		<ptr target="https://doi.org/10.1017/S0269888912000161" />
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="271" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">X-CAPM: An extrapolative capital asset pricing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barberis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shleifer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jfineco.2014.08.007</idno>
		<ptr target="https://doi.org/10.1016/j.jfineco.2014.08.007" />
	</analytic>
	<monogr>
		<title level="j">Journal of financial economics</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A model of investor sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barberis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishny</surname></persName>
		</author>
		<idno type="DOI">10.1515/9781400829125-015</idno>
		<ptr target="https://doi.org/10.1515/9781400829125-015" />
	</analytic>
	<monogr>
		<title level="j">Journal of Financial Economics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="307" to="343" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1306572110</idno>
		<ptr target="https://doi.org/10.1073/pnas.1306572110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Beaumont</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-030718-105212</idno>
		<ptr target="https://doi.org/10.1146/annurev-statistics-030718-105212" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and its Application</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="379" to="403" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning the value of information in an uncertain world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Rushworth</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1954</idno>
		<ptr target="https://doi.org/10.1038/nn1954" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1221" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-sensitive heuristics in statistical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<idno type="DOI">10.1080/14640749308401039</idno>
		<ptr target="https://doi.org/10.1080/14640749308401039" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section A</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="779" to="811" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Behavioral heterogeneity in stock prices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Boswijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hommes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manzan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jedc.2007.01.001</idno>
		<idno>1938-1970. doi</idno>
		<ptr target="https://doi.org/10.1016/j.jedc.2007.01.001" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic dynamics and control</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous beliefs and routes to chaos in a simple asset pricing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hommes</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0165-1889(98</idno>
		<ptr target="https://doi.org/10.1016/S0165-1889(98" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimism where there is none: asymmetric belief updating observed with valence-neutral life events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2021.104939</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2021.104939" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page">104939</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A mathematical model for simple learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0054388</idno>
		<ptr target="https://doi.org/10.1037/h0054388" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">313</biblScope>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A non-random walk down wall street</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mckinlay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic models of language processing and acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2006.05.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2006.05.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanborn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic biases meet the Bayesian brain</title>
		<idno type="DOI">10.1177/0963721420954801</idno>
		<ptr target="https://doi.org/10.1177/0963721420954801" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="506" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Empirical properties of asset returns: Stylized facts and statistical issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cont</surname></persName>
		</author>
		<idno type="DOI">10.1088/1469-7688/1/2/304</idno>
		<ptr target="https://doi.org/10.1088/1469-7688/1/2/304" />
	</analytic>
	<monogr>
		<title level="j">Quantitative Finance</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling in stock market data: stable laws and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Potters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Bouchaud</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-09799-15</idno>
		<ptr target="https://doi.org/10.1007/978-3-662-09799-15" />
	</analytic>
	<monogr>
		<title level="m">Scale invariance and beyond</title>
		<editor>B. Dubrulle, F. Graner, &amp; D. Sornette</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Where do hypotheses come from?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2017.05.001</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2017.05.001" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Computational precision of mental inference as critical source of human choice suboptimality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-D</forename><surname>Devauchelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016.11.005</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2016.11.005" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1398" to="1411" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intuitive time-series extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Eggleton</surname></persName>
		</author>
		<idno type="DOI">10.2307/2490763</idno>
		<ptr target="https://doi.org/10.2307/2490763" />
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="page" from="68" to="102" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Constructivist coding: Learning from selective feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Enkvist</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2007.01856.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.2007.01856.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="110" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Modelling extremal events for insurance and finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Embrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikosch</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33483-2</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33483-2" />
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient capital markets: A review of theory and empirical work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
		<idno type="DOI">10.2307/2325486</idno>
		<ptr target="https://doi.org/10.2307/2325486" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="417" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Computation noise in human learning and decision-making: Origin, impact, function. Current Opinion in Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Findling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2021.02.018</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2021.02.018" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Further perceptions of probability: In defence of associative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000410</idno>
		<ptr target="https://doi.org/10.1037/rev0000410" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chartists, fundamentalists, and trading in the foreign exchange market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Frankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Froot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="185" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exponential smoothing: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gardner</surname><genName>Jr</genName></persName>
		</author>
		<idno type="DOI">10.1002/for.3980040103</idno>
		<ptr target="https://doi.org/10.1002/for.3980040103" />
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Forecasting trends in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gardner</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mckenzie</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.31.10.1237</idno>
		<ptr target="https://doi.org/10.1287/mnsc.31.10.1237" />
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1237" to="1246" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo maximum likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd symposium on the interface: Computing science and statistics. Interface Foundation</title>
		<editor>E. M. Keramidas</editor>
		<meeting>the 23rd symposium on the interface: Computing science and statistics. Interface Foundation</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Simple heuristics that make us smart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cognitive emissions of 1/f noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gilden</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.108.1.33</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.108.1.33" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">1/f Noise in Human Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gilden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mallon</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.7892611</idno>
		<ptr target="https://doi.org/10.1126/science.7892611" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1837" to="1839" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature20101</idno>
		<ptr target="https://doi.org/10.1038/nature20101" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting the future as Bayesian inference: People combine prior knowledge with observations when estimating duration and extent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0024899</idno>
		<ptr target="https://doi.org/10.1037/a0024899" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">725</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graphs versus tables: Effects of data presentation format on judgemental forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolger</surname></persName>
		</author>
		<idno type="DOI">10.1016/0169-2070(95</idno>
		<ptr target="https://doi.org/10.1016/0169-2070(95" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="634" to="640" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Trend damping: Under-adjustment, experimental artifact, or adaptation to features of the natural environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reimers</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0029179</idno>
		<ptr target="https://doi.org/10.1037/a0029179" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="589" to="607" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Hastings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monte Carlo sampling methods using Markov chains and their applications</title>
		<idno type="DOI">10.1093/biomet/57.1.97</idno>
		<ptr target="https://doi.org/10.1093/biomet/57.1.97" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fredebohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hagmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The nature and perception of fluctuations in human musical rhythms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Geisel</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0026457</idno>
		<idno>e26457. doi</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0026457" />
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What is coded into memory in the absence of outcome feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Henriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0017893</idno>
		<ptr target="https://doi.org/10.1037/a0017893" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Expectations formation: Rational or adaptive or</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hey</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2681(94</idno>
		<ptr target="https://doi.org/10.1016/0167-2681(94" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Behavior &amp; Organization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">90104</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The heterogeneous expectations hypothesis: Some evidence from the lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hommes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jedc.2010.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.jedc.2010.10.003" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics &amp; Control</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Complex evolutionary systems in behavioral finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hommes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wagener</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-012374258-2.50008-7</idno>
		<ptr target="https://doi.org/10.1016/B978-012374258-2.50008-7" />
	</analytic>
	<monogr>
		<title level="m">Handbook of financial markets: Dynamics and evolution</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="217" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A tutorial on kernel methods for categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jäkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2007.06.002</idno>
		<ptr target="https://doi.org/10.1016/j.jmp.2007.06.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="343" to="358" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Noise: A flaw in human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sibony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>Little, Brown</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The pervasiveness of 1/f scaling in speech reflects the metastable basis of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Kello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Van Orden</surname></persName>
		</author>
		<idno type="DOI">10.1080/03640210801944898</idno>
		<ptr target="https://doi.org/10.1080/03640210801944898" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1217" to="1231" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Extreme value distributions: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nadarajah</surname></persName>
		</author>
		<idno type="DOI">10.1142/p191</idno>
		<ptr target="https://doi.org/10.1142/p191" />
		<imprint>
			<date type="published" when="2000" />
			<publisher>Imperial College Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsaneva-Atanasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juliusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive anchoring model: How static and dynamic presentations of time series influence judgments and predictions</title>
		<idno type="DOI">10.1111/cogs.12476</idno>
		<ptr target="https://doi.org/10.1111/cogs.12476" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab3050</idno>
		<ptr target="https://doi.org/10.1126/science.aab3050" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep neural networks predict category typicality ratings for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Cognitive Science Society</title>
		<editor>D. C. Noelle et al.</editor>
		<meeting>the 37th annual meeting of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An examination of the accuracy of judgmental extrapolation of time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Edmundson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Connor</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0169-2070(85</idno>
		<ptr target="https://doi.org/10.1016/S0169-2070(85" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80068" to="80074" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Factors affecting judgmental forecasts and confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makridakis</surname></persName>
		</author>
		<idno type="DOI">10.1016/0749-5978(89</idno>
		<ptr target="https://doi.org/10.1016/0749-5978(89" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90049" to="90055" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring judgemental forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Connor</surname></persName>
		</author>
		<idno type="DOI">10.1016/0169-2070(92)90004</idno>
		<ptr target="https://doi.org/10.1016/0169-2070(92)90004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The present-value relation: Tests based on implied variance bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Porter</surname></persName>
		</author>
		<idno type="DOI">10.2307/1911512</idno>
		<ptr target="https://doi.org/10.2307/1911512" />
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="555" to="574" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X1900061X</idno>
		<ptr target="https://doi.org/10.1017/S0140525X1900061X" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The anchoring bias reflects rational use of cognitive resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-017-1286-8</idno>
		<ptr target="https://doi.org/10.3758/s13423-017-1286-8" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="322" to="349" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Expectations and the neutrality of money</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Lucas</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-0531(72</idno>
		<ptr target="https://doi.org/10.1016/0022-0531(72" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90142" to="90143" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Forecasts of future prices, unbiased markets, and &quot;martingale&quot; models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Mandelbrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Business</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Introduction to econophysics: correlations and complexity in finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Mantegna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rational expectations and the theory of price movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Muth</surname></persName>
		</author>
		<idno type="DOI">10.2307/1909635</idno>
		<ptr target="https://doi.org/10.2307/1909635" />
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="335" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Spatiotemporal mechanisms for detecting and identifying image features in human vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn886</idno>
		<ptr target="https://doi.org/10.1038/nn886" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="812" to="816" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Decision-related activity in sensory neurons reflects more than a neuron&apos;s causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nienborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Cumming</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature07821</idno>
		<ptr target="https://doi.org/10.1038/nature07821" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7243</biblScope>
			<biblScope unit="page" from="89" to="92" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Exemplars, prototypes, and similarity rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780203763162</idno>
		<ptr target="https://doi.org/10.4324/9780203763162" />
	</analytic>
	<monogr>
		<title level="m">Essays in honor of William K. Estes</title>
		<editor>A. F. Healy, S. M. Kosslyn, &amp; R. M. Shiffrin</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="149" to="167" />
		</imprint>
	</monogr>
	<note>From learning theory to connectionist theory</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Going up-going down: How good are people at forecasting trends and changes in trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="176" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Psychophysical reverse correlation reflects both sensory and decision-making processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Okazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-05797-y</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-05797-y" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3479</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Theories of associative learning in animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Bouton</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.52.1.111</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.52.1.111" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="139" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A model for Pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hall</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.87.6.532</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.87.6.532" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">532</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A Bayesian perspective on magnitude estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Petzschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Glasauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2015.03.002</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2015.03.002" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A model for learning based on the joint estimation of stochasticity and volatility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-021-26731-9</idno>
		<ptr target="https://doi.org/10.1038/s41467-021-26731-9" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A descriptive Bayesian account of optimism in belief revision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual meeting of the Cognitive Science Society</title>
		<editor>H. R. J. Culbertson A. Perfors &amp; V. Ramenzoni</editor>
		<meeting>the 44th annual meeting of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pudlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Estoup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Cornuet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reliable abc model choice via random forests</title>
		<idno type="DOI">10.1093/bioinformatics/btv684</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btv684" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="859" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">What the mind&apos;s eye tells the mind&apos;s brain: A critique of mental imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0034650</idno>
		<ptr target="https://doi.org/10.1037/h0034650" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Sensitivity to autocorrelation in judgmental time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijforecast.2010.08.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijforecast.2010.08.004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1196" to="1214" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classical conditioning II</title>
		<editor>A. H. Black &amp; W. F. Prokasy</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Appleton-Century-Crofts</publisher>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Parallel distributed processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations. Cambridge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1986" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Proof that properly anticipated prices fluctuate randomly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Samuelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Industrial Management Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="49" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Fast and accurate learning when making discrete numerical estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004859</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1004859" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1004859</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Bayesian brains without probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.10.003" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="883" to="893" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The sampling brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2017.04.009</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2017.04.009" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="492" to="493" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: alternative algorithms for category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0020511</idno>
		<ptr target="https://doi.org/10.1037/a0020511" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1144" to="1167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Reconciling intuitive physics and Newtonian mechanics for colliding objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0031912</idno>
		<ptr target="https://doi.org/10.1037/a0031912" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="411" to="437" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Sampling as a resource-rational constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X19001584</idno>
		<ptr target="https://doi.org/10.1017/S0140525X19001584" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Falbén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/438nd</idno>
		<ptr target="https://doi.org/10.31234/osf.io/438nd" />
		<title level="m">Noise in cognition: Bug or feature? PsyArXiv Preprint</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Sampling as the human approximation to probabilistic inference</title>
	</analytic>
	<monogr>
		<title level="m">Human-Like Machine Intelligence</title>
		<editor>S. Muggleton &amp; N. Chater</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Accuracy of judgmental forecasts: A comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Sanders</surname></persName>
		</author>
		<idno type="DOI">10.1016/0305-0483(92)90040-E</idno>
		<ptr target="https://doi.org/10.1016/0305-0483(92)90040-E" />
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="364" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A pessimistic view of optimistic belief updating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Catmur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2016.05.004</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2016.05.004" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="71" to="127" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Toward a universal law of generalization for psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.3629243</idno>
		<ptr target="https://doi.org/10.1126/science.3629243" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4820</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Do stock prices move too much to be justified by subsequent changes in dividends?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Shiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Using Occam&apos;s razor and Bayesian modelling to compare discrete and continuous representations in numerosity judgements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2020.101309</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2020.101309" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">101309</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Perceptual and cognitive judgments show both anchoring and repulsion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<idno type="DOI">10.1177/09567976221089599</idno>
		<ptr target="https://doi.org/10.1177/09567976221089599" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Probabilistic models and generative neural networks: Towards a unified framework for modeling normal and impaired neurocognitive functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Testolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2016.00073</idno>
		<ptr target="https://doi.org/10.3389/fncom.2016.00073" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A tutorial on approximate Bayesian computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2012.02.005</idno>
		<ptr target="https://doi.org/10.1016/j.jmp.2012.02.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="85" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Approximating Bayesian inference through model simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.06.003</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2018.06.003" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="826" to="840" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<title level="m">Judgment under uncertainty: Heuristics and biases</title>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title/>
		<idno type="DOI">10.1126/science.185.4157.1124</idno>
		<ptr target="https://doi.org/10.1126/science.185.4157.1124" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">1/f noise&apos; in music and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.13.556</idno>
		<ptr target="https://doi.org/10.1103/PhysRevB.13.556" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="317" to="318" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">1/f noise&quot; in music: Music from 1/f noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.381721</idno>
		<ptr target="https://doi.org/10.1121/1.381721" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="258" to="263" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Misperception of exponential growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Wagenaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Sagaria</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03204114</idno>
		<ptr target="https://doi.org/10.3758/BF03204114" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="416" to="422" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Designing and interpreting psychophysical investigations of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Waskom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Okazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2019.09.016</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2019.09.016" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="112" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Choice variability and suboptimality in uncertain environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2016.07.003</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2016.07.003" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="109" to="115" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Understanding the structure of cognitive noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1010312</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1010312" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1010312</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000190</idno>
		<ptr target="https://doi.org/10.1037/rev0000190" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="719" to="748" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Cognitive variability matches speculative price dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/gfjvs</idno>
		<ptr target="https://doi.org/10.31234/osf.io/gfjvs" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">PsyArXiv Preprint. doi</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">The Autocorrelated Bayesian Sampler: A rational process for probability judgments, estimates, confidence intervals, choices, confidence judgments, and response times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2023" />
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
