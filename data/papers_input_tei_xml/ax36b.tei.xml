<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Running head: PYBEAM 1 PyBEAM: A Bayesian approach to parameter inference for a wide class of binary evidence accumulation models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Murrow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
							<email>william.holmes@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Physics and Astronomy</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<addrLine>6301 Stevenson Science Center</addrLine>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Running head: PYBEAM 1 PyBEAM: A Bayesian approach to parameter inference for a wide class of binary evidence accumulation models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Many decision-making theories are encoded in a class of processes known as evidence accumulation models (EAM). These assume that noisy evidence stochastically accumulates until a set threshold is reached, triggering a decision. One of the most successful and widely used of this class is the drift-diffusion model (DDM). The DDM however is limited in scope and does not account for processes such as evidence leakage, changes of evidence, or time varying caution. More complex EAMs can encode a wider array of hypotheses, but are currently limited by the computational challenges. In this work, we develop the python package PyBEAM (Bayesian Evidence Accumulation Models) to fill this gap. Toward this end, we develop a general probabilistic framework for predicting the choice and response time distributions for a general class of binary decision models. In addition, we have heavily computationally optimized this modeling process and integrated it with PyMC3, a widely used python package for Bayesian parameter estimation. This 1) substantially expands the class of EAM models to which Bayesian methods can be applied, 2) reduces the computational time to do so, and 3) lowers the entry fee for working with these models. Here we demonstrate the concepts behind this methodology, its application to parameter recovery for a variety of models, and apply it to a recently published data set to demonstrate its practical use.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Computational modeling has been used to study the properties of decision making for over years. Given the difficulty in both perturbing and observing the brain, models are vital for formally encoding and testing mechanistic hypotheses about decision making processes. For example, how do people process information over time, how do they modulate their levels of caution under different circumstances, or how do they extract information from complex choice sets with multiple alternatives and attributes. These types of questions are difficult to study through direct observation or statistical analysis alone. Models provide a rigorous way to study these types of questions indirectly by comparing the expected patterns of data predicted by the models to experimental observation.</p><p>One of the dominant classes of models in this area are Evidence Accumulation Models (EAMs). These models predict the outcome of decisions by modeling the process by which that decision is made. For example, the popular Drift Diffusion Model (DDM)  hypothesizes that people stochastically sample information over time, additively accumulate evidence based on that information, and make a decision when a critical threshold of evidence has been achieved. This and similar models are mathematically encoded in stochastic differential equations (SDE). A useful feature of this family of models is that they make predictions about both the choices made and the time it takes them to make choices, so-called Choice-Response Time (choice-RT) data. Importantly, the time required to complete the decision provides insight into the underlying decision process. For example, one might expect a strong preference for one alternative to yield a fast decision.</p><p>EAMs are a highly successful modeling framework in cognitive psychology. They both qualitatively and quantitatively capture a range of choice and response time benchmarks, including (1) speed-accuracy trade-off, (2) the positive skew of human response time distributions, (3) the relation between the mean and variance in response times, and (4) differences in fast and slow errors <ref type="bibr" target="#b2">(Brown &amp; Heathcote, 2008;</ref><ref type="bibr" target="#b42">Ratcliff, 1978;</ref><ref type="bibr" target="#b44">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b47">Ratcliff, Zandt, &amp; McKoon, 1999;</ref><ref type="bibr" target="#b62">Usher &amp; McClelland, 2001</ref>). These models have also been applied to a wide range of behaviors including learning <ref type="bibr" target="#b13">(Evans, Brown, Mewhort, &amp; Heathcote, 2018;</ref><ref type="bibr" target="#b20">Fontanesi, gluth, Spektor, &amp; Rieskamp, 2019)</ref>, categorization <ref type="bibr" target="#b37">(Nosofsky, Little, Donkin, &amp; Fific, 2011;</ref><ref type="bibr" target="#b38">Nosofsky &amp; Palmeri, 1997)</ref>, memory <ref type="bibr" target="#b40">(Osth &amp; Farrell, 2019;</ref><ref type="bibr" target="#b42">Ratcliff, 1978)</ref>, language processing <ref type="bibr" target="#b33">(Lerche, Christmann, &amp; Voss, 2018;</ref><ref type="bibr" target="#b63">Wagenmakers, Ratcliff, Gomez, &amp; McKoon, 2008)</ref>, and consumer choice <ref type="bibr" target="#b4">(Busemeyer, Gluth, Rieskamp, &amp; Turner, 2019;</ref><ref type="bibr" target="#b17">Evans, Holmes, &amp; Trueblood, 2019)</ref>. More recently, researchers have started combining EAMs with psychophysiological data, such as neural recordings <ref type="bibr" target="#b56">(Turner et al., 2013;</ref><ref type="bibr" target="#b57">Turner, Rodriguez, Norcia, McClure, &amp; Steyvers, 2016;</ref><ref type="bibr" target="#b60">Turner, van Maanen, &amp; Forstmann, 2015)</ref>, motor recordings <ref type="bibr" target="#b49">(Servant, White, Montagnini, &amp; Burle, 2016)</ref>, and eye movements <ref type="bibr" target="#b31">(Krajbich, Armel, &amp; Rangel, 2010)</ref>. Despite their popularity and success, most applications use only the simplest forms of EAMs, such as the DDM. Tools for quantitatively fitting general EAMs are still lacking.</p><p>All current theoretical modeling approaches in this area make some type of significant sacrifice that limits their use. The typical modeling process in this area involves three essential elements: a theoretical model, experimental data to test it, and efficient and accurate methods for challenging the model with data. Existing methods sacrifice at least one of these. Accurate and efficient methods exist for working with complex experimental designs, but only with simple models. Alternatively, there are approaches for testing complex models, but with either documented accuracy issues, poor efficiency that limits experimental design (i.e. too many experimental conditions = too much compute time), or both (more details in the Background). These limitations restrict the questions researchers can ask and the studies they design.</p><p>The main challenge of working with EAMs and choice-RT data is to determine, for a particular model and parameter set, how well the model matches the empirical data <ref type="bibr" target="#b11">(Dutilh et al., 2019)</ref>. The likelihood function is a natural way to do this.</p><p>Unfortunately, due to the difficulty in working with SDEs, only the simplest models in this family have sufficiently analytic likelihoods to be tractable. We say "sufficiently" because the likelihoods of even the simplest models (e.g. the DDM) involve infinite series or intractable integrals. For example, Smith <ref type="bibr" target="#b51">(Smith, 2000)</ref> developed an analytic approach to solve for the likelihood function of a range of SDE models. Unfortunately it tends to be slow to compute since it requires numerical calculation of many integrals.</p><p>Further, numerical integration produces resolution issues similar to that of numerical solutions. For these reasons, most methods utilize some way of approximating the quantitative agreement between model and data and make some sacrifice in the process.</p><p>Since much of the work on choice-RT model fitting applies to binary choice models, we will limit this brief discussion to binary decision modeling approaches (see <ref type="table">Table 1</ref>).</p><p>Until recently, the majority of quantitative fitting approaches were based on summary statistics <ref type="bibr" target="#b61">(Turner &amp; Van Zandt, 2018)</ref>. One common approach is the quantile maximization approach <ref type="bibr" target="#b23">(Heathcote, Brown, &amp; Mewhort, 2002;</ref><ref type="bibr" target="#b45">Ratcliff &amp; Tuerlinckx, 2002)</ref>. For a stochastic model, the user simulates a large number of outcomes, finds the quantiles of the RT distribution, compares those to the quantiles of the experimental RT distribution, and optimizes parameters for agreement. CHaRTr <ref type="bibr" target="#b5">(Chandrasekaran &amp; Hawkins, 2019)</ref>, a recently developed R modeling package for general binary EAMs, takes this approach. It is well known however that this compression of the model and data into summary statistics leads to significant errors in parameter estimates for EAMs <ref type="bibr" target="#b59">(Turner &amp; Sederberg, 2014)</ref>.</p><p>A second approach is the Probability Density Approximation <ref type="bibr" target="#b25">(Holmes, 2015;</ref><ref type="bibr" target="#b34">Lin, Heathcote, &amp; Holmes, 2019;</ref><ref type="bibr" target="#b59">Turner &amp; Sederberg, 2014)</ref> method. This method starts much the same way by simulating a large number of stochastic outcomes. Those samples are then used to construct a kernel density estimate approximation of the actual likelihood function. This avoids the problems with summary statistics and is easily generalizable to more complex models <ref type="bibr" target="#b15">(Evans, Holmes, Dasari, &amp; Trueblood, 2021;</ref><ref type="bibr" target="#b19">Evans, Trueblood, &amp; Holmes, 2020;</ref><ref type="bibr" target="#b26">Holmes, O'Daniels, &amp; Trueblood, 2020;</ref><ref type="bibr" target="#b28">Holmes, Trueblood, &amp; Heathcote, 2016;</ref><ref type="bibr" target="#b53">Trueblood, Heathcote, Evans, &amp; Holmes, 2021;</ref><ref type="bibr" target="#b54">Trueblood et al., 2018;</ref><ref type="bibr" target="#b59">Turner &amp; Sederberg, 2014)</ref>. However, the simulated likelihood</p><formula xml:id="formula_0">F o r R e v i e w O n l y PYBEAM 6</formula><p>function becomes a stochastic entity, which introduces significant problems for Bayesian inference . Specifically, a favorable estimate of the likelihood is easily accepted but difficult to reject in favor of a new parameter set, leading to MCMC chain stagnation, high MCMC rejection rates, and generally poor posterior approximations. Additionally, the large numbers of simulations needed are extremely computationally intensive.</p><p>Finally PyDDM <ref type="bibr" target="#b50">(Shinn, Lam, &amp; Murray, 2020)</ref> takes the approach of converting SDE models into Fokker-Planck models. This approach essentially reformulates the SDE description of the model as a probabilistic model described by a Partial Differential Equation (PDE). Solving this PDE then provides an approximation for the likelihood. Their approach is similar in idea to what we propose below, but has a number of drawbacks. First, relative to the needs of complex EAMs, it is inefficient due to inadequate numerical simulation approaches (which also introduces problematic accuracy issues), taking up to 5 hours to perform a maximum likelihood fit (according to their own benchmarks). Since Bayesian methods require sampling far more parameter sets than maximum likelihood (10-100x more), it would be infeasible to directly extend this. Second, their approach is not Bayesian. This is problematic because many models in this field have parameter indeterminacy issues that Bayesian methods naturally diagnose. Finally, their approach is limited to binary choice models. This is only a sub-sample of methods (see <ref type="table">Table 1</ref>) and there are a number of variations on them. However, they illustrate that there are currently no methods for  <ref type="table">Table 1</ref> List of existing software for fitting two-threshold, binary choice models to choice-RT data with the proposed characteristics of our approach PyBEAM in the final column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we describe the broad methodological details of our algorithm for performing Bayesian parameter estimation for two-threshold models of binary evidence accumulation. Before continuing, we make a few notes. First, we mainly discuss the general idea behind this method. There are numerous implementation details "under the hood" that are required to make this work; however, we leave this discussion for the Supplementary Information. Second, we do not describe the Pythonic implementation of this algorithm in this article. Instead, we will provide multiple Jupyter notebooks <ref type="bibr">(Kluyver et al., 2016)</ref> demonstrating the use of this method for both parameter recovery and application to data. These notebooks are more than just documented code. They are descriptive in nature and should be more effective than writing pseudo-code. These are publicly available and can be found at https://github.com/murrowma/pybeam. Finally, while reading these methods will likely help a reader understand how this method works at a high level, understanding these methods are not required to use the provided Python implementation.</p><formula xml:id="formula_1">F o r R e v i e w O n l y PYBEAM 8</formula><p>The general two-threshold binary accumulation model</p><p>In this article, we develop an approach for calculating choice-RT likelihoods for general two threshold models of binary decision making and integrate them with Bayesian parameter estimation. For this type of model <ref type="figure" target="#fig_2">(Figure 1</ref>), the evidence accumulation process begins at point z, corresponding to an initial bias prior to stimulus presentation. Upon stimulus presentation, evidence is noisily accumulated, described by the stochastic differential equation (SDE) <ref type="bibr" target="#b51">(Smith, 2000)</ref>,</p><formula xml:id="formula_2">dx(t) = v(x, t)dt + D(x, t)dB(t),<label>(1)</label></formula><p>where x(t) is the total evidence accumulated at time t and v(x, t) is the rate of evidence accumulation (referred to as the drift rate). The drift is determined by the quality of information in the presented stimulus, where decisions with clear stimulus information produce larger drift rates. The function D(x, t) is the diffusion rate, and though in some models it can depend on (x, t), it is most commonly constant and fixed for scaling purposes (commonly set to either D(x, t) = 1, 0.1). Lastly, B(t) is a Gaussian noise term.</p><p>The evidence accumulation process described by Equation (1) continues until one of the two opposing decision thresholds (c 1 (t) or c 2 (t)) is reached, triggering a response ( <ref type="figure" target="#fig_2">Figure 1</ref>). The separation s(t) between thresholds indicates the degree of time dependent caution exhibited by the decision maker. If thresholds are far apart (close together), the participant makes slower (faster), more (less) cautious decisions. Though in many evidence accumulation models the degree of caution is fixed, time changing caution and accordingly thresholds (as shown in <ref type="figure" target="#fig_2">Figure 1</ref>) is increasingly being investigated as a mechanism to optimize reward rates <ref type="bibr">(Drugowitsch, Moreno-Bote, Churchland, Shadlen, &amp; Pouget, 2012;</ref><ref type="bibr" target="#b52">Tajima, Drugowitsch, &amp; Pouget, 2016)</ref> or the speed accuracy trade-off <ref type="bibr" target="#b21">(Frazier &amp; Yu, 2007)</ref>. An additional parameter referred to as the non-decision time, t nd , is also generally included in EAMs. This term describes the time it takes for a participant to encode the stimulus information and the motor processes involved in selecting one of the available choices. Mathematically, this process is described by the noted stochastic differential equation,</p><formula xml:id="formula_3">F o r R e v i e w O n l y dx(t) = v(x,t)dt + D(x,t)dB(t) Bias (z) Evidence (v) Choice 1 Probability f 1 (t) Time Choice (c ) Choice 1 (c 1 ) Caution (s)</formula><p>where x(t) is the total accumulated evidence (or preference), v(x, t) is the rate of evidence accumulation, D(x, t) is the diffusion rate, and B(t) is the noise term. This process results in the choice probability distribution f i (x, t), which indicates how likely it is for an accumulator to cross the decision threshold at that time. t nd indicates the time it takes for non-decision processes, which is added to the decision time from the response time. So called choice-RT data is common in this field. In order to determine how well a particular set of parameters for a particular model describe data of this form, it is useful to calculate the "likelihood" function which describes the probability of making the observed choice at the observed time. Though the stochastic form of binary choice evidence accumulation models is simple to formulate and interpret, it does not immediately provide us with this quantity of interest. This can be circumvented in a number of ways. Smith <ref type="bibr" target="#b51">(Smith, 2000)</ref> devised an analytic integral based solution for this problem; however, it is computationally intensive and limited in its application.</p><p>Further, since it requires numerical calculation of integrals, it only can approximate the actual solution. Stochastic simulation based methods <ref type="bibr" target="#b5">(Chandrasekaran &amp; Hawkins, 2019;</ref><ref type="bibr" target="#b25">Holmes, 2015)</ref> are more general, but are computationally inefficient largely due to the necessity of generating immense quantities of random numbers.</p><p>Fortunately, this stochastic process can instead be written as the Fokker-Planck equation, which describes the probability that the accumulator has precisely state (x) at time (t) <ref type="bibr">(Öttinger, 1996)</ref>. We in particular use the forwards Fokker-Planck (FP) equation,</p><formula xml:id="formula_4">∂p(x, t) ∂t = − ∂ [v(x, t)p(x, t)] ∂x + 1 2 ∂ [D(x, t) 2 p(x, t)] ∂x 2 ,<label>(2)</label></formula><p>where p(x, t) is the probability of accumulated evidence x at time t, and v(x, t) and</p><p>D(x, t) are the drift and diffusion rates introduced earlier. The EAM response thresholds are modeled as the PDE's boundary conditions. Specifically, we use absorbing boundary conditions where p(c 1 , t) = p(c 2 , t) = 0, encoding the fact that when the preference state reaches the threshold, it is removed or absorbed by that threshold.</p><p>While this is a probability, it is still not quite the quantity we need. This describes the accumulator state rather than the probability of a choice. We require the probability of having first crossed either of the two thresholds at time t. This is often referred to as the first passage time problem. Fortunately, this first passage time probability can be directly calculated from p(x, t) by calculating the flux of probability </p><formula xml:id="formula_5">J(x, t) = v(x, t)p(x, t) − ∂ [D(x, t) 2 p(x, t)] ∂x ,<label>(3)</label></formula><p>is the probability flux at point (x,t) and the first passage time density at threshold i can be calculated as</p><formula xml:id="formula_6">f i (t) = J(c i (t), t).</formula><p>As a synopsis, here are the steps necessary to calculate the likelihood function. 1)</p><p>Transform the model from a SDE formalism to the FP formalism. 2) Solve the the FP equation in the relevant (x,t) domain determined by the model thresholds. 3) Use that solution to calculate probability fluxes at the threshold, which ultimately is the likelihood function.</p><p>Step 1 is strait-forward, any SDE of the form discussed here can be directly transformed into a FP.</p><p>Step 3 is also straightforward once p(x, t) is calculated.</p><p>Step 2, simulating the FP equation, is the most complex and requires the most care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical Solution</head><p>Here we describe the main ideas of the process for numerically simulating the accumulator state probability p(x, t) from the FP equation. We have added numerous bells and whistles to this scheme for numerical stability, generality, and numerical speed. These are further discussed in the Supplementary Information and we focus on the main ideas only here.</p><p>There are three sources of complexity in this model: the state dependent drift v(x, t), the state dependent diffusion D(x, t), and the time dependent bounds c i (t). It turns out the bounds are the most challenging to deal with. We will be using finite differences to solve this problem. This causes issues like thresholds crossing between discretized grid points and points being inside the threshold at one time and outside at the next, introducing significant error into the solution. Though these can be overcome, there is a better way to address this complexity.</p><p>In its current form, the FP model is described by a relatively simple PDE with a time changing bound. Since this time changing bound is complicated to deal with numerically, we make a change of coordinates that flattens the thresholds <ref type="bibr" target="#b7">(Crank, 1984)</ref>, at the expense of making the PDE more complicated. Note that since this is an</p><formula xml:id="formula_7">F o r R e v i e w O n l y</formula><p>exact transformation, no information about the model is lost. We introduce the following change of coordinates,</p><formula xml:id="formula_8">ε(x) = x − c 1 (t) c (t) − c 2 (t) .<label>(4)</label></formula><p>In this new coordinate (ε(x)), the thresholds are fixed at values zero and one.</p><p>Substituting into Equation <ref type="formula" target="#formula_4">2</ref>yields,</p><formula xml:id="formula_9">∂p(ε, t) ∂t = 1 s ε ds dt + dc 2 dt ∂p(ε, t) ∂ε − 1 s ∂ [v(ε, t)p(ε, t)] ∂ε + 1 2s 2 ∂ 2 [d(ε, t) 2 p(ε, t)] ∂ε 2 ,<label>(5)</label></formula><p>where p(ε, t) is the probability of accumulated evidence in the new coordinate frame ε, s = c 1 − c 2 is the separation between thresholds, and v(ε, t) and D(ε, t) are the drift and diffusion rates in the new coordinate frame. Though Equation <ref type="formula" target="#formula_9">5</ref>is more complex, it only needs to be solved on a rectangular (ε,t) domain. This transforms the space domain to (0,1), dramatically simplifying implementation.</p><p>This transformed problem now amounts to solving a PDE on a rectangular domain. To solve this, we use the second order Crank-Nicolson finite difference scheme . This is a highly robust, unconditionally stable numerical scheme used in a wide range of PDE simulation applications. We do not go into detail here, but we have extensively tested the numerical discretization used for this method and introduced adaptive stepping in coordinate t to improve it. While we do not focus on these implementation complexities here, we note that they are absolutely critical to the practical application of this method. First, they speed the numerical solution by a factor of 10x or more, which is important when integrating this into a Bayesian framework. Second, these optimizations allow the user to apply this method to a wide array of problems without having to substantially "tune" the algorithm. With this method, we can now robustly and efficiently calculate p(x, t), which can then be used to compute first passage dime probabilities and construct the desired likelihood function.</p><p>We do make a brief note about one computational complexity. As noted by Shin et al. <ref type="bibr" target="#b50">(Shinn et al., 2020)</ref>, the Crank Nicholson scheme can introduce a numerical instability arising from the initial condition to the FP equation (i.e. the start point distribution), which can be particularly problematic with time varying thresholds. This</p><formula xml:id="formula_10">F o r R e v i e w O n l y PYBEAM 13</formula><p>is a well known issue <ref type="bibr">(Østerby, 2003)</ref>. We ameliorate this by starting the PDE simulation with multiple small time steps to essentially smooth the transition from the initial condition to the remaining time domain. This adjustment removes this issue without any intervention by the user. With this and other similar optimizations, we have a highly efficient and stable algorithm for calculating choice-RT likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Inference</head><p>To perform Bayesian parameter estimation, we integrate this method of likelihood construction into the python package PyMC3 <ref type="bibr" target="#b48">(Salvatier &amp; andC. Fonnesbeck, 2016)</ref>.</p><p>PyMC3 is a highly robust, well supported python package designed specifically to perform Markov chain Monte Carlo. Using this Bayseian platform comes with a number of benefits. First, it has been tested by a wide array of researchers over a number of years. Second, it has a number of different MCMC samplers integrated into it, which is important for this application. Third, it simplifies posterior analysis since PyMC3 has a number of functionalities built in (e.g. trace plotting and posterior summary statistics)</p><p>and supporting packages such as ArviZ <ref type="bibr" target="#b32">(Kumar, Carroll, Hartikainen, &amp; Martin, 2019)</ref> are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Though PyMC3 is known principally for implementing gradient based Monte</head><p>Carlo methods like NUTS <ref type="bibr" target="#b24">(Hoffman &amp; Gelman, 2014)</ref>, we find that this algorithm is sub-optimal for this type of problem. Since these methods require the calculation of model derivatives with respect to every parameter in each step of the inference processsomething that is very slow when performed numerically -they work best when fast, analytic solutions are available. We instead utilize and recommend two different MCMC samplers which do not require gradients. The first and primary algorithm used in this work is history based Differential Evolution Markov Chain (DE-MCz) <ref type="bibr" target="#b1">(Braak &amp; Vrugt, 2008</ref>), a variant of the popular Differential Evolution Markov Chain (DE-MC)</p><p>algorithm <ref type="bibr" target="#b0">(Braak, 2006)</ref>. In brief, DE-MC runs many chains in parallel and uses information shared between chains to auto-sense the structure of the posterior and intelligently construct parameter proposals. The second algorithm, Slice <ref type="bibr" target="#b36">(Neal, 2003)</ref>, is the alternative option implemented in PyBEAM. Though Slice converges in many fewer samples than DE-MCz, this algorithm is not recommended for general use since it is about 2-5x slower than DE-MCz. Slice is only recommended for use in models with very slow convergence or models with more than one posterior mode. In these uncommon cases, DE-MCz may require many more samples than normal to reach a final solution, making Slice a useful alternative.</p><p>As demonstrated in the following results, coupling this approach to constructing model likelihoods with the capabilities of PyMC3 yields a robust and efficient tool for performing Bayesian inference with general binary choice-RT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EAMs Implemented in PyBEAM</head><p>To facilitate use of this approach while allowing maximum flexibility, we have implemented two sections of the python package implementing these methods. First, we have constructed pre-coded, default versions of a number of common models. This facilitates their rapid use with minimal interaction using the PyBEAM framework.</p><p>Second, we implement a general model class that allows users to go beyond the common models. This requires more interaction with the modeling framework, but provides increased flexibility. We address both approaches in the remainder of this section. replace z with w = z/(2a), referred to as the relative start point, which sets the accumulator start as a ratio of the threshold separation (ranging from 0 to 1). In PyBEAM, we refer to this as the "base" EAM. This EAM is sometimes referred to in the literature as the "simple DDM" since it is a DDM without inter-trial variability parameters <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>.</p><p>The next two pre-coded EAMs add additional model assumptions to the base EAM to account for other psychological factors. The first, referred to as the "leakage" model in PyBEAM, adds leaky integration to the base EAM. This changes the drift rate from a constant to v(x, t) = µ − lx, where l is the strength of leaky integration and</p><p>x is the total accumulated evidence. The second, referred to as "moving thresholds,"</p><p>adds time varying decision thresholds to the base EAM. They are free to either collapse towards zeros or move outwards, though collapse is the more common behavior in the literature. By default, three time varying decisions thresholds are implemented in PyBEAM. The first, linear, defines the decision thresholds as,</p><formula xml:id="formula_11">c 1 (t) = −c 2 (t) = a 0 − mt,<label>(6)</label></formula><p>where a 0 indicates the threshold location at time zero and m is the thresholds' slope.</p><p>The second, exponential, has thresholds defined as</p><formula xml:id="formula_12">c 1 (t) = −c 2 (t) = a 0 exp(−t/τ ),<label>(7)</label></formula><p>where a 0 , as before, is the threshold location at time zero and τ describes how quickly the threshold collapses. The last, weibull, uses a weibull distribution function for the decision threshold, given by,</p><formula xml:id="formula_13">c 1 (t) = −c 2 (t) = a 0 − a 0 (1 − c) 2   1 − exp   − t λ κ     .<label>(8)</label></formula><p>Here, a 0 has the same meaning as in the linear and exponential models. The next parameter, λ, is the scale parameter and approximately sets the time at which threshold collapse or expansion occurs. Parameter κ is referred to as the shape parameter and indicates whether the threshold behavior is logistic (κ &gt; 1) or exponential (κ &lt; 1).</p><p>Lastly, c is the collapse ratio and sets the amount the threshold collapses or expands.</p><formula xml:id="formula_14">F o r R e v i e w O n l y</formula><p>The weibull distribution is a particularly good choice for a collapsing threshold due to its flexibility in behavior <ref type="bibr" target="#b22">(Hawkins, Forstmann, Wagenmakers, Ratcliff, &amp; Brown, 2015)</ref>.</p><p>Depending upon the choice of parameters, it is able to model early collapse, late collapse, and no collapse, while simultaneously replicating logistic and exponential threshold behavior.</p><p>Though the weibull decision threshold still maintains the same functional form, we modify it slightly when running PyBEAM's MCMC sampler. Instead of sampling the shape λ and scale κ parameters directly, we instead sample from the base ten logarithm of these parameters. Since the weibull threshold can produce both logistic and exponential behavior, λ and κ have large functional parameter ranges. This makes it difficult for the MCMC algorithm to evenly sample from all parts of parameter space.</p><p>For example, common κ values for an exponential model are between 0.5 and 1, while κ values for a logistic model generally range from 1 to 10. This disparity in reasonable parameter range makes its difficult to sample from accurately. Thus, by defualt, PyBEAM samples from and reports the base ten logarithm for parameters λ and κ.</p><p>The last EAM pre-coded in PyBEAM is the Urgency Gating Model, referred to as the "UGM." <ref type="bibr" target="#b6">(Cisek, Puskas, &amp; El-Murr, 2009)</ref>. As in Trueblood et al. , it starts with the base EAM discussed above, then adds an urgency signal and leakage to both the drift and diffusion rates. The drift rate is defined as,</p><formula xml:id="formula_15">v(x, t) = µ(1 + kt) +   k 1 + kt − l   x.<label>(9)</label></formula><p>Here, µ and l are the drift and leakage rates from the base and leakage EAMs described above. The additional parameter, k, is referred to as the urgency ratio parameter and describes the strength of urgency in the model (see <ref type="bibr" target="#b53">Trueblood et al. (Trueblood et al., 2021</ref>) for further discussion). If k = 0, the model encodes no urgency and is instead the leakage EAM. If k → ∞, urgency dominates the decision process. In addition to altering the drift rate, the UGM modifies the EAM diffusion rate, D(x, t). It is given by,</p><formula xml:id="formula_16">D(x, t) = σ(1 + kt),<label>(10)</label></formula><p>where k is still the urgency ratio and σ is the same as the noise parameter from the Custom Models in PyBEAM. In addition to providing users with pre-coded models, PyBEAM also allows for the creation of user-defined custom scripts.</p><p>Instructions for creating a script can be found on https://github.com/murrowma/pybeam, but we provide a useful example here which we test in the results section of this paper.</p><p>Though we provide the UGM by default in PyBEAM (see Section ), it is known that parameter recovery for it can be challenging. The urgency parameter k is correlated with a and l, causing multiple combinations of each parameter to generate similar likelihood functions. To address this problem and improve parameter recovery, <ref type="bibr" target="#b53">Trueblood et al. (Trueblood et al., 2021)</ref> proposed an experiment with time changing stimulus information. Specifically, using a grid of pixels flashing one of two colors (blue / orange), they altered the fraction of each color on the screen at a given time while a decision was being made. So, early in the decision process, the grid may be 55 percent blue while later it might be 55 percent orange..</p><p>We implement a similar model as a custom script in PyBEAM. In the UGM, stimulus strength is encoded in the parameter µ. Thus, we modify the UGM drift rate from Equation 9 to account for the time changing stimulus information, giving</p><formula xml:id="formula_17">v(x, t) =          µ(1 + kt) + k 1+kt − l x, if t &lt; t 1 −µ(1 + kt) + k 1+kt − l x, if t ≥ t 1<label>(11)</label></formula><p>where t 1 is the time when stimulus information changes. For example, at time t &lt; t 1 , the grid may be dominated by blue which corresponds to positive µ. Then, at time t 1 , they flip to orange dominated, modeled by making µ negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In We first validate PyBEAM's likelihood construction. We do this for the three of the models discussed in Section : the base EAM, the base EAM with leaky integration, and the base EAM with weibull decision thresholds. For each model, we choose thirty parameter sets, then simulate 250 data points using Equation 1. For the base and weibull cases, we choose parameters from human data sets. Described fully in Section , we used PyBEAM to fit the base and weibull EAMs to a human data set collected by Evans et al. <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>. Thirty random parameter sets were chosen from this data for validation in this section. For the leaky integration EAM, we did not have data to draw upon, so we instead randomly chose parameters within reasonable ranges. The parameter sets from which these are drawn are located at https://github.com/murrowma/pybeam. Using the data generated from each model, we calculated the total loglikelihood of the data sets using both the Smith analytic solution discussed in Section and the PyBEAM numerical approach. The Smith analytic solution was pulled from Drugowitsch's lab github, https://github.com/DrugowitschLab/dm. The results of this are shown in <ref type="figure" target="#fig_7">Figure 2</ref>.</p><p>In panel A, we display the analytic loglikelihood versus the PyBEAM numerical loglikelihood for all parameter sets. The solid black line indicates the graphical location where the analytic and PyBEAM solutions are identical. In panel B, we report the error in the loglikelihood for each of the EAMs parameter sets. The error is calculated by taking the absolute difference between the analytic and numerical solutions. These results show that PyBEAM's numerical approach to calculating likelihood functions is highly accurate in all cases. Notably, the PyBEAM numerical solution is 5-10x faster than the Smith analytic solution with only minimal losses in precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Recovery</head><p>We next assess PyBEAM's ability to perform model and parameter recovery for EAMs of the type discussed in Section . To do this, we first simulate data from the model. Then, we use PyBEAM to fit the model to the simulated data set. Lastly, we compare both the posterior distributions output by PyBEAM to the generating parameters and the best fit choice-RT distribution to the true distribution.</p><p>We make a brief note that there are multiple ways to assess the efficacy of parameter recovery. The first and most straight forward is to determine whether the posterior distribution found matches the generating parameters. However, models, particularly more complex ones, often present parameter indeterminacy issues where a subspace of parameters can produce nearly identical model outputs. In this case, the estimation procedure may fail to recover the exact generating parameters but still fit the data well. This is a consequence of the model structure and is not a failing of the parameter estimation procedure. For this reason we visualize both posterior parameter distributions as well as the quality of fit to the data itself. Base EAM. We first demonstrate parameter recovery for the base EAM described in Section . Following the procedure outlined in , we first simulate five-hundred data points from the model using Equation 1. Then, we use PyBEAM to fit the model to this synthetic data (results in <ref type="figure" target="#fig_9">Figure 3)</ref>.  plotting the likelihood function generated by each parameter set (red). The posterior predictive displays the range of likelihood distributions which fit the data, providing an approximation of the variance in fit. In this case, the posterior predictive describes the data set well, with little variation between the predicted likelihood function and data.</p><formula xml:id="formula_18">−2 0 2 Time (s) A) Likelihood Val. PyBEAM Analytic −2 0 2 Time (s) B) Posterior Pred. PyBEAM Data 0.2 0.3 Non-decision Time (t nd ) C1) Posteriors True Posterior 0.4 0.6 Relative Start (w) C2) 0 1 2 Drift (μ) C3) 0.6 0.8 1.0 Decision Threshold (a) C4)</formula><p>The remaining panels C1-C4 display the model parameter posteriors generated by PyBEAM. The vertical blue lines indicate the true parameter values of the simulated data sets (displayed in <ref type="figure" target="#fig_9">Figure 3</ref>), while the grey histograms are the posteriors. These results show parameter recovery is excellent for the base EAM. In addition, PyBEAM is able to recover parameters quickly, with an approximate run time for this experiment of less than a minute on a 2020 Macbook Pro.</p><p>Base EAM with Leaky Integration. The second model we recover parameters from is the base EAM with leaky integration discussed in Section .</p><p>Recovering the leakage parameter from a single condition EAM can be challenging, so we instead simulate a slightly more complicated data set with two conditions: low decision threshold a 1 (condition 1) and high decision threshold a 2 (condition 2). This mimics a speed/accuracy trade-off scenario. The remaining parameters are shared between the two conditions and only the threshold parameter a differs between the two simulated conditions.</p><p>To validate PyBEAM's parameter recovery for the leakage EAM, we follow the process outlined at the beginning of the results section. We simulate 250 data points for each condition. Then, we use PyBEAM to find the model parameters which best describe this data. We report the results of parameter recovery in <ref type="figure" target="#fig_12">Figure 4</ref>.</p><p>In panel groups A and B of <ref type="figure" target="#fig_12">Figure 4</ref>, we report the likelihood validation and posterior predictive plots for condition 1 (low a) and condition 2 (high a), respectively.</p><p>As with the previous example, likelihood validation compares the PyBEAM likelihood to the analytic (using the Smith <ref type="bibr" target="#b51">(Smith, 2000)</ref> approach in this case) for the known   Panel group C displays the posteriors for both fits. We leave out the non-decision time and relative start posteriors since they are easily fit (see Supplementary Information for these fits). We find that the shared parameters, drift and leakage, have posteriors (grey) which closely match the true values (blue lines). The individual caution parameters a 1 and a 2 also match expectation closely. Parameter recovery is once again highly efficient, taking approximately a minute on a 2020 MacBook Pro.</p><p>Base EAM with Moving Decision Thresholds. We next use PyBEAM to recover parameters from the base EAM with moving decisions thresholds. Specifically, we use the model with weibull decision thresholds described in Section . To validate PyBEAM's parameter recovery for this model, we again follow the methodology described at the beginning of the results. First, we choose our parameters. Since the weibull threshold is capable of producing both logistic and exponential thresholds, we choose two parameter sets to replicate these behaviors: one with k &gt; 1 and one with k &lt; 1. The weibull threshold is also able to produce varying amounts of collapse via the c parameter. For convenience, for both parameter sets, we set this to -1.0, indicating that the thresholds will collapse to zero. As with the base EAM, we simulate 500 data points for each parameter set. We then use PyBEAM to recover parameters from the simulated data. We show the parameter recovery results in <ref type="figure" target="#fig_14">Figure 5</ref>.</p><p>Panel groups A and B display the likelihood validation and posterior predictive (described in Section )for the k &gt; 1 and k &lt; 1 parameter sets, respectively. In both cases, the PyBEAM numerical solution produces the same result as the analytic solution, and the posterior predictive matches the data set closely. Panel group C displays the posterior predictive for the threshold itself in each case. The blue line corresponds to the true threshold, while the red lines are generated by drawing 100 random random parameter sets from the posterior and plotting their threshold shape.  In both the k &gt; 1 and k &lt; 1 cases, the posterior produces time varying thresholds that closely match that used to generate the data.</p><p>Panel groups D and E display the posteriors for the k &gt; 1 and k &lt; 1 parameter sets, respectively. We only display the posteriors related to the decision thresholds (recovery for other parameters can be found in the Supplementary Information). As discussed in Section , for the scale λ and shape k parameters, we sampled from and report here the log of these parameter values. Both have large reasonable parameter regimes, so sampling in log space guarantees we get good coverage of all possible parameter space.</p><p>For the k &gt; 1 case, we see good agreement between the true parameter values (blue) and the PyBEAM prediction (grey histogram). However, for the k &lt; 1 case, the posteriors predictive for the threshold is slightly different than that of the simulated values, even though the likelihood and threshold recovery from panel groups B and C are very good. This is the result of parameter sloppiness inherent to the weibull threshold model, where multiple parameter combinations can produce similar likelihood functions. Though this is a common problem with the weibull model, particularly in the k &lt; 1 parameter regime, it doesn't significantly affect the interpretation of the results.</p><p>Different parameter sets produce qualitatively similar decision thresholds.</p><p>Urgency Gating Model with Changing Information. The final model we recover parameters from is the UGM with time changing stimulus information described in Section . In this model, the drift rate takes the form of Equation 11, where stimulus information changes from positive to negative at time t 0 . Note that since this is a more complex synthetic experimental design (time changing stimuli), fits here are implemented with the custom component of PyBEAM. We fit the model for two data conditions: low drift (condition 1) and high drift (condition 2), corresponding to low and high quality stimulus information. The parameters used are displayed in 2.</p><p>To validate PyBEAM's parameter recovery for the UGM with changing information, we simulate 500 data points for each condition using the parameters in <ref type="table">Table 2</ref>. As before, we then use PyBEAM to find the model parameters which best  <ref type="table">Table 2</ref> Parameters used for the UGM with changing information example. Parameters are t nd , the non-decision time; w, the relative start point; µ, the drift rate; l, the leakage rate; k, the urgency parameter; a, the decision threshold location; and t 1 , the time when stimulus information flips. describe this data. We report the results of parameter recovery in <ref type="figure" target="#fig_17">Figure 6</ref>.</p><p>In Panels A1 and B1, we display the likelihood validation for data conditions and 2, respectively. Since the UGM has no analytic solution, we instead simulate 25,000 data points from the model for each condition (black histograms) to compare our numerical result to (red). In both cases, the PyBEAM likelihood is once again highly accurate. In Panels A2 and B2, we report the posterior predictive for conditions 1 and 2, respectively. PyBEAM once again performs well at fitting the data.</p><p>In panel group C we report the posteriors output by PyBEAM. As before, we omit the non-decision time and threshold locations results here (reported in the <ref type="figure">Supplementary Information)</ref>. In panel C1, we report the drift rate posteriors for both condition 1 (left histogram) and condition 2 (right histogram). The remaining three posteriors in panels C2-4 are for the leakage, urgency, and decision threshold, and are shared between the two conditions. For all cases, the posteriors (grey) match the true data values (blue) well, indicating effective parameter recovery.  Since real data is always messier than simulated data, we last demonstrate the use of PyBEAM to perform parameter inference using choice-RT experimental data collected by Evans et al. <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>. This data set consists of three different experiments. In each, participants made decisions about direction of dot motion in a random dot kinematogram presented at different coherence's: 0%, 5%, 10%, and 40%. In the first experiment, sixty-three participants were instructed to maximize reward rate; in the second experiment, seventy-one participants were given a decision deadline; and in the third experiment, one hundred and fifty-four participants were instructed to emphasize speed. The goal of these experiments was to examine if and when participants might utilize collapsing thresholds. To answer this questions, they used a hierarchical Bayesian approach to fit three evidence accumulation models to their data: the base EAM discussed in Section ; a base EAM with parameter inter-trial variability; and a base EAM with weibull decision thresholds.</p><p>Our goal here is to fit models with and without changing thresholds to each of these data sets and compare results to those obtained by Evans et al <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>. The purpose here is not to study the psychological question motivating their work. Rather, the purpose is to assess the efficacy of this method and compare its conclusions to theirs. Their modeling approach would be considered the "gold standard" for this type of modeling. Thus we use it as a point of comparison while noting that: 1) their implementation utilized fully custom code compared to PyBEAM's more user friendly package approach and 2) PyBEAM is likely faster than their Smith solution approach <ref type="bibr" target="#b51">(Smith, 2000)</ref>. Though we cannot directly compare the computational speed of PyBEAM to the Evan's approach, the numerical integration required by it scales quadratically with changes in resolution <ref type="bibr" target="#b3">(Buonocore, Nobile, &amp; Ricciardi, 1987)</ref>, while Crank-Nicolson's speed scales linearly with resolution. In our experience, this results in a 5-10x slowdown when compared to versus the PyBEAM approach.</p><p>We do make a few notable changes in our fitting methodology compared to that in Evans et al. First, we do not use a hierarchical approach and instead fit the model to but this is left for future development. Second, we do not model inter-trial variability since 1) two of the models fit by Evans did not include it and 2) these parameters are usually not recoverable "Estimating across-trial variability parameters of the Diffusion Decision Model: Expert advice and recommendations" (2018). As discussed in the discussion, inter-trial variability in the start point and the non-decision distribution could be readily incorporated into PyBEAM, though inter-trial variability in all other parameters would incur significant extra computational cost.</p><p>We first examine the Choice Proportion (CP) and choice-RT predictions of the model in <ref type="figure" target="#fig_21">Figure 7</ref>. Panel groups A1, B1, and C1 compare the CP predicted by the model to that of the data for Experiments 1, 2, and 3, respectively. For all three Experiments, we find good model fits, with correlation coefficients R 2 far exceeding 0.9.</p><p>Additionally, for all three models, the correlation between model and data is very similar for both the flat and weibull decision thresholds.</p><p>Panel groups A2, B2, and C3 compare the average choice-RT of the model to that of the data for Experiments 1, 2, and 3, respectively. For Experiments 1 and 3, both models describe the data well, with R 2 values for both exceeding 0.9. However, in Experiment 2, we see a distinct preference for the weibull model. This matches closely the conclusions of Evans, who noted that only Experiment 2 displays clear evidence for collapsing decision thresholds.</p><p>In addition to reporting CP and choice-RT data comparison, we also report DIC values for each model in <ref type="figure" target="#fig_23">Figure 8</ref>. Specifically, we plot the absolute difference between base EAM's DIC (DIC f ) and the base EAM weibull decision thresholds' DIC (DIC w ).</p><p>If this value is positive, the moving thresholds model is preferred; if it is negative, the base EAM is preferred.</p><p>We see that, for Experiment 1, the vast majority of participants favored the moving thresholds model. The participants which preferred the base EAM over the moving thresholds version did so with small DIC differences. For Experiment 3, There is roughly an even split between those which favored the base and moving threshold   For Experiment 2, we see that all participants favor the moving thresholds model, with most strongly favoring (DIC difference greater than 10) that model.</p><p>Our DIC results also match closely those determined by Evans. In general, the base EAM was only rarely preferred for Experiment 1 and Experiment 2 in their work, while commonly preferred in Experiment 3. We see a similar pattern in our results. We last report the decision thresholds predicted by both the flat and weibull models in <ref type="figure" target="#fig_26">Figure 9</ref>. In the first row, we display the threshold which produces the max loglikelihood for each participant. In row two, we display the average threshold, calculated by averaging together all of the participants decision thresholds. We find that, for Experiments 1, little collapse occurs for most participants, producing a threshold similar to that of the base EAM. This mathches well with the CP and choice-RT analyses which indicated that there was little preference between the two models. Notably, the shape of the average threshold is comparable to that of Evan's results (shown in <ref type="figure" target="#fig_7">Figure 2</ref> of their publication <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>).</p><p>In Experiment 2, we see that all participants exhibit thresholds which collapse substantially, suggesting that a collapsing threshold best describes the data. This matches the results of the CP, choice-RT, and DIC results which indicated that Lastly, in Experiment 3, we observe decision thresholds which vary substantially from one participant to the next. For this data set, some participants had mean choice-RTs near a second, while others had mean choice-RTs upwards of three and four seconds. This difference in response time produces dramatically different thresholds for each individual which make describing the average behavior challenging. In spite of this, the average threshold we calculate is very similar to that determined by Evans (shown in <ref type="figure" target="#fig_7">Figure 2</ref> of their publication <ref type="bibr" target="#b14">(Evans, Hawkins, &amp; Brown, 2020)</ref>). Further, it also supports the conclusions of the CP, choice-RT, and DIC results which suggested that Experiment 3 had no clear model preference.   In this work, we introduced the Python package PyBEAM to provide an easy to use tool for Bayesian inference of complex EAMs. Unlike previous methods which have either generated likelihood functions using simulated SDEs or slow analytic solutions, PyBEAM instead uses the Fokker-Planck equation, dramatically improving both accuracy and speed. In addition to being fast, it increases flexibility by allowing custom inputs for the drift rate, diffusion rate, and decision thresholds. PyBEAM then pairs this likelihood construction method with the Python package PyMC3 <ref type="bibr" target="#b48">(Salvatier &amp; andC. Fonnesbeck, 2016)</ref> for rapid Bayesian inference of model parameter sets. Using PyBEAM, more complex models can be fit to data with both high precision and high speed. Further, to make this type of modeling accessible to others, we provide a Python based package which allows exploration of complex models with a relatively low cost of entry.</p><p>Looking forward, we plan to incorporate a number of additional features into the PyBEAM framework. First, we plan to incorporate hierarchical capability. Second, we plan to incorporate the ability to include inter-trial variability in the threshold and non-decision time parameters. These additions can be made without substantial changes to the main numerical engine within PyBEAM. We have opted to leave these for a v2 development as our focus here was producing the most user friendly framework possible. Finally, this approach can likely be extended to model multi-alternative choice models. This will however require significantly more numerical development and is beyond the scope of this initial article.  Supplementary Information for PyBEAM: A Bayesian approach to parameter inference for a wide class of binary evidence accumulation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solving the forwards Fokker-Planck equation using the Crank-Nicolson method</head><p>To numerically solve the Fokker-Planck equation discussed in the main body of the publication, PyBEAM uses the Crank-Nicolson method . This is a finite difference method of second order accuracy which, for the Fokker-Planck equation, requires discretization in two dimensions: space and time. To simplify the spatial discretization, we introduced the following change of coordinates,</p><formula xml:id="formula_19">ε(x) = x − c 1 (t) c 1 (t) − c 2 (t) .<label>(1)</label></formula><p>where x is the accumulator state, c 1 is the upper decision threshold, and c 2 is the lower decision threshold. In this new coordinate, the Fokker-Planck equation becomes,</p><formula xml:id="formula_20">∂p(ε, t) ∂t = 1 s ε ds dt + dc 2 dt ∂p(ε, t) ∂ε − 1 s ∂ [v(ε, t)p(ε, t)] ∂ε + 1 2s 2 ∂ 2 [d(ε, t) 2 p(ε, t)] ∂ε 2 ,<label>(2)</label></formula><p>where p(ε, t) is the probability of accumulated evidence in the new coordinate frame ε, s = c 1 − c 2 is the separation between thresholds, and v(ε, t) and d <ref type="bibr">(ε, t)</ref> are the drift and diffusion rates in the new coordinate frame.</p><p>This change of coordinates transforms the space domain to (0,1). Thus, the spatial discretization is independent of the decision threshold location. In PyBEAM, the default spatial step size is ∆ε = 0.01, which provides sufficient accuracy for nearly all models. It can be modified if needed, with documentation for how to do this located at https://github.com/murrowma/pybeam.</p><p>The time discretization required to guarantee high precision is more complicated.</p><p>Since PyBEAM uses computationally taxing Bayesian methods, choosing the correct time step ∆t is critical for maintaining high accuracy without sacrificing speed. To approximate an appropriate time step, PyBEAM simulates the model ten times using Equation (1) from the main publication. Then, we determine the average first passage time and multiply it by 0.025 to set the time step. This is the default time resolution in PyBEAM, and can be modified per the instructions at https://github.com/murrowma/pybeam.</p><p>In addition to approximating an appropriate time step, two additional issues occur due to the Crank-Nicolson method. The first is due to the discontinuity located at the initial condition. At time t = 0, the initial condition for p(ε, t) is</p><formula xml:id="formula_21">f (x) =          p(ε, 0) = δ(ε) if ε = w, 0 otherwise,<label>(3)</label></formula><p>where w is the relative start point and δ(ε) is the Dirac delta function. The delta function initial condition is discontinuous, causing large, damped oscillations in the numerical solution for p(ε, t) around the spatial region of the discontinuity. The damping time scale of the oscillations are proportional to the ratio ∆t/∆ε 2 . If this ratio is large, then the oscillations persist for longer times. If it is small, the oscillations damp more quickly. Fortunately, the effect of the oscillations are localized to the spatial region surrounding the initial condition. In cases where decision thresholds are not moving, the effect of the oscillations near the thresholds is insignificant. However, if you have a moving threshold which crosses the spatial coordinate of the initial condition (i.e. at some time t, threshold c(t) = z), the oscillations near the initial condition propagate into the first passage time distribution. This causes large inaccuracies in the likelihood function which are unacceptable for parameter estimation.</p><p>There are a number of ways to manage this problem, several of which are summarized by Østerby <ref type="bibr">(Østerby, 2003)</ref>. A more recent approach taken by Boehm et al. <ref type="bibr" target="#b64">(Boehm, Cox, Gantner, &amp; Stevenson, 2021</ref>) simplifies the backwards Fokker-Planck equation by using the known series solution  for the flat threshold, constant drift version of the equation. Doing so eliminates the discontinuity, avoiding the problem entirely.</p><p>In PyBEAM, we take an approach discussed in Østerby <ref type="bibr">(Østerby, 2003)</ref>. Since the oscillations about the initial condition are dependent upon the ∆t/∆x 2 ratio, if ∆t is small, the oscillations go away quickly. However, ∆t need not remain small always, just at the beginning of the numerical solution. So, in PyBEAM, we take twenty-five time steps ∆t i that are one-hundred times smaller than the approximated time step, eliminating the oscillations from the solution.</p><p>The second time discretization problem occurs when thresholds change location rapidly. Though we have transformed the spatial domain to (0,1), rapid changes in threshold location still produce large inaccuracies in the first passage time distribution.</p><p>This can cause otherwise appropriate time steps to fail in specific, localized regions of the likelihood function. The easiest way to handle this is to make the time step small so that your solution always remains precise; however, as discussed above, since Bayesian inference is computationally taxing, you ideally want the largest time step possible to increase speed.</p><p>To circumvent this speed-accuracy trade off, PyBEAM implements two changes to the time discretization. First, after the initial twenty-five time steps ∆t i used to reduce solution oscillations are completed, the time step is linearly increased over one-hundred time steps from the initial time step size ∆t i to the estimated time step size ∆t. This prevents thresholds which collapse early in the decision process (such as exponential thresholds) from reducing solution precision by guaranteeing that we have small time steps for small time t.</p><p>Second, PyBEAM monitors the distance between decision thresholds and, if the distance changes quickly, decreases the time step size to an acceptable level. The algorithm is roughly as follows. At time t, the decision threshold separation s n is calculated. The decision threshold separation is then calculated at the previous and next time steps, s n−1 and s n+1 , respectively. Time step n − 1 occurs at t − ∆t, while time step n + 1 is at t + ∆t. We then calculate the ratio of collapse s r for time steps n − 1 and n + 1. For example, for n + 1, this ratio is</p><formula xml:id="formula_22">s r,n+1 = abs(s n − s n+1 )/s n .<label>(4)</label></formula><p>If this ratio or that for time step n − 1 is more than 2%, the time step is halved. The process is repeated until a time can be found which limits the collapse ratio to 2% or less. and 6, we excluded posteriors which were easy to recover. The figures in this section add to those figures the excluded posteriors.</p><p>In <ref type="figure" target="#fig_2">Figure 1</ref>, we include the non-decision time and relative start posteriors excluded in <ref type="figure" target="#fig_12">Figure 4</ref> of the main publication. In <ref type="figure" target="#fig_7">Figure 2</ref>, we include the non-decision time, relative start, and drift rate posteriors excluded in Figure of the main publication. And, in <ref type="figure" target="#fig_9">Figure 3</ref>, we include the non-decision time and relative start posteriors excluded in <ref type="figure" target="#fig_17">Figure 6</ref> of the main publication. Jupyter notebooks <ref type="bibr">(Kluyver et al., 2016)</ref> which produce these figures and the main publication versions can be found on https://github.com/murrowma/pybeam.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Model Schematic. Evidence is accumulated (blue line) starting from an initial bias (black dot) until one of the two choice thresholds, c 1 or c 2 , is reached (red dot), triggering a decision. The distance between thresholds indicates an individuals degree of caution which can change or remain constant as a function of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Default EAMs .</head><label>.</label><figDesc>PyBEAM contains pre-coded versions of EAMs commonly used in the literature. The basis for all pre-coded models is the EAM described in Section , which is then modified to match the specific model needs. The simplest of these is an EAM with constant non-decision time t nd ; constant drift rate v(x, t) = µ; constant diffusion D(x, t) = σ, usually set to either σ = 0.1, 1.0; accumulation start point z; and flat, symmetric decision thresholds c 1 (t) = −c 2 (t) = a. For convenience, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>Validation of PyBEAM likelihood construction. A) Analytic loglikelihood versus PyBEAM's numerical loglikelihood of simulated data. Thirty parameter sets were chosen for each of the listed EAM types, being the base EAM (blue dot), and the base EAM with leaky integration (red upwards triangle), and the base EAM with weibull collapsing thresholds (orange downwards triangle). The black line indicates where the analytic and numerical are equal. B) Error in loglikelihood calculation for each parameter set. Calculated as the difference between the analytic and numerical loglikelihood for each parameter set. Marker shapes and colors are the same as in panel A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Results from the base EAM parameter recovery example. A) Likelihood function validation for the base EAM. The red line is the PyBEAM numerical solution, while the blue dashed line is the analytic solution (Navarro &amp; Fuss, 2009). B) PyBEAM posterior predictive. Red lines are the PyBEAM solutions, while the grey bars are the simulated data. C) PyBEAM posteriors for each of the base EAM parameters. Grey bars are the PyBEAM posteriors, while the blue lines are the true values used to generate the data. C1 displays the non-decision time posterior, with a true value of t nd = 0.25. C2 displays the relative start point posterior, with a true value of w = 0.5. C3 displays the drift rate posterior, with a true value of µ = 1.0. C4 displays the decision threshold posterior, with a true value of a = 0.75. Panel A of Figure 3 compares the analytic (dashed blue) and PyBEAM (red)likelihood functions using the known generating parameters. The results exactly overlap, demonstrating once again that the PyBEAM approach produces highly accurate RT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>B displays the simulated data and the posterior predictive fit given by PyBEAM. In this example and all going forwards, we generate the posterior predictive by drawing 100 random parameter sets from the PyBEAM posteriors and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 .</head><label>4</label><figDesc>Results from the base EAM with leaky integration example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low caution. In A1, we plot the PyBEAM solution in red and the analytic solution in blue. In A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high caution. Colors have same meaning as in A. C) Posteriors for both conditions. Vertical blue lines indicate the true parameter values, while the grey bars are the PyBEAM posteriors. The drift (C1) and leakage (C2) posteriors are shared between conditions with true values of µ = 1.0 and l = 3.0, respectively. The threshold locations for the low caution a 1 and high caution a 2 conditions are displayed in C3 and C4, with true values of a 1 = 0.5 and a 2 = 0.75, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>the posterior predictive panels compare the data to the RT distributions resulting from the PyBEAM model fits. Results demonstrate that once again, PyBEAM produces highly accurate likelihood functions and fits the generating data well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 .</head><label>5</label><figDesc>Results from the collapsing thresholds example. A) Likelihood validation and posterior predictive for the k &gt; 1 parameter set. In A1, we plot the PyBEAM likelihood function in red and the analytic likelihood function in blue. In A2, the PyBEAM likelihood predictions are in red while the simulated data is grey. B) Likelihood validation and posterior predictive for the k &lt; 1 parameter set. Colors have same meaning as in A. C) Predicted thresholds for both the k &gt; 1 and k &lt; 1 parameter sets. Red lines show the PyBEAM predicted thresholds, while the blue line is the true threshold. D) Posteriors for the k &gt; 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. D1 contains the threshold start posterior, with true value a 0 = 1.0. D2 contains the log of the shape parameter, with true value lambda = 1.0. D3 contains the log of the scale parameter, with true value k = 3.0. E) Posteriors for the k &lt; 1 parameter set. Colors have same meaning as in D. E1 contains the threshold start posterior, with true value a 0 = 1.0. E2 contains the log of the shape parameter, with true value lambda = 5.0. E3 contains the log of the scale parameter, with true value k = 0.67.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 .</head><label>6</label><figDesc>Results from the UGM with changing information example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low drift rate. In panel A1, we plot the PyBEAM solution in red and the analytic solution in blue. In panel A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high drift rate. Colors have same meaning as in A. C) Posteriors for all conditions. Blue indicates the true parameter value, while grey are the PyBEAM posteriors. Panel C1 contains the posteriors for both the low (left histogram) and high drift rate (right histogram)conditions, with drift rates of µ = 0.5 and µ = 2.0, respectively. Panels C2-C4 contains the leakage, urgency, and decision threshold posteriors which are shared between both conditions. These have true values of l = 3.0, k = 1.0 and a = 1.0, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>It is in principle possible to extend PyBEAM to fit hierarchical models,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 7 .</head><label>7</label><figDesc>Choice proportion (CP) and average choice-RT results from PyBEAM. Panels plot the model predictions on the vertical axis and the data values on the horizontal axis. All three experiments share the same legend displayed in the plot's first row. The text on the figure indicates which model is used, either the base EAM (Base) or the weibull moving thresholds EAM (Weib.), and the correlation coefficient R 2 of the model and data. A) CP (A1) and choice-RT (A2) results for Experiment 1. B) CP (B1) and choice-RT (B2) results for Experiment 2. C) CP (C1) and choice-RT (C2) results for Experiment 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 8 .</head><label>8</label><figDesc>Absolute difference between the base EAM's DIC (DIC f ) and the weibull moving thresholds' DIC (DIC w ) for Experiment 1 (A), Experiment 2 (B), and Experiment 3 (C). Participants are ordered by how the large this difference is, with smallest on the left and largest on the right. If the absolute difference is positive, the moving thresholds EAM is favored. If negative, the base EAM is favored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>the preferred model. This also matches the results of Evans, which indicated that Experiment 2 had the clearest support for collapsing decision thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 9 .</head><label>9</label><figDesc>Upper decision thresholds predicted by the model for Experiment 1 (A), Experiment 2 (B), and Experiment 3 (C) versus time. The upper and lower thresholds are symmetric, so the upper is only displayed here. Black lines are for the base EAM, while the red lines are for the weibull moving thresholds EAM. The first row displays the decisions thresholds for each participant, while the second row averages all decision thresholds together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>are one of the most dominant classes of computational models used to study decision making behavior. However, to date, few tools are available which allow researchers to study complex EAMs. Further, Bayesian methods have generally been too slow for practical use, restricting to model fitting to techniques like max loglikelihood or quantile maximization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>In the main body of the publication, we provided figures demonstrating that PyBEAM can recover parameters from a number of common models. InFigures 4, 5,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 1 .</head><label>1</label><figDesc>Results from the base EAM with leaky integration example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low caution. In A1, we plot the PyBEAM solution in red and the analytic solution in blue. In A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high caution. Colors have same meaning as in A. C) Posteriors for both conditions. Vertical blue lines indicate the true parameter values, while the grey bars are the PyBEAM posteriors. The drift (C1) and leakage (C2) posteriors are shared between conditions with true values of µ = 1.0 and l = 3.0, respectively. The threshold locations for the low caution a 1 and high caution a 2conditions are displayed in C3 and C4, with true values of a 1 = 0.5 and a 2 = 0.75, respectively. Panels C5 and C6 contain the non-decision time and relative start parameters, respectively, with true values of t nd = 0.25 and w = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 2 .</head><label>2</label><figDesc>Results from the collapsing thresholds example. A) Likelihood validation and posterior predictive for the k &gt; 1 parameter set. In the first panel, we plot the PyBEAM likelihood function in red and the analytic likelihood function in blue. In the second panel, the PyBEAM likelihood predictions are in red while the simulated data is grey. B) Likelihood validation and posterior predictive for the k &lt; 1 parameter set. Colors have same meaning as in A. C) Predicted thresholds for both the k &gt; 1 and k &lt; 1 parameter sets. Red lines show the PyBEAM predicted thresholds, while the blue line is the true threshold. D) Posteriors for the k &gt; 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. D1 contains the threshold start posterior, with true value a 0 = 1.0. D2 contains the log of the shape parameter, with true value lambda = 1.0. D3 contains the log of the scale parameter, with true value k = 3.0. D4, D5, and D6 contain the non-decision time, relative start, and drift rate posteriors, respectively. These have true values of t nd = 0.25, w = 0.5, and mu = 1.0. E) Posteriors for the k &lt; 1 parameter set. Blue lines indicate the true parameter values, while the PyBEAM posteriors are grey. E1 contains the threshold start posterior, with true value a 0 = 1.0. E2 contains the log of the shape parameter, with true value lambda = 5.0. E3 contains the log of the scale parameter, with true value k = 0.67. E4, E5, and E6 contain the non-decision time, relative start, and drift rate posteriors, respectively. These have true values of t nd = 0.25, w = 0.5, and mu = 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 3 .</head><label>3</label><figDesc>Results from the UGM with changing information example. A) Likelihood validation (A1) and posterior predictive (A2) for condition 1, low drift rate. In panel A1, we plot the PyBEAM solution in red and the analytic solution in blue. In panel A2, the PyBEAM data fit is in red while the simulated data is grey. B) Likelihood validation (B1) and posterior predictive (B2) for condition 2, high drift rate. Colors have same meaning as in A. C) Posteriors for all conditions. Blue indicates the true parameter value, while grey are the PyBEAM posteriors. Panel C1 contains the posteriors for both the low (left histogram) and high drift rate (right histogram) conditions, with drift rates of µ = 0.5 and µ = 2.0, respectively. Panels C2-C4 contains the leakage, urgency, and decision threshold posteriors which are shared between both conditions. These have true values of l = 3.0, k = 1.0 and a = 1.0, respectively. Panels C5 and C6 contain the non-decision time and relative start posteriors, with true values of t nd = 0.25 and w = 0.5, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>similarly except that each chain only looks at its own history, not other chains. Based on PyMC3's analysis of this algorithm, DE-MCz is able to achieve comparable results to DE-MC with many fewer chains (DEMetropolis(Z): Population vs. History efficiency comparison, 2022), making it an attractive algorithm for use on a personal computer with a limited number of cores.</figDesc><table><row><cell>F o</cell></row><row><cell>r</cell></row><row><cell>R e v</cell></row><row><cell>i e w</cell></row><row><cell>O n</cell></row><row><cell>l</cell></row><row><cell>y</cell></row></table><note>The history based version, DE-MCz, works</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A markov chain monte carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J F</forename><surname>Braak</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-006-8769-1</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differential evolution markov chain with snooker updater and fewer chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J F T</forename><surname>Braak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Vrugt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-008-9104-9</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="435" to="446" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The simplest complete model of choice response time: Linear ballistic accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2007.12.002</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new integral equation for the evaluation of first-passage-time probability densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buonocore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Nobile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ricciardi</surname></persName>
		</author>
		<ptr target="https://doi.org/1427102" />
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="784" to="800" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cognitive and neural bases of multi-attribute, multi-alternative, value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chartr: An r toolbox for modeling choice and response times in decision-making tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2019.108432</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="page">328</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decisions in changing conditions: The urgency-gating model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cisek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Puskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>El-Murr</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1844-09.2009</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="11560" to="11571" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Free and moving boundary problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Crank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Crank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nicolson</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0305004100023197</idno>
		<imprint>
			<date type="published" when="1947" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="50" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Population vs. history efficiency comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demetropolis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cost of accumulating evidence in perceptual decision making</title>
		<idno type="DOI">10.1523/JNEUROSCI.4010-11.2012</idno>
		<ptr target="https://docs.pymc.io/en/v3/pymc-examples/examples/samplers/DEMetropolisZ_EfficiencyComparison.html." />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neruoscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3612" to="3628" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The quality of response time data inference: A blinded, collaborative assessment of the validity of cognitive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dutilh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Annis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cassey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Grasman</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-017-1417-2</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1051" to="1069" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating across-trial variability parameters of the diffusion decision model: Expert advice and recommendations</title>
		<idno type="DOI">10.1016/j.jmp.2018.09.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="46" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refining the law of practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000105</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="592" to="605" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The role of passing time in decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000725</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="326" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The impact of presentation order on attraction and repulsion effects in decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<idno type="DOI">10.1037/dec0000144</idno>
	</analytic>
	<monogr>
		<title level="j">Decision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Response-time data provide critical constraints on dynamic models of multi-alternative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>multi-attribute choice</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<idno type="DOI">10.3758/s13423-018-1557-z</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="901" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A parameter recovery assessment of time-variant models of decision-making. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-019-01218-0</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A reinforcement learning diffusion decision model for value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fontanesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-018-1554-2</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1099" to="1121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf" />
		<title level="m">Sequential hypothesis testing under stochastic deadlines. NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2410-14.2015</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2476" to="2484" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quantile maximum likelihood estimation of response time distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03196299</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="401" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<ptr target="https://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1593" to="1623" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A practical guide to the probability density approximation (pda) with improved implementation and error characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2015.08.006</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">69</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A joint deep neural network and evidence accumulation modeling approach to human decision-making with naturalistic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42113-019-00042-1</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian analysis of the piecewise diffusion decision model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-017-0901-y</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="730" to="743" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new framework for modeling decisions about changing information: The piecewise linear ballistic accumulator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2015.11.002</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kluyver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bussonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frederic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jupyter notebooks -a publishing format for reproducible computational workflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Willing</surname></persName>
		</author>
		<idno type="DOI">10.3233/978-1-61499-649-1-87</idno>
	</analytic>
	<monogr>
		<title level="m">Positioning and power in academic publishing: Players, agents and agendas</title>
		<editor>F. Loizides &amp; B. Schmidt</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual fixations and the computation and comparison of value in simple choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Armel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn.2635</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1292" to="1298" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Arviz a unified library for exploratory analysis of bayesian models in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01143</idno>
		<idno>doi: 10.21105/joss.01143</idno>
		<ptr target="https://doi.org/10.21105/joss.01143" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page">1143</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Impact of context information on metaphor elaboration a diffusion model study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lerche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Christmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.1027/1618-3169/a000422</idno>
	</analytic>
	<monogr>
		<title level="j">Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="370" to="384" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-018-1153-1</idno>
		<title level="m">Parallel probability density approximation. Behavior research methods</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2777" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast and accurate calculations for first-passage times in wiener diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Fuss</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2009.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1056562461</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="705" to="767" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Short-term memory scanning viewed as exemplar-based categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fific</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0022494</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="315" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An exemplar-based random walk model of speeded classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Palmeri</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.104.2.266</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="300" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Five ways of reducing the crank-nicolson oscillations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Østerby</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:BITN.0000009942.00540.94</idno>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="811" to="822" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using response time distributions and race models to characterize primacy and recency effects in free recall initiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Osth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000149</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Stochastic processes in polymeric fluids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Öttinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-58290-5</idno>
		<ptr target="https://link.springer.com/book/10.1007/978-3-642-58290-5" />
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.85.2.59</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The diffusion decision model: Theory and data for two-choice decision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2008.12-06-420</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling response times for two-choice decisions. psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9280.00067</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<idno type="DOI">10.3758/BF03196302</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="438" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Connectionist and diffusion models of reaction time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Zandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.106.2.261</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic programming in python using pymc3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V W</forename><surname>Fonnesbeck</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.55</idno>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">55</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Linking theoretical decision-making mechanisms in the simon task with electrophysiological data: A model-based neuroscience study in human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Servant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montagnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burle</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_00989</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1501" to="1521" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A flexible framework for simulating and fitting generalized drift-diffusion models. eLife, 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.56938</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic dynamic models of response time and accuracy: A foundational primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmps.1999.1260</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="463" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optimal policy for value-based decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms12400</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Urgency, leakage, and the relative nature of information processing in decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000255</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The impact of speed and bias on the cognitive processes of experts and novices in medical image decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Seegmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Douds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Compton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Szentirmai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Eichbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<idno type="DOI">10.1186/s41235-018-0119-2</idno>
		<title level="m">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A bayesian framework for simultaneously modeling neural and behavioral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2013.01.048</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Why more is better: Simultaneous modeling of eeg, fmri, and behavioral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Norcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<idno type="DOI">10.1016/j.neuroimage.2015.12.030</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="96" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A generalized, likelihood-free method for posterior estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Sederberg</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-013-0530-0</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="250" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Informing cognitive abstractions through neuroimaging: The neural drift diffusion model. psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Maanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.12.003</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="336" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Approximating bayesian inference through model simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.06.003</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="826" to="840" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The time course of perceptual choice: The leaky, competing accumulator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.108.3.550</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="592" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A diffusion model account of criterion shifts in the lexical decision task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2007.04.006</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="159" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast solutions for the first-passage distribution of diffusion models with space-time-dependent drift functions and time-dependent boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2021.102613</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">102613</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Crank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nicolson</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0305004100023197</idno>
		<imprint>
			<date type="published" when="1947" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="50" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kluyver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bussonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frederic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Jupyter notebooks -a publishing format for reproducible computational workflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Willing</surname></persName>
		</author>
		<idno type="DOI">10.3233/978-1-61499-649-1-87</idno>
	</analytic>
	<monogr>
		<title level="m">Positioning and power in academic publishing: Players, agents and agendas</title>
		<editor>F. Loizides &amp; B. Schmidt</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast and accurate calculations for first-passage times in wiener diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Fuss</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2009.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Five ways of reducing the crank-nicolson oscillations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Østerby</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:BITN.0000009942.00540.94</idno>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="811" to="822" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
