<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policies in sequential decision-making</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Policies in sequential decision-making</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>uncertainty</term>
					<term>meta-decision making</term>
					<term>Markov Decision Process</term>
					<term>heuristics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Humans often need to make decisions that take future situations into account. This kind of sequential choice entails a trade-off between the present and the future, where decision-makers have to decide to accept or reject the currently available option. The approach-avoidance conflict, on the other hand, is a frequently used framework in psychology to study the behavioral trade-off regarding appetitive and aversive stimuli. This study investigates the influence of approach and avoidance conditions on sequential choice. Therefore, we developed experimental conditions in which participants should either approach or avoid environmental dangers in order to optimize their behavior. The main objective is to evaluate meta-decision making strategies under ecologically valid conditions. To accomplish this, we developed a game-like foraging paradigm with probabilistic outcomes where people could use different types of decision rules (or policies). While some policies would only account for one variable of our game environment, others would account for different variables depending on the time-point and the situation. Model comparisons allowed us to test whether participants would rely on a rather myopic/heuristic or a forward-looking policy. We then tested whether policies differed depending on our task&apos;s approach and avoidance condition. Our findings suggest that people generally use a myopic strategy in sequential decision-making. However, we observe increased optimal behavior in situations where the environment bears higher dangers that should be avoided. We suspect the underlying mechanisms to be linked to an internal energetic trade-off between efficient and accurate meta-decision making depending on the risks imposed by the environment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Decisions under uncertainty of outcomes (see <ref type="bibr" target="#b5">Bach &amp; Dolan, 2012;</ref><ref type="bibr" target="#b31">LaValle, 2006)</ref> is widely studied in economics <ref type="bibr" target="#b2">(Allais, 1953;</ref><ref type="bibr" target="#b16">Güth et al., 1982;</ref><ref type="bibr" target="#b24">Kahneman et al., 1982)</ref> and cognitive neuroscience <ref type="bibr" target="#b5">(Bach &amp; Dolan, 2012;</ref><ref type="bibr" target="#b20">Hayden &amp; Niv, 2021;</ref><ref type="bibr" target="#b25">Klein-Flügge et al., 2022)</ref>. In doing so, the majority of studies models decision-making with binary choice paradigms, where choice options are presented simultaneously. By valuating the choice options, an agent (human or animal) makes decisions and the values given to the available options can then be modeled based on the observed behavior (see <ref type="bibr" target="#b20">Hayden &amp; Niv, 2021)</ref>. Foraging theory, however, models behavior from the perspective that choice options appear sequentially in the majority of cases. The crux with this kind of decisions is that future options depend on the outcome of the current choices. The decision-making agent must then decide to accept or reject the appearing option at a given instance in time (see <ref type="bibr" target="#b19">Hayden &amp; Moreno-Bote, 2018;</ref><ref type="bibr" target="#b49">Yoo et al., 2021)</ref>. Since most choices in nature follow this sequential framework, the underlying paradigm is arguably more ecologically valid than binary choice.</p><p>This reject-accept decision paradigm can be compared to the approach-avoidance conflict (AAC). The AAC represents a fundamental framework in biology and behavioral ecology to optimize choice and has been frequently studied in psychology to model anxiety in animal research (see <ref type="bibr" target="#b32">La-Vu et al., 2020)</ref>. There are a number of successful attempts to reverse-translate these paradigms to human experiments (see <ref type="bibr" target="#b4">Aupperle et al., 2015)</ref>. Notably, the threat-anticipatory freezing reaction and the related bradycardia are associated with information gathering and action preparation in animals and humans (see <ref type="bibr" target="#b36">Livermore et al., 2021)</ref>. Based on the ubiquity of the AAC in making survival relevant choices in nature, it is reasonable to assume that the underlying decision-making process is essential for the cognitive functioning underlying our behavior under naturalistic conditions. Therefore, this study aims to model human behavior under sequential AAC conditions. Since the underlying processes have been shaped and optimized over the course of evolution, this line of research fol-lows the call for more consideration of the evolutionary context of the emergence of behavior and the nervous system in systems neuroscience <ref type="bibr" target="#b39">(Mobbs et al., 2018)</ref>.</p><p>Sequential choice includes conflicts between interests (or trade-offs) at different time-points. This entails the problem of inconsistencies with changing preferences over time (see <ref type="bibr" target="#b18">Hammond, 1976;</ref><ref type="bibr" target="#b37">Lotito, 2022;</ref><ref type="bibr" target="#b38">McClennen, 1990;</ref><ref type="bibr" target="#b41">Strotz, 1956)</ref>. For example, one may decide to eat a snack out of an urge, but would decide against when considering the time pressure due to an appointment in the near future. In this case, the temporal dimension of a sequential decision problem creates a trade-off. This, in turn, brings up the question of how people choose their decision rules (called policies) in order to make trade-off choices. The selection of choice rules (also called policies) concerns the topic of meta-decision making (see <ref type="bibr" target="#b8">Boureau et al., 2015)</ref>. The temporal context of future trade-offs in sequential choice can either be considered or not, which shapes the current policy of an agent. If dynamic changes in trade-offs are considered over multiple time-points, the choice strategy would be more sophisticated and so more likely to be optimal with respect to the agent's outcome.</p><p>If an agent would only focus on her preference of the current state, the behavior would be more myopic/naive (see <ref type="bibr" target="#b37">Lotito, 2022)</ref>. To date, it is little understood how people approach trade-offs in sequential choice paradigms on a meta-decision making level (choosing between different policies).</p><p>Furthermore, the AAC is an exemplary trade-off with widespread appearance in nature. Therefore, it is reasonable to assume that the AAC has an impact on the policy selection. Due to this, we are interested in how people change between different policies under approach and avoidance conditions in a sequential task. To study this, we developed a probabilistic foraging game (played on the computer) in which people are prompted to approach or avoid certain dangers in the environment in order to optimize their behavioral outcomes. Our task design allows us to identify different policies that are either more sophisticated or more naive. Via conventional model comparisons, we can identify eventual changes in the range of choice sophistication based on our task's conditions (approach or avoid danger).</p><p>Our task consists of a single-agent game allowing a unique optimal solution due to the Markov property, which states that future options only depend on the current choice, not the past (see <ref type="bibr" target="#b42">Sutten &amp; Barto, 2018)</ref>. By formulating a sequential decision-making paradigm as a Markov Decision Process (MDP), the cumulative sum of the expected future reward can be computed via dynamic programming. By these means, we can test a choice model that accounts for all aspects of the entire temporal sequence of the decision process. <ref type="figure">Figure 1</ref> illustrates the MDP algorithm in detail.</p><p>A problem with MDP-based models is that these algorithms are generally domain specific, while humans often perform effectively in new tasks after just a few of observations <ref type="bibr" target="#b33">(Lee et al., 2014;</ref><ref type="bibr" target="#b47">Wu et al., 2018)</ref>. This is striking, given the limited resource of computational power rendered by the human brain <ref type="bibr" target="#b23">(Juechems et al., 2021)</ref>. This limited cognitive capacity brings up an additional aspect about optimization, namely resource or computational rationality: meta-decision making under the constraint of lowering energetic costs for computing a policy <ref type="bibr" target="#b14">(Gershman et al., 2015;</ref><ref type="bibr" target="#b35">Lieder &amp; Griffiths, 2020)</ref>. In order to achieve this, a decision tree can be truncated into binarized or graded rules of thumb. Policies of this kind are called heuristics: a reduction of a complex task concerning the evaluation of outcome probabilities by predicting values of simpler judgments (Tversky &amp; Kah-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Illustration of Markov Decision Process (MDP) computation with backwards induction (or dynamic programming). A) Transition graph</head><p>between energy states for two actions: w stands for action "waiting" and f stands for action "foraging". Since the goal of the task was to "stay alive", a reward of (-1) was given for transitioning to an energy state of 0 (in red), which is absorbing. Transitions to all other energy states were associated with a reward of 0. B) Illustration of backwards induction with corresponding transition matrix. Value updates take place from last to first time-points ("days") in order to compute the state-action values. Values are computed by the dot product of the transitions vecctor (probabilities to transit between energy states) and reward vector (containing the rewards of the states). <ref type="bibr">neman, 1974)</ref>. There is evidence suggesting heuristics-based (or evolutionary) algorithms offer a scalable alternative to MDP-based solutions with sufficient performance and lower computational costs <ref type="bibr" target="#b40">(Salimans et al., 2017)</ref>. An agent that employs this kind of strategy may under-perform in problems where the precise optimal solution is computable, but show advantages in complex realworld scenarios where the solution space becomes intractable. In other words, it may be beneficial to approximate a solution with a more suboptimal/myopic consideration than to overthink a problem when optimization is unfeasible.</p><p>In a sequential choice context, the optimal policy (computed via MDP) and heuristics incorporate different meta-decision making approaches. While MDP-based methods emphasize different decision variables in different states, heuristics rely on preferences regarding one aspect of a state (such as "is the weather good or bad"). The reduction to one situational aspect of a choice is more naive or myopic in the sense that it does not include any temporal variability of preferences (see <ref type="bibr" target="#b37">Lotito, 2022)</ref>. Our task is designed to compare choice models related to naive/heuristic or sophisticated/MDP-based meta-decision strategies.</p><p>In summary, we elaborated two main meta-decision strategies: An MDP-based and a heuristics-based approach. Both strategies have different advantages. While the former offers the maximal solution, the latter puts itself above with its scalability and data efficiency. Based on former work, we have reason to believe that humans take advantage of both choice strategies <ref type="bibr" target="#b27">(Korn &amp; Bach, 2015</ref><ref type="bibr" target="#b29">, 2019</ref>. In the present paper, we are interested in exploring the extent to which people rely more on the one or the other approach when faced with difficult environments. To do so, we adopt the virtual hunter-gatherer task from former work (see <ref type="bibr" target="#b28">Korn &amp; Bach, 2018</ref><ref type="bibr" target="#b29">, 2019</ref>, in which participants can influence their survival in a game-like set-up in the following way: Over the course of several time-points, participants have to decide whether they want to risk searching for food (also called foraging) or wait for better conditions. The expenses of energetic resources of the foraging option could be higher than for waiting, since foraging had a certain probability to fail. Additionally, participants could also encounter predators (virtual agents) when foraging, which would cause even Participants were in a forest with a good and a bad weather type (lower and higher chance of finding food). Foraging outcome is probabilistic with hunting success, hunting failure and predator encounter as possible outcomes. The number of food fields or predators in a weather type divided by the total number of fields results in the probability of gaining food and the risk of a predator encounter. The probability of success in a certain weather type results from the probability of food gain corrected for the joint risk of a predator encounter (p success = p gain × (1 -r predator)). The goal of the game was to "stay alive" (prevent discrete energy bar from depleting to zero) over 8 discrete time-points ("days") in order to get a monetary reward. Note: the days were not displayed, so participants had to track them by themselves. A total of 72 forests were played per participants. The probability of foraging gain and the risk of a predator encounter were selected carefully to create 2 experimental conditions (38 forests each): in one condition the probability of success suggests approaching gains (approach forests), while in the other condition the probability of success suggests avoiding predators (avoidance forests). Since higher probability to gain food coincides with higher risk of a predator encounter, the two policies were contradicting at all times. The figure depicts an example of an avoidance forest since the probability of success is higher in the weather type without predator.</p><p>higher energetic costs. To test whether choice strategies are influenced by principles of the AAC, we included different levels of competition threat by the predators. While in one experimental condition, participants should approach higher predator risks due to higher probabilities of gaining food, they should avoid higher predator risks in the other condition. Our task design allows scaling choice models with divergent signature behaviors differently and bit them against one another via quantitative model comparison. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the details of our experimental task with all relevant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data recording</head><p>We recorded a total of 29 subjects (14 male, 15 female; age = 23.93 ± 3.73) without any history of psychiatric conditions. The total number of possible trials per participant was 576 (72 forests times 8 days). The average probability of surviving a forest was 0.22 ± 2.25. Of all responses, 0.02 ± 2.69 were non-responses (due to exceeding the time limit), which were excluded from the data when conducting the model comparison. This left us with an average number of 361.45 ± 23.22 trials per participant (min = 292, max = 411), summing to a total of 10270 decisions. Participants had an mean probability to survive a forest until the end (after 8 days) of μ = 0.11 with a s = 2.7. There was no significant difference of the survival rates between the two experimental conditions (approach vs. avoidance forests; t = -0.89, p = 0.38).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model comparisons</head><p>Our task allowed a number of heuristic models derived from the task variables. Heuristic models concern decision variables of the task with binary or graded judgmental value operations.</p><p>Simple examples of these models are the weather type, the probability of food gain, or the risk of a predator attack. There are numerous ways of recombining decision variables to models of higher complexity. The expected value, for example, is the probability-weighted average of all possible values. Heuristic policies of this kind represent models of higher judgmental operation, since they integrate multiple variables to produce one value. The probability of success also represents such an intermediate model, since its values account for the join risk of a predator encounter (p success = p gain × (1 -r predator)). Since heuristic preferences can change over the course of time in a myopic approach, we further created a so-called multi-heuristic policy that accounts for significant preference changes related to energy states. Since the cost for choosing to wait is one energy point, participants have to forage in order to have any chance of survival in the game (see <ref type="table" target="#tab_0">Table 1</ref>, binary energy state BES). Similarly, if the energy points exceed the number of trials left in a forest, the game can be won by simply waiting the rest of the time (see <ref type="table" target="#tab_0">Table 1</ref>, wait when safe WWS). Our multiheuristic policy accounts for these preferences in the respective states, while otherwise choosing according to the probability of success. All behavioral models including example values for the forest depicted in <ref type="figure" target="#fig_0">Figure 2</ref> are elaborated in <ref type="table" target="#tab_0">Table 1</ref>. Weather type In each forest, the grids representing weather types can be categorized into "good" weather with higher probability of foraging success and "bad" weather with lower probability of success.</p><p>Categorical: 0 "bad" or 1 "good" 0 for example with threat, 1 for example without threat (since corrected success probability is higher)</p><p>Binary energy state (BES)</p><p>Participants must forage in energy state 1. Therefore, this variable is binarized as 1 if the energy state is 1, and 0 for all the other energy states.</p><p>Binary: 1 or 0 0 (since waiting does not lead to starvation)</p><p>Wait when safe (WWS)</p><p>If energy state is higher than days left in a forest, participants can simply wait in order to win Binary: 1 or 0 0 if only 3 days left, otherwise 1 the game. Therefore, this variable is 0 if energy state is higher than days left, and 1 for all other energy states. The number of days in a forest was not displayed and had to be tracked by participants themselves.</p><p>Win-stay-lose-shift (WSLS)</p><p>Possible strategy that uses the outcome of the former trial to decide in the current trial. This policy chooses foraging if the energy state increased and waiting if it decreased in the former trial.</p><p>Categorical: 1 "energy state increased" or 0 "energy state decreased"</p><p>Not available since it depends on previous trial in the context.</p><p>Bayesian model inference suggests that the multi-heuristic policy performs the best (PEP = 0.98) over the entire data set ( <ref type="figure">Figure 3A)</ref>. Our multi-heuristic model was composed of the three best performing heuristics (p success, binary energy state and wait when safe), which we validated via a decision tree. When comparing the multi-heuristic policy with the optimal policy values qualitatively via their model fits ( <ref type="figure">Figure 3B)</ref>, we see that the optimal policy values underfit when participants are in energy state 1 and foraging becomes a forced choice. This can be seen in <ref type="figure">Figure 3C</ref> and D showing the actual responses and the predicted responses of the optimal policy value model for BES and WWS situations. Due to this, we were motivated to generate a capped optimal policy value model, in which we "capped" states where the BES and WWS models eliminate uncertainty due to the nature of the task (e.g. if you wait in energy state 1, you have no chance of surviving the current forest). Refer to <ref type="table" target="#tab_0">Table 1</ref> (OP values + cap) for the specifics of our capping procedure. To prevent the multi-heuristic policy winning the model comparison for the trivial reason of capturing non-trade-off situations without ambiguity, we include this additional model in our further model comparisons. With respect to our entire data set, we can conclude that people generally seem to apply a more heuristics-based/myopic meta-decision strategy in our task.</p><p>To see if the meta-decision strategy changes in response to an approach-avoidance conflict, we conducted model comparisons for our experimental conditions (approach and avoidance forests)</p><p>separately. The results suggest that the capped optimal policy values significantly gains importance in the avoidance forests (see <ref type="figure" target="#fig_1">Figure 4A</ref> and B). The PEP results even show a slight preference of the capped optimal policy values (PEP = 0.47) compared to the multi-heuristic policy (PEP = 0.45) in avoidance forests. This means that there are inter-individual differences between participants, with some showing more sophisticated decision-making in this condition than others. In an additional step, we tested whether the beta coefficients (β 0 and β 1 ) from fitting our optimal policy values model via maximum likelihood estimation (see Methods section 4. 0.91, p = 0.36). This suggests that participants generally do behave more optimally in avoidance forests. <ref type="figure" target="#fig_1">Figure 4D</ref> shows the model fits of the two leading models for the entire data set.</p><p>Next, we tested whether the survival rate (No. of survived forests) in our task can be predicted by the slope (β 1 coefficient) of the OP values + cap model. This serves as confirmation that the stronger use of the optimal policy by participants indeed shows better choices reflected by the task's outcome (No. of survived forests). <ref type="figure" target="#fig_1">Figure 4C</ref> shows the results of this analysis. It can be seen that there is a positive correlation (r 2 = 0.67, t = 7.51, p = 0) between β 1 coefficients (slope of the logistic model) and the success rate in the task. Since the optimal policy fit seems predictive for participants' task performances, we can assume that the quality of choices generally improved in avoidance forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Validation of the multi-heuristic policy</head><p>As a proof of concept for the multi-heuristic policy, we ran a linear regression of the uncertainty of the model predictions for participants' log transformed response times (see Methods section 5.8 Response time analysis). If the respective model applies well for peoples' choices, re-  sponses should get slower when decisions are more uncertain/harder. We tested our multi-heuristic policy with a linear regression between uncertainty values and log-transformed response times and found a significant association (z = 26.86, p = 0). The results of this analysis are illustrated qualitatively in <ref type="figure" target="#fig_2">Figure 5A</ref> and B. Note that BES states (multi-heuristic policy bins = 1) generally show higher response times than WWS states (multi-heuristic policy bins = 1). This means that BES choices were generally harder to make for participants compared to WWS decisions.</p><p>To further validate our multi-heuristic policy, we extracted a decision tree based on features generated of our heuristic variables (see Methods section 5.9 Decision tree). Since, by default, a decision tree is generated from binary features, some decision variables had to be binarized via mean split in order to perform this analysis. A feature that underwent such a transformation would suggest foraging when the current environment's value would be higher than the mean, and waiting otherwise. The hereby engineered features were then used for our decision tree analysis, which calculates the relative importance of a feature to explain the data. In the process, features are reduced ("pruned") to a statistically meaningful amount to avoid overfitting of the generated decision tree model. The branches of the resulting tree hierarchically classify the importance of the features in the nodes (with the root node on top representing the most important feature). By these means, we gen- erated an interpretable proxy for the heuristic considerations underlying participants' choices when preferences change in different states. As depicted in <ref type="figure" target="#fig_2">Figure 5C</ref>, the branches of the resulting decision tree support our multi-heuristic policy with the three relevant features BES, WWS and p success remaining after pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>Overall, our findings suggest the multi-heuristic policy to be a good model in the present data set. This suggests that participants generally use a heuristics-based/myopic meta-decision strategy.</p><p>However, optimal behavior becomes more likely in avoidance forests. Therefore, predator avoidance in our task increases the chance for better choices. This interpretation is supported by the model comparisons of the two task conditions (approach and avoidance forests), as well as, the significant interaction in our logistic mixed effects model between the capped optimal policy values and our task conditions. Furthermore, we found a positive correlation between the slope of the optimal policy values (capped) and task performance, suggesting that the quality of choices increases with higher evidence for the optimal policy value model. We suspect that the salience of the avoidance policy prompts participants to integrate a transition model comparable to the optimal policy in their choices. Since this observation is mainly based on the PEP results, there must be inter-individual differences in the degree to which choices improve in avoidance forests.</p><p>It remains unclear why people tend to use simpler decision rules in approach forests. We hypothesize that this effect may be rooted in the internal trade-off between efficiency and accuracy of a choice model. The affordance competition hypothesis <ref type="bibr" target="#b10">(Cisek, 2007)</ref> states that action scripts are processed directly from sensory inputs and filtered by simultaneously perceived biases for policies.</p><p>Concerning this, heuristics could be understood as some sort of inductive biases (see <ref type="bibr" target="#b15">Goyal &amp; Bengio, 2022)</ref> to increase efficiency in processing decision relevant information. In this sense, heuristics serve to behave according to computational/resource rationality (see <ref type="bibr" target="#b14">Gershman et al., 2015;</ref><ref type="bibr"></ref> Lieder &amp; Griffiths, 2020) by reducing energetic expenses of policy the computation if the prospect will not significantly increase by higher efforts. In our task, since the energetic cost of maximizing a sequential choice problem is relatively high, it may be beneficial for organisms to minimize this effort by finding an easy and sufficient (heuristic) solution.</p><p>When an external stimulus triggers the relative importance of aversive stimuli, like in our task's avoidance forests, paying the energetic cost for computing a better policy becomes highly relevant in order to survive. <ref type="bibr" target="#b14">(Gershman et al., 2015;</ref><ref type="bibr" target="#b35">Lieder &amp; Griffiths, 2020)</ref> Regarding this, we suspect there to be a specialized neural system (like, e.g., the brain's salience network) for detecting salient risks in the environment that prompts up-scaling the complexity of a meta-decision strategy.</p><p>Neuroimaging experiments paired with computational modeling of behavior are needed to test this.</p><p>Furthermore, the inter-individual differences in the degree of sophistication evoked by our task's avoidance forests could be explained by a latent factor such as intelligence. It is a promising path for the continuation of this work to test for correlations between beta coefficients of the optimal policy values and measurements for cognitive capacities like, e.g., fluid intelligence scores.</p><p>Another aspect of our task concerns the differentiation between so-called model-based vs.</p><p>model-free systems for learning and decision-making (see <ref type="bibr" target="#b12">Drummond &amp; Niv, 2020)</ref>. In this regard, we suggest that the avoidance forests in our task cause an increased usage of model-based decisionmaking. For such deliberate/model-based behavior to take place, known aspects of the environment have to be exploited in order to generate a transition model of the environment (see <ref type="bibr" target="#b11">Collins &amp; Shenhav, 2022)</ref>. From our data it is unclear when the transition model is established, but people tend to include related considerations more in avoidance forests. We consider this as evidence for the specialization of human decision-making functions to the demands of sequential choice. Concerning this, it can be hypothesized that this specialization is responsible for seemingly paradoxical signature choices observed in behavioral economics, such as the Allais paradox (see <ref type="bibr" target="#b2">Allais, 1953)</ref> or loss aversion (see <ref type="bibr" target="#b45">(Tversky &amp; Kahneman, 1992)</ref>. Further research is required to bridge our behavioral observations with algorithmic and physiological implementations of decision-making to test this idea.</p><p>Based on the here presented results, we assume that heuristic approximation and the generation of meta-strategic protocols based on these heuristics play important roles in sequential choice.</p><p>Such meta-strategic protocols can be imagined as more or less complex decision trees, where the number of nodes (or the hierarchical structure) defines the sophistication of behavior. The complexity of a meta-decision strategy may rely on the external level of demand. While strategy efficiency is generally preferred to save energy, more complex and sophisticated choice strategies rise under increased environmental risks. Evidence suggests that the way we approach and resolve sequential decision-making conflicts may strongly relate to aspects of mental functioning and psychological health (see <ref type="bibr" target="#b7">Bavolar &amp; Bacikova-Sleskova, 2020)</ref>. Therefore, the investigation of policy selection, implementation and switching in ecologically valid, sequential set-ups with competing interests is of fundamental interest to advance the scientific understanding of mental health, as well as, improve psychopathology prevention and treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>We tested 29 healthy participants (14 male, 15 female; age = 23.93 ± 3.73) in the lab. Recruitment was done via fliers and online tenders. Before recording, participants were asked whether they had any history of psychiatric or neurological diagnoses, which would have represented an exclusion criterion. None of the invited subjects were excluded. Ethics approval was given by the local ethics committee of the medical faculty of the University of Heidelberg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The hunter-gatherer task with predators</head><p>The here presented task was based on the so-called "Hunter-Gatherer Game" from <ref type="bibr" target="#b28">Korn and Bach (2018)</ref>. In its original form, the task constituted different environments (called forests) with two weather types each. Participants had to "survive" (prevent discrete energy levels/life points from depleting to zero) over a number of time-points (called days) in a forest in order to gain a monetary incentive. Every trial (called day) in the game, one of the weather types was selected to represent the current weather. The weather conditions of the day entailed information about the probability to find food. Participants then had to decide whether they would like to hunt/search for food (also called "foraging") or wait for better conditions. The outcome of the foraging option was probabilistic. The probabilities (to successfully find food or not) were fully derivable from the visual representation of the weather types. A trade-off was included by surely losing a low amount of energy (one energy points) when waiting, but potentially losing a higher amount (two energy points) when foraging without success. If the foraging outcome was successful, participants could gain one or two energy points.</p><p>Similar to Korn and Bach 2019, the crucial additional feature in the current version of the experiment was that we included some predators/threats. The probability of encountering a predator was also random and could be derived from the visual representation of the weather types. If subjects foraged and encountered a predator, they would lose a significant amount of energy (three lifepoints) potentially being fatal in the game. This should mimic a situation where energy expenses are increased due to a fight or flight reaction of the subject.</p><p>In all forests, the weather type with a higher probability of finding food also had a higher risk of encountering a predator. This created an approach-avoidance conflict. In some forests, it was better to maximize the probability of success, while in other forests it was better to minimize the risk of a threat encounter. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the task's visual representation. In the following, mathematical details about the task design that are relevant for the behavioral modeling will be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mathematical details of the task</head><p>In a game-like environment, Participants had to go out and find food in order to survive in foraging environments called "forest". Each forest had two so-called "weather types". These were abstract representations of the probability of gaining food when foraging p, the probability of foraging failure q = 1-p and the risk of encountering a predator r. A weather type consisted of a grid with a number of fields (ranging from 2 to maximum fields). Each field had an equal probability to be the location after the hunt. If the field contained food, the hunt was successful. The ratio between the food containing fields and the total number of fields, thus, corresponded to p. Hereby, we set the minimum p to 0.2 to avoid trials with close to zero chance of finding food. The values for p were drawn from the closed interval [0.2, 0.67].</p><p>A predator had the same condition to end up on any of the grid fields as the participants.</p><p>Therefore, r represented the joint probability of landing on the same field, which was sampled from the interval [0, 0.67]. Similarly to p, the values for r were the result of the design choice of our virtual environment: the ration between the number of threats in a forest (0, 1 or 2) and the number of fields (min. 2, max. 10) determined the probability to encounter a predator in the current weather.</p><p>Since r ≠ 1, the maximum risk of threat encounter was 0.67 (3 fields with 2 predators). In order to balance p and r, we limited the probability to gain food to maximum 0.67. Due to the joint probability of finding food and encountering a predator, the true (or safe) probability of foraging success was p s = p*(1-r). Correspondingly, the true probability of unsuccessful foraging without encountering a predator was q s = (1-p)*(1-r) = q*(1-r).</p><p>In total, we had 72 forests. The forests were split into 4 experimental blocks with 18 forests each. Before the actual task, participants went through a training session with 10 forests. Participants played inside the same forest for 8 consecutive time-points t (called "days"). On each day, one of the weather types was randomly drawn (0.5 probability). If the final location after foraging contained food, the participant would yield an energy gain g (+1 or +2 energy points depicted as green dots). If a participant ended on a field without food, the foraging cost c f of -2 energy points was paid. Some weather types (not all) contained predators. If a predator was encountered, the predator cost c p of -3 energy points was deducted.</p><p>An essential aspect of the experiment was to test meta-decision strategies in two types of environments: approach and avoidance forests (experimental conditions with 36 forests each). In approach forests, the number of predators in each weather type would range from 0 to 2, with six forests having zero predators in one of the weather types. These forests were characterized with a higher probability of success p s in the weather containing a higher number of predators, for which participants should "approach" competitors to maximize success. In avoidance forests, one weather type contained 2, while the other weather type contained 0 predators. Participants should generally avoid foraging in the weather condition containing predators in order to maximize the probability of success p s in these forests.</p><p>To "stay alive" in the present forest, participants had to keep their discrete energy level from depleting to 0. An incentive of 0.50 € was provided, which participants could win if they survived until the end of a forest. If an energy state of 0 was reached, the current trial was stopped and the participant was prompted to the next forest prematurely without completing all possible days. There was no incentive for higher energy states, the only goal was to "stay alive" (prevent energy from depleting to 0). The initial energy level on the first day of a forest was randomly drawn between 4 and 5. The energy levels were capped at a maximum of 6 discrete points. This gives us a total of 7 possible energy levels (0 to 6). Since there were 2 weather types with equal probability of occurring, the total number of possible states inside a forest was 14 (7 energy states × 2 weather types). This finite set of states with their corresponding transition probabilities fulfills all conditions for a fully observable Markov Decision Process. This allows to numerically derive the state-action values by dynamic programming, where the highest action value among the available options represents the best choice. In other words, the optimal policy dictates when it is best to forage and when to wait. The values underlying this optimal policy can be used as a model to fit responses via a logistic link function that captures the uncertainty of the model in a given environment. This allows a more finegrained model fit regarding the noise term of the link function than with a hard optimal policy. For a more detailed description of this model, see section 5.4 Computation of the optimal policy values.</p><p>Our experimental paradigm is built around the idea of trade-off choices. If the transition probabilities in the weather types of a forest are too similar to each other, the trade-off disappears and the preference becomes indifferent/random. In order to avoid indifferent choices, we required that the (hard) optimal policy should suggest foraging more in one of the weather types. Therefore, we took the absolute difference of foraging preferences of the optimal policy d f over all states and timepoints in the two weather types and filtered out forests with d f &lt; 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computation of the optimal policy values</head><p>The experimental set-up allowed to derive the optimal policy values from a fully observable MDP. Since the transition probabilities between states were fully overt, the state-action values could be computed iteratively via dynamic programming (or backwards induction). <ref type="figure">Figure 1</ref> illustrates the requirements (state-action transitions and transition matrix) for computing of the optimal policy values with the MDP algorithm.</p><p>Backwards induction was performed by iteratively solving the following Bellman equation for all state-action pairs and time-points:</p><formula xml:id="formula_0">V π =E π [ G t ∨ s=s t ] =E π [ ∑ j=0 T γ j R t + j +1 ∨ s=s t ] ,<label>(1)</label></formula><p>where is the expected value of the total return G when starting from state s at time t and following policy Л. We only considered the discount factor ɣ = 1, since the game mechanics of our task did not necessitate nor motivate discounted rewards over time. The policy Л can, thus, be computed from the argmax of the average future rewards of the action set a as follows:</p><formula xml:id="formula_1">π (s )=argmax a ∑ s ' ,r</formula><p>p <ref type="bibr">(s ' , r ∨ s , a)</ref> [r+γV <ref type="bibr">(s ' )]</ref> (2) Accordingly, the action values for foraging V f and waiting V w can be transformed into a single foraging value f in the following manner:</p><formula xml:id="formula_2">f =V f − V w (3)</formula><p>If f is positive, the optimal policy suggests foraging. If f is negative, the optimal policy suggests waiting. If f is zero, the optimal policy is indifferent. Taking the optimal policy values (not the policy itself) as a model to fit to the data entails the advantage that the derivative of the logistic link function captures the uncertainty of the current environment, which would be disregarded by a hard optimal policy. The code for calculatin the optimal policy values was made publicly available via github (see https://github.com/SAEG64/fora02/tree/main).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Task presentation</head><p>At the beginning of a forest, participants were shown both weather types in the so-called forest phase (3.5s), followed by 8 consecutive decision and feedback phases (the so-called "days") until completion of a forest. The feedback was displayed for 2s. Before each decision phase, a random fixation time between [0.5, 3.8] appeared in the form of a blank screen. Participants had to give a response within 3s, otherwise the option 'waiting' was chosen by default. In case of no response, these decisions were excluded from the data when conducting model comparisons. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the details of the task presentation. Before doing the task, subjects received written instruction about the set-up. The full instructions sheet can be found in S1 Task instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Behavioral modeling</head><p>The main objective of this experiment was to test whether participants would implement a sophisticated/MDP-based or rather naive/heuristics-based meta-decision making approach in different experimental conditions. To test this, our experiment allowed model comparisons between a number of single or multi-heuristic models and the state-action value difference of the optimal policy (called optimal policy values). A detailed description of our models can be found in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our task design is composed of the following number of variables that can be directly derived from the task and represent simple heuristic policies: probability of gaining food, risk of threat encounter, weather type (good and bad weather type), the internal (binary) energy state and the wait when safe policy (if sufficient energy points are collected, participants can simply wait to win the game). We also included the win-stay-lose-shift heuristic, which binarizes the evaluation of the last event in the choice history. These models represent directly observable and univariate features of our task, which do not require any further feature engineering. In addition to this, we included what we call "intermediate" models, a set of policies that require some recombination of variables to represent slightly more complex decision rules. Models of this kind are the probability of success (denoted as conditional probability p (success | risk) ), the expected energy gain (E[X] = ∑x i p i ; x i : energy gain/ loss magnitude, p i : respective probability to gain or lose energy), as well as, the marginal value model. The success probability accounts for the joint probability of a predator encounter and, thus, requires some additional computation compared to the simpler probability to gain food. The expected gain scales the gains and losses with their respective probabilities and sums up the resulting values. Note that we only include the "naive" expected gain, which takes the raw probability of gaining food as a scale without correcting for the joint probability of encountering a predator. This is due to the model performing better than mathematically more correct versions of the expected gain, e.g., accounting for the joint probability of a predator encounter. The marginal value model is a binary variable that uses the trial by trial mean expected gain as a threshold for accepting a weather type as good enough to forage. This idea stems from the marginal value theorem (MVT),</p><p>where patch foraging behavior is explained with a threshold value computed from the mean expectancy of sequentially experienced environments (see <ref type="bibr">Gabay &amp; Apps, 2021;</ref><ref type="bibr" target="#b39">Mobbs et al., 2018)</ref>.</p><p>Ultimately, we created a multi-heuristic policy that considers the situational context of the validity of the three best performing heuristics, namely the probability of success, the binary energy state (BES) and the wait when safe (WWS) strategy. We created our multi-heuristic policy by modifying the model values for p s in states that obviate the task's trade-off in the following manner:</p><p> If energy is 1: p s ← 1 (participant has to forage; binary energy state BES)  If energy is higher than time-points left: p s ← 0 (wait-when-safe state WWS)</p><p>When fitting our models, we realized that the optimal policy value model suffers from a disadvantage since it contains a certain noise in BES and WWS situations not included in the multiheuristic policy. This arises from our MDP algorithm, where f is calculated relative to transition probabilities of the choice options in an environment. In this regard, optimal policy values are lower in if the transition probabilities are more similar between the weather types, regardless of a state obviating this ambiguity due to the BES or WWS policy dictating a uniform action preference across all forests. To cope with this, we "capped" optimal policy values in an additional model to the max/min of all calculated values for f equivalently to the value transformation of p s when creating the multi-heuristic policy. Simply put, we entered the maximum value of all calculated values for f (0.625) if the multi-heuristic policy had a value of 1 and the minimum value of all calculated values for f (-0.84) if the multi-heuristic policy had a value of 0. This capping ensured that the multiheuristic policy was not falsely favored over the optimal policy value model due to a better fit in states without behavioral noise.</p><p>The optimal policy values (capped and uncapped) and the multi-heuristic policy are based on a multivariate evaluation of the environment, for which they represent decision rules of the highest complexity in our task. It should be noted that we could have generated multivariate logistic regressions instead of recombining model values to account for several variables. However, due to the BIC penalizing model complexity, multivariate logistic regression models would have a disadvan-tage, whereas the optimal policy considers multiple aspects of the decision sequence in a single model value. In order to assure a fair model comparison between the optimal policy values with a multi-heuristic policy, we created a single model based on multivariate considerations and used only univariate logistic regression for our Bayesian model inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Model comparison</head><p>The total amount of possible data points was reduced due to forests being exited prematurely with energy state 0 ("game-over") and none response trials. For the remaining trials, logistic regressions were performed for all models using the following form:</p><formula xml:id="formula_3">P forage = 1 1+ exp (− DV ) ,<label>(4)</label></formula><p>where DV is the decision variable as defined by:</p><formula xml:id="formula_4">DV =β 0 + β 1 * policy<label>(5)</label></formula><p>We compared the models using the Bayesian information criterion (BIC), which is a metric for the negative log likelihood of a model penalized for model complexity. The BIC was obtained by fitting each model to participants responses separately via logistic regression. To determine which model explained participants' behavior the best, we computed the log-group Bayes factor (BF) assuming a fixed intercept and the protected exceedance probability (PEP) assuming random intercepts. The BF refers to the posterior odds for the null hypothesis (see <ref type="bibr">Kass &amp; Raftery, 1995)</ref>.</p><p>The PEP is a measurement for the likeliness of a given model to be more frequent in a population (see <ref type="bibr">Rigoux et al., 2014)</ref>. To assure the quality of our modeling, we performed parameter recoveries (see S2 <ref type="figure" target="#fig_0">Figure 2</ref>) and calculated the confusion matrix (see S2 <ref type="figure">Figure 3</ref>) for the most correlating models (see S2 <ref type="figure">Figure 1</ref>). The entire codebase for our modeling analyses was made publicly available (see https://github.com/SAEG64/fora02/tree/main).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Response time analysis</head><p>The sigmoid function generated from the logistic regression allows deriving a metric for a respective model's predictive choice uncertainty u Л . The assumption of a policy's choice uncertainty was, that higher uncertainty values would linearly predict longer response times if the underlying model would indeed be a valid description of the decision process. A model's choice uncertainty is mathematically defined by the derivative of the respective model fit as follows:</p><formula xml:id="formula_5">u π = 1 exp ( DV ) * (1+exp (− DV )) 2<label>(6)</label></formula><p>Therefore, we ran a linear regression of the response times with the winning model's uncertainty values. Since response times usually follow a logarithmic distribution due to time pressure, we log transformed the dependent variable. The code for running this analysis can be found online (see https://github.com/SAEG64/fora02/tree/main).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Decision tree</head><p>To validate our multi-heuristic policy, we extracted a decision tree containing the most important heuristic features. In simple terms, the decision tree calculates the proportion of data across subjects that is consistent with a respective feature. The root node represents the feature having the highest conformity with the data. The further down, the nodes represent features with lower data conformity. In order to assure interpretability, we computed a hard decision tree, for which we had to binarize model values (if they were graded or continuous) e.g. by defining mean cutoffs. If a model had to be binarized like, for example, the p success model, the respective binarized feature would suggest foraging if an environment contained a value above the mean, and waiting otherwise.</p><p>We made sure that the features we selected were not highly correlated with each other (refer to S2 <ref type="figure">Figure 1)</ref>. To preserve the trajectories of the choice sequences we used all 10482 state-action pairs including none response trials. For validation purposes, 20% of the pairs were reserved as a test set.</p><p>To train the decision tree, we used a maximum depth of 5 selected nodes based on Shannon's en -tropy as a measure for impurity. The mathematical formulation of our impurity measure was as follows:</p><formula xml:id="formula_6">H ( X )=− Σᵢ ₌ ₁ ₋ pᵢlog ₙ ₂ pᵢ (7)</formula><p>where p i is the probability of randomly selecting an example in class i.</p><p>Then, to ensure that the tree is interpretable and has no redundancy in features, we pruned the tree by defining the minimum weighted fraction of the total sum of weights of leaf nodes to be 5%</p><p>and set the limit for inducing a split to be based on the impurity decrease as 0.02 (refer to the hyperparameters "min_weight_fraction_leaf" and "min_impurity_decrease" of scikit-learn's Decision</p><p>Tree Classifier in Python). The codebase for calculating the deicision tree I publicly available on github (see https://github.com/faizankshaikh/ForaGym/blob/main/notebooks/DataAnalysis.ipynb).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Sequential foraging task with predators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Effects of experimental condition (approach vs. avoidance forests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Validation of multi-heuristic policy. Left panel shows Response time as a function of model uncertainty. A) Uncertainty is the derivative of the model fit (of the multi-heuristic policy). B) Linear association of uncertainty values with response time. C) Simplified representation of the multiheuristic policy computed via decision tree analysis. Out of all possible features entered, the three branches remained after pruning the decision tree. Note: the success probability was transformed to a binary variable in order to compute hard deci -sion tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Overview of predictive variables representing behavioral models</head><label>1</label><figDesc></figDesc><table><row><cell>Variable name</cell><cell>Explanation</cell><cell>Range of</cell><cell>Example value of</cell></row><row><cell></cell><cell></cell><cell>possible values</cell><cell>respective task</cell></row><row><cell></cell><cell></cell><cell>in the task</cell><cell>variables depicted in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Figure 2</cell></row><row><cell cols="2">Optimal policy values Difference between state-action</cell><cell>-0.84 -0.625</cell><cell>Cannot be depicted</cell></row><row><cell></cell><cell>values computed with dynamic</cell><cell></cell><cell>since it depends on</cell></row><row><cell></cell><cell>programming for all possible</cell><cell></cell><cell>the context (current</cell></row><row><cell></cell><cell>future states s.</cell><cell></cell><cell>state and time-point)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of all relevant</cell></row><row><cell></cell><cell></cell><cell></cell><cell>variables</cell></row><row><cell>Optimal policy values</cell><cell>Equivalently to the multi-</cell><cell>-0.84 -0.625</cell><cell>Not available since it</cell></row><row><cell>"capped" to max/min</cell><cell>heuristic policy, model values</cell><cell></cell><cell>depends on the</cell></row><row><cell>values in non-trade-off</cell><cell>were modified in states that</cell><cell></cell><cell>context (current state</cell></row><row><cell>states</cell><cell>obviate trade-offs (see multi-</cell><cell></cell><cell>and time-point) of</cell></row><row><cell>(OP values + cap)</cell><cell>heuristic policy below). Hard-</cell><cell></cell><cell>all relevant variables</cell></row><row><cell></cell><cell>coded max of 0.625 if binary</cell><cell></cell><cell></cell></row><row><cell></cell><cell>energy state suggests foraging</cell><cell></cell><cell></cell></row><row><cell></cell><cell>and min of -0.84 entered if wait</cell><cell></cell><cell></cell></row><row><cell></cell><cell>when safe can be applied. In all</cell><cell></cell><cell></cell></row><row><cell></cell><cell>other states, the model is</cell><cell></cell><cell></cell></row><row><cell></cell><cell>identical with the optimal policy</cell><cell></cell><cell></cell></row><row><cell></cell><cell>values.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>7</head><label></label><figDesc>Model comparison) changed depending on the experimental conditions. This could hint to switches of focus between environmental features that, in turn, improve participant performance. To test this, we ran a mixed-effects model to see whether the experimental conditions interact with the two best models (namely OP values + cap and multi-heuristic policy). The capped optimal policy values show a significant inter-Empirical responses in BES and WWS states. D) Optimal policy value model fitted to BES and WWS states. Note that there is an underfit of the optimal policy values for these states due to the interdependent of the MDP on the current environment.</figDesc><table><row><cell>Figure 3: Model comparisons for entire data set (no condition effect). A) Bayesian model inference with information criterion (BIC) for fixed and</cell></row><row><cell>protected exceedance probability (PEP) for random intercepts (accounting for subject variance). B) Model fits of the two best models plotted for the</cell></row><row><cell>respective model's binned values (left: optimal policy value, right: multi-heuristic policy). Bubble sizes are proportional to the sampling of actual re -</cell></row><row><cell>sponses at the respective data bin. C)</cell></row><row><cell>action with our experimental conditions (z = -2.52, p &lt; 0.05), but not the multi-heuristic policy (z =</cell></row><row><cell>10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Primer on Foraging and the Explore/Exploit Trade-Off for Psychiatry Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Addicott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Sweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Barack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<idno type="DOI">10.1038/npp.2017.108</idno>
		<ptr target="https://doi.org/10.1038/npp.2017.108" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychopharmacology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Le Comportement de l&apos;Homme Rationnel devant le Risque: Critique des Postulats et Axiomes de l&apos;Ecole Americaine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="546" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.2307/1907921</idno>
		<ptr target="https://doi.org/10.2307/1907921" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural substrates of approach-avoidance conflict decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Aupperle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Melrose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbm.22639</idno>
		<ptr target="https://doi.org/10.1002/hbm.22639" />
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowing how much you don&apos;t know: A neural organization of uncertainty estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="572" to="586" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/nrn3289</idno>
		<ptr target="https://doi.org/10.1038/nrn3289" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decision-making styles and mental health-A person-oriented approach through clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bavolar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacikova-Sleskova</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.2183</idno>
		<ptr target="https://doi.org/10.1002/bdm.2183" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="629" to="642" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deciding How To Decide: Self-Control and Meta-Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sokol-Hessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="700" to="710" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tics.2015.08.013</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2015.08.013" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cortical mechanisms of action selection: The affordance competition hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cisek</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2007.2054</idno>
		<ptr target="https://doi.org/10.1098/rstb.2007.2054" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="1585" to="1599" />
			<date type="published" when="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advances in modeling learning and decision-making in neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenhav</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41386-021-01126-y</idno>
		<ptr target="https://doi.org/10.1038/s41386-021-01126-y" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychopharmacology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="118" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-based decision making and model-free learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2020.06.051</idno>
		<idno>R860-R865</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2020.06.051" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hypothalamic-thalamostriatal circuit that controls approach-avoidance conflict in rats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fernandez-Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Kirouac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beierlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Do-Monte</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-021-22730-y</idno>
		<ptr target="https://doi.org/10.1038/s41467-021-22730-y" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aac6076</idno>
		<ptr target="https://doi.org/10.1126/science.aac6076" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6245</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive biases for deep learning of higher-level cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspa.2021.0068</idno>
		<ptr target="https://doi.org/10.1098/rspa.2021.0068" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<biblScope unit="page">20210068</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An experimental analysis of ultimatum bargaining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Güth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmittberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schwarze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Behavior &amp; Organization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="388" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/0167-2681</idno>
		<ptr target="https://doi.org/10.1016/0167-2681" />
		<imprint>
			<biblScope unit="page" from="90011" to="90018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Changing Tastes and Coherent Dynamic Choice. The Review of Economic Studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<idno type="DOI">10.2307/2296609</idno>
		<ptr target="https://doi.org/10.2307/2296609" />
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="159" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neuronal theory of sequential economic choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno-Bote</surname></persName>
		</author>
		<idno type="DOI">10.1177/2398212818766675</idno>
		<ptr target="https://doi.org/10.1177/2398212818766675" />
	</analytic>
	<monogr>
		<title level="j">Brain and Neuroscience Advances</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="201" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/bne0000448</idno>
		<ptr target="https://doi.org/10.1037/bne0000448" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Paraventricular Thalamus as a Critical Node of Motivated Behavior via the Hypothalamic-Thalamic-Striatal Circuit. Frontiers in Integrative Neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Flagel</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnint.2021.706713</idno>
		<ptr target="https://doi.org/10.3389/fnint.2021.706713" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal utility and probability functions for agents with finite computational precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Juechems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2002232118</idno>
		<ptr target="https://doi.org/10.1073/pnas.2002232118" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511809477</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511809477" />
		<title level="m">Judgment under Uncertainty: Heuristics and Biases</title>
		<editor>A.</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Medial and orbital frontal cortex in decision-making and flexible behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Klein-Flügge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bongioanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F S</forename><surname>Rushworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2743" to="2770" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.neuron.2022.05.022</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2022.05.022" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maintaining Homeostasis by Decision-Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004301</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1004301" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Heuristic and optimal policy computations in the human brain during sequential decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-017-02750-3</idno>
		<ptr target="https://doi.org/10.1038/s41467-017-02750-3" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimizing threat via heuristic and optimal policies recruits hippocampus and medial prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="733" to="745" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/s41562-019-0603-9</idno>
		<ptr target="https://doi.org/10.1038/s41562-019-0603-9" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Planning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lavalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">To Approach or Avoid: An Introductory Overview of the Study of Anxiety Using Rodent Assays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>La-Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Schuette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adhikari</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbeh.2020.00145</idno>
		<ptr target="https://doi.org/10.3389/fnbeh.2020.00145" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural computations underlying arbitration between model-based and model-free learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimojo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="687" to="699" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.neuron.2013.11.028</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2013.11.028" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X1900061X</idno>
		<ptr target="https://doi.org/10.1017/S0140525X1900061X" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Approach-Avoidance Decisions Under Threat: The Role of Autonomic Psychophysiological States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J A</forename><surname>Livermore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Klaassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hulsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Meijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klumpers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>De Voogd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roelofs</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2021.621517</idno>
		<ptr target="https://doi.org/10.3389/fnins.2021.621517" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic Inconsistency in Choice and Different Models of Dynamic Choice -A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lotito</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4314092</idno>
		<ptr target="https://doi.org/10.2139/ssrn.4314092" />
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rationality and Dynamic Choice: Foundational Explorations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Mcclennen</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511983979</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511983979" />
		<imprint>
			<date type="published" when="1990" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Foraging for foundations in decision neuroscience: Insights from ethology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Blumstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41583-018-0010-7</idno>
		<ptr target="https://doi.org/10.1038/s41583-018-0010-7" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1703.03864</idno>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1703.03864" />
		<title level="m">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Myopia and Inconsistency in Dynamic Utility Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Strotz</surname></persName>
		</author>
		<idno type="DOI">10.2307/2295722</idno>
		<ptr target="https://doi.org/10.2307/2295722" />
	</analytic>
	<monogr>
		<title level="j">The Review of Economic Studies</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="180" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<ptr target="https://mitpress.mit.edu/9780262039246/reinforcement-learning/" />
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploration in neo-Hebbian reinforcement learning: Computational approaches to the exploration-exploitation balance with bio-inspired neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Triche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2022.03.021</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2022.03.021" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="16" to="33" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Judgment under Uncertainty: Heuristics and Biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Advances in prospect theory: Cumulative representation of uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="297" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Balancing exploration and exploitation with information and randomization. Current Opinion in Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Ebitz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2020.10.001</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2020.10.001" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalization guides human exploration in vast decision spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="915" to="924" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/s41562-018-0467-4</idno>
		<ptr target="https://doi.org/10.1038/s41562-018-0467-4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Continuous decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2019.0664</idno>
		<ptr target="https://doi.org/10.1098/rstb.2019.0664" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<date type="published" when="1819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
