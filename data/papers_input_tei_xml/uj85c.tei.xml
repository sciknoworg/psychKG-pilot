<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep exploration as a unifying account of explore-exploit behavior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cognitive Science Program</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashem</forename><surname>Sadeghiyeh</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychological Science</orgName>
								<orgName type="institution">Missouri University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Princeton Neuroscience Institute</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep exploration as a unifying account of explore-exploit behavior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Many decisions involve a choice between exploring unknown opportunities and exploiting well-known options. Work across a variety of domains, from animal foraging to human decision making, has suggested that animals solve such &quot;explore-exploit dilemmas&quot; with a mixture of two strategies: one driven by information seeking (directed exploration) and the other by behavioral variability (random exploration). Here we propose a unifying account in which these two strategies emerge from a kind of stochastic planning, known in the machine learning literature as Deep Exploration. In this model, the explore-exploit decision is made by stochastic simulation of plausible futures that are deep, in that they extend far into the future, and narrow, in that the number of possible futures they consider is small. By applying Deep Exploration to a simple explore-exploit task we show theoretically how directed and random exploration can emerge in these settings. Moreover, we show that Deep Exploration implies a tradeoff between directed and random exploration that is mediated by the number of simulations, or samples-with more samples leading to increased directed exploration and decreased random exploration at the expense of greater time taken to respond. By measuring human behavior on the same simple task, we show that this reaction-time-mediated tradeoff exists in human behavior both between and within participants. We therefore suggest that Deep Exploration is a unifying account of explore-exploit behavior in humans. When dining at a favorite restaurant, do you exploit the pizza you known and love, or try the ravioli just added to the menu? Exploiting the pizza guarantees a good meal, but is uninformative; you already knew the pizza was good. Alternatively, exploring the ravioli provides information about how tasty the ravioli is at this restaurant, but there&apos;s no guarantee you will enjoy it. Beyond the epicurean, such explore-exploit dilemmas occur at all levels of decision making, from picking a TV show to watch to a finding a person to marry, and there are real advantages to solving it wellexploit too much and you&apos;ll eat pizza for the rest of your life; explore too much and you will never</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>discover anything new that is good. Unfortunately, however, solving the explore-exploit dilemma optimally is intractable in all but the simplest settings <ref type="bibr" target="#b11">(Bellman, 1956;</ref><ref type="bibr" target="#b3">Gittins, 1979)</ref>, and so the question arises as to how agents balance exploration and exploitation in practice.</p><p>Work in psychology, ecology and neuroscience suggests that humans and non-human animals use two strategies for solving the explore-exploit dilemma <ref type="bibr" target="#b8">(Krebs et al., 1978;</ref><ref type="bibr">Meyer and Shi, 1995;</ref><ref type="bibr" target="#b0">Banks et al., 1997;</ref><ref type="bibr">Ölveczky et al., 2005;</ref><ref type="bibr">Frank et al., 2009;</ref><ref type="bibr">Steyvers et al., 2009;</ref><ref type="bibr" target="#b10">Lee et al., 2011;</ref><ref type="bibr">Payzan-LeNestour and Bossaerts, 2012;</ref><ref type="bibr" target="#b19">Wilson et al., 2014;</ref><ref type="bibr">Ebitz et al., 2018;</ref><ref type="bibr">Costa et al., 2019;</ref><ref type="bibr">Schulz and Gershman, 2019)</ref>. The first strategy is 'directed exploration.' In this strategy, choices are biased towards more informative options by an 'information bonus' that increases the relative value of unknown options. In the restaurant example, the known pizza receives no information bonus, because eating it tells us nothing new; while the unknown ravioli receives a large information bonus, because choosing it provides information about how good the ravioli is in this setting. All else being equal, the information bonus tips the balance in favor of exploring the unknown. While in some circumstances it is possible to compute the optimal value for the information bonus, this optimal computation comes at considerable computational cost and, in some cases it is not computable at all <ref type="bibr" target="#b11">(Bellman, 1956;</ref><ref type="bibr" target="#b4">Gittins and Jones, 1979)</ref>.</p><p>The second strategy is 'random exploration.' In this strategy, exploratory choices are driven by noise in the decision process that makes the highest value (i.e. exploit) option less likely to be chosen. In the restaurant example, one implementation of random exploration would be to add random decision noise to the values of the pizza and ravioli. In this way, even if the pizza has a higher value, the random noise will sometimes lead to exploration simply by chance. While this is unlikely to generate the optimal strategy for exploration (if and when that can be determined), it is less computationally costly and, in some circumstances, may be the only practical way to favor exploration <ref type="bibr">(Watkins, 1989;</ref><ref type="bibr">Sutton and Barto, 2018)</ref>.</p><p>Considered as above, the mechanisms of directed and random exploration seem distinct. However, recent work in machine learning has shown how directed and random exploration can arise naturally from a decision process based on stochastic simulation known as 'Deep Exploration' <ref type="bibr">(Osband et al., 2016)</ref>, sometimes also referred to as Posterior Sampling for Reinforcement <ref type="bibr">Learning (Strens, 2000)</ref>. In these algorithms, the explore-exploit choice is made by simulating a small number of (as low as 1) random, but plausible, future experiences in order to approximate the expected value of taking different actions. Once these simulated values are computed, the decision is made by picking the action with the highest simulated value.</p><p>Random exploration arises naturally from Deep Exploration because the simulations are stochastic. More subtly, and as outlined in the Supplementary Material, directed exploration arises from this model in the details of how the simulated choices play out. In general, the more simulations that are carried out, the better the estimate of the value of exploration and the closer the decision process approximates optimal directed exploration. Thus, Deep Exploration predicts a tradeoff between directed and random exploration that is mediated by the number of simulations used to compute values, with more simulations corresponding more directed and less random exploration, but at the expense of more computational effort and time taken to make the decision. In all cases a trial begins with the presentation of the exploit and explore options. The exploit option is indicated by the amount available for choosing it (in this case 64 points), and the explore option by two question marks, '??'. The location (left or right) of the two options is assigned randomly on each trial. Based on this information participants make a choice that takes a reaction time, RT. (A) If the participant chooses to exploit, the exploit value glows white for delay, d, indicating the choice and outcome, after which the boxes are marked with an 'XX' to indicate they are closed and cannot be chosen for the duration of the inter-trial interval (ITI). The next trial begins when the next pair of options is revealed. If the game does not end, then the exploit value stays the same. (B, C) If the participant chooses to explore, the value of the explore option is revealed, and then one of two things can happen on the next trial, depending on whether the outcome is worse (B) or better (C) than the exploit value. If the outcome is worse (B) then the exploit value remains the same on the next trial; if the outcome is better (C) then the exploit value for the next trial takes on the value of the explore option from the current trial. In this way, the value of the exploit option reflects the best outcome seen so far, and the choice is always between exploiting this best outcome and exploring a new one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Infinite Bandits Task</head><p>In this work, we tested the predictions of Deep Exploration in a simple explore-exploit task that we call the 'Infinite Bandits Task' <ref type="figure" target="#fig_0">(Figure 1)</ref>. In this task, participants play a series of games in which they make repeated choices between two options -a fully known exploit option with a value between 1 and 100 points, corresponding to the best outcome they have seen so far; and an unknown explore option, corresponding to a random draw from a uniform distribution between 1 and 100. Each game lasts for a random number of trials such that there is a fixed probability -the hazard rate, h = 0.1 -that the game will end after every trial.</p><p>At the start of each game, the exploit value is set randomly between 1 and 100. After that, the game plays out in the following way: Choosing the exploit option (like eating a known pizza) yields the known outcome that was cued at the start of the trial. Choosing the explore option (eating unknown ravioli) yields a random outcome, drawn uniformly from 1 to 100. If the explore outcome is greater than the current exploit value (if the ravioli is better than the pizza), then this higher value becomes the exploit option on the next trial (ravioli is the new favorite). If the explore outcome was less than the current exploit value (the ravioli is worse than the pizza), then this value is discarded and the exploit value stays the same for the next trial (pizza remains the favorite). In this way, the exploit option reflects the best outcome seen so far in this game, and the choice is always between exploiting this best outcome and exploring something new. Furthermore, because there is always a new explore option on every trial (a new item is added to the menu), our task is equivalent to choosing between infinitely many options with deterministic payouts, hence the name: the Infinite Bandits Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Exploration predicts a tradeoff between directed and random exploration in the Infinite Bandits Task</head><p>The simplicity of the Infinite Bandits Task makes it amenable to formal analysis, allowing us to derive expressions for the choice probabilities of Deep Exploration. In particular, we assume that Deep Exploration proceeds by simulating the values of exploiting and exploring out to some time horizon, T , (which is a free parameter of the model, but in principle should be 1/h on average) when this particular game is simulated to its end. If the payoff of the known exploit option is c, then the value of choosing the exploit option (assuming we continue to pick it for all trials up to the horizon) is</p><formula xml:id="formula_0">Q exploit = T c<label>(1)</label></formula><p>To simulate the value of the explore option, the algorithm first samples a plausible value of the unknown explore option,ũ i , based on its knowledge of the underlying generative process -in this case a draw from the uniform distribution between 1 and 100. For each sample, i, the agent then computes the expected value of exploring,Q i explore , as though the simulated valueũ i were the actual payoff from the unknown option. Ifũ i &gt; c the algorithm continues to pick that option for the remainder of the simulated game, otherwise the algorithm switches to the exploit option. Thus, Q i explore can be written asQ</p><formula xml:id="formula_1">i explore = Tũ i ifũ i &gt; c u i + (T − 1)c ifũ i ≤ c<label>(2)</label></formula><p>This mental simulation process is repeated n times, with independent samples forũ i each time giving independent samples ofQ i explore . These samples are then combined to compute the simulated expected value of exploring, Q explore , as</p><formula xml:id="formula_2">Q explore = 1 n n i=1Q i explore<label>(3)</label></formula><p>The explore-exploit decision is then made by comparing Q explore to the value of exploiting Q exploit , with the algorithm exploring when Q explore &gt; Q exploit and exploiting otherwise. When only one sample is used to compute Q explore (n = 1), Deep Exploration leads to probability matching behavior. This is easily seen from Equations 1 and 2 where Q explore will be less than Q exploit when the sampled explore valueũ i is less than c. Thus, the model will exploit with probability p(ũ i &lt; c), which, ifũ i comes from a uniform distribution, is given by</p><formula xml:id="formula_3">p exploit = p(ũ i &lt; c) = c 100 (4)</formula><p>This probability matching behavior (blue line in <ref type="figure">Figure 2</ref>) is a purely random strategy with no directed component. The indifference point of the choice curve is equal to 50.5, the mean of the uniform distribution over outcomes from the explore option, indicating that there there is no information bonus in favor of the explore option. In addition, the n = 1 strategy is independent of horizon, T . Such a probability matching strategy for exploration is known in the machine learning literature as Thompson sampling <ref type="bibr" target="#b17">(Thompson, 1933)</ref>. In psychology, probability matching has been observed in many tasks and across species <ref type="bibr">(Vulkan, 2000)</ref>, and Thompson sampling models can account for some properties of human random exploration <ref type="bibr" target="#b16">(Speekenbrink and Konstantinidis, 2015;</ref>. When infinitely many samples are used (n = ∞), Deep Exploration converges to the optimal strategy in the Infinite Bandits Task (see <ref type="figure">Supplementary Material)</ref>. This is the yellow line in <ref type="figure">Figure  2</ref>. In this case</p><formula xml:id="formula_4">p exploit = 1 if c &gt; θ * 0 if c ≤ θ *<label>(5)</label></formula><p>where θ * corresponds to the threshold between exploration and exploitation. This strategy is a purely directed strategy with an indifference point of θ * = 75.97 points (i.e. much larger than the expected value from a single play of the explore option, which is 50.5), but no random exploration because the behavior is deterministic. Moreover, this strategy is highly dependent on the simulation horizon, T , with the threshold between explore and exploit behavior changing as a function of T as</p><formula xml:id="formula_5">θ * = 100 × √ T 1 + √ T<label>(6)</label></formula><p>More generally (for 0 &lt; n &lt; ∞) it is possible to compute an approximate analytic, albeit somewhat opaque, expression for the probability of exploiting as a function of n and T .</p><formula xml:id="formula_6">p exploit (n, T ) ≈ n n + =0 n n + m − c m n + c m n−n + 1 n + ! c(n−n + ) 2T (m−c) k=0 (−1) k n + k c(n − n + ) 2T (m − c) − k n +<label>(7)</label></formula><p>As shown in <ref type="figure">Figure 2A</ref>, as the number of samples n increases, behavior becomes more deterministic (less random exploration) and more biased towards exploring (more directed exploration). Thus, assuming that sampling is costly in some form (in computational effort and/or in time), then there is a tradeoff between directed and random exploration that is mediated by the number of samples. In addition, it should be noted that, although more samples leads to better overall performance ( <ref type="figure">Figure 2B</ref>), in this particular case the overall improvement in performance is small, approximately 5% between 1 and infinitely many samples. Thus, given that there is a cost to sampling, the agent should favor a small number of samples. More generally, to the extent that the cost of sampling can be quantitatively related to the gains afforded by more sampling, then the formalism can be used to determine an optimal tradeoff between random (minimal sampling) and directed (maximal sampling) exploration.</p><p>To illuminate the tradeoff between directed and random exploration more explicitly, we quantified the Deep Exploration choice curves in terms of two parameters: the indifference point, θ, at which the model is equally likely to explore or exploit, and the decision noise, σ, which corresponds to the inverse of the slope of the choice curve at the indifference point ( <ref type="figure">Figure 2C</ref>). More directed exploration corresponds to a higher θ, while more random exploration corresponds to a higher σ ( <ref type="figure">Figure 2D</ref>). : Behavior of the algorithm as a function of the number of samples, n. (A) Solid lines correspond to the analytic approximation, while the dots correspond to the average behavior of 10, 000 simulations. As n increases, the model shows more directed exploration (choice curves shift to the right), and less random exploration (choice curves get steeper). As n → ∞, the algorithm approaches the behavior of the optimal model. (B) Performance of the model as a function of the number of samples. Increasing the number of samples increases the average number of points earned in the task, but even with one sample, the performance of Deep Exploration is within 6% of optimal. (C) Illustration of indifference point, θ, which captures directed exploration, and slope σ, which captures random exploration. (D) Theoretical tradeoff between directed and random exploration for a given value of hazard rate h = 0.1. Note colored stars correspond to different number of samples as given by legend in panel A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Between-subject tradeoff between directed and random exploration</head><p>To test whether the theoretically predicted tradeoff between directed and random exploration is observed in human behavior, 174 participants (ages 18-25, mean 19.1; 80 identified as male, 93 identified as female, 1 identified as transgender) performed the Infinite Bandits Task in the laboratory. In the simplest analysis, we quantified each person's indifference point, θ, and noise, σ, using a logistic regression model (see Methods). As shown in <ref type="figure" target="#fig_2">Figure 3A</ref>, we found a strong negative correlation between θ and σ (Pearson's r(172) = −0.48, p &lt; 0.001), consistent with a tradeoff between directed and random exploration across the population. Crucially, the negative correlation between θ and σ is robust to a wide variety of modeling assumptions and is not induced as an artifact of the fitting procedure (see <ref type="figure" target="#fig_10">Supplementary Figures S5,S6</ref>, and S7). This suggests that the tradeoff between directed and random exploration is a real feature of the data.</p><p>To further investigate the tradeoff between directed and random exploration, we fit the Deep Exploration model (Equation 7) to behavior. As shown in <ref type="figure" target="#fig_2">Figure 3A</ref> (insets on the right), the model provides an excellent fit to the behavior of individual participants (see Supplementary <ref type="figure" target="#fig_0">Figure S10</ref> for additional fits), and allows us to estimate the average number of simulations each participant used to make their decision ( <ref type="figure" target="#fig_2">Figure 3B</ref>). In particular, we find that most participants used only a small number of simulations (81% between 1 and 5).</p><p>Remarkably, the fit number of simulations correlates with overall reaction time across participants (r(172) = 0.23, p = 0.008, <ref type="figure" target="#fig_2">Figure 3C</ref>), such that individuals who use more samples are slower overall. Furthermore, by estimating the slope of the line in <ref type="figure" target="#fig_2">Figure 3C</ref>, we can compute a crude estimate of the rate at which samples are produced, giving us an estimate of 61ms per sample, a number which is broadly consistent with previous work on the speed of working memory search (38ms per symbol (Sternberg, 1966)), preplay in hippocampus <ref type="bibr">(Redish, 2016;</ref><ref type="bibr" target="#b7">Kay et al., 2019)</ref>, and more generally has been proposed as a fundamental timescale for cognitive processing Newell (1994).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Within-subject tradeoff between directed and random exploration</head><p>Explaining the tradeoff between directed and random exploration in terms sampling directly implies that there should be a speed-accuracy tradeoff in explore-exploit decisions: taking fewer samples is faster (as shown in <ref type="figure" target="#fig_2">Figure 3D</ref> above), but provides a less accurate estimate of the optimal information bonus (i.e., favors random exploration), whereas taking more samples takes more time but improves estimates of the optimal information bonus (i.e., directed exploration) and thereby performance. This suggests that if participants are given more time to respond, they should be more likely to use greater numbers of samples and thereby exhibit greater evidence of directed exploration.</p><p>To test this prediction we ran a version of the Infinite Bandits Task in which we manipulated the time between trials (inter-trial interval, ITI) from fast (0.5 seconds) to slow (3 seconds) in two blocks of 500 trials each (order counterbalanced across participants). Such block-wise ITI manipulations, while changing nothing about the reward structure of the task, are known to cause changes in speed-accuracy tradeoffs, such that people tend respond more slowly when given more time to respond in a task with longer ITIs <ref type="bibr">(Bogacz et al., 2006)</ref>. Consistent with previous work, we number of samples t(75) = 3.41, p = 1.06e-03</p><p>A B C D <ref type="figure">Figure 4</ref>: Within-subject tradeoff between directed and random exploration. Slowing the inter-trial interval increases directed exploration (A; significantly more points above the dashed identity line designating no change) and decreases random exploration (B; significantly more points below the line). In addition, the change in directed exploration is negatively correlated with the change in random exploration (C) as though the two were trading off. (D) Fitting the Deep Exploration model to this behavior shows that the change in directed and random is explained by an increase in the number of samples used in the slow condition (more points above the identity line).</p><p>found that prolonging the ITI lead to an increase in reaction time in 74% of participants (average change = 114ms, t(75) = 5.08, p &lt; 0.001). Consistent with the predictions of Deep Exploration, this increase in reaction time was associated with an increase in directed exploration (t(75) = 2.55, p = 0.013) and a decrease in random exploration (t(75) = −3.01, p = 0.0036) in the slow condition. Moreover, the change in directed exploration was negatively correlated with the change in random exploration (r(74) = −0.55, p &lt; 0.001), consistent with the idea that the two types of exploration trade off against one another. . Finally, by fitting the Deep Exploration model to behavior, we investigated whether people actually used more samples in the slow ITI condition. As shown in <ref type="figure">Figure 4D</ref>, this is indeed what we find with the average number of samples increasing by 1.4 (t-test on log n, t(75) = 3.41, p = 0.0011). This suggests that the tradeoff between the two types of exploration can be explained in terms of a single underlying variable: number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this article we have proposed a unifying account of two different types of exploration -directed and random -that have previously been described for explore-exploit decision making behavior. In our Deep Exploration account, we propose that explore-exploit decisions are made by simulating a small number of plausible futures and choosing the action that maximizes their simulated future value. We showed that directed and random exploration emerge naturally from this account and, further, that Deep Exploration implies a tradeoff between the two types of exploration. Using a simple behavioral task, we found evidence for this tradeoff both between and within participants, such that individuals who take more time to make their decisions exhibit more directed and less random exploration. Furthermore, greater time taken is associated with a greater number of samples taken, as estimated by a fit of the Deep Exploration model, with a time cost of approximately 61ms per sample.</p><p>Beyond the Infinite Bandits Task, we show in the Supplementary Material that Deep Exploration can account for a range of previously reported behavioral findings in the explore-exploit literature; that directed and random exploration both: increase with horizon <ref type="bibr" target="#b19">(Wilson et al., 2014)</ref>; depend on uncertainty <ref type="bibr" target="#b16">(Speekenbrink and Konstantinidis, 2015;</ref>; and are reduced when future information is not contingent on participants' present choice <ref type="bibr" target="#b13">(Findling et al., 2019)</ref>. Moreover, because of the simplicity of the algorithm -all that is needed to implement Deep Exploration is the ability to simulate a small number of plausible futures -Deep Exploration is computationally tractable even in complex cases such as those found in the real world. In this regard, Deep Exploration stands in contrast to optimal (and even many heuristic) exploration policies that quickly become intractable for problems of even modest complexity <ref type="bibr" target="#b11">Bellman (1956)</ref>. This makes it appealing not only as an account of how people manage the explore-exploit tradeoff, but also as an efficient algorithm in artificial systems.</p><p>More generally, our model accords with work showing that mental simulation (including sampling) may be involved in a variety of judgments and decisions <ref type="bibr">(Stewart et al., 2006;</ref><ref type="bibr">Vul et al., 2014)</ref>. Perhaps most prominent in this literature is the work on intuitive physics, where judgments about the stability of a tower of blocks, or the path of a ball on a pool table, are well described by mental simulation models <ref type="bibr" target="#b9">(Kubricht et al., 2017)</ref>. Moreover, as we found above, human behavior in these tasks is best described by models using only a small number of simulations (between 1 and 10, <ref type="bibr" target="#b1">(Battaglia et al., 2013;</ref><ref type="bibr" target="#b5">Hamrick et al., 2016)</ref>), with the exact number depending in part on the individual but also on the demands of the task <ref type="bibr" target="#b6">(Hamrick et al., 2015)</ref>. Thus mental simulation may be a general strategy for judgment and decision making, as well as an important factor underlying individual differences in these behaviors. <ref type="bibr" target="#b11">Bellman, R. (1956)</ref>. A problem in the sequential design of experiments. Sankhyā: The Indian Journal of <ref type="bibr">Statistics (1933</ref><ref type="bibr">Statistics ( -1960</ref>, 16(3/4):221-229.</p><p>Bogacz, R., Brown, E., Moehlis, J., <ref type="bibr">Holmes, P., and Cohen, J. D. (2006)</ref>. The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks. Psychological review, 113 <ref type="formula">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Participants</head><p>All participants were recruited from the University of Arizona Psychology subject pool (217 for Experiment 1 and 85 for Experiment 2) and the Princeton student population (27 participants for Experiment 2). Participants were unpaid but received course credit for taking part in the experiment. All participants gave informed consent and the study was approved by the Institutional Review Boards at the University of Arizona and Princeton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Baseline Infinite Bandits Task</head><p>In Experiment 1, we aimed to measure the tradeoff between directed and random exploration across the population. To this end 217 participants (100 male, 116 female, 1 transgender, ages 18-25, mean 19.1) performed the Infinite Bandits Task. In this version of the task, participants who completed the task performed 500 trials with a fixed hazard rate of h = 0.1. Participants who completed fewer than 400 trials were excluded from further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Timing manipulation</head><p>In Experiment 2, we aimed to manipulate the directed-random tradeoff by changing the timing in the task to favor fast or slow responding. The motivation for this manipulation was that slower responding should lead to people using more mental simulations to make their decision. This, in turn, should increase directed exploration and decrease random exploration. To bias faster or slower responding we changed the inter-trial interval (ITI) in the experiment from 0.5 to 3 seconds while keeping the hazard rate fixed. Such changes in ITI have been previously shown to induce faster and slower responding without explicitly instructing participants to speed up or slow down <ref type="bibr">(Bogacz et al., 2006)</ref>. 112 participants (35 male, 74 female, 3 declined to answer, ages 18-25, mean age = 19.1) played two blocks of the Infinite Bandits Task. Across blocks we varied the inter-trial interval (ITI) from 0.5 to 3 seconds. The hazard rate was kept constant across the two blocks (at h = 0.1) and the order of the ITI conditions was counterbalanced across participants. Participants who completed the task (see below) were presented with either 500 or 600 trials in each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic performance and exclusion criteria</head><p>Participants were excluded from further analysis if they did not complete the required number of trials or if they did not perform the task better than chance. For trials, in Experiment 1, participants were excluded if they performed fewer than 400 trials. In Experiment 2, participants were excluded if they performed fewer than 400 trials in either of the two blocks. For performance, in both experiments we excluded participants on the basis of a normalized reward score. In particular, we took the mean reward per trial, RP T , for each subject in each block of the experiment and normalized this relative to optimal RP T opt and random performance RP T rand , which were computed according to Equations S28 and S29 in the Supplementary material. Thus we computed the normalized reward as</p><formula xml:id="formula_7">N R = RP T − RP T rand RP T opt − RP T rand (8)</formula><p>This normalized reward has value 1 for optimal performance and 0 for random performance. As a conservative threshold, we excluded participants whose normalized reward was less than 0. Thus, in Experiment 1, participants were excluded if their normalized reward was less than 0 overall in Experiment 1, or less than 0 in either block in Experiment 2. With these exclusion criteria, as shown in <ref type="figure">Figure 5,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head><p>Figure 5: Normalized performance and exclusion criteria. The normalized reward score is 1 for performance on a par with the optimal model and 0 for purely random performance. (A) In Experiment 1, participants were excluded if they achieved a normalized reward of less than 0, or if they performed fewer than 400 trials. (B) In Experiment 2, participants were excluded if they achieved a normalized reward of less than 0 in either the fast ITI or slow ITI block, or if they performed fewer than 400 trials in either block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptive model</head><p>We quantified behavior on the Infinite Bandits Task using a series of descriptive models that quantified the overall level of directed and random exploration as well as accounting for possible confounding factors such as lapsing and sequential effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic form of the θ-σ model</head><p>In its most basic form, the descriptive model assumes that participants make choices by assigning different values to each of possible options (either explore and exploit). We assume that the exploit option has a value equal to its cued value, c, and the explore option has a value given by the 'explore threshold', θ, that corresponds to the expected value of the explore option plus the information bonus associated with choosing it. Next we assume that participants make a decision based on a noisy difference between these two values,</p><formula xml:id="formula_8">∆ = θ − c + σn (9)</formula><p>where n is zero mean logistic noise with variance 1, and σ describes the magnitude of the decision noise. Finally we assume that participants choose to explore if ∆ &gt; 0, exploit if ∆ &lt; 0 and choose randomly if ∆ = 0. Thus on average (over the decision noise) the model assumes that participants choose to exploit with probability</p><formula xml:id="formula_9">p exploit = 1 1 + exp θ−c σ (10)</formula><p>The choice probability for the 'θ-σ model' is plotted in <ref type="figure">Figure 6A</ref> as function of exploit value c and shows how the parameters θ and σ relate to the choice curve. A B <ref type="figure">Figure 6</ref>: Relationship between model parameters and choice curves. As the exploit value increases the model is more likely to choose the exploit option. (A) In the θ-σ model, the explore threshold, θ, (in this case θ = 70) determines the exploit value at which the model is indifferent between the explore and exploit options. The decision noise, σ, (in this case σ = 10) determines the slope of the choice curve at the indifference point. (B) In the θ-σ-model, 1 = 0.3, 2 = 0.15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of the θ-σ model to include lapsing and sequential effects</head><p>To account for the possibility of factors other than directed and random exploration driving behavior, we extended the θ-σ model to incorporate lapsing and sequential effects.</p><p>Lapsing occurs when participants choose randomly, with no regard to the exploit value c. Lapses are most apparent at the far asymptotes of the choice curve and is captured by two parameters ( <ref type="figure">Figure 6B</ref>): 1 , for the lapse rate for exploiting (left asymptote of curve for small c), and 2 the lapse rate for exploring (right asymptote of curve for large c). Such lapsing is a common feature of many decisions (e.g. <ref type="bibr">Busse et al., 2011;</ref><ref type="bibr">Gold and Ding, 2013;</ref><ref type="bibr">Carandini and Churchland, 2013)</ref> and models that include two lapse rates have been shown to fit data better and provide better estimates of other parameters such as θ and σ <ref type="bibr">(Wichmann and Hill, 2001;</ref><ref type="bibr">Prins and Kingdom, 2018)</ref>.</p><p>Sequential effects occur when information from past trials affects the decision on the present trial. Here we consider three such factors: the effect of past exploration (e past , whether the last choice was to explore or exploit), the effect of past physical actions (a past , whether the last choice left or right), and the effect of past change points (cp past , whether the last trial was the start of a new game). We assume that these sequential effects combine linearly with weights β e for past exploration, β a for past physical action, and β cp for past change points.</p><p>Combining lapses and sequential effects gives us the following general form for choice probabilities</p><formula xml:id="formula_10">p exploit = (1 − 1 − 2 ) 1 1 + exp θ−c+βeepast+βaapast+βcpcppast σ + 1<label>(11)</label></formula><p>The free parameters of this extended model are described in Previous change-points −∞ to ∞ By including and excluding the factors 1 , 2 , β e , β a , and β cp in Equation 11, we created a suite of different models. For the effects of past side and past change points we only included the effect of the last trial, while for the effect of past explore/exploit choice we included the effects of up to two trials into the past. This lead to 48 different forms of the model generated from the presence or absence of 1 , 2 , β e , and β cp (2 4 = 16 combinations), and the absence or presence of 1 or 2 trials back from β e (giving 16 × 3 = 48 combinations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Exploration model</head><p>As with the Descriptive model above, to fit the Deep Exploration model to behavior, we augmented the basic choice probability (Equation 7) to include the possibility of lapsing and sequential effects. That is, we assumed the probability of exploiting was given bỹ</p><formula xml:id="formula_11">p exploit = (1 − 1 − 2 ) 1 1 + exp (logit (p exploit (n, T )) + β e e past + β a a past + β cp cp past ) + 1 (12)</formula><p>where p exploit (n, T ) was given by equation 7. Note that for the purposes of fitting, instead of parameterizing the model through the simulation horizon T , we used the implied hazard rate, h = 1/T . While this substitution does not change the properties of the model, it has the advantage (especially for visualization purposes) that h is between 0 and 1 while T is unconstrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model fitting</head><p>All models were fit using a maximum likelihood approach. In particular, each model gives us a closed form expression for the probability of choosing the exploit option, p exploit , as a function of parameters, φ, which we then use to compute the likelihood of the observed actions, a 1:t , as</p><formula xml:id="formula_12">p(a 1:t |φ) = t i=1 p(a i |φ, a 1:i−1 )<label>(13)</label></formula><p>where</p><formula xml:id="formula_13">p(a i |φ, a 1:i−1 ) = p exploit a i = exploit 1 − p exploit a i = explore<label>(14)</label></formula><p>Maximum likelihood parameter fits,φ, were determined by maximizing the log likelihood</p><formula xml:id="formula_14">φ = argmax φ log p(a 1:t |φ)<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparison</head><p>Model fits were compared using Bayes Information Criterion scores <ref type="bibr">(BIC, Schwarz et al., 1978)</ref> computed</p><formula xml:id="formula_15">as BIC = k log T − 2 logLL (16)</formula><p>where k is the number of free parameters in the model, T is the number of trials andLL is the maximum log likelihood. For each model, we computed the number of participants best fit by that model as well as the exceedance probability <ref type="bibr">(Rigoux et al., 2014)</ref>. This latter measure estimates the probability that each model generated the data from all participants under the assumption that all participants used the same model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods References</head><p>Bogacz, R., Brown, E., Moehlis, J., Holmes, P., and Cohen, J. D. <ref type="bibr">(2006)</ref>. The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks. Psychological review, 113(4):700.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Optimal strategy for the Infinite Bandits Task</head><p>The optimal strategy in the Infinite Bandits Task is a threshold policy whereby one should always exploit when the known option is greater than some threshold θ * , otherwise one should explore.</p><p>To compute the value of the optimal threshold we imagine a situation where the exploit value c exactly equals the threshold, θ * . Also, to begin, we assume that the game length, T , is fixed. In this case, the value of choosing the exploit option is simply</p><formula xml:id="formula_16">Q c (T ) = T c = T θ * (S1)</formula><p>This is because if one exploits on the first choice one should exploit to the end of the game because, by definition the value of exploration and exploitation are equal when c = θ * . We can also compute the value of exploration in the case where c = θ * as</p><formula xml:id="formula_17">Q u (T ) = good outcome u &gt; θ * T p(u &gt; θ * )E(u|u &gt; θ * ) + bad outcome u ≤ θ * p(u ≤ θ * ) (E(u|u ≤ θ * ) + (T − 1)θ * ) = T m − θ * m m + θ * 2 + θ * m θ * 2 + (T − 1)θ * = T (m 2 − θ * 2 ) 2m + θ * 2 2m + (T − 1)θ * 2 m = T m 2 − (T − 1) θ * 2 2m + (T − 1) θ * 2 m = T m 2 + (T − 1) θ * 2 2m = Tū + (T − 1) θ * 2 2m (S2)</formula><p>whereū is the average reward for choosing the explore option. Taking the average over T we get the average value associated with this policy when c = θ * as</p><formula xml:id="formula_18">Q u = ∞ T =1 p(T ) Tū + (T − 1) θ * 2 2m =Tū + (T − 1) θ * 2 2m<label>(S3)</label></formula><p>whereT = 1/h. Likewise, the expected value of exploiting is</p><formula xml:id="formula_19">Q c =T θ * (S4)</formula><p>By definition Q c = Q u when c = θ * so we havē</p><p>Tū</p><formula xml:id="formula_20">+ (T − 1) θ * 2 2m =T θ * =⇒ T − 1 2m θ * 2 −T θ * +Tū = 0<label>(S5)</label></formula><p>Solving for θ * we get</p><formula xml:id="formula_21">θ * = m √T 1 + √T = m 1 + √ h (S6)</formula><p>which for the experiment, where h = 0.1 and m = 100, gives θ * = 75.97 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Exploration algorithm for the Infinite Bandits Task</head><p>A simple deep exploration algorithm for the Infinite Bandits Task proceeds in the following way. First sample n plausible values for the unknown bandit,ũ = (ũ 1 ,ũ 2 , ...,ũ n ). If the agent knows the prior is a uniform between 0 and m then this is done by sampling eachũ i asũ i ∼ U (0, m). For each sample, i, the agent then computes the expected value of exploring,Q i u , as thoughũ i were the actual payoff from the unknown option. This expected value can be written as</p><formula xml:id="formula_22">Q i u = Tũ i ifũ i &gt; c u i + (T − 1)c ifũ i ≤ c<label>(S7)</label></formula><p>Dividing by T we get the expected value per trial for this samplẽ</p><formula xml:id="formula_23">q i u = ũ i ifũ i &gt; c 1 T (ũ i + (T − 1)c) ifũ i ≤ c<label>(S8)</label></formula><p>Finally, we compute the approximate expected value,q u , by averaging over the n samples,</p><formula xml:id="formula_24">q u = 1 n n i=1q i u (S9)</formula><p>The decision is made by comparingq u to the value of exploiting c, with the algorithm exploring whenq u &gt; c and exploiting otherwise. Behavior of the algorithm for different values of n for fixed horizon, T = 16, is shown in <ref type="figure" target="#fig_2">Figure S3</ref>. As the number of samples increases the algorithm gradually converges to the optimal choice curve. More insight into this behavior can be obtained by computing analytic forms for the choice curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Choice curves for n = 1</head><p>When n = 1 theq u is based on just one sample. From equation S8, we can easily see that the probability that theq u is less than c is equal to the probability that the single sample u 1 is less than c; i.e. p exploit = p(q u &lt; c) = p(u &lt; c) = c m (S10)</p><p>Thus the n = 1 case is a simple probability matching strategy whereby the algorithm choose each option with the probability that it will be the best. Note that this strategy has only random exploration with the indifference point being equal to the mean of the prior, m/2. In addition, the n = 1 strategy is independent of horizon, T . In the bandits literature such a probability matching strategy is known as Thompson sampling <ref type="bibr" target="#b17">(Thompson, 1933)</ref> and it is a general result that the n = 1 case of Deep Exploration is equivalent to Thompson sampling.  <ref type="figure" target="#fig_0">Figure S1</ref>: Probability distribution over the expected value of one sample, p(q i u ). From equation S8, this probability is T /m for (T − 1)c/T ≤q i u ≤ c, 1/m for c &lt;q i u ≤ m and zero otherwise. From here it is easy to see that the probability of exploiting in the one-sample case is simply c/m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Choice curves for n = 2</head><p>In this case we can compute an exact expression for the choice curves. To do this we need to determine the probability that the mean of two samples, 1 2 (q 1 u +q 2 u ), is less than c. This can occur in two different ways. First, both samples could be less than c, that is, they could lie in the region 1 in <ref type="figure" target="#fig_7">figure S2</ref>. When this happens, the average of the two samples is guaranteed to be less than c and so the algorithm will always exploit when this happens. From equation S8 it is easy to see that both samples will be less than c with probability (c/m) 2 .</p><p>The second way the mean of the samples could be less than c is if one sample was greater than c, the other was less than c and the sum of the two was less than 2c, that is if the samples lie in one of the red triangle regions in <ref type="figure" target="#fig_7">figure S2</ref>. The probability of one sample being larger than c and on smaller than c is</p><formula xml:id="formula_25">p(u 1 &gt; c)p(u 2 ≤ c) = m − c m c m (S11)</formula><p>The probability that the sum of the two samples is less than 2c depends on the size of c. If c ≤ mT /(T + 1) then the probability is the relative size of red triangle to the rectangle 2 in panel A of <ref type="figure" target="#fig_7">figure S2</ref>. This is simply c/2T (m − c). If c &gt; mT /(T + 1) then the probability is the relative size of the red area to the rectangle in panel B of figure S2. This is 1 − c/2T (m − c).</p><p>Putting it all together this gives the probability of exploiting in the n = 2 case as</p><formula xml:id="formula_26">p exploit = c 2 m 2 T +1 T c ≤ T m/(T + 1) c 2 m 2 1 + m−c c 2 − T m−c c c &gt; T m/(T + 1)<label>(S12)</label></formula><p>From equation S12, we can compute analytic expressions for the indifference point as the value of c at which</p><formula xml:id="formula_27">p exploit = 1/2 c * m = T 2(T + 1)<label>(S13)</label></formula><p>Note that c * &lt; T m/(T + 1) and so this expression holds regardless of T . In addition we can compute the inverse of the gradient of the choice curve at the indifference point, a proxy for the decision noise, as Note that c * and σ * have identical dependence on T . This means that for the two-sample case both directed and random exploration have the same horizon dependence. Note also that the Tdependence for c * is different to the optimal model.</p><formula xml:id="formula_28">σ * m = 1 m dp exploit dc c * = T 2(T + 1)<label>(S14</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Approximate choice curves for general n</head><p>To compute choice curves for general n we will first compute an approximation for the choice curves when exactly n + of the samples have u i &gt; c and n − (= n − n + ) of the samples have u i ≤ c. That is, we write</p><formula xml:id="formula_29">p exploit = p(q u ≤ c) = n n + =0 p(q u (n + ) ≤ c)p(n + |c) = n n + =0 p n + exploit p(n + |c) (S15) where p n +</formula><p>exploit is the probability of exploiting given that n + of the samples have u i &gt; c and p(n + |c) is the probability of seeing n + samples with u i &gt; c, which is simply the binomial probability</p><formula xml:id="formula_30">p(n + |c) = n n + m − c m n + c m n−n + (S16) To compute p n +</formula><p>exploit , we first writẽ</p><formula xml:id="formula_31">q u (n + ) = 1 n i∈u i &gt;cq i+ u + 1 n i∈u i ≤cq i− u<label>(S17)</label></formula><p>whereq u (n + ) is the expected value given that n + of the samples have u i &gt; c,q i+ u is the expected value of a single sample given that u i &gt; c andq i− u is the expected value of a single sample given that u i ≤ c.</p><p>The point of the decomposition in equation S17 is that, unlikeq i u ,q i+ u andq i− u are distributed uniformly. Thus, the sums in equation S17 are sums of uniformly sampled random variables and will follow an Irwin-Hall distribution. In addition, we note thatq i− u ranges between c(T − 1)/T and c, which for large T , becomes small. Thus we can approximate the second sum as</p><formula xml:id="formula_32">i∈u i ≤cq i− u ≈ (2T − 1)(n − n + )c 2T (S18) Thus p n +</formula><p>exploit is approximately given by</p><formula xml:id="formula_33">p n + exploit ≈ p i∈u i &gt;cq i+ u &gt; c n − (2T − 1)(n − n + ) 2T (S19)</formula><p>Using the standard form of the cumulative distribution function of the Irwin-Hall distribution, after some algebra, this can be written as</p><formula xml:id="formula_34">p n + exploit ≈ 1 n + ! c(n−n + ) 2T (m−c) k=0 (−1) k n + k c(n − n + ) 2T (m − c) − k n + (S20)</formula><p>Putting this all together we can write the probability of exploiting as <ref type="figure" target="#fig_2">Figure S3</ref>, this expression provides a good approximation of the choice curves.  <ref type="figure" target="#fig_2">Figure S3</ref>: Behavior of the algorithm as a function of the number of samples, n. Solid lines correspond to the analytic approximation, while the dots correspond to the average behavior of 10, 000 simulations. As n increases, the model shows more directed exploration, choice curves shift to the right, and less random exploration, choice curves get steeper. As n → ∞, the algorithm approaches the behavior of the optimal model.</p><formula xml:id="formula_35">p exploit ≈ n n + =0 n n + m − c m n + c m n−n + 1 n + ! c(n−n + ) 2T (m−c) k=0 (−1) k n + k c(n − n + ) 2T (m − c) − k n + (S21) As shown in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluating performance of different strategies on the Infinite Bandits Task</head><p>In this section we outline how to compute the performance of a given strategy on the Infinite Bandits Task as used in <ref type="figure">Figure 2B</ref> in the main text. In particular, we show how to compute the expected number of points per trial, V (π), for playing the task with strategy (or policy) π. Our approach for computing this value involves three steps. First we compute the average earnings per trial when the exploit value is c, R(c|π). Next we compute the frequency with which trials with exploit value c occur, F (c|π). Finally we compute V (π) by averaging over c.</p><p>Step 1 -Computing the reward per trial, R(c|π) Because an exploit choice always yields reward c and an explore choice yields an average reward ofū = 50.5 (the average of all possible reward values from 1 to 100), R(c, π), is simply the average of c andū over the choice probability, p(a|c, π); i.e., R(c|π) = cp(a = exploit|c, π) +ūπ(a = explore|c, π) (S22)</p><p>Step 2 -Computing the frequency of trials with exploit value c, F (c, π) This step is slightly more involved. First we write down the transition matrix, T , that is made up of elements T c ,c denoting the probability of moving to an exploit value of c on the next trial given that we have an exploit value c on the current trial. In particular, T c ,c can be written in terms of the choice probability in Equation S21, p(a|c, π), and the transition probability given the choice, p(c |c, a), as</p><formula xml:id="formula_36">T c ,c = a p(c |c, a)π(a|c, π)<label>(S23)</label></formula><p>It is then straightforward to write down p(c |c, a) for the two different actions, exploit and explore. For exploit choices, when there is no new game (which occurs with probability 1 − h), the exploit value does not change on the next trial, i.e. c = c. When there is a new game (which occurs with probability h) c is equally likely to take on any value from 1 to 100 (including c). Thus we can write</p><formula xml:id="formula_37">p(c |c, a = exploit) = 1 − h + h/100 c = c h/100 c = c (S24)</formula><p>For explore choices, when there is no new game, the exploit value only stays the same (c = c) if the explore value was less than or equal to c, which occurs with probability c/100. Otherwise, a given value of c above c occurs with probability of 1/100. Finally, when there is a new game, as for exploit choices, c is equally likely to take on any value. Thus,</p><formula xml:id="formula_38">p(c |c, a = explore) =    (1 − h)/100 + h/100 c &gt; c (1 − h)s/100 + h/100 c = c h/100 c &lt; c<label>(S25)</label></formula><p>Equations S24 and S25 allow us to compute the transition probability T c ,c , which in turn allows us to compute F (c|π). In particular, since F (c|π) is the stationary distribution over exploit values given the choice probabilities in Equation S21, it will be invariant under transformation by T; i.e.</p><formula xml:id="formula_39">F (c |π) = 100 c=1 T c ,c F (c|π)<label>(S26)</label></formula><p>This equation implies that F (c|π) is the eigenvector of the matrix T with eigenvalue 1, something that is readily obtained by standard numerical methods (in our case using the eigs function in Matlab).</p><p>Step 3 -Computing the expected number of points per trial, V (π) Now that we have R(c|π), the expected reward given the exploit value is c, and F (c|π), the frequency with which trials with exploit value c occur, it is straightforward to write down the expected performance, V (π), as the average over s; i.e.,</p><formula xml:id="formula_40">V</formula><formula xml:id="formula_41">(π) = 100 s=1 F (s|π)R(s|π)<label>(S27)</label></formula><p>In this way we can compute the reward per trial for optimal and random performance used in the normalized reward (Equation 8 in the main paper). In particular, for optimal performance we have RP T opt = V (π = optimal) = 79.35 points (S28)</p><p>where the optimal policy is a deterministic threshold policy with threshold equal to 75.97. Similarly, for random performance</p><formula xml:id="formula_42">RP T rand = V (π = random) = 67.34 points (S29)</formula><p>where the random policy is to choose each option with 50% probability.</p><p>4 Descriptive model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model comparison for descriptive model variants</head><p>We fit all 48 combinations of the descriptive model along with a purely random model that assumed a fixed probability b for exploring or exploiting. Model performance was compared using BIC as described above. For each model we computed the number of participants best fit and the exceedance probability -the probability that a model can best fit data from all participants under the assumption that all participants used the same model. Results are summarized in <ref type="figure">Figure S4</ref>. The best two models -that were almost tied for both number of participants best fit and exceedance probability -were the pure θ-σ model, with no additional factors, and the θ-σ model including the effect exploring or exploiting on the last trial. These findings accord well with recent work by <ref type="bibr" target="#b15">(Song et al., 2019)</ref>, who showed a similar dependence on previous exploration in a related task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlation between directed and random exploration is conserved across models</head><p>Is the negative correlation between directed and random exploration robust to the inclusion of different factors in the model? To answer this question, we computed the correlation between θ and σ parameters for all 48 versions of the descriptive model. Regardless of the form of the model, we found that the strong negative correlation between directed and random exploration was maintained ( <ref type="figure" target="#fig_10">Figure S5</ref>). This suggests that the trade off is not an artifact of the parameterization of the model. exceedance probability <ref type="figure">Figure S4</ref>: Model comparison for different versions of the descriptive model. Left Parameters in each model ranked by number of participants best fit. Each row corresponds to a different model and each column to a different parameter. White cells correspond to the absence of the parameter in that model, grey cells to the presence of a parameter and (for β e ) black cells correspond to case where the sequential effect is considered up to two trials in the past. The purely random model (the worst fit of all at rank 49) is represented by the absence of all parameters, i.e. all entries are white. Middle The number of participants best fit by each model. Right The exceedance probability for each model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The five parameter descriptive model</head><p>The remaining descriptive model analyses (including those in <ref type="figure" target="#fig_2">Figures 3 and 4</ref> from the main paper) used a five parameter version of the descriptive model that included terms for directed and random exploration (θ and σ), lapsing ( 1 and 2 ), and past effects of the explore-exploit choice (β e ). While this model was only ranked fifth in terms of BIC ( <ref type="figure">Figure S4)</ref>, it captures the main suboptimalities exhibited by people (i.e. lapsing and sequential effects of last choice), and includes most of the better models as special cases (e.g. 1 = 2 = 0). Thus it represents a flexible choice of model that accounts for most of the observed behavior without introducing too many free parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter recovery for the five-parameter descriptive model</head><p>While the analysis in <ref type="figure" target="#fig_10">Figure S5</ref> shows that all versions of the descriptive model exhibit a negative correlation between directed and random exploration, it is still possible that this correlation could result from some bias in the fitting procedure or even a bug in the code. To determine whether the the negative correlations were introduced artificially we ran two parameter recovery analyses for the five-parameter descriptive model <ref type="bibr" target="#b18">(Wilson and Collins, 2019)</ref>.</p><p>In these analyses we simulated data according to the model with known parameter values. We then refit this simulated data to ask whether the parameters fit to the simulated data were similar to the actual parameters used in the simulation. In the first analysis, we simulated data for each subject using the parameter values fit for that subject. As shown in <ref type="figure" target="#fig_11">Figure S6</ref> this leads to a close correspondence between simulated and fit parameters; i.e. parameter recovery is excellent for this model in this task. Next we asked whether the tradeoff between directed and random exploration could be induced artificially by the fitting procedure. To this end we simulated data from 174 participants using scrambled versions of the fit parameters that removed the negative correlation between θ and σ across the population ( <ref type="figure" target="#fig_13">Figure S7 left)</ref>. Because there was no correlation between θ and σ in the simulations a negative correlation in the recovered parameters would show that the tradeoff was an artifact. As shown in <ref type="figure" target="#fig_13">Figure S7</ref> our fitting procedure did not artificially induce such a correlation suggesting that the tradeoff between directed and random exploration is a real feature of human behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fitting the Deep Exploration model to behavior</head><p>As with the Descriptive model above, to fit the Deep Exploration model to behavior, we augmented the basic choice probability (Equation 7 in the main paper) to include the possibility of lapsing and sequential effects. That is, we assumed the probability of exploiting was given bỹ</p><formula xml:id="formula_43">p exploit = (1− 1 − 2 )</formula><p>1 1 + exp (logit (p exploit (n, T )) + β e e past + β a a past + β cp cp past ) + 1 (S30)  where p exploit (n, T ) was given by equation S21. Note that for the purposes of fitting, instead of parameterizing the model through the simulation horizon T , we used the implied hazard rate, h = 1/T . While this substitution does not change the properties of the model, it has the advantage (especially for visualization purposes) that h is between 0 and 1 while T is unconstrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model comparison</head><p>As with the descriptive models, we fit multiple versions of the Deep Exploration that included/excluded different factors. As shown in <ref type="figure" target="#fig_14">Figure S8</ref> the best fitting models included lapse rates and the effect of the past explore/exploit choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameter recovery</head><p>As with the descriptive model we performed a parameter recovery analysis to confirm that parameters could be recovered in the best case scenario in which the behavior is actually generated by the model. In particular, we focused on the version of the Deep Exploration model with yoked lapse rates (i.e. 1 = 2 = ) that included an exploration kernel (i.e. β e = 0). Simulated data were generated using the fit parameter values. As shown in <ref type="figure" target="#fig_15">Figure S9</ref> simulation and fit parameter value are highly correlated for all four parameters demonstrating good parameter recovery for this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison between simulated and actual choice curves</head><p>To confirm that the Deep Exploration model provides a good fit for all participants, in <ref type="figure" target="#fig_0">Figure S10</ref> we plot choice curves from all 174 participants along with the simulated choice curves from the Deep Exploration model. This shows excellent correspondence between model and experiment for all participants in our data set.   6 Deep exploration can account for horizon, uncertainty, and feedback dependence of directed and random exploration</p><p>Previous work has revealed three key features of directed and random exploration: (1) that both directed and random exploration depend on the time horizon <ref type="bibr" target="#b19">(Wilson et al., 2014)</ref>; (2) that directed and random exploration are both diminished when feedback is not contingent on choice <ref type="bibr" target="#b13">(Findling et al., 2019)</ref>; and (3) that directed and random exploration depend on the uncertainty in the options <ref type="bibr" target="#b16">(Speekenbrink and Konstantinidis, 2015;</ref>. In this section we show that all of these results can be accounted for by Deep Exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Horizon Task and its variants</head><p>To ground our analysis, we focus on three variants of a gambling task known as the 'Horizon Task' <ref type="bibr" target="#b19">(Wilson et al., 2014)</ref>. In these tasks participants make a series of choices between two onearmed bandits that pay out probabilistic rewards. To maximize their rewards, participants need to exploit the option with the highest average payout, but because the reward probabilities are initially unknown, participants must explore both options in order to find out which option is best. Three variants of this task manipulate the horizon, feedback structure, and uncertainty, allowing us to investigate the three key features of directed and random exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Horizon Task</head><p>In the Horizon Task <ref type="bibr" target="#b19">(Wilson et al., 2014)</ref>, participants play a series of 'games,' lasting either 5 or 10 trials each. On each trial they choose between two one-armed bandits that pay out rewards from Gaussian distributions. The means of the Gaussians are different for each bandit and change from game to game. In each game, one option is randomly assigned a mean of either 40 or 60 points, and the other a mean that is either higher or lower by 4, 8, 12, 16, or 20 points (thus one option is always better on average). The standard deviation of the Gaussians is identical for both bandits and fixed throughout the experiment at 8 points. The first 4 trials of each game are 'forced' trials, in which participants are instructed as to which options they must play. By controlling which options participants play, these forced trials allow us to control the information people have about the options before their first 'free' choice. In particular, we use the forced trials to setup one of two information conditions: an unequal (or [1 3]) condition, in which participants play one option once and the other three times, and an equal (or [2 2] condition) in which participants play both options twice.</p><p>In addition to the information manipulation, we also manipulate the horizon by setting the number of trials in each game to be either 5 or 10. Because the first four trials are forced trials, this gives participants a horizon of either 1 or 6 free choices per game. The horizon dependence of directed and random exploration can then be measured by comparing behavior on the first free choice of each game, where the only difference between conditions is the number of decisions participants will make in the future.</p><p>To quantify directed and random exploration we fit a logistic model to the behavior. In particular, we assume that choice is a function of the difference in the observed mean rewards for each option, ∆R = R lef t − R right , and the observed difference in information between the two options, ∆I (∆I = 0 in the [2 2] condition, ∆I = +1 when the left option was played once in the [1 3] condition, ∆I = −1 when the right option was played once in the [1 3] condition). That is, we write the probability of choosing the left option as</p><formula xml:id="formula_44">p(choose left) = 1 1 + exp − ∆R+A∆I+B √ 2σ (S31)</formula><p>where the free parameters A, B, and σ are the information bonus, side bias, and decision noise respectively. These free parameters are fit to each subject in each of the conditions: the two horizon conditions for A, and the four horizon × uncertainty conditions for B and σ. From there, directed exploration is quantified using the information bonus and random exploration by the decision noise. Data from the original Horizon Task paper <ref type="bibr" target="#b19">(Wilson et al., 2014</ref>) (30 participants after exclusions, 20 women, 10 men, ages 18-24) are shown in the top row of <ref type="figure" target="#fig_0">Figure S11</ref>. For simplicity we focus on decision noise in the [2 2] condition only, although, as shown in the original paper, similar results hold for decision noise in the [1 3] condition. The key finding is that both information bonus and decision noise increase with horizon (paired t-test comparing information bonus between horizon 1 and 6: t(29) = 5.06, p &lt; 0.001; paired t-test comparing decision noise in [2 2] condition between horizon 1 and 6: t(29) = 5.61, p &lt; 0.001). This indicates that both directed and random exploration are horizon dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Feedback Task</head><p>In the original Horizon Task, participants only receive information about the options they choose. Such 'partial feedback' is critical for the explore-exploit dilemma to be present because it makes information contingent on choice. When information is not contingent on choice, for example in a 'full feedback' condition where participants gain information about both options regardless of choice, then exploration has no value because it yields the same information as exploiting. In the Feedback Task we explicitly manipulate the feedback participants receive between partial and full feedback.</p><p>The Feedback Task replicates the main features of the Horizon Task, with the same two horizon and two uncertainty conditions controlled by four forced trials (note that the forced trials always have partial feedback). On top of this, however, the Feedback Task also manipulates feedback. In the partial feedback condition, participants play 160 games of the original Horizon Task where information is dependent on choice. In the full feedback condition, participants play 160 games in which they see the outcome of both options regardless of choice on the free-choice trials. Critically, participants only receive reward for the option they choose. Thus, in the full feedback condition, participants are incentivized to exploit, but not to explore.</p><p>Behavior is fit using the same logistic model as used in the Horizon Task, and the key question is whether the information bonus and decision noise differ between feedback conditions. Data from 31 subjects (23 women, 8 men, ages 18-42) performing the Feedback Task is shown in the middle row of <ref type="figure" target="#fig_0">Figure S11</ref>. We see an interaction between horizon and feedback condition for both information bonus and decision noise (see Tables S1) that is driven by a reduced information bonus (t(27) = −3.5, p = 0.0018) and reduced decision noise (t(27) = −3.6, p = 0.0014) in horizon 6 in the full feedback condition. Thus, both directed and random exploration are reduced when feedback is not contingent on choice. Note that the result for noise replicates the earlier finding of   <ref type="bibr" target="#b13">Findling et al. (2019)</ref> who showed that exploratory noise is reduced in a full feedback task. Also, of note -and as we shall see, something that is predicted by Deep Exploration -is that decision noise is horizon dependent in the full feedback condition (t(27) = 2.7, p = 0.011).  2) performed 240 games of the Parametric Uncertainty Task. Behavior was fit using the same logistic model, with the proviso that the information variable ∆I indicates which option is more informative to choose. That is ∆I = 0 for the equal uncertainty conditions ∆I = ±1 for the unequal uncertainty conditions. This allows us to estimate an information bonus and decision noise for each uncertainty condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information bonus</head><p>Analyses of variance revealed a trend towards a main effect of uncertainty condition on information bonus and a strong main effect of uncertainty condition on decision noise (see <ref type="table" target="#tab_8">table S2</ref>). This trend for directed exploration was driven by an increased information bonus between the [2 3] and [1 2] conditions (paired t-test, t(28) = 2.6, p = 0.015) and the [2 3] and [1 3] conditions (paired t-test t(28) = 2, p = 0.059).  Further insight into these results can be obtained by plotting the information bonus and decision noise as a function of the uncertainty that a Bayesian ideal observer would have about the options after the forced choice trials. In particular, if we assume that the model knows the true variance of the reward distribution (σ n = 8 points), then the uncertainty in the estimate of the mean of the rewards after the forced choice trials will be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information bonus</head><formula xml:id="formula_45">σ i = σ n √ n i (S32)</formula><p>where n is the number of times option i has been played during the forced choice trials. Plotting the information bonus as a function of difference in uncertainty between the two options and the decision noise as a function of total uncertainty (S11, bottom row) reveals a near linear relationship between the variables. This result is consistent with the findings of  and represents a conceptual replication of his results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Optimal model for Horizon Task variants</head><p>Subject to some assumptions, the optimal policy in the Horizon Task (and its variants) is computationally tractable. This analysis was first described in the Supplementary Material of <ref type="bibr" target="#b19">(Wilson et al., 2014)</ref>, but is summarized here as it helps frame the Deep Exploration model.</p><p>In particular, the optimal model assumes that participants know that the rewards on trial t, r t , are generated from Gaussian distributions and, further, that participants know the standard where the posterior over the mean, p(µ j |D t ), is given by Equation S33. Given the sampled means, similar to the Deep Exploration algorithm for the Infinite Bandits Task, whenever a bandit is chosen in the simulation the reward is taken to be equal to that mean; i.e. r t =μ a i t (S46)</p><p>For the simulated actions, the model chooses according to a Thompson sampling scheme. In particular, we assume that the model chooses action a t in state S t with probability equal to the probability that option a t has the higher mean in this state; i.e.,</p><formula xml:id="formula_46">p(a i t = 1|S i t ) ∼ p(µ 1 &gt; µ 2 |S t )<label>(S47)</label></formula><p>After the simulated action is taken, the corresponding reward is sampled and the simulated state is updated. For the Horizon and Parametric Uncertainty Tasks, the state update follows equation S48. For the full feedback task, because information about both rewards is revealed, the state updates to S t+1 = (n 1 t + 1, R 1 t + r 1 t , n 2 t + 1, R 2 t + r 2 t ) (S48)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Deep exploration can account for behavior in Horizon Task variants</head><p>We used the Deep Exploration model to simulate behavior in each of the three tasks. To illustrate the model's behavior, we set the number of samples n = 2 and the standard deviation of the reward distribution, σ = 16. Although we did not explicitly fit these parameter values to behavior (because the goal of our analysis is to show that Deep Exploration can capture the qualitative properties of human behavior) we note that these parameter settings give values for the information bonus and decision noise that are comparable to those seen in human performance. As shown in <ref type="figure" target="#fig_0">Figure S12</ref>, the model captures the main qualitative features of all three experiments. In particular, the model captures the horizon dependence of directed and random exploration ( <ref type="figure" target="#fig_0">Figure S12A and B)</ref>; the reduction in directed and random exploration in the full feedback task ( <ref type="figure" target="#fig_0">Figure S12A and B)</ref>; the horizon dependence of decision noise in the full feedback task <ref type="figure" target="#fig_0">(Figure S12B)</ref>; and the uncertainty dependence of directed and random exploration ( <ref type="figure" target="#fig_0">Figure S12C</ref> and D). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Infinite Bandits Task showing the different trial types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Behavior of the algorithm as a function of the number of samples, n. (A) Solid lines correspond to the analytic approximation, while the dots correspond to the average behavior of 10, 000 simulations. As n increases, the model shows more directed exploration (choice curves shift to the right), and less random exploration (choice curves get steeper). As n → ∞, the algorithm approaches the behavior of the optimal model. (B) Performance of the model as a function of the number of samples. Increasing the number of samples increases the average number of points earned in the task, but even with one sample, the performance of Deep Exploration is within 6% of optimal. (C) Illustration of indifference point, θ, which captures directed exploration, and slope σ, which captures random exploration. (D) Theoretical tradeoff between directed and random exploration for a given value of hazard rate h = 0.1. Note colored stars correspond to different number of samples as given by legend in panel A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Between-subject tradeoff of directed versus random exploration in the Infinite Bandits Task. (A) Explore threshold and decision noise are negatively correlated, consistent with a tradeoff between directed and random exploration. Choice curves for two example participants, one showing more directed than random exploration (subject 146, estimated number of samples n = 19.0) and the other showing more random than directed exploration (subject 46, n = 1.2). The solid lines correspond to simulated behavior from the Deep Exploration model. (B) Average choice curves (dots) and Deep Exploration behavior (lines) for participants grouped according to the best fitting number of samples. (C) Number of simulations used by each subject showing that most participants use between 1 and 5 simulations to make their decision. (D) participants who use more samples respond more slowly. Each additional sample increases reaction time by approximately 50ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>:700.Costa, V. D., Mitz, A. R., and Averbeck, B. B. (2019). Subcortical substrates of explore-exploit decisions in primates. Neuron.Ebitz, R. B.,Albarran, E., and Moore, T. (2018). Exploration disrupts choice-predictive signals and alters dynamics in prefrontal cortex. Neuron, 97(2):450-461.<ref type="bibr" target="#b13">Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., and Wyart, V. (2019)</ref>. Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature neuroscience, pages 1-12.Frank, M. J.,Doll, B. B., Oas-Terpstra, J., and Moreno, F. (2009). Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation.Nature neuroscience, 12(8):1062. Meyer, R. J. and Shi, Y. (1995). Sequential choice under ambiguity: Intuitive solutions to the armed-bandit problem. Management Science, 41(5):817-834. Newell, A. (1994). Unified theories of cognition. Harvard University Press. Olveczky, B. P., Andalman, A. S., and Fee, M. S. (2005). Vocal experimentation in the juvenile songbird requires a basal ganglia circuit. PLoS biology, 3(5):e153. Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. (2016). Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pages 4026-4034. Payzan-LeNestour,É. and Bossaerts, P. (2012). Do not bet on the unknown versus try to find out more: estimation uncertainty and unexpected uncertainty both modulate exploration. Frontiers in neuroscience, 6:150. Redish, A. D. (2016). Vicarious trial and error. Nature Reviews Neuroscience, 17(3):147. Schulz, E. and Gershman, S. J. (2019). The algorithmic architecture of exploration in the human brain. Current opinion in neurobiology, 55:7-14. Speekenbrink, M. and Konstantinidis, E. (2015). Uncertainty and exploration in a restless bandit problem. Topics in cognitive science, 7(2):351-367. Sternberg, S. (1966). High-speed scanning in human memory. Science, 153(3736):652-654. Stewart, N., Chater, N., and Brown, G. D. (2006). Decision by sampling. Cognitive psychology, 53(1):1-26. Steyvers, M., Lee, M. D., and Wagenmakers, E.-J. (2009). A bayesian analysis of human decisionmaking on bandit problems. Journal of Mathematical Psychology, 53(3):168-179. Strens, M. (2000). A bayesian framework for reinforcement learning. In ICML, volume 2000, pages 943-950. Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press. Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294. Vul, E., Goodman, N., Griffiths, T. L., and Tenenbaum, J. B. (2014). One and done? optimal decisions from very few samples. Cognitive science, 38(4):599-637. Vulkan, N. (2000). An economists perspective on probability matching. Journal of economic surveys, 14(1):101-118. Watkins, C. J. C. H. (1989). Learning from delayed rewards. Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., and Cohen, J. D. (2014). Humans use directed and random exploration to solve the explore-exploit dilemma. Journal of Experimental Psychology: General, 143(6):2074.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Busse, L.,Ayaz, A., Dhruv, N. T., Katzner, S., Saleem, A. B., Schölvinck, M. L., Zaharia, A. D.,  and Carandini, M. (2011). The detection of visual contrast in the behaving mouse.Journal of Neuroscience, 31(31):11351-11361. Carandini, M. and Churchland, A. K. (2013). Probing perceptual decisions in rodents. Nature neuroscience, 16(7):824. Gold, J. I. and Ding, L. (2013). How mechanisms of perceptual decision-making affect the psychometric function. Progress in neurobiology, 103:98-114. Prins, N. and Kingdom, F. A. (2018). Applying the model-comparison approach to test specific research hypotheses in psychophysical research using the palamedes toolbox. Frontiers in psychology, 9.Rigoux, L.,Stephan, K. E., Friston, K. J., and Daunizeau, J. (2014). Bayesian model selection for group studiesrevisited. Neuroimage, 84:971-985.Schwarz, G. et al. (1978). Estimating the dimension of a model. The annals of statistics, 6(2):461-464.Wichmann, F.A. and Hill, N. J. (2001). The psychometric function: I. fitting, sampling, and goodness of fit. Perception &amp; psychophysics, 63(8):1293-1313.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S2 :</head><label>S2</label><figDesc>Intuition for the choice-curve derivation for n = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure S5 :</head><label>S5</label><figDesc>Correlation between theta and sigma is robust to modeling assumptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S6 :</head><label>S6</label><figDesc>Parameter recovery for five-parameter descriptive model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure S7 :</head><label>S7</label><figDesc>Parameter recovery for theta-sigma model shows that correlation between theta and sigma is not induced by model fitting procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S8 :</head><label>S8</label><figDesc>Model comparison for different versions of Deep Exploration model. Left Each row corresponds to a different model, each column to a different parameter. White indicates the absence of a parameter, grey the presence of a parameter, and black indicates cases in which the two lapse rates are constrained to be equal, i.e. 1 = 2 = . Middle Number of participants best fit by each model. Right Exceedance probability for each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure S9 :</head><label>S9</label><figDesc>Parameter recovery for the Deep Exploration model with lapse rate and explore kernel parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure S10 :</head><label>S10</label><figDesc>Simulated (blue) vs actual (red) choice curves for all participants in Experiment 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure S12 :</head><label>S12</label><figDesc>Deep exploration model of Horizon Task can account for the dependence of directed and random exploration on horizon, feedback and uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Parameter Description</cell><cell>Range</cell></row><row><cell>θ</cell><cell>Explore threshold</cell><cell>0 to 100</cell></row><row><cell>σ</cell><cell>Decision noise</cell><cell>0 to ∞</cell></row><row><cell>1</cell><cell>Lapse rate (exploit)</cell><cell>0 to 1</cell></row><row><cell>2</cell><cell>Lapse rate (explore)</cell><cell>0 to 1</cell></row><row><cell>β</cell><cell></cell><cell></cell></row></table><note>e Previous action (explore or exploit) weight −∞ to ∞ βa Previous action (left or right) weight −∞ to ∞ β cp</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Parameters in the θ-σ variant models</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Choice curves for n = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Choice curves for n = 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Approximate choice curves for general n . . . . . . . . . . . . . . . . . . . . . . . 5 Model comparison for descriptive model variants . . . . . . . . . . . . . . . . . . 8 4.2 Correlation between directed and random exploration is conserved across models . 8 4.3 The five parameter descriptive model . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.4 Parameter recovery for the five-parameter descriptive model . . . . . . . . . . . . 10 Model comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Parameter recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.3 Comparison between simulated and actual choice curves . . . . . . . . . . . . . . 12 The Horizon Task and its variants . . . . . . . . . . . . . . . . . . . . . . . . . . 16 6.1.1 Horizon Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 6.1.2 Feedback Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6.1.3 Parametric Uncertainty Task . . . . . . . . . . . . . . . . . . . . . . . . . 19 6.2 Optimal model for Horizon Task variants . . . . . . . . . . . . . . . . . . . . . . 20 6.3 Deep Exploration algorithm in the Horizon Task variants . . . . . . . . . . . . . . 22</figDesc><table><row><cell cols="2">Supplementary Material: Deep exploration as a unifying</cell></row><row><cell>account of explore-exploit behavior</cell><cell></cell></row><row><cell>Robert C. Wilson a,b, *  , Siyu Wang a , Hashem Sadeghiyeh c ,</cell><cell></cell></row><row><cell>&amp; Jonathan D. Cohen d,e</cell><cell></cell></row><row><cell>Contents</cell><cell></cell></row><row><cell>Optimal strategy for the Infinite Bandits Task</cell><cell>2</cell></row><row><cell>Deep Exploration algorithm for the Infinite Bandits Task</cell><cell>3</cell></row><row><cell>2.1 Evaluating performance of different strategies on the Infinite Bandits Task</cell><cell>7</cell></row><row><cell>Descriptive model</cell><cell>8</cell></row><row><cell>4.1 Fitting the Deep Exploration model to behavior</cell><cell>11</cell></row><row><cell>5.1 Deep exploration can account for horizon, uncertainty, and feedback dependence of</cell><cell></cell></row><row><cell>directed and random exploration</cell><cell>16</cell></row><row><cell>6.1</cell><cell></cell></row></table><note>6.3.1 Deep exploration can account for behavior in Horizon Task variants . . . . 23</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S1 :</head><label>S1</label><figDesc>ANOVA tables for the information bonus and decision noise in the Feedback Task 6.1.3 Parametric Uncertainty Task investigated the uncertainty dependence of directed and random exploration by having a wider variety of forced trials. Here we focus on a conceptual replication of that study. In this Parametric Uncertainty Task, we use up to six forced trials to create six uncertainty conditions, In this task the horizon is fixed at 5 across all games.29 participants (21 female, 8 male, age 18-42, mean 22.</figDesc><table><row><cell>three unequal uncertainty conditions ([1 2], [1 3], and [2 3]) and three equal uncertainty conditions</cell></row><row><cell>([1 1], [2 2], and [3 3]).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S2 :</head><label>S2</label><figDesc>ANOVA table for the information bonus and decision noise in the Parametric Uncertainty Task</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project was made possible through the support of National Institute on Aging grants R56AG061888 and R01AG061888 to RCW and a grant from the John Templeton Foundation to JDC. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the funders.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>RCW and JDC conceived the experiment. HS and SW acquired the data. RCW derived the theory and performed the analyses with help from SW and HS. RCW and JDC wrote the paper with input from SW and HS. deviation of the Gaussian distributions, σ n = 8. The model assumes that participants do not know the mean of the Gaussian distributions for option, i, µ i , but that they can infer it from the previously observed rewards, the 'data,' D t . If participants are good Bayesians then the inferred distribution over µ i is</p><p>where n i t is the number of times option i has been played, R i t is the cumulative sum of the rewards obtained from playing option i and p(µ i ) is the prior on the mean. For simplicity we assume a uniform prior for p(µ i ), although other forms, such as a Gaussian prior, are relatively straightforward to implement.</p><p>Equation S33 shows that the optimal model's state of knowledge about option i can be summarized by two sufficient statistics: the number of plays, n i t , and the cumulative rewards, R i t . This allows us to define the state of information of the optimal model as the number of plays and cumulative rewards from both options; i.e.,</p><p>This in turn defines a state space from which the optimal policy can be computed by solving a dynamic programming problem <ref type="bibr" target="#b11">(Bellman, 1956;</ref><ref type="bibr" target="#b12">Duff and Barto, 2002)</ref>. In particular, the optimal model takes the action that maximizes the expected future reward, Q(a|S t ); i.e.,</p><p>Following <ref type="bibr" target="#b11">(Bellman, 1956)</ref>, Q(a|S t ) satisfies</p><p>where, T (S t+1 |S t , a t ) is the transition matrix, denoting the probability of moving to state S t+1 given that action a t is taken in the current state S t , r t (S t+1 |S t ) denotes the reward obtained going into state S t+1 from S t , and</p><p>denotes the value of being in state S t+1 under the optimal policy. The transition matrix, T (S t+1 |S t , a t ), is computed by noting that if option a t is played and reward r t is observed, then the sufficient statistics for option a t update as</p><p>updating the state as</p><p>and that this state transition occurs with probability</p><p>Finally, to compute the values state and action values, V (S t ) and Q(a|S t ), we start at the horizon t = H where the action values are simply the mean of the posterior</p><p>and then iterate equations S36 and S37 backwards in time to compute the action values for the initial state Q(a|S 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Deep Exploration algorithm in the Horizon Task variants</head><p>The Deep Exploration model uses the same state space as the optimal model, but instead of considering all possible states, it only considers a small number visited during mental simulation. In order to approximate the action values for the two options in the initial state, for each initial action (a t = 1 or 2) the model simulates a sequence of states, actions, and rewards up to the time horizon H. For example, a simulation for choosing option 1 on the first trial would produce a trace of states, actions and rewards visited upto the time horizon; e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Output</head><p>The value of each simulation is simply the sum of the simulated rewards; e.g.</p><p>The model then aggregates these values to compute the two action values</p><p>where n is the number of simulations. The model then chooses the option with highest simulated value.</p><p>A critical component of the model is exactly how simulations play out -that is, how rewards and actions are sampled during the simulations. For the simulated rewards, for each simulation i, the model samples an underlying mean for each bandit, j, from the posterior distribution over mean given the outcomes of the forced trials; i.e.,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An experimental analysis of the bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Theory</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deconstructing the human algorithms for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="164" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dynamic allocation index for the discounted multiarmed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="561" to="565" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring mass in complex scenes by mental simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="61" to="76" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Think again? the amount of mental simulation tracks uncertainty in the outcome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<editor>CogSci. Citeseer</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Schor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Constant sub-second cycling between representations of possible futures in the hippocampus. bioRxiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">528976</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Test of optimal sampling by foraging great tits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5675</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intuitive physics: Current research and controversies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kubricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="749" to="759" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Psychological models of human and optimal performance in bandit problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="174" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A problem in the sequential design of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhyā: The Indian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Optimal Learning: Computational procedures for Bayesadaptive Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts at Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational noise in reward-guided learning drives behavioral variability in volatile environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Findling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Skvortsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dromnelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deconstructing the human algorithms for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sources of suboptimality in a minimalistic exploreexploit task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bnaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncertainty and exploration in a restless bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="367" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ten simple rules for the computational modeling of behavioral data. eLife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">49547</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Humans use directed and random exploration to solve the explore-exploit dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2074</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
