<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Algorithmic Discrimination Causes Less Moral Outrage than Human Discrimination 1 In press at Journal of Experimental Psychology: General Algorithmic Discrimination Causes Less Moral Outrage than Human Discrimination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochanan</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desman</forename><surname>Wilson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Kellogg School of Management</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mads</forename><forename type="middle">N</forename><surname>Arnestad</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">BI Norwegian Business School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Waytz</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Kellogg School of Management</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Gray</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Algorithmic Discrimination Causes Less Moral Outrage than Human Discrimination 1 In press at Journal of Experimental Psychology: General Algorithmic Discrimination Causes Less Moral Outrage than Human Discrimination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/xge0001250</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>moral outrage</term>
					<term>motivation attribution</term>
					<term>human-robot interaction</term>
					<term>discrimination</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Considering the ever-increasing cases of algorithm-driven discrimination, it is crucial to understand its psychological consequences. We synthesize work on the role of perceived motivation in moral judgment (e.g., Bigman &amp; Tamir, 2016; Carlson &amp; Zaki, 2018; Reeder et al., 2002) with work on machine morality (e.g., Bigman &amp; Gray, 2018) to predict an algorithmic outrage deficit-that discrimination at the &quot;hands&quot; of algorithms will incite less outrage because machines are seen to lack the motivation to be prejudiced. This work sheds light on the causes of moral outrage and reveals how people think about wrongdoing by artificial intelligence (AI).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discrimination often incites moral outrage <ref type="bibr" target="#b10">(Batson et al., 2007;</ref><ref type="bibr" target="#b114">Russell &amp; Giner-Sorolla, 2011;</ref><ref type="bibr" target="#b129">Spring et al., 2018;</ref><ref type="bibr" target="#b133">Sunstein et al., 1998;</ref><ref type="bibr" target="#b136">Tetlock, 2002)</ref>, especially when perpetrated by companies <ref type="bibr" target="#b61">(Halzack, 2019)</ref>-such as when Walmart was accused of systematically underpaying women and overlooking them for promotion <ref type="bibr" target="#b37">(Covert, 2019)</ref>. Ultimately, organizational discrimination is perpetrated by other people-those who control hiring, firing, and promotions.</p><p>However, the rise of artificial intelligence raises a new possibility: organizational discrimination can be perpetrated by algorithms. For example, Amazon developed a machine-learning-based algorithm to screen applicant resumes, but the algorithm turned out to discriminate against women, penalizing applicants whose resumes contained terms such as "women's chess club captain" or the names of women's colleges. The algorithm was soon scrapped amid outrage about its systematic gender discrimination, and, according to Amazon, was never actually used <ref type="bibr" target="#b40">(Dastin, 2018)</ref>.</p><p>Although the Walmart and the Amazon cases both involved systematic discrimination that elicited outrage, Amazon's algorithm-driven discrimination seemingly elicited less moral outrage than Walmart's human-driven discrimination. Here we explore whether people truly are more blasé when witnessing discrimination at the "hands" of an algorithm (vs. a human)-what we call an algorithmic outrage deficit. We also explore one potential reason for this asymmetry in outragebecause algorithms are perceived as lacking mind (compared to humans; <ref type="bibr" target="#b13">Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b130">Srinivasan &amp; Sarial-Abi, 2021)</ref> people may perceive the discriminatory decisions of algorithms as less motivated by prejudice-and therefore be less outraged at that discrimination.</p><p>We also examine a downstream consequence of the algorithmic outrage deficit-that people are less likely to find the company legally liable when it discriminated via algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Rise of Artificial Intelligence</head><p>The past decades have seen a rapid increase in the integration of autonomous machines and algorithms into human society. Increasingly, tasks that used to be performed by humans are performed by autonomous machines and algorithms, including assembly line work <ref type="bibr" target="#b79">(Levitin et al., 2006)</ref>, customer service <ref type="bibr" target="#b7">(Bares et al., 2007)</ref>, house cleaning <ref type="bibr" target="#b134">(Takeshita et al., 2006)</ref> and supermarket checkout <ref type="bibr" target="#b6">(Aquilina &amp; Saliba, 2019)</ref>. Machine-learning has facilitated much of this development, providing data-based predictions that often outperform humans across many areas, from predicting the spread of disease <ref type="bibr" target="#b30">(Chen et al., 2017)</ref> to the dynamics of crime <ref type="bibr" target="#b126">(Shapiro, 2017)</ref>. Algorithms have particularly revolutionized many practices in the business world, helping to manage inventory <ref type="bibr" target="#b22">(Cárdenas-Barrón et al., 2012)</ref>, distribution chains <ref type="bibr" target="#b143">(Validi et al., 2015)</ref>, and staff scheduling <ref type="bibr" target="#b20">(Cai &amp; Li, 2000)</ref>.</p><p>The predictive abilities of AI systems are undeniable, but many are uncomfortable with humanity's growing reliance on algorithms. One concern is whether the increased use of machines will take jobs away from humans-similar to other technological revolutions-causing widespread economic unrest <ref type="bibr" target="#b47">(Ford, 2015)</ref>. Although increased automation appears to increase inequality, which is a driver of social unrest, the rise of AI may also bring people together by emphasizing their shared humanity <ref type="bibr" target="#b67">(Jackson et al., 2020)</ref>. Another concern about the rise of algorithms is distrust in machines' decision-making capacities. Many legal, medical, and military decisions involve life or death outcomes, and people seem averse to machines making these weighty decisions, in part because machines lack the ability to feel emotions <ref type="bibr" target="#b16">(Bigman et al., 2019;</ref><ref type="bibr" target="#b13">Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b52">Gogoll &amp; Uhl, 2018;</ref><ref type="bibr" target="#b72">Kramer et al., 2018;</ref><ref type="bibr" target="#b154">Young &amp; Monroe, 2019)</ref>. Unlike people who care deeply about their family and friends, machines appear devoid of compassion, and people are hesitant to allow algorithms to make decisions about human lives because of their dispassionate impartiality.</p><p>Some may see the impartiality of machines as a disadvantage-at least in some situations <ref type="bibr" target="#b85">(Longoni et al., 2019)</ref>. Others argue that this impartiality holds the promise to free decisionmaking from human biases <ref type="bibr" target="#b100">(Mullainathan, 2019)</ref>. Basic propensities for prejudice can lead humans to discrimination across many domains, including hiring processes. Substantial research reveals that people are biased in their decisions about who to interview, hire, and promote. For example, in one large-scale study, researchers sent out equivalent resumes to employers that differed only by the name at the top-Greg Baker versus Jamal Jones-and employers responded to the white name 50% more than the black name <ref type="bibr" target="#b12">(Bertrand &amp; Mullainathan, 2004)</ref>.</p><p>In the face of this type of discrimination, companies are increasingly relying on algorithms to make hiring practices unbiased <ref type="bibr" target="#b65">(Heilweil, 2019)</ref>.</p><p>Unfortunately, the promise of unbiased decision-making is currently unfulfilled, as AIs often discriminate. We previously reviewed one high profile example of algorithm discrimination-the Amazon hiring algorithm-but there are many others, such as racial discrimination by algorithms used to make parole decisions <ref type="bibr" target="#b5">(Angwin et al., 2016)</ref> and healthcare decisions <ref type="bibr" target="#b107">(Obermeyer et al., 2019)</ref>, and gender discrimination by algorithms used to assign credit scores <ref type="bibr" target="#b132">(Stankiewicz, 2019)</ref> and to display career ads <ref type="bibr" target="#b76">(Lambrecht &amp; Tucker, 2019)</ref>. As these algorithms are usually intellectual properties of the companies and are not shared with the public, it is hard to know exactly what caused the discrimination, but there are several possibilities. One possibility is intentional programming, such as when a programmer designs the algorithm to give women a lower score than men. A second possibility is that the algorithm is trained to mimic existing human decisions, thereby showing the same bias as human decision-makers <ref type="bibr" target="#b32">(Cheong et al., 2020)</ref>. A third possibility is that bias creeps in when the algorithm is trained with existing data-and existing data typically reflects the outcomes of a biased decision process. For example, a hiring algorithm may penalize women applicants because they show higher turnover-but this higher turnover may be driven by a sexist work environment or lower rates of promotions for women.</p><p>Regardless of the ultimate cause of algorithm discrimination, it is important to understand how people respond to cases in which algorithms perpetrate racial and gender discrimination. To answer this question, we need to first understand how people respond to discrimination in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moral Outrage</head><p>Although discrimination is often widespread and institutionalized <ref type="bibr" target="#b48">(Fornili, 2018;</ref><ref type="bibr" target="#b51">Goel, 2018;</ref><ref type="bibr" target="#b95">Manuel et al., 2017)</ref>, salient cases of discrimination are typically seen as unfair. Human reactions to unfairness have deep roots in our evolutionary history and elicit moral outrage <ref type="bibr" target="#b10">(Batson et al., 2007;</ref><ref type="bibr" target="#b114">Russell &amp; Giner-Sorolla, 2011;</ref><ref type="bibr" target="#b129">Spring et al., 2018;</ref><ref type="bibr" target="#b133">Sunstein et al., 1998;</ref><ref type="bibr" target="#b136">Tetlock, 2002)</ref> 1 . Moral outrage can serve several important social functions. For example, moral outrage mobilizes people to punish unfair behavior <ref type="bibr" target="#b45">(Fiske &amp; Tetlock, 1997;</ref><ref type="bibr" target="#b59">Gummerum et al., 2016;</ref><ref type="bibr" target="#b101">Nelissen &amp; Zeelenberg, 2009;</ref><ref type="bibr" target="#b117">Salerno &amp; Peter-Hagene, 2013;</ref><ref type="bibr" target="#b129">Spring et al., 2018)</ref> which deters uncooperative behaviors <ref type="bibr" target="#b73">(Kurzban et al., 2007;</ref><ref type="bibr" target="#b152">Xiao &amp; Houser, 2005)</ref>. In addition, moral outrage can promote collective action <ref type="bibr" target="#b96">(Martin et al., 1984;</ref><ref type="bibr" target="#b98">Miller et al., 2011)</ref>. Indeed, some argue that moral outrage evolved in human society because it served the adaptive function of enforcing cooperation within groups <ref type="bibr" target="#b129">(Spring et al., 2018)</ref>. When companies discriminate, in addition to eliciting moral outrage, it demotivates employees <ref type="bibr" target="#b62">(Hausknecht et al., 2004)</ref>, increases turnover <ref type="bibr" target="#b138">(Uggerslev et al., 2012)</ref>, and even causes the public to boycott the discriminating company <ref type="bibr" target="#b82">(Lindenmeier et al., 2012)</ref>.</p><p>As with other aspects of human morality <ref type="bibr" target="#b87">(Machery &amp; Mallon, 2010)</ref>, responses of moral outrage evolved in social groups with other humans, which raises the question of whether the actions of nonhuman agents-like algorithms-would also generate outrage. One possibility is that discrimination by an algorithm could generate more outrage than discrimination perpetrated by a human. Algorithms are usually implemented to manage or transform entire large-scale operations, which means they can impact a very large number of people. Whereas a single hiring administrator could discriminate against potentially hundreds of applicants, an algorithm-with its limitless throughput-has the capacity to discriminate against thousands of applicants. As people weigh the impact (or potential impact) in their moral judgments <ref type="bibr" target="#b10">(Batson et al., 2007</ref><ref type="bibr" target="#b8">(Batson et al., , 2009</ref><ref type="bibr" target="#b60">Haidt, 2003;</ref><ref type="bibr" target="#b133">Sunstein et al., 1998;</ref><ref type="bibr" target="#b136">Tetlock, 2002)</ref>, the scalability of an algorithm could elicit substantial outrage.</p><p>The novelty of algorithms making hiring decisions may also elicit considerable moral outrage. Moral judgments are typically weaker for descriptively normative acts-acts that are frequent or typical <ref type="bibr" target="#b49">(Gawronski et al., 2017;</ref><ref type="bibr" target="#b90">Malle et al., 2014;</ref><ref type="bibr" target="#b99">Monroe &amp; Malle, 2017)</ref>. For example, if everyone evades taxes, tax evasion seems less morally wrong. The reason for this link between descriptive normativity and moral judgment is because people conflate descriptive norms (what is) with injunctive norms (what should be) <ref type="bibr" target="#b43">(Eriksson et al., 2015)</ref>. Acts that are less typical and less frequent, might therefore be more likely to evoke moral outrage. Given that algorithms are infrequently used to make hiring decisions (at least currently), people might be especially outraged when companies use them to perpetrate discrimination.</p><p>Although these factors suggest algorithms might generate more moral outrage than the actions of humans, here we suggest that algorithms might actually generate less moral outrage for similar discrimination. We label this prediction the algorithmic outrage deficit, and suggest it stems from people's perceptions of the mental states of those perpetrating discrimination as guiding moral judgments. Synthesizing the work emphasizing the role of perceived intentions <ref type="bibr" target="#b2">(Alicke, 2000;</ref><ref type="bibr" target="#b38">Cushman, 2008;</ref><ref type="bibr" target="#b90">Malle et al., 2014;</ref><ref type="bibr" target="#b91">Malle &amp; Knobe, 1997;</ref><ref type="bibr" target="#b111">Pizarro &amp; Tannenbaum, 2011)</ref> and perceived motivation <ref type="bibr" target="#b15">(Bigman &amp; Tamir, 2016;</ref><ref type="bibr" target="#b24">Carlson et al., 2022;</ref><ref type="bibr" target="#b25">Carlson &amp; Zaki, 2018;</ref><ref type="bibr" target="#b77">Levine &amp; Schweitzer, 2014;</ref><ref type="bibr" target="#b112">Reeder et al., 2002)</ref> with the work showing that people ascribe different mental states to algorithms and robots <ref type="bibr" target="#b57">(Gray &amp; Wegner, 2012;</ref><ref type="bibr" target="#b81">Li et al., 2016;</ref><ref type="bibr" target="#b147">Waytz et al., 2014;</ref><ref type="bibr" target="#b154">Young &amp; Monroe, 2019)</ref>, we propose that as people are less likely to attribute negative motivations (i.e., prejudice) to an algorithm, they would be less outraged when algorithms discriminate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>When someone acts immorally, observers often ask "Why?"-why did the agent behave the way they did? These questions boil down to perceived motivationwhat drove the person to act the way they did. Understanding the motivations underlying a person's behavior allows us to make sense of the social world, predict people's future behavior and guides the way we interact with them <ref type="bibr" target="#b35">(Cosmides, 1989;</ref><ref type="bibr" target="#b148">Waytz, Morewedge, et al., 2010)</ref>. Moral psychology now emphasizes the importance of person-centered moral judgment (i.e., character; <ref type="bibr" target="#b54">Goodwin et al., 2014;</ref><ref type="bibr" target="#b111">Pizarro &amp; Tannenbaum, 2011;</ref><ref type="bibr" target="#b142">Uhlmann et al., 2013</ref><ref type="bibr" target="#b141">Uhlmann et al., , 2014</ref><ref type="bibr" target="#b139">Uhlmann et al., , 2015</ref>, and nothing is more central to judging a person's morality than their motivations <ref type="bibr" target="#b15">(Bigman &amp; Tamir, 2016;</ref><ref type="bibr" target="#b24">Carlson et al., 2022;</ref><ref type="bibr" target="#b77">Levine &amp; Schweitzer, 2014;</ref><ref type="bibr" target="#b112">Reeder et al., 2002)</ref>. Consider the trolley problem, in which a person decides whether or not to actively kill one person to save five others <ref type="bibr" target="#b46">(Foot, 1967)</ref>. A person who decides to kill one person because they are motivated to save the five people will be judged more positively than a person who did the same action, but did the action because they wanted to push someone to their death <ref type="bibr" target="#b69">(Kahane et al., 2018)</ref>. We propose that the motivation attributed to a wrongdoer will affect moral outrage.</p><p>The motivation that people attribute to others might be especially important in reactions to hiring decisions because there are many possible reasons why one candidate might be preferred over another. Hiring involves weighing many different candidate features, such as education, experience, skills, and general "fit" <ref type="bibr" target="#b19">(Bowen et al., 2011)</ref>. When a white person or a male candidate is hired over a person of color or a female candidate, there is ambiguity surrounding whether that decision reflects prejudice (e.g., sexism, racism) or a perceived difference in qualifications. Indeed, the complexity of hiring decisions is one of the reasons why it is hard to prove hiring discrimination in the court of law <ref type="bibr" target="#b71">(Kotkin, 2009)</ref>. The attributional ambiguity of hiring decisions means that general cues about the existence (or lack) of prejudiced motivation could impact people's moral reactions. If a decision-maker seems unlikely-or unable-to harbor ill-will towards a social group, their biased decisions may elicit less outrage.</p><p>For example, when a woman fails to hire a qualified woman, people may be less likely to perceive discrimination because they assume that other women are not prejudiced against women.</p><p>People may assume that algorithms are incapable of harboring prejudice. Research shows that people perceive the mind of artificial agents such as algorithms differently than the mind of humans <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b57">Gray &amp; Wegner, 2012;</ref><ref type="bibr" target="#b92">Malle et al., 2016;</ref><ref type="bibr" target="#b151">Weisman et al., 2017</ref>). They are seen as being less able to think rationally and plan their actions, and especially less able to experience emotions <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b56">Gray et al., 2007;</ref><ref type="bibr" target="#b57">Gray &amp; Wegner, 2012)</ref>. The way people perceive algorithms affects how much people trust them <ref type="bibr" target="#b52">(Gogoll &amp; Uhl, 2018)</ref>, blame them <ref type="bibr" target="#b92">(Malle et al., 2016;</ref><ref type="bibr" target="#b124">Shank &amp; DeSanti, 2018;</ref><ref type="bibr" target="#b130">Srinivasan &amp; Sarial-Abi, 2021)</ref>, and want them to make decisions <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b154">Young &amp; Monroe, 2019)</ref>. Based on this research, we suggest that seeing less mind in machines may contribute to an algorithmic outrage deficit. Humans are perceived as having a full mind-capable of both intention and antipathy-and therefore when they discriminate, they are likely to be seen as motivated by prejudice. Because algorithms are perceived as lacking a full mind, people should be less likely to attribute their behavior to a prejudiced motivation, which should then make observers less outraged when algorithms perpetrate discrimination. We test the algorithmic outrage deficit in the studies described below.</p><p>Importantly, although we predict that people will not perceive algorithms per se as prejudiced, they might attribute such a prejudiced motivation to the people who created and trained the algorithm. We also test this possibility, examining whether participants perceive more prejudiced motivation in a discriminatory algorithm when it is programmed by a software company known for sexist work conditions. We also test the positive counterpart of algorithm discrimination-how would people respond when algorithms actually help reduce discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Research</head><p>We systematically investigate how much people are morally outraged by algorithmic (vs. human) discrimination through 8 studies with diverse methods and samples, including American online samples, a nationally representative sample of the UK, and a sample of employees at Norwegian technology firms. In Study 1 we tested whether people perceived algorithms as less motivated by prejudice than humans, and Study 2 tested whether this difference in motivation perception leads people to be less outraged by algorithmic discrimination. Study 3 examined how algorithm discrimination affects moral outrage towards the company that used the algorithm.</p><p>Study 4 investigated the positive counterpart of algorithm discrimination by examining people's judgments of a company that used an algorithm that reduced gender inequality. Study 5 sought to replicate our findings in a sample of professionals in the technology industry, which allowed us to test whether knowledge about AI moderated outrage toward algorithmic discrimination. Study 6 examined whether the algorithmic outrage deficit was moderated by the identity of the programmers (e.g., if they seemed prejudiced), and Study 7 examined the impact of algorithm anthropomorphism on moral outrage at algorithm discrimination. Finally, in Study 8 we tested another downstream consequence of the reduced attribution of prejudiced motivation to algorithms-judgments of legal liability, such that companies may be held less liable for algorithmic vs human discrimination.</p><p>Across these eight studies, we predicted that in cases of discrimination, people will attribute less prejudiced motivation to an algorithm (vs. a human) and that this reduced attribution will lead to less moral outrage. All studies were approved by the IRB of The University of North Carolina at Chapel Hill. We pre-registered all studies except for Study 5.</p><p>Full study materials and data can be found at https://osf.io/87yu5/?view_only=36fc6f1a3e004665a1e916be5fd180db. For all studies, we report all measures, conditions, data exclusions, and how we determined our sample sizes, acknowledging that variation in sample size occurred for procedural and theoretical reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1: Perceived Prejudiced Motivation</head><p>In Study 1 we tested whether people are less likely to attribute prejudiced motivation to an algorithm vs. a human. We also tested whether people see algorithms as more objective than humans, and whether people see the same action as less discriminatory when it was perpetrated by an algorithm. Participants read about a human HR specialist or an algorithm that discriminated against women in hiring decisions, based on the real story of the algorithm that Amazon used for candidate selection <ref type="bibr" target="#b40">(Dastin, 2018)</ref>. Following the Amazon case, we described the discriminator as involved in the initial stage of the hiring process-scanning resumes and giving each a rating of between one (lowest) and 5 (highest) stars. We then asked participants to rate the prejudiced motivation of the agent, and how objective and discriminatory the decisionmaking was. We predicted that participants would attribute less prejudiced motivation to the algorithm, and perceive its actions as both less discriminatory and more objective than identical actions performed by a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>Two hundred and forty participants (116 male, 120 female, 4 other or declined to respond; age: M = 32.92, SD = 10.83) from the United States completed the study on Prolific in exchange for 50 cents. Accounting for participants who might fail attention checks and therefore be excluded from the analysis, this sample size gives us a power of 0.95 to detect a two-tailed medium effect size (Cohen's d = 0.5, calculated with G*Power 3.1.9.2). As specified in the preregistration (https://aspredicted.org/e2dq8.pdf), we did not include in the analysis participants who failed to answer either the attention check or comprehension question correctly, leading to the exclusion of 10 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure.</head><p>Participants were randomly assigned to an Algorithm or a human condition. All participants read the following:</p><p>AeonTech is a high-tech company that creates software to provide interconnectivity between cloud-based enterprise systems.</p><p>An external audit found that despite receiving large numbers of applications from women for software engineering positions, AeonTech hires almost no women.</p><p>An external audit found the reason for AeonTech's gender discrimination. AeonTech has a two-phased process for hiring software developers. The second stage involves a standard review by a committee of executives, but this committee only receives applications that have been passed forward at the first stage.</p><p>Participants in the Algorithm condition read the following (Human condition in parentheses):</p><p>At the first stage, an AeonTech algorithm-SigmaEvalu8, an unsupervised self-learning AI system (an AeonTech HR specialist) scans each application and gives it a rating between one star (lowest fit) and five stars (highest fit). Applicants with the highest ratings are then forwarded to the hiring committee. This self-learning algorithm (HR specialist) systematically gave women a lower star rating than men.</p><p>Participants then rated the following dependent variables on a 0 (strongly disagree) to 100 (strongly agree) slider scale.</p><p>Assessing perceived discrimination. We assessed perceived discrimination with three items: "SigmaEvalu8 (The HR specialist) discriminated against women", "SigmaEvalu8 (The HR specialist) treated people differently according to their gender" and "SigmaEvalu8 (The HR specialist) treated men and women differently". We created a perceived discrimination index by averaging all three items, Cronbach's α = .96.</p><p>Assessing perceptions of objectivity. We assessed perceptions of objectivity with three items: "SigmaEvalu8 (The HR specialist) is data-driven", "SigmaEvalu8 (The HR specialist) relies on facts", and "SigmaEvalu8 (The HR specialist) is unaffected by personal opinions". We created a perceived data-driven behavior index by averaging all three items, Cronbach's α = .87.</p><p>Assessing perceived prejudiced motivation. We then assessed perceived prejudiced motivation with four items: SigmaEvalu8 (The HR specialist) is sexist", "SigmaEvalu8 (The HR specialist) does not want to hire women for high-skilled jobs", "SigmaEvalu8 (The HR specialist) dislikes women", and "SigmaEvalu8 (The HR specialist) is prejudiced". We created a composite perceived prejudiced motivation index by averaging all four items, Cronbach's α = .91.</p><p>As a comprehension question, we asked participants which of the following best describes the hiring practices at AeonTech: "A hiring committee decided who to hire", "An HR specialist gives each application an initial rating and the hiring committee decides who to hire from the top-rated applicants" or "An algorithm gives each application an initial rating and the hiring committee decides who to hire from the top-rated applicants". Finally, participants provided demographic information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Replication</head><p>These results suggest that for the same action, people see algorithms as less motivated by prejudice than humans. In another study (see "Study 1: Replication" in the supplemental materials; final N=216) we replicated this finding with a different paradigm in which participants read about discrimination by an algorithm or a human and wrote what they thought the reason for the discrimination was. Two independent coders rated whether participants attributed the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating</head><p>Algorithm Human discriminatory behavior to data or a prejudiced motivation. The results of this replication study were consistent with our findings in Study 1, participants attributed less prejudice to the human discriminator than the algorithm discriminator t(214) = 5.38, p &lt; .001, Cohen's d = 0.74. Not only did people perceive algorithms as less motivated by prejudice than humans, Study 1 revealed that people see algorithms as more objective than humans, and see their actions as less discriminatory than the same actions performed by humans. Given that people perceive discrimination to the extent they perceive prejudice <ref type="bibr">(Major et al., 2003)</ref> Study 1 supports the first part of our theory: people see algorithms as less motivated by prejudice than humans. In Study 2</p><p>we tested our full model by assessing perceived prejudice motivation directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: Algorithmic Outrage Deficit</head><p>In Study 2 we tested our algorithmic outrage deficit hypothesis, examining whether people are less outraged when an algorithm discriminates than when a human discriminates. We also tested whether this effect is mediated by people perceiving algorithms as less motivated by prejudice than humans. Participants read the same story as in Study 1 and reported their moral outrage and the prejudiced motivation they attributed to the agent. We predicted that we would find an algorithmic outrage deficit, such that people will be less outraged when the discrimination was done by an algorithm than when it was done by a human. We further predicted that perceived prejudiced motivation would mediate the algorithmic outrage deficit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Eight hundred and four participants 2 (300 male, 486 female, 18 other or preferred not to disclose; age: M = 32.52, SD = 11.54) from the United States completed the study on Prolific in exchange for 30 cents. Accounting for participants who might fail attention checks and therefore be excluded from the analysis, this sample size gives us a power of 0.80 to detect a two-tailed small effect size (Cohen's d = 0.2, calculated with G*Power 3.1.9.2). As specified in the preregistration (https://aspredicted.org/7e7e3.pdf), we did not include in the analysis participants who failed the attention check, leading to the exclusion of thirty-four participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>All participants first read the following, a slightly modified version of the vignette we used in Study 1:</p><p>AeonTech is a high-tech company that creates software to provide interconnectivity between cloud-based enterprise systems. An external audit found some discrimination in AeonTech's hiring practices. Despite receiving large numbers of applications from women for software engineering positions, AeonTech hires almost no women.</p><p>An external audit found the reason for AeonTech's gender discrimination. AeonTech has a two-phased process for hiring software developers. The second stage involves a standard review by a committee of executives, but this committee only receives applications that have been passed forward at the first stage.</p><p>Participants were then randomly assigned to either a Human or an Algorithm condition.</p><p>In the Algorithm condition participants read the following (Human condition in parentheses):</p><p>At the first stage, an AeonTech algorithm -SigmanEvalu8, an unsupervised self-learning AI system (an AeonTech HR specialist) scans each application and gives it a rating between one star (lowest fit) and five stars (highest fit). Applicants with the highest ratings are then forwarded to the hiring committee. This self-learning algorithm (HR specialist) systematically gave women a lower star rating than men.</p><p>We first measured the prejudiced motivation participants perceived the human/algorithm had, using the same items as in Study 1, Cronbach's α = .92. We then measured participants' moral outrage.</p><p>Measuring Moral Outrage. We used items that focused on the actions, rather than the agent, because participants might think that the algorithm itself is not a viable target of moral outrage. Due to the discussion about the role of disgust in moral outrage <ref type="bibr" target="#b114">(Russell &amp; Giner-Sorolla, 2011</ref>; Salerno &amp; Peter-Hagene, 2013) we included one item that measures disgust, as well as items measuring anger and outrage. Specifically, we asked participants to rate their agreement on a 0 (strongly disagree) to 100 (strongly agree) slider, with the following three items (adapted from Russell &amp; Giner-Sorolla, 2011): "I am angry at the HR specialist's/the algorithm's discriminatory actions", " I am outraged by the HR specialist's/the algorithm's discriminatory actions" and "I am disgusted by the HR specialist's/the algorithm's discriminatory actions". We created a moral outrage index by averaging these three items,</p><p>Cronbach's α = .96. To control for intention, we then measured perceived intentionality by asking participants to rate their agreement with the following item: "The HR specialist/the algorithm intended not to hire women".</p><p>Finally, participants answered the same attention check as in Study 1 and provided demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>An independent samples t-test revealed that, as predicted, participants perceived the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mediation</head><p>To test whether perceived prejudiced motivation mediated the effect of agent on moral outrage, we performed a bootstrapping mediation analysis <ref type="bibr">(Preacher &amp; Hayes, 2008</ref>; 5000 iterations, model 4) with agent as the IV, coding the algorithm condition as 1 and the human condition as -1, moral outrage as a DV, and perceived prejudiced motivation as a mediator.</p><p>As predicted, the effect of agent on moral outrage, b = -6.15, SE = 1.06, CI.95 <ref type="bibr">[-8.22, -4</ref>.07] was mediated by an indirect effect of perceived prejudiced motivation, b = -6.55, SE = 0.81, CI.95 <ref type="bibr">[-8.13, -4</ref>.95], see <ref type="figure" target="#fig_4">Figure 2</ref>. When accounting for the mediation by perceived prejudiced motivation, the direct effect of agent on moral outrage was not significant, b = 0.40, SE = 0.70, CI.95 <ref type="bibr">[-0.98, 1.78]</ref>. The results of the mediation analysis remain significant when controlling for intention 3 . The reduced moral outrage for algorithm discrimination appears to be driven by people attributing less of a prejudiced motivation to the algorithm (vs. the human). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>3 An exploratory mediation analysis testing for motivation and intention as mediators revealed that both motivation, b=-5.29, SE = 0.72, CI.95 <ref type="bibr">[-6.74, -3</ref>.91], and intention, b =-2.07, SE = 0.47, CI.95 <ref type="bibr">[-3</ref>.04, -1.22], were significant mediators. Although motivation is a significant mediator even when accounting for intention, the results suggest that perceived intentions also might contribute to the algorithm outrage deficit. = 0.8</p><p>Indirect Effect: = .</p><p>Study 2 supported our model. The results show that people are less morally outraged by a discriminatory algorithm than a discriminatory human, establishing an algorithmic outrage deficit. We further found that the algorithmic outrage deficit is statistically mediated by perceiving less prejudicial motivation in algorithms, an effect that remains robust even when controlling for perceived intentionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replications</head><p>We replicated these findings in five additional studies, reported in full in the supplemental materials. Four (three pre-registered) studies tested the effect of algorithm (vs.</p><p>human) discrimination on moral outrage, and the fifth (pre-registered) study tested mediation by perceived prejudiced motivation (see "Study 2: Replications A-E" in the supplemental materials). We report the studies in the supplementary materials rather than the main text, because their methodology leaves open more ambiguities than the current Study 2. First, in these replication studies, the agent (the human HR specialist or the algorithm) actually makes the hiring decision rather than just screening the applicant (as in Study 2). Second, we measured moral outrage with a more general set of items, asking participants how morally outraged they were, how unjust they thought the actions were, how immoral they thought the actions were and how wrong they thought they were, all on a 1 to 7 scale. The results of these replication studies are presented in <ref type="figure" target="#fig_7">Figure 3</ref>.</p><p>We found support for the algorithmic outrage deficit in all studies.  In Study 2 we examined how algorithmic discrimination affects outrage at the discrimination itself. However, beyond general outrage at the event, people might be outraged at the company that used the discriminatory algorithm (or human). In Study 3 we expanded our investigation and examined how algorithm discrimination affects judgment of the company that used the algorithm for decision-making. Focusing on the company required us to take into account that people might not want algorithms to make such decisions <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018)</ref> or view algorithm decision-making in HR decisions as generally unfair <ref type="bibr" target="#b102">(Newman et al., 2020)</ref>.</p><p>Therefore, people might be more outraged initially at companies for using algorithms for hiring decisions, even if the increase in outrage at discrimination by algorithms is smaller than the increase in outrage at discrimination by humans. To account for this possibility, we measured moral outrage twice. The first time after telling participants that an algorithm or a human was used to screen resumes in the company, and the second time after telling them that the algorithm or the human discriminated against women. We predicted that people will be initially more outraged at the company that used an algorithm for screening resumes than the company that used a human HR specialist to screen resumes, but that the increase in outrage due to discrimination would be smaller for the company that used an algorithm. We measured how permissible people thought it was for the algorithm to make these decisions <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018</ref>) and perceptions of the algorithm's fairness <ref type="bibr" target="#b102">(Newman et al., 2020)</ref> as well to test if these documented effects might affect people's moral outrage at the company for the use of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Four hundred and eighty-two participants (222 male, 253 female, 7 other or declined to respond; age: M = 31.26, SD = 10.44) from the United States completed the study on Prolific in exchange for 60 cents. Accounting for participants who might fail attention checks and therefore be excluded from the analysis, this sample size gives us a power of 0.95 to detect a two-tailed small to medium effect size (Cohen's d = 0.35, calculated with G*Power 3.1.9.2). As specified in the pre-registration (https://aspredicted.org/fc2ni.pdf), we did not include in the analysis participants who failed to answer either the attention check or comprehension question correctly, leading to the exclusion of 24 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The procedure had four stages. First, participants read about the agent (algorithm/human) screening resumes at a company. Second, participants reported their moral outrage. Third, participants read about discrimination via the agent. Fourth, participants reported their moral outrage at the company again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent introduction</head><p>Participants were randomly assigned to an Algorithm or a Human condition. In the Algorithm condition participants first read the following (Human condition in parentheses):</p><p>AeonTech is a high-tech company that creates software to provide interconnectivity between cloud-based enterprise systems.</p><p>AeonTech has a two-phased process for hiring software developers.</p><p>In the first phase, an algorithm-SigmaEvalu8, an unsupervised Bayesian AI system (an HR specialist) scans each application. Each application gets between one star (lowest fit) and five stars (highest fit), according to the initial evaluation of the algorithm (HR specialist). Applicants with the highest scores are then forwarded to the hiring committee.</p><p>At the second phase, a hiring committee of software engineers and senior executives goes over the applications that got the highest initial scores, conducts interviews and decides who to hire.</p><p>Participants then completed the following two measures (all items were answered on a 0 (strongly disagree) to 100 (strongly agree) slider scale).</p><p>Measurement of judgments pre-discrimination.</p><p>Outrage. We measured participants initial moral outrage at the company by asking them to rate their agreement with three items: "I am angry at AeonTech's hiring practices", "I am outraged by AeonTech's hiring practices" and "I am disgusted by AeonTech's hiring practices".</p><p>We created an index of moral outrage at the company by averaging all three items (Cronbach's α = .93).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Permissibility.</head><p>We then measured how permissible they thought it was for the agent to screen resumes by asking them to rate their agreement with the following three items (adapted from Bigman &amp; Gray, 2018 : "It is appropriate for SigmaEvalu8 (the HR specialist) to screen resumes", "SigmaEvalu8 (The HR specialist) should be the one to screen resumes" and "SigmaEvalu8 (The HR specialist) should be forbidden from screening resumes" (reversed scored). We created an index of permissibility by averaging all three items (Cronbach's α = .82).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrimination manipulation</head><p>After measuring pre-outcome moral outrage and permissibility, we told participants about the discriminatory outcome. Participants in the Algorithm condition read the following (Human condition in parentheses):</p><p>Despite receiving large numbers of applications from women for software engineering positions, AeonTech has hired almost no women. The reason for this has to do with the initial screening. AeonTech's algorithm, SigmaEvlau8H, (HR specialist) systematically gave women a lower star rating than men. Therefore, women almost never made it to the second phase, and were not in the final list that the hiring committee considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measurement of judgments post-discrimination</head><p>Outrage, perceived motivation. After reading about the discrimination, we measured participants' moral outrage at the company again, using the same items as they did in the first measurement (Cronbach's α = .97), and perceived prejudiced motivation, using the same items as in Studies 1-2 (Cronbach's α = .93).</p><p>Fairness. We then measured how fair participants thought the company's hiring process was, by asking participants to rate their agreement with the following four items (adapted from <ref type="bibr" target="#b102">Newman et al., 2020)</ref>: "The way AeonTech decides who to hire seems fair", "AeonTech's process for deciding who to hire was fair", "The decision who to hire was fair" and "The outcome of the hiring process at AeonTech was fair" (Cronbach's α = .94).</p><p>As an exploratory measure, we then measured participants' beliefs about men being better than women in math with the following item: "Statistically, men are better at math than women". Finally, participants completed the same attention check as in Studies 1-2 and provided demographic information.  <ref type="bibr" target="#b102">(Newman et al., 2020)</ref>. This result is likely because we measured fairness after participants learned about the discriminatory outcome while previous work examined perceptions of fairness independently of discrimination. A fourth independent samples t-test did not find a difference in participants agreement that statistically men might be better than women at math when an algorithm discriminated against women (M = 19.81, SD = 25.93) than when a HR specialist discriminated against women (M = 19.67, SD = 24.36), t (443) = 0.057, p =.955.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation, permissibility, fairness and math ability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moral Outrage</head><p>If discrimination by an algorithm causes less moral outrage at the company than discrimination by a human, we would expect a smaller increase in moral outrage following algorithm (vs. human) discrimination. We tested this in a 2 (agent: human, algorithm) x 2 (time:</p><p>pre-discrimination, post-discrimination) mixed-model ANOVA predicting moral outrage at the company. We found a main effect for time, such that participants were more morally outraged after hearing about the discrimination (M = 62.35, SD = 21.78) than before hearing about the discrimination (M = 17.58, SD = 21.78), F(1, 456) = 827.36, p &lt; .001, partial η 2 = .645, suggesting that across conditions, discrimination increased moral outrage. We also found a main effect for agent, such that participants were more outraged at the company when it used an algorithm to screen applicants (M = 44.03, SD = 20.96) than when it used an HR specialist to screen applicants (M = 35.79, SD = 21.29), F(1, 456) = 17.46, p &lt; .001, partial η 2 = .037.</p><p>However, this effect was qualified by a significant time x agent interaction, F(1, 456) = 6.27, p = .013, partial η 2 = .014. Follow-up pair-wise comparisons revealed that while pre-discrimination participants were more outraged at the company when it used algorithms for screening applicants (M = 23.58, SD = 23.50) than when it used an HR specialist (M = 11.42, SD = 17.93), F(1, 456) = 38.56, p &lt; .001, partial η 2 = .078, post-discrimination the difference between algorithm discrimination (M = 64.50, SD = 29.41) and human discrimination (M = 60.15, SD = 34.03) was not significant, F(1, 456) = 2.14, p = .144. These results suggest that while participants were initially more outraged at the company for using an algorithm, the increase in moral outrage at the company following discrimination by an algorithm was smaller than the increase following discrimination by a human. We note that post-discrimination, the difference between outrage at the company that used an algorithm and the company that used a human is not significant, suggesting a possible boundary for algorithm outrage deficit. Anchoring people on their preexisting outrage toward the use of algorithms in hiring might have produced outrage equivalent to human-based discrimination after discrimination has occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mediation by perceived prejudiced motivation</head><p>We conducted a mediation analysis to test whether perceived prejudice mediated the reduced increase in outrage following discrimination by an algorithm (vs. human). We used the appears, therefore, to be driven by people attributing less of a prejudiced motivation to the algorithm (vs. the human), consistent with our findings from Study 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results of Study 3 paint a nuanced picture of the effect of algorithm discrimination on moral outrage towards the company that used the algorithm. Participants were initially more morally outraged at the company for using algorithms for such decisions. However, consistent with our theory and our findings from Study 2, the increase in moral outrage following discrimination was smaller for discrimination by an algorithm than for discrimination by a human, albeit the difference in outrage at the company post-discrimination was not significant.</p><p>Consistent with our previous findings, the reduced increase in moral outrage following algorithm (vs. human) discrimination was mediated by people attributing less prejudiced motivation to a discriminatory algorithm than a discriminatory human. One limitation of Study 3 is that although we measured outrage at the company using the same items (e.g., "I am outraged by AeonTech's hiring practices" , these items might have a different meaning in different contexts. Before reading about the discrimination, participants may have focused on the fact that AeonTech was using an algorithm for hiring decisions (and not general outrage at the company). After reading about the discrimination, participants maybe focused on the fact that AeonTech was discriminatory. Accordingly, there is some ambiguity about the exact meaning of the differences in outrage scores between pre-and post-discrimination.</p><p>In Studies 1-3 we focused on people's responses to algorithms that discriminate against women, a group that has faced discrimination within the technology sector. In Study 4 we examine people's reactions to algorithms that favor women.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 4: Positive and Negative Outcomes</head><p>In Study 4 we expanded our investigation to cases in which an algorithm (vs. a human)</p><p>behaves in a way that people evaluate positively. According to our theory, if perceived motivation affects reactions to algorithm behavior, then reactions to an algorithm that acts positively (i.e., increases rather than decreasing gender equality) will not be judged as positively as a person-who presumably acts upon a motivation to do good. In other words, not only is there an algorithmic outrage deficit but also an algorithm praise deficit.</p><p>To test this idea, participants were randomly assigned to read about companies that either discriminated against women or had gender equality. Each participant read about two companies.</p><p>In one company the source for this was a human HR specialist who screened resumes, and in the other company the source was an algorithm that screened resumes. Because moral outrage does not have a positive equivalent, we measured participants' broad positive and negative evaluations of the companies. We predicted that companies that used algorithms would be judged less extremely in both cases: less negatively when promoting gender inequality and less positively promoting gender equality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Two hundred and forty-one participants (128 male, 113 female; age: M = 35.12, SD = 10.51) from the United States completed the study on Prolific in exchange for 60 cents.</p><p>Accounting for participants who might fail attention checks and therefore be excluded from the analysis, this sample size gives us a power of 0.95 to detect a medium effect size (Cohen's d = 0.5, calculated with G*Power 3.1.9.2). As specified in the pre-registration (https://aspredicted.org/zr7m8.pdf), we did not include in the analysis participants who failed to answer either of the attention checks correctly, leading to the exclusion of 30 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>All participants first read a description of two high-tech companies, one using a human HR specialist for screening applicants and the other, an algorithm. Participants were randomly assigned to either a discrimination or an equality condition. In the discrimination condition participants read the following (equality condition in parentheses):</p><p>AeonTech and CompSolutions are high-tech companies. Both companies hire a significantly lower (higher) percentage of women than the average at high-tech companies. They are on the bottom (top) 5% of high-tech companies in regards to gender equality. 95% of high-tech companies hire a larger (smaller) proportion of women than AeonTech and CompSolutions do. A series of external audits revealed why. At AeonTech, Sigma-Evalu8, a machinelearning-based algorithm was used to rate applicants. The algorithm systematically evaluated women more negatively (positively) than it evaluated men. At CompSolutions, a human HR specialist was used to rate applicants. The HR specialist systematically evaluated women more negatively (positively) than it evaluated men.</p><p>Assessing positive and negative evaluations of the companies. We then measured participants' positive and negative evaluations of each company by asking participants to rate their agreement with the following six statements on a 0 (strongly disagree) to 100 (strongly agree) slider for each company (positive evaluation in parentheses, company order was randomized): "AeonTech/CompSolutions deserves blame (praise for its hiring decisions", "AeonTech/CompSolutions should be punished (rewarded for its hiring decisions" and "I am angry at (happy with AeonTech's/CompSolutions' hiring decisions". We created indices of negative moral evaluations by averaging the three negative items (Cronbach's αs &gt; .89), and for positive moral evaluations by averaging the three positive items (Cronbach's αs &gt; .92).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manipulation and attention checks.</head><p>To test whether participants saw both companies as more moral in the high gender equality and as less moral in the low gender equality condition, we asked participants to rate their agreement with the following two items on a 0 (strongly disagree to 100 (strongly agree slider: "AeonTech and CompSolutions hiring decisions are moral" and "AeonTech and CompSolutions hiring decisions are immoral". We then asked participants two attention checks. First, whether AeonTech used an algorithm and CompSolutions a human HR specialist or vis versa. Second, whether AeonTech and</p><p>CompSolutions hired more or less women than other high-tech companies. Finally, participants provided demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manipulation check</head><p>A mixed-model 2 (evaluation type: positive, negative; within-subject) x 2 (condition:</p><p>discrimination, equality; between-subject) ANOVA revealed that the companies were evaluated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of company</head><p>We conducted a 2 (outcome: discrimination, equality; between-participant) x 2 (agent: algorithm, human; within-participant) x 2 (evaluation: positive, negative; within-participant)</p><p>mixed model ANOVA to test our hypothesis that people would evaluate discrimination by an algorithm (vs. a human) less negatively, and also see equality-supporting decisions by an algorithm (vs. a human) less positively.</p><p>As expected, we found a significant three-way interaction,  .394, and type of evaluation x agent interaction, partial η 2 = .022. As these analyses are not important for the current investigation, we do not elaborate on them (full data is available at OSF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results of Study 4 show that the use of algorithms for screening applicants has a mitigating effect on how people evaluate the company. People evaluated the company less negatively when the algorithm screening resulted in low gender equality but also evaluate the company less positively when the algorithm screening resulted in high gender equality. This pattern is consistent with our theoretical framework. Since perceived motivation affects judgment, the acts of ostensibly motivationless algorithms affect judgments of the company less than the acts of motivated humans.</p><p>In Study 3 we did not find a difference in outrage post-discrimination between algorithm and human discrimination, while in Study 4 we did find a significant difference in negative evaluation. One possible reason for this apparent discrepancy is that in Study 3 we measured moral outrage, and in Study 4 general evaluations, such as blame and punishment, and these different measures might differ in how sensitive they are to the motivation of the agent. Another possibility is that the joint evaluation in Study 4 might make the difference between the algorithm and the human more salient <ref type="bibr" target="#b66">(Hsee et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Studies 5-7: Potential Moderators of the Algorithmic outrage deficit</head><p>After finding that people are less morally outraged by algorithm discrimination and that algorithm discrimination causes less moral outrage at the company that uses the algorithm, we turn to explore possible moderators. In Study 5 we examine whether knowledge of AI moderates the effect of discrimination by algorithm on moral outrage (with professional technology workers). In Study 6 we examine whether the identity of the programmers influences this effect.</p><p>In Study 7 we examine whether people will be more outraged at discrimination by an algorithm when the algorithm is anthropomorphized. Understanding factors that mitigate algorithmic outrage deficit can outline the scope of the effect, and also provide additional support for the role of perceived prejudice in algorithmic outrage deficit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 5: Tech Workers</head><p>In Study 5 we sampled workers in the technology industry in Norway. We had two goals in this study. First, we wanted to test the generalizability of the algorithmic outrage deficit using a sample of people that actually experienced a hiring process for the type of job we describe in our manipulation. Second, we wanted to explore whether the algorithmic outrage deficit might be a result of a lack of knowledge about algorithms. Participants in this study read about gender discrimination in hiring decisions done by a human or an algorithm and reported how outraged they were as well as other questions. We predicted that, as in our previous studies, people will be less outraged when algorithms discriminate. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. We recruited participants working in the tech industry by approaching the HR managers of five Norwegian tech companies. The five organizations all provide financial <ref type="bibr">5</ref> We conducted this study before studies 1-4, and prior to getting valuable feedback from reviewers. This is why the vignette and the measures are slightly different than the ones in Studies 1-4 (but are similar to the replications reported in the discussion of Study 2, and to Study 6).</p><p>technology and enterprise technology services for banking and finance. The HR managers in the companies forwarded an email invitation to the study to all employees who work with technology. Out of the 292 people who started the survey 206 (159 male, 42 female, 5</p><p>other/preferred not to answer; age: M = 34.51, SD = 10.63) completed it, of which 51 failed the attention check and were excluded from the analysis. As we were not able to estimate in advance how fast data collection would be from this sample, we did not pre-register this study. We report all conditions and measures.</p><p>Procedure and measures. Participants were randomly assigned to an Algorithm or a Human condition. The study was conducted in Norwegian; we describe the English translation.</p><p>In the Algorithm condition participants read the following (Human condition in parentheses):</p><p>Imagine the following:</p><p>COMPNET, an Artificial-Intelligence-based computer program (Mr. Davie, an HR specialist), was given ultimate power in the hiring process of programmers and engineers in your company two years ago.</p><p>It was recently found that COMPNET (Mr. Davie) was biased against women when rating applicants' resumes'. CO NET (Mr. Davie) put penalties on any resume using the word "women's", as in "women's chess club captain".</p><p>This prevented many talented and qualified women engineers from getting high-paid jobs at your company.</p><p>Assessing outrage. After reading the scenario, participants rated their moral outrage. To assess moral outrage we used five items. The first item asked participants "Which of the following best expresses your opinion of the discriminatory actions of r. avie/Compnet" (1 = completely acceptable; 3 = objectionable; 5 = absolutely shocking; 7 = outrageous). The other items asked participants to rate their agreement with the following four statements: "I am morally outraged by the discriminatory actions of r. avie/Compnet", "The discriminatory actions of r. avie/Compnet are unjust", "The discriminatory actions of r. avie/Compnet were immoral" and "The discriminatory actions of r. avie/Compnet were wrong" (1 = Strongly disagree; 7 = Strongly agree). We then created a composite moral outrage index by averaging all five items, Cronbach's α = .86.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional measures.</head><p>We asked participants how worried and how concerned they would be about such discrimination (inter-item correlation: r = .61, p &lt; .001), to what extent they thought that the human or the algorithm should be fired and replaced/discarded, and the extent to which they thought the company should make a public apology, do an internal audit and make an effort to hire more women (Cronbach's α = .69). We found no significant difference between conditions for these variables (ps &gt; .11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge about AI.</head><p>We then asked participants "compared to the average Norwegian, how knowledgeable are you about AI" (1 = much less knowledgeable; 7 = much more knowledgeable). Finally, as an attention check we asked participants whether a human or a software made hiring decisions in the story they read about and to provide demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>An independent samples t-test revealed that, as predicted, participants were less outraged when the discrimination was done by an algorithm (M = 6.16, SD = 1.55) than when the discrimination was done by a human (M = 6.60, SD = 0.98), t(153) = 2.15, p = .033, Cohen's d = 0.34.</p><p>To examine whether algorithm outrage is dependent on people's knowledge about AIs, we tested whether self-reported knowledge about AI moderated the algorithmic outrage deficit using a bootstrapping moderation analysis <ref type="bibr">(we used Preacher &amp; Hayes, 2008</ref>; 5000 iterations, model 1). We found a significant Knowledge about AI x Condition interaction, b = 0.24, SE = 0.10, t(151) = 2.46, p = .015. A follow-up analysis revealed that while there was no difference between conditions for people 1 SD below the average of knowledge about AI (p = .793), there was a significant difference for people with an average knowledge about AI (conditional effect: b = 0.21, SE = 0.10, t(151) = 2.09, p = .038) and people 1 SD above the average of knowledge about AI (conditional effect: b = 0.46, SE = 0.14, t(151) = 3.24, p = .002), see <ref type="figure" target="#fig_12">Figure 5</ref>. To further examine this interaction, we ran two regression analyses, one for the Algorithm condition and one for the Human condition, each testing the relation between AI knowledge and moral outrage. AI Knowledge was negatively related (marginally significant) to moral outrage in the Algorithm condition (β = -.23, t (69)= -1.958, p = .054), suggesting that the more self-reported knowledge people had about AIs, the less outraged they were at discrimination by an algorithm.</p><p>In contrast, in the Human condition, the relation between AI knowledge and moral outrage was not significant (β = 0.15, t (82) = 1.36, p = .177). Knowledge about AI, therefore, didn't mitigate the algorithmic outrage deficit, but actually aggravated it. The more people said they knew about AIs, the less outraged they were at algorithm (vs. human) discrimination. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Knoweldge x Condition</head><p>Human Algorithm did not allow us to test this possibility. We further examined the role of attributed prejudiced motivation in Study 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 6: Manipulating Programmers</head><p>In our previous studies we found that perceived prejudiced motivation statistically explains (mediates) the algorithmic outrage deficit. However, mediation analysis provides only limited support to arguments about causality. A more powerful way to examine causality is by manipulating the mediator <ref type="bibr" target="#b109">(Pirlott &amp; MacKinnon, 2016)</ref>. In Study 6, to test the causal role of perceived prejudiced motivation we manipulated perceived prejudiced motivation by manipulating the identity of the algorithm's programmers. There are two reasons why manipulating the identity of the programmers might affect the motivation attributed to the algorithm. The first is that people attribute to objects properties of disliked individuals who have been brief contact with them. For example, people are unwilling to wear a shirt that was previously worn by Hitler <ref type="bibr" target="#b113">(Rozin et al., 1986)</ref>. This attribution can be even stronger for objects that were created by disliked people, such as sexist programmers. The second is that people might think that sexist programmers will program their sexism into the algorithms they create.</p><p>Participants read about gender discrimination in hiring decisions and were randomly assigned to one of four conditions. The first two conditions were similar to those in Study 5, describing gender discrimination in hiring decision either by an algorithm or by a human. In the two new conditions we provided participants with information about the identity of the algorithm's programmers. In one condition they were described as working for a known sexist company, and in the other as working for a more egalitarian company. We predict that in addition to replicating our previous findings, people will be more outraged at discrimination by an algorithm when the algorithm was programmed by a more sexist company than when it was programmed by a more egalitarian company, as people might perceive the algorithm as sharing to some extent the motivation of its creators. We note that there is some research suggesting that people might be actually more outraged at socially responsible companies that behave unethically <ref type="bibr" target="#b70">(King &amp; McDonnell, 2012)</ref>. However, according to our theory, discrimination is not just an action, it is an action in a context, and the context is prejudiced motivation. We therefore predict that since people will attribute less of a prejudiced motivation to an algorithm programmed by an egalitarian company, people will be less outraged when such an algorithm discriminates. Procedure. We randomly assigned participants to one of four conditions. Two of these conditions were similar to those used in Study 5, but instead of telling participants to imagine this happened in their company, we told them to imagine this happened in Amazon. In the Human condition participants read that a human, Mr. Davie, discriminated against women. In the "Algorithm Control" condition participants read that an algorithm, CompNet, discriminated against women. We included two additional conditions, in which participants read that CompNet discriminated against women and were giving information about the identity of CompNet's programmers. In the "Sexist rogrammers" condition participants read the following:</p><p>"COMPNET was developed by a company named Beyond Computers. Beyond</p><p>Computers, founded and managed by men, is known in the industry as being a hostile work environment for women. 95% of its programmers are men, and men are systematically paid more than women."</p><p>In the "Egalitarian rogrammers" condition participants read that:</p><p>"COMPNET was developed by a company named Beyond Computers. Beyond</p><p>Computers, founded and managed by women, is known in the industry as being very women friendly. It has an equal number of men and women programmers, and pays men and women exactly the same."</p><p>After reading the scenario participants reported their moral outrage using the same items we used Study 5, with one difference: the items were framed as asking about "CompNet's/ r. avie's behavior", rather than "discriminatory actions" (Cronbach's α = .91). As a manipulation check, participants rated the prejudiced motivation they attributed to the agent, using the items we used in Studies 1-2 (α = .70). As an attention check we then asked participants who made the hiring decision in the story they read: a human, a software without mention of its programmers, a software programmed by a company founded and managed by women, or a software programmed by a company founded and managed by men. Finally, participants provided demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Manipulation checkattribution of prejudiced motivation. A one-way ANOVA revealed that, as predicted, condition affected the attribution of prejudiced motivation, F(3, 779) = 82.76, p &lt; .001, η 2 = .21, see <ref type="figure" target="#fig_15">Figure 6</ref>. We then ran a follow-up planned and pre-registered  Moral outrage. A one-way ANOVA revealed that, as predicted, condition affected moral outrage, F(3, 779) = 26.28, p &lt; .001, η 2 = .10, see <ref type="figure" target="#fig_16">Figure 7</ref>. We ran two follow-up planned contrasts. The first contrast revealed that, as predicted, participants were less morally outraged by the discrimination in the Algorithm Control condition (M = 5.52, SD = 1.32) than in the Human condition (M = 5.85, SD = 1.08), t(779) = 2.74, p = .00 , Cohen's d = 0.28, replicating </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Study 6 provides further causal support for our hypothesis about the role of perceived prejudiced motivation in algorithmic outrage deficit, by examining the identity of the programmers as a moderator. For the same discriminatory actions, when the programmers are known to be sexist, people perceived the algorithm as having a stronger prejudiced motivation and were more outraged at the discrimination. On the other hand, when the programmers are known to be egalitarian, people perceived the algorithm as less motivated by prejudice and were less morally outraged when it discriminated. The results of Study 6 also point to a possible positive outcome for companies who have a diverse workforce. Even if the product (the algorithm) they create ends up discriminating, people will be less outraged at the discrimination, because they will see the algorithm as less motivated by prejudice, due to the diversity of its programmers. Study 6 rules out an alternative explanation for our findings. In all conditions, people attributed mid-level and higher prejudiced motivation to algorithms, suggesting that our results are not due to people seeing algorithms as inherently incapable of prejudiced motivation, but rather of people attributing less such motivation to algorithms. We note that our results in this study might be due to the halo effect <ref type="bibr" target="#b104">(Nisbett &amp; Wilson, 1977)</ref>. Specifically, people's outrage might be affected by learning about the sexist or egalitarian company, and not due to an effect of the programmer's identity on how people perceive the algorithm. To address this possibility and further examine the causal role of perceived motivation on moral outrage, in Study 7 we approached this differentlyby manipulating anthropomorphism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 7: Anthropomorphism</head><p>In Study 7 we wanted to further examine the role of perceived prejudiced motivation in algorithmic outrage deficit. Rather than doing so by manipulating the identity of the programmers, as we did in Study 6, in Study 7 we did so by manipulating anthropomorphismthe extent that which people attribute human-like abilities, such as motivation, to non-human entities <ref type="bibr" target="#b41">(de Visser et al., 2016;</ref><ref type="bibr" target="#b122">Schroeder &amp; Epley, 2016;</ref><ref type="bibr" target="#b147">Waytz et al., 2014)</ref>. This allowed us to examine the role of perceived motivation while keeping the identity of the agent constant (always an algorithm) and without mentioning the identity of the programmers (they are never mentioned). Participants read about an algorithm low on anthropomorphism or an algorithm high on anthropomorphism that discriminated against women in hiring decisions. We predicted that people will perceive anthropomorphized algorithms as motivated by prejudice more than non-anthropomorphized algorithms, causing participants to be more morally outraged at discrimination by the anthropomorphized algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Eight hundred and one participants (378 male, 407 female, 16 other or preferred not to disclose; age: M = 32.00, SD = 11.93) from the United States completed the study on Prolific in exchange for 3 dollars 7 . Accounting for participants who might fail attention checks and therefore be excluded from the analysis, this sample size gives us a power of 0.80 to detect a two-tailed small effect size (Cohen's d = 0.2, calculated with G*Power 3.1.9.2). As specified in the pre-registration (https://aspredicted.org/q397v.pdf), we did not include in the analysis participants who failed the attention check, leading to the exclusion of sixty-four participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants were randomly assigned to a high-anthropomorphism or a lowanthropomorphism condition. All participants first read the following paragraph:</p><p>AeonTech is a technology company that has an automated process for hiring software developers. They rely on an algorithm-SigmaEvalu8, an unsupervised Bayesian AI system that scans each application and assigns it between one (lowest fit) and five stars (highest fit). Applicants with the highest scores are then forwarded to the hiring committee.</p><p>Then, in the high-anthropomorphism, participants read the following:</p><p>The algorithm is just like a human being in that it develops its own tastes and social preferences from scanning past job applications. You can think of it as a human on a hiring committee. It gets a feeling for which features of an application it likes in predicting future job performance and then chooses new applications based on the opinions it has formed. In other words, the algorithm is just like a human that thinks about what it wants in terms of a job candidate and uses its beliefs to select people it likes based on these features.</p><p>Despite receiving large numbers of applications from women for software engineering positions, AeonTech has hired almost no women.</p><p>The reason for this has to do with the initial screening. AeonTech's algorithm, SigmaEvalu8, through forming its own humanlike preferences for a particular social category, systematically gave women a lower star rating than men. Therefore, women almost never made it to the second phase, and were not in the final list that the hiring committee considered.</p><p>In the low-anthropomorphism condition, participants read the following:</p><p>The algorithm is a tool that computes information from scanning past job applications. You can think of it as a supercomputer. It calculates what features of an application are useful in predicting good performance and scores new applications based on this information. In other words, the algorithm is a mathematical process that extracts useful features from existing data sources and uses them to select optimal job candidates. Despite receiving large numbers of applications from women for software engineering positions, AeonTech has hired almost no women. The reason for this has to do with the initial screening. AeonTech's algorithm, SigmaEvalu8, through producing its positive computational ouput for a particular social category, systematically gave women a lower star rating than men. Therefore, women almost never made it to the second phase, and were not in the final list that the hiring committee considered.</p><p>Assessing likeness as a manipulation check. Participants then rated their agreement with the four statements on a 0 (strongly disagree) to 100 (strongly agree) slider. Two items measured perceptions of human-likeness: "SigmanEvalu8 behaves like a human being" and "SigmaEvalu8 has a mind". We created an index of perceived human-likeness by averaging these two items, r =.61. The other two items measured perceived machine-likeness:</p><p>"SigmaEvalu8 operates like a tool" and "SigmaEvalu8 works like a machine". We created an index of perceived machine-likeness by averaging these two items, r =.71.</p><p>Assessing perceived prejudiced motivation and moral outrage. We then measured perceived prejudiced motivation (Cronbach's α = .89) and moral outrage (Cronbach's α = .96), using the same items as in Studies 1-2 and 6.</p><p>Participants then answered two attention check questions. The first asked whether an algorithm or a human HR specialist screened the applications. The second asked whether the story they read described gender discrimination in hiring practices. Finally, participants provided demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manipulation check.</head><p>To test whether our anthropomorphism manipulation was successful in changing participants' perceptions of the algorithm, we conducted a 2 (condition: highanthropomorphism, low-anthropomorphism; between-participants) x 2 (likeness: human, machine; within-participants) mixed model ANOVA. The ANOVA revealed a main effect for likeness, F(1, 734) = 817.34, p &lt; .001, partial η 2 =.527, such that across conditions, participants perceived the algorithm as higher in machine-likeness (M = 73.54, SD = 24.75) than in humanlikeness (M = 30.22, SD = 26.92). The main effect for condition was also significant, F(1, 737) = 14.69, p &lt; .001, partial η 2 =.020, such that across types of likeness, participants perceived the  <ref type="bibr">[-6.27, -2.53</ref>], see <ref type="figure" target="#fig_18">Figure 8</ref>. Taken together, these results show anthropomorphism has a dual effect on moral outrage. It causes people to perceive the anthropomorphized algorithm as being more motivated by prejudice and therefore feel more outraged. But there is another factor, evident by the significant direct effect, causing them to be less outraged at the anthropomorphized algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results of Study 7 provide further support for the role of perceived motivation in algorithmic outrage deficit. Participants perceived highly anthropomorphized algorithms as being more motivated by prejudiced motivation, causing them, indirectly, to be more outraged at algorithm discrimination. While in our previous studies the perceived motivation was confounded with the identity of the agent (algorithm vs. human), or the identity of the programmers, Study 7 shows that even for the same agentan algorithmthe perceived prejudiced motivation affects moral outrage. However, the overall effect on moral outrage was more nuanced. In addition to the significant indirect effect vie perceived prejudiced motivation, we found a significant direct effect in the opposite direction: taking into account the increased outrage due to an increase in perceived prejudiced motivation, people were less outraged at discrimination by the highly-anthropomorphized algorithm. This is consistent with research showing that anthropomorphism tends to increase people's trust in algorithms and robots <ref type="bibr" target="#b41">(de Visser et al., 2016;</ref><ref type="bibr" target="#b147">Waytz et al., 2014)</ref> and with research showing that anthropomorphism increases people's forgiveness after wrong-doing <ref type="bibr" target="#b135">(Tang &amp; Gray, 2021;</ref><ref type="bibr" target="#b153">Yam et al., 2020)</ref>.</p><p>Overall, the non-significant main effect of anthropomorphism could be explained by these opposing forces-perceived motivation (activated by anthropomorphism) increases moral outrage (as our other studies show), but anthropomorphism generally can orient people toward positive evaluations of a machine, engendering trust and forgiveness.</p><p>In Studies 1-7 we examined the effect of algorithm discrimination on moral outrage and general evaluations of companies who use algorithms. In Study 8 we examine a possible downstream consequence of algorithm discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 8: Perceived Legal Liability</head><p>In Study 8 we examined a possible downstream consequence of people perceiving algorithms as less motivated by prejudice than humanscompany liability. Title VII of the United States Civil Rights Act of 1964 prohibits discrimination based on gender, race, color or national origin, or religion <ref type="bibr">(Civil Rights Act, 1964)</ref>. One type of discrimination lawsuit that can be filed under Title VII is cases of intentional discrimination <ref type="bibr">(Civil Rights Act, 1964)</ref>. Since people see algorithms as less motivated by prejudice than humans, they might see companies as less liable for intentional discrimination when an algorithm (vs. a human) discriminates. In Study 8, participants read about gender discrimination in a high-tech company by an algorithm or a human and rated the perceived prejudiced motivation of the agent. Participants then read that the female applicants who were not hired are suing the company for intentional discrimination under Title VII. We asked participants how liable they thought the company is. We predicted that the company will be judged as less liable when an algorithm (vs. a human) discriminated, and that this will be mediated by participants perceiving the algorithm as less motivated by prejudice than the human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. To assess responses from the group not subject to discrimination in our scenario, we sampled only men for this study. Two hundred and forty participants from the US and Canada (age: M = 32.19, SD = 11.46) completed the study on Prolific in exchange for 45 cents. As specified in the pre-registration (https://aspredicted.org/de5r2.pdf), we did not include in the analysis participants who failed to answer any of the attention checks correctly, leading to the exclusion of fifteen participants.</p><p>Procedure. All participants first read the same scenario as in Study 1 about gender discrimination in hiring decisions at a high-tech company. We then measured the prejudiced motivation participants attributed to the agent they read about using the same items as in Studies 1, 2, 6 and 7 (Cronbach's α = .94). After that, participants read about female programmers who are suing the company for intentional liability:</p><p>Protective Title VII of the Civil Rights Act of 1964 makes it illegal to discriminate against people based on gender. One type of discrimination lawsuit that can be filed under Title VII are cases of intentional discrimination, which require discriminatory intent. In other words, it requires showing intentional prejudice. A group of female programmers who applied for jobs at AeonTech are suing AeonTech under Title VII. As a reminder, an audit found that the reason AeonTech didn't hire women is that SigmaEvalu8 (the HR specialist) gave women a lower rating than men.</p><p>Assessing liability. To assess how much participants thought the company is liable for intentional discrimination, we asked participants to rate how much they disagree or agree with the following three statements on a 0 (strongly disagree) to 100 (strongly agree) slider: "I would support a verdict that said the company had discriminatory intent", "The company is liable because it acted out of intentional prejudice" and "The company is not liable, because there is no evidence of intentional discrimination" (reverse scored . We then created a composite stereotype endorsement index by averaging all three items, Cronbach's α = .90.</p><p>Participants then answered two attention check questions. The first asked whether an HR specialist or an algorithm did the initial screening in the story they read. The second asked whether the company they read about hardly hired any women or hired a similar number of men and women. Finally, participants provided demographic information.  When accounting for the mediation by attribution of prejudiced motivation, the direct effect of agent on stereotype endorsement was not significant, b = -2.32, SE = 1.47, CI.95 <ref type="bibr">[-5.22, 0.57]</ref>. The decrease in liability judgments when the discrimination was done by an algorithm appears, therefore, to be driven by people attributing less of a prejudiced motivation to the algorithm (vs. the human), see <ref type="figure" target="#fig_22">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results of Study 8 demonstrate one possible downstream effect of algorithm discrimination. People find companies less liable when they discriminate because of human bias than when they discriminate because of algorithm bias, an effect mediated by people perceiving algorithms as less motivated by prejudiced motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Eight studies provide evidence for the idea of algorithmic outrage deficit. People are less outraged when gender discrimination is perpetrated by algorithms versus other people because algorithms are not perceived to possess the motivation to discriminate. We found that people attribute less of a prejudiced motivation to algorithms than humans (Study 1), leading people to be less outraged at discrimination by an algorithm than discrimination by a human (Study 2).</p><p>Algorithm discrimination had a nuanced effect on people's moral outrage at the company using the algorithms. Learning that a company used an algorithm (vs. a human) for screening applicants initially increased moral outrage (consistent with <ref type="bibr" target="#b13">Bigman &amp; Gray, 2018)</ref>. However, relative to this baseline of outrage, people were less outraged when an algorithm (vs. a human)</p><p>caused the discrimination (Study 3). The use of algorithms (vs. humans) for hiring decisions mitigated how people evaluated a company. Companies were judged less negatively for negative outcomes such as low gender equality and less positively for positive outcome such as high gender equality (Study 4). The algorithmic outrage deficit is not due to people's lack of knowledge about AIs. The more people know about AI, the less outraged they were at algorithm discrimination (Study 5). We found converging evidence for the role of perceived motivation (Studies 6-7). When the discriminatory algorithm was created by sexist programmers, people perceived it as more motivated by prejudice and were more outraged (Study 6). In contrast, when the algorithm was created by egalitarian programmers it was perceived as less motivated by prejudice and people were less outraged (Study 6). Furthermore, when the algorithms were anthropomorphized, people attributed to them more prejudiced motivation, causing an increase in moral outrage (Study 7). Finally, we also found that people are less likely to find the company legally liable for intentional discrimination when an algorithm (vs. a human) was the cause of discrimination (Study 8).</p><p>Algorithms are playing an increasingly active role in several domains where they sometimes discriminate, including: the legal system <ref type="bibr" target="#b5">(Angwin et al., 2016)</ref>, healthcare <ref type="bibr" target="#b107">(Obermeyer et al., 2019)</ref>, banking <ref type="bibr" target="#b132">(Stankiewicz, 2019)</ref>, advertising <ref type="bibr" target="#b76">(Lambrecht &amp; Tucker, 2019)</ref> and HR decisions <ref type="bibr" target="#b40">(Dastin, 2018)</ref>. As a first step, in our studies, we focused on hiring decisions, but we suspect that a similar effect will be found in other domains such as sentencing, parole, consumer targeting, and medical assessment as well. Most of our studies examined gender discrimination, inspired by the algorithm Amazon used <ref type="bibr" target="#b40">(Dastin, 2018)</ref>, but some of our studies We do not argue that the algorithmic outrage deficit is irrational or a bias. It is indeed possible that algorithms are less likely to unfairly discriminate between people according to their race, age, and gender than humans <ref type="bibr" target="#b100">(Mullainathan, 2019)</ref>. However, it is still crucial to understand how people respond to algorithms when they do show bias.</p><p>Our research contributes to a growing literature on how people respond to the presence of algorithms in day-to-day life. Algorithms are seen as more impartial and less capable of bias than human decision-makers <ref type="bibr" target="#b68">(Jago &amp; Laurin, 2021)</ref>, leading people to prefer them as decision-makers in some situations <ref type="bibr" target="#b18">(Bigman et al., 2021;</ref><ref type="bibr" target="#b84">Logg et al., 2019)</ref>. However, in some cases people show an aversion to algorithm decision-making <ref type="bibr" target="#b42">(Dietvorst et al., 2015)</ref> and prefer for algorithms not to act as decision-makers. At least sometimes, this aversion can be a result of the flip side of algorithm impartialityalgorithms have a reductionist approach to human nature that lacks empathy and emotion which are seen as necessary for some decisions <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b85">Longoni et al., 2019;</ref><ref type="bibr" target="#b102">Newman et al., 2020)</ref>. Our research shows another consequence of the perceived impartiality of algorithmspeople perceive them as lacking prejudiced motivation and are therefore less morally outraged when they discriminate. We note that although algorithms do discriminate, algorithm biases are easier to correct than human biases <ref type="bibr" target="#b100">(Mullainathan, 2019)</ref> and can actually be used to detect human discrimination <ref type="bibr" target="#b83">(Logg, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications</head><p>Our research has both theoretical and practical implications. First, our research contributes to research on moral outrage. By demonstrating the role of attribution of prejudiced motivation we contribute to the literature suggesting that moral outrage is not only a response to harm <ref type="bibr" target="#b64">(Hechler &amp; Kessler, 2018)</ref>. While previous research on moral outrage compared intentional to non-intentional harm <ref type="bibr" target="#b64">(Hechler &amp; Kessler, 2018;</ref><ref type="bibr" target="#b114">Russell &amp; Giner-Sorolla, 2011)</ref>, our current work proposes that a broader set of mental statesmotivationsaffect moral outrage. People care not only whether an agent intentionally discriminatedperformed the action with the belief that it will lead to a specific outcomeor not, but also about the agent's motivationwhy they discriminated (see <ref type="bibr" target="#b24">Carlson et al., 2022</ref>, for a discussion of the distinction between intention and motivation). We note that while our results from Study 2 suggest that intention might also play a role in explaining moral outrage (see Footnote 3), perceived motivation mediates algorithm outrage deficit above and beyond perceived intention. In doing so, our work contributes to the growing literature in moral psychology highlighting the role of perceived motivation in moral judgment (e.g., <ref type="bibr" target="#b15">Bigman &amp; Tamir, 2016;</ref><ref type="bibr" target="#b25">Carlson &amp; Zaki, 2018;</ref><ref type="bibr" target="#b77">Levine &amp; Schweitzer, 2014;</ref><ref type="bibr" target="#b112">Reeder et al., 2002)</ref>.</p><p>Second, our work has implications for research in human-robot interaction. Specifically, our research contributes to the literature on how people perceive the mental states of artificial agents such as robots and algorithms <ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018;</ref><ref type="bibr">Graaf &amp; Malle, 2019;</ref><ref type="bibr" target="#b57">Gray &amp; Wegner, 2012;</ref><ref type="bibr" target="#b120">Schein &amp; Gray, 2015;</ref><ref type="bibr" target="#b147">Waytz et al., 2014;</ref><ref type="bibr" target="#b151">Weisman et al., 2017;</ref><ref type="bibr" target="#b154">Young &amp; Monroe, 2019)</ref> by showing that people are less likely to attribute prejudiced motivation to algorithms. While previous work focused on the mind (mental capacities) robots and algorithms are perceived to have (e.g., <ref type="bibr" target="#b13">Bigman &amp; Gray, 2018)</ref>, our work explores motivation, the content of a specific mental state. Looking at this more detailed level of attributions opens new promising venues for investigating how people react, respond, and interact with algorithms and other artificial agents.</p><p>Third, our research complements current work in computer science, legal studies, and other disciplines on how to create fair algorithms <ref type="bibr" target="#b0">(Abdul et al., 2018;</ref><ref type="bibr" target="#b3">Ananny, 2016;</ref><ref type="bibr" target="#b75">Kusner &amp; Loftus, 2020;</ref><ref type="bibr" target="#b119">Sandvig et al., 2016;</ref><ref type="bibr" target="#b123">Selbst &amp; Barocas, 2018;</ref><ref type="bibr" target="#b145">Wachter et al., 2017;</ref><ref type="bibr" target="#b155">Zou &amp; Schiebinger, 2018)</ref>. While work on "algorithm ethics" discusses how to create fair algorithms, our work starts exploring the psychological response of biased algorithms, and how they differ from the psychological response to biased humans. Understanding these differences is a first necessary step to address the unique challenges that biased algorithms pose to society.</p><p>Finally, our research has implications for organizations that use algorithms for decisions such as hiring. Fairness perceptions of the recruitment and selection procedure affect the overall reputation of an organization <ref type="bibr" target="#b27">(Chapman et al., 2005;</ref><ref type="bibr">Ryan &amp; Ployhart, 2000)</ref>. Furthermore, applicants who are hired at the end of an unfair selection process will develop lower levels of organizational commitment, less organizational citizenship behaviors, and higher turnover <ref type="bibr" target="#b62">(Hausknecht et al., 2004;</ref><ref type="bibr" target="#b138">Uggerslev et al., 2012)</ref>. Our research suggests that, at least for some decisions, emphasizing the role of algorithms in the hiring process might increase perceptions of fairness, increase worker satisfaction, and reduce moral outrage when the outcomes of the process are discriminatory. In addition, if companies recognize that while "blaming the algorithm" could protect the company reputationally, it might have the adverse effect of making discrimination feel more permissible. Furthermore, the results of Study 6 suggest that when creating algorithms, companies can reduce public outrage when those algorithms discriminate by having a diverse team of programmers develop these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and future directions</head><p>Despite the robustness of our findings, our work has several limitations. People might be less outraged at discrimination by algorithms because algorithms are easier to de-bias than humans <ref type="bibr" target="#b100">(Mullainathan, 2019)</ref>. We note that one reason why it might be easier to de-bias algorithms than humans is that algorithms lack the prejudiced motivation that might cause some people to discriminate. Future research should investigate this interesting possibility. There also might be individual and cultural differences in the way people respond to discrimination by algorithms. For example, it is possible that people who are high on anthropomorphizing nonhumans  might attribute more motivation to algorithms and therefore not show reduced outrage at discrimination by machines.</p><p>Our results suggest that people might attribute to algorithms characteristics of their creators and programmers (see Study 6). This finding raises several interesting questionswhen would people see algorithms as independent of their creators? Who would people blame for harm done by algorithms: the algorithms, the programmers, or the company that uses the algorithms?</p><p>Future research is needed to investigate this question.</p><p>The results of Study 5 show that the more people know about AIs the less outraged they are by algorithm discrimination. Presumably, this is because people with high knowledge about</p><p>AIs have different beliefs about why algorithms discriminate. It is also possible that the results we obtained in our studies are due to people having certain beliefs about the reasons for algorithm discrimination. We explored this in a short study where participants read about a screening algorithm that discriminated against women. We asked participants (N=183 after exclusions) how much they thought the algorithm was 1) trained to mimic previous decisions; 2) trained to make decisions based on the views and beliefs of its programmers; and 3) trained to make its own independent decisions. Participants showed similar levels of agreement with all of these statements, F (2, 176) = 0.79, p = .465. These results suggest that there is not a strong distinction in people's beliefs about the reasons for algorithm discrimination. Still, future research is needed to explore how beliefs about the technical reasons for algorithm discrimination (beyond lack of prejudiced motivation) affect moral outrage.</p><p>There are also specific sub-findings in certain studies that could benefit from future research. In Replication D, reported in the discussion of Study 2, we found a significant interaction with gender (F(1, 1267) = 5.07, p = .025, partial η 2 = .004), such that while men were less outraged at gender discrimination by an algorithm (F(1, 1267) = 26.02, p &lt; .001, partial η 2 = .020, women's reduced outrage at algorithm discrimination was only marginally significant (F(1, 1267) = 3.78, p = .052, partial η 2 = .003) (see supplemental materials). This analysis was exploratory and further research is needed to systematically explore whether the group suffering from the discrimination reacts differently to algorithm discrimination than an unaffected group.</p><p>Across studies, there are also differences in effect sizes that need to be explored. In the replications reported in the discussion of Study 2 we found algorithmic outrage deficit for race and age discrimination in addition to gender discrimination. The effect sizes ranged between</p><p>Cohen's d's of 0.26 and 0.80. Future research is needed to fully understand the source of the variance in effect sizes between the different types of discrimination and different populations.</p><p>Finally, our research focused on how people who are not affected by the discrimination of an algorithm respond to it. Another interesting question is how people who were targets of discrimination will respond to discrimination by an algorithm. Research shows that people who are discriminated against suffer from negative psychological consequences such as depression <ref type="bibr" target="#b44">(Finch et al., 2000;</ref><ref type="bibr" target="#b106">Noh et al., 1999)</ref> and anxiety <ref type="bibr" target="#b127">(Soto et al., 2011)</ref>. Would people show less or more of these responses when they are discriminated against by an algorithm rather than a human? Further research is needed to explore this question, which will help us understand the full psychological consequences of discrimination by algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding remarks</head><p>The increasing abilities and prevalence of machine-learning-based AI and autonomous machines raise new ethical and societal concerns. Here we highlight one of them, the algorithmic outrage deficit. We find that people attribute less prejudiced motivation to algorithms and consequently are less morally outraged by discrimination by algorithms. Beyond the contribution of this research to the study of moral outrage and human-robot interaction, our work has a warning sign for society. Algorithms carry the promise of being fairer than humans. However, when they are not, people's defenses against injustice might be lowered when the agent is an algorithm, making it easier for discrimination to go unnoticed and unopposed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>independent samples t-test revealed that participants perceived the algorithm (M = 80.17, SD = 25.47) as less discriminatory than the human (M = 89.09, SD = 17.26), t (228)=3.10, p = .002, Cohen's d = 0.41. A second t-test revealed that participants perceived the algorithm (M = 51.04, SD = 25.79) as more objective than the human M = 25.45, SD = 23.20), t (228)=7.91, p &lt; .001, Cohen's d = 1.04. A third t-test revealed that participants perceived the algorithm (M = 54.04, SD = 29.83) as less motivated by prejudice than the human (M = 72.43, SD = 22.98), t (228)=5.23, p &lt; .001, Cohen's d = 0.69. Results are shown in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Ratings of discriminatory actions by an algorithm vs. a human (Study 1). Error bars reflect standard errors. All differences are statistically significant (p &lt; .05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>algorithm as being less motivated by prejudice (M = 59.36, SD = 29.75) than the human HR specialist (M = 74.67, SD = 22.15), t(768) = 8.07, p &lt; .001, Cohen's d = 0.58, replicating our results from Study 1. A second independent samples t-test revealed that, as predicted, participants were less morally outraged by the algorithm's discriminatory actions (M = 61.10, SD = 31.71) vs. the human HR specialist's discriminatory actions (M = 73.40, SD = 26.53), t(768) = 5.82, p &lt; .001, Cohen's d = 0.42, supporting our theory about algorithmic outrage deficit. A third independent samples t-test revealed that participants perceived the algorithm's discrimination as less intentional (M = 54.13, SD = 33.94) than the human HR specialist's discrimination (M = 74.77, SD = 25.20), t(768) = 9.55, p &lt; .001, Cohen's d = 0.69.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Mediation analysis reveals that perceived prejudiced motivation mediates the effect of agent on moral outrage (Study 2). * Denotes p &lt; .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Replication 2A (N = 122) examined race discrimination, Cohen's d = 0.80. Replication 2B (N = 241) examined age discrimination, Cohen's d = 0.34. Replication 2C (N = 241) examined gender discrimination, Cohen's d = 0.46. Replication 2D (N = 1503) examined gender discrimination in a quasirepresentative sample from the UK, Cohen's d = 0.26. Replication 2E (N = 240) examined race discrimination, Cohen's d = 0.39, and also tested for mediation by perceived prejudiced motivation. The mediation analysis replicated the results of the mediation analysis in Study 2,and found that the reduced outrage at algorithm discrimination, b = -0.23, SE = 0.08, p = .004, was mediated by people perceiving the algorithm as less motivated by prejudice than the human, b = -0.30, SE = 0.06, CI.95[-0.42, -19].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Mean moral outrage by agent (Study 2: Replications A-E). Error bars reflect standard errors. All differences are statistically significant (p &lt; .05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>increase in moral outrage (calculated as outrage post-discrimination minus outrage prediscrimination) as our DV, agent (Human = -1, Algorithm = 1) as our IV, and perceived prejudiced motivation as the mediator. The analysis (Preacher &amp; Hayes, 2008; 5000 iterations, model 4), revealed that, as predicted, the effect of agent on moral outrage, b = -3.90, SE = 1.56, p = .013, was mediated by an indirect effect of attribution of prejudiced motivation, b = -2.46, SE = 0.95, CI.95[-4.30, -0.55]. When accounting for the mediation by attribution of prejudiced motivation, the direct effect of agent on moral outrage was not significant, b = -1.44, SE = 1.26, CI.95[-3.92, 1.03]. The smaller increase in moral outrage for discrimination by an algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>more negatively in the discrimination condition (M = 62.58, SD = 29.17) than in the equality condition (M = 31.43, SD = 31.08), F(1, 209) = 56.17, p &lt; .001, partial η 2 = .212, and more positively in the equality condition (M = 49.86, SD = 29.13) than in the discrimination condition (M = 24.44, SD = 25.47), F(1, 209) = 45.29, p &lt; .001, partial η 2 = .178, interaction: F(1, 209) = 63.46, p &lt; .001, partial η 2 = 0.213, supporting the success of our manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>F(1, 209) = 16.63, p &lt; .001, partial η 2 = .074. A series of follow-up pairwise comparisons revealed that, as predicted, participants evaluated the company less negatively in the discrimination condition when it used an algorithm (M = 54.44, SD = 27.87) than a human (M = 59.74, SD = 27.81), F(1, 209) = 6.13, p = .014, partial η 2 = .028. We further found that, as predicted, participants had a less positive evaluation of the company for equality when it used an algorithm (M = 41.88, SD = 27.87) than a human (M = 50.83, SD = 27.62), F(1, 209) = 19.53, p &lt; .001, partial η 2 = .085. We also found an unexpected effect such that participants evaluated equality by an algorithm (M = 28.20, SD = 26.65) more negatively than equality by a human (M = 20.23, SD = 23.94), F(1, 209) = 14.78, p &lt; .001, partial η 2 = .066. The difference in positive evaluation of the discriminating company that used an algorithm (M = 14.36, SD = 19.04) and the company that used a human (M = 14.45, SD = 22.73), was not significant, p = .965. SeeFigure 4. Controlling for the order in which participants rated the companies (AeonTech first or CompSoluation first) did not change the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Negative and positive evaluations of companies that discriminated or had gender equality that used either a human HR specialist or an algorithm for screening applicants, Study 4. * Denotes p &lt; .05The ANOVA further revealed significant effects for type of evaluation (positive vs. negative), partial η 2 = .061, agent, partial η 2 = .036, type of evaluation x outcome interaction, partial η 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 .</head><label>5</label><figDesc>The interaction effect of AI knowledge and condition on moral outrage (Study 5), p = .015.DiscussionThe results of Study 5 generalized our findings in a sample of people who work in the high-tech sector, are familiar with algorithms, are in another country (Norway), and shows the robustness of the phenomenon. The results also point to an interesting influence of knowledge of AIs on algorithmic outrage deficit. The more people know about AIs, the less outraged they are at algorithm (vs. human) discrimination. This suggests that algorithmic outrage deficit is not due to people's lack of knowledge about AIs. It is possible that people who know more about AIs are less likely think that AIs are not motivated by prejudice, but the data we collected in this study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>MethodParticipants.</head><label></label><figDesc>Nine hundred and sixty-four participants 6 from the US and Canada (47.9% male, 51.6% female, 0.4% other or preferred not to disclose; age: M = 36.93, SD = 11.96) completed the study on Amazon's echanical Turk in exchange for 40 cents. As specified in the pre-registration (https://aspredicted.org/xq68n.pdf), we did not include in the analysis participants who failed to answer any of the attention check/comprehension questions correctly, leading to the exclusion of 181 participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>contrasts. The first contrast revealed that participants attributed less of a prejudiced motivation to CompNet in the Algorithm Control condition (M = 4.20, SD = 1.35) than in the Human condition (M = 5.10, SD = 0.92), t(779) = 7.97, p &lt; .001, Cohen's d = 0.77, replicating our findings from Studies 1-2. The second contrast revealed that, as predicted, people attributed a more prejudiced motivation to CompNet in the Sexist Programmers condition (M = 5.05, SD = 0.98) than in the Egalitarian Programmers condition (M = 3.79, SD = 1.21), t(779) = 12.08, p &lt; .001, Cohen's d = 1.14. Another contrast, which we did not pre-register, did not find a difference between the Human condition (M = 5.10, SD = 0.92) and the Sexist Programmers condition (M = 5.05, SD = 0.98), p = .656.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Perceived prejudiced motivation by condition (Study 6). All differences are significant (p &lt; .05) expect for between the Human condition and the Sexist Programmers condition. Error bars reflect standard errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 .</head><label>7</label><figDesc>Studies 2-5. The second contrast revealed that, as predicted, people were more outraged by CompNet in the Sexist Programmers condition (M = 5.83, SD = 1.09) than in the Egalitarian Programmers condition (M = 4.96, SD = 1.34), t(779) = 7.56, p &lt; .001, Cohen's d = 0.79. Another contrast, which we did not pre-register, did not find a significant difference between the Human condition (M = 5.85, SD = 1.08) and the Sexist Programmers condition (M = 5.83, SD = 1.09), p = .498. Moral outrage by condition (Study 6). All differences are significant (p &lt; .05) except for between the Human condition and the Sexist Programmers condition. Error bars reflect standard errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>algorithm as being higher in human and machine likeness in the high-anthropomorphism condition (M = 53.69, SE = 0.68) than in the low-anthropomorphism condition (M = 50.00, SE = 0.69). These main effects were qualified by a significant condition x likeness interaction, F(1, 737) = 117.96, p &lt; .001, partial η 2 =.138. Follow-up pairwise comparisons revealed that participants perceived the algorithm as being more humanlike in the high-anthropomorphism condition (M = 40.18, SD = 26.63) than in the low-anthropomorphism condition (M = 19.94, SD = 23.11), F(1, 734) = 120.95, p &lt; .001, partial η 2 =.141. The analysis further revealed that participants attributed to the algorithm more machine-likeness in the low-anthropomorphism condition (M = 80.06, SD = 21.65) than in the high-anthropomorphism condition (M = 67.20 , SD = 25.93), F(1, 734) = 53.20, p &lt; .001, partial η 2 =.068. These results suggest that our anthropomorphism manipulation successfully changed the way the participants perceived the algorithm. Perceived prejudiced motivation and moral outrage. An independent samples t-test revealed that, as predicted, participants attributed more prejudiced motivation to the algorithm in the high-anthropomorphism condition (M = 59.79, SD = 28.65) than in the low-anthropomorphism condition (M = 46.42, SD = 33.12), t(735) = 5.87, p &lt; .001, Cohen's d = 0.43. Contrary to our prediction, a second independent samples t-test did not find a significant difference in moral outrage between the high-anthropomorphism condition (M = 51.83, SD = 31.93) and the low-anthropomorphism condition (M = 52.12, SD = 33.82), t(735) = 0.12, p = .905. Mediation Following our pre-registration and to further test the relation between anthropomorphism, perceived prejudiced motivation, and moral outrage, we performed a bootstrapping mediation analysis (Preacher &amp; Hayes, 2008; 5000 iterations, model 4) with anthropomorphism as the IV, coding the low-anthropomorphism condition as -1 and the high-anthropomorphism condition as 1, moral outrage as a DV, perceived prejudiced motivation as a mediator. The analysis revealed, as predicted, a significant indirect effect of anthropomorphism on moral outrage mediated by perceived prejudiced motivation b = 4.55, SE = 0.81, CI.95[2.97, 6.13]. The direct effect of anthropomorphism on moral outrage was significant and negative, b = -4.40, SE = 0.95, CI.95</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 8 .</head><label>8</label><figDesc>Mediation analysis reveals that anthropomorphism indirectly affects moral outrage via perceived prejudiced motivation (Study 7). While the total effect of anthropomorphism is not significant, the indirect effect vie perceived prejudiced motivation is significant (b = 4.55, SE = 0.81, CI.95[2.99, 6.16]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>of prejudiced motivation. An independent samples t-test revealed that participants attributed less prejudiced motivation to SigmaEvalu8 (M = 48.25, SD = 33.39) than they did to the HR specialist (M = 67.21, SD = 24.10), t(223) = 4.89, p &lt; .001, Cohen's d = 0.65.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Liability.</head><label></label><figDesc>An independent samples t-test also revealed that participants found the company less liable when the discrimination was done by an algorithm (M = 49.08, SD = 31.59) than a human (M = 66.58, SD = 35.63), t(223) = 4.57, p &lt; .001, Cohen's d = 0.61. Mediation. To test whether perceived prejudiced motivation mediated the effect of agent on liability, we performed a bootstrapping mediation analysis (Preacher &amp; Hayes, 2008; model 4, 5000 iterations), coding the algorithm condition as 1 and the human condition as -1. As predicted, the effect of agent on liability, b = -8.75, SE = 1.91, p &lt; .001, was mediated by an indirect effect of attribution of prejudiced motivation, b = -6.43, SE = 1.32, CI.95[-9.13, -3.91].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 9 .</head><label>9</label><figDesc>Mediation analysis reveals that perceived prejudiced motivation mediates the effect of agent on liability judgments (Study 8). * Denotes p &lt; .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>examined cases of race (see "Study 2 Replication A: Race iscrimination" in the supplemental materials) and age discrimination (see "Study 2 Replication B: Age iscrimination" in the supplemental materials) and found the same pattern, showing generalizability beyond the type of discrimination. We used diverse samples including online panels from Mturk and Prolific, a quasi-representative sample (see "Study 2 Replication D: Quasi-Representative Sample" in the supplemental materials), and workers in high-tech companies (Study 5). Our samples include people from the US, UK, Canada, and Norway. This diversity of samples demonstrates the robustness of our findings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An independent samples t-test revealed that participants found it less permissible for the algorithm (M = 57.38, SD = 22.81) to screen resumes than the HR specialist (M = 73.37, SD = 20.32), t (456)=7.91, p &lt; .001, Cohen's d = 0.74, replicating previous work showing that people are averse to algorithms making certain high-stake decisions<ref type="bibr" target="#b13">(Bigman &amp; Gray, 2018)</ref>. A second independent samples t-test revealed that participants thought the algorithm was less motivated by prejudice (M = 61.74, SD = 29.44) than the human HR specialist (M = 68.95, SD = 28.89), t (456)= 2.65, p = .008, Cohen's d = 0.25, replicating our results from Studies 1-2. A third independent samples t-test did not find a difference between how fair participants thought the decisions were when an algorithm made them (M = 17.89, SD = 20.89) than when a human did (M = 20.32, SD = 22.98), t (452) 4 = 1.52, p =.129, failing to replicate previous showing that people find hiring decision by algorithms less fair than hiring decisions by humans</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We note that there is an ongoing debate on whether moral outrage is construct which is independent of other types of anger (e.g.,<ref type="bibr" target="#b8">Batson et al., 2009</ref><ref type="bibr" target="#b10">Batson et al., , 2007</ref><ref type="bibr" target="#b64">Hechler &amp; Kessler, 2018)</ref>. Our focus in this paper is the emotional response to discrimination as an outcome measure, rather than the construct validity of moral outrage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We initially pre-registered, ran, a sample of 240 participants. The result for moral outrage was not significant in a two-tailed t-test (p =.098). We decided to increase our sample size to see whether this initial non-significant result likely reflects a type II error, or the possibility that the algorithm outrage deficit does not exist in this context. Therefore, to test our hypotheses with more statistical power, we ran additional participants, aiming for a total of 800. We pre-registered adding the participants (https://aspredicted.org/3m8xa.pdf). All of the results hold when Bonferroni correcting for multiple comparisons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The degrees of freedom vary across these tests because some participants did not answer all of the questions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">As specified in the pre-registration, we started with 480 participants. However, one of the effects that was significant in our previous studies, the comparison between the "Algorithm-control" and the "Human HR specialist" conditions, was not significant in this sample (p = .115). In order to understand if this is a type I error in our previous studies or a type II error in this study, we ran an additional 480 participants, for increased statistical power. We pre-registered these additional participants, see https://aspredicted.org/4r5iq.pdf. We note that all tests are significant even after applying the Bonferroni correction for multiple comparisons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This study was conducted through a different lab's prolific account, with different payment norms.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
		<imprint>
			<date type="published" when="2018-04-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3173574.3174156</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174156" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Culpable control and the psychology of blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Alicke</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.126.4.556</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.126.4.556" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="556" to="574" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Toward an Ethics of Algorithms: Convening, Observation, Probability, and Timeliness. Science Technology and Human Values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ananny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="93" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0162243915606523</idno>
		<ptr target="https://doi.org/10.1177/0162243915606523" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lauren</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
		<title level="m">Machine Bias. ProPublica</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An automated supermarket checkout system utilizing a SCARA robot: preliminary prototype development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aquilina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saliba</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.promfg.2020.01.130</idno>
		<ptr target="https://doi.org/10.1016/j.promfg.2020.01.130" />
	</analytic>
	<monogr>
		<title level="m">Procedia Manufacturing</title>
		<meeting>edia Manufacturing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1558" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">US Patent 7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="345" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pursuing moral outrage: Anger at torture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Givens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="160" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jesp.2008.07.017</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2008.07.017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anger at unfairness: Is it moral outrage?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Nord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Stocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Marzette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Lishner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1272" to="1285" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1002/ejsp.434</idno>
		<ptr target="https://doi.org/10.1002/ejsp.434" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are Emily and Greg More Employable Than Lakisha and Jamal?: A Field Experiment on Labor Market Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780429499821-53</idno>
		<ptr target="https://doi.org/10.4324/9780429499821-53" />
	</analytic>
	<monogr>
		<title level="m">The American Economic Review</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="991" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">People are averse to machines making moral decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<idno type="DOI">10.1016/j.cognition.2018.08.003</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.08.003" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="21" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The road to heaven is paved with effort: Perceived effort amplifies moral judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tamir</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000230</idno>
		<ptr target="https://doi.org/10.1037/xge0000230" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1654" to="1669" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Holding Robots Responsible: The Elements of Machine Morality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alterovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="368" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tics.2019.02.008</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2019.02.008" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Threat of racial and economic inequality increases preference for algorithm decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marciano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2021.106859</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2021.106859" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hiring for the organization, not the job</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Ledford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Nathan</surname></persName>
		</author>
		<idno type="DOI">10.5465/ame.1991.4274747</idno>
		<ptr target="https://doi.org/10.5465/ame.1991.4274747" />
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">Executive</biblScope>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A genetic algorithm for scheduling staff of mixed skills under multicriteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S0377-2217(99</idno>
		<ptr target="https://doi.org/10.1016/S0377-2217(99" />
		<imprint>
			<biblScope unit="page" from="391" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple and better algorithm to solve the vendor managed inventory control system of multi-product multi-constraint economic order quantity model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Cárdenas-Barrón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Treviño-Garza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3888" to="3895" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.eswa.2011.09.057</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2011.09.057" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">How inferred motives shape moral judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Crockett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Manuscript under Review</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Good deeds gone bad: Lay theories of altruism and selfishness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="36" to="40" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jesp.2017.11.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2017.11.005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Uggerslev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Piasentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Applicant Attraction to Organizations and Job Choice: A Meta-Analytic Review of the Correlates of Recruiting Outcomes</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="928" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0021-9010.90.5.928</idno>
		<ptr target="https://doi.org/10.1037/0021-9010.90.5.928" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disease Prediction by Machine Learning Over Big Data From Healthcare Communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8869" to="8879" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ACCESS.2017.2694446</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2017.2694446" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcloughney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Njoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruppanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wirth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ethical Implications of AI Bias as a Result of Workforce Gender Imbalance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Pub. L. No. 78 Stat</title>
		<imprint>
			<biblScope unit="page">241</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
	<note>Civil Rights Act</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cosmides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="276" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/0010-0277</idno>
		<ptr target="https://doi.org/10.1016/0010-0277" />
		<imprint>
			<biblScope unit="page" from="90023" to="90024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nearly two decades ago, women across the country sued Walmart for discrimination. They&apos;re not done fighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Covert</surname></persName>
		</author>
		<ptr target="https://time.com/5586423/walmart-gender-discrimination/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crime and punishment: Distinguishing the roles of causal and intentional analyses in moral judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="380" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cognition.2008.03.006</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2008.03.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Amazon scraps secret AI recruiting tool that showed bias against women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dastin</surname></persName>
		</author>
		<ptr target="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Almost human: Anthropomorphism increases trust resilience in cognitive agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>De Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mckendrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000092</idno>
		<ptr target="https://doi.org/10.1037/xap0000092" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Strimling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Coultas</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2014.09.011</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2014.09.011" />
		<title level="m">Bidirectional associations between descriptive and injunctive norms. Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Perceived Discrimination and Depression among Mexican-Origin Adults in California</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kolody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Health and Social Behavior</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="313" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Taboo trade-offs: Reactions to transactions that transgress the spheres of justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Tetlock</surname></persName>
		</author>
		<idno type="DOI">10.1111/0162-895X.00058</idno>
		<ptr target="https://doi.org/10.1111/0162-895X.00058" />
	</analytic>
	<monogr>
		<title level="j">Political Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="297" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Problem of Abortion and the Doctrine of the Double Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oxford Review</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The Rise of the Robots: Technology and the Threat of Mass Unemployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Oneworld publications</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Racialized Mass Incarceration and the War on Drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Fornili</surname></persName>
		</author>
		<idno type="DOI">10.1097/JAN.0000000000000215</idno>
		<ptr target="https://doi.org/10.1097/JAN.0000000000000215" />
	</analytic>
	<monogr>
		<title level="j">Journal of Addictions Nursing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Consequences, norms, and generalized inaction in moral dilemmas: The CNI model of moral decisionmaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gawronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Friesdorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hütter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="376" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/pspa0000086</idno>
		<ptr target="https://doi.org/10.1037/pspa0000086" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Third generation sexism in workplaces: Evidence from India</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1080/12259276.2018.1496616</idno>
		<ptr target="https://doi.org/10.1080/12259276.2018.1496616" />
	</analytic>
	<monogr>
		<title level="j">Asian Journal of Women&apos;s Studies</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="387" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rage against the machine: Automation in the moral domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gogoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral and Experimental Economics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.socec.2018.04.003</idno>
		<ptr target="https://doi.org/10.1016/j.socec.2018.04.003" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Moral character predominates in person perception and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rozin</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0034726</idno>
		<ptr target="https://doi.org/10.1037/a0034726" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="168" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">eople&apos;s Explanations of Robot Behavior Subtly Reveal Mental State Inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">A</forename><surname>Graaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Human-Ro ot Interaction, HRI&apos;19</title>
		<meeting>the International Conference on Human-Ro ot Interaction, HRI&apos;19</meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dimensions of mind perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1134475</idno>
		<ptr target="https://doi.org/10.1126/science.1134475" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5812</biblScope>
			<biblScope unit="page">619</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feeling robots and human zombies: Mind perception and the uncanny valley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cognition.2012.06.007</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2012.06.007" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Costly third-party interventions: The role of incidental anger and attention focus in punishment of the perpetrator and compensation of the victim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gummerum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Van Dillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>López-Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2016.04.004</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2016.04.004" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="94" to="104" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The moral emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Affective Science</title>
		<editor>R. J. Davidson, K. R. Scherer, &amp; H. H. Goldsmith</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Peloton, Nike, Walmart and other brands get savaged online, but are fine in real life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halzack</surname></persName>
		</author>
		<ptr target="https://www.bloomberg.com/opinion/articles/2019-12-16/nike-peloton-walmart-etc-savaged-online-fine-in-real-life" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Applicant Reactions to Selection rocedures: An Updated odel and eta-Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personnel Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="683" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<idno type="DOI">10.1111/j.1744-6570.2004.00003.x</idno>
		<ptr target="https://doi.org/10.1111/j.1744-6570.2004.00003.x" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the difference between moral outrage and empathic anger: Anger about wrongful deeds or harmful consequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hechler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kessler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2018.03.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2018.03.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="270" to="282" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Artificial intelligence will help determine if you get your next job</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heilweil</surname></persName>
		</author>
		<ptr target="https://www.vox.com/recode/2019/12/12/20993665/artificial-intelligence-ai-job-screen" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Preference reversals between joint and separate evaluations of options: A review and theoretical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Hsee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Bazerman</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.125.5.576</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.125.5.576" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="576" to="590" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Could a rising robot workforce make humans less prejudiced? American Psychologist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000582</idno>
		<ptr target="https://doi.org/10.1037/amp0000582" />
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Assumptions About Algorithms&apos; Capacity for iscrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Laurin</surname></persName>
		</author>
		<idno type="DOI">10.1177/01461672211016187</idno>
		<ptr target="https://doi.org/10.1177/01461672211016187" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="page">014616722110161</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Beyond sacrificial harm: A two-dimensional model of utilitarian psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A C</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Earp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caviola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Crockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Savulescu</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000093</idno>
		<ptr target="https://doi.org/10.1037/rev0000093" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="164" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Good Firms, Good Targets: The Relationship between Corporate Social Responsibility, Reputation, and Activist Targeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.2079227</idno>
		<ptr target="https://doi.org/10.2139/ssrn.2079227" />
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<biblScope unit="page" from="12" to="30" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Diversity and discrimination: a look at complex bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kotkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">William and Mary Law Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1439" to="1500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">When Do People Want AI to ake ecisions ?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sinnott-Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First Annual AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society</title>
		<meeting>First Annual AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Audience effects on moralistic punishment☆</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurzban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Descioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Obrien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolution and Human Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.evolhumbehav.2006.06.001</idno>
		<ptr target="https://doi.org/10.1016/j.evolhumbehav.2006.06.001" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The long road to fairer algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-020-00274-3</idno>
		<ptr target="https://doi.org/10.1038/d41586-020-00274-3" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="issue">7793</biblScope>
			<biblScope unit="page" from="34" to="36" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Algorithmic bias? An empirical study of apparent genderbased discrimination in the display of stem career ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.2018.3093</idno>
		<ptr target="https://doi.org/10.1287/mnsc.2018.3093" />
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2966" to="2981" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Are liars ethical? On the tension between benevolence and honesty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jesp.2014.03.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2014.03.005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A genetic algorithm for robotic assembly line balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rubinovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shnits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="811" to="825" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.ejor.2004.07.030</idno>
		<ptr target="https://doi.org/10.1016/j.ejor.2004.07.030" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">From Trolley to Autonomous Vehicle: Perceptions of Responsibility and Moral Norms in Traffic Accidents with Self-Driving Cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<idno type="DOI">10.4271/2016-01-0164</idno>
		<ptr target="https://doi.org/10.4271/2016-01-0164" />
		<imprint>
			<date type="published" when="2016-04-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Consumer outrage: Emotional reactions to unethical corporate behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lindenmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schleer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pricl</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2011.09.022</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2011.09.022" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1364" to="1373" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Using Algorithms to Understand the Biases in Your Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<ptr target="https://hbr.org/2019/08/using-algorithms-to-understand-the-biases-in-your-organization" />
	</analytic>
	<monogr>
		<title level="m">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2018.12.005" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Resistance to Medical Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/jcr/ucz013</idno>
		<ptr target="https://doi.org/10.1093/jcr/ucz013" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Evolution of Morality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Machery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Moral Psychology Handbook</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/acprof:oso/9780199582143.003.0002</idno>
		<ptr target="https://doi.org/10.1093/acprof:oso/9780199582143.003.0002" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">It&apos;s not my fault: When and why attributions to prejudice protect self-esteem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ajor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="DOI">10.1177/0146167203029006009</idno>
		<ptr target="https://doi.org/10.1177/0146167203029006009" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="772" to="781" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A theory of blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guglielmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Monroe</surname></persName>
		</author>
		<idno type="DOI">10.1080/1047840X.2014.877340</idno>
		<ptr target="https://doi.org/10.1080/1047840X.2014.877340" />
	</analytic>
	<monogr>
		<title level="j">Psychological Inquiry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="186" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The folk concept of intentionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knobe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="101" to="121" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Which robot am I thinking about? The impact of action and appearance on people&apos;s evaluations of a moral robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Voiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/HRI.2016.7451743</idno>
		<ptr target="https://doi.org/10.1109/HRI.2016.7451743" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">No Rest for the Stigmatized: A Model of Organizational Health and Workplace Sexism (OHWS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Howansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11199-017-0755-x</idno>
		<ptr target="https://doi.org/10.1007/s11199-017-0755-x" />
	</analytic>
	<monogr>
		<title level="j">Sex Roles</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="697" to="708" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Moral outrage and pragmatism: Explanations for collective action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="484" to="496" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/0022-1031</idno>
		<ptr target="https://doi.org/10.1016/0022-1031" />
		<imprint>
			<biblScope unit="page" from="90039" to="90047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">From moral outrage to social protest: The role of psychological standing. The Psychology of Justice and Legitimacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Effron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Zak</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780203837658</idno>
		<ptr target="https://doi.org/10.4324/9780203837658" />
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="103" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Two paths to blame: Intentionality directs moral information processing along two distinct tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000234</idno>
		<ptr target="https://doi.org/10.1037/xge0000234" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="133" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Biased Algorithms Are Easier to Fix Than Biased People. The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html" />
		<imprint>
			<date type="published" when="2019-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Moral emotions as determinants of third-party punishment: Anger, guilt, and the functions of altruistic sanctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M A</forename><surname>Nelissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeelenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="543" to="553" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">When eliminating bias isn&apos;t fair: Algorithmic reductionism and procedural justice in human resource decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Harmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="149" to="167" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.obhdp.2020.03.008</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2020.03.008" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">The halo effect: Evidence for unconscious alteration of judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Nisbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="250" to="256" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<idno type="DOI">10.1037//0022-3514.35.4.250</idno>
		<ptr target="https://doi.org/10.1037//0022-3514.35.4.250" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Perceived Racial Discrimination , epression , and Coping : A Study of Southeast Asian Refugees in Canada Author ( s ): Samuel Noh , Morton Beiser , Violet Kaspar , Feng Hou and Joanna Rummens ublished by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaspar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rummens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Health and Social Behavior</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="207" />
			<date type="published" when="1999" />
			<publisher>American Sociological Association Stable UR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6464</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title/>
		<idno type="DOI">10.1126/science.aax2342</idno>
		<ptr target="https://doi.org/10.1126/science.aax2342" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Design approaches to experimental mediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Pirlott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.jesp.2015.09.012</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2015.09.012" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Bringing character back: How the motivation to evaluate character influences judgments of moral blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tannenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1037/13091-005</idno>
		<ptr target="https://doi.org/10.1037/13091-005" />
		<editor>M. Mikulincer &amp; P. R. Shaver</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>American Psychological Association</publisher>
			<biblScope unit="page" from="91" to="108" />
		</imprint>
	</monogr>
	<note>The social psychology of morality: Exploring the causes of good and evil</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Inferences about the morality of an aggressor: The role of perceived motive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Reeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hesson-Mcinnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trafimow</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.83.4.789</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.83.4.789" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="789" to="803" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Operation of the Laws of Sympathetic Magic in Disgust and Other Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nemeroff</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.50.4.703</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.50.4.703" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="703" to="712" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Moral anger, but not moral disgust, responds to intentionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giner-Sorolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/a0022598</idno>
		<ptr target="https://doi.org/10.1037/a0022598" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Applicants&apos; erceptions of Selection Procedures and Decisions: A Critical Review and Agenda for the Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="DOI">10.1177/014920630002600308</idno>
		<ptr target="https://doi.org/10.1177/014920630002600308" />
	</analytic>
	<monogr>
		<title level="j">Journal of Management</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="606" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">The Interactive Effect of Anger and Disgust on Moral Outrage and Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Salerno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Peter-Hagene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2069" to="2078" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0956797613486988</idno>
		<ptr target="https://doi.org/10.1177/0956797613486988" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">When the algorithm itself is a racist: Diagnosing ethical harm in the basic Components of Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Langbort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="4972" to="4990" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">The eyes are the window to the uncanny valley: Mind perception, autism and missing souls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interaction Studies</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="179" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title/>
		<idno type="DOI">10.1075/is.16.2.02sch</idno>
		<ptr target="https://doi.org/10.1075/is.16.2.02sch" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Mistaking minds and machines: How speech affects dehumanization and anthropomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000214</idno>
		<ptr target="https://doi.org/10.1037/xge0000214" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1427" to="1437" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The intuitive appeal of explainable machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3126971</idno>
		<ptr target="https://doi.org/10.2139/ssrn.3126971" />
	</analytic>
	<monogr>
		<title level="j">Fordham Law Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1085" to="1139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Attributions of morality and mind to artificial intelligence after real-world moral violations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Shank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.chb.2018.05.014</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2018.05.014" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Reform predictive policing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
		<idno type="DOI">10.1038/541458a</idno>
		<ptr target="https://doi.org/10.1038/541458a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">541</biblScope>
			<biblScope unit="issue">7638</biblScope>
			<biblScope unit="page" from="458" to="460" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">The relationship between perceived discrimination and Generalized Anxiety Disorder among African Americans, Afro Caribbeans, and non-Hispanic Whites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Dawson-Andoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Belue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Anxiety Disorders</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="258" to="265" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.janxdis.2010.09.011</idno>
		<ptr target="https://doi.org/10.1016/j.janxdis.2010.09.011" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">The Upside of Outrage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cikara</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.09.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2018.09.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1067" to="1069" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">When Algorithms Fail: Consumers&apos; Responses to Brand Harm Crises Caused by Algorithm Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="74" to="91" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0022242921997082</idno>
		<ptr target="https://doi.org/10.1177/0022242921997082" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Twitter complainer says Apple is to blame for credit card issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stankiewicz</surname></persName>
		</author>
		<ptr target="https://www.cnbc.com/2019/11/11/apple-shouldnt-pass-the-blame-on-gender-bias-says-complainant.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Assessing punitive damages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schkade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yale Law Journal</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="2071" to="2153" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A house cleaning robot system -Path indication and position estimation using ceiling camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takeshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tomizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ohya</surname></persName>
		</author>
		<idno type="DOI">10.1109/SICE.2006.315049</idno>
		<ptr target="https://doi.org/10.1109/SICE.2006.315049" />
	</analytic>
	<monogr>
		<title level="m">SICE-ICASE International Joint Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2653" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Feeling empathy for organizations: Moral consequences, mechanisms, and the power of framing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2021.104147</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2021.104147" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Social functionalist frameworks for judgment and choice: Intuitive politicians, theologians, and prosecutors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Tetlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title/>
		<idno type="DOI">10.1037//0033-295X.109.3.451</idno>
		<ptr target="https://doi.org/10.1037//0033-295X.109.3.451" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Recruiting Through the Stages: A Meta-Analytic Test of Predictors of Applicant Attraction at Different Stages of the Recruiting Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Uggerslev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Fassina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kraichy</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1744-6570.2012.01254.x</idno>
		<ptr target="https://doi.org/10.1111/j.1744-6570.2012.01254.x" />
	</analytic>
	<monogr>
		<title level="j">Personnel Psychology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="660" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A person-centered approach to moral judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Diermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="72" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/1745691614556679</idno>
		<ptr target="https://doi.org/10.1177/1745691614556679" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">When actions speak volumes: The role of inferences about moral character in outrage over racial bigotry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Diermeier</surname></persName>
		</author>
		<idno type="DOI">10.1002/ejsp.1987</idno>
		<ptr target="https://doi.org/10.1002/ejsp.1987" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">When it takes a bad person to do the right thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tannenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2012.10.005</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2012.10.005" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A solution method for a two-layer sustainable supply chain distribution model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Validi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="204" to="217" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cor.2014.06.015</idno>
		<ptr target="https://doi.org/10.1016/j.cor.2014.06.015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
		<title level="m">Transparent, Explainable, and Accountable AI for Robotics. Science Robotics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6080</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Who sees human? The stability and importance of individual differences in anthropomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691610369336</idno>
		<ptr target="https://doi.org/10.1177/1745691610369336" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">The mind in the machine: Anthropomorphism increases trust in an autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2014.01.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2014.01.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="113" to="117" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monteleone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Making sense by making sentient: effectance motivation increases anthropomorphism</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="410" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/a0020240</idno>
		<ptr target="https://doi.org/10.1037/a0020240" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Rethinking people&apos;s conceptions of mental life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1704347114</idno>
		<ptr target="https://doi.org/10.1073/pnas.1704347114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Emotion expression in human punishment behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Houser</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0502399102</idno>
		<ptr target="https://doi.org/10.1073/pnas.0502399102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="7398" to="7401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Robots at work: People prefer-and forgive-service robots with perceived feelings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ilies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1037/apl0000834</idno>
		<ptr target="https://doi.org/10.1037/apl0000834" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Autonomous morals: Inferences of mind predict acceptance of AI behavior in sacrificial moral dilemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Monroe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2019.103870</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2019.103870" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Design AI so that its fair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schiebinger</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-018-05707-8</idno>
		<ptr target="https://doi.org/10.1038/d41586-018-05707-8" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="issue">7714</biblScope>
			<biblScope unit="page" from="324" to="326" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
