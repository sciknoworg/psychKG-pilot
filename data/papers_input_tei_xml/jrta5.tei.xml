<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Words: The Impact of Eye-gaze Sharing on Collaboration and Cognitive Load</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvodip</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Future Resilient Systems</orgName>
								<orgName type="institution">Singapore-ETH Centre</orgName>
								<address>
									<addrLine>1 CREATE Way</addrLine>
									<postCode>138602</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kiefer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Cartography and Geoinformation</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Stefano-Franscini-Platz 5</addrLine>
									<postCode>8093</postCode>
									<settlement>Zurich, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raubal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Cartography and Geoinformation</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Stefano-Franscini-Platz 5</addrLine>
									<postCode>8093</postCode>
									<settlement>Zurich, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Words: The Impact of Eye-gaze Sharing on Collaboration and Cognitive Load</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaboration</term>
					<term>N-back</term>
					<term>Eye-tracking</term>
					<term>Cognitive load</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Distributed cognition systems rely on collaboration among decision-makers to address complex problems that are challenging for a single individual. The success of collaborative decision-making hinges on effective communication, enabling stakeholders to develop a shared understanding of both the scenario and the decision-making process. However, traditional decision support systems still predominantly rely on verbal communication, which tends to lose efficacy as cognitive load increases. This study aims to leverage gazesharing to enhance collaboration under high cognitive load conditions. We used a gamified N-back task to test across three communication conditions: speech-sharing, gaze-sharing, and a hybrid approach combining both gazeand-speech sharing. Results, including NASA-TLX workload scores and pupil size measurements, reveal that the real-time gaze-sharing condition leads to lower cognitive load and lower reaction times compared to speech or the hybrid approach, particularly as task difficulty increases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction 1</head><p>In control rooms and other professional scenarios, multiple decision-makers 2 collaborate in order to reach a joint decision, for which they need to reach 3 situational awareness, often under time pressure <ref type="bibr" target="#b13">(Chakraborty et al., 2022;</ref><ref type="bibr"></ref> 4 <ref type="bibr" target="#b66">Robison and Unsworth, 2019)</ref>. This can be achieved through a combination 5 of technologies, processes, and training. One key technology is the use of con-6 trol displays that provide real-time information on the status of the system 7 Preprint submitted to International Journal of <ref type="bibr">Human-Computer StudiesJanuary 16, 2025</ref> This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d and its components <ref type="bibr" target="#b71">(Starke et al., 2017)</ref>. The decision-makers must visually explore the shared and individual displays, make sense of the information available, and find the critical information.</p><p>The vast amounts of complex data, time pressure, and differences in expertise necessitate collaboration among stakeholders <ref type="bibr" target="#b13">(Chakraborty et al., 2022)</ref>. Control rooms rely on different communication techniques, i.e. verbal communication among team members, to ensure collaboration. The theoretical foundation for this experiment rests on the concept of distributed cognition <ref type="bibr" target="#b74">(Van Veen et al., 2020)</ref>. This framework posits that cognitive processes are not solely confined to individual minds but are distributed across the environment and interactions with others. In collaborative decision-making tasks, information is often dispersed among team members, necessitating effective communication to integrate and utilize this information effectively.</p><p>The current state-of-the-art control rooms still use speech as a communication medium to facilitate collaboration between control room personnel <ref type="bibr" target="#b54">(Monahan, 2013)</ref>. However, researchers have shown that speech communication amongst stakeholders in high-stress scenarios breaks down (Phillips-Wren and Adya, 2020); therefore, our research focuses on examining the effects of gaze-sharing to share information for helping decision-makers develop shared situational awareness. Gaze-sharing, which allows individuals to see each other's eye gaze in real-time, has emerged as a promising tool for enhancing collaboration and communication. By providing a visual channel for nonverbal cues, gaze-sharing can improve shared understanding, attention alignment, and social interaction <ref type="bibr" target="#b35">(John et al., 2014;</ref><ref type="bibr" target="#b0">Akkil et al., 2018)</ref>.</p><p>In various domains, research has demonstrated the benefits of gaze-sharing.</p><p>For instance, in collaborative learning, studies have shown that gaze-sharing can support joint attention and encourage more active and meaningful discussions, leading to deeper understanding and improved problem-solving abilities <ref type="bibr" target="#b35">(John et al., 2014)</ref>. Similarly, in remote design meetings, gaze-sharing can facilitate better coordination, faster decision-making, and more innovative solutions <ref type="bibr" target="#b8">(Cai and Tanaka, 2019;</ref><ref type="bibr" target="#b0">Akkil et al., 2018)</ref>. Additionally, gaze-sharing is beneficial for collaboration in domains such as coding and puzzle-solving, where it can improve problem-solving performance and efficiency <ref type="bibr" target="#b8">(Cai and Tanaka, 2019;</ref><ref type="bibr" target="#b35">John et al., 2014;</ref><ref type="bibr" target="#b34">James, 2009;</ref><ref type="bibr" target="#b17">Cheng et al., 2022;</ref><ref type="bibr" target="#b55">Müller et al., 2019)</ref>.</p><p>Contrary to the commonly held belief that gaze sharing enhances team performance, there are instances where it has failed to deliver these benefits <ref type="bibr" target="#b52">(McCarley et al., 2021;</ref><ref type="bibr" target="#b2">Atweh and Riggs, 2024)</ref>. <ref type="bibr">For example,</ref><ref type="bibr">McCarley 2</ref> This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d <ref type="bibr">et al. (2021)</ref> found that sharing gaze information did not improve target detection accuracy in collaborative tasks, suggesting that gaze cues may not always align with the cognitive demands of the task. Similarly, <ref type="bibr" target="#b2">Atweh and Riggs (2024)</ref> reported no significant improvement in team performance when gaze sharing was used as a communication tool, highlighting the need for a nuanced understanding of its application.</p><p>In general, decision-makers in control rooms often operate under substantial cognitive load due to the high demand for working memory and the need to process large amounts of information <ref type="bibr" target="#b12">(Chakraborty et al., 2021)</ref>. This challenge is further amplified in collaborative scenarios, where cognitive demands are shared and compounded by the need to coordinate effectively with others.</p><p>Furthermore, collaborative cognitive load theory by <ref type="bibr" target="#b45">Kirschner et al. (2018)</ref> identifies support as a key factor influencing cognitive load during team tasks.</p><p>This raises a vital question: Can gaze or speech sharing effectively serve as a supportive mechanism under varying cognitive load conditions? Therefore, investigating the relationship between communication and collaborative performance with increasing cognitive demand becomes crucial to collaboration success.</p><p>More specifically, this study aims to investigate the effectiveness of different HCI techniques in supporting collaboration among decision-makers under increasing levels of cognitive load. The main contribution of this paper is to present a novel investigation of the impact of different communication techniques -individual, speech-sharing, gaze-sharing, and hybrid conditionson reaction times, and cognitive load in a collaborative, image-based N-back task. By integrating eye-tracking data and self-reported metrics, the study comprehensively analyses how gaze, speech, and hybrid (gaze and speech) sharing affect cognition under high cognitive load.</p><p>Traffic control rooms are one of the most frequently used type of control rooms around the world, and will be used as a use case in this study.</p><p>Traffic control rooms typically use multiple video cameras at various points throughout the road network to capture real-time traffic data. The operators in traffic control rooms regularly track suspicious vehicles, manage traffic jams, and check the vehicle registrations <ref type="bibr" target="#b81">(Zakria et al., 2021)</ref>. Modern vehicle re-identification systems use both video and spatio-temporal features for improved detection accuracy <ref type="bibr" target="#b78">(Yang et al., 2022)</ref>. The task used in our study follows a similar paradigm as that described as state of the art by <ref type="bibr" target="#b80">Yang et al. (2023)</ref>, in which a pre-filtered list of candidate vehicles is presented to 3 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d operators who then make a final decision in identifying a vehicle.</p><p>The following section positions our work within the literature (section 2). The Methods section provides details on the experimental design and data collection (section 3), followed by the results in section 4. Finally, the discussion and conclusion (section 5) interprets these results in relation to existing research and summarizes the contributions. The outlook and future scope (section 6) highlight the implications for future applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>The concept of collaboration in cognitive tasks has its roots in social learning theory and shared cognition <ref type="bibr" target="#b42">(Kirschner et al., 2009)</ref>. Vygotsky's theories of social constructivism emphasized the role of social interaction in learning and cognitive development, positing that group collaboration can lead to higher levels of thinking than individuals might achieve alone <ref type="bibr" target="#b18">(Cole and Scribner, 1978;</ref><ref type="bibr" target="#b51">Mahanama et al., 2023)</ref>. This foundational idea has driven much of the research on collaborative learning, which suggests that working with others can improve problem-solving, creativity, and memory retention <ref type="bibr" target="#b3">(Barron, 2003;</ref><ref type="bibr" target="#b36">Johnson and Johnson, 1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Collaborative Tasks</head><p>Collaborative tasks differ from individual tasks in cognitive load distribution and the social interactions required for success. In collaborative tasks, participants must communicate effectively, synchronize their responses, and negotiate task strategies, which involves cognitive and social processes <ref type="bibr" target="#b47">(Kolfschoten et al., 2014)</ref>. For instance, research by <ref type="bibr" target="#b20">Cooke et al. (2024)</ref>highlights how team cognition, the process by which groups construct shared mental models, can be essential in complex tasks. In a collaborative setting, group members can improve performance by sharing memory and attentional resources, leading to improved performance on more complex versions of the task <ref type="bibr" target="#b42">(Kirschner et al., 2009)</ref>. Additionally, social factors such as trust, coordination, and shared goals play a crucial role in the success of collaborative tasks. Empirical evidence suggests that when participants perceive their teammates as competent and trustworthy, they are more likely to engage in effective collaboration and communication, further enhancing task performance <ref type="bibr" target="#b67">(Salas et al., 2008)</ref>.</p><p>However, there are also potential downsides to collaboration. Research on "social loafing" suggests that individuals may put in less effort when 4</p><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d working in groups, assuming that others will compensate for their lack of engagement <ref type="bibr" target="#b50">(Latané et al., 1979)</ref>. This phenomenon could decrease individual accountability in a vehicle identification task such as ours. Similarly, miscommunication or poorly coordinated efforts could result in cognitive overload that outweighs the expected benefits of shared cognition. A study by <ref type="bibr" target="#b77">Weldon and Bellinger (1997)</ref> has pointed out that groups are not always superior to individuals in cognitive tasks, especially when group dynamics are dysfunctional. However, a previous paper from <ref type="bibr" target="#b43">Kirschner et al. (2011)</ref> hypothesized that collaboration could be helpful in instances with high cognitive load and working memory requirements. We used the collaborative task criterion proposed by Kolfschoten, which stated that both participants should agree to the same response based on the information available <ref type="bibr" target="#b47">(Kolfschoten et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gaze and speech sharing in collaborative tasks</head><p>Gaze is a powerful non-verbal cue that supports collaborative efforts by allowing participants to direct attention, coordinate focus, and predict others' actions <ref type="bibr" target="#b51">(Mahanama et al., 2023)</ref>. Research on shared gaze, or gaze awareness, has demonstrated its positive impact on group performance across various settings, including educational and workplace environments <ref type="bibr" target="#b7">(Brennan et al., 2008;</ref><ref type="bibr" target="#b64">Richardson and Dale, 2005;</ref><ref type="bibr" target="#b57">Nyström et al., 2017)</ref>. Gaze sharing allows group members to establish joint attention, which is critical for ensuring that all participants are focused on the same task or object. In collaborative problem-solving tasks, joint attention helps participants align their mental models, leading to more effective coordination and communication <ref type="bibr" target="#b7">(Brennan et al., 2008;</ref><ref type="bibr" target="#b1">Atweh et al., 2023;</ref><ref type="bibr" target="#b51">Mahanama et al., 2023)</ref>.  <ref type="bibr" target="#b3">(Barron, 2003)</ref>. Verbal interactions allow collaborators to articulate their thoughts and formulate strategies, which leads to a more transparent and coordinated approach to task completion.</p><p>The integration of gaze sharing and speech creates a powerful synergy that enhances collaborative decision-making <ref type="bibr" target="#b7">(Brennan et al., 2008)</ref>. Research by <ref type="bibr" target="#b5">Bee et al. (2009)</ref> investigated how gaze awareness and speech interact to support collaboration in multimodal systems. Their findings revealed that the combination of gaze and speech cues improved task performance by facilitating quicker turn-taking and smoother transitions between collaborative actions. Although the effectiveness of gaze and speech sharing has been studied in detail, the effects of cognitive load on gaze and speech sharing in collaborative tasks have not been explored yet. Therefore, we are considering speech sharing, gaze sharing, and a hybrid approach to promote collaboration in this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Working Memory and Cognitive Load in Collaboration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Working memory and N-back tasks</head><p>Working memory, which allows for the temporary storage and manipulation of information, plays a critical role in collaborative tasks <ref type="bibr" target="#b40">(Kirchner, 1958)</ref>. N-back tasks are considered the gold standard measure of working memory <ref type="bibr" target="#b33">(Jacola et al., 2014)</ref>. In image-based N-back tasks, the participants are asked to identify if a stimulus presented matches one shown "n" steps earlier, with "n" representing a variable difficulty level based on how far back participants must recall the stimulus <ref type="bibr" target="#b75">(Walter and Bex, 2021)</ref>. The natural scene-based N-back task has been used to assess cognitive load, attention, and memory capacity, particularly in individual contexts <ref type="bibr" target="#b75">(Walter and Bex, 2021)</ref>. <ref type="bibr" target="#b68">Scharinger et al. (2023)</ref> associated gamified N-back tasks with fewer erratic gaze shifts, which may indicate that participants experienced less mental fatigue and could focus more effectively on critical elements of the task. This aligns with research suggesting that gamification enhances motivation and cognitive engagement, leading to more efficient task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>While research on individual N-back tasks is robust, studies investigating 6</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d collaborative N-back tasks and how to support collaboration under cognitive load are limited.</p><p>In individual N-back tasks, participants rely solely on their cognitive resources, while in collaborative N-back tasks, the load is distributed across group members <ref type="bibr" target="#b51">(Mahanama et al., 2023)</ref>. This introduces a new dimension: shared working memory. Research by <ref type="bibr" target="#b70">Sepp et al. (2019)</ref> showed that distributing the cognitive load across group members could improve performance, as each person can focus on different aspects of the task. These findings suggest that in a collaborative N-back task, group members may enhance one another's working memory performance through a coordinated effort, mutual feedback, and shared strategies <ref type="bibr" target="#b70">(Sepp et al., 2019;</ref><ref type="bibr" target="#b48">Kolfschoten and Brazier, 2013)</ref>. Therefore, in this paper, we wanted to explore the effects of interaction methods on collaboration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Cognitive load and eye-tracking</head><p>Cognitive load refers to the mental resources expended by an individual to process, understand, and complete a task <ref type="bibr" target="#b73">(Sweller et al., 1998)</ref>. It encompasses two dimensions: a task-based dimension, known as mental workload, which focuses on the mental resources allocated to the task (measured through pupil-based metrics and maximum N-back levels) and a person-based dimension, referred to as cognitive workload, which considers individualspecific factors (measured through self reported questionnaires) <ref type="bibr" target="#b47">(Kolfschoten et al., 2014;</ref><ref type="bibr" target="#b49">Kosch et al., 2023)</ref>. Analyzing both dimensions provides a comprehensive understanding of performance by examining how task demands interact with individual capacities <ref type="bibr" target="#b47">(Kolfschoten et al., 2014)</ref>. While Cognitive Load Theory (CLT) focuses on the individual's cognitive architecture, Collaborative Cognitive Load Theory (CCLT) builds upon the foundation of CLT, emphasising shared cognitive resources between individuals working together on a task. In collaboration, cognitive load is not only affected by individual task demands but also by the need to coordinate, communicate, and share information effectively among team members <ref type="bibr" target="#b45">Kirschner et al. (2018)</ref>.</p><p>This extension is particularly relevant in complex, high-stakes environments, such as control rooms, where coordination and cognitive load management can significantly impact performance <ref type="bibr" target="#b46">(Knisely et al., 2020)</ref>. Increased cognitive load has also been shown to affect blink parameters, such as blink rate, latency, and duration. <ref type="bibr" target="#b25">Goldstein et al. (1992)</ref> demonstrated that task complexity leads to a noticeable rise in blink frequency and duration. Recent research further supports blink rate as an indicator of dopaminergic activity, closely linked to working memory and perceived cognitive load, highlighting the potential of blink parameters as proxies for neural activity <ref type="bibr" target="#b23">(Eckstein et al., 2017)</ref>.</p><p>Since the 1960s, researchers have investigated pupillary responses as indicators of cognitive load <ref type="bibr" target="#b32">(Hess and Polt, 1960)</ref>. Pupillary activity correlates strongly with the locus coeruleus-noradrenergic (LC-NA) system, which governs various cognitive processes. Studies have found that increased cognitive load is associated with a rise in pupil diameter <ref type="bibr" target="#b37">(Kiefer et al., 2016)</ref> and changes in metrics like pupil deviation, maximum pupil dilation, pupil entropy <ref type="bibr" target="#b61">(Piu et al., 2019)</ref>, the difference between left and right pupil diameter <ref type="bibr" target="#b14">(Chakraborty et al., 2024)</ref>, and the low/high index of pupillary activity (LHIPA) <ref type="bibr" target="#b22">(Duchowski et al., 2020)</ref>.</p><p>In particular, studies have demonstrated that different pupil-based metrics are highly sensitive to different levels of task difficulty. For example, <ref type="bibr" target="#b60">Pillai et al. (2020)</ref> showed that pupil size increases as participants engage with progressively challenging tasks, such as the N-back task, which requires working memory and attentional resources. This sensitivity to task difficulty allows researchers to track cognitive load dynamically in real-time <ref type="bibr" target="#b4">(Bauer et al., 2022)</ref>.</p><p>Recent articles used maximum pupil dilation to evaluate user interfaces and systems, helping researchers understand how design elements contribute to or mitigate cognitive load <ref type="bibr" target="#b65">(van Rij et al., 2019;</ref><ref type="bibr">Cui, 2022, 2024)</ref>. Maximum pupil dilation has become a reliable metric in cognitive load research related to usability and HCI studies, offering insights into how humans process and respond to complex information <ref type="bibr" target="#b53">(Mitre-Hernandez et al., 2021;</ref><ref type="bibr" target="#b4">Bauer et al., 2022;</ref><ref type="bibr" target="#b84">Zhang and Cui, 2024;</ref><ref type="bibr" target="#b65">van Rij et al., 2019)</ref>.</p><p>In contrast to other experimental paradigms, our task requires a time-8</p><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d invariant measure of cognitive load as the acquired eye tracking data's length varies with the response time. Blink and saccadic rates are inherently less sensitive than pupil-based metrics (see <ref type="bibr" target="#b26">(Gorin et al., 2024)</ref>). This makes it difficult to draw consistent conclusions across trials or subjects. Mean pupil dilation is sensitive to the duration of the data. A larger window might smooth out transient changes, leading to an underrepresentation of taskinduced cognitive load (see van Rij et al. <ref type="formula">2019</ref>for more details). LHIPA, a metric derived from wavelet analysis of pupil diameter, relies on capturing variations in low-and high-frequency components over a consistent time frame. When the window size varies, the wavelet components might get distorted, making it challenging to calculate. Unlike the methods mentioned earlier, maximum pupil dilation provides a snapshot of the highest cognitive load experienced during a task, independent of window size, making it more robust for comparisons across trials with varying durations. Therefore, we have used maximum pupil dilation to measure cognitive load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This study employs a within-subject design, where the independent variable was the interaction method used to assist decision-makers. The study had four conditions: in the Individual (I) condition, participants completed the task without collaboration; in the speech-sharing (S) condition, they communicated by speaking aloud only; in the Gaze sharing (ET) condition, real-time gaze-sharing provided participants with a visual overlay of their partner's gaze on the screen. In the Hybrid (ETS) condition, participants had access to both speech and gaze sharing. The study was approved by the IRB of ETH Zurich <ref type="bibr">(number EK 2023-N-192)</ref>. Participants were compensated for their time with 30 Singapore dollars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Participants</head><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Setup</head><p>Before the main study, we conducted a pre-study to validate the setup and to calibrate the task difficulty. Based on the pre-study, we were able to assess the environmental conditions, including setup latency (ensuring it was &lt;2 ms), presentation times, and lighting conditions (maintained at 120 lux).</p><p>The experiment was conducted in a controlled laboratory setting to minimize distractions, maintaining sound levels below 60 dB. <ref type="figure" target="#fig_2">Figure 1</ref> depicts participants seated at individual workstations equipped with eye trackers mounted below the screen, ensuring a consistent viewing distance (60cm). The participants were separated using a wall to ensure they would not divert their attention to each other in speech or hybrid conditions while not hindering verbal communication.</p><p>The setup consisted of two computers with the same specifications (Intel Core i7-8700 processors, NVIDIA GTX 1080 Ti graphics cards, Samsung S27 monitors 1,920 x 1,080 resolution, a standard mouse and keyboard) and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d Tobii Pro Fusion 120Hz eye trackers (with 0.04 • precision), as illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>. The stimuli files were preloaded into Psychopy before each trial in order to ensure smooth visual presentation and accurate real-time eyetracking <ref type="bibr" target="#b58">(Peirce, 2007)</ref>.</p><p>A third computer on the same network, with identical specifications but without an eye tracker, was running Lab Streaming Layer (LSL), to record, share real-time eye-tracking data, and manage synchronization across systems <ref type="bibr" target="#b76">(Wang et al., 2023)</ref>. This ensured accurate performance and communication data tracking, allowing us to collect eye-tracking and behavioural data during the trials.</p><p>The setup also included Galvanic Skin Response (GSR) sensors whose data were also synchronized with LSL. However, data from the GSR was affected by interference from keyboard tapping and hand movements as participants' movements were not restricted, introducing noise and inaccuracies in the measurements. Therefore, we did not use the GSR data in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Task and Material</head><p>The primary task was an image-based N-back task adapted to vehicle reidentification ( <ref type="figure" target="#fig_3">Figure 2</ref>), designed to assess working memory and cognitive load, based on previous research by <ref type="bibr" target="#b75">Walter and Bex (2021)</ref>. The participants were presented with a series of traffic images (scene viewing task) and, in a forced-choice format, identified a vehicle present in the scene "N" steps earlier (choice task). Participants received assistance based on four conditions during both, the traffic scene viewing and the forced-choice task (see <ref type="figure" target="#fig_3">Figure   2</ref>).</p><p>The stimuli used in the scene viewing task consisted of traffic scenes depicting 2 to 3 cars (we chose this number based on the pre-study), as seen from the perspective of a traffic camera. We utilized the traffic simulator CARLA built with Unreal Engine to generate virtual street scenarios and vehicle images to maintain consistency in weather and visibility conditions <ref type="bibr" target="#b21">(Dosovitskiy et al., 2017)</ref>. This approach allowed us to avoid confounding variables, such as changes in weather, illumination, and colour shifts, while precisely controlling the number of vehicles in each scene. In the scene viewing task, the participants were asked to memorize all the cars in the given traffic scene.</p><p>For the forced-choice task, the participants were shown four cars, arranged in a 2x2 grid, one of which had been present in a traffic scene N steps prior. The remaining three cars were randomly selected, ensuring different</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d car types, models, and colours and a consistent background that matched the traffic scene. Participants were instructed to identify which vehicle among the four was appearing "n" steps back in the previous scenes. For instance, in a 2-back condition, participants would select the vehicle that had appeared two traffic scenes earlier. Each trial concluded with feedback on the participant's response, displaying either a green checkmark or a red cross for one second.</p><p>In conditions with gaze sharing (ET, ETS), each participant saw the other participant's gaze as an overlay in both tasks, using ring-type gaze marker, identified by <ref type="bibr" target="#b85">Zhang et al. (2017)</ref> as the most effective one for conveying information in collaborative search tasks. To reduce the visual noise caused by gaze jitter, we smoothed the gaze data using a moving average filter, as described in <ref type="bibr" target="#b85">Zhang et al. (2017)</ref>. for their first trial of collaborative decision-making. This experiment used a Latin-square design to counterbalance the conditions <ref type="bibr" target="#b38">(Kim and Stein, 2009)</ref>.</p><p>Then, the first trial of the N-back task started. <ref type="figure" target="#fig_3">Figure 2</ref> shows the sequence of screens with an example. The scene-viewing task lasted 10 seconds, followed by a choice task (see <ref type="figure">Figure 3)</ref>. Depending on the condition, they could do the task only by themselves (I), either discuss the task verbally (S), or use gaze data (ET) or both (ETS), to reach a decision. A trial ended when one participant submitted their response via mouse click in collaboration conditions, or when both participants had answered separately in condition I <ref type="bibr" target="#b48">(Kolfschoten and Brazier, 2013;</ref><ref type="bibr" target="#b47">Kolfschoten et al., 2014)</ref>.</p><p>The N-back level increased after two successful trials, and decreased by one in case of one incorrect response. The condition ended when participants made two consecutive incorrect responses <ref type="bibr" target="#b75">(Walter and Bex, 2021)</ref>. This approach allowed for the progressive buildup of cognitive load, as higher N-levels require larger working memory, effectively simulating real-world collaborative decision-making demands under varying cognitive load levels <ref type="bibr" target="#b33">(Jacola et al., 2014)</ref>. At the end of each condition, participants completed two questionnaires (see subsection 3.5). Participants completed multiple rounds of the N-back task, with each round corresponding to a different condition. After the completion of all four conditions, the experiment ended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Measures</head><p>The dependent variables in this experiment were the cognitive load and response times across the four conditions. Cognitive load was measured in both dimensions: in task dimension (mental workload) through both the 13 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d maximum N-back level achieved <ref type="bibr" target="#b75">(Walter and Bex, 2021)</ref> and maximum pupil dilation <ref type="bibr" target="#b65">(van Rij et al., 2019;</ref><ref type="bibr" target="#b84">Zhang and Cui, 2024)</ref>, and in the person-based dimension (cognitive workload) through NASA Task Load Index (NASA-TLX) <ref type="bibr" target="#b29">(Hart and Staveland, 1988)</ref>. Following the research by <ref type="bibr" target="#b83">Zhang and Cui (2022)</ref> into the relationship between maximum pupil diameter and N-back tasks, we analysed the left and right pupil diameter separately.</p><p>Furthermore, as usability is a major component influencing cognitive load, we used the User Experience Questionnaire (UEQ) at the trial's conclusion to identify potential issues related to the round's collaboration method. <ref type="bibr" target="#b29">(Hart and Staveland, 1988;</ref><ref type="bibr" target="#b69">Schrepp et al., 2014)</ref>.</p><p>Behavioural metrics consisted of reaction times, which served as an indicator of participants' efficiency. Final reaction time and decision-making outcomes were computed based on mouse presses during the choice task. The task timer started when the choice task began, and ended when the participant's mouse pressed on one of the four available options. The participants had their own N-level counter for the individual condition (I), as one might have dropped out while the other continued. Therefore, the metrics for the I condition were determined based on the group member who reached the highest N-level. <ref type="table" target="#tab_1">Table 1</ref> presents the number of groups for each condition which reached certain N-levels. Eye-tracking data for scene viewing and choice tasks, mean reaction time metrics, maximum N-level reached, NASA-TLX, and UEQ data were analyzed across conditions using the Wilcoxon signed-rank test, as the data was non-normal <ref type="bibr" target="#b39">(Kiprijanovska et al., 2023;</ref><ref type="bibr" target="#b86">Zimmerman and Zumbo, 1993;</ref><ref type="bibr" target="#b63">Proudfoot et al., 2018)</ref>. The False Discovery Rate (FDR) method corrected all hypothesis testing for multiple comparisons, with pvalues indicated in the figures as follows: * for 0.01 to 0.05, ** for 0.001 to 0.01, *** for 0.0001 to 0.001, and **** for p-values less than 0.0001.  <ref type="bibr" target="#b24">(Gehan, 1965)</ref>. Significant differences in maximum pupil dilation were found at <ref type="bibr">N-level 0, no-14</ref> This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d   This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d levels, significant differences in maximum pupil dilation were also observed, particularly between the I and S and I and ETS conditions. <ref type="figure" target="#fig_5">Figure 5</ref> provides a visual representation of these findings, illustrating variations in maximum pupil dilation across different N-levels.  in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="figure" target="#fig_6">Figure 6</ref> (left), the maximum N-back level achieved by participants varied for each condition, with the collaborative conditions (S, ET, and ETS) showing significantly higher maximum n-levels than the individual (I)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Maximum Pupil Dilation and Cognitive Load</head><p>condition. This suggests that participants performed better in the conditions where collaborative or hybrid communication strategies were implemented.</p><p>Furthermore, while both S and ETS conditions demonstrated enhanced performance compared to I, the difference between these two conditions was also significant (p &lt; 0.05). This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d  Wilcoxon statistics for each condition pair, highlighting significant differences. Notably, highly significant differences in RTs were observed between the individual (I) and collaborative conditions (S, ET and ETS), suggesting that task performance varied when participants collaborated compared to when they did the task alone. The test further revealed a significant difference in RTs between the S and ET conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">User Experience</head><p>Participants rated various dimensions of the task, including clarity, comfort, completeness, ease, effectiveness, and satisfaction. No significant effects were observed across the UEQ dimensions (refer to <ref type="figure" target="#fig_8">Figure 8</ref>). Such differences at N-level 0 might result from differences in scanning patterns at lower cognitive load while using ET. This suggests that cognitive load varies not only on the task's difficulty but also on the collaborative aid employed, which is in line with previous research <ref type="bibr" target="#b70">(Sepp et al., 2019;</ref><ref type="bibr" target="#b43">Kirschner et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>The choice task revealed significant differences in maximum pupil dilation between the I and other collaborative aids, suggesting that the collaborative communication strategy elicited a significantly higher cognitive load. While insignificant, the participants in the ET condition demonstrated lower maximum pupil dilation compared to those in the speech and hybrid condition, indicating that combining verbal and hybrid approaches may incur more cognitive load.</p><p>In very high N-back levels (above 3), there was not enough data available</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d for statistically significant outcomes. However, the analysis of pupil dilation during N-back tasks revealed variations in cognitive load that were consistent across both left and right eyes. These findings reinforce the reliability of pupil dilation as a measure of cognitive load <ref type="bibr" target="#b84">(Zhang and Cui, 2024)</ref>, suggesting that physiological responses can provide valuable insights into the underlying cognitive processes during collaborative tasks.</p><p>The analysis of maximum N-back levels achieved by participants across different conditions revealed significant differences, emphasizing the impact of collaborative strategies on cognitive engagement. Participants in the collaborative conditions (S, ET, and ETS) consistently reached higher maximum N-back levels than those in the individual condition (I). This suggests that using collaborative communication strategies enhances performance, allowing participants to manage the increasing complexity of the task more effectively.</p><p>These findings align with previous research into collaborative tasks like visual search and puzzle solving, where collaborative strategies have been shown to improve performance <ref type="bibr" target="#b1">(Atweh et al., 2023;</ref><ref type="bibr" target="#b51">Mahanama et al., 2023)</ref>.</p><p>In addition to maximum N-back levels, the mean reaction times illustrate that participants in the individual condition exhibited faster reaction time than those in the collaborative conditions. Among collaborative conditions, eye-gaze-sharing condition is faster than the speech-sharing and hybrid conditions. Eye gaze offered distinct results in terms of reaction time compared to other collaborative strategies. In addition to this, the results indicate significant differences in cognitive processing across conditions. This might suggest that while collaboration can enhance overall performance, it may also incur additional cognitive load that impacts response speed. The presence of additional stimuli (gaze marker) and the necessity for coordination in collaborative tasks likely led to slower reaction time, as participants needed to integrate and respond to shared information from others. This is consistent with the literature, which suggests that speech becomes less effective as cognitive load increases, particularly in high-stress or complex tasks (?).</p><p>The NASA-TLX scores corroborated these findings by highlighting perceived mental demand between I and ET. Participants reported higher mental demand in the collaborative conditions, consistent with the increased pupil dilation observed during the choice task for these conditions. This perceived workload aligns with the eye-tracking indicators of cognitive load, as higher cognitive demands are associated with increased task complexity, the necessity for communication, and information sharing in collaborative scenarios.</p><p>The UEQ results did not reveal significant differences across the various</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d conditions, suggesting a consistent perceived user experience regardless of the collaboration method employed. This finding aligns with previous research indicating that subjective user experience may not always correlate with cognitive load <ref type="bibr" target="#b31">(Hassenzahl, 2013)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Outlook and Future scope</head><p>Gaze sharing can help multiple individuals within the control room have a shared understanding of the visual information <ref type="bibr" target="#b57">(Nyström et al., 2017)</ref>. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d group's shared knowledge may facilitate collaborative decision-making as op-</p><p>erators can simultaneously discuss and analyze the same visual data, which might be available on different screen positions for other users <ref type="bibr" target="#b51">(Mahanama et al., 2023;</ref><ref type="bibr" target="#b57">Nyström et al., 2017)</ref>. Control rooms can use such technologies for training purposes, particularly for new operators or during onboarding.</p><p>Experienced operators can share their gaze with trainees, allowing them to observe and learn from the expert's visual focus and decision-making processes <ref type="bibr" target="#b10">(Castner et al., 2020;</ref><ref type="bibr" target="#b71">Starke et al., 2017)</ref>.</p><p>Future research should explore the long-term effects of gaze-sharing and speech on decision-making, particularly in more complex and dynamic environments. Furthermore, gaze sharing in traffic control rooms can be used to enhance collaboration, training, situational awareness, communication, and remote support <ref type="bibr" target="#b10">(Castner et al., 2020;</ref><ref type="bibr" target="#b51">Mahanama et al., 2023)</ref>. Gaze-sharing markers can influence the effectiveness of decision-making in collaborative settings <ref type="bibr" target="#b1">(Atweh et al., 2023)</ref>. Therefore, gaze markers' visualizations should be tested under varying cognitive loads to ensure reliability.</p><p>Furthermore, future studies could explore the potential of combining other communication modalities, such as haptic feedback or augmented reality, with gaze-sharing and speech. By expanding the range of communication tools available to decision-makers, these systems could further reduce cognitive load and improve task performance in distributed cognition settings.</p><p>Moreover, future research could investigate the neural and physiological correlates of gaze-sharing and cognitive load, utilizing techniques such as EEG or heart rate variability to understand better how these interventions impact cognitive processes. This could help refine the design of distributed cognition systems by identifying the optimal balance between cognitive load and task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work is an outcome of the Future Resilient Systems project at the </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A5</head><label></label><figDesc>study by<ref type="bibr" target="#b56">Neider et al. (2010)</ref> demonstrated that shared gaze in a collaborative visual search task improved the efficiency of finding targets, as participants could track each other's gaze movements and adjust their behaviouraccordingly. Along similar lines, research by Richardson and Dale found that gaze coordination between individuals predicted successful collaborative problem-solving in conversation-based tasks, indicating the critical role that visual attention plays in facilitating mutual understanding (Richardson and Dale, 2005). These findings are particularly relevant to collaborative N-back tasks, where individuals must synchronize their efforts to recall past stimuli. Contrary to this, a recent research by McCarley et al. (2021) found that sharing eye gaze fails to improve signal detection accuracy among operators. This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d In addition to gaze sharing, verbal communication is fundamental in supporting collaboration by facilitating the exchange of ideas, coordinating actions, and resolving ambiguities. Studies on the role of speech in collaborative problem-solving have found that effective communication improves performance and leads to greater satisfaction among participants</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Recent studies employing N-back tests to examine cognitive load have revealed complex interactions between eye movements (saccades and fixations) and cognitive load<ref type="bibr" target="#b27">(Guo et al., 2021;</ref><ref type="bibr" target="#b75">Walter and Bex, 2021;</ref><ref type="bibr" target="#b16">Chen and Epps, 2013)</ref>.<ref type="bibr" target="#b75">Walter and Bex (2021)</ref> studied how progressively increasing workload 7 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d affects eye movement features, such as fixational and saccadic behaviours, and reaction time latency averaged across trials. Their findings show that cognitive load correlates with a reduction in mean saccadic duration and fewer saccades overall, suggesting a more focused, though less frequent, exploration of visual stimuli. Additionally, an increase in maximum saccade count and duration under high cognitive load conditions reflects intensive information gathering, with larger and more deliberate jumps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>recruited 64 participants, 35 males and 29 females, through online platforms (such as Telegram, online notice boards, internal newsletters, and university bulletin boards). The age distribution of the participants was as follows: 19-21 years: 1 participant; 22-25: 16 participants; 26-29 years: 19participants; 30-33: 11 participants; 33-36: 11 Participants; 37-40: 3 participants; above 40: 3 participants. The participants were randomly assigned into groups (pair). 9 Experimental setup: Participants working on the collaborative task, visually separated through a wall, equipped with eye-trackers and microphones Before registering, we informed participants of specific inclusion criteria for the experiment, which included normal or corrected-to-normal vision to ensure accurate eye-tracking data, no history of neurological or psychological conditions (including color blindness) that could impact cognitive performance, familiarity with basic computer tasks, and English as the primary language for communication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Picture based N-back task 3.4. Procedure Participants provided informed consent before the experiment, which outlined the study's objectives, procedures, potential risks, data collected, and their right to withdraw. A practice session, including individual and gazesharing conditions (up to 2-back), was conducted to familiarize participants with the task. The participants were then assigned one of the conditions 12 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d Figure 3: A sample trial</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Maximum pupil size in scene viewing task for both left and right eye.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Maximum pupil in choice task for both left and right eye.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Maximum N-level and mean reaction time for each condition. reaction times (RTs) across all conditions. The table shows the p-values and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 Figure 7 :</head><label>77</label><figDesc>Figure 6(right) visualizes these differences, with slower reaction times in the ETS and S conditions compared to ET and I, further underscoring the effect of condition on redepicts the aggregate NASA-TLX scores by the participants.Participants reported significantly lower mental demands (MD) when mak-18 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d The NASA-TLX ratings across the conditions (I, S, ET, and ETS) for different dimensions of Mental Demand (MD), Physical Demand (PD), Temporal Demand (TD), Effort (E), Frustration (F), and Performance (P) ing decisions individually (I) compared to eye-gaze-sharing (ET) conditions [W=-2.522, p &lt; 0.05]. Furthermore, they reported a significant increase in performance (P) in speech conditions (S) compared to individual conditions (I) [W=-2.147, p &lt; 0.05].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>This study explored how different communication strategies -speechsharing, gaze-sharing, and a combination of both -affect reaction times, cognitive load, and user experience in a collaborative N-back task. The results prove that the integration of gaze-sharing (ET) significantly improves 19 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d Mean scores for various UEQ dimensions across the different collaboration conditions (I, S, ET, ETS). the maximum N-back level reached. ET and ETS outperform non-gazesharing communication strategies (S, I), with ET showing better performance than I and ETS performing better than S. In the scene tasks, the maximum pupil dilation was significantly larger in other collaborative conditions (ETS and S) compared to eye-gaze sharing (ET). The differences in maximum pupil dilation observed in N-level 0 across different collaborative conditions (ET vs. ETS and S vs. ET) reflect the heightened cognitive effort required to effectively integrate shared information as participants navigated through complex stimuli while speaking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The interplay between maximum pupil dilation, maximum N-back levels, reaction time, and NASA-TLX scores suggests a nuanced relationship between cognitive performance and collaborative strategies. While collaboration can foster N levels, it leads to slower reaction time. In distributed cognition systems, collaboration leads to higher cognitive load, as reflected by the maximum pupil dilation measures. However, among the three collaborative conditions, ET has a clear significant advantage in reaction time over other conditions, including speech. Moreover, though not significant, we see that ET has lower maximum pupil dilation compared to collaborative con-ditions, including speech. This complex dynamic between maximum pupil dilation and NASA-TLX underscores the importance of understanding how different collaboration methods influence cognitive processes and subjective workload experiences. In addressing our research question -Can gaze or speech sharing effectively serve as a supportive mechanism under varying cognitive load conditions? -our findings suggest that the ET condition achieves a balance, combining high N-level attainment, relatively low reaction time, and manageable cognitive load. While ET shows promise, system design should consider the relative importance of these factors, weighing efficiency, speed, and cognitive demand to optimize collaborative support effectively. While gaze-sharing technology holds great potential for enhancing collaboration and decisionmaking in environments like traffic control rooms, it is essential to consider the privacy and security implications accompanying eye-tracking systems. Eye-tracking data is highly sensitive, as it can reveal an individual's focus, attentiveness, and cognitive states such as fatigue, stress, or alertness. In a professional setting, such data could be misused if not handled with care, potentially leading to unintended consequences, such as the surveillance of employees' performance or punitive measures based on perceived levels of alertness or cognitive load.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>presents the p-values and test statistics from the Wilcoxon signedrank test comparing maximum pupil dilation across conditions during the scene task, with False Discovery Rate (FDR) correction applied. The test concluded upon reaching the specified stopping criteria</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :0D[3XSLO'LDPHWHU5LJKW(\HE\1/HYHOGXULQJ6FHQHWDVNPDUNVVLJQLILFDQWGLIIHUHQFHV</head><label>1</label><figDesc>Distribution of groups across conditions reaching certain N-Levels.</figDesc><table><row><cell>N-level</cell><cell>2 3 4 6 7 8 9</cell></row><row><cell>I</cell><cell>32 30 20 12 0 0 0 0 0</cell></row><row><cell>Condition S</cell><cell>32 30 25 18 11 5 3 2 2 0</cell></row><row><cell>ET</cell><cell>32 30 22 15 11 6 1 0 0</cell></row><row><cell cols="2">ETS 32 26 23 16 8 6 3 2 1 1</cell></row><row><cell cols="2">tably between the ET and ETS and the S and ET conditions, with p-values</cell></row><row><cell cols="2">less than 0.05 for both left and right eyes. Additionally, significant differ-</cell></row><row><cell cols="2">ences were observed between the I and ET conditions for the left eye at</cell></row><row><cell cols="2">N-level 0. This analysis highlights variations in cognitive load reflected by</cell></row><row><cell cols="2">pupil dilation, with increased cognitive demands at higher N-levels (N-level</cell></row><row><cell cols="2">2) for the right eye in the I and S conditions. Using a box plot, figure 4</cell></row><row><cell cols="2">comprehensively visualizes the relationship between maximum pupil dilation</cell></row><row><cell cols="2">and the highest N-level achieved for each condition.</cell></row><row><cell cols="2">0D[3XSLO'LDPHWHU/HIW(\HE\1/HYHOGXULQJ6FHQHWDVNPDUNVVLJQLILFDQWGLIIHUHQFHV</cell></row><row><cell></cell><cell>FRQGLWLRQ</cell></row><row><cell></cell><cell>,</cell></row><row><cell></cell><cell>6</cell></row><row><cell>0D[3XSLO'LDPHWHUPP</cell><cell>(7 (76</cell></row><row><cell></cell><cell>1/HYHO</cell></row><row><cell></cell><cell>&amp;RQGLWLRQ</cell></row><row><cell></cell><cell>,</cell></row><row><cell></cell><cell>6</cell></row><row><cell>0D[3XSLO'LDPHWHUPP</cell><cell>(7 (76</cell></row><row><cell></cell><cell>1/HYHO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>presents the p-values and test statistics from the Wilcoxon signedrank test, comparing maximum pupil dilation between conditions during the 15</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>P-values and Wilcoxon signed rank test statistic (W) of significant p-values comparing maximum pupil dilation during the scene task, reported after FDR correction.</figDesc><table><row><cell cols="3">N-level Condition 1 Condition 2</cell><cell>Left eye</cell><cell>Right eye</cell></row><row><cell></cell><cell></cell><cell></cell><cell>p</cell><cell>W</cell><cell>p</cell><cell>W</cell></row><row><cell></cell><cell>ET</cell><cell>ETS</cell><cell cols="2">p &lt; 0.001 75 p &lt; 0.01 123</cell></row><row><cell>0</cell><cell>S</cell><cell>ET</cell><cell cols="2">p &lt; 0.0001 57 p &lt; 0.01 120</cell></row><row><cell>0</cell><cell>I</cell><cell>ET</cell><cell cols="2">p &lt; 0.05 138</cell></row><row><cell>2</cell><cell>I</cell><cell>S</cell><cell></cell><cell>p &lt; 0.01 34</cell></row><row><cell cols="5">choice task. Significant differences emerged across condition pairs, notably</cell></row><row><cell cols="5">at N-level 0 among I and S, I and ET, and I and ETS conditions, with</cell></row><row><cell cols="5">both left and right eyes showing p-values well below 0.05. At higher N-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Significant p-values and Wilcoxon signed rank test statistic (W), adjusted for multiple comparisons using the FDR procedure, are reported for comparing maximum pupil dilation during the choice task.</figDesc><table><row><cell>N-level Condition 1 Condition 2 0.0 I S 0.0 I ET 0.0 I ETS 1.0 I S 1.0 I ETS 2.0 I S 2.0 I ETS 3.0 I S 3.0 I ETS 4.2. Maximum N-back levels and reaction time Left eye p p &lt; 0.01 103 p &lt; 0.0001 58 Right eye W p W p &lt; 0.05 133 p &lt; 0.05 136 p &lt; 0.001 84 p &lt; 0.001 82 p &lt; 0.001 55 p &lt; 0.01 67 p &lt; 0.01 67 p &lt; 0.0001 10 p &lt; 0.01 30 p &lt; 0.01 27 p &lt; 0.01 28 p &lt; 0.01 28 p &lt; 0.05 8 p &lt; 0.05 6 p &lt; 0.01 6 p &lt; 0.01 4 The analysis of maximum N-back levels reached by participants across dif-ferent conditions (I, S, ET, ETS) revealed significant differences. As shown 16 P r e p r i n t n o t p e e r r e v i e w e d</cell></row><row><cell>com/abstract=5100355</cell></row></table><note>This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Significant Wilcoxon signed-rank statistics (W) for maximum N-levels reached by the participants across different conditions reported after FDR correction</figDesc><table><row><cell></cell><cell cols="2">Condition 1 Condition 2</cell><cell>p</cell><cell>W</cell></row><row><cell></cell><cell>I</cell><cell>S</cell><cell cols="2">p &lt; 0.0001 −10.74</cell></row><row><cell>.</cell><cell>I</cell><cell>ET</cell><cell cols="2">p &lt; 0.0001 −8.52</cell></row><row><cell></cell><cell>S</cell><cell>ETS</cell><cell>p &lt; 0.05</cell><cell>2.04</cell></row><row><cell></cell><cell>I</cell><cell>ETS</cell><cell cols="2">p &lt; 0.0001 −7.76</cell></row><row><cell cols="5">Table 5 and Figure 6 (right) together present the comparison of mean</cell></row><row><cell></cell><cell></cell><cell>17</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Wilcoxon-signed rank test statistics (W) and p-values for mean reaction times across all levels and conditions reported after FDR correction</figDesc><table><row><cell></cell><cell cols="3">Condition Condition 2</cell><cell>p</cell><cell>W</cell><cell></cell><cell></cell></row><row><cell></cell><cell>I</cell><cell></cell><cell>S</cell><cell cols="2">p &lt; 0.0001 −5.46</cell><cell></cell><cell></cell></row><row><cell></cell><cell>S</cell><cell></cell><cell>ET</cell><cell cols="2">p &lt; 0.0001 4.32</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell>ETS</cell><cell cols="2">p &lt; 0.0001 −4.53</cell><cell></cell><cell></cell></row><row><cell></cell><cell>I</cell><cell></cell><cell>ETS</cell><cell cols="2">p &lt; 0.0001 −5.71</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0D[LPXP1OHYHOE\&amp;RQGLWLRQ</cell><cell></cell><cell></cell><cell cols="2">0HDQ5HDFWLRQ7LPHIRU(DFK&amp;RQGLWLRQ</cell><cell></cell></row><row><cell>1OHYHO</cell><cell></cell><cell></cell><cell>0HDQ5HDFWLRQ7LPH57V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>,</cell><cell>6</cell><cell>(7</cell><cell>(76</cell><cell>,</cell><cell>6</cell><cell>(7</cell><cell>(76</cell></row><row><cell></cell><cell>&amp;RQGLWLRQ</cell><cell></cell><cell></cell><cell></cell><cell>&amp;RQGLWLRQ</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Singapore-ETH Centre (SEC) supported by the National Research Foundation, Prime Minister's Office, Singapore, under its Campus for Research Excellence and Technological Enterprise (CREATE) program. We want to thank the Director, Dr. Joerin Jonas, and the Manager, Mrs. Jean Tang, of the Future Resilient Systems programme hosted by the Singapore-ETH Centre for their support in organizing the studies.</figDesc><table><row><cell>23</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5100355 P r e p r i n t n o t p e e r r e v i e w e d</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">I see what you see: gaze awareness in mobile video collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thankachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isokoski</surname></persName>
		</author>
		<idno type="DOI">10.1145/3204493.3204542</idno>
		<idno>doi:10. 1145/3204493.3204542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can real-time gaze sharing help team collaboration? a preliminary examination of its effectiveness with pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Atweh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hazimeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Riggs</surname></persName>
		</author>
		<idno type="DOI">10.1177/21695067231193659</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications Sage CA</publisher>
			<date type="published" when="2023" />
			<biblScope unit="page" from="716" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaze sharing, a double-edged sword: Examining the effect of real-time gaze sharing visualizations on team performance and situation awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Atweh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Riggs</surname></persName>
		</author>
		<idno type="DOI">10.1177/00187208241272060</idno>
		<idno>doi:10.1177/ 00187208241272060</idno>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When smart groups fail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barron</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15327809JLS1203_1</idno>
	</analytic>
	<monogr>
		<title level="j">The journal of the learning sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="307" to="359" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pupillometry as a measure of cognitive load in mental rotation tasks with abstract and embodied figures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-021-01568-5</idno>
		<idno>doi:10.1007/ s00426-021-01568-5</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1382" to="1396" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Breaking the ice in human-agent communication: Eye-gaze based initiation of contact with an embodied conversational agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tober</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04380-2_26</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Virtual Agents</title>
		<editor>Ruttkay, Z., Kipp, M., Nijholt, A., Vilhjálmsson, H.H.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="229" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coordinating cognition: The costs and benefits of shared gaze during collaborative search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2007.05.012</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1465" to="1477" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Go together: providing nonverbal awareness cues to enhance co-located sensation in remote communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tanaka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<idno type="DOI">10.1186/s13673-019-0180-y</idno>
		<idno>doi:10.1186/ s13673-019-0180-y</idno>
	</analytic>
	<monogr>
		<title level="j">Human-centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards expert gaze modeling and recognition of a user&apos;s attention in realtime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Castner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Geßler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hüttig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving resilience by communicating predicted disruptions in control rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98388-8_27</idno>
		<idno>doi:10.1007/ 978-3-030-98388-8_27</idno>
	</analytic>
	<monogr>
		<title level="m">IFIP conference on human-computer interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="302" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving resilience by communicating predicted disruptions in control rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98388-8_27</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">13198</biblScope>
			<biblScope unit="page" from="302" to="315" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating perceived mental workload from eye-tracking data based on benign anisocoria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1109/THMS.2024.3432864</idno>
		<idno>doi:10</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="499" to="507" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/THMS.2024.3432864</idno>
		<idno>/THMS.2024.3432864</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic classification of eye activity for cognitive load measurement with emotion interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2012.10.021</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collaborative eye tracking based code review through real-time shared gaze visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11704-020-0422-1</idno>
		<idno>doi:10.1007/ s11704-020-0422-1</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mind in Society. The Development of Higher Psychological Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scribner</surname></persName>
		</author>
		<editor>Vygotsky, Lev S.</editor>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
	<note>Harvard university press</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From teams to teamness: Future directions in the science of team cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Inderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Lematta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teo</surname></persName>
		</author>
		<idno type="DOI">10.1177/00187208231162449</idno>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1669" to="1680" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning, PMLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The low/high index of pupillary activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krejtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Gehrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bafna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baekgaard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376394</idno>
		<idno>doi:10. 1145/3313831.3376394</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond eye gaze: What else can eyetracking reveal about cognition and cognitive development?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guerra-Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Miller Singley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bunge</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dcn.2016.11.001</idno>
	</analytic>
	<monogr>
		<title level="j">Developmental Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="69" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A generalized wilcoxon test for comparing arbitrarily singly-censored samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Gehan</surname></persName>
		</author>
		<idno type="DOI">10.2307/2333825</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effect of task difficulty and interstimulus interval on blink parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Stern</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-8760(92)90050-L</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="111" to="117" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A review of the use of gaze and pupil metrics to assess mental workload in gamified and simulated sensorimotor tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merians</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adamovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fluet</surname></persName>
		</author>
		<idno type="DOI">10.3390/s24061759</idno>
		<idno>doi:10. 3390/s24061759</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">1759</biblScope>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Eye-tracking for performance evaluation and workload estimation in space telerobotic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/THMS.2021.3107519</idno>
		<idno>doi:10.1109/ THMS.2021.3107519</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Development of the nasa-tlx (task load index): Results of empirical and theoretical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Staveland</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0166-4115(08)62386-9</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="62386" to="62395" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">User experience and experience design. The encyclopedia of human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassenzahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pupil size as related to interest value of visual stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Polt</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.132.3423.349</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="349" to="350" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Clinical utility of the N-back task in functional neuroimaging studies of working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Jacola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Willard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ashford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Ogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Scoggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Conklin</surname></persName>
		</author>
		<idno type="DOI">10.1080/13803395.2014.953039</idno>
		<idno>doi:10.1080/ 13803395.2014.953039</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical and Experimental Neuropsychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="875" to="886" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How to make sense of treasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0003598X00098215</idno>
	</analytic>
	<monogr>
		<title level="j">Antiquity</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="206" to="208" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative eye tracking for image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="DOI">10.1145/2578153.2578215</idno>
	</analytic>
	<monogr>
		<title level="j">Eye Tracking Research and Applications Symposium (ETRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cooperation and competition: Theory and research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Interaction Book Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Measuring cognitive load for map tasks through pupil diameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-45738-3_21</idno>
	</analytic>
	<monogr>
		<title level="m">The Annual International Conference on Geographic Information Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="323" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A spreadsheet program for making a balanced latin square design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.17533/udea.rccp.324493</idno>
	</analytic>
	<monogr>
		<title level="j">Revista Colombiana de Ciencias Pecuarias</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="591" to="596" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards smart glasses for facial expression recognition using omg and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kiprijanovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stankoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Broulidakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fatoorechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gjoreski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nduka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gjoreski</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-43135-5</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16043</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Age differences in short-term retention of rapidly changing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Kirchner</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0043688</idno>
		<idno>doi:10</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">352</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A cognitive load approach to collaborative learning: United brains for complex tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Kirschner</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10648-008-9095-2</idno>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Task complexity as a driver for collaborative learning efficiency: The collective working-memory effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Kirschner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<idno type="DOI">10.1002/acp.1730</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From Cognitive Load Theory to Collaborative Cognitive Load Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zambrano</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11412-018-9277-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer-Supported Collaborative Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="213" to="233" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A cognitive decomposition to empirically study human performance in control room environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Knisely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Joyner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barksdale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kharod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaughn-Cooke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2020.102438</idno>
		<ptr target="doi:10.1016/j.ijhcs.2020.102438" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human Computer Studies</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page">102438</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A discussion of the cognitive load in collaborative problem-solving: The decision-making phase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kolfschoten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brazier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40070-014-0034-9</idno>
		<idno>doi:10.1007/ s40070-014-0034-9</idno>
	</analytic>
	<monogr>
		<title level="j">EURO Journal on Decision Processes</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="257" to="280" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cognitive Load in Collaboration: Convergence. Group Decision and Negotiation 22</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kolfschoten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Brazier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10726-012-9322-6</idno>
		<idno>doi:10.1007/ s10726-012-9322-6</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="975" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey on measuring cognitive workload in human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karolus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zagermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Reiterer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Woźniak</surname></persName>
		</author>
		<idno type="DOI">10.1145/3582272</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Many hands make light the work: The causes and consequences of social loafing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Latané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harkins</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.37.6.822</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">822</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DisETrac: Distributed Eye-Tracking for Online Collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahanama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayarathna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3576840.3578292</idno>
	</analytic>
	<monogr>
		<title level="m">CHIIR 2023 -Proceedings of the 2023 Conference on Human Information Interaction and Retrieval</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shared Gaze Fails to Improve Team Visual Monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leggett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Enright</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720820902347</idno>
		<idno>doi:10.1177/ 0018720820902347</idno>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="696" to="705" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pupillary responses for cognitive load measurement to classify difficulty levels in an educational video game: Empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mitre-Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lara-Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Serious Games</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">war rooms&quot; of the street: surveillance practices in transportation control centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Monahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Media of Surveillance. Routledge</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="88" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">User-Centered Development of a Public Transportation Vehicle Operated in a Demand Responsive Environment 964</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deißer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20503-4</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="545" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Coordinating spatial referencing using shared gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<idno type="DOI">10.3758/PBR.17.5.718</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="718" to="724" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Realtime sharing of gaze data between multiple eye trackers-evaluation, tools, and advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Niehorster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cornelissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garde</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-016-0806-1</idno>
		<idno>doi:10.3758/ s13428-016-0806-1</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1310" to="1322" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Psychopy-psychophysics software in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Peirce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Decision making under stress: the role of information overload, time pressure, complexity, and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Phillips-Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adya</surname></persName>
		</author>
		<idno type="DOI">10.1080/12460125.2020.1768680</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Decision Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Response time and eye tracking datasets for activities demanding varying cognitive load</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ayare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balasingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Biondi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dib.2020.106389</idno>
	</analytic>
	<monogr>
		<title level="j">Data in Brief</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">106389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A cross-recurrence analysis of the pupil size fluctuations in steady scotopic conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Serchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rufa</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2019.00407</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Tests for paired count outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Proudfoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Looking to understand: The coupling between speakers&apos; and listeners&apos; eye movements and its relationship to discourse comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog0000_29</idno>
		<idno>1045-1060. doi:10.1207/ s15516709cog0000_29</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Analyzing the time course of pupillometric data. Trends in hearing 23</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Rij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1177/2331216519832483</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">2331216519832483</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Pupillometry tracks fluctuations in working memory performance. Attention, Perception, and Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Robison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Unsworth</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-018-1618-4</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="407" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On teams, teamwork, and team performance: Discoveries and developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872008X288457</idno>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="540" to="547" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gamification of an n-back working memory task -Is it worth the effort? An EEG and eye-tracking study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scharinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prislan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ninaus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2023.108545</idno>
		<idno>doi:10.1016/j. biopsycho.2023.108545</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page">108545</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Applying the user experience questionnaire (ueq) in different evaluation scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinderks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomaschewski</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-07668-3_37</idno>
		<idno>doi:10.1007/ 978-3-319-07668-3_37</idno>
	</analytic>
	<monogr>
		<title level="m">Design, User Experience, and Usability. Theories, Methods, and Tools for Designing the User Experience: Third International Conference, DUXU 2014, Held as Part of HCI International</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 3</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cognitive load theory and human movement: Towards an integrated model of working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tindall-Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agostinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paas</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10648-019-09461-9</idno>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="293" to="317" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Workflows and individual differences during visually guided routine tasks in a road traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">This preprint research paper has not been peer reviewed</title>
		<idno type="DOI">10.1016/j.apergo.2017.01.006</idno>
		<idno>doi:10.1016/ j.apergo.2017.01.006</idno>
		<ptr target="https://ssrn.com/abstract=5100355room" />
	</analytic>
	<monogr>
		<title level="j">Applied Ergonomics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="79" to="89" />
		</imprint>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cognitive architecture and instructional design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Paas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational psychology review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="296" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An Agent-Based Model of Collective Decision-Making: How Information Sharing Strategies Scale with Information Overload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Kudesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Heinimann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSS.2020.2986161</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="751" to="767" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cognitive load influences oculomotor behavior in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bex</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-91845-5</idno>
		<idno>1-12. doi:10.1038/ s41598-021-91845-5</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A scoping review of the use of lab streaming layer framework in virtual and augmented reality research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Barmaki</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10055-023-00799-8</idno>
		<idno>doi:10.1007/ s10055-023-00799-8</idno>
	</analytic>
	<monogr>
		<title level="j">Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2195" to="2210" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Collective memory: collaborative and individual processes in remembering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Weldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Bellinger</surname></persName>
		</author>
		<idno type="DOI">10.1037/0278-7393.23.5.1160</idno>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">1160</biblScope>
		</imprint>
	</monogr>
	<note>Journal of experimental psychology: Learning, memory, and cognition 23</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Traffic-informed multi-camera sensing (tims) system based on vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/TITS.2022.3154368</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="17189" to="17200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Cooperative multicamera vehicle tracking and traffic surveillance with edge artificial intelligence and representation learning. Transportation research part C: emerging technologies 148</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.trc.2022.103982</idno>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="page">103982</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Trends in vehicle re-identification past, present, and future: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zakria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Aftab</surname></persName>
		</author>
		<idno type="DOI">10.3390/math9243162</idno>
		<idno>doi:10.3390/ math9243162. 31</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3162</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">This preprint research paper has not been peer reviewed</title>
		<ptr target="https://ssrn.com/abstract=5100355" />
		<imprint/>
	</monogr>
	<note>Electronic copy available at</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reliability of muse and tobii pro nano at capturing mobile application users&apos; real-time cognitive workload changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2022.1011475</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Adopting maximum pupil diameter to detect subtle usability issues of a smartphone application, conflict solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2024.2327186</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Look together: using gaze for assisting co-located collaborative search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-016-0969-x</idno>
		<idno>doi:10.1007/ s00779-016-0969-x</idno>
	</analytic>
	<monogr>
		<title level="j">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="173" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Relative power of the wilcoxon test, the friedman test, and repeated-measures anova on ranks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Zumbo</surname></persName>
		</author>
		<idno type="DOI">10.1080/00220973.1993.9943832</idno>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
