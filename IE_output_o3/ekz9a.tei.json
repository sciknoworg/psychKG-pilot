[
  {
    "topic_or_construct": [
      "algorithm aversion"
    ],
    "measured_by": [
      "survey of preferences for humans versus algorithms"
    ],
    "justification": "The article states that measurement of algorithm aversion is \"based on surveying people's preferences for humans versus algorithms (e.g., Araujo et al., 2020; Lennartz et al., 2021; Thurman et al., 2019).\""
  },
  {
    "topic_or_construct": [
      "algorithm aversion"
    ],
    "measured_by": [
      "tendency to choose human over algorithmic judgments"
    ],
    "justification": "It notes that algorithm aversion is also \"operationalized as ... the tendency to choose human over algorithmic judgments (e.g., Castelo et al., 2019; Dietvorst et al., 2015; Logg et al., 2019).\""
  },
  {
    "topic_or_construct": [
      "algorithm aversion"
    ],
    "measured_by": [
      "magnitude of judgmental shifts toward human rather than algorithmic advice"
    ],
    "justification": "The text lists \"stronger judgmental shifts in the direction of human rather than algorithmic advice (e.g., Logg et al., 2019; \u00d6nkal et al., 2009; Prahl & van Swol, 2017)\" as another operationalization."
  },
  {
    "topic_or_construct": [
      "algorithm aversion"
    ],
    "measured_by": [
      "emotional and sentimental responses to erroneous algorithms vs humans"
    ],
    "justification": "Algorithm aversion is further measured by \"more emotional and sentimental responses in interactions with erroneous algorithms as compared to erroneous humans (e.g., Renier et al., 2021; see also Lee, 2018).\""
  },
  {
    "topic_or_construct": [
      "algorithm appreciation"
    ],
    "measured_by": [
      "degree of discounting one\u2019s own initial judgment relative to algorithmic advice"
    ],
    "justification": "The article explains that \"'algorithm appreciation' was originally defined to also include cases in which users discount their own initial judgments more strongly than the algorithmic advice they receive.\""
  }
]