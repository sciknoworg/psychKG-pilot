[
  {
    "topic_or_construct": [
      "facial expressions"
    ],
    "measured_by": [
      "Affectiva face-recognition software (iMotions)"
    ],
    "justification": "\u201cVideos of the faces of both participants can then be analysed with specialized tools such as Affectiva, a face recognition software from iMotions \u2026 to automate recognition of specific facial expressions.\u201d"
  },
  {
    "topic_or_construct": [
      "non-linguistic vocalizations",
      "emotional vocalisations"
    ],
    "measured_by": [
      "Hume AI automatic vocal classification"
    ],
    "justification": "\u201cSoftware, such as Hume AI \u2026 can automatically identify and classify non-verbal utterances that appear in recordings of longer duration into 24 distinct emotional dimensions.\u201d"
  },
  {
    "topic_or_construct": [
      "linguistic vocalizations",
      "speech content"
    ],
    "measured_by": [
      "OpenAI Whisper speech-transcription system"
    ],
    "justification": "\u201cRecent advances in AI, e.g., OpenAI Whisper, can be incorporated to automate speech transcription, allowing researchers to time lock speech to the temporal properties of stimulus presentation and response timing.\u201d"
  },
  {
    "topic_or_construct": [
      "gaze behaviour",
      "visual attention"
    ],
    "measured_by": [
      "head-mounted eyetrackers (e.g., Pupil Labs Core / Tobii glasses)"
    ],
    "justification": "\u201cModern binocular head-mounted eyetrackers allow researchers to collect participants' gaze behaviour in the real world \u2026 enable the tracking of gaze to the social partner's hand movements towards the screen.\u201d"
  },
  {
    "topic_or_construct": [
      "physiological arousal",
      "heart rate"
    ],
    "measured_by": [
      "image-based photoplethysmography (iPPG)"
    ],
    "justification": "\u201cImage-based photoplethysmography (iPPG) \u2026 allowing the extraction of pulse information from videos of a subject\u2019s face.\u201d"
  }
]