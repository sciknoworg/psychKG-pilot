[
  {
    "topic_or_construct": [
      "working memory",
      "short-term memory processes"
    ],
    "measured_by": [
      "Working-Memory module parameters (capacity C, weighting w) of the RLWM computational model"
    ],
    "justification": "\u201cThe RLWM model captures the parallel recruitment of working memory \u2026 The parameter is determined by two free parameters, the working memory capacity (i.e., resource limit) C, and the initial WM weighting w.\u201d"
  },
  {
    "topic_or_construct": [
      "reinforcement learning",
      "stimulus-action value learning"
    ],
    "measured_by": [
      "Reinforcement-Learning module in the RLWM model (learning rate \u03b1, prediction-error update)"
    ],
    "justification": "\u201cThe RL module is characterized by Equations 1-2\u2026 action-value Q(s,a) is updated on each trial using the delta rule \u2026 capturing the gradual RL process.\u201d"
  },
  {
    "topic_or_construct": [
      "decision-making reaction time",
      "evidence accumulation speed"
    ],
    "measured_by": [
      "Linear Ballistic Accumulator (LBA) model parameters (drift rate v, boundary b, non-decision time t0)"
    ],
    "justification": "\u201cA similar evidence accumulation model, the Linear Ballistic Accumulator, or LBA, can easily accommodate any number of actions \u2026 RT is modeled by the accumulation of evidence where parameters A, b, and t0 determine the reaction time.\u201d"
  },
  {
    "topic_or_construct": [
      "action uncertainty",
      "prior uncertainty over actions"
    ],
    "measured_by": [
      "Shannon entropy of the average action policy (Equation 13)"
    ],
    "justification": "\u201cThis prior uncertainty term was modelled by \u2026 the Shannon entropy is computed on this vector\u2026 we incorporate the uncertainty quantity from Equation 13 into the evidence accumulation rate.\u201d"
  }
]