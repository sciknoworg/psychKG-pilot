[
  {
    "topic_or_construct": [
      "Model-based planning",
      "Model-based control"
    ],
    "measured_by": [
      "model-based weighting parameter w (dual-system RL model)"
    ],
    "justification": "\u201cThe model-based weighting parameter w in our dual-system RL model reflects the degree to which participants used the task's transition structure to plan towards an island.\u201d"
  },
  {
    "topic_or_construct": [
      "Explore\u2013exploit balance",
      "Choice stochasticity"
    ],
    "measured_by": [
      "inverse temperature parameter \u03b2 (soft-max choice rule)"
    ],
    "justification": "\u201c\u03b2 is the inverse temperature parameter which controls the exploitation-exploration trade-off between two choice options given their difference in value.\u201d"
  },
  {
    "topic_or_construct": [
      "Reward learning rate"
    ],
    "measured_by": [
      "learning rate \u03b1 in the dual-system RL model"
    ],
    "justification": "\u201c\u2026the prediction error is then used to update the value\u2026 where \u03b1 is a learning rate used to scale the prediction error\u2026\u201d"
  },
  {
    "topic_or_construct": [
      "Eligibility trace influence"
    ],
    "measured_by": [
      "eligibility-trace decay parameter \u03bb"
    ],
    "justification": "\u201c\u2026updated with \u03b4 PE where \u03bb represents an eligibility trace decay parameter representing how much a prediction error is used to update previous actions.\u201d"
  }
]