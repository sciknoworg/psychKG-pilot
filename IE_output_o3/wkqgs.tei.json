[
  {
    "topic_or_construct": [
      "Metacognitive confidence"
    ],
    "measured_by": [
      "self-reported confidence ratings on a 50%\u2013100% probability scale"
    ],
    "justification": "\u201cparticipants were also instructed to report how confident they were that they selected the correct response on a scale ranging from 50% (guessing) to 100% (certain correct)\u201d"
  },
  {
    "topic_or_construct": [
      "Intra-rater disagreement"
    ],
    "measured_by": [
      "percentage of images where the same participant\u2019s two classifications differ"
    ],
    "justification": "\u201cSince participants made two responses on each image, we calculated the rate at which both of these responses were different. The intra-rater disagreement rate was \u2026\u201d"
  },
  {
    "topic_or_construct": [
      "Inter-rater disagreement"
    ],
    "measured_by": [
      "percentage of disagreement between first responses of randomly paired participants"
    ],
    "justification": "\u201cwe calculated the inter-rater disagreement by pairing the participants randomly and calculating the inter-rater disagreement with their first responses\u201d"
  },
  {
    "topic_or_construct": [
      "Confidence calibration"
    ],
    "measured_by": [
      "Brier score and Murphy\u2019s Brier decomposition"
    ],
    "justification": "\u201cWe calculated the Brier score and Murphy's Brier decomposition for each experiment. The Brier score is the mean squared error between the confidence rating and the outcome.\u201d"
  }
]