[
  {
    "topic_or_construct": [
      "reward sensitivity"
    ],
    "measured_by": [
      "scaling parameter (in RL models)"
    ],
    "justification": "\u201cHuys and colleagues 63 \u2026 reward sensitivity was operationalized through a scaling parameter reflecting how strongly individuals valued the rewards they received.\u201d"
  },
  {
    "topic_or_construct": [
      "learning rate"
    ],
    "measured_by": [
      "learning-rate parameter (\u03b1)"
    ],
    "justification": "\u201cThe learning rate quantified the extent to which individuals updated their expectations based on new feedback.\u201d"
  },
  {
    "topic_or_construct": [
      "exploration\u2013exploitation trade-off"
    ],
    "measured_by": [
      "inverse temperature parameter"
    ],
    "justification": "\u201cThis trade-off is often quantified using the inverse temperature parameter, where lower values indicate greater exploration and higher values lead to deterministic exploitation of high-value options.\u201d"
  },
  {
    "topic_or_construct": [
      "risk aversion"
    ],
    "measured_by": [
      "risk-aversion parameter"
    ],
    "justification": "\u201cMathematical models incorporate parameters such as a delay discounting rate and risk aversion to quantify these effects 36.\u201d"
  },
  {
    "topic_or_construct": [
      "delay discounting / temporal impulsivity"
    ],
    "measured_by": [
      "delay-discounting rate"
    ],
    "justification": "\u201cMathematical models incorporate parameters such as a delay discounting rate and risk aversion to quantify these effects 36.\u201d"
  },
  {
    "topic_or_construct": [
      "model-based vs. model-free control balance"
    ],
    "measured_by": [
      "weighting parameter between model-based and model-free systems"
    ],
    "justification": "\u201cHybrid reinforcement learning models capture this interplay \u2026 and weighting parameters that quantify their relative contributions 37,38.\u201d"
  },
  {
    "topic_or_construct": [
      "working-memory influence on choice"
    ],
    "measured_by": [
      "weighting parameter between working memory and reinforcement learning"
    ],
    "justification": "\u201cIn the model \u2026 the weighting parameter is adapted to determine how much decision-making is influenced by working memory.\u201d"
  },
  {
    "topic_or_construct": [
      "evidence-accumulation dynamics"
    ],
    "measured_by": [
      "drift-diffusion model parameters"
    ],
    "justification": "\u201cSequential sampling models such as drift-diffusion models \u2026 provide a mechanistic account of choice accuracy and reaction time.\u201d"
  },
  {
    "topic_or_construct": [
      "recency bias"
    ],
    "measured_by": [
      "recency-bias parameter"
    ],
    "justification": "\u201cComputational modeling identified an elevated recency bias in opioid-addicted individuals, meaning they are more likely to repeat previous responses regardless of reinforcement history.\u201d"
  }
]