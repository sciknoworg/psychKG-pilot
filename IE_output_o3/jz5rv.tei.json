[
  {
    "topic_or_construct": [
      "learning rate (win outcomes)"
    ],
    "measured_by": [
      "\u03b1_W parameter of the reinforcement-learning model fitted to the Information Bias Learning Task"
    ],
    "justification": "\u201cIn these equations, \u03b1_W\u2208[0,1] \u2026 express learning rates for Q_W \u2026\u201d \u2013 explicitly defining \u03b1_W as the measure of the win-side learning rate."
  },
  {
    "topic_or_construct": [
      "learning rate (loss outcomes)"
    ],
    "measured_by": [
      "\u03b1_L parameter of the reinforcement-learning model fitted to the Information Bias Learning Task"
    ],
    "justification": "\u201c\u03b1_L\u2208[0,1] express learning rates for Q_L \u2026\u201d \u2013 identifying \u03b1_L as the quantitative indicator of the loss-side learning rate."
  },
  {
    "topic_or_construct": [
      "exploration-exploitation tendency (inverse temperature)"
    ],
    "measured_by": [
      "\u03b2 parameter (inverse temperature) of the reinforcement-learning model"
    ],
    "justification": "\u201cthe inverse temperature, \u03b2\u2208(0,20], adjusts the sharpness of the value difference between the options in the choice probability\u201d \u2013 showing \u03b2 is the metric used to index exploration versus exploitation."
  },
  {
    "topic_or_construct": [
      "choice perseveration tendency"
    ],
    "measured_by": [
      "\u03c6 parameter (perseveration magnitude) in the RL model with a perseveration term (M1s/M1m)"
    ],
    "justification": "\u201cThese models have a new parameter \u03c6 that represents the perseveration magnitude and biases actions toward repeated selection of a previously chosen stimulus, independent of outcome history.\u201d"
  }
]