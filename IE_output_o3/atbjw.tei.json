[
  {
    "topic_or_construct": [
      "Outcome context-dependence",
      "relative value learning"
    ],
    "measured_by": [
      "two-phase learning/transfer reinforcement-learning task",
      "range-normalization paradigm"
    ],
    "justification": "\u201cA typical experimental demonstration of outcome context-dependence employs a two-phase design, comprising a learning phase and a transfer phase \u2026 the preferences expressed during the transfer phase will differ depending on whether option values are encoded in an objective manner or in a context-dependent manner.\u201d"
  },
  {
    "topic_or_construct": [
      "Positivity bias",
      "asymmetric learning rates"
    ],
    "measured_by": [
      "different learning rates for positive vs. negative prediction errors",
      "bandit reinforcement-learning tasks"
    ],
    "justification": "\u201cStudies have shown that option values are updated more following positive prediction errors than negative ones. Computationally, this bias is modeled by assuming different learning rates for positive and negative prediction errors.\u201d"
  },
  {
    "topic_or_construct": [
      "Choice perseveration",
      "praxic bias"
    ],
    "measured_by": [
      "mismatch between calculated option values and chosen actions",
      "perseveration parameter in RL choice models"
    ],
    "justification": "\u201cA common example of a praxic bias in human reinforcement learning is choice repetition or perseveration, where actions are repeated regardless of the rewards received, causing actions to deviate from the calculated option values.\u201d"
  }
]