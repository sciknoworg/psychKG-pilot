[
  {
    "topic_or_construct": [
      "Reward prediction error"
    ],
    "measured_by": [
      "phasic midbrain dopamine neuron activity"
    ],
    "justification": "\u201cPhasic bursts from midbrain dopamine (DA) neurons have been proposed as a neural correlate of the reward prediction error (RPE) signal central to RL theories.\u201d"
  },
  {
    "topic_or_construct": [
      "Policy-update signal"
    ],
    "measured_by": [
      "striatal BOLD response"
    ],
    "justification": "\u201c\u2026the BOLD signal in the striatum was consistent with a policy-update signal, rather than an action-value update [47].\u201d"
  },
  {
    "topic_or_construct": [
      "Context-dependent preference learning"
    ],
    "measured_by": [
      "rich-versus-lean context choice task"
    ],
    "justification": "\u201cWhen given a choice between two options\u2026 learned in a rich environmental context\u2026 and\u2026 in a lean context\u2026 humans and other animals display a marked preference for the lean-context option.\u201d"
  },
  {
    "topic_or_construct": [
      "Implicit behavioral policy updating"
    ],
    "measured_by": [
      "sensorimotor adaptation task with mirror-reversed visual feedback"
    ],
    "justification": "\u201cHadjiosif et al. used mirror-reversed visual feedback in a sensorimotor adaptation task, and found that subjects\u2019 patterns\u2026 were well explained by a model in which an implicit behavioral policy was updated\u2026\u201d"
  },
  {
    "topic_or_construct": [
      "Continuous-action learning"
    ],
    "measured_by": [
      "reaction-time choice task modeled with a two-parameter gamma policy"
    ],
    "justification": "\u201c\u2026policy-gradient RL algorithms are straightforwardly applicable to continuous action spaces\u2026 (e.g., a two-parameter gamma distribution for reaction time choices; [32]).\u201d"
  }
]