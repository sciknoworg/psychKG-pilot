[
  {
    "topic_or_construct": [
      "Overprecision",
      "excessive certainty"
    ],
    "measured_by": [
      "self-reported confidence intervals (e.g., 75%, 80%, 90%) compared with actual hit rates"
    ],
    "justification": "\"The most common method researchers use to assess reported confidence is by comparing their confidence intervals with actual accuracy.\""
  },
  {
    "topic_or_construct": [
      "Overprecision"
    ],
    "measured_by": [
      "item-confidence probability judgments (estimate probability selected answer is correct)"
    ],
    "justification": "\"Many studies ask participants to select the best answer from a set of options and then estimate the probability they selected the correct one. These item-confidence judgments have the advantage that people seem to understand them better than confidence-intervals.\""
  },
  {
    "topic_or_construct": [
      "Overprecision"
    ],
    "measured_by": [
      "subjective probability distributions via histogram / \u2018distribution builder\u2019"
    ],
    "justification": "\"Goldstein and Rothschild (2014) report that people show impressively good calibration using a graphical 'distribution builder' that asked for estimates of the likelihood of each bin in a histogram.\""
  },
  {
    "topic_or_construct": [
      "Overprecision"
    ],
    "measured_by": [
      "behavioral response to asymmetric error penalties (distance moved under shifted incentives)"
    ],
    "justification": "\"Mannes and Moore (2013) inferred confidence by observing how much people respond to asymmetric penalties for error\u2026 This lack of movement implies overprecision.\""
  },
  {
    "topic_or_construct": [
      "Overprecision"
    ],
    "measured_by": [
      "peak-confidence minus accuracy index for most-likely bin"
    ],
    "justification": "\"My preregistered measure of overprecision is the difference between peak confidence (the bin assigned highest probability) and accuracy (the frequency with which the truth falls inside that bin).\""
  }
]