[
  {
    "topic_or_construct": [
      "Pavlovian bias"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "\u201cThe Go/No-Go task \u2026 captures the tendency to approach rewarding stimuli \u2026 The strengths of the \u2018Go\u2019 biases are controlled by two parameters: b for the unlearned (instrumental) bias and \u03c0 for the learned (Pavlovian) bias.\u201d"
  },
  {
    "topic_or_construct": [
      "Go bias"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "The same model estimates \u201cb for the unlearned (instrumental) bias,\u201d making the Go bias one of the fitted computational parameters of the Go/No-Go task."
  },
  {
    "topic_or_construct": [
      "Learning rate"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "\u201cThe instrumental and Pavlovian learning processes are jointly controlled by a learning rate parameter (\u03b5) \u2026 We fit \u2026 \u03b5 \u2026 to individual participants.\u201d"
  },
  {
    "topic_or_construct": [
      "Lapse rate"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "\u201cNoise in the action selection process is controlled by a lapse rate parameter (\u03be) that captures the probability of random responses.\u201d"
  },
  {
    "topic_or_construct": [
      "Effective reward/punishment size"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "\u201cThe instrumental and Pavlovian learning processes are jointly controlled by \u2026 the \u2018effective size\u2019 (\u03c1) of different outcome types \u2026 we fit a separate \u03c1 parameter.\u201d"
  },
  {
    "topic_or_construct": [
      "Effective neutral-outcome size"
    ],
    "measured_by": [
      "Go/No-Go task \u2013 Pavlovian reinforcement-learning model"
    ],
    "justification": "They \u201cintroduced an additional parameter \u2026 effective size of neutral outcomes\u201d to capture how neutral feedback influences behavior in the Go/No-Go task."
  },
  {
    "topic_or_construct": [
      "Detection threshold"
    ],
    "measured_by": [
      "Change Detection task \u2013 Absolute Difference (MAD) model"
    ],
    "justification": "In the Change Detection section: \u201cThe noise in the image representation is accounted for by a sensitivity parameter \u03c3, and the decision threshold is controlled by the detection threshold \u03b8. \u2026 we focus on the dynamics of \u03c3 and \u03b8.\u201d"
  },
  {
    "topic_or_construct": [
      "Sensitivity (noise parameter)"
    ],
    "measured_by": [
      "Change Detection task \u2013 Absolute Difference (MAD) model"
    ],
    "justification": "Same passage states that \u03c3 is the \u201csensitivity parameter\u201d capturing noise in representations and is estimated from Change Detection trials."
  },
  {
    "topic_or_construct": [
      "Drift rate"
    ],
    "measured_by": [
      "Random Dot Motion task \u2013 Drift Diffusion Model"
    ],
    "justification": "\u201cIn the drift diffusion model, evidence accumulation is controlled by the drift rate parameter \u03b4 \u2026 we fit a single drift rate for each session.\u201d"
  },
  {
    "topic_or_construct": [
      "Decision boundary (boundary separation)"
    ],
    "measured_by": [
      "Random Dot Motion task \u2013 Drift Diffusion Model"
    ],
    "justification": "\u201cThe threshold for decision is controlled by the decision boundary \u03b1.\u201d"
  },
  {
    "topic_or_construct": [
      "Non-decision time"
    ],
    "measured_by": [
      "Random Dot Motion task \u2013 Drift Diffusion Model"
    ],
    "justification": "Reaction times \u201care modeled as the sum of the evidence accumulation time and the non-decision time \u03c4, which captures processes such as stimulus encoding and motor response.\u201d"
  },
  {
    "topic_or_construct": [
      "Risk attitude"
    ],
    "measured_by": [
      "Lottery Ticket task \u2013 Risk-sensitive utility model"
    ],
    "justification": "\u201cWe used a risk-sensitive utility function \u2026 governed by the risk attitude parameter (\u03c1). Higher \u03c1 indicates a higher tendency to prefer risky choices.\u201d"
  },
  {
    "topic_or_construct": [
      "Inverse temperature (choice stochasticity) \u2013 Lottery"
    ],
    "measured_by": [
      "Lottery Ticket task \u2013 Softmax decision rule"
    ],
    "justification": "\u201cWe used a softmax probability function to model participants' choices, with the inverse temperature parameter (\u03b2) as a free parameter.\u201d"
  },
  {
    "topic_or_construct": [
      "Discount rate"
    ],
    "measured_by": [
      "Intertemporal Choice task \u2013 Hyperbolic discounting model"
    ],
    "justification": "\u201cThe tendency to prefer delayed rewards is represented by the discount rate parameter (k). Larger k values indicate greater impulsivity and preference to receive immediate rewards.\u201d"
  },
  {
    "topic_or_construct": [
      "Inverse temperature (choice stochasticity) \u2013 Intertemporal"
    ],
    "measured_by": [
      "Intertemporal Choice task \u2013 Softmax decision rule"
    ],
    "justification": "The hyperbolic model was \u201cadapted \u2026 with a softmax probability function \u2026 governed by the inverse temperature parameter (\u03b2).\u201d"
  },
  {
    "topic_or_construct": [
      "Weight on estimated value difference"
    ],
    "measured_by": [
      "Two-Armed Bandit task \u2013 Probit exploration model"
    ],
    "justification": "\u201cExploitative choices are promoted by the weight of the estimated value difference of the two machines (w_V).\u201d"
  },
  {
    "topic_or_construct": [
      "Weight on relative uncertainty (directed exploration)"
    ],
    "measured_by": [
      "Two-Armed Bandit task \u2013 Probit exploration model"
    ],
    "justification": "\u201cDirected exploration is governed by the weight of the relative uncertainty between the two machines (w_RU).\u201d"
  },
  {
    "topic_or_construct": [
      "Weight on signed total uncertainty (random exploration)"
    ],
    "measured_by": [
      "Two-Armed Bandit task \u2013 Probit exploration model"
    ],
    "justification": "\u201cRandom exploration is controlled by the weight of the signed total uncertainty (w_sign(V)/TU).\u201d"
  },
  {
    "topic_or_construct": [
      "Weber fraction"
    ],
    "measured_by": [
      "Numerosity Comparison task \u2013 Noisy magnitude-encoding model"
    ],
    "justification": "\u201cThe parameter w is the Weber fraction, which controls the magnitude-dependence of noise\u201d in encoding numerosities during the Numerosity Comparison task."
  }
]