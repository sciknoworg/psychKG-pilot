<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling assumptions in inductive generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
							<email>d.navarro@unsw.edu.au.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Dry</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hughes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Kinnell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Schultz</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling assumptions in inductive generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1111/j.1551-6709.2011.01212.x</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Inductive generalization, where people go beyond the data provided, is a basic cognitive capability, and underpins theoretical accounts of learning, categorization and decision-making. To complete the inductive leap needed for generalization, people must make a key &quot;sampling&quot; assumption about how the available data were generated. Previous models have considered two extreme possibilities, known as strong and weak sampling. In strong sampling, data are assumed to have been deliberately generated as positive examples of a concept, whereas in weak sampling, data are assumed to have been generated without any restrictions. We develop a more general account of sampling that allow for an intermediate mixture of these two extremes, and test its usefulness. In two experiments, we show that most people complete simple one-dimensional generalization tasks in a way that is consistent with their believing in some mixture of strong and weak sampling, but that there are large individual differences in the relative emphasis different people given to each type of sampling. We also show experimentally that the relative emphasis of the mixture is influenced by the structure of the available information. We discuss the psychological meaning of mixing strong and weak sampling, and possible extensions of our modeling approach to richer problems of inductive generalization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The ability to make sensible inductive inferences is one of the most important capabilities of an intelligent entity. The capacity to go beyond the data and make generalizations that can hold for future observations and events is extremely useful, and is of interest not only to psychologists, but also to philosophers (e.g., <ref type="bibr" target="#b10">Goodman, 1955)</ref> and researchers with an interest in formal theories of learning (e.g., <ref type="bibr" target="#b45">Solomonoff, 1964)</ref>. From a psychological perspective, experimental research dating back to <ref type="bibr" target="#b33">Pavlov (1927)</ref> demonstrates the tendency of organisms to generalize from one stimulus to another, with learned contingencies being applied to novel but similar stimuli. Critically, in many cases these generalizations do not involve a failure of discrimination. Stimulus generalization is better characterized as a form of inductive inference than of perceptual failure, and indeed the two have a somewhat different formal character <ref type="bibr" target="#b3">(Ennis, 1988)</ref>. As <ref type="bibr" target="#b43">Shepard (1987</ref><ref type="bibr">Shepard ( , p. 1322</ref> notes, "we generalize from one situation to another not because we cannot tell the difference between the two situations but because we judge that they are likely to belong to a set of situations having the same consequence."</p><p>One of the best-known analyses of inductive generalization is <ref type="bibr" target="#b43">Shepard's (1987)</ref> exponential law, which emerges from a Bayesian analysis of an idealized single-point generalization problem. In this problem, the learner is presented with a single item known to belong to some target category and the learner is asked to judge the probability that a novel item belongs to the same category. Shepard's analysis correctly predicts the empirical tendency for these generalization probabilities to decay exponentially as a function of distance in a psychological space (this decay function is called a generalization gradient). The exponential generalization function is treated as a basic building block for a number of successful theories of categorization and concept learning (e.g., <ref type="bibr" target="#b31">Nosofsky, 1984;</ref><ref type="bibr" target="#b19">Kruschke, 1992;</ref><ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a;</ref><ref type="bibr" target="#b22">Love, Medin, &amp; Gureckis, 2004</ref>) that seek to explain how people learn a category from multiple known category members. The analysis by <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref>, in particular, is notable for adopting much the same probabilistic formalism as Shepard's original approach, while extending it to handle multiple observations and cases where spatial representations may not be appropriate (see also <ref type="bibr" target="#b40">Russell, 1986)</ref>. In a related line of work, other researchers have examined much the same issue using "property induction" problems, leading to the development of the similaritycoverage model <ref type="bibr" target="#b32">(Osherson, Smith, Wilkie, Lopez, &amp; Shafir, 1990)</ref>, as well as feature-based connectionist models <ref type="bibr" target="#b44">(Sloman, 1993)</ref> and a range of other Bayesian approaches <ref type="bibr" target="#b13">(Heit, 1998;</ref><ref type="bibr" target="#b41">Sanjana &amp; Tenenbaum, 2003;</ref><ref type="bibr" target="#b18">Kemp &amp; Tenenbaum, 2009)</ref>.</p><p>In this paper we investigate the implicit "sampling" models underlying inductive generalizations. We begin by discussing the ideas behind "strong sampling" and "weak sampling" <ref type="bibr" target="#b43">(Shepard, 1987;</ref><ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a)</ref>, and developing an extension to the Bayesian generalization model that incorporates both of these as special cases of a more general family of sampling schemes. We then present two experiments designed to test whether people's generalizations are consistent with the model, and more specifically, to allow us to determine what sampling assumptions are involved. These experiments are designed so that we are able to look both at the overall tendencies that people display, but also so that we can infer the sampling models used by each individual participant. Our main findings are that there are clear individual differences in the mixture between strong and weak sampling used by different people, and that these mixtures are sensitive to the patterns of observed data. We conclude with a discussion of the psychological meaning of mixing strong and weak sampling, and of possible extensions of our modeling approach to richer problems of inductive generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Models of Inductive Inference</head><p>In Bayesian accounts of inductive inference, the learner makes use of two different sources of information: the pre-existing knowledge that they bring to the task (the prior), and the information in the problem itself (the likelihood). If we let x denote the information in the problem and let h denote some hypothesis about the property to be inferred, then Bayes' theorem implies that</p><formula xml:id="formula_0">P (h | x) = P (x | h) P (h) h P (x | h ) P (h ) .<label>(1)</label></formula><p>The numerator in this expression is composed of two terms: the prior P (h), which acts to characterize the prior beliefs, and the likelihood P (x | h), which describes the probability that one would have observed the data x if the hypothesis h were correct. The denominator is composed of the same two terms, summed over all possible hypotheses (i.e., over all possible h ). When combined in this manner, the prior and the likelihood produce P (h | x), the learner's posterior belief in the hypothesis h. It is important to recognize that Bayesian cognitive models are typically functionalist in orientation: they represent analyses of the computational problem facing the learner <ref type="bibr" target="#b23">(Marr, 1982)</ref>, and tend to remain agnostic about specific psychological processes. For the current purposes, this means that we are interested in determining whether people's generalizations are in agreement with the predictions of a Bayesian analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Priors</head><p>Much of the variation among Bayesian induction models can be characterized in terms of different choices of prior. In the simplest case, the learner has an unstructured set of hypotheses, which form the hypothesis space H, and places some simple (possibly uniform) prior over all h ∈ H. However, it is possible to adopt a more structured approach, in which latent mental representations constrain the hypothesis space and generate a more sophisticated and psychologically plausible prior over the hypothesis space (e.g., <ref type="bibr" target="#b41">Sanjana &amp; Tenenbaum, 2003;</ref><ref type="bibr" target="#b18">Kemp &amp; Tenenbaum, 2009)</ref>. For instance, in Shepard's analysis there is assumed to exist some low-dimensional space in which stimuli are mentally represented, and candidate hypotheses typically correspond to connected regions in that space. Alternatively, hypotheses could be organized into a tree structure, a causal graph, or any of a range of other possibilities (see <ref type="bibr" target="#b17">Kemp &amp; Tenenbaum, 2008</ref>. In our experiments, we restrict ourselves to the case where the prior is constrained by a simple spatial representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Likelihoods</head><p>In contrast to the extensive array of mental representations that can constrain the prior, the likelihood functions considered in the Bayesian literature on inductive reasoning have largely been restricted to two possibilities, "strong sampling" and "weak sampling", with almost no examples of empirical tests of these assumptions existing in the literature (see later). 1 Many models <ref type="bibr" target="#b43">(Shepard, 1987;</ref><ref type="bibr" target="#b13">Heit, 1998;</ref><ref type="bibr" target="#b18">Kemp &amp; Tenenbaum, 2009)</ref> assume that the only role of the likelihood function is to determine whether the data are consistent with the hypothesis. If the data are inconsistent with a hypothesis, then the hypothesis is ruled out and so the likelihood is zero, P (x | h) = 0. If the data are consistent with the hypothesis, then the likelihood function is constant, P (x | h) ∝ 1. Accordingly, the relative plausibility of two hypotheses h 1 and h 2 that are consistent with the data does not change:</p><formula xml:id="formula_1">P (h 1 | x) P (h 2 | x) = P (h 1 ) P (h 2 ) .<label>(2)</label></formula><p>This kind of likelihood function is referred to as "weak sampling". 2 A different approach, introduced by <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref>, suggests that the learner might assume data are generated from the true hypothesis. This "strong sampling" assumption can take many different forms. In the simplest case, a hypothesis h that is consistent with |h| possible observations (i.e., has "size" |h|) is associated with a uniform distribution over these possibilities. As with weak sampling, if hypothesis h is inconsistent with the data x then P (x | h) = 0. However, when the data are consistent with the hypothesis, then P (x | h) = 1/|h|. What this means is that if two hypotheses h 1 and h 2 are both consistent with the data, their relative plausibility now depends on their relative sizes:</p><formula xml:id="formula_2">P (h 1 | x) P (h 2 | x) = |h 2 | |h 1 | × P (h 1 ) P (h 2 ) .<label>(3)</label></formula><p>Supporting the original theoretical work by <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref>, the strong sampling approach has been applied successfully to property induction <ref type="bibr" target="#b41">(Sanjana &amp; Tenenbaum, 2003)</ref>, similarity judgment <ref type="bibr" target="#b29">(Navarro &amp; Perfors, 2010)</ref> and word learning <ref type="bibr" target="#b54">(Xu &amp; Tenenbaum, 2007b)</ref>. Other models are less explicit in their sampling assumptions. For instance, the featural similarity model used by <ref type="bibr" target="#b44">Sloman (1993)</ref> relies on the contrast model for featural similarity <ref type="bibr" target="#b50">(Tversky, 1977)</ref>. It does not explicitly re-weight hypotheses according to their size, and more closely approximates the weak sampling rule in Equation 2 than the strong sampling rule in Equation 3. The similarity-coverage model does not make any explicit statements either, but insofar as the "coverage" term attempts to measure the extent to which the observations span the range of possibilities encompassed by the hypothesis, it is consistent with strong sampling. Relatedly, if the generalization gradients used by various category learning models (e.g. <ref type="bibr" target="#b31">Nosofsky, 1984;</ref><ref type="bibr" target="#b19">Kruschke, 1992;</ref><ref type="bibr" target="#b22">Love et al., 2004)</ref> are interpreted in terms of the Bayesian generalization model, then a strong sampling model would require the steepness of the generalization gradient to increase as a function of the number of exemplars. In some models (e.g. <ref type="bibr" target="#b31">Nosofsky, 1984)</ref> this is not the case, and so we might consider them to be weak sampling models. In other cases, the model allows the width of the the generalization gradient to be adapted over time in order to match the observed data (e.g. <ref type="bibr" target="#b19">Kruschke, 1992;</ref><ref type="bibr" target="#b22">Love et al., 2004)</ref>, and so might be considered to embody a strong sampling assumption, though the mapping is not exact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why do Sampling Assumptions Matter?</head><p>As outlined at the start of the paper, inductive inference is a central problem in cognitive science, and has therefore been studied a great deal. What may be less obvious is the critical importance played by sampling assumptions. Most work in this area has focused on how inductive inferences are shaped by the pre-existing biases of the learner, and on the mental representations that underpin these biases. However, induction is in large part a process in which the learner's beliefs are shaped by data. In order for that to happen, the learner must rely on some (probably implicit) assumptions about the evidentiary value of his or her observations. In other words, he or she needs to have some theory for how the data were sampled, and some way of linking that theory to beliefs about the state of the world. The "strong sampling" and "weak sampling" models are two examples of such a theory, but the basic issue is more general.</p><p>To illustrate the generality of the issue, consider the dilemma faced by 17th century European ornithologists trying to catalogue the birds of the world. To their knowledge, nobody had ever observed a non-white swan. Should they have inferred that all swans are white? This question, identified by <ref type="bibr">Popper (1935</ref><ref type="bibr">Popper ( /1990</ref> while discussing David Hume's problem of induction, is a classic. Although the learner in this instance has a very large number of observations of white swans, the obvious inference (that all swans are white) is of course incorrect. As it turned out, European swans are systematically different to Australian swans, and so the inference fails. A naive learner does not account for the systematic spatial variation, and overestimates the informativeness of the observed data. Indeed, the evidentiary value of observations are shaped by a great many factors. The data might be very old (a new mutation might produce black swans), collected by someone untrustworthy (medieval bestiaries do not provide reliable biological data), or copies of one another (the same swan could be seen multiple times). Moreover, people are often quite sensitive to these factors <ref type="bibr" target="#b1">(Anderson &amp; Milson, 1989;</ref><ref type="bibr" target="#b2">Anderson &amp; Schooler, 1991;</ref><ref type="bibr" target="#b52">Welsh &amp; Navarro, 2007)</ref>, altering their inferences based of their assumptions about the informativeness and relevance of the data.</p><p>These issues are all sampling assumptions: they relate to the learner's theory of how the data are generated. Within the Bayesian framework, such issues are handled through the likelihood function. To return to the specific issues discussed in the previous section, a learner who makes a strong sampling assumption (Equation 3) is clearly relying on a very different theory of the data than one who uses a weak sampling model (Equation 2), and his or her beliefs and inductions change accordingly. In other words, what these equations are saying that the informativeness of a particular observation changes depending on how it was sampled. It is for this reason that the sampling assumptions play a key role in the specification of any Bayesian theory. That being said, these issues are by no means restricted to Bayesian theories. Rather, all theories of learning are reliant on assumptions about the nature of the observations. For instance, connectionist models (e.g., <ref type="bibr" target="#b14">Hinton, McClelland, &amp; Rumelhart, 1986</ref>) implement learning rules that describe how networks change in response to feedback, with different learning rules embodying different assumptions about data. Decision heuristics <ref type="bibr" target="#b6">(Gigerenzer &amp; Goldstein, 1996)</ref> specify a learning method by describing how to estimate "cue validities" and related quantities, with different estimation methods producing different inferences <ref type="bibr" target="#b20">(Lee, Chandrasena, &amp; Navarro, 2002)</ref>. In short, although our approach to this problem is explicitly Bayesian, and covers only some of the issues at hand, the underlying problem itself has much broader scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conservative Learning in a Complicated World</head><p>The previous section illustrates the central role played by sampling assumptions in guiding inductive inferences, and illustrates that the core issue is much more general than the "strong versus weak" distinction. In particular, the focus on the evidentiary value of data is critical. In the weak sampling model described previously, an observation that is consistent with two hypotheses is assumed to convey no information about their relative plausibility. In contrast, a strong sampling assumption means that the observation is highly informative. However, in light of the issues raised in the last section (old data, untrustworthy data, correlated data, and so on) it seems plausible to suspect that some people would adopt a kind of intermediate position: when an observation is consistent with hypotheses h 1 and h 2 , such a learner will make some adjustment to their beliefs, but this adjustment will be less extreme than the |h 2 |/|h 1 | scaling that Equation 3 implies. In fact, it has long been recognized that people generally adapt their beliefs more slowly than the naive application of Bayes' theorem would imply, a phenomenon known as conservatism <ref type="bibr" target="#b36">(Phillips &amp; Edwards, 1966)</ref>. Moreover, one of the main reasons why conservatism might be expected to occur is that people are sensitive to the fact that real-life observations are correlated, corrupted and so on <ref type="bibr" target="#b30">(Navon, 1978)</ref>.</p><p>To illustrate the implications of conservative learning, consider the problem faced by a learner who encounters only positive examples of some category. If all category members are equally likely to be observed, and all observations are generated independently from the true category, then a strong sampling model is the correct assumption to make <ref type="bibr" target="#b48">(Tenenbaum &amp; Griffiths, 2001a)</ref>. However, few if any real life situations are this simple, so while a "naive" learner would adopt a strong sampling model, a more skeptical learner would not. In the most extreme case, our skeptical learner might decide that the sampling process is not informative, and would and up with a weak sampling model as a consequence. What this makes clear, however, is that strong and weak sampling are two ends of a continuum, since it is perfectly sensible to suppose that the learner is not completely naive (not strong) but not completely distrusting (not weak).</p><p>How might such a learner think? To motivate this, consider the problems associated with learning how alcoholic a beer is. The first beer that the world provides to the learner might be a Victoria Bitter (4.6% alc/vol), as is the second. The third beer might be Tooheys New (4.6% alc/vol), and the fourth a Coopers Pale Ale (4.5% alc/vol). If (as per strong sampling) the learner were to construe these as independent draws from a uniform distribution over beers, he or she would have strong evidence in favor the hypothesis that all beers have alcohol content between 4% and 5% by volume. However, a little thought suggests that this inference is too strong. The problem is that sampling scheme here is neither uniform nor independent, making it inappropriate to apply the strong sampling model in a straightforward fashion. In this example, the learner might reasonably conclude that the second Victoria Bitter "token" is really just an example of the same "type" of beer, and conveys no new evidence over the first. Moreover, since Tooheys New and Victoria Bitter are both lagers, one might expect them to be a little more similar to one another than two randomly chosen beers might be. Finally, since all three brands are Australian beers and are often served in similar establishments, it is probable that none of them are truly independent, and so a conservative learner might be unsurprised to discover that Duvel (a Belgian beer) has 8.5% alc/vol. In short, while all four observations are legitimate positive examples of the category "beer", it would not be unreasonable to believe that only one or two of them actually qualify as having been strongly sampled.</p><p>To formalize this intuition, we construct a sampling model in which there is some probability that an observation is strongly sampled (drawn independently from the true distribution) but with some probability the observation conveys no new information, and may therefore be assumed to be weakly sampled. With this in mind, let θ denote the probability that any given observation is strongly sampled, and correspondingly 1 − θ is the probability that an observation is weakly sampled. Then, cleaning up the notation, the learner has the model:</p><formula xml:id="formula_3">P (x, x ∈ h | h, θ) =    (1 − θ) 1 |X | + θ 1 |h| if x ∈ h 0 otherwise,<label>(4)</label></formula><p>where X is the set of all possible stimuli, and |X | counts the total number of possible items. When θ = 0 this model is equivalent to weak sampling, and when θ = 1 it is equivalent to strong sampling. For values of θ in between, the model behaves in accordance with the "beer generation" idea discussed above: only some proportion θ of the observations are deemed to be strongly sampled. As a consequence, this more general family of models smoothly interpolates between strong and weak sampling. What interpretation should we give to this more general family of models? One psychologically plausible possibility is suggested by the beer sampling model discussed earlier: some observations are deemed to be correlated, so much so that while he or she has observed n "tokens" (observations), they correspond to only m "types" (genuine samples from the distribution to be learned), where m = θn &lt; n. This idea has some precedent in the computational learning literature (e.g., <ref type="bibr" target="#b9">Goldwater, Griffiths, &amp; Johnson, 2006)</ref>, and seems very plausible in light of both the conservatism phenomenon <ref type="bibr" target="#b36">(Phillips &amp; Edwards, 1966)</ref> and its rational explanation in terms of violations of independence <ref type="bibr" target="#b30">(Navon, 1978)</ref>. This idea explains why the learner might want to adopt this class of models: if he or she thinks that the world is messy, and generates observations in a correlated fashion rather than independently from the true category distribution, then Equation 4 describes a sensible psychological assumption for the learner to make. Like both strong and weak sampling, it is an idealization: in any particular learning situation, the particular way in which the world turns out to be "messy" will produce a slightly different family of sampling models. Nevertheless, we suspect that the simple idea of reducing n strongly sampled observations to some smaller value m = θn can provide a sensible first approximation to use in many cases.</p><p>More generally, while the primary justification for the model is based on an theory of psychological conservatism, it curious to note that Equation 4 has an interpretation as an example of a "James-Stein type" shrinkage estimator (see <ref type="bibr" target="#b12">Hausser &amp; Strimmer, 2009;</ref><ref type="bibr" target="#b42">Schäfer &amp; Strimmer, 2005)</ref> for the distribution that generates the data. 3 The weak sampling model acts as a low variance high bias estimator, whereas the strong sampling model is high variance and low bias (see, e.g., <ref type="bibr" target="#b11">Hastie, Tibshirani, &amp; Friedman, 2001</ref> for a statistical introduction, and <ref type="bibr" target="#b5">Gigerenzer &amp; Brighton, 2009</ref> for a psychological discussion). Our "mixed" sampling model is based on the assumption that the learner aggregates these two. The interesting point is that James-Stein type estimators have the ability to outperform either of the two estimators from which they are constructed <ref type="bibr" target="#b46">(Stein, 1956;</ref><ref type="bibr" target="#b16">James &amp; Stein, 1961)</ref>. As such, the usefulness of mixed sampling model may extend across a quite broad range of situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Sampling Assumptions</head><p>The Bayesian theory of generalization was developed a decade ago <ref type="bibr" target="#b47">(Tenenbaum, 1999;</ref><ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a)</ref>, with the assumption of strong sampling playing a key role in the extension to <ref type="bibr" target="#b43">Shepard's (1987)</ref> exponential law. The strong-weak distinction appears in a different form within the theoretical literature on computational learning theory, with some things being difficult to learn with a weak sampling model <ref type="bibr" target="#b8">(Gold, 1967)</ref> but other evidence suggesting that strong sampling methods are able to do so <ref type="bibr" target="#b24">(Muggleton, 1997</ref>). Yet to date, the psychological evidence for this theoretical principle is somewhat limited.</p><p>Previous work <ref type="bibr" target="#b47">(Tenenbaum, 1999;</ref><ref type="bibr" target="#b41">Sanjana &amp; Tenenbaum, 2003;</ref><ref type="bibr" target="#b29">Navarro &amp; Perfors, 2010;</ref><ref type="bibr" target="#b54">Xu &amp; Tenenbaum, 2007b</ref><ref type="bibr" target="#b53">, 2007a</ref> provides some support to the notion, but the evidence available tends to be restricted in various ways. The stimuli used tend to be organized into a tree-structured hypothesis space <ref type="bibr">Tenenbaum (2007b, 2007a)</ref> or variation thereof <ref type="bibr" target="#b41">(Sanjana &amp; Tenenbaum, 2003)</ref>, with only a few studies examining generalization with respect to spatially organized stimuli <ref type="bibr" target="#b47">(Tenenbaum, 1999)</ref> or less structured representations <ref type="bibr" target="#b29">(Navarro &amp; Perfors, 2010)</ref>. In some cases, the participants are explicitly told that the stimuli are randomly sampled from the true concept <ref type="bibr" target="#b47">(Tenenbaum, 1999)</ref>, or are given some other more subtle indication of which sampling model is appropriate <ref type="bibr" target="#b53">(Xu &amp; Tenenbaum, 2007a)</ref>. While none of these restrictions seem critical -and indeed tend to reflect various different situations that exist in real life -it does highlight a lack of diversity in the available evidence.</p><p>One respect in which the limited research is quite noticeable is that in all cases the data presented are aggregated across participants. This raises the issue of individual differences <ref type="bibr" target="#b21">(Lee &amp; Webb, 2005;</ref><ref type="bibr" target="#b28">Navarro, Griffiths, Steyvers, &amp; Lee, 2006)</ref>. It has been long known <ref type="bibr" target="#b4">(Estes, 1956</ref>) that this can induce very large distortions in the data. Do people tend to make the same default assumptions about sampling, or is this something that varies across individuals? If so, can the Bayesian generalization theory capture this variation? Indeed, the reassurance that the theory holds for individual people is of particular importance in this case, where the learner receives only positive instances of a category. In such "positive only" learning scenarios there is an ongoing question about what it is theoretically possible for the human learner to actually learn from the data, dating back to <ref type="bibr" target="#b8">Gold's (1967)</ref> theorem regarding language acquisition. The distinction between strong sampling and weak sampling matters in this regard: under weak sampling, positive data are not powerful enough for 9 the learner to acquire some types of complex knowledge structures, whereas under strong sampling a great deal more is learnable (see, e.g., <ref type="bibr" target="#b34">Perfors, Tenenbaum, &amp; Regier, 2006)</ref>. If it were to be the case that only some people apply strong sampling assumptions, then it may be more difficult to use strong sampling as an explanation for phenomena (such as language acquisition) that are genuinely universal across people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimulus Generalization in Psychological Space</head><p>In this section we describe the Bayesian theory of generalization as it applies to stimuli that vary along a single continuous dimension, and illustrate how inductive generalizations change as a function of the sampling assumptions and the prior beliefs of the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning from examples</head><p>Suppose that the learner has observed a single item x, that is known to possess some property. Since stimuli vary along a continuous dimension, we follow the approach taken by <ref type="bibr" target="#b43">Shepard (1987)</ref> and <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref> and assumes that there exists a true "consequential region" (denoted r t ) over which the property is true. If the property of interest is constrained to occupy a single connected region, then we may define the learner's hypothesis space to be R, the set of all such regions. For instance, the stimulus dimension might be the latitude of a geographical location on planet Earth, and the property in question could be whether the location lies within the tropics. In this scenario, the stimulus dimension ranges from a latitude of −90 • to a latitude of 90 • , and the consequential region for the tropics corresponds to the range from −23.5 • to 23.5 • . In general, consequential regions need not correspond to a single interval: for instance, the temperate zone on Earth covers two disconnected regions, the southern temperate zone from −66.5 • to −23.5 • and the northern temperate zone from 23.5 • to 66.5 • . As discussed by several authors <ref type="bibr" target="#b43">(Shepard, 1987;</ref><ref type="bibr" target="#b49">Tenenbaum &amp; Griffiths, 2001b;</ref><ref type="bibr" target="#b27">Navarro, 2006)</ref>, the Bayesian generalization model can be extended to handle these situations without conceptual difficulty, but for the purposes of the current paper the simpler version is sufficient. The basic idea is shown on the left hand side of <ref type="figure" target="#fig_0">Figure 1</ref>, which depicts 10 possible hypotheses that the learner might have about how far the tropics extend.</p><p>When presented with this learning scenario, one natural goal for the learner is to discover the identity of this unknown region. What information does he or she have to work with? Notice that there are two distinct pieces of information inherent in this problem. Firstly, the learner has observed the item x itself (and not some other possible item), which may or may not be a useful clue. Secondly, the learner has discovered that this item x possesses the property of interest: that is, he or she knows that x ∈ r t . Suppose now that the learner hypothesizes that the correct region is r; that is, that r t = r. What is the probability P (r t = r | x, x ∈ r t ) that this hypothesis is correct, in light of the information presented to the learner? We may calculate this probability by applying Bayes' rule (Equation 1), which gives</p><formula xml:id="formula_4">P (r t = r | x, x ∈ r t ) = P (x, x ∈ r t | r t = r) P (r t = r) R P (x, x ∈ r t | r t = r ) P (r t = r ) dr .<label>(5)</label></formula><p>The denominator in this expression is an integral rather than a sum, because the space of possible regions is continuous rather than discrete. In the Appendix we present the solution to all of the integrals relevant to this paper, but for the purposes of explaining the basic ideas, these solutions are not important. As such, it is convenient to ignore the denominator (since it is just a normalizing constant), and note that</p><formula xml:id="formula_5">P (r t = r | x, x ∈ r t ) ∝ P (x, x ∈ r t | r t = r) P (r t = r).<label>(6)</label></formula><p>This makes clear that the learner's degree of belief in r depends on two things. Firstly, it depends on the extent to which he or she originally believed that r was the true region, which is expressed by the prior probability P (r t = r). Secondly, it depends on the likelihood</p><formula xml:id="formula_6">P (x, x ∈ r t | r t = r)</formula><p>, which describes probability that the learner would have observed x and learned that x ∈ r t if the learner's hypothesis were correct. We will discuss these two terms in detail shortly, but notice that at a bare minimum the likelihood term</p><formula xml:id="formula_7">P (x, x ∈ r t | r t = r)</formula><p>is zero if the hypothesized region does not contain the item (i.e., if x / ∈ r). As noted by <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref>, the theory extends to multiple examples in a very simple way. We now imagine that the learner has encountered n items that possess the property, corresponding to the observations x = (x 1 , . . . , x n ). In this situation, the learner knows that the items x have been generated, and also knows that these items belong to the true consequential region, x ∈ r t . If we suppose that the items are conditionally independent 4 and apply Bayes' rule, we obtain the expression</p><formula xml:id="formula_8">P (r t = r | x, x ∈ r t ) ∝ P (x, x ∈ r t | r t = r) P (r t = r) (7) = n i=1 P (x i , x i ∈ r t | r t = r) P (r t = r).<label>(8)</label></formula><p>Given the simplicity of this expression, multiple items may be handled easily. Moreover, since it is constructed from the same two functions (the prior and the likelihood), there is nothing conceptually different about the multiple item case. Again, noting that the likelihood is zero for any region r that does not contain all of the training items x, one part of the learning process is to eliminate all hypotheses that are inconsistent with the data. To continue with the "tropics" example earlier, the learner might be told that Cairns (latitude −16.9 • ) and Singapore (latitude 1.3 • ) lie in the tropics. On the basis of this knowledge, many of the originally possible regions are eliminated, as illustrated on the right hand side of <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Making predictions</head><p>When solving an inductive generalization problem, the learner's task is slightly more complex. Rather than needing to infer the true consequential region associated with the property, the learner needs to determine if some novel item y also shares the property. In the tropics example, this might correspond to a situation in which the learner has been told that Cairns and Singapore lie in the tropics, and is asked to guess if Brisbane (latitude −27.4 • ) also lies in the tropics. Formally, the problem at hand is to infer the probability that y ∈ r t given that the learner has observed x and knows that x ∈ r t . Clearly, if the identity of the true region is known to the learner then this problem becomes trivial. However, the learner does not have this knowledge. Instead, what the learner has is a set of beliefs about the plausibility of different regions, as captured by P (r t = r | x, x ∈ r t ) as discussed earlier. If the learner weights each hypothesis according to its probability of being correct, then he or she can infer that the probability that the new item also possesses the property, as follows:</p><formula xml:id="formula_9">P (y ∈ r t | x, x ∈ r t ) = R P (y ∈ r t | r t = r) P (r t = r | x, x ∈ r t ) dr.<label>(9)</label></formula><p>In this expression the P (y ∈ r t | r t = r) term is very simple: it is just the "probability" that y falls inside some known region r. Therefore, it equals 1 if y is inside this region and 0 if it is not. Not surprisingly then, only those regions that include y make any contribution to the integral in Equation 9, and so we can simplify this expression by restricting the domain of the integration to R y , the set of regions that contain y. Thus we may write</p><formula xml:id="formula_10">P (y ∈ r t | x, x ∈ r t ) = Ry P (r t = r | x, x ∈ r t ) dr,<label>(10)</label></formula><p>As noted earlier, the solution to this integral is presented in the Appendix for all cases relevant to this paper. However, in order to understand the behavior of the model, it suffices to note that all this integral does is "add up" the total degree of belief associated with those regions that include y. To return again to our tropics learning example in <ref type="figure" target="#fig_0">Figure 1</ref>, there are five hypotheses that are consistent with the training data (Singapore and Cairns). Of those five hypotheses, four also contain the query item (Brisbane). Therefore, if the learner treats all five of these hypotheses as equally likely, he or she would rate the probability of Brisbane being tropical at 80%. Of course, in the actual model the learner's hypothesis space consists of all possible regions, not just the ten shown in the <ref type="figure">Figure,</ref> but the basic principle is the same: the learner generalizes using only those hypotheses that are consistent with the data. The more important issue is whether the learner would really treat all of the data-consistent hypotheses as equally plausible. To answer that question, we must now look at the priors and the likelihoods in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Priors and likelihoods</head><p>To complete the model, we need to specify P (r t = r), the learner's prior degree of belief in region r, and the likelihood function P (x, x ∈ r t | r t = r). As discussed earlier, the family of likelihood functions that we consider in this paper are those that correspond to the "mixed sampling" model (Equation 4). In order to do so we may (without loss of generality) assume that the range X over which stimuli can vary is the unit interval, [0, 1]. In the tropics example, for instance, we can do this by dividing the latitude by 180 and then adding 0.5. In these new co-ordinates, the tropics correspond to the region that runs from 0.37 to 0.63. Having made this assumption, the mixed sampling likelihood functions are given by</p><formula xml:id="formula_11">P (x, x ∈ r t | r t = r, θ) =    (1 − θ) + θ 1 |r| if x ∈ r 0 otherwise .<label>(11)</label></formula><p>where |r| denotes the size of the region, and as discussed previously θ = 0 yields the weak sampling model and θ = 1 is the strong sampling model. In these normalized co-ordinates, therefore, the size of the tropics is 0.63 − 0.37 = 0.26. What degree of prior belief should the learner place in the hypothesis that region r is the correct one? One natural possibility is the uniform prior, P (r t = r) ∝ 1, in which the learner treats every possibility as equally likely. However, this is not the only possibility that the learner might consider. Consider the region r that covers the interval <ref type="bibr">[l, u]</ref>. The size of this region is given by |r| = u − l, and the location of this region is the center c = (u + 1)/2. In keeping with <ref type="bibr" target="#b43">Shepard (1987)</ref> we assume that it is only the size of a region that matters, not its location. With this in mind, one natural choice of prior is the simple one-parameter Beta(1, φ) model, in which</p><formula xml:id="formula_12">P (r t = r) ∝ |r| φ−1 .<label>(12)</label></formula><p>Although simple, this prior is quite principled. It has the same structure as the likelihood function (i.e., size raised to some power), which means that φ can be interpreted as pseudodata. That is, increasing φ by one has exactly the same effect on the generalization function as decreasing the sample size by one. In other words, if the learner's pre-existing knowledge can be described "as if" it corresponded to a set of fictitious previous observations, then the prior distribution should take this form. The result is a family of priors in which φ = 1 corresponds to a uniform distribution over possible regions, whereas φ &lt; 1 expresses a prior assumption that the region is small, and φ &gt; 1 corresponds to a prior belief that the region extends over a larger range. To illustrate this idea, <ref type="figure" target="#fig_1">Figure 2</ref> shows the prior distribution that would be involved in the tropics example, if the 10 hypotheses shown in <ref type="figure" target="#fig_0">Figure 1</ref> were in fact the entire hypothesis space. When φ = 1 (middle panel), all regions are treated as equally plausible a priori. A prior that is biased towards small regions (φ = 0.5) is shown in the left panel, while the right panel depicts a bias towards larger regions (φ = 2).  <ref type="figure">Figure 3</ref> . The tropics posterior. Lower panels show 5 hypotheses that are consistent with the observed data (i.e., the fact that Singapore and Cairns are both tropical), while the upper panels show the posterior degree of belief that the learner might have, depending on the sampling assumptions θ (assuming that φ = 1). The black bars depict hypotheses that would predict Brisbane to be tropical as well, while the grey bars correspond to the hypothesis that predicts Brisbane not to be tropical. Now that we have both the prior and the likelihood specified, the influence of the sampling assumptions θ can be made explicit. <ref type="figure">Figure 3</ref> plots three posterior distribution for the tropics problem, where the prior distribution is assumed to be uniform (as per the middle panel of <ref type="figure" target="#fig_1">Figure 2</ref>), using a weak sampling model (θ = 0), a strong sampling model (θ = 1) and an intermediate model <ref type="bibr">(θ = .33)</ref>. Note that the pattern of falsification is the same in all three cases: regardless of the value of θ, hypotheses 1, 2, 5, 6 and 10 are all inconsistent with the data, because the corresponding regions do not contain both Singapore and Cairns. Thus the posterior probability for these regions is zero. In weak sampling (left panel), this is the only change from the prior distribution, and so the five remaining hypotheses are equally weighted. Since the only one of the remaining hypotheses not to contain Brisbane is hypothesis 3 (shown in grey), in this case the learner would say that there is an 80% chance that Brisbane lies in the tropics. However, if θ &gt; 0 then the likelihood favors the smallest regions that are consistent with the data (i.e., contain Singapore and Cairns). Thus, the likelihood will always favor hypothesis 8 most strongly, followed by hypotheses 3, 7, 4 and 9 in that order. When θ is near 1, this effect is very strong (right panel of <ref type="figure">Figure 3</ref>) whereas for θ close to 0 this effect is much smaller. As a result, when θ = .33 the learner's estimate of the probability that Brisbane is tropical falls to 77%, which drops to 75% when θ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constructing a Generalization Gradient</head><p>Up to this point, the tropics example that we have used to illustrate model behavior has relied on an ad hoc hypothesis space consisting of only 10 possible consequential regions. Moreover, we have only examined the model predictions for a single query item (i.e., Brisbane, latitude −24.4 • ). However, in order to understand the overall behavior of the model, we need to remove both of these restrictions. To start with, suppose we expanded the hypothesis space to include all regions where the edge points are a member of the set . <ref type="bibr">05, .15, . . . , .85, .95</ref> (that is, a very crude discrete approximation to R). Using this expanded hypothesis space, we can then calculate the posterior distribution over possible regions P (r t = r | x, x ∈ r t ) for different choices of the two parameters θ and φ, as well as the generalization probability P (y ∈ r t | x, x ∈ r t ) for all possible query items y.</p><p>In the simplest case, suppose the learner has a uniform prior over regions φ = 1 and applies a weak sampling model θ = 0. This case is illustrated in the left panels of <ref type="figure">Figure 4</ref>. The lower part of the plot draws all of those regions that are consistent with the two observations (Singapore and Cairns). Since all of these regions were equally likely in the prior, and the weak sampling likelihood function only acts to falsify regions that do not contain the data, all of these regions remain equally likely in the posterior (illustrated schematically by the fact that all of the lines are of the same thickness). In the top part of the panel, we plot the generalization probability P (y ∈ r t | x, x ∈ r t ) for all possible latitudes of the query item y. Since all of the regions in the lower panel are weighted equally, for any given value of y this probability is just a count of the proportion of non-falsified regions that contain the query item.</p><p>When we use other parameter values, it is no longer the case that all regions are equally likely. For instance, suppose the learner switched from a weak sampling model to a strong sampling model, by increasing θ from 0 to 1. It is now the case that the likelihood function strongly favors the smallest regions that contain both Singapore and Cairns. This situation is shown in the middle panel of <ref type="figure">Figure 4</ref>. The lower part of the plot schematically illustrates the effect of the likelihood on the posterior distribution over regions: smaller regions are more likely, and are thus drawn with thicker lines. The upper panel shows what effect this has on the generalization gradient: it "tightens" around the data, because the narrowest regions are preferred by the learner.</p><p>The right hand panel of <ref type="figure">Figure 4</ref> shows what influence the prior has, by raising φ from 1 to 5. By choosing a prior with φ = 5, the learner has a prior bias to believe that the region is large (i.e., that the tropics covers a very wide band of latitudes). Even though the learner has a weak sampling likelihood function (i.e., θ = 0) the posterior distribution in the lower panel is quite uneven, as a result of the learner's prior biases. In this case, because the prior bias was to favor large regions, the effect of the generalization gradient is the opposite of the effect we observed in the middle panel: the generalization gradient spreads out, and in fact becomes convex.</p><p>One critical implication of this phenomenon should be made explicit: the effects of φ and θ are very similar if we consider only a single generalization gradient. That is, raising θ has the same effect as reducing φ. Indeed, this is a general property of Bayesian models. In order to disentangle the influence of the learner's prior beliefs (φ) from their assumptions about data (θ) it is necessary to look at how the overall pattern of generalizations changes across multiple generalization gradients.</p><p>Obviously, the stepped generalization functions shown in <ref type="figure">Figure 4</ref> are not psychologically plausible. This lack of smoothness is an artifact of limiting the hypothesis space to a discrete approximation. If we expand the hypothesis space to consist of all possible regions, the problem becomes continuous, as do the corresponding generalization gradients. In the Appendix, we present solutions to the integrals involved in the continuous case, allowing us to directly calculate smooth generalization gradients rather than relying on numerical approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Gradients in the Full Model</head><p>Having illustrated how the model predictions are constructed, we now turn to a discussion of what kinds of generalization patterns can be produced by varying the prior (via φ) and the likelihood (via θ). Of the two parameters, the one in which we are interested is θ, but it is important to show how to disambiguate the two. To do this, consider what happens to the generalization gradients when the learner makes three observations, but sees them one at a time. For simplicity, assume that φ = 1 so the learner treats all regions as equally plausible a priori. The generalizations that are possible in this situation are illustrated in <ref type="figure">Figure 5</ref>. In the weak sampling case, the generalization gradients are linear and do not tighten. 5 Critically, notice that for any θ &gt; 0 the generalization gradients get tighter as more data are observed (from left to right). As will become clear when we discuss the priors, this is the critical prediction -the nonlinear shapes of the gradients in the lower panels can easily be mimicked by weak sampling if we choose the priors in the right way, but the tightening from left to right cannot. This is illustrated in <ref type="figure">Figure 6</ref>, which shows what happens when we vary φ (while keeping θ fixed at 0). The prior cannot influence the  <ref type="figure">Figure 4</ref> . Construction of the generalization gradients. The lower three panels show a collection of hypotheses that are consistent with the training data in the tropics problem (i.e., Cairns and Singapore), where the width of each line illustrates the degree of belief that the learner has in that hypothesis. The generalization gradients at the top are constructed by calculating the probability of being tropical for all possible query items, as a function of their latitude. The differences between the plots arise due to differences in sampling models and priors. manner in which generalizations change as a function of data. It is this critical regularity that we seek to investigate experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment One</head><p>The question we want to investigate in this experiment is the extent to which people change their generalizations as a function of the number of observations they make. According to the weak sampling hypothesis (as per <ref type="bibr" target="#b43">Shepard, 1987;</ref><ref type="bibr" target="#b13">Heit, 1998;</ref><ref type="bibr" target="#b18">Kemp &amp; Tenenbaum, 2009)</ref>, the generalization gradients should not tighten purely because more data are available. In contrast, the strong sampling hypothesis (as per <ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a;</ref><ref type="bibr" target="#b41">Sanjana &amp; Tenenbaum, 2003)</ref> implies that the generalization gradients should narrow as data arrive, even if the new observations fall within the same range as the old ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Participants were twenty-two undergraduates (16 female, 6 male) from the University of Adelaide, who were given a $10 book voucher for their participation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query item</head><p>Query item <ref type="figure">Figure 5</ref> . The effect of varying the sampling model θ, for a situation in which the learner places uniform priors over all possible connected regions that contain the training items. The top panels show the weak sampling case (θ = 0), with the three panels showing the situation when the learner has observed one, two and three training items respectively. Note that in the weak sampling case, the generalization gradients do not tighten. In contrast, the middle panel shows an intermediate case (θ = .33) while the lower panel shows the strong sampling case (θ = 1); in both cases the gradients get steeper from left to right. Note also that including some strong sampling in the mixture has a clear effect, in the sense that the curves for θ = .33 are more similar to those for θ = 1 than they are for θ = 0.  <ref type="figure">Figure 6</ref> . The effect of varying φ, when the sampling model is weak (θ = 0) for a case involving three training items (black dots). The middle row shows the case where the prior is uniform (φ = 1), and is thus identical to the top row in <ref type="figure">Figure 5</ref>. The top row shows a situation where the learner has a prior bias to expect the region to be small (φ = .05), yielding convex gradients that look very similar to those observed in the lower panels of <ref type="figure">Figure 5</ref>. The bottom row, on the other hand, shows what happens when the learner has a prior bias to believe that the region will be large: the generalization gradients become concave. Bandicoot foraging hours <ref type="figure">Figure 7</ref> . The experimental design. Each panel corresponds to one of the three scenarios, and shows the three different sets of stimuli known to possess the property. The tick marks are located at each of the test points. See main text for details.</p><p>Materials &amp; Procedure. Participants were asked about three different induction problems, presented in a random order via computer. All problems involved stimuli that varied along one continuous dimension. One problem involved the temperatures at which a bacterium can survive, another asked about the range of soil acidity levels that produce a particular colored flower, and the third related to the times at which a nocturnal animal might forage. The cover story then explained that a small number of "training" observations were available -in the bacterial scenario, for instance, it would indicate temperatures at which the bacterium was known to survive -and asked participants to make guesses about whether or not the property generalized to different stimulus values (the "query" items). The cover stories are provided in the Appendix.</p><p>To minimize any differences between the psychological representation of the stimuli and the intended one, participants were shown a visual representation of the data, which marked the locations of training items with a black dot, and the query item was shown using a red question mark. The cover stories were constructed to imply that the to-be-inferred property did not hold for beyond the range shown on screen. Responses were obtained by allowing people to position a slider bar using the mouse, with response values ranging from 0% probability to 100% probability. Once the participant was satisfied with the positioning of the slider bar, they clicked a button to move to the next question. For every set of training data, we presented 24 query items, spread evenly across the range of possible items and presented in a random order.</p><p>Since it is impossible to infer the sampling model from a single generalization gradient, we measured three generalization gradients for each problem, as illustrated in <ref type="figure">Figure 7</ref>. Initially, participants were asked to make generalizations from a small number of observations. These were then supplemented with additional observations, and people were asked to generalize from this larger sample. Finally, the data set was expanded a third time to allow us to measure a third generalization gradient. In effect, we present people with three samples-collectively referred to as a "data structure"-and obtain the corresponding generalization gradients. The assignment of cover stories (bacterial, soil, foraging) to data structures was fixed rather than randomized, as illustrated in <ref type="figure">Figure 7</ref> (we address this in experiment two). For example, the top panel shows the data structure associated with the bacterial cover story. Each of the three rows corresponds to the three samples: three known observations were initially given, which was then expanded to five data points, and then finally to a sample of ten observations. In total, each participant made 216 judgments (3 scenarios × 3 samples × 24 queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Results</head><p>Figure 8 plots all 216 judgments made by two of the participants. In both cases the judgments appear sensible, but it is clear that the generalization gradients are quite different from one another. Indeed, inspection of the raw data across all 22 participants made clear that individual differences are the norm rather than the exception. With this in mind, in the model-based analyses we present later in this section we took care not to average data across participants. Moreover, later in the paper we examine the individual-level data in some detail. However, before modeling the data, we present some more basic analyses.</p><p>As discussed earlier, the basic effect that we are looking for is the "gradient tightening" effect shown in <ref type="figure">Figure 5</ref>. That is, do people narrow their generalization gradients as sample size increases? A simple way to test this proposition is to note that the experimental design is such that there are multiple occasions where more data are added to the sample, but at least one of the two edge points remains unchanged <ref type="figure">(Figure 7)</ref>. For instance, one of the seven cases corresponds to the transition from the first to second sample in the bacterial scenario: on the left hand side, the edge point does not change. To a first approximation, it is reasonable to assume that if θ &gt; 0, the generalization probabilities should decrease for the query items that lie beyond this edge. 6 In total, there are seven situations in which the edge point does not change, yielding a total of 64 pairs of queries. Since there are 11 query items to the left of the leftmost training example in this instance, this contributes 11 of the 64 query pairs that we can consider, and thus a total of 22 × 64 = 1408 pairs of responses. For each such pair of queries, we are interested in the difference between generalization probabilities reported by each participant, since this provides a measure of the extent of "gradient tightening." On average, people showed a small but significant amount of gradient tightening: the generalization probabilities are lower on average when the sample size is larger (t 1407 = −4.98, p &lt; .001). Individually, the trend is the correct direction for 51 of the 64 query pairs, and for 17 of the 22 participants. However, notice that this a small effect: the raw judgments vary from a probability of 0% to 100%, but the average change corresponds to a decrease in generalization probability of only 2.3% from one case to the next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-Based Analyses</head><p>The simplified analysis in the previous section provides some evidence that the "gradient tightening" effect is present in these data, but the effect is quite weak, and leaves a great many questions unanswered. Is the tightening effect actually consistent with the strong sampling model proposed by <ref type="bibr" target="#b48">Tenenbaum and Griffiths (2001a)</ref>, or indeed with any of the sampling models discussed in this paper? Is the effect present for all scenarios, or only one or two? Do people tend to rely on the same sampling assumptions, or do people differ in this respect? In order to answer these questions, we need to fit the model to data explicitly. In this section we present an analysis that discusses two key questions: (1) the extent to which the Bayesian model successfully fits the data, and <ref type="formula" target="#formula_1">2</ref>what assumptions about θ are implied by these data. However, although we are careful not to average data across participants because of the individual differences that exist, we do not discuss these differences in any detail in this section: we return to this topic later in the paper.</p><p>The first question of interest relates to the adequacy of the generalization model. In order to address this question, we estimated the best fitting values of θ and φ separately for all 22 participants and all 3 scenarios, yielding a total of 66 parameter estimates (details are provided in the Appendix). We then measured the correlation between human responses and the model predictions at these best-fitting parameters, where the correlation is taken  <ref type="figure">Figure 9</ref> . Correlations between human data and model predictions in experiment 1, displayed for all 22 participants and all 3 scenarios. After making degrees-of-freedom corrections for the two extra free parameters (θ and φ), there are 4 non-significant correlations among the 66 cases, involving participants 5 (scenarios 2 and 3) 8 (scenario 3) and 16 (scenario 16). These four cases are plotted as having a correlation of zero. Of the remaining 62 significant correlations, 61 are significant at the .001 level and 1 is significant at the .01 level. Grey bars plot the thresholds used to classify model performance as "good", "moderate" or "poor". over the 72 judgments (24 queries × 3 cases) that each participant made in each scenario. Note that this set up means that the model needs to predict every single judgment made by every participant: no aggregation of data at any level is involved. The resulting correlations are plotted in <ref type="figure">Figure 9</ref>. After making a degrees of freedom adjustment to account for the two extra free parameters (i.e., θ and φ), almost all correlations (62 of 66) are significant. More importantly, the correlations are quite strong, especially in view of the fine grain at which we are modeling the data: the median correlation is 0.77, with an interquartile range of <ref type="bibr">[.65, .85]</ref>. Overall, it is clear that the model is able to describe the data well. Given that the model fits appear to be quite good, it is natural to ask what values of θ are involved. As the histograms in <ref type="figure" target="#fig_0">Figure 10</ref> illustrate, there is evidence for both strong sampling and weak sampling in this task. However, it is also there are clear differences between people and between scenarios. While the distributions are unimodal and biased towards weak sampling (small θ) in two of the three scenarios, the distribution of estimates for the foraging scenario is rather bimodal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The overall pattern of results to experiment one highlights several points. Firstly, there is quite strong evidence that the Bayesian generalization model is able to describe accurately the behavior of participants at a quite fine-grained level of analysis (prediction of every judgment in the experiment). However, it is important to take note of the technical literature that discusses how to measure safely the adequacy of a model's performance, addressing issues such as model complexity (e.g. I. J. <ref type="bibr" target="#b25">Myung, 2000;</ref><ref type="bibr" target="#b37">Pitt, Myung, &amp; Zhang,</ref>   <ref type="figure" target="#fig_0">Figure 10</ref> . Distributions over θ observed in experiment 1. Each panel plots the distribution of best estimates (grey histograms) across the 20 participants, with each panel corresponding to one of the three scenarios. The black lines plot essentially the same analysis, but using the full posterior distributions over θ rather than just the best estimate for each participant. Reassuringly, the two versions of the analysis are in close agreement.</p><p>2002; J. I. <ref type="bibr" target="#b26">Myung, Navarro, &amp; Pitt, 2006)</ref>, individual differences (e.g. <ref type="bibr" target="#b21">Lee &amp; Webb, 2005;</ref><ref type="bibr" target="#b28">Navarro et al., 2006)</ref> and contaminant processes (e.g. <ref type="bibr" target="#b15">Huber, 1981;</ref><ref type="bibr" target="#b39">Ratcliff &amp; Tuerlinckx, 2002;</ref><ref type="bibr" target="#b55">Zeigenfuse &amp; Lee, 2010)</ref>. In fact, although we glossed over the details, our analysis does in fact accommodate all of these topics: the Appendix provides a discussion of how this was done.</p><p>Secondly, the data are quite informative as regards the default assumptions that people make about how observations are generated (at least in some contexts). Notice that the scenario descriptions did not explicitly state how the data were generated: participants were expected to supply the missing "sampling" assumption on their own. That said, there is a sense in which the data themselves suggest a strong sampling model, since participants only ever observed positive examples of a category. If people relied solely on this as a cue, then we would expect a bias towards larger values of θ. Despite this, the reverse is true: on the whole, smaller values of θ tended to predominate.</p><p>Thirdly, the pattern of variation in the estimated θ values between scenarios is interesting. When the scenarios were originally designed we did not anticipate finding any differences between them: it was assumed that these differences would be presentational gloss and no more. Nevertheless, what we in fact observed is a relatively substantial difference in the distributions over θ. People did seem to have a strong bias towards weak sampling in two of the scenarios, and a weak bias to prefer strong sampling in the third.</p><p>This third point deserves further investigation. Specifically, the fact that we found differences across the scenarios raises additional questions regarding the origin of these differences. In particular, we note that the "foraging" scenario (which induced larger θ values than the other two) differed from the other two problems in two respects. Firstly, as illustrated in <ref type="figure">Figure 7</ref>, the changes in sample size involves an increase from 1 to 3 data points, and from 3 to 5 data points. In contrast, while the other two scenarios also have a jump from 3 to 5, the jump from 5 to 10 data points involves a smaller proportional increase (doubling) than the jump from 1 to 3 (tripling). One possible explanation for the use of strong sampling in this situation is that the ratios in the "1:3:5" structure are larger than in the "3:5:10" case, which (a) makes it easier for us to detect an effect, and (b) may make the sampling regime more salient to people. More generally, the data structures involved in the three cases are noticeably different to one another, suggesting the possibility that this is the source of the difference.</p><p>An alternative explanation involves the cover stories themselves. In particular, the bacterial temperature and soil pH scenarios both suggested an element of "experimental control", whereas the bandicoot foraging scenario did not. Experimental control over the sampling locations means that the absence of data in other locations is uninformative. That is, if an experimenter chooses the locations at which observations are to made, then these locations clearly are not sampled from the true hypothesis. Thus for the purposes of generalization it implies weak sampling. Accordingly, the effect may be due to the cover story and not the data structure. In essence, the cover stories may have subtly given participants instructions as to which sampling model is most plausible. Given that we have multiple hypotheses for the origin of the effect, the next section describes an experiment that disambiguates between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Two</head><p>The goals for experiment two were threefold. Firstly, we aimed to replicate the effects from experiment one using a new set of cover stories. Secondly, we aimed to disambiguate between the "different data structure" explanation from the "implicit instruction" explanation for the scenario differences. Finally, we altered methodological aspects the task slightly, to check that the results are robust to these manipulations. Specifically, queries were phrased as confidence judgments rather than probability estimates, and responses given on a Likert scale rather than by positioning a continuous slider bar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Twenty participants (7 male, 13 female, aged 18-36) were recruited from the general university community. Since the cover story manipulation relied on participants' reading the materials carefully and understanding the nuances, all participants were fluent English speakers, and most were graduate students. All were naive to the goals of the study, though one was familiar with Bayesian generalization models.</p><p>Materials &amp; Procedure. The experiment presented people with a cover story in which they were asked to imagine themselves as an explorer collecting samples of tropical plants and animals, and asked them to make inferences about the different types of foods encountered on the journey. There were three scenarios, relating to "bobo fruit", "walking birds" and "pikki-pikki leaves". After presenting people with introductory text explaining the experiment, text appeared on screen introducing one of the three scenarios.</p><p>Participants were then shown a number of stimuli and asked to make generalizations, under an implicit "strong sampling" or implicit "weak sampling" regime. For instance, if the scenario was bobo fruit and the sampling regime was strong, the text indicated that a local guide gives them delicious fruit to eat. This is presumably a strong sampling situation, since a helpful guide is not going to choose bad-tasting fruit as an example of the local  <ref type="figure" target="#fig_0">Figure 11</ref> . Correlations between human data and model predictions in experiment 1, displayed for all 22 participants and all 3 data structures. All 60 correlations are significant at the .001 level, even after making degrees-of-freedom corrections for the two extra free parameters (θ and φ). Grey bars plot the thresholds used to classify model performance as "good", "moderate" or "poor".</p><p>cuisine. In contrast, under a weak sampling regime the text implied that the participant had collected the fruit themselves, more or less at random.</p><p>Participants responded using the keyboard, indicating their confidence on a nine-point Likert scale (1 = very low confidence and 9 = very high confidence). The locations of the training data and the generalization questions were identical to those used in experiment 1. Each participant saw all three data structures (i.e., all three panels in <ref type="figure">Figure 7)</ref>, and all three cover stories (bobo fruit, walking bird, pikki-pikki leaves). The assignment of cover story to data structure was randomized, and the data structures were presented in a randomized order. Participants either saw all three scenarios in the strong sampling form, or all three in weak sampling form. Twelve participants saw the weak sampling version and eight saw the strong sampling version.</p><p>In order to make the cover story feel more engaging to participants, the actual task was accompanied by thematically-appropriate background music, various props (including pith helmets among other things) and the onscreen display had images of cartoon tropical trees to add to the atmosphere. Qualitative feedback from participants did suggest that they found the task both engaging and enjoyable, and indicated that they did spend some time thinking about the scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Results</head><p>The analysis for experiment two proceeds in the same fashion as for experiment one. As before, we examine the 64 simple test cases to see whether the generalization gradients tighten due to sample size. If we translate the Likert scale responses to probability judgments in the obvious way (i.e., treating 1 as 0%, 9 as 100%, and interpolating linearly for all other values) then what we observe in the data corresponds to an average decrease in generalization probabilities of 3.0%. Once again, this is significant (t 1299 = −6.5, p &lt; .001)  <ref type="figure" target="#fig_0">Figure 12</ref> . Distributions over θ observed in experiment 2. Each panel plots the distribution of best estimates (grey histograms) across the 20 participants, with each panel corresponding to one of the three data structures. (Black lines plot essentially the same analysis, but using the full posterior distributions over θ rather than just the best estimate for each participant).</p><p>but weak effect, of a similar magnitude to the 2.3% decrease found in experiment one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-Based Analyses</head><p>As with experiment one, we found the best-fitting values of θ and φ for each participant and each scenario. The correlations between model predictions and human responses were somewhat higher in experiment two. As shown in <ref type="figure" target="#fig_0">Figure 11</ref>, all 60 correlations were significant, with a median of .90 and interquartile range of <ref type="bibr">[.83, .94]</ref>. Overall, the model performance is at least moderately good in all cases, and is usually very good.</p><p>The main difference between experiment one and experiment two is that the cover story, data structure and the sampling scheme implied by the cover story were all varied independently. After checking that (as expected) there were no effects of the cover story itself (i.e., no differences between bobo fruits, walking birds and pikki-pikki leaves) and no interaction effects, we ran a 3 × 2 ANOVA with main effect terms for data structure and sampling scheme (weak vs strong) implied by the cover story, using the best-fitting value of θ as the dependent variable. The results suggest that the data structure influenced the choice of θ (F 2,52 = 2.98, p = .06), but that people did not make adjustments to θ as a result of the different sampling schemes implied by the cover story (F 1,52 = 0.004, p = .95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Overall, the results from experiment two replicate those from experiment one, and suggest that the differences in θ across conditions are largely due to the data structure and not to an implicit instruction buried in the cover story. Consistent with our original intuition, the cover stories in these experiments are too "generic" to induce strong differences in people's sampling assumptions.</p><p>A question that these results raise is why there was no effect of cover story, especially in light of the results of <ref type="bibr" target="#b53">Xu and Tenenbaum (2007a)</ref>. One possibility might be that the participants simply did not read the cover story closely, and hence did not take the implied sampling scheme into account. This seems rather unlikely, since the participants were explicitly told to pay close attention to the story. This is no guarantee that they did, but most participants did spend some time reading the stories. More plausibly, the manipulation may simply have been too subtle: people may not have given a great deal of thought to the implied sampling assumptions, and made no accommodation of them as a result. This suggests that a more overt manipulation of actual sampling procedures (such as the one used by <ref type="bibr" target="#b53">Xu and Tenenbaum (2007a)</ref>) is likely to have an effect, but subtle differences in cover story may not. In the meantime, it seems that it is the data structure that is the origin of the effect. One possible reason for this might be that people are learning about the sampling model itself as the experiment progresses, and that the 1:3:5 structure induces a different amount of learning to the 3:5:10 scenarios. Future research might address this possibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Sampling Assumptions of Individuals</head><p>Between the two experiments there are a total of 126 posterior distributions over θ (42 participants × 3 scenarios each). In the analyses presented up to this point we have attempted to summarize these individual distributions in terms of the marginal distributions in <ref type="figure" target="#fig_0">Figures 10 and 12</ref>. However, the aggregate distributions do not tell the whole story.</p><p>Firstly, a focus on the aggregate distributions and goodness of fit statistics tends to obscure the actual behavior of people and the model. For instance, consider the comparison between the data from participants 2 and 15 in the bacterial temperatures scenario from experiment 1, shown in <ref type="figure" target="#fig_0">Figure 13</ref>. In the first panel, the generalization gradients are quite similar, because the differences in priors (φ = 1.03 and φ = 1.71 respectively) are almost perfectly balanced by the differences in sampling assumptions (θ = .02 and θ = .47) when the learner has seen only three observations. However, as more data are observed, the two people diverge in their predictions, because the differences in the likelihood functions comes to dominate their beliefs. Analogous examples can be found in experiment 2.</p><p>The opposite effect can be seen by comparing participants 10 and 19 on scenario 3 in experiment 1. These two people have different priors (φ = 1.79 and φ = 5.77 respectively) but the same likelihood (best fitting value is θ = 1 in both cases). When only a single datum is observed (left panels) these participants behave in quite different ways, but these differences begin to disappear as more data arrive (right panels). As is generally the case with Bayesian models, the data "swamp" the prior. That is, in the previous comparison <ref type="figure" target="#fig_0">(Figure 13</ref>), participants made different assumptions about sampling, and so grew more dissimilar as the sample size increased. However, in this second comparison <ref type="figure" target="#fig_0">(Figure 14)</ref>, participants agree about how data are produced: as a consequence, their prior differences are erased as the sample size increases from left to right.</p><p>The second thing that is missing from the earlier analyses is a detailed examination of the posterior distributions over θ for each participant. In analyses to this point we have focused on the best fitting values of θ, giving little consideration to the full posterior distribution. Nevertheless, for each participant and each scenario we have 72 judgments available, so the analysis is worth doing. We used standard Markov chain Monte Carlo techniques (MCMC; <ref type="bibr" target="#b7">Gilks, Richardson, &amp; Spiegelhalter, 1996)</ref> to estimate a complete posterior distribution over θ for every participant and every scenario (assuming a uniform  <ref type="figure" target="#fig_0">Figure 13</ref> . A comparison between partipants 2 (top) and 15 (bottom) on scenario 1 (bacterial temperature), as the sample size is increased (from left to right). The solid black lines are the theoretical generalization gradients, and white circles denote the actual responses given. Black circles show the observations given to the participants. Parameter estimates are θ = .02 and φ = 1.03 for participant 2, and θ = .47 and φ = 1.71 for participant 15. prior). Specifically, we employ a Metropolis sampler with Gaussian proposal distribution, with burn in of 1000 iterations, and the distributions are estimated using 5000 samples drawn at a lag of 10 iterations between successive samples. For experiment one, <ref type="figure" target="#fig_0">Figure 15</ref> plots each of the 66 posterior distributions over θ separately. As is clear from inspection, the apparent bimodality involved in the foraging scenario is an artifact of aggregating over participants. All of the individual distributions over θ are unimodal, but they are centered on different values. For both the bacterial and soil scenarios, most people either adopted a weak sampling model, or else used a mixed sampling model centered on a fairly modest value of θ. For the foraging scenario, however, larger values of θ dominate, with only a few participants adopting weak sampling assumptions. The corresponding plots for experiment two are shown in <ref type="figure" target="#fig_0">Figure 16</ref>, and shows a similar pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>The tendency to observe modest but non-zero values of θ in the experiments is strikingly similar to the phenomenon of conservatism in decision-making: people's belief revision tends to be slower than the rate predicted by simple Bayesian models (e.g., <ref type="bibr" target="#b36">Phillips &amp; Edwards, 1966)</ref>. In the experiments reported here, there is a sense in which θ = 1 is the  <ref type="figure" target="#fig_0">Figure 14</ref> . Comparing two people with different priors and the same likelihood. Data are from participants 10 (top) and 19 (bottom), for the foraging scenario. For both participants the most likely sampling model was a purely strong account, with θ = 1, but the best fitting value of φ differed. Participant 10 uses a prior that has very little by way of bias (φ = 1.79) whereas participant 19 has a quite strong prior bias to expect the concept to have fairly broad extension (φ = 5.77). The key thing to note is that the participants have very different generalization profiles when only a single datum is observed (left panels), but these differences begin to disappear as more data arrive (right panels).</p><p>"correct" sampling model to use, since the experimental design was such that all observed data points were constrained to lie inside the true consequential region r. Nevertheless, although some participants show clear evidence of having done so, most are at least a little conservative (θ &lt; 1). Indeed, some participants do not adjust their generalization gradients at all (θ = 0), appearing to be insensitive to the effects of increasing the sample size (as per <ref type="bibr" target="#b51">Tversky &amp; Kahneman, 1974)</ref>. Conservative sampling assumptions also help resolve an odd discrepancy between two different types of theory. Most category learning models (e.g. <ref type="bibr" target="#b0">Anderson, 1991;</ref><ref type="bibr" target="#b19">Kruschke, 1992;</ref><ref type="bibr" target="#b22">Love et al., 2004)</ref> do not tighten the generalization gradients as the experiment progresses, 7 and are capable of fitting empirical data on the basis of this implicit weak sampling assumption. Our findings are more or less consistent with this: although a small (a) bacterial temperature scenario  <ref type="figure" target="#fig_0">Figure 15</ref> . Posterior distributions over θ for each participant, for all three scenarios in experiment 1. In these plots, we treated the model fit as "good" if the model correlated above 0.8 with the raw data, "moderate" if the correlation was between 0.5 and 0.8, and "poor" if the correlation fell below 0.5. All plots are on the same scale, with θ ranging from 0 to 1 on the horizontal axis and the probability density ranging from 0 to 12 on the vertical axis.  <ref type="figure" target="#fig_0">Figure 16</ref> . Posterior distributions over θ for each participant, for all three data structures in experiment 2. Starred cases correspond to participants who saw the cover story in the strong sampling form. In these plots, we treated the model fit as "good" if the model correlated above 0.8 with the raw data, "moderate" if the correlation was between 0.5 and 0.8, and "poor" if the correlation fell below 0.5. All plots are on the same scale, with θ ranging from 0 to 1 on the horizontal axis and the probability density ranging from 0 to 12 on the vertical axis.  <ref type="figure" target="#fig_0">Figure 17</ref> . Distinguishability from the strong sampling case as sample size increases. Distinguishability is measured as the mean absolute difference in generalization probability across the entire range of possible query items [0,1], where training data always span the range <ref type="bibr">[.3,.6</ref>]. The topmost curve plots the θ = 0 case, with the curves beneath increasing all the way to θ = 1 in increments of 0.1. Except for the pure weak sampling case, all sampling models eventually look indistinguishable from strong sampling as the sample size gets large. number of people did display very strong tendencies to tighten their generalization gradients, most did not. The typical change of 2-3% is small enough that it would probably not be noticed in a great many categorization experiments. This seems particularly likely in view of the fact that all of our problems were "positive evidence only" in design (in which strong sampling is the true model), whereas the majority of category learning experiments involve supervised classification designs (in which weak sampling is the true model). The conservative θ = .2 value even in this case goes a long way toward explaining why strong sampling is not the typical assumption made when modeling category learning.</p><p>In light of this result, the heavy reliance on strong sampling observed in other contexts (e.g., <ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a;</ref><ref type="bibr" target="#b54">Xu &amp; Tenenbaum, 2007b</ref>) might seem unjustified. However, note that these models are typically concerned with learnability questions, often over developmental timeframes. In such cases, the critical problem facing the model is to ask how it is possible for the learner to acquire rich mental representations when exposed only to positive examples <ref type="bibr" target="#b24">(Muggleton, 1997)</ref>, and in order to do so, some variation of the strong sampling assumption is often implicitly adopted. For instance, in a language learning context, the amount of data strictly required to acquire abstract knowledge about syntax may be surprisingly small <ref type="bibr" target="#b34">(Perfors et al., 2006;</ref><ref type="bibr">Perfors, Tenenbaum, &amp; Regier, under review)</ref>: the data presented to the model by <ref type="bibr" target="#b34">Perfors et al. (2006)</ref> corresponds only to a few hours of verbal input. In real life, of course, children require a great deal more data, presumably because they have other things to learn besides syntax, their raw data are messier than preprocessed corpus data, they have processing constraints that slow the learning, and so on. These factors should act to weaken the sampling model, but, as long as the resulting model is not strictly weak (i.e., as long as θ &gt; 0), the learner should eventually acquire the knowledge in question. In fact, as <ref type="figure" target="#fig_0">Figure 17</ref> illustrates for the spatial generalization problem we considered in this paper, as the sample size gets arbitrarily large, every sampling model except weak sampling eventually becomes indistinguishable, because they all converge on the true region. Thus, a learner who consistently employed a θ = .0001 model would acquire knowledge in a fashion that is consistent with both the "strong sampling" pattern used in some models of language acquisition, and the "weak sampling" pattern often found in categorization experiments.</p><p>A number of other avenues for future work present themselves. Even within the context of simple spatial generalization problems, the assumption that the underlying hypothesis space consists solely of connected regions is probably too simple <ref type="bibr" target="#b49">(Tenenbaum &amp; Griffiths, 2001b)</ref>. In particular, people may be willing to entertain the notion that the correct hypothesis consists of multiple regions (see <ref type="bibr" target="#b27">Navarro, 2006</ref>, for a formalization of this idea). To the extent that people believe that categories can cover multiple regions, the effects on the generalization gradients are likely to be twofold. Firstly, consistent with the general findings from our experiments, the generalization gradients over the extrapolation region (i.e., beyond the training data) should tighten more slowly than would be expected under the simpler model, because the learner remains open to the possibility that there exists some additional region of the stimulus space for which the hypothesis is true, about which they have yet to uncover any information. Secondly, the generalization gradients will not remain flat inside the interpolation region, because the learner must entertain the hypothesis that the training data that he or she has seen actually come from multiple regions. In fact, <ref type="figure" target="#fig_5">Figure 8</ref> shows evidence of this effect. Participant 14 (dark lines) decreases the generalization probability in the gaps between training examples, whereas participant 2 (lighter lines) does not. More generally, if we examine the interpolation judgments across experiment one generally there is a weak correlation (Spearman's ρ = −.16, p ≈ 10 −5 ) between the generalization probability and the distance from the nearest training exemplar to the query items. This is both consistent with standard exemplar models, as well as with Bayesian generalization models that allow multiple regions. However, since the experimental designs used in this paper were not optimized for examining interpolative judgments (i.e., there is a strong ceiling effect in these data since most people gave generalization judgments very close to 1 for all interpolation questions) this remains an open question for future work.</p><p>The use of uniform distributions as a basic building block is fairly standard for models in which the learner's goal is to identify a "consequential set" (e.g. <ref type="bibr" target="#b43">Shepard, 1987;</ref><ref type="bibr" target="#b48">Tenenbaum &amp; Griffiths, 2001a;</ref><ref type="bibr" target="#b41">Sanjana &amp; Tenenbaum, 2003;</ref><ref type="bibr" target="#b27">Navarro, 2006)</ref>, and helps avoid the circularity of explaining graded generalizations in terms of graded sampling distributions. However, it is important to recognize that it too is an over-simplification. One of the more interesting extensions that one might consider is the situation in which the learner has to determine both the extension of the consequential set itself (i.e., which things are admissable members of the category) as well as the distribution over those entities (i.e., which things are more typical members of the category). Along similar lines, we have considered only those situations in which the learner observes positive and noise-free data from a single category. Natural extensions to the work would look at how sampling assumptions operate in other experimental designs, and in the presence of labeling noise. In the meantime, however, it seems reasonable to conclude that individual participant's inductive generalizations are in close agreement with the predictions of the Bayesian theory, but vary considerably in terms of the default assumptions about how data were generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Analytic Expressions for the Generalization Model</head><p>In this section, we derive exact expressions for the generalization probabilities referred to in the main text. We start by deriving the basic form for all such probabilities, then dealing with the weak sampling, strong sampling and mixed sampling cases in turn, under the assumption of uniform priors. We then extend this to the non-uniform case.</p><p>Preliminaries. Let x i ∈ X denote the location of the ith observed item, and let x = (x 1 , . . . , x n ) denote the set of items known to belong to the same category. The learner assumes that every element of x belongs to a single true region r t ∈ R. Since the tasks that we consider explicitly provide upper and lower bounds, we may assume without loss of generality that X = [0, 1] and R consists of all closed intervals on X . Given a novel stimulus with co-ordinate value y are interested in P (y ∈ r t | x, x ∈ r t ), the probability that it is also in r t .</p><formula xml:id="formula_13">P (y ∈ r t | x, x ∈ r t ) = R P (y ∈ r t | r t = r)P (r t = r | x, x ∈ r t ) dr<label>(13)</label></formula><p>where r denotes one possibility as to the identity of the true region r t , and the integration is taken over R, the set of all such regions. Noting that P (y ∈ r t | r t = r) equals 1 if y falls inside r and zero if it does not, we can expand the integral, and obtain:</p><formula xml:id="formula_14">P (y ∈ r t | x, x ∈ r t ) = R P (y ∈ r t | r t = r)P (r t = r | x, x ∈ r t ) dr = Ry P (r t = r | x, x ∈ r t ) dr = Ry P (x, x ∈ r t | r t = r)P (r t = r) P (x, x ∈ r t ) dr = Ry P (x, x ∈ r t | r t = r)P (r t = r) dr R P (x, x ∈ r t | r t = r)P (r t = r) dr<label>(14)</label></formula><p>where R y ⊂ R denotes the set of possible regions that include y. We initially assume a uniform prior over regions, P (r t = r) ∝ 1, and hence this expression simplifies to</p><formula xml:id="formula_15">P (y ∈ r t | x, x ∈ r t ) = Ry P (x, x ∈ r t | r t = r) dr R P (x, x ∈ r t | r t = r) dr<label>(15)</label></formula><p>Letting z l = min i x i and z u = max i x i , the requirement that x ∈ r t implies that P (x, x ∈ r t | r t = r) = 0 for any r that does not contain every element of x. Thus, under the uniform prior the denominator in Eq. 15 is given by</p><formula xml:id="formula_16">R P (x, x ∈ r t | r t = r) dr = z l 0 1 zu P (x | r t = [l, u]) du dl<label>(16)</label></formula><p>Similarly, the numerator in Eq. 15 is given by</p><formula xml:id="formula_17">Ry P (x, x ∈ r t | r t = r) dr = min(y,z l ) 0 1 max(y,zu) P (x | r t = [l, u]) du dl<label>(17)</label></formula><p>Clearly, when z l ≤ y ≤ z u , the integrals in Eqs 16 and 17 are identical, and hence the generalization probability is 1. However, for other cases (i.e., y &lt; z l or z u &lt; y) the integrals must be explicitly evaluated. Since the integrals behave differently for strong and weak sampling, we deal with them separately. We begin with the weak sampling case, since it is the simpler of the two. Weak sampling. For weak sampling, P (x | r t = [l, u]) = 1 across the range of integration, so the denominator in Eq. 15 is simply</p><formula xml:id="formula_18">R P (x, x ∈ r t | r t = r) dr = z l 0 1 zu 1 du dl = (1 − z u )z l .<label>(18)</label></formula><p>where u denotes the upper bound on r and l is the lower bound. The numerator in those cases where y &lt; z l or z u &lt; y becomes</p><formula xml:id="formula_19">Ry P (x, x ∈ r t | r t = r) dr = y 0 1 zu 1 du dl = (1 − z u )y if y &lt; z l ,<label>(19)</label></formula><formula xml:id="formula_20">Ry P (x, x ∈ r t | r t = r) dr = z l 0 1 y 1 du dl = (1 − y)z l if y &gt; z u .<label>(20)</label></formula><p>Thus, when we substitute into Eq. 15, under weak sampling the Bayesian theory predicts that the generalization gradients are linear:</p><formula xml:id="formula_21">P (y ∈ r t | x, x ∈ r t ) =      y/z l if y &lt; z l 1 if z l ≤ y ≤ z u (1 − y)/(1 − z u ) if z u &lt; y .<label>(21)</label></formula><p>Strong sampling. Under strong sampling, the story is slightly more complex. For the moment, assume n = 1 and n = 2, since these cases need to be addressed separately. Under strong sampling, each observation is generated independently from a uniform distribution on r t , so</p><formula xml:id="formula_22">R P (x, x ∈ r t | r t = r) dr = z l 0 1 zu P (x | r t = [l, u]) du dl = z l 0 1 zu (u − l) −n du dl = z l 0 (1 − n) −1 (u − l) 1−n 1 zu dl = (1 − n) −1 z l 0 (1 − l) 1−n − (z u − l) 1−n dl = (1 − n) −1 −(2 − n) −1 (1 − l) 2−n + (2 − n) −1 (z u − l) 2−n z l 0 = (1 − n) −1 (2 − n) −1 (1 + (z u − z l ) 2−n − (1 − z l ) 2−n − z u 2−n )<label>(22)</label></formula><p>The integral over R y has the same form, but with the outer integral taken from 0 to y when y &lt; z l , and the inner integral taken from y to 1 when y &gt; z u . Thus, by substitution into Eq. 15, it is straightforward to note that when n &gt; 2, the Bayesian theory with strong sampling predicts that</p><formula xml:id="formula_23">P (y ∈ r t | x, x ∈ r t ) =              1 + (z u − y) 2−n − (1 − y) 2−n − z u 2−n 1 + (z u − z l ) 2−n − (1 − z l ) 2−n − z u 2−n if y &lt; z l 1 if z l ≤ y ≤ z u 1 + (y − z l ) 2−n − (1 − z l ) 2−n − y 2−n 1 + (z u − z l ) 2−n − (1 − z l ) 2−n − z u 2−n if z u &lt; y (23)</formula><p>In the case where n = 1 we observe that,</p><formula xml:id="formula_24">R P (x 1 , x 1 ∈ r t | r t = r) dr = z l 0 1 zu P (x 1 | r t = [l, u]) du dl = z l 0 1 zu (u − l) −1 du dl = z l 0 [ln(u − l)] 1 zu dl = z l 0 ln(1 − l) − ln(z u − l) dl = [(l − 1) ln(1 − l) − l] z l 0 − [(l − z u ) ln(z u − l) − l] z l 0 = ((z l − 1) ln(1 − z l ) − z l ) − ((z l − z u ) ln(z u − z l ) − z l + z u ln z u ) = (z u − z l ) ln(z u − z l ) − (1 − z l ) ln(1 − z l ) − z u ln z u<label>(24)</label></formula><p>Applying the same procedure as before yields the expression</p><formula xml:id="formula_25">P (y ∈ r t | x 1 , x 1 ∈ r t ) =              (z u − y) ln(z u − y) − (1 − y) ln(1 − y) − z u ln z u (z u − z l ) ln(z u − z l ) − (1 − z l ) ln(1 − z l ) − z u ln z u if y &lt; z l 1 if z l ≤ y ≤ z u (y − z l ) ln(y − z l ) − (1 − z l ) ln(1 − z l ) − y ln y (z u − z l ) ln(z u − z l ) − (1 − z l ) ln(1 − z l ) − z u ln z u if z u &lt; y<label>(25)</label></formula><p>In this case, however, the expression can be further simplified since z l = z u = x 1 :</p><formula xml:id="formula_26">P (y ∈ r t | x 1 , x 1 ∈ r t ) =              (1 − y) ln(1 − y) + x 1 ln x 1 − (x − y) ln(x 1 − y) (1 − x 1 ) ln(1 − x 1 ) + x 1 ln x 1 if y &lt; x 1 1 if y = x 1 (1 − x 1 ) ln(1 − x 1 ) + y ln y − (y − x 1 ) ln(y − x 1 ) (1 − x 1 ) ln(1 − x 1 ) + x 1 ln x 1 if x 1 &lt; y<label>(26)</label></formula><p>(Obviously, this expression could be derived directly, rather than found as a special case of the "z l , z u " formulation, but the more general version is useful for the mixed sampling model, which is why we derive it this way). Turning to the case where n = 2,</p><formula xml:id="formula_27">R P (x 1 , x 2 , x 1 ∈ r t , x 2 ∈ r t | r t = r) dr = z l 0 1 zu P (x 1 , x 2 | r t = [l, u]) du dl = z l 0 1 zu (u − l) −2 du dl = z l 0 −(u − l) −1 1 zu dl = − z l 0 (1 − l) −1 − (z u − l) −1 dl = [ln(1 − l)] z l 0 − [ln(z u − l)] z l 0 = ln(1 − z l ) + ln z u − ln(z u − z l )<label>(27)</label></formula><p>Thus,</p><formula xml:id="formula_28">P (y ∈ r t | x 1 , x 2 , x 1 ∈ r t , x 2 ∈ r t ) =              ln(1 − y) + ln z u − ln(z u − y) ln(1 − z l ) + ln z u − ln(z u − z l ) if y &lt; z l 1 if z l ≤ y ≤ z u ln(1 − z l ) + ln y − ln(y − z l ) ln(1 − z l ) + ln z u − ln(z u − z l ) if z u &lt; y<label>(</label></formula><p>28) Mixed sampling. The more general mixed sampling approach implies that with probability θ, items are sampled from the consequential region, but with probability (1 − θ) they are generated independently of the region:</p><formula xml:id="formula_29">P (x i , x i ∈ r t | r t = r, θ) = (1 − θ) + θ|r| −1 .<label>(29)</label></formula><p>So we obtain the expression</p><formula xml:id="formula_30">P (x, x ∈ r t | r t = [u, l], θ) = n k=1 (1 − θ) + θ(u − l) −1 = (1 − θ) + θ(u − l) −1 n = n k=0 ( n k ) (1 − θ) k θ n−k (u − l) n−k<label>(30)</label></formula><p>Having written the posterior probability in this form, it is simple to note that</p><formula xml:id="formula_31">R P (x, x ∈ r t | r = r t , θ) dr = n k=0 ( n k ) (1 − θ) k θ n−k z l 0 1 zu (u − l) n−k du dl (31)</formula><p>and hence</p><formula xml:id="formula_32">P (y ∈ r t | x, x ∈ r t , θ) = n k=0 ( n k ) (1 − θ) k θ n−k min(y,z l ) 0 1 max(y,zu) (u − l) n−k du dl n k=0 n k (1 − θ) k θ n−k z l 0 1 zu (u − l) n−k du dl<label>(32)</label></formula><p>where the integrals in this expression are of the same form as those solved in the previous section.</p><p>Non-uniform priors. Previously we assumed a uniform prior distribution, such that P (r t = r) = P (u, l) ∝ 1. As noted in the main text, we generalize this to a simple family of location-invariant priors, in which P (r t = r) ∝ P (u − l) (note that location-invariance of the prior does not necessarily imply translation-invariant generalization gradients due to the finite range restriction). For X = [0, 1] we adopt a one-parameter Beta(1, φ) family, in which P (u − l) = φ(u − l) φ−1 , for valid choices of u and l. This prior has the same form as the likelihood, allowing φ to be interpreted as pseudo-data and allowing the integrals to be solved simply. First, note that constant of proportionality φ vanishes since it appears in every term in the numerator and denominator when we substitute Eq. 32 into Eq. 15, so we may assume P (u − l) ∝ (u − l) φ−1 . Accordingly, if we let</p><formula xml:id="formula_33">f (w, a, b) = a 0 1 b (u − l) −w du dl<label>(33)</label></formula><p>denote the basic integrals in the model for w &gt; 0, 0 ≤ a ≤ b ≤ 1, and let</p><formula xml:id="formula_34">b(n, k, θ) = ( n k ) (1 − θ) k θ n−k<label>(34)</label></formula><p>denote the binomial probability for k successes out of n trials with success rate parameter θ, then we may write the "complete" model predictions for the two-parameter generalization model as follows:</p><formula xml:id="formula_35">P (y ∈ r t | x, x ∈ r t , θ, φ) = n k=0 b(n, k, θ)f (n − k − φ + 1, min(y, z l ), max(y, z u )) n k=0 b(n, k, θ)f (n − k − φ + 1, z l , z u )<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Additional modeling details</head><p>As noted in the main text, our data analysis also seeks to infer the precision of the data τ , the proportion of observations that are contaminants , as well as an estimate of the participants' calibration, operationalized in terms of the range of the responses (from a lower bound j l to an upper bound j u ) that the participant was willing to give. In this section, we discuss the technical details of the data analysis.</p><p>The first issue that needs to be considered is that of participant caligration. That is, it is important to recognize that the generalization gradients discussed in the main text correspond to a latent subjective probability. Regardless of the method used to elicit judgments from participants, it is likely that people may not report this value in a straightforward fashion (e.g., typically people overestimate small probabilities and underestimate large ones). To address this issue, we assume that the "calibration function" that relates subjective probability to reported value is approximately linear, and is described in terms of the upper bound j u (the expected judgment when the subjective probability is 1) and the lower bound j l (when subjective probability is 0). That is, we link the Bayesian predictions P (y ∈ r t | x, x ∈ r t , θ, φ) about y to typical subjective judgmentsp(y) via a linear calibration function:p (y) = j l + (j u − j l )P (y ∈ r t | x, x ∈ r t , θ, φ)</p><p>where the function is parameterized by j u and j l , the upper and lower bounds on values that the participant is willing to report when making probability judgments. Although one could easily modify this to incorporate the the S-shaped nonlinearities that characterize probability weighting functions (e.g., we did try the Prelec weight function), these distortions appear to be too small to matter for these data, and so we restrict ourselves to the linear case in the interests of parsimony. Besides the issue of calibration, in order to convert the model from a single prediction about human performance to a complete statistical model for the data, we need a formalism for describing measurement error. While fitting models based on simple heuristics such as ordinary least squares (OLS estimation) is very common in psychology, it is not robust in the face of skewed error distributions (which are to be expected when judgements are near 0 or 1) or contamination by outlier data (e.g., <ref type="bibr" target="#b15">Huber, 1981)</ref>. Explicit error theories, complete with outlier detection, represent the correct solution to this issue. With this in mind the Beta distribution is likely to provide a better account of the errors: we use a Beta(1 +pτ, 1 + (1 −p)τ ) in all applications in this paper. For our contaminant model, we assume that on occasions people provide somewhat arbitrary responses, either due to inattention, accidental responding, or any of a range of possibilities. In such cases we expect there to be no relationship at all between the predicted and observed data. To handle these contaminant cases we assume that with some probability , d is sampled from a uniform distribution on [0,1]. Thus, error distribution becomes:</p><formula xml:id="formula_37">d |p, τ, ∼</formula><p>Beta(1 +pτ, 1 + (1 −p)τ ) with probability 1 − Uniform with probability (It should be noted that we did compare the behavior of this approach to standard Gaussian models. The results were fairly similar, but the Gaussian model showed the expected distortions caused by one or two outliers). Having specified the model precisely, a Bayesian approach to data analysis requires that we also set priors over the model parameters. To do so, we set a uniform prior over θ, and for φ we set let P (φ) ∝ φ exp(−φ) to which means that the expected value of φ is 1. We chose to set a prior over the calibration function that was biased towards assuming that the participant was well calibrated, n which P (j u , j l ) ∝ j u (1 − j l ), and we required that j l &lt; j u . The prior on the precision P (τ ) ∝ (τ + 3) −3/2 is chosen so as to induce an approximately uniform prior over the standard deviation of the errors. Finally, we use a very tight prior P ( ) ∝ −3 (with the lower bound fixed at = 10 −4 ) over the contaminant probability, since our assumption is that genuine outliers are rare. When testing the model we varied these assumptions to verify that the results are not highly sensitive to these priors.</p><p>All of the parameter estimates for θ and φ referred to in the main text were constructed by finding the parameter values (θ, φ, j l , j u , τ, ) that had maximum posterior probability. Only those parameters of substantive interest are discussed in the manuscript, to avoid complicating the discussion. However, it is important to recognize that although the model has six parameters, it is not a particularly complex model in terms of its ability to fit data (see I. J. <ref type="bibr" target="#b25">Myung, 2000)</ref>. Firstly, it is important to note that j l , j u and τ are parameters that are in fact present (in slightly different forms) in nearly every model that is assessed in terms of "correlation with the data"; it is just that these parameters are usually "hidden" from view. To see this in a qualitative sense, suppose we had not introduced any of the four auxiliary parameters at all, and simply reported correlations between the generalization functions produced by the basic model (i.e., using θ and φ only). Had we done so, the calculation of the correlation coefficient itself would have relied on an implicit slope, intercept and error variance term (i.e., correlation is the same thing as linear regression). These parameters are part of the statistical model for the data, and are (almost) precisely equivalent to j l , j u and τ . In essence, all we have done by describing these parameters is make the "standard" assumptions of the correlation statistic explicit. Next, consider the role played by : all it does is allow the model to "ignore" a very small proportion of the observations when attempting to estimate the values of θ and φ. We did this for an important reason: we did not want outliers to distort our parameter estimates, since parameter estimates are the core of the paper. However, when we calculated the model fit using correlation coefficients, the influence of was not included. The generalization model was still penalized for failing to predict the outliers. As a consequence, the only thing that can do to the correlation coefficient is make it weaker, because it means that the values of θ and φ are chosen to optimize a slightly different loss function than the standard "least squares" estimator, but the correlation is calculated in those standard terms. Finally, consider the role played by θ and φ. In terms of the generalization functions that can be produced, notice that there are some very severe constraints: they must be unimodal, may not become wider as more observations arrive, and must remain flat across the region spanned by the observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Additional Methodological Details</head><p>The third part of the Appendix describes additional details relating to the experiments reported in the main text. Firstly, here is the relevant text for the "bacterial", "soil" and "foraging" questions in experiment one:</p><p>Bacillus cereus is a type of bacteria that causes food-poisoning. This bacteria is sensitive to temperature, and exposure to very high temperatures (&gt;70 • C) or very low temperatures (&lt;5 • C) will quickly kill the bacteria. In an experiment, food was contaminated with Bacillus cereus and then either heated or chilled to a given temperature. If the experiment found that the bacteria was alive in food that was kept at the temperatures shown as black dots below, what is the probability that it would also be found alive in food kept at the temperature specified by the red question mark?</p><p>The colour of the flowers of Hydrangea macrophylla change depending upon soil pH levels. Soils with a pH of less than 6 produce blue flowers, and soils with a pH greater than 8 produce pink flowers. Neutral soils tend to produce very pale cream flowers. Given that cream flowers were produced by Hydrangeas growing in soils with the pH levels shown as black dots below, what is the probability that Hydrangeas would also produce cream flowers if they were grown in soil with the pH level specified by the red question mark?</p><p>The southern bandicoot is a nocturnal marsupial. It is never seen outside of its burrow in daylight hours. A recent field study examined the foraging habits of the bandicoot. If the field study found that bandicoots were seen foraging at the times shown as black dots in the figure below, what is the probability that they would also be foraging at the time specified by the red question mark?</p><p>For experiment two, the three cover stories related to the "bobo fruit", the "walking bird" and the "pikki-pikki leaves". After presenting people with introductory text explaining the experiment, text appeared on screen introducing one of the three scenarios. The text for these conditions read as follows:</p><p>Your faithful guide Tahu has told you about a local delicacy: the Bobo-Fruit. He explains that they range in color from dark green through yellow to dark brown, and that the dark green and dark brown fruits do not taste very good.</p><p>The Maldive Walking-Bird is notoriously stupid -if you light a cooking fire it will walk up to investigate, allowing you to simply hit it over the head. But what it lacks in intelligence it makes</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A schematic illustration of the tropics example, showing a collection of possible hypotheses (left), a data set and novel query items (right), and showing how only some of the hypotheses are consistent with the data (middle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The tropics prior. Lower panels show 10 different hypotheses the learner might have regarding how far the tropics extend. The top panels plot the prior degree of belief that a learner might have, depending on the value of φ. distribution, φ = 1, θ = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>All 216 judgments made by participant 14 (dark solid lines) and participant 2 (light dashed lines). The gradients are all sensible, but the participants clearly differ from one another.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this is a comment about the likelihood functions used in models for classic inductive reasoning tasks: in the Bayesian literature more generally, there has of course been a much wider range of likelihoods used by modellers.2 It is important to recognize that Equations 2 and 3 are simplifications. They depend both on the "strong versus weak" distinction (namely, the extent to which distribution the generates stimuli is dependent on the distribution over category labels) and on the specific form that the likelihood function takes (uniform distributions in this case). The story is not so simple in general; for simplicity we restrict our discussion to the case relevant to the experiments in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We thank Fermin Moscoso del Prado Martin for drawing our attention to this link.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In view of our discussion about correlated environments, this assumption may seem odd. However, it is important to recognize that conditional independence is not inappropriate: part of the qualitative idea behind the mixed sampling model is to use θ to express the dependencies among items. That is, with probability θ the generation of observation x may be deemed to be formative, but is otherwise deemed to convey no new information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The linearity may seem surprising, since<ref type="bibr" target="#b43">Shepard's (1987)</ref> analysis produced exponential gradients: the difference is partly due to the priors and partly due to the restriction to finite range.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This is not strictly correct, insofar as the behavior of a strong sampling model can sometimes depend on what happens to both of the edge points, not just the nearest one. Even so, the results of this simple analysis are in accordance with the results of the more detailed model fitting presented later in the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">It is true that these models do adjust the generalization gradients in order to accommodate learned selective attention, the effect is generally to tighten generalization along one dimension at the expense of other dimensions. What is not generally part of the model is an overall tightening along all dimensions, as per strong sampling.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>up for in flavour and as such is a staple in the local villagers diet. The Walking-Birds range in size from very small to very large, and you have been told that the small birds are too bony and the large birds are too fatty.</p><p>The leaves of the Pikki-pikki tree contain a mild stimulant that suppresses hunger and provides the local natives with energy during long walks. They also have a pleasant minty taste and are very refreshing to chew. The shape of Pikki-pikki leaves changes with the age of the tree -from elongated through oval to round. You have been told that the elongated leaves of the young trees and the round leaves of the old trees are not as refreshing as the oval leaves of the middle-aged trees.</p><p>An example of a strong sampling instruction:</p><p>Tahu gives you some Bobo-Fruit to eat. They are delicious! You see many different colored Bobo-Fruit hanging on the trees in front of you. Given the color of the delicious fruit that Tahu gave you to eat (represented by the white circles on the scale below) how confident are you that the other different colored fruit that you see (represented by red circles) will be equally as delicious?</p><p>An example of a weak sampling instruction:</p><p>You pick and eat some Bobo-Fruit. They are delicious! You see many different colored Bobo-Fruit hanging on the trees in front of you. Given the color of the delicious fruit that you picked and ate (represented by the white circles on the scale below) how confident are you that the other different colored fruit that you see (represented by red circles) will be equally as delicious?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The adaptive nature of human categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="429" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human memory: An adaptive perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="703" to="719" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reflections of the environment in memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schooler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="408" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Confusable and discriminable stimuli: Comment on Nosofsky (1986) and Shepard</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="408" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The problem of inference from curves based on group data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Estes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="134" to="140" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning the fast and frugal way: Models of bounded rationality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="650" to="669" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Markov chain Monte Carlo in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spiegelhalter</surname></persName>
		</author>
		<editor>D. J.</editor>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>Suffolk</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language identification in the limit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="447" to="474" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpolating between types and tokens by estimating power-law generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fact, fiction, and forecast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some forms of inductive reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rational models of cognition</title>
		<editor>M. Oaksford &amp; N. Chater</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="248" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<editor>D. E. Rumelhart &amp; J. L. McClelland</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="75" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation with quadratic loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fourth berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="361" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The discovery of structural form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="10687" to="10692" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured statistical models of inductive reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="20" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Alcove: An exemplar-based connectionist model of category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kruschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="44" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using cognitive decision models to prioritize E-mails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chandrasena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference of the cognitive science society</title>
		<editor>W. D. Gray &amp; C. D. Schunn</editor>
		<meeting>the 24th annual conference of the cognitive science society<address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="478" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling individual differences in cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="605" to="621" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SUSTAIN: A network model of category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Medin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="309" to="332" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>W. H. Freeman</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from positive data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected papers from the 6th international workshop on inductive logic programming</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="358" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The importance of complexity in model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model selection by Normalized Maximum Likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From natural kinds to complex categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference of the cognitive science society</title>
		<meeting>the 28th annual conference of the cognitive science society<address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="621" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling individual differences using Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="101" to="122" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Similarity, feature discovery, and the size principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Perfors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="256" to="268" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The importance of being conservative: Some reflections on human Bayesian behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Navon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Choice, similarity and the context theory of classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="114" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Category-based induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Osherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="200" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Conditioned reflexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Pavlov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1927" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Poverty of the stimulus? a rational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Regier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference of the cognitive science society</title>
		<meeting>the 28th annual conference of the cognitive science society<address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The learnability of abstract syntactic principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Regier</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>under review. Manuscript under review</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conservatism in a simple probability inference task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="346" to="357" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward a method of selecting among computational models of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="472" to="491" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The logic of scientific discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<publisher>Unwin Hyman</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="438" to="481" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A quantitative analysis of analogy by similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on artificial intelligence</title>
		<meeting>the national conference on artificial intelligence<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian models of inductive generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Sanjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>S. Becker, S. Thrun, &amp; K. Obermayer</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Applications in Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward a universal law of generalization for psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature-based induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sloman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="231" to="280" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A formal theory of inductive inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part I. Information and Control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Inadmissibility of the usual estimator for the mean of a multivariate distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third berkeley symposium on mathematical statistics and probability</title>
		<meeting>the third berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1956" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A Bayesian framework for concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachussets Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished doctoral dissertation</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalization, similarity, and Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="629" to="641" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Some specifics about generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="762" to="778" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Features of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: heuristics and biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Seeing is believing: Priors, trust and base rate neglect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 29th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sensitivity to sampling in Bayesian word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Word learning as Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="245" to="272" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A general latent-assignment approach for modeling psychological contaminants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeigenfuse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="354" to="362" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
