<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Fisher information matrix: A tutorial for calculation for decision making models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Fujita</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensuke</forename><surname>Okada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Katahira</surname></persName>
						</author>
						<title level="a" type="main">The Fisher information matrix: A tutorial for calculation for decision making models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Measuring trait parameters with high estimation precision is important in psychology. In terms of estimation precision, there is a fundamental quantity, the Fisher information, in statistics. This paper introduces the Fisher information from the perspective of estimation precision and experimental stimulus selection. This paper explained the asymptotic efficiency of the maximum likelihood estimator to explain the importance of the Fisher information in statistics. Then, this paper introduced Computerized adaptive testing (CAT), which uses the Fisher information for stimulus selection as an example of application in psychology. In addition, we recommended CAT for cognitive models due to the extent of the effect and low cost of CAT. In section 3, this paper explained how to calculate the Fisher information of decision making models. In section 4, this paper showed simulation example to explain how to use the Fisher information for selecting stimuli in cognitive experiment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a fundamental quantity called the Fisher information matrix <ref type="bibr" target="#b15">(Ly et al., 2017)</ref>. The Fisher information matrix determines the characteristics of statistical estimation and hypothetical testing. The derivatives of log-likelihood are called the score function. The Fisher information matrix is defined as the variance of the score function. For now, it can be said that the Fisher information is something like the amount of information obtained from observations. When a parameter is unidimensional (multi-dimensional), we call the Fisher information (matrix).</p><p>The most important example which implies the importance of the Fisher information matrix for the present paper is that the inverse of Fisher information (matrix) can predict variance (covariance matrix) of the Maximum Likelihood estimator (MLE; <ref type="bibr" target="#b6">Chang, 2015)</ref>. Higher Fisher information leads to higher estimation precision (lower variance of estimation). Further, the example which uses the Fisher information matrix directly in psychological field is Computerized adaptive testing (CAT; <ref type="bibr" target="#b31">Weiss &amp; Kingsbury, 1984;</ref><ref type="bibr" target="#b25">Segall, 2004;</ref><ref type="bibr" target="#b18">Meijer &amp; Nering, 1999;</ref><ref type="bibr" target="#b29">van der Linden, 2018;</ref><ref type="bibr" target="#b28">van der Linden, 1998)</ref>. In CAT, the Fisher information (matrix) is used for selecting stimuli (e.g., quiz of an educa-tional test, an item for questionnaire, reward and reward probability for decision making task) adaptively and optimally.</p><p>Although the Fisher information matrix is a fundamental and important quantity in statistics, there are few cases where it is used effectively in psyc hology or psychological statistics. It will be useful for determining the experimental design, especially selecting stimuli, with the optimal and objective procedures.</p><p>In our understanding, there are some reasons why the Fisher information matrix is not often used in psychology. The first reason is that researchers do not recognize the value of the Fisher information matrix. The second reason is that researchers will have some hurdles in deriving its analytical solution. Therefore, this paper explains why the Fisher information matrix is important, especially for statistical inference in section 2. Then this paper explains how to calculate the Fisher information matrix with some calculation examples in section 3. In addition, this paper provides R code example in section 4.</p><p>2 The importance of the Fisher information matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Maximum likelihood estimation</head><p>This section check prerequisite knowledge, including probability density function, independent and identically distribution, likelihood function, and maximum likelihood (ML) estimation. If you are not familiar with them, you can refer <ref type="bibr" target="#b20">Myung (2003)</ref>. This section is based on his tutorial article. Let y = (y 1 , ..., y N ) be N participants (or trials) observations. Probability density function (pdf) provides probability of observation y i , p(y i |ξ), where ξ are parameters. For instance, normal distribution with mean µ and variance σ 2 is defined with below pdf:</p><formula xml:id="formula_0">p(y i |µ, σ 2 ) = 1 √ 2πσ 2 exp(− 1 2σ 2 (y i − µ) 2 ).<label>(1)</label></formula><p>Observation y is usually assumed independent and identically distributed, which is represented by i.i.d. The assumption that y is generated independent and identically distribution means</p><formula xml:id="formula_1">p(y 1 , ..., y N |ξ) = p(y 1 |ξ) • • • p(y N |ξ),<label>(2)</label></formula><p>which indicates joint distribution, p(y 1 , ..., y N |ξ), can be calculated with the products of marginal distribution, p(y i |ξ). Let L(ξ|y) be likelihood function which is calculated with pdf (e.g., equation (1)). Maximum likelihood estimator (MLE) is defined bŷ ξ = arg max ξ L(ξ|y)</p><p>where arg max ξ provides argument of maximum likelihood. Therefore, MLE can be estimated by solving ∂ ∂ξ log L(ξ|y) = 0 (4)</p><p>where ∂ ∂ξ is differential w.r.t. ξ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Asymptotic characteristic of ML</head><p>To understand the importance of the Fisher information, this section explains the basic fact in statistics, unbiased estimator, information inequality (Cramer-Rao inequality), and asymptotic normality of MLE. Estimation is to map some (real) values from observed data from statistical perspective. There are many candidate as estimation method. Let ξ(χ) be estimator where χ is observation pattern. For extreme example, constant function which map a specific value regardless of observed data can be treated as a estimator. We have to evaluate the goodness of the estimation method.</p><p>One of the evaluation criteria is mean square error (MSE). Let us assume random variable x i is generated by distribution having pdf, p(x|ξ * ), with true parameter value ξ * . Although for simplicity parameter ξ is assumed unidimensional, the case parameters are multidimensional is the same. The estimator is better when it is closer to the true parameter value. We consider the mean square error defined by</p><formula xml:id="formula_3">MSE = E (ξ(χ) − ξ * ) 2<label>(5)</label></formula><p>as criterion. Note that, when continuous data x is generated by</p><formula xml:id="formula_4">p(x), x ∼ p(x), expectation E[x] and variance V ar[x] of x are defined by E[X] = χ xp(x)dx, V ar[X] = χ (x − E[X]) 2 p(x)dx.<label>(6)</label></formula><p>MSE can be decomposed as</p><formula xml:id="formula_5">M SE = V ar[γ(χ)] + E[γ(χ)] − ξ * 2<label>(7)</label></formula><p>where the first term is the variance of the estimator, and the second term is square of bias between the expectation of estimator and true parameter value. They are all positive values. It is better to make MSE smaller. There are two ways to make MSE smaller. One is to restrict estimator to unbiased estimator which has zero bias, i.e., E[ξ(χ)] − ξ * = 0. The second is to reduce the variance of the estimator while permitting bias a bit. We are focusing on the former method. If we restrict the unbiased estimator, the second term of equation (7) becomes 0, therefore MSE becomes variance of estimator exactly. In this sense, the estimator with the smallest variance becomes the best estimator if we restrict the unbiased estimator. Furthermore, the estimator is represented by just ξ hereafter for brevity.</p><p>Although lower variance of estimator means better estimation method, there is a lower bound of variance of the estimator. The below formula, which is called information inequality or Cramer-Rao inequality, provides this lower bound. Letξ be an unbiased estimator of true parameter ξ * , and let F (ξ * ) be the Fisher information. Then, (in some appropriate conditions) the variance of the estimator satisfies</p><formula xml:id="formula_6">V ar[ξ] ≥ 1 F (ξ * ) .<label>(8)</label></formula><p>Although the lower variance of the estimator is better, it can not be smaller than the inverse of the Fisher information when the parameter is unidimensional. That is, the inverse of the Fisher information is bound. In other words, an estimator which can reduce its variance up to the inverse of the Fisher information is the best estimation method in the class of unbiased estimators. The estimator which holds</p><formula xml:id="formula_7">V ar[ξ] = 1 F (ξ * )</formula><p>is called efficient estimator. In this paper, we omit conditions, proof, and multidimensional results of the equation <ref type="formula" target="#formula_6">8</ref>.</p><p>Let us go back to the ML estimator, although the above discussion can be applied to various estimators. ML has various desirable aspects. One of them is asymptotic normality and characteristic of asymptotic variance. ML estimator ξ holdsξ</p><formula xml:id="formula_8">→ N ξ * , 1 F (ξ * )<label>(9)</label></formula><p>in regularity conditions <ref type="bibr" target="#b6">(Chang, 2015)</ref>. This means that the estimator asymptotically follows the normal distribution, which is called asymptotic normality. The mean of normal distribution becomes true parameter value ξ * , and the variance (covariance matrix) of the estimator becomes the inverse of the Fisher information (matrix) asymptotically. Although we omit the regularity conditions and proof of equation <ref type="formula" target="#formula_8">9</ref>, most psychological models hold that equation. In summary of this section, the estimator, the variance of which is the inverse of the Fisher information, is the best from the MSE perspective if we restrict the unbiased estimator. From equation <ref type="formula" target="#formula_8">9</ref>, MLE holds this characteristic asymptotically. This characteristic is called asymptotic efficiency. Furthermore, psychology researchers can optimize not only estimation method (i.e., ML) but also experimental design and stimuli from equation <ref type="formula" target="#formula_8">9</ref>. That is, there is room to reduce variance (i.e., increase the Fisher information) in terms of experimental design and stimuli (e.g., <ref type="bibr" target="#b14">Lindley, 1956;</ref><ref type="bibr" target="#b5">Chaloner &amp; Verdinelli, 1995</ref>) even if we restrict the estimator to MLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Definition of the Fisher information matrix</head><p>Let ξ = (ξ 1 , ..., ξ k ) be k-dimensional parameters. The Fisher information matrix of i-th participant (or trial) for paramter ξ is defined as</p><formula xml:id="formula_9">F i (ξ) = E ∂ ∂ξ log L(ξ|y i ) ∂ ∂ξ log L(ξ|y i ) t<label>(10)</label></formula><p>where ∂ ∂ξ log L(ξ|y i ) is k × 1 column vector, t is transpose symbol. That is, F i (ξ) is k × k matrix. The (m, n) element of the Fisher information matrix</p><formula xml:id="formula_10">is F i (ξ) m,n = E ∂ ∂ξm log L(ξ|y i ) ∂ ∂ξn log L(ξ|y i ) .</formula><p>The expectation is over dependent variables y, assuming model p(y i ) is true. The Fisher information is dependent on parameter values ξ and stimuli (and, of course, model). Conventionally, the stimuli symbol is omitted in the Fisher information matrix as above representation, however, note that the Fisher information is dependent on experimental design and stimuli.</p><p>In addition, when true model is known, below equation</p><formula xml:id="formula_11">E ∂ ∂ξ log L(ξ|y i ) ∂ ∂ξ log L(ξ|y i ) t = −E ∂ 2 ∂ξ 2 log L(ξ|y i )<label>(11)</label></formula><p>holds. That is, researchers can calculate the Fisher information matrix with either square of score function or second derivatives of the log-likelihood function. Which method is easier is dependent on the models. In second derivative's definition, the (m, n) element of the Fisher information matrix is</p><formula xml:id="formula_12">F i (ξ) m,n = −E ∂ 2 ∂ξm∂ξn log L(ξ|y i ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Fisher information for stimulus selection</head><p>We have explained the background theory of computerized adaptive testing (CAT). When the parameter ξ is unidimensional, equation <ref type="formula" target="#formula_8">9</ref>indicates that larger Fisher information means higher estimation precision (lower variance of estimator) ofξ. That is, from equation (8) and equation <ref type="formula" target="#formula_8">9</ref>, ML estimation and stimulus selection based on the Fisher information is the optimal method in terms of estimation precision, though we should meet some conditions such as unbiased characteristic, etc. When the parameters ξ are multidimensional, researchers have to map the Fisher information matrix to a scalar for maximization. One of the most famous methods is to maximize the determinant of the Fisher information matrix <ref type="bibr" target="#b24">(Segall, 1996)</ref>, which is called D-optimal <ref type="bibr" target="#b19">(Mulder &amp; van der Linden, 2009)</ref> stimulus selection.</p><p>Previous researchers have been studying how to select stimuli for parameter estimation <ref type="bibr">(Ahn et al., 2020;</ref><ref type="bibr" target="#b7">DiMattina, 2015;</ref><ref type="bibr" target="#b13">Kontsevich &amp; Tyler, 1999;</ref><ref type="bibr" target="#b26">Toubia, Johnson, Evgeniou, &amp; Delquié, 2013;</ref><ref type="bibr" target="#b30">Watson &amp; Pelli, 1983)</ref> or model selection <ref type="bibr" target="#b10">(Heck &amp; Erdfelder, 2019;</ref><ref type="bibr" target="#b22">Myung &amp; Pitt, 2009;</ref><ref type="bibr" target="#b4">Cavagnaro, Pitt, Gonzalez, &amp; Myung, 2013)</ref>. CAT is stimulus selection method for parameter estimation. One of the merits of CAT (i.e., the Fisher information-based method) is low computational cost. This is because the Fisher information is defined as expectation over observations, which means we do not calculate expectation (integral) numerically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Some comments</head><p>Although CAT is applied in questionnaire and educational test situations <ref type="bibr" target="#b31">( Weiss &amp; Kingsbury, 1984;</ref><ref type="bibr" target="#b17">McGlohen &amp; Chang, 2008;</ref><ref type="bibr" target="#b9">Gibbons et al., 2012;</ref><ref type="bibr" target="#b28">van der Linden, 1998)</ref>, it can be applied psychological, especially cognitive, experiments. It is obvious considering the statistical background. In addition, there are two merits to apply to psychological experiments compared to questionnaires or education tests. One is the cost to apply it. In educational tests, researchers have to collect over 100 participant data to quantify test items as difficulty parameter etc. In contrast, some cognitive experiments which use objective stimulus (e.g., physical stimulation such as sound, length, and so on, or reward, reward probability) do not need to do it because the scale of stimuli is fixed beforehand. The Second is the extent of the stimulus effect. In general, the effect of CAT will be larger when the number of candidate stimuli patterns is large. The number of stimuli patterns in cognitive experiments tends to be larger than that of educational tests. For instance, suppose two alternatives decision making tasks consisting of reward and reward probability. If the number of reward and reward probability candidates are 10, then researchers have to choose stimuli from 10 4 = 10, 000 patterns.</p><p>CAT is the method for adaptive stimulus selection. The adaptive means to select stimuli after collecting participant's observations. This is because the appropriate stimuli with higher information tend to be different among participants (i.e., trait parameters). For example, suppose that researchers have already collected first to j-th trials data. Let F 1:j (ξ) denote the Fisher information matrix of a participant's responses to the first to j-th trials and let F j+1 (ξ) denote the Fisher information matrix based on the candidate stimuli that are presented at (j + 1)-th trial. In the D-optimal selection, stimuli that maximize</p><formula xml:id="formula_13">det(F 1:j (ξ) + F j+1 (ξ))<label>(12)</label></formula><p>are selected as the stimuli of the (j + 1)-th trial to be presented <ref type="bibr" target="#b24">(Segall, 1996)</ref>. Further,ξ is estimates using first to j-th trial observation. The Fisher information matrix can be used for selecting stimuli before conducting experiments (i.e., fixed stimuli design). This is because the Fisher information matrix is calculated with the expectation of a dependent variable, therefore researchers do not need to collect dependent variable observations. Researchers can use the Fisher information matrix as reference information in addition to a previous study's design and researcher's experience. From equation (9), the Fisher information can be used for calculating confidence interval and sample size design for reducing its variance up to specific values <ref type="bibr" target="#b15">(Ly, Marsman, Verhagen, Grasman, &amp; Wagenmakers, 2017)</ref> . Furthermore, <ref type="bibr" target="#b15">Ly et al. (2017)</ref> introduce application of the Fisher information matrix in Jeffreys's prior <ref type="bibr" target="#b11">(Jeffreys, 1946;</ref><ref type="bibr" target="#b12">Jeffreys, 1961)</ref> and minimum description length (MDL; <ref type="bibr" target="#b21">Myung, Navarro, &amp; Pitt, 2006)</ref> which is one of model selection criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Calculation of the Fisher information matrix</head><p>This section explains how to derive the Fisher information matrix step-by-step way. First, this paper introduces the Fisher information matrix of normal distribution because it is the most famous. Second, this paper introduces the Fisher information matrix of the logistic regression model because it is the most basic and useful in terms of much application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normal distribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Log function and exponential function</head><p>Let us check prerequisite knowledge for below calculation.</p><formula xml:id="formula_14">N i=1 a i = a 1 + a 2 + • • • + a N N i=1 a = N * a Π N i=1 a i = a 1 * a 2 * • • • * a N log a b = b * log a!!!F IXM E!!! (A1) log ab = log a + log b, log Π N i=1 a i = N i=1 log a i (A2) log(e x ) = x, exp(log(x)) = x (A3) √ a = a 1 2 , 1 a = a −1 , 1 √ a = (a 1 2 ) −1 = a − 1 2 , (a b ) c = a bc (A4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Calculation of log likelihood function</head><p>Suppose data y i is generated by N (µ, σ 2 ), and we assume i.i.d. That is, suppose</p><formula xml:id="formula_15">y i ∼ N (µ, σ 2 ), i = 1, ..., N.<label>(14)</label></formula><p>The pdf of normal distribution is defined by</p><formula xml:id="formula_16">p(y i |µ, σ 2 ) = 1 √ 2πσ 2 exp − 1 2σ 2 (y i − µ) 2 .<label>(15)</label></formula><p>Calculating log function and using equations (A1) to (A4), this is represented by</p><formula xml:id="formula_17">log p(y i |µ, σ 2 ) = − 1 2 log 2π − 1 2 log σ 2 − 1 2σ 2 (y i − µ) 2<label>(16)</label></formula><p>The entire data y = (y 1 , ..., y N ) is represented by</p><formula xml:id="formula_18">log p(y|µ, σ 2 ) = log Π N i=1 p(y i |µ, σ 2 ) = N i=1 log p(y i |µ, σ 2 ) = − N 2 log 2π − N 2 log σ 2 − N i=1 1 2σ 2 (y i − µ) 2<label>(17)</label></formula><p>using i.i.d assumption and equation <ref type="formula" target="#formula_1">A2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Differential operation</head><p>Let us check prerequisite knowledge about differential. The differential of the function</p><formula xml:id="formula_19">f (x) over x is represented by ∂ ∂x f (x) or f ′ (x). The equations ∂ ∂x (f 1 (x) + f 2 (x)) = f ′ 1 (x) + f ′ 2 (x), ∂ ∂x a * f (x) = a * f ′ (x) (B1) {f (g(x))} ′ = f ′ (g(x)) * g ′ (x) (B2) ∂ ∂x x α = αx α−1 , ∂ ∂x c = 0 (B3) ∂ ∂x e x = e x , ∂ ∂x e f (x) = e f (x) ∂ ∂x f (x) (B4) ∂ ∂x log x = 1 x , ∂ ∂x log f (x) = 1 f (x) ∂ ∂x f (x) (B5) 1 g(x) ′ = − g ′ (x) g(x) , f (x) g(x) ′ = f ′ (x)g(x) − f (x)g ′ (x) g(x) 2 (B6)</formula><p>are used for below calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Calculation of score function</head><p>The differential with respect to µ is represented by</p><formula xml:id="formula_20">∂ ∂µ log p(y|µ, σ 2 ) = 0 − 0 − 1 2σ 2 N i=1 2(y i − µ)(−1) (∵ B1, B2, B3) = 1 σ 2 N i=1 (y i − µ)<label>(19)</label></formula><p>and, the differential with respect to σ 2 is represented by</p><formula xml:id="formula_21">∂ ∂σ 2 log p(y|µ, σ 2 ) = − N 2 1 σ 2 + 1 2σ 2 N i=1 (y i − µ) 2 . (∵ B1, B2, B3, B5) (20)</formula><p>3.1.5 The characteristic of expectation and variance</p><p>Let us check prerequisite knowledge about expectation and variance as below:</p><formula xml:id="formula_22">V ar[X] = E[(x − E[X]) 2 ] (D1) E[X + Y ] = E[X] + E[Y ], E[aX] = aE[X]<label>(D2)</label></formula><p>When X and Y are independent,</p><formula xml:id="formula_23">E[XY ] = E[X]E[Y ].<label>(D3)</label></formula><p>3.1.6 Calculation of the Fisher information matrix</p><p>The Fisher information matrix is 2×2 matrix because there are two parameters, ξ = (µ, σ 2 ). (1, 1) element is the Fisher information of µ and (2, 2) element is that of σ 2 . For the (1, 1) element, we should calculate expectation on the square of the score function. From equation <ref type="formula" target="#formula_20">19</ref>, the square of the score function is</p><formula xml:id="formula_24">∂ ∂µ log p(y|µ, σ 2 ) 2 = 1 σ 4 N i=1 (y i − µ) N j=1 (y j − µ) = 1 σ 4 (y 1 − µ) + • • • + (y N − µ) (y 1 − µ) + • • • + (y N − µ) .<label>(22)</label></formula><p>In this way, the calculation for summation of the product of "observation y imean µ" is often needed in the Fisher information derivation. Let us calculate the part of expectation of equation <ref type="formula" target="#formula_24">22</ref>. When i = j, the expectation is</p><formula xml:id="formula_25">E y [(y i − µ) 2 ] = σ 2<label>(23)</label></formula><p>because y i is generated by i.i.d N(µ, σ 2 ). In addition, from i.i.d and N(µ, σ 2 ) assumption, when i ̸ = j, the expectation is</p><formula xml:id="formula_26">E y [(y i − µ)(y j − µ)] = E y [y i y j − y i µ − y j µ + µ 2 ] = E y [y i y j ] − µ 2 − µ 2 + µ 2 (∵ D2, E[y i ] = µ) = µ 2 − µ 2 − µ 2 + µ 2 (∵ D3) = 0.<label>(24)</label></formula><p>That is, when observation is independent of each other, the expectation on the product of "observation -mean" equals 0. In Item response theory, the assumption that observation is independent of each other (conditional on parameters) is called (local) independent.</p><p>This calculation appears in many models. Assuming local independence, the equation</p><formula xml:id="formula_27">E y N i=1 (Observation i − Mean) N j=1 (Observation j − Mean) = N i=1 E y (Observation i − Mean) 2 = N × Variance (25)</formula><p>holds. Eventually, the expectation of equation <ref type="formula" target="#formula_24">22</ref>, (1, 1) element of the Fisher information matrix, is represented by</p><formula xml:id="formula_28">E ∂ ∂µ log p(y|µ, σ 2 ) 2 = E 1 σ 4 N i=1 (y i − µ) N j=1 (y j − µ) = 1 σ 4 E N i=1 (y i − µ) 2 (∵ local independence) = 1 σ 4 * N * σ 2 (∵ D2, V ar[y i ] = σ 2 ) = N σ 2 .<label>(26)</label></formula><p>Further, the variance of MLE becomes σ 2 N . Although researchers can calculate (1, 2) and (2, 2) elements in the same way, we omit them because they are not used below. In this case, using second derivatives is easier for these derivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logistic regression model</head><p>The second example is the logistic regression model. As we explain below, many decision making models have the common structure with the logistic model. Therefore, this section's derivation is basic and important for below models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Log likelihood function</head><p>Without loss of generality, assume the logistic model having only one independent variable. Let y j , j = 1, ..., J be 0-1 observation (e.g., Yes/No, correct/incorrect). Suppose observation y j is generated by bernoulli distribution (i.i.d) having probability η j (ξ). The parameters are intercept α and slope β, namely ξ = (α, β). The logistic model assumes this probability η j (ξ) is calculated with the logistic function. That is, the logistic model is defined by</p><formula xml:id="formula_29">y j ∼ Bernoulli(η j (ξ)) η j (ξ) = logit −1 {α + βx j } = 1 1 + exp{−(α + βx j )} .<label>(27)</label></formula><p>The pdf of this model is</p><formula xml:id="formula_30">p(y j |ξ) = η j (ξ) yj (1 − η j (ξ)) 1−yj<label>(28)</label></formula><p>because it assumes bernoulli distribution. Using equations (A1, A2) and i.i.d assumption likewise the example of normal distribution, log likelihood can be represented</p><formula xml:id="formula_31">log p(y|ξ) = J j=1 y j η j (ξ) + (1 − y j ) log(1 − η j (ξ))<label>(29)</label></formula><p>where y = (y 1 , ..., y J ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Score function</head><p>The Fisher information matrix of the logistic model is easy to derive. One reason is that the differential of the sigmoid function, like a logistic function, is easy to calculate. In general, the derivative of the sigmoid function is represented by</p><formula xml:id="formula_32">∂ ∂x σ(x) = σ(x)(1 − σ(x))<label>(30)</label></formula><p>where σ(x) is sigmoid function. For example, the derivative of the logistic function can be represented by</p><formula xml:id="formula_33">∂ ∂x logit −1 (x) = ∂ ∂x 1 1 + e −x = − 1 {1 + e −x } 2 e −x (−1) (∵ B6 etc.) = 1 1 + e −x e −x 1 + e −x = logit −1 (x)(1 − logit −1 (x)) (∵ e −x 1 + e −x = 1 + e −x 1 + e −x − 1 1 + e −x ). (31)</formula><p>In Bernoulli distribution case, we have to derive the derivative of choice probability to calculate the differential of log likelihood. In the same way with equation <ref type="formula">31</ref>, the derivatives of probability η j (ξ) with respect to α and β are represented by</p><formula xml:id="formula_34">∂ ∂α η j (ξ) = − exp{−α − βx j } 1 + exp{−α − βx j } 2 (−1) (∵ B6) = η j (ξ)(1 − η j (ξ)) (∵ Third line of 31) ∂ ∂β η j (ξ) = − exp{−α − βx j } 1 + exp{−α − βx j } 2 (−x j ) = η j (ξ)(1 − η j (ξ))x j .<label>(32)</label></formula><p>Using this equation, the derivatives of the log likelihood are represented by</p><formula xml:id="formula_35">∂ ∂β log p(y|ξ) = J j=1 y j 1 η j (ξ) ∂ ∂β η j (ξ) + (1 − y j ) 1 1 − η j (ξ) ∂ ∂β (1 − η j (ξ)) (∵ B5) = J j=1 y j (1 − η j (ξ))x j − (1 − y j )η j (ξ)x j (∵ equation 32) = J j=1 y j x j − y j η j (ξ)x j − η j (ξ)x j + y j η j (ξ)x j = J j=1 x j (y j − η j (ξ)) ∂ ∂α log p(y|ξ) = • • • = J j=1 (y j − η j (ξ)) .<label>(33)</label></formula><p>We abbreviate the derivation of the alpha because it is the same way as the beta case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The Fisher information matrix</head><p>The expectation of bernoulli distribution is E[y j ] = η j (ξ), and the variance of that is V ar[y j ] = η j (ξ)(1 − η j (ξ)). Looking at equation <ref type="formula" target="#formula_35">33</ref>, the summation of the product of "observation -mean" appears likewise normal distribution example. Therefore, using the local independence assumption, we can use the formula of equation <ref type="formula">25</ref>to calculate the Fisher information matrix. The (1, 1) element of the Fisher information matrix (i.e., the Fisher information of α) becomes</p><formula xml:id="formula_36">E ∂ ∂α log p(y|ξ) 2 = E J j=1 (y j − η j (ξ)) 2 J l=1 (y l − η l (ξ)) 2 (∵ (33)) = E J j=1 y j − η j (ξ) 2 (∵ First line of (25)) = J j=1 η j (ξ)(1 − η j (ξ)) (∵ (D2), V ar[y j ] = η j (ξ)(1 − η j (ξ))).<label>(34)</label></formula><p>We can derive (1, 2) and (2, 2) elements in the same way because the difference in score function is only whether it includes x j . The (1, 2) and (2, 2) elements are represented by</p><formula xml:id="formula_37">E ∂ ∂α log p(y|ξ) ∂ ∂β log p(y|ξ) = E J j=1</formula><p>x j y j − η j (ξ)</p><formula xml:id="formula_38">2 = J j=1 x j η j (ξ)(1 − η j (ξ)) E ∂ ∂β log p(y|ξ) 2 = E J j=1 x 2 j y j − η j (ξ) 2 = J j=1 x 2 j η j (ξ)(1 − η j (ξ))<label>(35)</label></formula><p>respectively. In summary, The Fisher information matrix of the logistic model can be represented by</p><formula xml:id="formula_39">F (ξ) = J j=1 η j (ξ)(1 − η j (ξ)) 1 x j x j x 2 j .<label>(36)</label></formula><p>Derivation of a model having multiple independent variables is the same way. For instance, suppose the model which has P independent variables, x jp , and no slope. The (p, p ′ ) element of the Fisher information matrix for this logistic model is</p><formula xml:id="formula_40">J j=1 η j (ξ)(1 − η j (ξ))x jp x jp ′ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decision making model: Delay discounting model</head><p>The next example is the delay discounting (DD; <ref type="bibr" target="#b16">Madden, Begotka, Raiff, &amp; Kastern, 2003;</ref><ref type="bibr" target="#b23">Odum, 2011)</ref> model, which is one of the decision making models. People tend to prefer immediate but small rewards to delayed but large rewards. This is explained by discounting of delayed reward with some ratio (delay discounting rate). In experiments, researchers usually use the task which is consists alternative A (A s ) providing $x s j with time t j = 0 (i.e., immediate reward alternative), and alternative B (A d ) providing $x d j with time t j = d j (i.e., delayed reward alternative). The research object is often to determine the discounting function's shape or estimate the parameter determining discounting function (i.e., discounting rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Log likelihood function</head><p>Some decision making models can be represented by a kind of logistic regression model with elaborated structure in the exponential function of the logistic function. It looks like a logistic regression, the exploratory variables of which are the difference in the alternative's value.</p><p>The DD model assumes</p><formula xml:id="formula_41">y j ∼ Bernoulli(η j (ξ)), j = 1, ..., J η j (ξ) = logit −1 {β(u(A d ) − u(A s ))}, ξ = (k, β) u(A s ) = x s j u(A d ) = x d j 1 + k * t d j<label>(37)</label></formula><p>where u(•) is utility of alternative, k is discounting rate, and β is inverse temperature parameter. The first two lines represent the "general" two alternative choice model, which looks like a logistic regression, the exploratory variables of which are the difference of alternative's value. The calculation of the alternative's utility is different among decision making models. In the DD model, we assumes the immediate reward's utility is as it is, and delayed reward is discounted by hyperbolic function like a denominator. The reward x s j , x d j and time delay t d j are stimuli to be set by experimenter, and ξ = (k, β) is parameter to be estimated.</p><p>The pdf of DD model (decision making model) is</p><formula xml:id="formula_42">p(y j |ξ) = η j (ξ) yj (1 − η j (ξ)) 1−yj<label>(38)</label></formula><p>in the same way as before. Therefore, the log likelihood function is represented by</p><formula xml:id="formula_43">log p(y|ξ) = J j=1 {y j log η j (ξ) + (1 − y j ) log(1 − η j (ξ))}.<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Score function</head><p>The derivation of the score function is the same as before. That is, we have to derive the differential of the log likelihood function after derivation of the differential of choice probability η j (ξ). The differential of η j (ξ) can be represented by</p><formula xml:id="formula_44">∂ ∂k η j (ξ) = − 1 1 + exp{−β(u(A d ) − u(A s ))} 2 exp{−β(u(A d ) − u(A s ))} −β ∂ ∂k u(A d ) (∵ (B6)) = η j (ξ)(1 − η j (ξ)) β ∂ ∂k u(A d ) (∵ Third line of (31)) ∂ ∂β η j (ξ) = η j (ξ)(1 − η j (ξ)) u(A d ) − u(A s )<label>(40)</label></formula><p>in the same way as equations (30) to (32). We abbreviate the derivation about β because it is the same as the k case. Because most decision making models just add elaborated structure in the exponential function, this has the structure of "choice probaility×(1−choice probability)×the differential of argument of exponential function". The score function can be represented by</p><formula xml:id="formula_45">∂ ∂k log p(y|ξ) = J j=1 y j (1 − η j (ξ)) β ∂ ∂k u(A d ) − (1 − y j )η j (ξ) β ∂ ∂k u(A d ) (∵ (40)) = J j=1 β ∂ ∂k u(A d ) (y j − η j (ξ)) ∂ ∂β log p(y|ξ) = J j=1 u(A d ) − u(A s ) (y j − η j (ξ))<label>(41)</label></formula><p>in the same way as equation <ref type="formula" target="#formula_35">33</ref>. This has also a structure of "observationmean" as well as the logistic regression model example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">The Fisher information matrix</head><p>Using the local independence assumption, we can use the formula of equation <ref type="formula">25</ref>in the same way as before. The (1, 1) element of the Fisher information matrix (i.e., the Fisher information of k) can be derived as</p><formula xml:id="formula_46">E ∂ ∂k log p(y|ξ) 2 = E J j=1 β ∂ ∂k u(A d ) 2 (y j − η j (ξ)) 2 = J j=1 η j (ξ)(1 − η j (ξ)) β ∂ ∂k u(A d ) 2<label>(42)</label></formula><p>in the same way as equation <ref type="formula" target="#formula_36">34</ref>. Further, (1, 2) and (2, 2) elements of that can be derived as</p><formula xml:id="formula_47">E ∂ ∂k log p(y|ξ) ∂ ∂β log p(y|ξ) = J j=1 η j (ξ)(1 − η j (ξ)) β ∂ ∂k u(A d ) u(A d ) − u(A s ) E ∂ ∂β log p(y|ξ) 2 = J j=1 η j (ξ)(1 − η j (ξ)) u(A d ) − u(A s ) 2<label>(43)</label></formula><p>respectively. Finally, we have to calculate ∂ ∂k u(A d ). Using model assumption (equation (37)) and equation (B6), ∂ ∂k u(A d ) can be derived as</p><formula xml:id="formula_48">∂ ∂k u(A d ) = −x d j 1 {1 + kt d j } 2 t d j .<label>(44)</label></formula><p>We can calculate the Fisher information using above equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">General formula for two-alternative decision making models</head><p>As we described above, some decision making models look like the logistic regression, the exploratory variables of which are the difference of alternative's values after evaluating each alternative's value separately. In this class, models have the same structure as the Fisher information matrix. Therefore, we derive the "general" formula of the Fisher information matrix for two-alternative decision making models in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Model</head><p>Let ξ = (ξ 1 , ..., ξ P ) be P parameters. Suppose the 0-1 data y = (y 1 , ..., y J ), j = 1, ..., J is generaged by</p><formula xml:id="formula_49">y j ∼ Bernoulli(η j (ξ)), j = 1, ..., J η j (ξ) = logit −1 {g(A j ; ξ) − g(B j ; ξ)} = 1 1 + exp −{g(A j ; ξ) − g(B j ; ξ)}<label>(45)</label></formula><p>where g(•) is function for calculating alternative's value. For now, we do not need to assume a specific function shape to derive the formula. The g(A j ; ξ), g(B j ; ξ) are the value of A and value of B, respectively. The g(•) depends on parameters and stimuli (and obviously, task structure and models). Let D(ξ) = g(A j ; ξ) − g(B j ; ξ) be for simple notation.</p><p>Delay discounting (DD) model For example, DD model assumes g(A j ; ξ) = βu(A s ), g(B j ; ξ) = βu(A d ). The definitions of each symbol are defined by equation (37).</p><p>Expected utility theory (EUT) model Suppose that alternative A j provide $x jA with probability p jA and $y jA with probability 1 − p jA . The alternative B j is defined by the same way. The EUT assumes g(</p><formula xml:id="formula_50">A j ; ξ) = β * V (A j ) = β * {v(x jA )p jA + v(y jA )(1 − p jA )}. V (A j ) is total value of alternative A. v(x jA )</formula><p>is utility of reward $x jA , which is often defined by v(x jA ) = x α jA , where α is a parameter.</p><p>Cumulative prospect theory (CPT) model CPT <ref type="bibr" target="#b27">(Tversky &amp; Kahneman, 1992)</ref> </p><formula xml:id="formula_51">assumes V (A j ) = v(x jA )w(p jA ) + v(y jA )(1 − w(p jA )) if x jA &gt; y jA .</formula><p>The function w(•) is the probability weighting function. One of the most famous definition is w(p) = <ref type="bibr" target="#b27">(Tversky &amp; Kahneman, 1992)</ref>. Furthermore, the logistic regression can be represented with D(ξ) = α+βx j , which means the logistic model is one of the special case of above general model.</p><formula xml:id="formula_52">p γ {p γ +(1−p) γ } 1/γ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Log likelihood function</head><p>As we will describe hereafter, the derivation procedure is almost the same as before. The pdf can be represented by</p><formula xml:id="formula_53">p(y j |ξ) = η j (ξ) yj (1 − η j (ξ)) 1−yj<label>(46)</label></formula><p>because we assumes bernoulli distribution. Therefore, the log likelihood becomes</p><formula xml:id="formula_54">log p(y|ξ) = J j=1 {y j log η j (ξ) + (1 − y j ) log(1 − η j (ξ))}<label>(47)</label></formula><p>in the same way as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Score function</head><p>The differential of choice probability η j (ξ) with respect to m-th parameter ξ m can be derived as</p><formula xml:id="formula_55">∂ ∂ξ m η j (ξ) = − 1 1 + exp(−D(ξ)) 2 exp(−D(ξ)) − ∂ ∂ξ m D(ξ) = η j (ξ)(1 − η j (ξ)) ∂ ∂ξ m D(ξ)<label>(48)</label></formula><p>in the same way as equation <ref type="bibr">(31)</ref>. Note that the differential of the difference of each alternative,</p><formula xml:id="formula_56">∂ ∂ξm D(ξ) = ∂ ∂ξm g(A j ; ξ) − g(B j ; ξ) = ∂ ∂ξm g(A j ; ξ) − ∂ ∂ξm g(B j ; ξ)</formula><p>, has to be calculated. This part is different among models. Therefore, we derive the Fisher information matrix's general formula without derive a specific equation for now. The differential of the log likelihood function can be represented by</p><formula xml:id="formula_57">∂ ∂ξ m log p(y|ξ) = J j=1 y j (1 − η j (ξ)) ∂ ∂ξ m D(ξ) − (1 − y j )η j (ξ) ∂ ∂ξ m D(ξ) = J j=1 ∂ ∂ξ m D(ξ) (y j − η j (ξ))<label>(49)</label></formula><p>in the same way as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">The Fisher information matrix</head><p>Using equation <ref type="formula">25</ref>, the Fisher information matrix can be derived in the same way as before. The (m, n), m, n = 1, ..., P , element of the Fisher information matrix, F (ξ) m,n can be derived as</p><formula xml:id="formula_58">F (ξ) m,n = E ∂ ∂ξ m log p(y|ξ) ∂ ∂ξ n log p(y|ξ) (∵ def) = J j=1 E (y j − η j (ξ)) 2 ∂ ∂ξ m D(ξ) ∂ ∂ξ n D(ξ) (∵ (25)) = J j=1 η j (ξ)(1 − η j (ξ)) ∂ ∂ξ m D(ξ) ∂ ∂ξ n D(ξ) .<label>(50)</label></formula><p>This is the Fisher information matrix of general decision making models represented by equation <ref type="formula" target="#formula_49">45</ref>.</p><p>Finally, we have to derive ∂ ∂ξm D(ξ) . This is a different part among models. Although choice probability is dependent on models, it is calculated with the equation <ref type="formula" target="#formula_49">45</ref>directly. Let us check some examples.</p><p>In logistic regression model of above example, parameters are <ref type="formula" target="#formula_39">36</ref>is the same as the equation <ref type="formula" target="#formula_58">50</ref>. In DD model, the parameters are ξ = (k, β) and D(ξ) =</p><formula xml:id="formula_59">ξ = (α, β). Because the differential is ∂ ∂α D(ξ) = 1, ∂ ∂β D(ξ) = x i , equation</formula><formula xml:id="formula_60">β{u(A d )−u(A s )}. Because the differential is ∂ ∂k D(ξ) = β ∂ ∂k u(A d ) , ∂ ∂β D(ξ) = u(A d ) − u(A s )</formula><p>, equations (42) and (43) are the same as the equation (50). In CPT model, Fujita and Okada (preprint) derived the Fisher information matrix of CPT model added another parameter. Although it is not easy compared to above models, they derived ∂ ∂ξ D(ξ) in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The example code</head><p>This section provides R code example for Delay Discounting (DD) model. This will help you re-learn how to calculate the Fisher information and use the Fisher information for designing experimental stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The functions for calculating the Fisher information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The basic function</head><p>Let us define the functions for calculating the alternative's value and choice probability for j-th trial. Let Para be (true) parameter values which is (k, β) vector. Let Stimulus j be stimuli set of j-th trial which is 2 (alternative A, B) × 2 (stimulus, reward and delayed time) matrix. Stimulus j[1, 2] is always 0 because alternative A is the immediate reward alternative. CalcValue j is the function for calculating the alternative's value, and CalcAllValue j is the function for calculating all alternative's values. CalcProb j is the function for calculating choice probability. These functions are just representing model assumptions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">The function for calcula ting the Fisher information</head><p>Let us define the function for calculating the Fisher information. PartValue is the function for calculating the derivatives of the difference of each alternative value. The first (second) element is derivatives with respect to α(β). As we derive them in section 3.4, we can use ∂ ∂k</p><formula xml:id="formula_61">D(ξ) = β ∂ ∂k u(A d ) , ∂ ∂β D(ξ) = u(A d ) − u(A s ). Listing 2: Second derivatives 1 PartValue &lt;-list() 2 PartValue[[1]] &lt;-function(Para, Stimulus_j) { 3 res &lt;--Stimulus_j[2, 1] * ( Stimulus_j[2, 2] / (1 + Para[1] * Stimulus_j[2, 2])^2 ) 4 res &lt;-res * Para[2] 5 return(res) 6 } 7 PartValue[[2]] &lt;-function(Para, Stimulus_j) { 8 Value &lt;-CalcAllValue_j(Para = Para,Stimulus_j = Stimulus_j) 9 res &lt;-Value[2] -Value[1] 10 return(res) 11 }</formula><p>Let us define the CalcFisher function for calculating the Fisher information matrix of j-th trial. In addition, the TestIF function calculates the Fisher information matrix of all trials. The variable Stimulus is all trial's stimuli which is the number of trials × 2(alternative) × 2(reward, delayed time) array, namely, it is the collection of Stimulus j. Note that the Fisher information matrix of all trials is a simple summation of the Fisher information matrix of a single trial. When other model is used, you can use this code example by modifying the CalcValue j function and the PartValue function corresponding to a new model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The simulation for evaluating the Fisher information of each stimuli pattern</head><p>Using above functions, researchers can calculate the Fisher information matrix with specific (true) parameter values and stimuli (i.e., Stimulus j, Stimulus). As we explained before, the calculation of the Fisher information matrix in code is not needed a dependent variable (choice data). Therefore, researchers can quickly evaluate the goodness of stimuli and design before conducting experiments. Note that the calculation is dependent on assumed parameter distribution and values. Researchers can determine this distribution with a vague distribution like uniform distribution covering almost parameter values, their own experience, the theoretical background of the model, or posterior distribution obtained from previous studies. In addition, it is important to check that selected stimuli are good enough for specific parameter values, as we elaborate on later.</p><p>In the below simulation, the parameter is generated by distribution we assumed multiple times (50 times in this simulation), and the expectation of the Fisher information over these parameters is calculated. In the below code, for the later explanation, we assumed delta distribution for k. However, researchers usually better to assume a more dispersed distribution (e.g., normal, uniform, beta distribution, etc.). As for candidate stimuli which will be evaluated in the Fisher information, the number of reward candidate values (rewardCand) is 20, and the number of delayed time values (timeCand) is 10. Therefore, we will evaluate the goodness of stimuli for parameter estimation at 4000 (20 2 × 10) stimuli points.</p><p>Listing 4: Stimuli analysis by the Fisher information</p><formula xml:id="formula_62">1 #---settings 2 Iter &lt;-50 # the number of calculation repetition 3 4 #---parameter values 5 k &lt;-rep(0.2, Iter) # delta distiribution 6 beta &lt;-runif(n = Iter, min = 0.5, max = 3) 7 Para &lt;-cbind(k, beta) 8</formula><p>9 #--stimulus candidate 10 rewardCand &lt;seq(1, 20, by=1); NumRew &lt;length(rewardCand) # reward 11 timeCand &lt;seq(1, 10, by=1); NumTime &lt;length(timeCand) # time delay 12 13 #--box 14 ResFisherMat_iter &lt;array(NA, dim = c(Iter, NumRew, NumRew, NumTime, 2, 2)) 15 ResFisherMat &lt;array(0, dim = c(NumRew, NumRew, NumTime, 2, 2)) # the  </p><formula xml:id="formula_63">31 } # reward of A loop 32 ResFisherMat &lt;-ResFisherMat / Iter # mean</formula><p>The (expectation of) Fisher information matrix of each stimuli is saved in the ResFisherMat variable. Let us focus on the Fisher information for k. <ref type="figure" target="#fig_1">Figure  1</ref> represents the Fisher information for k of each stimuli pattern. In this figure, we fixed the reward value of A as 10 because the 3-d plot is difficult to figure out.</p><p>These results are accordant with our intuition. When the reward value of B is smaller than the reward value of A, this trial will not contribute to estimation because the reward of B is discounted, which means most participants will choose alternative A regardless of their trait parameters. Further, when the reward value is too large (plus small time delay) or the time delay is too large, the trial will not contribute to estimation because most participants will choose one alternative regardless of their trait parameters. In contrast, some stimuli patterns provide over 100 Fisher information. In short, there are "sweet" spots that will provide higher estimation precision (higher Fisher information) with reflecting individual differences well. The merit of using the Fisher information is that researchers can detect these sweet spots with well-organized and objective calculations.</p><p>This result is based on 2-d plot. Therefore, researchers may be able to detect good stimuli patterns without the above calculation if the reward value of A is fixed. However, how about if the reward value of A is not determined? In addition, there are more elaborated tasks and models that may have more dimensions of stimulus, which will make designing stimuli be more difficult. Further, whether a specific stimuli pattern is good for parameter estimation dependents on the participant's trait parameter values. Using the Fisher information, researchers can evaluate the goodness of stimuli with any dimensions and parameter values in the same framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Application to fixed design: selecting all trial's stimuli</head><p>The previous simulation is for evaluating stimuli of a single trial. In psychological experiments, researchers usually estimate parameters with multiple trials. Therefore, we have to select multiple trial stimuli. Fortunately, each trial of most tasks does not influence each other, at least theoretically. Therefore, we can use a greedy way, which selects optimal stimuli in each trial. In this example, to generate parameter values from assumed distribution and select optimal design based on the Fisher information at above parameter values is used to determine fixed design. We selected optimal stimuli for the 20 trials in the below simulation. We have calculated the Fisher information matrix at specific parameter values, which is saved on ResFisherMat iter, therefore we reuse it.</p><p>Listing 5: Selecting optimal design 1 Trial &lt;-20 # the number of trials 2 Stimulus &lt;array(NA, dim = c(Trial, 2, 2)) # Trial * alternative 2 * stimuli 2 (reward, time delay) for specific participants. For example, in the above example, stimuli for parameter k around 0.2 is not good for parameter k above 0.6. Therefore, it is important to check the above figure. In other words, researchers better to check whether stimuli will provide enough estimation precision for each parameter value. Further, in the CAT framework, researchers can select stimuli based on the Fisher information after obtaining response data (i.e., temporal parameter estimates). Therefore, it will be more efficient than a fixed design. In summary, the Fisher information matrix is important and fundamental quantity from some statistical aspects. This paper focused on the fact that the Fisher information can predict variance (covariance matrix) of ML estimator and stimulus selection method. Maximizing the (determinant of) Fisher information matrix means maximizing estimation precision. This paper provided some examples calculating of the Fisher information matrix from the simple regression models to more elaborated decision making models. In addition, this paper provides example code to explain how to use the Fisher information as a stimulus selection criteria.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Listing 3 :</head><label>3</label><figDesc>The Fisher information 1 CalcFisher &lt;function(Para, Stimulus_j) { 2 PNum &lt;length(Para) # the number of parameters 3 Mat &lt;matrix(NA, ncol = PNum, nrow = PNum) # Fihser information matrix 4 probj &lt;-CalcProb_j(Para = Para,Stimulus_j = Stimulus_j) # choice probability 5 for(irow in 1:PNum) { 6 for(jcol in 1:PNum) { 7 Mat[irow, jcol] &lt;probj * (1probj) dim(Stimulus)[1] # the number of trials 17 PNum &lt;length(Para) 18 Mat &lt;matrix(data = 0, nrow = PNum, ncol = PNum) 19 for(t in 1:J) { 20 Mat &lt;-Mat + CalcFisher(Para = Para,Stimulus_j = Stimulus[t, , ])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The Fisher information of k for each stimulus. The x-axis represents reward of B, and the y-axis represents delayed time of B. Higher fisher information means better stimuli pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>34Figure 2 :</head><label>2</label><figDesc>iter_sa &lt;sample(1:Iter, size = Trial, replace = FALSE) 5 for(trial in 1:Trial) { 6 #--calculate determinant ---7 ResDetTemp &lt;array(NA, dim = c(NumRew, NumRew, NumTime)) , jRewB, ktimeB] &lt;det(ResFisherMat_iter[ iter_sa[trial], iRewA, jRewB, ktimeB, which( ResDetTemp == max(ResDetTemp), arr.ind = TRUE ) # index of stimuli maximizing det(F) 18 # set stimuli 19 Stimulus[trial, 1, 1] = rewardCand[ind[1, 1]]; Stimulus[trial, 1, 2] &lt;-0 # stimuli of A The aysmptotic PSD of each parameter value. The x-axis represents parameter k, and the y-axis represents parameter β.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What we did in the above code is just to calculate the determinant of Fisher information matrix and find out stimuli maximizing this value. We iterated 20 times (i.e., the number of trials). Although we have selected optimal stimuli already, we better to check some characteristics of the selected stimuli. One checkpoint is that researchers better to run MCMC based simulation to check estimation precision more precisely because the Fisher information is asymptotic prediction. Note that MCMC based simulation will take much time, therefore it should be done after fixing (optimal) design. Second, researchers better to check predicted estimation precision for each parameter value.</p><p>In the simulation below, we confirm how much estimation precision this selected stimuli provide for participants with specific parameter values. Namely, the code below changes (true) parameter values while fixing the stimuli.</p><p>Listing 6: Asymptotic SD of each parameter Let us focus on the parameter k. <ref type="figure">Figure 2</ref> represents the predicted posterior standard deviation (PSD) for each parameter value. From this figure, we can infer that parameter k around 0.2 will be estimated with high estimation precision, however, parameter k above 0.6 will not be. This is because we selected stimuli assuming the true parameter k value is 0.2, which means selected stimuli are for this values. It is unrealistic in real experiments to know true parameter values as a delta distribution. This setting was for an explanation. Therefore, researchers can assume some distribution for parameter values as a vague distribution like a uniform distribution, distribution based on theoretical background or researcher's experience, or posterior distribution obtained from previous experiments. In this case, if you change from "k &lt;-rep(0.2, Iter)" to "k &lt;-(some distribution)", the results will be changed.</p><p>In general, if researchers set vague distribution, they can estimate most parameters well. However, vague distribution means it includs ineffective stimuli</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Stimulus</title>
		<imprint/>
	</monogr>
	<note>trial, 2, 1] = rewardCand. ind[1, 2</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>&lt;-Timecand</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>ind[1, 3]] # stimuli of B</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Teater</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rapid , precise , and reliable measurement of delay discounting using a bayesian learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-68587-x</idno>
		<ptr target="https://doi.org/10.1038/s41598-020-68587-xdoi:10.1038/s41598-020-68587-x" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminating among probability weighting functions using adaptive design optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Cavagnaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11166-013-9179-3</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Uncertainty</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="255" to="289" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian experimental design: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaloner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Verdinelli</surname></persName>
		</author>
		<idno type="DOI">10.1214/ss/1177009939</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="273" to="304" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-014-9401-5</idno>
	</analytic>
	<monogr>
		<title level="m">Psychometrics behind computerized adaptive testing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast adaptive estimation of multidimensional psychometric functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dimattina</surname></persName>
		</author>
		<idno type="DOI">10.1167/15.9.5</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adaptive optimal stimulus selection in cognitive models using a model averaging approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>preprint. preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Development of the cat-anx: A computerized adaptive test for depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Pilkonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kupfer</surname></persName>
		</author>
		<idno type="DOI">10.1001/archgenpsychiatry.2012.14</idno>
	</analytic>
	<monogr>
		<title level="j">Arch Gen Psychiatry</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1104" to="1112" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximizing the expected information gain of cognitive modeling via design optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42113-019-00035-0</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Brain and Behavior</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="202" to="209" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An invariant form for the prior probability in estimation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences</title>
		<imprint>
			<biblScope unit="volume">1007</biblScope>
			<biblScope unit="page" from="453" to="461" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
		<title level="m">Theory of probability. (third ed)</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian adaptive estimation of psychometric slope and threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Kontsevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Tyler</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(98)00285-5</idno>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2729" to="2737" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On a measure of the information provided by an experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Lindley</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177728069</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="986" to="1005" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tutorial on fisher information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Grasman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2017.05.006</idno>
		<ptr target="http://dx.doi.org/10.1016/j.jmp.2017.05.006doi:10.1016/j.jmp.2017.05.006" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="40" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delay discounting of real and hypothetical rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Begotka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Raiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Kastern</surname></persName>
		</author>
		<idno type="DOI">10.1037/1064-1297.11.2.139</idno>
	</analytic>
	<monogr>
		<title level="j">Experimental and Clinical Psychopharmacology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="139" to="145" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining computer adaptive testing technology with cognitively diagnostic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcglohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.3758/BRM.40.3.808</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="808" to="821" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computerized adaptive testing: Overview and an example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Meijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Nering</surname></persName>
		</author>
		<idno type="DOI">10.1177/01466219922031310</idno>
		<ptr target="https://doi.org/10.1177/01466219922031310" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multidimensional adaptive testing with optimal design criteria for item selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Van Der Linden</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-008-9097-5</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="273" to="296" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tutorial on maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-2496(02)00028-7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model selection by normalized maximum likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2005.06.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal experimental design for model discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0016104</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="499" to="518" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Odum</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.beproc.2011.02.007</idno>
		<title level="m">Delay discounting: Trait variable? Behavioural Processes</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multidimensional adaptive testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Segall</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02294343</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="331" to="354" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computerized adaptive testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Segall</surname></persName>
		</author>
		<idno type="DOI">10.1016/B0-12-369398-5/00444-8</idno>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Social Measurement</title>
		<imprint>
			<biblScope unit="page" from="429" to="438" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic experiments for estimating preferences: An adaptive method of eliciting time and risk parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delquié</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.1120.1570</idno>
		<idno>doi: 10.1287/mnsc.1120.1570</idno>
		<ptr target="http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1120.1570" />
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="613" to="640" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Advances in prospect theory: Cumulative representation of uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Uncertainty</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="297" to="323" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian item selection criteria for adaptive testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Van Der Linden</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02294775</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="201" to="216" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Handbook of item response theory, volume three: application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Van Der Linden</surname></persName>
		</author>
		<editor>W. J. van der Linden</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CRC Press</publisher>
			<biblScope unit="page" from="197" to="228" />
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
	<note>Adaptive testing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quest: A bayesian adaptive psychometric method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Pelli</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03202828</idno>
		<ptr target="https://doi.org/10.3758/BF03202828" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Application of computerized adaptive testing to educational problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Kingsbury</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1984.tb01040.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="361" to="375" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
