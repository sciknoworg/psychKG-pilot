<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMILAR FAILURES OF CONSIDERATION ARISE IN HUMAN AND MACHINE PLANNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-08-01">August 1, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Preprint</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Langenkamp</surname></persName>
							<email>maxlangenkamp@me.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kleiman-Weiner</surname></persName>
							<email>maxhkw@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Oikarinen</surname></persName>
							<email>toikarinen@ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiery</forename><surname>Cushman</surname></persName>
							<email>cushman@fas.harvard.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIMILAR FAILURES OF CONSIDERATION ARISE IN HUMAN AND MACHINE PLANNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-01">August 1, 2023</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Decision-making</term>
					<term>Cognitive puzzles</term>
					<term>Consideration</term>
					<term>Machine learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Humans are remarkably efficient at decision-making, even in &quot;open-ended&quot; problems where the set of possible actions is too large for exhaustive evaluation. Our success relies, in part, on efficient processes of calling to mind and considering the right candidate actions for evaluation. When this process fails, however, the result is a kind of cognitive puzzle in which the value of a solution or action would be obvious as soon as it is considered, but never gets considered in the first place. Recently, machine learning (ML) architectures have attained or even exceeded human performance on certain kinds of open-ended tasks such as the games of chess and go. We ask whether the broad architectural principles that underlie ML success in these domains tend to generate similar consideration failures to those observed in humans. We demonstrate a case in which they do, illuminating how humans make open-ended decisions, how this relates to ML approaches to similar problems, and how both architectures lead to characteristic patterns of success and failure.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Efficient consideration of promising options during decision-making is a cornerstone of human intelligence <ref type="bibr" target="#b0">[Phillips et al., 2019</ref><ref type="bibr" target="#b1">, Simon, 1955</ref>. Many everyday problems present too many possible solutions for exhaustive consideration: Imagine all the books we could choose to read next; all the people we could spend a Sunday afternoon with; all the different phrases that could express an idea. Fortunately, when facing these kinds of open-ended problems, we efficiently Over the last 10 years machine learning algorithms have achieved performance on par with human experts in certain open-ended problems such as the games of chess and go <ref type="bibr" target="#b8">[Silver et al., 2018]</ref>. These successes depend in part on a new architectural motif that efficiently selects candidate actions for consideration and further evaluation by using heuristic value estimates based on past experience. This motif also parallels some current models of how humans generate efficient consideration sets. We therefore ask whether it can be used to understand not only the shared successes of human and ML planning, but also their characteristic failures of consideration. The key idea is simple: If the necessary action to solve the current problem has been assigned high heuristic value based on prior experience, it will come quickly to mind. If not, the correct solution will evade consideration, resulting in a cognitive puzzle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">ML planning in open ended problems</head><p>Several recent ML algorithms with breakthrough performance in open-ended problems, such as AlphaGo and its descendents, rely on a common architecture <ref type="figure" target="#fig_0">(Fig. 1</ref>). Candidate actions are evaluated by simulating their likely consequences, a model-based method of evaluation sometimes called roll-out or tree search. In open-ended problems there are, however, too many action sequences for exhaustive consideration. A key innovation addressees this problem by guiding tree search towards the most promising candidates based on a heuristic estimate related to their value. This estimate is furnished by a myopic neural net trained on previously played or observed games, a model-free method. Since players encounter novel circumstances-states of the game that have never before been observed-the network must generalize from past experience. When combined together, the neural network efficiently guides model-based roll-outs towards promising candidates. Some elements of this architecture resonate closely with both classic and current themes in the study of human decision-making under resource constraint <ref type="bibr" target="#b1">[Simon, 1955</ref><ref type="bibr" target="#b9">, Lieder et al., 2018</ref>. People often evaluate candidate actions by model-based methods resembling tree search <ref type="bibr" target="#b10">[Dolan and Dayan, 2013]</ref>. And, in open-ended problems, people often prioritize valuable actions as candidates for evaluation <ref type="bibr" target="#b3">[Morris et al., 2021</ref><ref type="bibr" target="#b11">, Peters et al., 2017</ref><ref type="bibr" target="#b12">, Johnson and Raab, 2003</ref><ref type="bibr" target="#b14">, Klein, 1993</ref><ref type="bibr" target="#b4">, Zhang et al., 2021</ref>. Finally, people sometimes generate a set of candidate actions based on model-free estimates of value, while choosing among these candidates by model-based methods <ref type="bibr" target="#b3">[Morris et al., 2021</ref><ref type="bibr" target="#b15">, Hauser and Wernerfelt, 1990</ref>. This suggests a potential high-level correspondence between human and machine approaches to open-ended decision problems, although the precise implementations likely differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Failures of consideration in cognitive puzzles</head><p>One obvious approach to establishing correspondence between humans and ML methods is to see whether they show comparable areas of success. Our experiments provide this kind of evidence. Yet, our main focus is not areas of shared success, but rather on a common pattern of failures. After all, starkly different algorithms can be used to arrive at the same successful solutions. Often, an algorithm's most distinctive fingerprint is its pattern of failures <ref type="bibr" target="#b17">[Saxe, 2005]</ref>. In order to elicit failures, we designed a task for which we expected certain solutions to systematically evade consideration, whether by humans or machine planners.</p><p>We designed a sequential decision-making task of the kind that the relevant ML architectures are optimized to solve. Specifically, we designed a gridworld game in which an agent can move valuable objects onto target locations for points. We described these objects to humans as cargo, and the setting as a train yard. The grid also contained trains (autonomous moving objects that destroy any objects they hit, resulting in a loss of points and bringing the train to a stop) and switches (which change the direction of motion of the trains).</p><p>We trained humans and an ML planning architecture on a set of grids comprising several mundane types, such as moving cargo onto its target location ("push-control" cases), or flipping a switch to redirect a train away from valuable cargo ("switch control" cases), among others <ref type="figure">(Fig 2)</ref>. Crucially, however, our training regime omitted two particular types of configuration. In the first ("switch" cases), the optimal action is to redirect the train away from collision with a high-value object such that it unavoidably collides with a low-value object. In the second ("push" cases), the optimal action is to push a low value object into the path of the train in order to stop it, thus preventing it from hitting a high-value object.</p><p>To preview our results, both humans and the ML planner reliably identified the optimal solution to switch cases, but often failed on push cases. Our findings suggest that this occurs because, based on training, switching a train away from cargo is heuristically estimated to have high value, while pushing cargo into the path of a train is heuristically estimated to have low value. Because these heuristic value estimates govern which actions get evaluated, the optimal action is quickly discovered in switch cases but not in push cases. Across a variety of additional manipulations, we find a common pattern of susceptibility to this cognitive puzzle.</p><p>Our task resembles the well-studied "trolley problem" <ref type="bibr">[Foot, 1967</ref><ref type="bibr" target="#b18">, Greene, 2014</ref>, but it differs in two important ways. First, it does not pose a moral dilemma: The objects in question are merely cargo, not people. Second, our data suggest that participants do not consider and then reject the possibility of pushing cargo in front of a train; rather, they fail to consider the action in the first place. Thus, our "trolley puzzle" is quite different from the classic trolley problem, in which harm to people is considered and then rejected on moral grounds. In the discussion, however, we consider ways in which these two distinct phenomena may nevertheless be related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Behavioral Experiments</head><p>We asked participants to solve a series of railway operations problems structured as finite Markov decision processes <ref type="figure">(Fig. 2)</ref>. Their objective was to maximize points by pushing cargo onto target locations, earning 2 points for high value cargo and 1 point for low value cargo, while avoiding collisions with the train. The train moved across the screen in a straight line, one grid cell per time step, unless the participant flipped a switch to turn it 90 • counterclockwise. The participant lost points in the event of a train collision: -1 for low value cargo, -2 for high value cargo, and -4 for a collision with themselves. Collisions always stopped the motion of the train. Participants performed a sequence of 5 actions by moving in any cardinal direction or standing still.</p><p>Participants completed a training phase comprising four problem-types, but omitting the switch and push cases. Specifically, training included push-control cases that favored pushing object(s) into their target locations (47%), switch-control cases that favored redirecting the train away from an object collision (25%), "push away" cases that favored pushing an object out of the path of the train (20%), and "do nothing" cases where nothing could be done to prevent an object collision (8%). Grids of each type were procedurally generated by randomly setting the starting positions of key items within a set of constraints (see SI).</p><p>Then, during the test phase, we presented participants with push and switch cases for the first time, as well as novel instances of the switch-control and push-control types used in training. In order to standardize the comparison of participant scores across distinct categories of test grids we constructed all test grids so that the minimum and <ref type="figure">Figure 2</ref>: Example starting state and optimal solution for each test case type in our task. Points are earned when cargo are pushed onto their targets and points are lost when the train collides with cargo. Push and switch cases favor the sacrifice of the lower value cargo in order to prevent the train from colliding with the higher value cargo. Push-control and switch-control cases favor pushing cargo onto their target locations and hitting the switch to prevent collisions, respectively. Control cases were among the categories featured in training, and were also included at test. Push and switch cases were omitted from training and presented exclusively at test. maximum attainable scores differed by exactly one point, and then standardized these to range (0, 1) by subtracting from participant's score the grid-specific minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experiment 1: Eliciting failures of consideration</head><p>First we characterized patterns of failure during the test phase. Naturally, participants might solve the types of cases that were presented in training (push-control and switch-control) more often than types that were not (push and switch). Thus, we ask whether switch and push cases reliably elicited failures, presumably because participants fail to even consider the highest-scoring solution in the first place. We assessed this in two ways. First, we simply measured overall levels of success vs. failure on these items. Second, we asked whether participants showed evidence of a discontinuity in performance following their first success, consistent with the idea that once they first appreciated the utility of the action in question (a sacrificing switch, or a sacrificing push), this candidate solution became more cognitively available in future problems of the same type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Results</head><p>At test, participants scored close to ceiling on control cases of the types included in training (push-control: .94, 95% CI .93-.95; switch control: .96, 95% CI .95-.97). They scored similarly on the novel type of switch cases (.95, 95% CI .94-.95), but lower on the novel type of push cases (.63, 95% CI .0.60-.0.65). We then fit a series of preregistered linear mixed models, separately for switch cases and for push cases, that predicted standardized scores on these test items with one term capturing linear improvement over the test block and a second term capturing a discontinuity in performance following the first correct answer. The estimated marginal means from this model indicate that, after participants successfully solved their first push case, they became about 20% more likely to successfully solve subsequent push cases (p &lt; .001). No such effect was evident in switch cases (p = 0.89; additional details in SI, see <ref type="figure" target="#fig_1">Figure 3A</ref>). Thus, push cases present a unique challenge for participants. It seems that, by default, they often fail to consider solutions in which one uses a less costly collision to prevent a more costly one. Once they "see" the possibility of this solution, however, it becomes relatively more available in subsequent problems of the same type. Our subsequent experiments further explore the possibility that push cases elicit uniquely potent failures of consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiment 2: Manipulating deliberation time</head><p>Experiment 2 extended the basic design of Experiment 1 to conditions of time pressure versus time delay. We reasoned that suboptimal performance due to a failure of consideration would be sensitive to this manipulation, since people tend to consider more options when given more time <ref type="bibr" target="#b3">[Morris et al., 2021</ref>] and thus will progressively consider even actions assigned lower heuristic value under time delay. In contrast, alternative explanations for suboptimal performance (e.g., an unwillingness to "sacrifice" one object for another, or a misunderstanding of the rules or scoring of the task) would not be sensitive to this manipulation. Push cases elicit worse performance overall but, uniquely, relative improvement following the first success. B. Time pressure impaired performance especially for push cases. C. Training experience improved scores on switch cases but not push cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Results</head><p>Under time pressure, when we expect constraints on consideration to be most applicable, participants scored lower on push cases (0.37; see <ref type="figure" target="#fig_1">Figure 3B</ref>) than switch cases (0.84), push-control cases (0.90), or switch-control cases (0.88). Additionally, imposing a time delay prompted a greater change in average score for push cases (+ .23) than the remaining three types of cases (+ .12, + .04, and + .08, respectively). This suggests that that push cases elicit uniquely large consideration failures in our task. We fit a series of preregistered linear mixed models that predicted standardized scores using the lme4 package in R to test several contrasts of interest (see SI for details). Our analysis revealed a significant effect of time constraint across all grid types, χ 2 = 20.451, P &lt; 0.001, significantly higher scores for switch than for push cases χ 2 = 20.026, P &lt; 0.001, and a significant interaction such that the effect of time delay versus time pressure was greater for push cases than switch cases χ 2 = 6.6546, P &lt; 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experiment 3: Manipulating training</head><p>Experiment 3 explores the effect of training on test performance. We reasoned that participants likely fail to consider the optimal action in push cases because training (which contains neither push nor switch cases) yields a low heuristic value estimate for pushing objects into the path of trains-ordinarily, a costly action. They likely succeed in considering the optimal action in switch cases because training yields a high heuristic value estimate for redirecting trains away from objects-ordinarily, a beneficial one. If so, then training should facilitate performance on switch cases, but not push cases, due to its opposite effects on which actions get considered.</p><p>Thus, Experiment 3 contrasted a "training" condition, where participants complete the training phase as normal, and a "no training" condition where participants complete the test phase directly after the task instructions. All participants completed the task under time pressure, where failures of consideration are most evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Results</head><p>As expected, the inclusion of training had little effect on participants' scores in push cases (.17 with versus .16 without, see <ref type="figure" target="#fig_1">Figure 3C</ref>). For switch cases, however, participants scored significantly worse without training (.53), than with training (.66). We fit a series of preregistered linear mixed models using the lme4 package in R modeling standardized scores by case type (push versus switch), training, and their interaction. Random effects in the final model included grid id and participant id. Our analysis showed significant effects of training χ 2 = 5.31, P &lt; 0.05, case type χ 2 = 26.50, P &lt; 0.001, and their interaction χ 2 = 4.53, P &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ML Planning Architecture and Experiments</head><p>Next we explored whether similar patterns of performance arise if we attempt to solve our task using an architectural motif behind recent advances in machine decision-making in open-ended problems. Our goal was to adopt methods that are broadly representative of one family of current approaches, not to introduce ad hoc innovations. We begin by describing the architecture in more detail, and then report the results of in silico experiments designed to mirror those we performed on humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architectural details</head><p>Our architecture has two parts ( <ref type="figure" target="#fig_0">Fig. 1)</ref>: A neural net that approximates the state-action value function based on training, and a model-based method for improving these value estimates by Monte Carlo rollouts. The heuristic value estimates are used to prioritize rollouts, directing model-based exploration towards promising candidate actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Approximating the value function with a trained neural network</head><p>We constructed a simple Convolutional Neural Network <ref type="bibr">[CNN;</ref><ref type="bibr">LeCun et al., 2015</ref>] that predicts the Q values [Sutton and <ref type="bibr" target="#b19">Barto, 1998</ref>] for each available action <ref type="bibr">(up, right, down, left, and stay)</ref> given an input game state. The state input representation contains information about the time step (1-5), the locations of all objects in the grid, and the direction of the train.</p><p>To train the CNN, we generated a set of potential inputs (200,000 grid problems from a fixed distribution) and outputs (optimal Q-values derived from value iteration). The training grids comprised the same four types presented to humans in training, but with a slightly different distribution. (humans saw fewer "do nothing" grids and more "push control" grids to provide a less frustrating experience). Then, for each grid, we identified the optimal action sequence and associated set of states visited. These states, and the optimal Q values for each action in these states, comprised the training inputs and outputs. (This parallels human experience during training, in which 80% of human-generated action sequences were optimal for those participants included in analysis.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Action evaluation and control by First Visit Monte Carlo</head><p>Our architecture next evaluated candidate actions by a standard model-based method: First-Visit Monte Carlo (FVMC) [Sutton and <ref type="bibr" target="#b19">Barto, 1998</ref>]. This algorithm refines its action policy over a series of iterations, where each iteration involves exploring a full action sequence and updating Q values based on cumulative returns. Exploration for FVMC was determined by an ϵ-greedy strategy with ϵ = .2. Crucially, we initialized the Q values that guided FVMC using the estimates from our trained CNN. In this manner the CNN guided "consideration" of candidate actions, especially early in the process of model-based evaluation (See SI for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">In silico Experiments</head><p>Next we asked whether this architecture generates the same patterns of consideration failure observed in humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Manipulating "decision time"</head><p>First, we tested our architecture on the same novel grids that we presented to humans. We predicted that, like humans in Experiment 1, it would exhibit especially poor performance on push cases, failing to quickly identify the optimal action, relative to switch, switch-control and push-control cases. In Experiment 2 with humans we manipulated people's decision time. In our architecture, the "decision time" variable is analogous to the number of candidate action sequences that FVMC is allowed to consider: the number of iterations. We therefore explored the effect of this parameter setting. <ref type="figure">Figure 4A</ref> shows the performance of our architecture on the four different types of test cases as a function of the number of iterations. As with humans under time pressure, when the number iterations is small, it performs much worse on push cases than the other types of test case. As with humans under time delay, when the number of iterations increases, this disparity is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Manipulating training</head><p>Next, we asked how our architecture's performance on the four types of test case varies as a function of training. In this context, the relevant training is encoded in the CNN that approximates the value function. We therefore ask how performance on test cases changes if the contribution of this CNN is removed. In the lesioned architecture, the state-action value estimates that guide FVMC are therefore uniformly initialized to zero. This manipulation of the architecture analogizes to Experiment 3, in which participants either received 60 training rounds or none. Of course, we assume that humans bring some relevant experience to the task that allows them to approximate a value function even in the absence of task-specific training. Thus, we might expect differences in performance without training and in the magnitude of the effect of training across humans versus AI. <ref type="figure">Figure 4B</ref> shows the difference in average standardized scores obtained by the full architecture and the lesioned one, as a function of both test case type and the number of FVMC iterations. Positive values indicate a training advantage, and negative scores a training disadvantage. We find that training generates performance advantage on <ref type="figure">Figure 4</ref>: A. Average standardized score of the ML architecture as a function of the number of FVMC iterations, for each test trial type. Analogous to human performance on push cases under time pressure, ML performance on push cases is uniquely degraded at small numbers of iterations. B. Performance advantage afforded by a trained CNN as a function of the number of FVMC iterations, for each test trial type. Analogous to human results, for switch, push-control, and switch-control cases the trained CNN induces a large performance advantage especially at a small number of iterations, while for the push case selectively there is a performance decrement induced by training. The results in both figures are obtained by averaging over 100 runs, as each Monte Carlo rollout occurs with some degree of randomness.</p><p>switch, save-switch and score cases. This aligns with the results of our human experiment, where training trials tended to improve performance on the same types of cases. In contrast, we find that training interferes with performance on push cases. This partially aligns with the results of our human experiment, where training trials provided no benefit in push cases but did not interfere, either.</p><p>Unlike humans, performance of the FVMC model without training does not vary much by problem type. Rather, it generally takes more iterations to achieve an optimal score on those specific cases with only one optimal sequence of actions, rather than several equivalent ones. To facilitate comparison between problem types, <ref type="figure">Fig. 4</ref> excludes four cases of this kind which are performance outliers. However, we replicate the same pattern of model results when these are included (see SI, <ref type="figure" target="#fig_0">Fig S1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Interrogating value function generalization</head><p>We have proposed that both humans and our ML architecture fail to consider the optimal action in push cases because their heuristic value estimates systematically underestimate the value of pushing an object into the path of a train. In the case of our ML architecture we can explore these value estimates directly. To do so, we computed the average signed error in value estimates generated by our trained CNN for the key action in each category of test case: Pushing an object into the path of the train (for push cases), redirecting the train away from one object and towards another (for switch cases), redirecting the train safely (for safe-switch cases), and pushing the object onto the target location (for score cases). We did this for all of the test cases used in our human and in silico experiments <ref type="figure" target="#fig_2">(Fig. 5 )</ref> As expected, the CNN systematically underestimates the value of the optimal action in push cases (a class on which it was not trained), while it exhibits no meaningful bias in switch-control and push-control cases (classes on which it was trained). Notably, however, the CNN also exhibits systematic error in its value estimate of the switch cases. This is not unexpected, since switch cases were not included in the training set. But, of course, we find that neither humans nor our ML architecture have much difficulty identifying the optimal solution for switch cases. This is presumably because of the sign of the bias. In switch cases value is overestimated (reflecting a training set in which switching the train often saves some cargo without causing harm to other cargo). This leads to model-based evaluation of the optimal action early, ensuring its proper consideration. In push cases, value is instead underestimated (reflecting a training set in which pushing cargo into a train harms that cargo without saving any other cargo). This inhibits model-based evaluation of the optimal action, leading to a systematic failure of consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We generated a set of sequential decision-making problems to be solved by humans and an ML planning architecture. We find that the same class of problems that reliably generate failures of consideration in humans also generates failures in our ML planner. This suggests an intriguing correspondence between the broad organizational architecture of human and machine approaches to decision-making in open-ended problems. Three similarities characterized human and machine planners. First, both performed worse on push than switch cases. Second, this performance decrement was especially pronounced when time devoted to model-based evaluation was limited and, therefore, the initial value estimates used to determine consideration presumably played their greatest role. Third, training improved out-of-sample performance on switch cases but failed to improve (in humans) or actually inhibited (in ML) out-of-sample performance on push cases.</p><p>Interrogating our ML planning architecture, it is clear why push cases elicited consideration failures. The architecture prioritizes evaluation of candidate actions that are assigned high value estimates by a neural net. The neural net must generalize from training to novel types of cases in which low-value cargo must be destroyed in order to save high-value cargo. We find that the neural net provides biased value estimates across both switch cases and push cases, but in opposite directions: It overestimates the value of switch actions, but underestimates the value of push actions. As a consequence, it quickly evaluates the switch actions and discovers that, while less valuable than expected, they are better than the alternatives. In contrast, it inhibits evaluation of push actions and therefore fails to discover that they, too, are better than the alternatives. The result is a cognitive puzzle specific to push cases.</p><p>The qualitative similarities between human and ML performance suggest that they may employ broadly similar approaches to choice in this task: Consideration of candidate actions generated by a statistical approximation of the value-function trained on prior experience in related tasks, followed by refinement of those value estimates via model-based rollout. This is consistent with prior work on how people use value-based methods to solve certain open-ended problems <ref type="bibr">[Morris et al., 2021, Hauser and</ref><ref type="bibr" target="#b15">Wernerfelt, 1990]</ref>. It is also broadly consistent with prior work on certain cognitive puzzles that arise from the failure to generate appropriate candidate solutions, rather than the failure to evaluate them correctly <ref type="bibr" target="#b5">[Gilhooly and</ref><ref type="bibr">Murphy, 2005, Batchelder and</ref><ref type="bibr" target="#b6">Alexander, 2012]</ref>.</p><p>Of course, there are several ways in which human performance likely diverges from our ML planning architecture in its details. We designed our CNN to be representative of standard approaches in contemporary machine learning, but we do not assume, nor does our data imply, any strong correspondence between the construction of the CNN and the mechanisms that humans use to approximate a value function in our task. Moreover, humans' heuristic value estimates are likely guided by a wealth of experience and structured knowledge (e.g., about trains, cargo, etc.) that go well beyond the specific instructions and training that we provided in the context of our experiment. To the extent that human and ML performance align, we suggest that it is for two reasons: one about the structure of experience, and another about the structure of planning. First, like our model, humans experience many situations in which it makes sense to avoid collisions to prevent harm, and few situations in which it makes sense to cause collisions to prevent greater harm. Second, statistical approaches to value estimation may be biased when generalizing to new case types and, when these heuristic value estimates are used to guide consideration, this will lead to planning failures.</p><p>Our task resembles the famous "trolley problem" <ref type="bibr">[Foot, 1967</ref><ref type="bibr" target="#b18">, Greene, 2014</ref>. Unlike the real trolley problem, however, in our "trolley puzzle" participants are unlikely to have conceived of cargo operations in moral terms, and our ML architecture was not designed for moral evaluation. Also, the trolley problem has not traditionally been used to demonstrate a failure to consider sacrificial harm, but instead the judgment that it is wrong. Nevertheless, the same kinds of mechanisms that furnish model-free value estimates guiding option consideration in our puzzle could potentially play a parallel role in certain forms of moral evaluation, as has been suggested elsewhere <ref type="bibr" target="#b20">[Cushman, 2013</ref><ref type="bibr" target="#b21">, Crockett, 2013</ref><ref type="bibr">, Phillips and Cushman, 2017</ref>. This deserves further study.</p><p>Currently, humans are unparalleled in their capacity for intelligent, creative thought, and yet we are also prone to certain startling and predictable failures. Recent advances in AI have begun to carve away selected domains where human performance can be matched, or even exceeded. Presumably some AI architectures have close human parallels and other do not. Here, focusing on a case of apparent alignment, we show that human and machine planning is characterized not just by the same dazzling successes, but also by the same failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>All studies were approved by the Harvard University Institutional Review Board and performed with participants' informed consent. Participants were recruited from Amazon MTurk through CloudResearch and paid for their participation. All study materials, code, and analysis scripts are available at github.com/Alice2hang/RL_trolley_puzzles. Experiment 1 Preregistration: aspredicted.org/GCL_1KQ. Of 364 participants, 109 were excluded for failing to complete all assigned rounds or scoring less than 0 points in training. The rules of the game were provided in an extensive, interactive training procedure to all participants. All subjects completed 60 training rounds, followed by 50 test rounds. The 50 test rounds consisted of 8 of each grid-type of interest (push, switch, push-control, and switch-control) and 18 filler grids drawn from the training distribution. All test grids were performed under a 7 second time delay.</p><p>Experiment 2 Preregistration: aspredicted.org/QTQ_ESN. Of 200 participants, 64 were excluded for failing to complete all assigned rounds, timing out in more than 6 rounds in the "time pressure" condition, or scoring less than 0 points in training. Participants were randomly assigned to the "time pressure" condition (limit of 7 seconds to complete each test round) or the "time delay" condition (wait 7 seconds before taking first action). Under time pressure, not completing the round in time results in a four point penalty.</p><p>Experiment 3 Preregistration: aspredicted.org/KL2_CTC. Of 600 participants, 202 were excluded for failing to complete all assigned rounds, timing out more than 6 times, timing out more than twice on either test case type of interest, or scoring less than -35 points in training. Participants were randomly assigned to either the "training" or "no-training" condition. During the test phase, all subjects were placed under time pressure with a 7 second time limit. Each participant completed 24 test grids, consisting of 3 of each grid-type of interest and 12 filler grids drawn from the training distribution. This reduction in number was made in order to reduce the effect of learning from push and switch experiences during test.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>In the family of ML planning architectures that inspired our approach, a model-free estimator guiding model-based evaluation is able to efficiently solve open-ended problems by preferentially evaluating actions with high estimated value. The model-free component is implemented by a Convolutional Neural Net (CNN) trained on optimal solutions. The model-based component is implemented by Monte-Carlo rollout; in our case, First Visit Monte-Carlo. This motif of model-free value estimates guiding efficient model-based evaluation is echoed in contemporary work on human planning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Behavioral results for Expt. 1-3. Error bars are SEMs. A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>An analysis of the CNN's value estimates for the key action of interest (pushing cargo or hitting the switch) for each test type. For the test types included in training (push control and switch control) value estimates are unbiased. For the test types not included in training value estimates are biased, but in opposite directions: Upwards for switch cases, and downwards for push cases.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the Moral Psychology Research Lab for helpful feedback throughout. This work was supported by grants N000141912205 and N000142212205 from the Office of Naval Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we know what not to think</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiery</forename><surname>Cushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1026" to="1040" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jonathan Phillips and Fiery Cushman. Morality constrains the default representation of what is possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Simon ; John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4649" to="4654" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
	<note>Journal of Business Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The prototypicality of brands: Relationships with brand awareness, preference and usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Nedungadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Consumer Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="489" to="503" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating options and choosing between them depend on distinct forms of value representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiery</forename><surname>Cushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1731" to="1746" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrieval-constrained valuation: Toward prediction of open-ended decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyana</forename><surname>Hristova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">2022685118</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differentiating insight from non-insight problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gilhooly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="302" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Insight problem solving: A critical examination of the possibility of formal theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">E</forename><surname>Batchelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Problem Solving</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When a stereotype dumbfounds: Probing the nature of the surgeon= male belief</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>Kirsten N Morehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ece</forename><surname>Kurdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahzarin R</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Research in Ecological and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100044</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overrepresentation of extreme events in decision making reflects rational use of cognitive resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falk</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Goals and habits in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="325" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The ventromedial frontal lobe contributes to forming effective solutions to real-world problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesley</forename><forename type="middle">K</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Signy</forename><surname>Fellows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="991" to="1001" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Take the first: Option-generation and resulting choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="229" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cognitive and neural basis of option generation and subsequent choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annemarie</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Kalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">N</forename><surname>Schweizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Tobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mojzisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="814" to="829" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A recognition-primed decision (RPD) model of rapid decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Ablex Publishing Corporation</publisher>
			<biblScope unit="page" from="138" to="147" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An evaluation cost model of consideration sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birger</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wernerfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="408" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why we should talk about option generation in decision-making research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annemarie</forename><surname>Kalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mojzisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">555</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Philippa Foot. The problem of abortion and the doctrine of the double effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="174" to="179" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
	<note>Against simulation: the argument from error</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moral tribes: Emotion, reason, and the gap between us and them. Penguin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Action, outcome, and value: A dual-system framework for morality. Personality and social psychology review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiery</forename><surname>Cushman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="273" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Models of morality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Molly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="363" to="366" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
