<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rational and irrational consequences of state-dependence valuation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Bavard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Neurosciences Cognitives Computationnelles</orgName>
								<orgName type="institution">Institut National de la Santé et Recherche Médicale</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;Etudes Cognitives</orgName>
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut d&apos;Etudes de la Cognition</orgName>
								<orgName type="institution">Université de Paris Sciences et Lettres</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maël</forename><surname>Lebreton</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Business and Economics</orgName>
								<orgName type="laboratory">CREED lab</orgName>
								<orgName type="institution" key="instit1">Amsterdam School of Economics</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Amsterdam Brain and Cognition</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Khamassi</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institut des Sciences de l&apos;Information et de leurs Interactions</orgName>
								<orgName type="institution">Sorbonne Universités</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Institut des Systèmes Intelligents et Robotiques</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Coricelli</surname></persName>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">Departement of Economics</orgName>
								<orgName type="department" key="dep2">Los Angels</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Centro Mente e Cervello</orgName>
								<orgName type="institution">Università di Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Palminteri</surname></persName>
							<email>stefano.palminteri@ens.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Neurosciences Cognitives Computationnelles</orgName>
								<orgName type="institution">Institut National de la Santé et Recherche Médicale</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;Etudes Cognitives</orgName>
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut d&apos;Etudes de la Cognition</orgName>
								<orgName type="institution">Université de Paris Sciences et Lettres</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rational and irrational consequences of state-dependence valuation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>context-dependence</term>
					<term>reinforcement learning</term>
					<term>value normalization</term>
					<term>computational phenotyping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In economics and in perceptual decision-making contextual effects are well documented, where decision weights are adjusted as a function of the distribution of stimuli. Yet, in reinforcement learning literature whether and how contextual information pertaining to decision states is integrated in learning algorithms has received comparably little attention. Here, in an attempt to fill this gap, we investigated reinforcement learning behavior and its computational substrates in a task where we orthogonally manipulated both outcome valence and magnitude, resulting in systematic variations in state-values. Over two experiments, model comparison indicated that subjects&apos; behavior is best accounted for by an algorithm which includes both reference point-dependence and range-adaptation-two crucial features of state-dependent valuation. In addition, we found state-dependent outcome valuation to progressively emerge over time, to be favored by increasing outcome information and to be correlated with explicit understanding of the task structure. Finally, our data clearly show that, while being locally adaptive (for instance in negative valence and small magnitude contexts), state-dependent valuation comes at the cost of seemingly irrational choices, when options are extrapolated out from their original contexts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In everyday life, our decision-making abilities are solicited in situations that range from the most mundane (choosing how to dress, what to eat, or which road to take to avoid traffic-jams) to the most consequential (deciding to get engaged, or to give up on a long-lasting costly project). In other words, our actions and decisions result in outcomes which can dramatically differ in terms of affective valence (positive versus negative) and intensity (small versus big magnitude). These two features of the outcome value are captured by different psychological concepts -affect vs. salience -, and by different behavioral and physiological manifestations (approach/avoidance vs. arousal/energization levels) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> .</p><p>In ecological environments, where new options and actions are episodically made available to a decision-maker, both the valence and magnitude associated with the newly available option and action outcomes have to be learnt from experience.</p><p>The reinforcement-learning (RL) theory offers simple computational solutions, where the expected value (product of valence and magnitude) is learnt by trial-and-error, thanks to an updating mechanism based on prediction error correction <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> . RL algorithms have been extensively used during the past couple of decades in the field of cognitive neuroscience, because they parsimoniously account for behavioral results, neuronal activities in both human and non-human primates, and psychiatric symptoms induced by neuromodulatory dysfunction <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> .</p><p>However, this simple RL model is unsuited to be used as is in ecological contexts <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> . Rather, similarly to the perceptual and economic decision-making domains, growing evidence suggests that reinforcement learning behavior is sensitive to contextual effects <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> . This is particularly striking in loss-avoidance contexts, where an avoided-loss (objectively an affectively neural event) can become a relative reward if the decision-maker has frequently experienced losses in the considered environment. In that case, the decision-maker's knowledge about the reward distribution in the recent history or at a specific location, affects her perception of the valence of outcomes. Reference-dependence, i.e., the evaluation of outcomes as gains or losses relative to a temporal or spatial reference point (context), is one of the fundamental principles of prospect theory and behavioral economics <ref type="bibr" target="#b16">17</ref> . Yet, only recently have theoretical and experimental studies in animal and human investigated this reference-dependence in RL <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> . These studies have notably revealed that reference-dependence can significantly improve learning performances in contexts of negative valence (loss-avoidance), but at the cost of generating post-learning inconsistent preferences <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> .</p><p>In addition to this valence reference-dependence, another important contextual effect that may be incorporated in ecological RL algorithms is range adaptation. At the behavioral level, it has long been known that our sensitivity to sensory stimuli or monetary amounts is not the same across different ranges of intensity/magnitude <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22</ref> . These findings have recently paralleled with the description of neuronal range adaptation: in short, the need to provide efficient coding of information in various ranges of situations entails that the firing rate of neuron adapts to the distributional properties of the variable being encoded <ref type="bibr" target="#b22">23</ref> . Converging pieces of evidence have recently confirmed neuronal range-adaptation in economic and perceptual decision-making, although its exact implementation remains debated <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> .</p><p>Comparatively, the existence of behavioral and neural features of range-adaptation has been less explored in RL, where it could critically affect the coding of outcome magnitude. In the reinforcement-learning framework the notion of context, which is more prevalent in the economic or perception literatures, is embodied in the notion of state. In the RL framework the environment is defined as a collection of discrete states, where stimuli are encountered, decisions are made and outcomes are collected. Behavioral and neural manifestations of context-dependence could therefore be achieved by (or reframed as) state-dependent processes.</p><p>Here, we hypothesized that in human RL, the trial-by-trial learning of option and action values is concurrently affected by reference-point centering and range adaptation. To test this hypothesis and investigate the computational basis of such state-dependent learning, we adapted a well-validated RL paradigm <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28</ref> , to include orthogonal manipulations of outcome valence and outcome magnitude.</p><p>Over two experiments we found that human RL behavior is consistent with value-normalization, both in terms of state-based reference-dependence and range-adaptation. To better characterize this normalization process at the algorithmic level, we compared several RL algorithms, which differed in the extent and in the way they implement state-dependent valuation (reference-dependence and range adaptation). In particular, we contrasted models implementing full, partial or no value normalization <ref type="bibr" target="#b28">29</ref> . We also evaluated models implementing state-dependent valuation at the decision stage (as opposed to the outcome evaluation stage) and implementing marginally decreasing utility (as proposed by Bernoulli) <ref type="bibr" target="#b21">22</ref> . Overall, the normalization process was found to be partial, to occur at the valuation level, to progressively arise during learning and to be correlated with explicit understanding of the task structure (environmental). Finally, while being optimal in an efficient coding perspective, this normalization leads to irrational preference when options are extrapolated out from their original learning context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral paradigm to challenge context-dependence</head><p>Healthy subjects performed two variants of a probabilistic instrumental learning task with monetary rewards and losses. In those two variants, participants saw at each trial a couple of abstract stimuli (options) which were probabilistically paired with good or bad outcomes, and had to select the one they believed would be most beneficial for their payoff. The options were always presented in fixed pairs, which defined stable choice contexts. These contexts were systematically manipulated, so as to implement a 2x2 factorial design across two qualities of the option outcomes: outcome valence (reward or loss) and outcome magnitude (big; 1€; or small: 10c). In all contexts, the two options were associated with different, stationary, outcome probabilities (75% or 25%). The 'favorable' and 'unfavorable' options differ in their net expected value. The favorable option in the reward and big magnitude context is paired with a reward of 1€ with probability 75%, while the unfavorable option only 25% of the time. Likewise, the favorable option in the loss and small magnitude context is paired with a loss of 10 cents with probability 25%, while the unfavorable option 75% of the time <ref type="figure" target="#fig_3">(Figure 1)</ref>. Subjects therefore had to learn to choose the options associated either with highest reward probability or those associated with lowest loss probability.</p><p>After the last learning session, subjects performed a transfer test in which they were asked to indicate the option with the highest value, in choices involving all possible binary combinations -that is, including pairs of options that had never been associated during the task. Transfer test choices were not followed by feedback, to not interfere with subjects' final estimates of option values. In the second variant of the experiment, an additional factor was added to the design: the feedback information about the outcomes (partial or complete) was manipulated to make this variant a 2x2x2 factorial design. In the partial context, participants were only provided with feedback about the option they chose, while in the complete context, feedback about the outcome of the non-chosen option was also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcome magnitude moderately affects learning performance</head><p>In order to characterize the learning behavior of participants in our tasks, we first simply analyzed the correct response rate in the learning sessions, i.e., choices directed toward the most favorable stimulus (i.e. associated with the highest expected reward or the lowest expected loss). In all contexts, this average correct response rate was higher than chance level 0.5, signaling significant instrumental learning effects (T(59)=16.6, P&lt;0.001). We also investigated the effects of our main experimental manipulations (outcome valence (reward/loss), outcome magnitude (big/small) and feedback information (partial/complete, Experiment 2 only)) ( <ref type="table">Table 1)</ref>. Because there was no significant effect of the experiment (i.e., when explicitly entered as factor 'Experiment': F(59)=0.96, P&gt;0.3), we polled the two experiments to assess the effects of common factors (outcome valence and magnitude). Replicating previous findings <ref type="bibr" target="#b18">19</ref> , we found that the outcome valence did not affect learning performance (F(59)=0.167, P&gt;0.6), and that feedback information significantly modulated learning in Experiment 2 (F(39)=7.4, P&lt;0.01). Finally, we found that the outcome magnitude manipulation, which is a novelty of the present experiments, had a significant effect on learning performance (F(59)=9.09, P&lt;0.004); Post-hoc test confirmed that across both experiments subjects showed significantly higher correct choice rate in the big-magnitude compared with the smallmagnitude contexts (T(59)&gt;3.0, P&lt;0.004), and similar correct choice rate in the reward compared to the losses contexts (T(59)=0.41, P&gt;0.13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Option preferences in the transfer test cannot be explained by option expected value</head><p>Following the analytical strategy used in previous studies <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> , we next turned to the results from the transfer test, and analyzed the pattern of correct choice rates, i.e., the proportion of choices directed toward the most favorable stimulus (i.e., associated with the highest expected reward or the lowest expected loss). Overall, the correct choice rate in the transfer was significantly higher than chance, thus providing evidence of significant value transfer and retrieval (T(59)&gt;3.0, P&lt;0.004). We also analyzed how our experimental factors (outcome valence (reward/loss), outcome magnitude (big/small) and option favorableness (i.e., being the symbol the most favorable of its pair during the learning sessions)) influenced the choice rate per symbol. The choice rate per symbol is the average frequency with which a given symbol is chosen in the transfer test, and can therefore be taken as a measure of the subjective preference for a given option. Consistent with significant value transfer and retrieval, the ANOVA revealed significant effects of outcome valence (F(59)=76, P&lt;0.001) and option correctness (F(59)=203.5, P&lt;0.001) indicating that -in average -symbols associated with favorable outcomes were preferred compared to symbols associated with less favorable ones. However, and in line with what we found in simpler contexts <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28</ref> , the analysis of the transfer test revealed that option preference did not linearly follow the objective ranking based on their absolute expected value (Probability(Outcome) x Magnitude(Outcome)). For example, the favorable option of the reward/small context was chosen more often than the less favorable option of the reward/big context (0.71±0.03 vs 0.41±0.04; T(59)=6.43, P&lt;0.0001). Similarly, the favorable option of the loss/small magnitude context was chosen more often than the less favorable option of the reward/small context (0.42±0.03 vs 0.56±0.03; T(59)=2.88, P&lt;0.006). Crucially, while the latter value inversion reflects reference-point dependence, as shown in previous studies <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28</ref> , the former effect is new and could be a signature of a more global range-adaptation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Delineating the computational hypothesis</head><p>Although these overall choice patterns appear puzzling at first sight -since they would be classified as "irrational" from the point of view of the classical economic theory based on absolute values <ref type="bibr" target="#b29">30</ref> -, we previously reported that similar seemingly irrational behavior and inconsistent results could be coherently generated and explained by state-dependent reinforcementlearning models. To hypothesize this reasoning, we next turned to computational modeling to provide a parsimonious explanation of the present results.</p><p>To do so, we fitted the behavioral data with several variations of standard RL models (see Methods). The first model is a standard Q-learning algorithm, referred to as ABSOLUTE. The second model is a modified version of the Q-learning model that encodes outcomes in a state-dependent manner:</p><formula xml:id="formula_0">(1) !"# = !"# + max 0, −</formula><p>where the state value V(s) is initialized to 0, takes the value of the first non-zero (chosen or unchosen) outcome in each context s, and then remains stable over subsequent trials. The first term of the question implements range adaptation (divisive normalization) and the second term reference point-dependence (subtractive normalization). As a result, favorable/unfavorable outcomes are encoded in a binary scale, despite their absolute scale. We refer to this model as RELATIVE, while highlighting here that this model extends and generalizes the so-called "RELATIVE model" employed in a previous study, since the latter only incorporated a reference-point-dependence subtractive normalization term, and not a range adaptation divisive normalization term <ref type="bibr" target="#b18">19</ref> .</p><p>The third model, referred to as HYBRID, encodes the reward as a weighted sum of an ABSOLUTE and a RELATIVE reward:</p><p>!"# ( ) = * !"# ( ) + (1 − ) * !"# ( ) The weight parameter (ω) of the HYBRID model quantifies at the individual level the balance between absolute (ω=0.0) and relative value encoding (ω=1.0).</p><p>The fourth model, referred to as the UTILITY model, implements the economic notion of marginally decreasing subjective utility <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22</ref> . Since our task included only two non-zero outcomes, we implemented the UTILITY model by scaling the big magnitude outcomes (|1€|) with a multiplicative factor (0.1&lt;υ&lt;1.0).</p><p>Finally, the fifth model, referred to as the POLICY model, normalizes (range adaptation and reference point correction) values at the decision step (i.e., in the softmax), where the probability of choosing 'a' over 'b' is defined by:</p><p>(3)</p><formula xml:id="formula_2">! , = 1 1 + ! ! !,! !! ! !,! ! ! !,! !! ! !,! * ! !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparison favors the HYBRID model</head><p>For each model, we estimated the optimal free parameters by likelihood maximization. The Bayesian Information Criterion (BIC) was then used to compare the goodness-of-fit and parsimony of the different models. We ran three different optimization and comparison procedures, for the different phases of the experiments: learning sessions only, transfer test only, and both tests. Thus we obtained a specific fit for each parameter and each model in the learning sessions, transfer test, and both.</p><p>Overall (i.e., across both experiments and experimental phases), we found that the HYBRID model significantly better accounted for the data compared to the RELATIVE, the ABSOLUTE, the POLICY and the UTILITY models (HYB vs. ABS T(59)=6.35, P&lt;0.0001; HYB vs. REL T(59)=6.07, P&lt;0.0001; HYB vs. POL T(59)=6.79, P&lt;0.0001; HYB vs. UTY T(59)=2.72, P&lt;0.01). This result was robust across experiments and across experimental sessions (learning sessions vs. transfer test) ( <ref type="table" target="#tab_1">Table 3</ref>). In the main text we focus on discussing the ABSOLUTE and the RELATIVE models, which are nested within the HYBRID and therefore represent extreme cases (absent or complete) of value normalization. We refer to the Supplementary Materials for a detailed analysis of the properties of the POLICY and the UTILITY models, and the reasons of their rejections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model simulations falsify the ABSOLUTE and the RELATIVE models</head><p>Although model comparison unambiguously favored the HYBRID model, we next aimed to falsify the alternative models, using simulations <ref type="bibr" target="#b30">31</ref> . To do so, we compared the correct choice rate in the learning sessions to the model predictions of the three main models (ABSOLUTE, RELATIVE and HYBRID). We generated for each model and for each trial t the probability of choosing the most favorable option, given the subjects' history of choices and outcomes, using the individual best-fitting sets of parameters. Concerning the learning sessions, we particularly focused on the magnitude effect (i.e., the difference in performance between big and small magnitude contexts). As expected, the ABSOLUTE model exacerbates the observed magnitude effect (simulations vs. data, T(59)=5.8, P&lt;0.001). On the other side, the RELATIVE model underestimates the actual effect (simulations vs. data, T(59)=3.0, P&lt;0.004). Finally (and unsurprisingly), the HYBRID model manages to accurately account for the observed magnitude effect (T(59)=0.93, P&gt;0.35) <ref type="figure" target="#fig_4">(Figure 2 A-B)</ref>. We subsequently compared the choice rate in the transfer test to the three models' predictions. Both the ABSOLUTE and the RELATIVE models failed to correctly predict choice preference in the transfer test ( <ref type="figure" target="#fig_4">Figure 2</ref>.C and <ref type="table">Table S2</ref>). Crucially, both models failed to predict the choice rate of intermediate value options. The ABSOLUTE model predicted a quite linear option preference, predicting that the transfer test choice rate should be highly determined by the expected utility of the options. On the other side, the RELATIVE model's predictions of the transfer test option preferences were uniquely driven by the option context-dependent favorableness. Finally, choices predicted by the HYBRID model accurately captured the observed option preferences by predicting both an overall correlation between preferences and expected utility and the violation of the monotony of this relation concerning intermediate value options <ref type="figure" target="#fig_4">(Figure 2</ref>.D). To summarize, and similarly to what was observed in previous studies <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29</ref> , choices in both the learning and transfer test could not be explained by assuming that option values are encoded in an absolute manner, nor by assuming that they are encoded in a fully context-dependent manner, but are consistent with a partial context dependence. In the subsequent sections we analyze the factors that affect value contextualization both within and between subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative value encoding emerges during learning</head><p>Overall we found that a weighted mixture of absolute and relative value encoding (the HYBRID model) better explained the data compared to the "extreme" ABSOLUTE or RELATIVE models. However, this model comparison integrates over all the trials, leaving open the possibility that, while on average subjects displayed no neat preference for either of the two extreme models, this result may arise from averaging over different phases in which one of the models could still be preferred. To test this hypothesis, we analyzed the trial-by-trial likelihood difference between the RELATIVE and the ABSOLUTE model. This quantity basically measures which model better predicts the data in a given trial: if positive, the RELATIVE model better explains the data, if negative, the ABSOLUTE model does. We submitted the trial-by-trial likelihood difference during a learning session to a repeated measure ANOVA with 'trial' (1:80) as within-subject factor. This analysis showed a significant effect of trial indicating that the evidence for the RELATIVE and the ABSOLUTE model evolves over time (F(79)=6.2, P&lt;2e-16). Post-hoc tests revealed two big clusters of trials with non-zero likelihood difference: a very early cluster (10 trials from the 4 th to the 14 th ) and a very late one (17 trials from the 62 th to the 78 th ). To confirm this results, we averaged across likelihood difference in the first half (1:40 trials) and in the second half (41:80 trials). In the first half we found this differential to be significantly negative, indicating that the ABSOLUTE model better predicted subjects' behavior (T(59)=2.1, P=0.036). In contrast, in the second half we found this differential to be significantly positive, indicating that the RELATIVE model better predicted subjects' behavior (T(59)=2.1, P=0.039). Furthermore, a direct comparison between the two phases also revealed a significant difference (T(59)=3.9, P=0.00005) <ref type="figure" target="#fig_5">(Figure 3.A-B)</ref>. Finally, consistent with a progressively increasing likelihood of the RELATIVE compared the ABSOLUTE model during the learning sessions, we found that the weight parameter (ω) of the HYBRID model obtained from the transfer test (0.50±0.05) was numerically higher compared to that of the learning sessions (0.44±0.05) ( <ref type="table">Table S1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterfactual information favors relative value learning</head><p>The two experiments differed in that in the second one (Experiment 2) half of the trials were complete feedback trials. In complete feedback trials, subjects were presented with the outcomes of both the chosen and the forgone options. In line with the observation that information concerning the forgone outcome promotes state-dependent valuation both at the behavioral and neural levels <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32</ref> , we tested whether or not the presence of such "counterfactual" feedbacks affects the balance between absolute and relative value learning. To do so, we compared the negative log-likelihood difference between the RELATIVE and the ABSOLUTE model separately for the two experiments. Note that since the two models have the same number of free parameters, they can be directly compared using the log-likelihood. In Experiment 2 (where 50% of the trials were "complete feedback" trials) we found this differential to be significantly positive, indicating that the RELATIVE model better fits the data (T(39)=2.5, P=0.015). In contrast, in Experiment 1 (where 0% of the trials were "complete feedback" trials), we found this differential to be significantly negative, indicating that the ABSOLUTE model better fits the data (T(19)=2.9, P=0.001). Furthermore, a direct comparison between the two experiments also revealed a significant difference (T(58)=3.9, P=0.0002) <ref type="figure" target="#fig_5">(Figure 3.C)</ref>. Accordingly, we also found the weight parameter (ω) of the HYBRID model to be significantly higher in Experiment 2 compared to Experiment 1 (T(58)=2.8, P=0.007) <ref type="figure" target="#fig_5">(Figure 3.D)</ref>. Finally, consistently with reduced relative value learning, we found that the correct choice difference between the 1€ and the 0.1€ contexts in Experiment 1 (mean: +0.10; range: -0.24/+0.51) was 189.5% of that observed in Experiment 2 (mean: +0.05; range: -0.32/+0.40).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit understanding of task structure is linked to relative value encoding</head><p>In our learning protocol the fact that options were presented in fixed pairs (i.e. contexts) has to be discovered by subjects, because the information was not explicitly given in the instructions and the contexts were not visually cued. In between the learning and the transfer phases subjects were asked whether or not they believed that options were presented in fixed pairs and how many pairs there were (in the second session). Concerning the first question ("fixed pairs"), 71.7% of subjects responded correctly. Concerning the second question ("pairs number"), 50.0% of subjects responded correctly and the average number of pairs was 3.60±0.13, which significantly underestimated the true value (four: T(59)=3.0, P=0.0035). To test whether or not the explicit knowledge of the subdivision of the learning task in discrete choice contexts was correlated with the propensity to learn relative values, we calculated the correlation between the number of correct responses in the debriefing (0, 1 or 2) and the weight parameter (ω) of the HYBRID model. We found a positive and significant correlation (R 2 =0.11, P=0.009) (direct comparison of the weight parameter (ω) between subjects with 0 vs. 2 correct responses in the debriefing: T(37)=2.8, P=0.0087) <ref type="figure" target="#fig_5">(Figure 3</ref>.E). To confirm this result, we ran the reciprocal analysis, by splitting subjects into two groups according to their weight parameter and we found that subjects with ω&gt;0.5 had a significantly higher number of correct responses in the debriefing compared to subjects with ω&lt;0.5 (T(58)=3.0, P=0.0035) <ref type="figure" target="#fig_5">(Figure 3</ref>.F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rational and irrational consequences of relative value encoding</head><p>Previous behavioral analyses, as well as model comparison results, showed that a mixture of relative and absolute value learning (the HYBRID model) explained subjects' behavior. In particular, during the learning sessions, subjects displayed a correct choice difference between the 1€ and the 0.1€ contexts smaller than that predicted by the ABSOLUTE model. During the transfer test, the response pattern indicated, consistent with the RELATIVE model, "correct" options with lower expected utility were often preferred to "incorrect" options with higher expected utility. To formally test the hypothesis that relative value learning is positively associated with correct choice in the learning phase (i.e., rational) and negatively associated with correct choice (i.e., choice of the option with the highest absolute value) in the transfer phase (i.e., irrational), we tested the correlation between correct choice rates in these two phases and the weight parameter (ω), which quantifies the balance between the ABSOLUTE (ω=0.0) and RELATIVE models (ω=1.0). Consistent with this idea we found a positive and significant correlation between the weight parameter and the correct choice rate in the 0.1€ contexts (R 2 =0.19, P=0.0005) and a negative and significant correlation between the same parameter and the correct choice rate in the transfer test (R 2 =0.42, P=0.00000003) <ref type="figure" target="#fig_5">(Figure 3.G-H)</ref>. This means that, the better a subject was at picking the correct option during the learning phase (rational behavior), the least often she would pick the option with the highest absolute value during the test phase (irrational behavior).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In the present paper, we investigated state-dependent valuation in human reinforcement learning. In particular, we adapted a task designed to address the reference-dependence <ref type="bibr" target="#b18">19</ref> to include an additional manipulation of the magnitude of outcomes, in order to investigate range-adaptation <ref type="bibr" target="#b25">26</ref> . In the learning sessions, analyses of behavioral data showed that the manipulation of outcome valence had a significant effect on learning performance, with high-magnitude outcomes inducing better learning compared to low-magnitude outcomes. On the contrary, and in line with what we reported previously <ref type="bibr" target="#b18">19</ref> , the manipulation of outcome valence had no such effect. In the transfer test, participants exhibited seemingly irrational preferences, sometimes preferring options that had objectively lower expected values than other options. Crucially, these irrational preferences are compatible with state-dependent valuation.</p><p>State-dependent (or context-dependent) valuation has been ascribed to a large number of different behavioral, neural and computational manifestations <ref type="bibr" target="#b15">16</ref> . Under this rather general umbrella, reference-dependence and range-adaptation constitute two specific, and in principle dissociable, mechanisms: on the one hand, reference-dependence is the mechanism through which, in a context where monetary losses are frequent, loss avoidance (an affective neural event) is experienced as a positive outcome. On the other hand, range-adaptation is the mechanism through which, in contexts with different outcome magnitudes (i.e., different affective saliency), high-magnitude and low-magnitude outcomes are experienced similarly.</p><p>In order to formally and quantitatively test for the presence of these two components of state-dependent valuation in our experimental data, we used computational modelling. Our model space included two 'extreme' models: the ABSOLUTE and the RELATIVE models. The ABSOLUTE model learns the context-independent -absolute -value of available options. In contrast, the RELATIVE model implements both reference-dependence and range-adaptation ('full' adaptation; <ref type="bibr" target="#b28">29</ref> ). These two 'extreme' models predict radically different choice patterns in both the learning sessions and the transfer test. While the ABSOLUTE model predicts a big effect of outcome magnitude in the learning sessions and rational preferences in the transfer test, the RELATIVE model predicts no magnitude effect and highly irrational preferences in the transfer test.</p><p>Specifically, according to the RELATIVE model, the choices in the transfer test are not affected by the outcome valence or by the outcome magnitude, but dominated by options' context-dependent favorableness factor. Comparison between model simulations and experimental data falsified both models <ref type="bibr" target="#b30">31</ref> , since in both the learning sessions and in the transfer test, subjects performance lied in between the predictions of the ABSOLUTE and RELATIVE models. To account for this pattern we designed a HYBRID model. The HYBRID model implements a trade-off between the absolute and relative learning modules, which is governed by an additional free parameter ('partial adaptation'; <ref type="bibr" target="#b28">29</ref> ). Owing to this partial adaptation, the HYBRID model accurately accounts for the performance in the learning sessions and for the preferences expressed in the transfer test, including the preference inversion patterns.</p><p>Using model comparison, we attempted to provide a specific description of the process at stake in our task, and ruled out alternative accounts of normalization. Crucially, normalization can be implemented as an adaptation over time of the valuation mechanism to account for the distribution of option values encountered in successive choices, or as a timeindependent decision mechanism limited to the values of options considered in one choice event <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33</ref> . In the present case, model comparison favored the HYBRID model which implements a time-adapting value normalization against the POLICY model which implements a time-independent decision normalization. This result derives from the fact that during the learning sessions, the POLICY model uses a divisive normalization at the moment of choice to level the learning performance in different contexts (e.g. big and small magnitudes), while still relying on learning absolute values <ref type="bibr" target="#b24">25</ref> . Therefore, these absolute values cannot produce the seemingly irrational preferences observed in the transfer test.</p><p>The idea that the magnitude of available outcomes is somewhat rescaled by decision-makers is the cornerstone of the concept of utility <ref type="bibr" target="#b21">22</ref> . In economics, this magnitude normalization is considered a stable property of individuals, and typically modelled with a marginally decreasing utility function whose parameters reflect individual core preferences <ref type="bibr" target="#b33">34,</ref><ref type="bibr">35</ref> This approach was implemented in the UTILITY model, present in our model space. However, this model did not provide a satisfactory account of the behavioral data, and hence was not favored by the model-comparison approach. Similarly to the case of the POLICY model, this result derives from the fact that the UTILITY model cannot account for the emergence of reference-dependence, which is necessary to produce preference reversals between the symbols of opposite valence in the transfer test. Crucially, correct choice rate during the learning sessions were equally well predicted by the UTILITY and the HYBRID models, thus highlighting the importance of using a transfer test, where options are extrapolated from original contexts, to challenge computational models of value learning and encoding <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37</ref> . Overall, our model comparison (based on both goodness-of-fit criteria and simulation-based falsification) favored the HYBRID model, which indicates that the pattern of choices exhibited by our subjects in the learning sessions and in the transfer test is most probably the result of a trade-off between absolute and relative values. In the HYBRID model, this tradeoff was implemented by a subject-specific weight parameter (ω), which quantified the relative influence of the normalized versus absolute value-learning modules. A series of subsequent analyses revealed that several relevant factors affect this trade-off. First, we showed using an original trial-by-trial model comparison that the trade-off between absolute valuelearning and normalized value learning implemented by the HYBRID model is progressive and gradual. This is an important novelty compared to previous work which only suggested such progressivity by showing that value rescaling was dependent of progressively acquired feedback information <ref type="bibr" target="#b17">(18)</ref>. Note that learning normalized value ultimately converges to learning which option of a context is best, regardless of its valence or relative value compared to the alternative option. Second, and in line with the idea that information concerning the forgone outcome promotes state dependent valuation <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32</ref> , we also found that the relative weight of the normalized-value learning module (ω) increased when more information was available (counterfactual feedback). Finally, individuals whose pattern of choices was indicative of a strong influence of the normalized value learning module (i.e., with higher ω) appeared to have a better understanding of the task, assessed in the debriefing.</p><p>Overall, these findings suggest that value normalization is the results of a 'high-level' -or 'model-based' -process through which outcome information is not only used to update action values, but also to build an explicit representation of the embedding context where outcomes are experienced. Consistent with this interpretation, value normalization has recently been shown to be degraded by manipulations imposing a penalty for high-level costly cognitive functions, such as high memory load conditions in economic decision-making tasks <ref type="bibr" target="#b36">38</ref> . One can also speculate that value contextualization should be impaired under high cognitive load <ref type="bibr" target="#b37">39</ref> and when outcome information is made unconscious <ref type="bibr" target="#b38">40</ref> . Future research using multitasking and visual masking could address these hypotheses <ref type="bibr" target="#b39">41</ref> . An additional feature of the design suggests that this value normalization is an active process. In our paradigm the different choice contexts were presented in an interleaved manner, meaning that a subject could not be presented with the same context more than a few times in a row. Therefore, contextual effects could not be ascribed to slow and passive habituation (or sensitization) processes.</p><p>Although the present results, together with converging evidence in economics and psychology, concordantly point that statedependent valuation is needed to provide a satisfactory account of human behavior, there is still an open debate concerning the exact implementation of such contextual influences. In paradigms where subjects are systematically presented with full feedback information, it would seem that subjects simply encode the difference between obtained and forgone outcome, thus parsimoniously achieving full context-dependence without explicitly representing and encoding state value <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32</ref> . However, such models cannot be easily and effectively adapted to tasks where only partial feedback information is available. In these tasks, context-dependence has been more efficiently implemented by assuming separate representational structures for action and state values which are then used to center action-specific prediction errors <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> . In the present paper, we implemented this computational architecture in the HYBRID model, which builds on a partial adaptation scheme between an ABSOLUTE and a RELATIVE model. Although descriptive by nature, such hybrid models are commonly used in multi-step decision-making paradigms, e.g., to implement trade-offs between model-based and model free learning <ref type="bibr" target="#b40">[42]</ref><ref type="bibr" target="#b41">[43]</ref><ref type="bibr" target="#b42">[44]</ref> , because they allow to readily quantify the contributions of different learning strategies, and to straightforwardly map to popular dualprocess accounts of decision-making <ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b44">46</ref> . In this respect, future studies adapting the present paradigm for functional imaging will be crucial to assess whether absolute and relative (i.e., reference-point centered and range adapted) outcome values are encoded in different regions (dual valuation), or whether contextual information is readily integrated with outcome values in a single brain region (partial adaptation). However, it should be noted that previous studies using similar paradigms, consistently provided support for the second hypothesis, by showing that contextual information is integrated in a brain valuation system encompassing both the ventral striatum and the ventral prefrontal cortex, which therefore represent 'partially adapted' values <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29</ref> . This is corroborated by similar observations from electrophysiological recordings of single neurons in monkeys <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b46">48</ref> .</p><p>As in our previous study <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28</ref> , we also manipulated outcome valence in order to create 'gain' and 'loss' decision frames.</p><p>While focusing on the results related to the manipulation of outcome magnitude, which represented the novelty of the present design, we nonetheless replicated previous findings indicating that subjects perform equally well in both decision frames and that this effect is parsimoniously explained assuming relative value encoding. This robust result contradicts both standard reinforcement principles and behavioral economic results. In the context of animal learning literature, while Thorndike's famous law of effect parsimoniously predicts reward maximization in a 'gain' decision frame, it fails to explain punishment minimization in the 'loss' frame. Mower elegantly formalized this issue ( 49 'how can a shock that is not experienced, i.e., which is avoided, be said to provide […] a source of […] satisfaction?') and proposed the two-factor theory that can be seen as an antecedent of our relative value-learning model. In addition, the gain/loss behavioral symmetry is surprising with respects to behavioral economic theory because it contradicts the loss aversion principle <ref type="bibr" target="#b16">17</ref> . In fact, if 'losses loom larger than gains', one would predict a higher correct response rate in the 'loss' compared to the 'gain' domain in our task. Yet, such deviations to standard behavioral economic theory are not infrequent when decisions are based on experience rather than description <ref type="bibr" target="#b48">50</ref> , an observation referred to as the "experience/description gap" <ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b50">52</ref> . While studies of the "experience/description gap" typically focus on deviations regarding attitude risky and rare outcomes, our and other groups' results indicate that a-less documented but nonetheless -robust instance of the experience/description gap is precisely the absence of loss aversion <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">53</ref> .</p><p>To conclude, state-dependent valuation, defined as the combination of reference-point dependence and range-adaptation, is a double-edged sword of value-based learning and decision-making. Reference-point dependence provides obvious beneficial behavioral consequences in punishment avoidance contexts and range-adaptation allows to perform optimally when decreasing outcome magnitudes. The combination of these two mechanisms (implemented in the HYBRID model) is therefore accompanied with satisfactory learning performance in all proposed contexts. However, these beneficial effects on learning performance are traded-off against possible suboptimal preferences and decisions, when options are extrapolated from their original context. Crucially, our results show that state-dependent valuation remains only partial. As a consequence, subjects under-performed in the learning sessions relative to full context-dependent strategies (RELATIVE model), as well as in the transfer test relative to absolute value strategies (ABSOLUTE model). These findings support the idea that bounded rationality may not only arise from intrinsic limitations of the brain computing capacity, but also from the fact that different situations require different valuation strategies to achieve optimal performance. Given the fact that humans and animals often interact with changing and probabilistic environments, apparent bounded rationality may simply be the result of the effort for being able to achieve a good level of performance in a variety of different contexts. These results shed new light on the computational constraints shaping everyday reinforcement learning abilities in humans, most-likely set by evolutionary forces to optimally forage in changing environments <ref type="bibr" target="#b34">36</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental subjects</head><p>We tested 60 subjects (39 females; aged 22.3±3.3 years). Subjects were recruited via Internet advertising in a local mailinglist dedicated to cognitive science-related activities. We experienced no technical problems, so we were able to include all 60 subjects. The research was carried out following the principles and guidelines for experiments including human participants provided in the declaration of Helsinki <ref type="figure" target="#fig_3">(1964, revised in 2013)</ref>. The local Ethical Committee approved the study and subjects provided written informed consent prior to their inclusion. To sustain motivation throughout the experiment, subjects were given a bonus dependent on the actual money won in the experiment (average money won: 3.73±0.27, against chance T(59)=13.9, P&lt;0.0001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral protocol</head><p>Subjects performed a probabilistic instrumental learning task adapted from previous imaging and patient studies <ref type="bibr" target="#b18">19</ref> . Subjects were first provided with written instructions, which were reformulated orally if necessary. They were explained that the aim of the task was to maximize their payoff and that seeking monetary rewards and avoiding monetary losses were equally important. For each experiment, subjects performed two learning sessions. Cues were abstract stimuli taken from the Agathodaimon alphabet. Each session contained four novel pairs of cues. The pairs of cues were fixed, so that a given cue was always presented with the same other cue. Thus, within sessions, pairs of cues represented stable choice contexts.</p><p>Within sessions, each pair of cues was presented 20 times for a total of 80 trials. The four cue pairs corresponded to the four contexts (reward/big magnitude, reward/small magnitude, loss/big magnitude and loss/small magnitude). Within each pair, the two cues were associated to a zero and a non-zero outcome with reciprocal probabilities (0.75/0.25 and 0.25/0.75). On each trial, one pair was randomly presented on the left and the right side of a central fixation cross. Pairs or cues were presented in a pseudo-randomized and unpredictable manner to the subject (intermixed design). The side in which a given cue was presented was also pseudo-randomized, such that a given cue was presented an equal number of times in the left and the right of the central cue. Subjects were required to select between the two cues by pressing one of the corresponding two buttons, with their left or right thumb, to select the leftmost or the rightmost cue, respectively, within a 3000ms time window. After the choice window, a red pointer appeared below the selected cue for 500ms. At the end of the trial, the cues disappeared and the selected one was replaced by the outcome ("+1.0€","+0.1€", "0.0€", "-0.1€" or "-1.0€") for 3000ms. In Experiment 2, in the complete information contexts (50% of the trials), the outcome corresponding to the unchosen option (counterfactual) was displayed. A novel trial started after a fixation screen (1000ms, jittered between 500-1500ms). After the two learning sessions, subjects performed a transfer test. This transfer test involved only the 8 cues (2*4 pairs) of the last session, which were presented in all possible binary combinations (28, not including pairs formed by the same cue) (see also <ref type="bibr" target="#b17">18</ref> ). Each pair of cues was presented 4 times, leading to a total of 112 trials. Instructions for the transfer test were provided orally after the end of the last learning session. Subjects were explained that they would be presented with pairs of cues taken from the last session, and that all pairs would not have been necessarily displayed together before. On each trial, they had to indicate which of the cues was the one with the highest value by pressing on the buttons as in the learning task.</p><p>Subjects were also explained that there was no money at stake, but encouraged to respond as they would have if it were the case. In order to prevent explicit memorizing strategies, subjects were not informed that they would have to perform a transfer test until the end of the second (last) learning sessions. Timing of the transfer test differed from that of the learning sessions in that the choice was self-paced and in the absence of outcome phase. During the transfer test, the outcome was not provided in order not to modify the option values learned during the learning sessions. Between the leaning sessions and the transfer test subjects were interviewed in order to probe the extent of their explicit knowledge of the task's structure.</p><p>More precisely the structured interview assessed: 1) whether or not the subjects were aware about the cues being presented in fixed pairs (choice contexts); 2) how many choice contexts they believed were simultaneously present in a learning session. The experimenter recorded the responses, but provided no feedback about their correctness in order to not affect subjects' performance in the transfer test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-free analyses</head><p>For the two experiments, we were interested in three different variables reflecting subjects' learning: (1) correct choice rate (i.e. choices directed toward highest expected reward or the lowest expected loss) during the learning task of the experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model space</head><p>We analyzed our data with extensions of the Q-learning algorithm <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b52">54</ref> . The goal of all models was to find in each choice context (or state) the option that maximizes the expected reward R.</p><p>At trial t, option values of the current context s are updated with the Rescorla-Wagner rule 5 :</p><formula xml:id="formula_3">( 4 ) !!! , = ! , + ! !,! !!! , = ! , + ! !,!</formula><p>where ! is the learning rate for the chosen (c) option and ! the learning rate for the unchosen (u) option, i.e. the counterfactual learning rate. ! and ! are prediction error terms calculated as follows:</p><formula xml:id="formula_4">( 5 ) !,! = ! ( ) − ! ( , ) !,! = ! ( ) − ! ( , )</formula><p>! is updated in both partial and complete feedback contexts and ! is updated in the complete feedback context only (Experiment 2, only).</p><p>We modelled subjects' choice behavior using a softmax decision rule representing the probability for a subject to choose one option a over the other option b:</p><formula xml:id="formula_5">( ) ! , = 1 1 + ! ! !,! !! ! (!,!) !</formula><p>where β is the temperature parameter. High temperatures cause the action to be all (nearly) equi-probable. Low temperatures cause a greater difference in selection probability for actions that differ in their value estimates <ref type="bibr" target="#b3">4</ref> .</p><p>We compared four alternative computational models: the ABSOLUTE model, which encodes outcomes in an absolute scale independently of the choice context in which they are presented; the RELATIVE model which encodes outcomes on a binary (correct/incorrect) scale, relative to the choice context in which they are presented <ref type="bibr" target="#b53">55</ref> ; the HYBRID model, which encodes outcomes as a weighted sum of the absolute and relative value; the POLICY model, which encodes outcome in an absolute scale, but implements divisive normalization in the policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSOLUTE model</head><p>The outcomes are encoded as the subjects see them as feedback. A positive outcome is encoded as its "real" positive value (in euros) and a negative outcome is encoded as its "real" negative value (in euros):</p><formula xml:id="formula_6">!"# ∈ −1.0€ , −0.1€ , 0.0€ , 0.1€ , 1.0€ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATIVE model</head><p>The outcomes (both chosen and unchosen) are encoded on a context-dependent correct/incorrect relative scale. The model assumes the effective outcome value to be adapted to the range of the outcomes present in a given context. The option values are no longer calculated in an absolute scale, but relatively to their choice context value: in the delta-rule, the correct option is updated with a reward of 1 and the incorrect option is updated with a reward of 0. To determine the context of choice, the model uses a state value V(s) stable over trials, initialized to 0, which takes the value of the first non-zero (chosen or unchosen) outcome in each context s.</p><p>!"# ( ) = !"# ( ) ( ) + max 0, − ( ) ( ) Thus, the outcomes (chosen and unchosen) are now normalized to a context-dependent correct/incorrect encoding:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POLICY model</head><p>We also considered a fourth POLICY model that encodes option values as the ABSOLUTE model and normalizes them in the softmax rule, i.e., at the decision step <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">47</ref> :</p><formula xml:id="formula_8">( 9 ) ! , = 1 1 + ! ! !,! !! ! (!,!) ! ! !,! !! ! (!,!) * ! ! UTILITY model</formula><p>Finally, we considered a fifth UTILITY model, which implements the economic notion of marginally decreasing subjective utility <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22</ref> . The big magnitude outcomes ( = 1) are re-scaled with a multiplicative factor 0.1 &lt; υ &lt; 1.0:</p><p>!"# = υ * !"# = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model fitting, comparison and simulation</head><p>Specifically for the learning sessions, transfer test, and both, we optimized model parameters, the temperature , the factual learning rate ! , the counterfactual learning rate ! (in Experience 2 only) and the weight (in the HYBRID model only), by minimizing the negative log likelihood !"# using Matlab's fmincon function, initialized at starting points of 1 for the temperature and 0.5 for the learning rates and the weight. As a quality check we replicated this analysis using multiple starting points and this did not change the results (S <ref type="table">Table 4</ref>). We computed at the individual level the Bayesian Information Criterion (BIC) using, for each model, its number of free parameters ! (note that the Experiment 2 has an additional parameter ! ) and the number of trials (note that this number of trials varies with the optimization procedure: learning sessions only, 160, transfer test only, 112, or both, 272):</p><formula xml:id="formula_10">( 11 ) = 2 * !"# + log * !</formula><p>Model estimates of choice probability were generated trial-by-trial using the optimal individual parameters. We made comparisons between predicted and actual choices with a one-sample t-test and tested models' performances out of the sample by assessing their ability to account for the transfer test choices. On the basis of model-estimate choice probability, we calculated the log-likelihood of learning sessions and transfer test choices that we compared between computational models. Finally, we submitted the model-estimate transfer-test choice probability to the same statistical analyses as the actual choices (ANOVA and post-hoc t-test; within-simulated data comparison) and we compared modeled choices to the actual data. In particular, we analyzed actual and simulated correct choice rates (i.e., the proportions of choices directed toward the most advantageous stimulus) and compared transfer-test choices for each symbol with a sampled t-test between the behavioral choices and the simulated choices.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Statistical effects were assessed using multiple-way repeated measures ANOVAs with feedback valence, feedback magnitude, and feedback information (in Experiment 2 only) as within-subject factors; (2) correct choice rate during the transfer test, i.e., choosing the option with the highest absolute expected value (each symbol has a positive or negative absolute expected value, calculated as Probability(outcome) x Magnitude(outcome)); and (3) choice rate of the transfer test (i.e., the number of times an option is chosen, divided by the number of times the option is presented). The variable represents the value attributed to one option, i.e., the preference of the subjects for each of the symbols. Transfer test choice rates were submitted to multiple-way repeated measures ANOVAs, to assess the effects of option favorableness (being the most advantageous option of the pair), feedback valence and feedback magnitude as within-subject factors. Post-hoc tests were performed using one-sided, one-sample t-tests. As a control analysis, additional post-hoc tests were performed against chance. All statistical analyses were performed using Matlab (www.mathworks.com).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>!</head><label></label><figDesc>"# ( ) ∈ 0 , 1 . The chosen and unchosen option values and prediction errors are updated with the same rules as in the ABSOLUTE model. HYBRID model At trial t the prediction errors of the chosen and unchosen options are updated as a weighted sum of the absolute and relative outcomes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 8 ) 1 .</head><label>81</label><figDesc>!"# ( ) = * !"# ( )+ (1 − ) * !"# ( )where ω is the individual weight. At each trial t, the model independently encodes both outcomes as previously described and updates the final HYBRID outcome: !"# = The chosen and unchosen option values and prediction errors are updated with the same rules as in the ABSOLUTE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Experimental design and normalization process (A) Learning task with 4 different contexts: reward/big, reward/small, loss/small, loss/big. Each symbol is associated with a probability (P) of gaining or losing an amount of money or magnitude (M). M varies as a function of the choice contexts (reward seeking: +1.0€ or +0.1€; loss avoidance: -1.0€ or -0.1€; small magnitude: +0.1€ or -0.1€; big magnitude: +1.0€ or -1.0€). (B) The graph schematizes the transition from absolute value encoding (where values are negative in the loss avoidance contexts and smaller in the small magnitude contexts) to relative value encoding (complete adaptation as in the RELATIVE model), where favorable and unfavorable options have similar values in all contexts, thanks to both reference-point and range adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Behavioral results and model simulations. (A) Correct choice rate during the learning sessions. (B) Big magnitude contexts' minus small magnitude contexts' correct choice rate during the learning sessions. (C) and (D) Choice rate in the transfer test. Colored bars represent the actual data. Black (RELATIVE), white (ABSOLUTE), and grey (HYBRID) dots represent the model-predicted choice rate. White stars indicate significant difference compared to zero **p&lt;0.01. Green arrows indicate significant differences between actual and predicted choices at p&lt;0.001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Computational properties and behavioral correlates of value normalization. (A) Likelihood difference (from model fitting) between the ABSOLUTE and the RELATIVE models over the 80 trials of the task sessions. A negative likelihood difference means that the ABSOLUTE model is the best-fitting model for the trial and a positive likelihood difference means that the RELATIVE model is the best-fitting model for the trial. Green dots: likelihood difference significantly different from 0 (P&lt;0,05). (B) Likelihood difference between the ABSOLUTE and the RELATIVE models over the first part of the task (40 first trials) and the last part (40 last trials). (C) Likelihood difference between the ABSOLUTE and the RELATIVE models for the two experiments. A negative likelihood difference means that the ABSOLUTE model is the bestfitting model for the experiment and a positive likelihood difference means that the RELATIVE model is the best-fitting model for the experiment. (D) Subject-specific free parameter weight (ω) comparison for the two experiments. (E) Subject-specific free parameter weight (ω) as a function of correct debriefing for the two questions ("fixed pairs" and "number of pairs"). (F) Debriefing as a function of the weight parameter. (G) and (H) Correct choice rate as a function of subjects' weight parameter in the learning sessions and the transfer test for both Experiment 1 and Experiment 2. One dot corresponds to one participant (N=60); green lines represent the linear regression calculations. ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :Table 1 :Table 2 :</head><label>112</label><figDesc>Correct choice rate of the learning sessions as a function of task factors in the Experiment 1, Experiment 2 and both experiments. Model parameters of the HYBRID model as a function of the dataset used for parameter optimization (Learning sessions, Transfer test or Both) and the computational model. Model simulated choice rate of the transfer test as a function of the eight symbols presented in the transfer test, and the computational models. For each model the simulation of the choice rate of the transfer test is tested against real data for both Experiment 1 and Experiment 2.</figDesc><table><row><cell></cell><cell cols="2">Experiment 1 (N=20)</cell><cell cols="2">Experiment 2 (N=40)</cell><cell cols="2">Both Experiments (N=60)</cell></row><row><cell></cell><cell>F-val</cell><cell>P-val</cell><cell>F-val</cell><cell>P-val</cell><cell>F-val</cell><cell>P-val</cell></row><row><cell>Val</cell><cell>0,002</cell><cell>0,969</cell><cell>0,285</cell><cell>0,597</cell><cell>0,167</cell><cell>0,684</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Reaction times rate as a function of task factors in the Experiment 1, Experiment 2 and both experiments.</figDesc><table><row><cell cols="3">Experiment 1 (N=20)</cell><cell cols="2">Experiment 2 (N=40)</cell><cell cols="2">Both Experiments (N=60)</cell></row><row><cell>F-val</cell><cell></cell><cell>P-val</cell><cell>F-val</cell><cell>P-val</cell><cell>F-val</cell><cell>P-val</cell></row><row><cell>Valence</cell><cell>90,27</cell><cell>***1,19e-08</cell><cell>45,31</cell><cell>***5,02e-08</cell><cell>92,58</cell><cell>***1,07e-13</cell></row><row><cell>Information</cell><cell>-</cell><cell>-</cell><cell>0,175</cell><cell>0,403</cell><cell>-</cell><cell>-</cell></row><row><cell>Magnitude</cell><cell>14,55</cell><cell>**0,00117</cell><cell>1,889</cell><cell>0,177</cell><cell>9,896</cell><cell>**0,0026</cell></row><row><cell>Val x Inf</cell><cell>-</cell><cell>-</cell><cell>4,616</cell><cell>*0,038</cell><cell>-</cell><cell>-</cell></row><row><cell>Val x Mag</cell><cell>10,79</cell><cell>**0,0039</cell><cell>8,4399</cell><cell>**0,00602</cell><cell>15,89</cell><cell>***0,000187</cell></row><row><cell>Inf x Mag</cell><cell>-</cell><cell>-</cell><cell>4,773</cell><cell>*0,035</cell><cell>-</cell><cell>-</cell></row><row><cell>Val x Inf x Mag</cell><cell>-</cell><cell>-</cell><cell>0,001</cell><cell>0,97</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Emmanuel Noblins and Alexander Salvador provided help for data collection.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure S1</ref><p>).</p><p>We also analyzed the generative performances of the UTILITY model: similarly to the HYBRID model, the UTILITY model is able to perfectly capture the size of the magnitude effect in the learning sessions (simulation vs. data, T(59)=0.2, P&gt;0.80).</p><p>Accordingly, the quality of fit (BIC) difference between these two models was not different when considering the learning sessions alone (HYB vs. UTY, T(59)=0.2, P&gt;0.84, <ref type="table">Table 3</ref>). However, when considering the transfer test, the UTILITY model unsurprisingly also predicted linear patterns (similar to the ABSOLUTE model), and failed to predict the value inversion between the intermediate options ( <ref type="figure">Figure S1)</ref>. Accordingly, the quality of fit (BIC) difference between the HYBRID and the UTILITY models was significantly different when considering the learning sessions alone (HYB vs. UTY, T(59)=3.3, P&lt;0.002, <ref type="table">Table 3</ref>) ( <ref type="figure">Figure S1</ref>).  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action versus valence in decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guitart-Masip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="194" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring affect from fMRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knutson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="422" to="428" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Losses as modulators of attention: review and analysis of the unique effects of losses over gains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yechiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hochman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="497" to="518" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1054" to="1054" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Class. Cond. II Curr. Res. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="64" to="99" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural substrate of prediction and reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1593" to="1599" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dissociable roles of ventral and dorsal striatum in instrumental conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O'doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page" from="452" to="454" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">By carrot or by stick: cognitive reinforcement learning in parkinsonism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Seeberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>O'reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="page" from="1940" to="1943" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pessiglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Flandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Frith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">442</biblScope>
			<biblScope unit="page" from="1042" to="1045" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pharmacological modulation of subliminal learning in Parkinson&apos;s and Tourette&apos;s syndromes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="19179" to="19184" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The ecological rationality of state-dependent valuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Houston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="114" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-dependent utility overrides absolute memory as a determinant of choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pompilio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="508" to="512" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="617" to="629" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Space and time in visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="522" to="535" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Choices, Values, and Frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.39.4.341</idno>
	</analytic>
	<monogr>
		<title level="j">Am. Psychol</title>
		<imprint>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chapter 24 -The Neurobiology of Context-Dependent Valuation and Choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Martino</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-416008-8.00024-3</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroeconomics</title>
		<editor>Glimcher, P. W. &amp; Fehr, E.</editor>
		<imprint>
			<biblScope unit="page" from="455" to="476" />
			<date type="published" when="2014" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prospect Theory: An Analysis of Decision under Risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econom. J. Econom. Soc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning relative values in the striatum induces violations of normative decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ullsperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16033</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual modulation of value signals in reward and punishment learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khamassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joffily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coricelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8096</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural processes mediating contextual influences on human choice behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rigoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12416</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Elemente der psychophysik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Fechner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1860" />
			<publisher>Breitkopf und Härtel</publisher>
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Specimen Theoriae Novae de Mensura Sortis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bernoulli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Normalization as a canonical neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="51" to="62" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rationalizing Context-Dependent Preferences: Divisive Normalization and Neurobiological Constraints on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Glimcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.2462895</idno>
	</analytic>
	<monogr>
		<title level="j">Choice. SSRN Electron. J</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Free choice shapes normalized value signals in medial orbitofrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tymula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Range-adapting representation of economic value in the orbitofrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Padoa-Schioppa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Off. J. Soc. Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="14004" to="14014" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal coding and neuronal adaptation in economic decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rustichini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Conen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Padoa-Schioppa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1208</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Computational Development of Reinforcement Learning during Adolescence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Kilford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coricelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Blakemore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1004953</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Partial Adaptation of Obtained and Observed Value Signals Preserves Information about Gains and Losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Tobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10016" to="10025" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Theory of Games and Economic Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Importance of Falsification in Computational Cognitive Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palminteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koechlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="425" to="433" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Signals in human striatum are appropriate for policy update rather than value prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Off. J. Soc. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5504" to="5511" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Value normalization in decision making: theory and evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Clithero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="970" to="981" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Appendix -Prospect Theory and the Brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-416008-8.00042-5</idno>
		<editor>Glimcher, P. W. &amp; Fehr, E.</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="533" to="567" />
		</imprint>
	</monogr>
	<note>in Neuroeconomics. Second Edition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tools for thought or thoughts for tools?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="10071" to="10072" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Preference by association: how memory mechanisms in the hippocampus bias decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shohamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="270" to="273" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adaptive Value Normalization in the Prefrontal Cortex Is Reduced by Memory Load</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Holper</surname></persName>
		</author>
		<idno type="DOI">10.1523/ENEURO.0365-17.2017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Working-memory capacity protects model-based learning from stress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Raio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="20941" to="20946" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The what and where in visual masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ogmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Breitmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Melvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1337" to="1350" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How the brain translates money into force: a neuroimaging study of subliminal motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pessiglione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="904" to="906" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1704" to="1711" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">States versus Rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gläscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>O'doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="585" to="595" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modelling Individual Differences in the Form of Pavlovian Conditioned Approach Responses: A Dual Learning Systems Approach with Factored Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lesaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Flagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khamassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003466</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dual-processing accounts of reasoning, judgment, and social cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A perspective on judgment and choice: mapping bounded rationality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Psychol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="697" to="720" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Divisive Normalization Predicts Time-Varying Value Coding in Decision-Related Circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lofaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16046" to="16057" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Normalization is a general neural mechanism for context-dependent decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Khaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="6139" to="6144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The motivation to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herzberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When Do Losses Loom Larger Than Gains?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wertenbroch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mark. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="134" to="138" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Within-subject preference reversals in description-and experience-based choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Camilleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Newell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cognitive Science Society</publisher>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The description-experience gap in risky choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="517" to="523" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Of Black Swans and Tossed Coins: Is the Description-Experience Gap in Risky Choice Limited to Rare Events?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Spetch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20262</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Does the brain calculate value?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D A</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="546" to="554" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
