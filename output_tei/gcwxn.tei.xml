<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">D</forename><surname>Mcdougle</surname></persName>
							<email>samuel.mcdougle@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Helen Wills Neuroscience Institute</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<address>
									<addrLine>Yale University 2 Hillhouse Ave New Haven</addrLine>
									<postCode>06520</postCode>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Life is full of decisions, and decisions take time. Consider a labored deliberation in the cheese section of a grocery store -do you opt for your old stand-by, the Irish Cheddar, or take a risk on a fragrant Roquefort? Or maybe the Gouda? Research on decision-making typically focuses on the choices people make (which cheese?), though studying decision time can also shed light on underlying cognitive processes. In our grocery example, several factors may influence decision time: For instance, decision time could be affected by both how much you like a particular option over the others (which can become stronger with experience), but also the total number of options there are to choose from (which will vary in different contexts).</p><p>Most preferences emerge via learning, suggesting that learning models could be useful for explaining decision latencies. Indeed, a body of recent research <ref type="bibr" target="#b19">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b20">Frank et al., 2015;</ref><ref type="bibr" target="#b34">Miletić et al., 2020;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017;</ref><ref type="bibr" target="#b61">Shahar et al., 2019)</ref> has attempted to combine models derived from reinforcement learning (RL) theory with a class of sequential sampling process models derived from perceptual decision-making -"evidence accumulation" modelswhich account for choice and RT data simultaneously. In evidence accumulation models, such as Ratcliff's drift diffusion model (DDM; <ref type="bibr" target="#b48">Ratcliff, 1978)</ref> or <ref type="bibr">Brown and Heathcote's Linear Ballistic Accumulator (LBA;</ref><ref type="bibr" target="#b4">Brown &amp; Heathcote, 2008)</ref>, RT is determined by the accumulation of evidence for different choices, where accumulators move towards a decision boundary. Evidence accumulation models are traditionally used to fit RT distributions in decision-making tasks, where human and other animal subjects have to, for instance, integrate noisy evidence over time to make perceptual discriminations <ref type="bibr" target="#b51">(Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b59">Shadlen &amp; Newsome, 1996;</ref><ref type="bibr" target="#b67">Usher &amp; McClelland, 2001)</ref>, perform categorical classifications <ref type="bibr" target="#b38">(Nosofsky &amp; Palmeri, 1997;</ref><ref type="bibr" target="#b57">Sewell et al., 2019)</ref>, or choose between well-known items with different subjective values <ref type="bibr" target="#b6">(Busemeyer et al., 2019)</ref>. These sequential sampling models provide good fits to RT data, and provide a link between psychological processes and neural mechanisms. For example, the incremental accumulation of perceptual evidence has been linked to parametric changes in the spiking of cortical neurons <ref type="bibr" target="#b59">(Shadlen &amp; Newsome, 1996)</ref>.</p><p>Recent studies directly linking evidence accumulation with reinforcement learning <ref type="bibr" target="#b19">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b20">Frank et al., 2015;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017;</ref><ref type="bibr" target="#b61">Shahar et al., 2019)</ref> have used tasks where subjects have to choose between two actions to maximize probabilistic rewards. These models suggest that the rate of accumulation may be proportional to differences in the learned value of actions: If two actions have similar values, internal evidence accumulation (and thus choice RT) will be slow relative to a situation where one action is strongly preferred over the other. Because of this principled relationship between RT and choice, sequential sampling models can also be leveraged for fitting choice data, providing a more mechanistic account of the decision-making process compared to simpler choice policies (e.g., softmax).</p><p>To our knowledge, however, no modeling effort that links RL and RT has addressed the full range of established choice RT effects. These effects include the set size effect, where increasing the number of choice options drives a logarithmic increase in RT (" <ref type="bibr">Hick's Law";</ref><ref type="bibr" target="#b25">Hick, 1952;</ref><ref type="bibr" target="#b46">Rabbitt, 1968)</ref>, repetition effects (i.e., attenuated RT when a choice stimulus is repeated; <ref type="bibr" target="#b2">Bertelson, 1965)</ref>, delay effects (i.e., changes in RT based on how long ago a choice stimulus was last observed; <ref type="bibr" target="#b28">Hyman, 1953;</ref><ref type="bibr" target="#b52">Remington, 1969)</ref>, and learning and set size interactions (i.e., gradual reductions in RT and the attenuation of set size effects over time; <ref type="bibr" target="#b15">Davis et al., 1961;</ref><ref type="bibr" target="#b36">Mowbray &amp; Rhoades, 1959;</ref><ref type="bibr" target="#b44">Proctor &amp; Schneider, 2018;</ref><ref type="bibr" target="#b55">Schneider &amp; Anderson, 2011)</ref>. Although some memory-based accumulation models can capture set size effects <ref type="bibr" target="#b40">(Pearson et al., 2014)</ref>, they often do not address learning or repetition effects. In contrast, the aforementioned RL-based DDM models <ref type="bibr" target="#b20">(Frank et al., 2015;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017)</ref> can capture choices and RT distributions, but are not suited for capturing set size effects, as they are usually designed for two-alternative forced-choice tasks.</p><p>One short-term memory model built on the ACT-R framework <ref type="bibr" target="#b55">(Schneider &amp; Anderson, 2011</ref>) was able to capture Hick's Law and learning-related RT effects, but did not model RT distributions. A recent neural model provides a normative account of multi-alternative decision-making that captures set size effects, but does not address learning <ref type="bibr" target="#b66">(Tajima et al., 2019)</ref>. Taking inspiration from these previous efforts, we propose a simple model of choice and RT that can capture this range of behavioral phenomena.</p><p>Furthermore, we test two specific hypotheses about RT and instrumental learning by analyzing two previous behavioral data sets and performing one new experiment. First, we test the idea that choice RT is best modeled by taking into account both a labile working memory process and a slow RL process that operate in parallel during learning. Previous work has shown that choices during instrumental learning are best explained by simultaneous contributions from both of these systems (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. Second, we posit a key latent variable that modulates decision time: a prior uncertainty over actions, where the speed of action selection is influenced by an internal estimate of action uncertainty averaged over all states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and behavior overview</head><p>Even in simple tasks, multiple cognitive processes may be recruited to optimize our decisions.</p><p>For instance, a driver approaching an intersection has to select well-practiced motor movements to slow the vehicle at the proper rate, while also guiding attentional control to various external factors as they decide which lane to enter (e.g., the position of neighboring cars, the distance until the next turn, etc.). Various studies show that decision-making in a simple laboratory stimulusresponse learning task is best modeled by accounting for these two systems, exemplified by, respectively, RL and working memory (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. Specifically, when human subjects learn deterministic stimulus-response mappings, they appear to rely on short-term memory of recent trial outcomes, in addition to gradual, implicit consolidation of the correct stimulus-response map.</p><p>The trade-off between these qualitatively distinct processes may be influenced by set size (the number of stimulus-response instances to be learned), where lower set sizes lead to more working memory-driven learning and higher set sizes lead to more RL-driven learning. A dualprocess model that captures this idea -the RLWM model -has been shown to provide a better fit to choice data in these tasks than models that postulate a single learning mechanism (A. G. E. <ref type="bibr" target="#b9">Collins et al., , 2017</ref><ref type="bibr" target="#b12">A. G. E. Collins &amp; Frank, 2018;</ref><ref type="bibr">A. G. E. Collins &amp; Frank, 2012)</ref>.</p><p>However, the RLWM model was not developed to account for RT data. Here we build on this body of work to provide a more complete model that captures both choice and RT, and we also extend this model to stochastic learning contexts with probabilistic reward feedback. We start by describing the laboratory task, previous behavioral findings, and the RLWM model introduced in previous studies.</p><p>The standard version of the RLWM task ( <ref type="figure">Fig. 1A</ref>; A. G. E. <ref type="bibr" target="#b9">Collins et al., , 2017</ref><ref type="bibr" target="#b12">A. G. E. Collins &amp; Frank, 2018;</ref><ref type="bibr">A. G. E. Collins &amp; Frank, 2012)</ref> proceeds as follows: Subjects are instructed to learn which of three responses is associated with a particular image to maximize reward. Stimuli are presented in a pseudorandomized sequence within a block of trials, and subjects are required to respond to each stimulus with a button press (either the "j", "k", or "l" keys on a keyboard) in under 1.5 seconds. When a stimulus appears and the correct response is made, a reward of "+1" points is earned; when an incorrect response is made, no reward is earned. In the standard version of this task, rewards are deterministic -each stimulus is associated with one correct response (however, see below for results of a probabilistic version of the task). Each stimulus is seen 9-12 times per block.</p><p>Critically, each block is associated with a particular set size, which represents the number of distinct stimulus-response pairs to be learned during that block. Moreover, to discourage subjects from trying to infer correct actions for unseen stimuli (e.g., via process-of-elimination), different stimulus-response "mappings" are used within set sizes. For example, in one set size 3 block, each of the three stimuli could be associated with exactly one of the 3 available response buttons, while in another set size 3 block, two stimuli could be associated with one response, with the third stimulus associated with a second response and no stimuli associated with the third response.</p><p>The first key behavioral result is the effect of set size on performance: Average learning curves at each set size are shown in <ref type="figure">Figure 1B</ref>. Subjects learn to select correct actions in all set sizes, but they are slower to learn at higher set sizes. This negative effect of set size on performance has multiple possible sources: First, it could be the result of interference between stimulus representations or value decay within the RL system, where higher set sizes lead to a greater degree of interference. A non-mutually exclusive proposal is that subjects also recruit working memory processes in this task, and that the restrictive capacity limitations of working memory account for most of the set size effects observed (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. Indeed, support for the latter has been observed in computational (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>, neuropsychological , and neurophysiological studies <ref type="bibr" target="#b9">(Collins et al., 2017;</ref><ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref>.</p><p>Similarly, delay (i.e., the number of trials since the current stimulus was last responded to) also has a negative effect on performance ( <ref type="figure">Figure 1C</ref>), and subjects' performance at different delays interacts with set size, where longer delays at higher set sizes leads to relatively weaker performance. This result reflects a form of sequential learning effects <ref type="bibr" target="#b32">(Lohse et al., 2020)</ref> that are consistent with trial-or time-based decay (i.e., forgetting) of items held in short-term memory <ref type="bibr" target="#b43">(Posner &amp; Keele, 1967)</ref>.</p><p>Lastly, the magnitude of the adverse influence of set size on performance diminishes with practice ( <ref type="figure">Figure 1D</ref>). This suggests that subjects may cache learned associations over time, perhaps reducing their reliance on more costly, capacity-limited executive functions. We note that the behavioral effects depicted in <ref type="figure">Figure 1</ref> (A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref> have been replicated across several different studies using this task (A. G. <ref type="bibr" target="#b9">Collins et al., , 2017</ref><ref type="bibr" target="#b14">A. G. Collins &amp; Frank, 2012)</ref>. <ref type="bibr" target="#b14">Collins and Frank (2012)</ref> formalized the concept of working memory (WM) and reinforcement learning (RL) working in parallel in a simple learning model, the RLWM model. In the RLWM model, two modules learn the stimulus-response contingencies (i.e., state-action values) over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RLWM model of choice</head><p>The learning of stimulus-action values is modeled using a variant of a standard RL model <ref type="bibr" target="#b65">(Sutton &amp; Barto, 1998)</ref>. The task consists of two main variables -the state, s (i.e., the stimulus on the <ref type="figure">Figure 1</ref>. RLWM task and behavioral signatures. (A) Task design. In the RLWM task, subjects learn stimulusresponse associations over several blocks of trials. Two example blocks are shown, each with a different set size, or the number of associations to be learned in that block. Regardless of set size, three actions are available. Stimuli are presented in a pseudo-randomized sequence, and each stimulus is seen 9 times within a block. (B) Learning, plotted as a function of stimulus iteration, is less robust as set size increases. (C) A greater number of intervening trials between responses to a specific stimulus decreases performance. The effect of this trial-based delay is stronger in higher set sizes. (D) The effect of set size on performance is most pronounced early in learning versus late in learning. All error bars = 95% CIs. Data from <ref type="bibr" target="#b12">Collins &amp; Frank, 2018.</ref> screen), and the action, a (i.e., the button pressed). The action-value in a given state, Q(s,a), is updated on each trial, t, using the delta rule <ref type="bibr" target="#b53">(Rescorla &amp; Wagner, 1972)</ref>:</p><formula xml:id="formula_0">1) !"# ( , ) = ! ( , ) + ! 2) ! = − ! ( , )</formula><p>where is the learning rate, is the reward prediction error, and r is the (binary) reward received.</p><p>In the basic RLWM model of choice, values are transformed into probabilities, or "weights", with</p><formula xml:id="formula_1">the softmax function, 3) ( | ) = $ !(#,%)' % ( $ !)#,% ( *'</formula><p>where constitutes the inverse temperature parameter, and the sum in the denominator is taken over the three possible actions, &amp; .</p><p>The RLWM model captures the parallel recruitment of working memory (WM) and reinforcement learning (RL) by training two simultaneous learning modules ( <ref type="figure" target="#fig_1">Figure 2A</ref>). The RL module is characterized by Equations 1-2. The WM module learns stimulus-response associations (W), and is formally similar to Equations 1-2 albeit with a fixed learning rate of WM = 1:</p><formula xml:id="formula_2">4) !"# ( , ) = ! ( , ) + '( ( − ! ( , )) =</formula><p>Thus, the WM module has, in principle, perfect learning of the observed outcome, which makes it qualitatively distinct from a gradual RL process. Critically, however, working memory is vulnerable to short-term forgetting after updating is performed: The model captures trial-by-trial</p><formula xml:id="formula_3">decay of W, 5) ! 3 ) , &amp; 4 = ! 3 ) , &amp; 4 + 6 * − ! 3 ) , &amp; 47</formula><p>where draws W (over all stimuli j and actions i) toward their initial values,</p><formula xml:id="formula_4">* = # + +</formula><p>, where , is the number of actions (in this task, 3).</p><p>Separate WM and RL policies ( '( and -. ) are computed using the softmax function (Equation 3), and are then combined in the calculation of the final policy via a weighted sum,</p><formula xml:id="formula_5">6) = '( + (1 − ) -.</formula><p>where w approximates how much WM should contribute to the decision <ref type="figure" target="#fig_1">(Figure 2A</ref>). This parameter is determined by two free parameters, the working memory capacity (i.e., resource limit) C, and the initial WM weighting ,</p><formula xml:id="formula_6">7) = * 61, / +_1 , 7</formula><p>where n_S represents the set size in block k. In short, this equation says that the influence of WM on choice is reduced if the set size exceeds WM capacity C. This weighting step, and the free parameters C and , are critical for capturing the quantitative and qualitative effects of set size on performance in this task (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>.</p><p>Lastly, the model also captures learning biases, in particular, the neglect of negative feedback consistently observed in this task: When an action is incorrect and thus generates a negative prediction error (i.e., &lt; 0), the learning rate is reduced multiplicatively:</p><formula xml:id="formula_7">8) =</formula><p>where controls the degree of perseveration (higher values cause less perseveration, and lower values more). Perseveration occurs for both the RL and WM modules; in the latter case, the fixed learning rate of 1 is scaled by . Critically, the final output of the RLWM model is , which represents the action policy. We note here that instead of referring to the quantities represented in the policy as probabilities, as they're typically referred to in RL, we will refer to them as weights given to each of the three possible responses. In several of the models described below, we extend the function of these weights to serve as the input to an accumulation process, allowing us to model both choice and RT.</p><p>Expanding the RLWM choice model to RTs</p><p>In settings with binary choices, the drift-diffusion model (DDM) is often used to model choice and RT <ref type="bibr" target="#b20">(Frank et al., 2015;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017;</ref><ref type="bibr" target="#b48">Ratcliff, 1978;</ref><ref type="bibr" target="#b49">Ratcliff &amp; McKoon, 2008)</ref>. While powerful, this model is not particularly well-suited to situations where there are more than , = 2 available actions. A similar evidence accumulation model, the Linear Ballistic Accumulator, or LBA ( <ref type="figure" target="#fig_1">Figure 2B</ref>; <ref type="bibr" target="#b4">Brown &amp; Heathcote, 2008)</ref>, can easily accommodate any number of actions ( , = 1, 2, 3,...∞). The LBA shares many key properties with the DDM, although within-trial accumulation is simplified to a noiseless linear process.</p><p>Accumulation via the LBA is schematized in <ref type="figure" target="#fig_1">Figure 2B</ref>. Parameter A corresponds to the upper limit of a uniform distribution from which the starting point (or bias) of the accumulator is drawn.</p><p>The parameter b corresponds to the boundary of accumulation (i.e., the threshold at which the accumulator terminates and generates a reaction time). The parameter * determines the "nondecision time", commonly interpreted as time taken for visual processing of the stimulus and motor execution (not shown).</p><p>The density function associated with the ith accumulator in the LBA is given by <ref type="bibr" target="#b4">(Brown &amp; Heathcote, 2008)</ref>:</p><formula xml:id="formula_8">9) &amp; ( ) = &amp; ( ) ∏ 61 − ) ( )7 )2&amp;</formula><p>where ) ( ) is the cumulative probability function associated with all other competing accumulators, ≠ . Here, the probability density of a response time for a particular accumulator is normalized by the probability of the agent making the response associated with that particular accumulator, with other accumulators (competing actions) not having reached threshold. The termination time distribution function for the ith accumulator to be the first to reach threshold ( &amp; )</p><p>is the given by the probability density function:</p><formula xml:id="formula_9">10) &amp; ( ) = # , L− &amp; Φ 6 34,4!5 ( !65 7 + ϕ 6 34,4!5 ( !65 7 + &amp; Φ 6 34!5 ( !65 7 − ϕ 6 34!5 ( !65 7P</formula><p>where the drift rate is drawn from the normal N( &amp; , ), and ϕ and Φ refer to, respectively, the Gaussian distribution's density and cumulative probability functions. Further details concerning the LBA distribution specifications and their mathematical derivations can be found in <ref type="bibr" target="#b4">Brown &amp; Heathcote (2008)</ref>.</p><p>We have thus far presented two separate modeling frameworks -the RLWM model of learning and the LBA model of reaction time <ref type="figure" target="#fig_1">(Figure 2A</ref>, B). How should we connect these two models to capture both learning and RT in our instrumental learning task?</p><p>We start with a baseline model inspired by previous work connecting reinforcement learning processes with the DDM. In these other models, the difference between learned Q-values of two competing actions directly scales a single mean drift rate v of a diffusion process <ref type="bibr" target="#b20">(Frank et al., 2015;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017)</ref>. Thus, when two action values are far apart, RT will be short, and</p><p>when two values are close, RT will be long. Directly replicating that model with an LBA, which instead has individual accumulators for each action, is not possible; however, individual drift rates can be scaled proportionally by their associated action weights.</p><p>The first model we tested (the model) posits a nonlinear relationship between latent action weights and accumulation rates. In the model, weights of each action scale the drift rate of their associated accumulators: Each drift rate mean parameter &amp; is directly multiplied by the associated weight &amp; of each action i on trial t:</p><formula xml:id="formula_10">11) &amp;,! = &amp;,!</formula><p>where is a scaling parameter (simply allowing for the scaling of all drift rates across subjects).</p><p>Critically, this model performs softmax normalization (Equation 3) to compute and thus to determine the accumulation drift rates associated with each action; this step reflects the assumption that a non-linearity (i.e., the transformation of Q and W into weights) governs both the differential weighting of Q and W and the relationship between action value and reaction time.</p><p>This aspect of the model is consistent with similar recent work <ref type="bibr" target="#b19">(Fontanesi et al., 2019)</ref>, as well as assumptions from the actor-critic framework <ref type="bibr" target="#b65">(Sutton &amp; Barto, 1998)</ref>, where state-action weights in the striatum govern decision latency.</p><p>Consider that the time needed for a decision should not only be affected by the difference of one action's value over another (e.g., a strong preference for eating chocolate ice cream versus vanilla), but, more generally, the uncertainty over all relevant actions (e.g., choosing between ten flavors that are all similarly valued). Indeed, uncertainty is thought to be a key ingredient in capturing choice RT <ref type="bibr" target="#b28">(Hyman, 1953)</ref>. Thus, we hypothesized that drift rates should vary as a function of two quantities: First, individual accumulation rates should be affected by the estimated weight of each action i given the current state ( &amp; ), as reflected in the model above. Second, we intuited that prior uncertainty over actions (i.e., over their average weights over all stimuli within a block) would also influence decision time. That is, the time it takes to select an action in a given state may be affected to some degree by the distribution of average action weights across all states. Thus, if the average weights of the three possible actions across all states are very similar, we should expect maximum uncertainty, and a slow RT. This prior uncertainty term ( 89&amp;:9 ) was modeled by first computing an average policy (S⃑ ; ), which requires averaging action weights for each action i over each state/stimulus k ( &amp;,&lt; ) across all n_S possible states/stimuli:</p><formula xml:id="formula_11">12) S⃑ &amp; ; = # +1 ∑ &amp;,&lt; +_1 &lt;=#</formula><p>This simple averaging step thus collapses latent action weights into a single 1 X , vector that putatively represents the probability of choosing each of the three actions prior to encoding the current trial's stimulus. Then, to ascertain the degree of uncertainty over this prior, the Shannon entropy <ref type="bibr" target="#b62">(Shannon, 1948)</ref> is computed on this vector, inspired by classic work on RT and uncertainty <ref type="bibr" target="#b28">(Hyman, 1953)</ref>. (We note here that using inverse variance instead of entropy to quantify uncertainty produced qualitatively similar results.):</p><formula xml:id="formula_12">13) 89&amp;:9 = − ∑ S⃑ &amp; ; &gt; 3S⃑ &amp; ; 4 ? &amp;=#</formula><p>Because action weights change with learning, the quantity above will take on a unique value for each trial t. To illustrate, if the current block was a set size 4 block, the three-element vector S⃑ ;</p><p>used to compute 89&amp;:9 is the column-wise average over a 4 X 3 matrix of states X actions.</p><p>Finally, we incorporate the uncertainty quantity from Equation 13 into the evidence accumulation rate for the ith accumulator using division:</p><formula xml:id="formula_13">14) &amp;,! = Y @ (,- A ./(0/,- Z</formula><p>Thus, in this model, all three drift rates are scaled down equally by the degree of uncertainty associated with taking any particular action in that trial. This heuristic could be interpreted as capturing conflict between actions, which occurs at the level of their prior probabilities going into each trial. We hypothesized that this additional consideration would help the model better estimate RTs. We refer to this as the A model.</p><p>We also tested two additional control models, the Q model and the A -RL model. In the Q model, we tested an alternative assumption where latent variables from the separate RL (Q-values) and</p><p>WM (W stimulus-response associations) modules linearly scale drift rates. Thus, we exclude the step where those values are nonlinearly transformed with the softmax function (Equation 3).</p><p>These quantities are still differentially weighted according to Equation 6 to reflect respective WM and RL contributions across different set sizes. However, the mean accumulation rate for each accumulator (vi) corresponding to each action i is directly proportional to the weighted Q and W quantities for each action (Vi) on trial t:</p><formula xml:id="formula_14">15) &amp;,! = &amp;,!</formula><p>Lastly, the A -RL model was included to test the utility of including a working memory module in the underlying learning process (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. This model is identical to the A model, but only a single action policy is learned (Equations 1 and 2). In this model, the three working memory-related free parameters -capacity (C), weighting ( ), and decay ( ) -are not included.</p><p>In all four models, choices and RTs are fit simultaneously. That is, a model's fit to subjects' RTs determines its likelihood during the fitting process, and the probability of a given RT is linked to the probability of the choice associated with that RT (Equation 9; <ref type="bibr" target="#b4">Brown &amp; Heathcote, 2008)</ref>.</p><p>Models were fit to the data using maximum likelihood estimation by minimizing the negative log likelihood using the MATLAB function fmincon. Fit quality was determined using both the Bayesian Information Criterion (BIC ; <ref type="bibr" target="#b56">Schwarz, 1978)</ref>, as well as a leave-p-out cross-validation procedure (see Methods for further details on model fitting, parameter recovery, validation, and model simulation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparisons</head><p>The A model assumes that action uncertainty modulates RTs and choices. To test this claim we compared it to the three other variants highlighted above ( , , π A -RL), performing model fitting on a previously-published data set (data set 1; N = 40; A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref>. As shown in <ref type="figure" target="#fig_1">Figure 2C</ref>, the A model fit the RT data better than the other three models (i.e., lower BIC values; all average BIC differences &gt; 120; protected exceedance probability = 1.0). <ref type="figure" target="#fig_1">Figure   2D</ref> shows individual BIC comparisons of the π A model versus the second-best model, the model, for each subject. Best fit parameter values for all models are shown in <ref type="table">Table S1</ref>.</p><p>We also performed a cross-validation comparison analysis <ref type="figure">(</ref> G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. Here, we extend this finding to RT data. Second, as predicted, the model outperformed the model, showing that incorporating a nonlinearity (e.g., via the softmax) better captures the relationship between latent value estimates and evidence accumulation rates, consistent with previous work <ref type="bibr" target="#b19">(Fontanesi et al., 2019)</ref>. Finally, the A model outperformed all other models. This suggests that prior uncertainty over actions has a measurable influence on subjects' behavior in this task.</p><p>We emphasize that the model comparisons highlighted in <ref type="figure" target="#fig_1">Figure 2</ref> reflect how well the models fit the RT distributions, which, in the LBA architecture are also linked to subjects' choices <ref type="bibr" target="#b4">(Brown &amp; Heathcote, 2008)</ref>. Thus, this analysis reflects each model's ability to characterize both RT and choice data simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter recovery</head><p>Although the A model performed well in the fitting procedure, this does not guarantee that the model is well identified. To investigate the model's identifiability, we performed a parameter recovery experiment, simulating choices and RTs using the best-fit parameters from the fitting procedure, and then attempting to fit the resulting synthetic data to recover those parameters (see Methods). The A performed well in the recovery experiment, showing consistent recovery of all 8 of its free parameters ( <ref type="figure" target="#fig_1">Figure S2)</ref>. Moreover, the A model recovered 4 of the 5 parameters of the underlying RLWM learning model significantly better than the RLWM model recovered those free parameters when fit to the same choice data (for statistics see <ref type="figure" target="#fig_1">Figure S2</ref>). This improvement in recovery supports recent findings showing that leveraging RT data in addition to choice data in RL tasks improves the identifiability of underlying RL parameters <ref type="bibr" target="#b1">(Ballard &amp; McClure, 2019;</ref><ref type="bibr" target="#b61">Shahar et al., 2019)</ref>.</p><p>For completeness, we conducted an additional control analysis: In all tested models, the nondecision time parameter t0 -which is meant to capture the portion of the RT that involves perception of the stimulus as well as motor execution -was fixed at 150 ms. We also fit a version of the A model where t0 was allowed to freely vary ( <ref type="figure" target="#fig_2">Figure S3</ref>). In this fitting analysis we found that t0 traded-off with various other model parameters, and also tended to take on values well below biologically reasonable minimum human RTs (i.e., &lt; 100 ms), even when fitting constraints were altered or additional rapid RTs were screened. Moreover, as shown in <ref type="figure" target="#fig_2">Figure S3</ref>, while allowing t0 to vary freely improved the model fit versus having a fixed t0, as was expected, parameter recovery was modestly but consistently attenuated. We note that the main results and conclusions of our study are not significantly altered by using a fixed versus a free t0 parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model simulations</head><p>To validate the A model, we used simulations to test its ability to produce qualitative choice and RT behavior that echoed subjects' behavior. We simulated the model using the best fit parameters from the fitting procedure.</p><p>The model was able to capture the learning time course of reaction times in each set size, showing the expected facilitation of RTs as learning progressed ( <ref type="figure" target="#fig_2">Figure 3A)</ref>. Moreover, the model mimicked the effect of set size on performance in the task ( <ref type="figure" target="#fig_2">Figure 3B</ref>), consistent with previous models fit on choices only (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. We note that our model did not successfully capture RTs in the earliest stimulus iterations, particularly in the lower set sizes (we return to this point in the Discussion).</p><p>The A model also mimicked the logarithmic relationship between set size and average RT (i.e.,</p><p>Hick's Law, also known as the Hick/Hyman Law), as shown in <ref type="figure" target="#fig_3">Figure 4A</ref>. The A model outperformed the model in capturing this relationship (t-test comparing regression coefficients between modeled and observed Hick's Law slopes: t(39) = 8.23, p &lt; 0.001). The model approximated a sigmoidal set size effect rather than a logarithmic one, suggesting a  misspecification in the relationship between action policies and RT. This fundamental error in the model occurs because the model essentially re-capitulates, in RTs, the effect of set size on choice performance ( <ref type="figure">Figure 1B)</ref>. That is, in the choice data, the larger set size effects are present in the higher set sizes, whereas in the RT data, the larger set size effects are present in the lower set sizes. The result presented in <ref type="figure" target="#fig_3">Figure 4A</ref> suggests that the action uncertainty term is critical for capturing the set size effect in RTs. As illustrated in <ref type="figure" target="#fig_3">Figure 4B</ref>, the quantity computed in Equation 14 (which sets the drift rates), here depicted using simulations from the fitted model, decreases exponentially as a function of set size. Taken together, these results echo the classic finding that uncertainty is a key element in the effect of set size on RT <ref type="bibr" target="#b28">(Hyman, 1953)</ref>.</p><p>To further understand why the inclusion of an action uncertainty term helped the A model perform better than the more straightforward model, we next looked at how RTs differed between different stimulus-response (S-R) mappings within each set size.</p><p>Recall that in set sizes greater than 1, subjects could be faced with different S-R mappings within particular set sizes (see Methods). That is, in one set size 4 block, two stimuli could map onto one of the response buttons, another two stimuli could map onto a second response, and no stimuli could map onto the third response. For simplicity, we can notate this mapping as [0 2 2], where each number in this vector represents the number of stimuli assigned to each of the three possible actions (we note here that the order of responses in this notation is not consequential, as actual button assignments were randomized across blocks). In contrast, on a different set size 4 block, two stimuli could map onto one of the responses, and the remaining two stimuli could each separately map onto one of each of the remaining two responses (i.e., a [1 1 2] mapping). Overall, in all n_S &gt; 1 blocks subjects experienced a total of 12 possible mappings ( <ref type="figure">Figure 5</ref>).</p><p>Crucially, after some learning has occurred, different S-R mappings within a set size should be associated with different degrees of action uncertainty. To illustrate, if we imagine a situation where a subject has perfect knowledge of the correct S-R associations, the entropy over the average policy (Equation 13) for a [0 2 2] mapping will be H([0/4 2/4 2/4]) = 1 bit, and for a [1 1 2] mapping will be H([1/4 1/4 2/4]) = 1.5 bits. Thus, the A model will tend to predict a higher RT in the latter condition, even though both conditions have an identical set size. In contrast, the model predicts no such distinction. <ref type="figure">Figure 5</ref> shows average RTs (with 95% confidence intervals) for each mapping within each set size, as well as average simulated RTs from both the A and models. As expected, the A model was able to capture within-set-size variance in RTs while the model was not. Crucially, significant RT mapping effects were not limited to comparisons between mappings with a different number of 0's (i.e., blocks where one action was not associated with any stimuli). For example,</p><p>RTs were significantly lower in the set size 5 [1 1 3] mapping versus the set size 5 [1 2 2] mapping (t(39) = 2.56, p = 0.01), and the set size 4 [0 1 3] mapping versus set size 4 [0 2 2] mapping (t(39) <ref type="figure">Figure 5</ref>. S-R mapping effects. Different stimulus-response (S-R) mappings were used within each set size (n_S &gt; 1).</p><p>Mappings on the x-axis refer to the specific assignment of stimuli to their associated responses -each response could be associated with 0-3 stimuli depending on the particular mapping and set size. S-R mappings are notated by a sorted 3-element vector describing the number of stimuli associated with each response (the order of values in this notation does not reflect the actual response buttons used). Mappings are also visually schematized, where each colored square represents a single stimulus (note that specific stimuli were never repeated across blocks). Error bars = 95% CIs.</p><p>= 4.69, p &lt; 0.001). These results are consistent with the action uncertainty account and rule out potential action "pruning" strategies as an explanation of our results.</p><p>To quantify these effects independent of the modeling analysis, we entered subjects' mean RTs for each block into a repeated-measures ANOVA, with independent variables for the set size and for the S-R mapping entropy given an idealized asymptotic action policy (as described above).</p><p>We observed robust main effects of set size (F(1,39) = 1124.00, p &lt; 0.001), mapping entropy (F(1,39) = 228.10, p &lt; 0.001), and a significant (negative) interaction (F(1,39) = 13.97, p &lt; 0.001).</p><p>Critically, these findings could not be explained by differences in the proportion of correct/incorrect trials between mappings: First, a similarly robust main effect of mapping entropy on RT was observed when this analysis was restricted to correct trials (F(1,39) = 352.80, p &lt; 0.001).</p><p>Moreover, when the above ANOVA was performed with choice performance (i.e., probability correct) as the dependent variable instead of RT, we unsurprisingly observed a significant (negative) main effect of set size (F(1,39) = 98.45, p &lt; 0.001), but we did not observe significant effects of mapping entropy (F(1,39) = 2.32, p = 0.14) nor any interaction (F(1,39) = 0.00, p = 0.98).</p><p>Linking back to our observations in <ref type="figure" target="#fig_3">Figure 4</ref>, the findings in <ref type="figure">Figure 5</ref> may partly explain the model's ability to capture overall set size effects on RT: The key role of uncertainty echoes classic interpretations of Hick's Law that point to uncertainty over the probability of the stimulus as the main determinant of RT <ref type="bibr" target="#b28">(Hyman, 1953)</ref>; here, this idea is extended to uncertainty over internal representations of action values learned via reinforcement, as stimulus appearance probability was identical within set sizes. <ref type="figure" target="#fig_4">Figure 6A</ref> and B show full distributions of pooled subject RT data (bars) and the distribution of pooled simulation data (black lines), for, respectively, correct and incorrect trials, collapsed over set sizes. The model's ability to capture RT distributions across set sizes is further illustrated by comparing the simulated and observed RT data quantiles within each set size ( <ref type="figure" target="#fig_4">Figure 6C)</ref>.</p><p>We illustrate the model's ability to fit the data at the level of individual subjects in <ref type="figure" target="#fig_5">Figure 7</ref>. The model appeared to perform well at the level of fitting individuals, shown in the fit to five example subjects' RT distributions, RT time courses, and choice learning curves <ref type="figure" target="#fig_5">(Figure 7</ref>; ordered from top to bottom by membership in choice performance quantiles computed on the group).</p><p>We hypothesized that due to working memory limitations, evidence accumulation speed should decrease as a function of the number of intervening trials between successive presentations of a given stimulus, and thus RT would increase. The effect of these delays on average RT is shown in <ref type="figure" target="#fig_6">Figure 8A</ref> for subjects (purple triangles) and model simulations (black triangles), collapsed across set sizes. As predicted, the model approximated the effects of trial delay on RT. It follows from the delay effects that repeated presentations of a stimulus should produce relatively fast RTs. Repetition RT effects of this nature have been widely documented <ref type="bibr" target="#b2">(Bertelson, 1965;</ref><ref type="bibr" target="#b7">Campbell &amp; Proctor, 1993;</ref><ref type="bibr" target="#b22">Hale, 1969;</ref><ref type="bibr" target="#b44">Proctor &amp; Schneider, 2018)</ref>. In this analysis, we examined subjects' RTs when they responded to the same stimulus two trials in a row. Consistent with our predictions, the model replicated the effect of repetition on RT ( <ref type="figure" target="#fig_6">Figure 8B</ref>).</p><p>One widely documented amendment to Hick's Law is the effect of practice <ref type="bibr" target="#b15">(Davis et al., 1961;</ref><ref type="bibr" target="#b36">Mowbray &amp; Rhoades, 1959;</ref><ref type="bibr" target="#b44">Proctor &amp; Schneider, 2018)</ref>. That is, if a learner is thought to have proceduralized stimulus-response contingencies, Hick's effect should be attenuated or even abolished. This can be quantified as a decrease in the slope of a linear function, where the xvariable is defined as &gt; ( ) and the y-variable is the average RT in each set size. The attenuation of this slope should occur on long learning time scales, especially in higher set sizes like those used in classic studies (e.g., <ref type="bibr" target="#b25">Hick, 1952)</ref>. Given the relatively brief blocks in our task, we thus chose to analyze the late phase of learning for this analysis (iterations 7, 8, and 9).</p><p>We predicted that in this later phase of learning, where working memory retrieval processes presumably become less important, the slope of the log-linear set size effect would decrease because the reinforcement learning system has begun to cache a stimulus-response map <ref type="bibr" target="#b33">(McDougle &amp; Taylor, 2019)</ref>. Consistent with previous work, the log-linear slope of the set size effect significantly decreased over time (t-test on regression coefficients of slope change: t(39) = 2.29, p = 0.03; <ref type="figure" target="#fig_6">Figure 8C</ref>). The model produced a qualitatively similar decrease in the set size effect (black triangles), reflecting the effect of practice on crystalizing action policies and attenuating set size effects.</p><p>As shown in <ref type="figure">Figure 1C</ref>, trial-based delay has a marked effect on choice performance, especially in higher set sizes. Here, to capture interactions between delay and learning, we operationalized delay as the previous time a given stimulus was responded to (A. G. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>, and separated choice data into an early learning phase (iteration &lt; 5) and a late learning phase (iteration ³ 5). As learning progressed, the effect of delay was attenuated (t-test on regression coefficients of delay effect slope change from early to late learning: t(39) = 3.35, p = 0.002). This attenuation is potentially due to a gradual trade-off between working memory and RL systems.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 8D</ref>, the model also approximates this process. Taken together, the results of our model simulations suggest that the A model provides a parsimonious account of learning and decision-making processes in our task, accounting for a variety of choice and reaction time phenomena. One concern in any behavioral study is the replicability of the main behavioral trends. In our case, we have multiple data sets from previous studies using the same task in independent samples of subjects. We demonstrate the replicability of the task's average behavioral trends, and illustrate the model's ability to capture these trends, as follows ( <ref type="figure" target="#fig_3">Figure S4)</ref>: We took the average of the A model parameters derived from fitting the model to data set 1 (n = 40; A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref>, then we simulated the A model with those average parameter values on the block and stimulus sequences subjects experienced in data set 2 (n = 79; A. G. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012</ref>). The behavioral trends were similar in the two data sets, and the A model was able to capture RT time courses, set size effects on choice,</p><p>Hick's Law, and RT distributions in this separate group of subjects ( <ref type="figure" target="#fig_3">Figure S4</ref>). This result is expected -if the behavior is replicated across experiments, the model's ability to capture trends in that behavior should be replicated as well. More importantly, the simulated A model also fit better than the simulated model on these out-of-set data (average BIC difference: 22.21;</p><p>protected exceedance probability: 0.97), further favoring the former model over the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model performance in probabilistic learning</head><p>Having established that the A model can characterize various choice and RT effects in a simple deterministic instrumental learning task, we next wanted to test the model's ability to capture data in a probabilistic learning context. The vast majority of research on traditional choice RT effects use simple deterministic tasks, where stimulus-response associations are fixed and often explicitly explained to subjects <ref type="bibr" target="#b25">(Hick, 1952;</ref><ref type="bibr" target="#b28">Hyman, 1953;</ref><ref type="bibr" target="#b44">Proctor &amp; Schneider, 2018)</ref>, or perceptual discriminations have a ground truth correct answer <ref type="bibr" target="#b49">(Ratcliff &amp; McKoon, 2008)</ref>. On the other hand, RL tasks typically involve stochastic reward schedules <ref type="bibr" target="#b41">(Pedersen et al., 2017</ref>).</p><p>Because our model characterizes RT as a function of probabilistic action policies acquired via reinforcement learning and short-term memory maintenance, it should, in theory, generalize to situations where rewards are not perfectly reliable. In this context, in addition to set size effects, the reliability of stimulus-response associations should influence RT in a similar manner (i.e., by decreasing stimulus-action weights and generally increasing uncertainty). The goal of this experiment was to test the hypotheses that a) the concept of concurrent working memory and RL processes would generalize to a stochastic learning setting (i.e., the RLWM framework), and b) the A extension of the RLWM model would capture the effects of probabilistic feedback on both RT and choice.</p><p>In a new experiment ( <ref type="figure" target="#fig_7">Figure 9A</ref>) we modified our deterministic instrumental learning task by adding two reliability conditions: In the High-prob condition, the "correct" response to a stimulus was rewarded on 92% of trials, and an incorrect response was rewarded on 8% of trials. In the Low-Prob conditions, the correct response was rewarded on 77% of trials, and an incorrect response was rewarded on 23% of trials. In both cases, either of the two incorrect responses could produce a reward on the pre-designated low-probability trials. Two set sizes, 3 and 6, were used, creating a 2 X 2 design ( <ref type="figure" target="#fig_7">Figure 9A</ref>; see Methods for further details of the task).</p><p>We performed two separate repeated-measures ANOVAs to quantify the effects of set size and feedback reliability on reaction time and choice performance in the probabilistic context. In terms of RT ( <ref type="figure" target="#fig_7">Figure 9B, D)</ref>, we observed a significant main effect of set size (F(1,33) = 55.99, p &lt; 0.001),</p><p>but no significant effect of reliability (F(1,33) = 0.73, p = 0.79) nor any interaction (F(1,33) = 0.26, p = 0.61). In terms of choice ( <ref type="figure" target="#fig_7">Figure 9C</ref>, E), as predicted, we observed both significant set size (F(1,33) = 31.56, p &lt; 0.001) and reliability main effects (F(1,33) = 105.4, p &lt; 0.001), but a nonsignificant interaction (F(1,33) = 1.77, p = 0.19). The strong effect of set size on learning in the probabilistic context suggests that putative working memory recruitment in our task may not be contingent on there being deterministic stimulus-response associations.</p><p>To test the generalizability of the A model, we fit it to these new data. As predicted, the model was able to approximate the time courses of subjects' reaction times ( <ref type="figure" target="#fig_7">Figure 9B, dashed lines)</ref> and learning curves ( <ref type="figure" target="#fig_7">Figure 9C</ref>, dashed lines) in this probabilistic setting (fit parameter values are presented in <ref type="table">Table S1</ref>). In particular, the model was able to recapitulate the result where feedback reliability and set size have comparable effects on choice, but the effect of reliability on RT is much weaker than the effect of set size on RT ( <ref type="figure" target="#fig_7">Figure 9D</ref>). These results endorse the generalizability of our model, suggesting that the underlying action policy, if modeled accurately, can predict RT and choice dynamics across experimental contexts. (We note here that the mapping analysis shown in <ref type="figure">Figure 5</ref> could not be conducted on the probabilistic experiment, as only a single mapping was used within each set size.)</p><p>Relative to the deterministic experiment, we observed several significant changes in fit</p><p>A parameter values in the probabilistic experiment: First, as predicted, the learning bias to neglect negative feedback (as captured in the parameter) was significantly higher in the probabilistic experiment (Mann-Whitney U tests, comparing fitted values from the probabilistic versus the deterministic experiment, p &lt; 0.001). Moreover, the weight given to the working memory module ( ) was lower in the probabilistic context (p &lt; 0.001), while the reinforcement learning rate ( ) did not differ between conditions (p = 0.49). Interestingly, the capacity parameter (C) was higher in the probabilistic task (p = 0.003), as was the accumulation rate scaling factor ( ; p = 0.01). No other parameters differed significantly between experiments (all p's &gt; 0.24). In terms of the unexpected bi-directional differences between the key working memory parameters ( and C) across experiments, we note that these parameters can be difficult to independently estimate when there are only two set sizes (as in the probabilistic experiment).</p><p>However, using Equation 7, the actual weight given to the working memory (WM) module during learning can be directly computed using these two free parameters. As shown in <ref type="figure">Figure S5</ref>, we found significantly greater WM weighting in the deterministic experiment (data set 1) versus the probabilistic experiment in both set size 3 (2-sample t-tests; t(72) = 6.55, p &lt; 0.001) and set size 6 (t(72) = 4.13, p &lt; 0.001). Moreover, the decrease in WM weighting from set size 3 to set size 6 was larger in the deterministic versus probabilistic experiment (t(72) = 6.88, p &lt; 0.001), also indicating less reliance on WM in the probabilistic experiment. The results presented here provides further support to the hypothesis that working memory and RL act in concert during instrumental learning (A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. Our model expands this idea into the more mechanistic framework of evidence accumulation. Evidence accumulation models have provided many insights in the domain of perceptual decision making tasks <ref type="bibr" target="#b49">(Ratcliff &amp; McKoon, 2008)</ref>, and recent efforts have extended this class of models to instrumental learning <ref type="bibr" target="#b19">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b20">Frank et al., 2015;</ref><ref type="bibr" target="#b34">Miletić et al., 2020;</ref><ref type="bibr" target="#b41">Pedersen et al., 2017)</ref>. This is an important development, as RL models generally characterize choice policies using simple functions like the softmax or rigid "greedy" policies. However, these characterizations of the choice process are clearly oversimplifications, and do not make predictions about RT. Our model suggests that the concurrent operation of working memory and RL, as well as an internal representation of action uncertainty, shape RT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Simultaneously modeling choice and RT can provide practical benefits that modeling each in isolation cannot. For instance, recent work shows that incorporating RT data during model fitting improves the estimation of RL model parameters <ref type="bibr" target="#b1">(Ballard &amp; McClure, 2019)</ref>. We replicated this result, demonstrating that our combined choice/RT model led to improved recovery of both the RL and working memory parameters ( <ref type="figure" target="#fig_1">Figure S2</ref>). In general, rich RT data, while often neglected in reinforcement learning tasks, can be leveraged to better understand the underlying cognitive and neural processes driving decision making. Furthermore, when attempting to characterize RL model parameters in the clinical setting, as in the burgeoning field of computational psychiatry <ref type="bibr" target="#b26">(Huys et al., 2016)</ref>, achieving more reliable parameter estimates could improve the replicability of between-group comparisons and clinical interpretations.</p><p>One critical component of our model is prior action uncertainty (Equation 13). This value represents the effect that uncertainty over the full action policy space (i.e., over all states) has on an agent's reaction time. Incorporating this value in our model was critical for accurately capturing the effect of different stimulus-response mappings on RT ( <ref type="figure">Figure 5</ref>), appeared to aid in the modeling of set size effects <ref type="figure" target="#fig_3">(Figure 4</ref>), and also lead to an improved fit to the data when compared to models omitting this quantity <ref type="figure" target="#fig_1">(Figure 2)</ref>. Normatively, the average action policy <ref type="formula">Equation 12</ref>used in our uncertainty heuristic could be interpreted as the Bayes-optimal policy going into a trial, before the state/stimulus is observed. Even though our states/stimuli have virtually no observation noise (as they were all saliently different colors, objects, shapes etc.; see Methods), this prior policy still appeared to exert an influence on RT.</p><p>At the process-level, one interpretation of the uncertainty heuristic we used in the A model is that it approximates a form of competition between actions <ref type="bibr" target="#b67">(Usher &amp; McClelland, 2001</ref>). In our model, this conflict is implemented by decreasing all drift rates based on the degree of action uncertainty (Equation 14). One speculative process-level extension of this is that uncertaintyrelated RT effects in our task are the result of proactive, versus reactive, cognitive control processes being recruited before each trial <ref type="bibr" target="#b3">(Braver, 2012)</ref>. At the neural level, this could perhaps be implemented by top-down parallel preparation of actions.</p><p>The results depicted in <ref type="figure" target="#fig_3">Figure 4</ref> also speak to the psychological processes underlying Hick's Law <ref type="bibr" target="#b44">(Proctor &amp; Schneider, 2018)</ref>. One common explanation is that the law represents the amount of time it takes subjects to extract the (Shannon) information related to a stimulus. For instance, RT is affected by the probability that a given stimulus will be presented within a specific trial sequence <ref type="bibr" target="#b28">(Hyman, 1953</ref>). If we assume a uniform distribution of stimulus presentations, as used in our task, these stimulus probabilities will decrease with set size because the probability that a stimulus appears is 1/n_S, with n_S reflecting the number of stimuli that could be seen in a block.</p><p>Leveraging this simple fact could explain both the basic set size effect, as well as the effect of delays and repetitions on choice RT <ref type="bibr" target="#b28">(Hyman, 1953)</ref>. However, this particular explanation of Hick's Law does not address the learning of S-R associations, nor uncertainty over actions versus stimuli (exemplified in, respectively, the numerator and denominator of the values depicted in <ref type="figure" target="#fig_3">Figure 4B</ref>).</p><p>Our findings, particularly on within-set-size S-R mapping effects ( <ref type="figure">Figure 5</ref>), suggest that a more generalized account of Hick's Law should incorporate both trial-by-trial learning dynamics and, critically, uncertainty over learned action weights <ref type="bibr" target="#b69">(Wifall et al., 2016)</ref>. We note that a learningbased approach to explaining Hick's Law has been taken before -Schneider and Anderson <ref type="bibr" target="#b55">(Schneider &amp; Anderson, 2011)</ref> proposed an elegant model within the ACT-R framework to capture set size RT effects. They were able to show that Hick's Law, and the impact of practice and repetition on RTs, could be linked to the effects of load, time, and forgetting in short term memory.</p><p>Another important detail in our model is the proposed nonlinear relationship between latent action values and the rate of evidence accumulation, operationalized by a sigmoidal transfer function (Equation 3) that transforms those values into weights, which are then used to set accumulation rates. This step is inspired by the RL actor-critic framework, which suggests that while ventral striatum (the critic) tracks state values and enables prediction error computations, the dorsal striatum (the actor) instead tracks stimulus-action weights <ref type="bibr" target="#b29">(Joel et al., 2002;</ref><ref type="bibr" target="#b39">O'Doherty et al., 2004;</ref><ref type="bibr" target="#b65">Sutton &amp; Barto, 1998)</ref>. Consistent with our model, some theories support a nonlinear relationship between values represented in the critic system and striatal state-action weights represented in the "actor" system (A. G. E. . This nonlinearity can be captured by a softmax transfer function such as that used in our model, which, importantly, improved the model fit (see Fontanesi et al., 2019 for a similar conclusion). Moreover, the use of a softmax function could be interpreted as a simplified implementation of lateral inhibition between competing actions <ref type="bibr" target="#b67">(Usher &amp; McClelland, 2001</ref>).</p><p>The ability of the A model to also capture probabilistic stimulus-response learning <ref type="figure" target="#fig_7">(Figure 9</ref>) has several implications. First, this result shows that the model is flexible enough to capture learning in different task contexts. Second, our results show that the underlying hypothesis of concurrent working memory and RL contributions to instrumental learning, which has to date only been tested via tasks with deterministic feedback (A. G. E. <ref type="bibr" target="#b9">Collins et al., , 2017</ref><ref type="bibr" target="#b12">A. G. E. Collins &amp; Frank, 2018;</ref><ref type="bibr">A. G. E. Collins &amp; Frank, 2012)</ref>, may generalize to a probabilistic setting. However, we note that do not have direct evidence that working memory strategies are leveraged in the probabilistic task.</p><p>Interestingly, the cross-experiment comparisons supported our expectation that working memory is given a lower weight in the probabilistic context ( <ref type="figure">Figure S5</ref>), suggesting that if working memory strategies are leveraged here, they may have a weaker influence on decisions. Further research could attempt to better model working memory processes in these probabilistic settings by going beyond the simplified one-trial-back algorithm presented here (Equation 4). One approach could be to model learned associations held in working memory as probabilistic hypotheses rather than deterministic stimulus-response associations. Lastly, we observed no significant change in the reinforcement learning rate ( ) between deterministic and probabilistic contexts. This suggests that the slower learning observed in the probabilistic task may primarily be a consequence of noisier explicit working memory strategies, though this should be more fully explored in future research.</p><p>We note several limitations in this study. First, our model was clearly ineffective at capturing RTs in the earliest iterations of each block ( <ref type="figure" target="#fig_2">Figure 3A</ref>). This was partly expected, as action values are initialized to the same number in all set sizes (1/3); thus, both the action weights and prior uncertainty over actions were identical in the first trial of every block, leading to similar RTs across blocks. Why, then, did we observe a set size effect in subjects' RTs at the start of the block? First, subjects are likely guessing on most of these early trials <ref type="bibr" target="#b54">(Schaaf et al., 2019)</ref>, and this kind of undirected exploration <ref type="bibr" target="#b71">(Wilson et al., 2014)</ref> is not explicitly specified in our models. We also speculate that some subjects may covertly name or label stimuli early in the block, especially in the higher set sizes, and associate those labels with their guesses -strategies like this could appear as early as the first iteration because subjects are informed about the upcoming set size before each block begins and shown a preview of the full set of images. In some situations, subjects could even perform deterministic hypothesis-testing strategies in the early phases of a learning block (e.g., trying each finger from left to right; <ref type="bibr" target="#b35">Mohr et al., 2018)</ref>. Despite these caveats and alternative learning strategies, our model was still able to closely approximate the time course of subjects' reaction times and choices <ref type="figure" target="#fig_2">(Figures 3, 7, and 9)</ref>. Second, our model also appeared to underestimate variance in RT distributions, particularly for incorrect trials ( <ref type="figure" target="#fig_4">Figure 6B</ref>), and to underestimate the RT set size effect ( <ref type="figure" target="#fig_4">Figure 6C</ref>). The reason for this is not clear, although we note that there were clear variations in fit quality on the individual level ( <ref type="figure" target="#fig_5">Figure 7)</ref>. Moreover, the model tended to overestimate RTs in the set size 1 condition. This was partly expected, as a true decision process would not be needed once subjects learned the correct action in the set size 1 condition (rather, they simply needed to detect the appearance of the stimulus). These fitting limitations may relate to model misspecification as discussed above;</p><p>subjects likely leverage additional learning and hypothesis-testing strategies (e.g., systematic guesses, pre-planning responses) that are simply not specified in our modeling approach.</p><p>Another limitation of our modeling effort is the requirement to fix certain parameters, namely, nondecision time t0, the noise parameter s_v, and the softmax sensitivity parameter . Although fixing versus fitting these parameters did not alter the main conclusions of our study, we made decisions to fix these parameters for several reasons: First, as shown in <ref type="figure" target="#fig_2">Figure S3</ref>, fitting t0 provided an expected increase in fit quality at the expense of interpretability and model recoverability. If and how different experimental conditions may influence non-decision time in our task is an issue for future model development. In terms of the noise parameter s_v, previous studies have shown that fixing this parameter is important for LBA model identifiability <ref type="bibr" target="#b17">(Donkin et al., 2009)</ref>, a finding we replicated in our own control analyses (not shown). Lastly, fixing at a relatively high value is important for identifiability and recovery of the RLWM choice model (A. G. E. <ref type="bibr" target="#b10">Collins, 2018)</ref>.</p><p>Evidence accumulation has been directly linked to specific neural dynamics underlying decision making. Most prominently, this has been demonstrated in activity profiles of neural populations that may reflect the accumulation of perceptual evidence <ref type="bibr" target="#b59">(Shadlen &amp; Newsome, 1996</ref><ref type="bibr">, see also Latimer et al., 2015</ref>. Recent evidence also suggests that neurons in the striatum, a key substrate in reinforcement learning and decision making, perform evidence accumulation during decisionmaking <ref type="bibr" target="#b72">(Yartsev et al., 2018)</ref>. Future human physiological studies, perhaps using techniques with high temporal resolution (e.g., intracranial electroencephalography), could attempt to measure putative neural accumulation processes at play during instrumental learning and action selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Here we presented a model of choice and RT that captures decision-making behaviors in two stimulus-response learning tasks. The model was able to capture a variety of RT and choice phenomena, including set size, repetition, delay, and practice effects, and was effectively validated with multiple methods. Modeling RT and choice together improved the estimation of underlying reinforcement learning parameters and incorporating internal estimates of action uncertainty in the model markedly improved model fit and validation. Lastly, the model was able to characterize choice and RT in both deterministic and probabilistic feedback contexts. Our results suggest that modeling choice and RT together can provide a more nuanced account of instrumental learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral task</head><p>The protocol for all behavioral tasks was approved by the institutional review board at Brown</p><p>University. Details of subject samples for data set 1 and the out-of-sample data set (data set 2, <ref type="figure" target="#fig_3">Figure S4</ref>) can be found in the original source papers (respectively, A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018;</ref><ref type="bibr" target="#b14">A. G. Collins &amp; Frank, 2012)</ref>. Forty-one subjects were recruited for the probabilistic task (data set 3; mean age = 21, 23 females). Given the increased difficulty of the probabilistic task, seven subjects were excluded for having average choice performance that was at or below the chance level (0.33) for selection of the optimal action (i.e., the one most likely to be rewarded for a given stimulus), leaving a sample of 34 (mean age = 20.97, 20 females) for the model fitting analysis of the probabilistic task.</p><p>The basic structure of the task is depicted in <ref type="figure">Figure 1A</ref>. The task was administered as follows:</p><p>Subjects were seated in front of a computer monitor and made responses on an external USB computer keyboard. Subjects were instructed by the experimenter to learn which of three responses was associated with each presented image, in order to maximize earned rewards. On correct trials, positive feedback ("+1") was displayed centrally in green font; on incorrect trials, negative feedback ("0") was displayed centrally in red font. On trials where subjects responded too slowly, a "Too Slow" warning appeared in red font on the center of the screen. Across experiments and analyses, trials where responses were too slow (1.33%) or overly rapid (&lt;150 ms, 0.75%) were excluded.</p><p>Each experiment was divided into several blocks of trials, and each block was associated with a particular set of images and a particular set size, defined by the number of stimulus-response pairs to be learned in that block. The number of actions was held constant across all blocks at 3.</p><p>Key press responses were made with the dominant hand, and required pressing one of three adjacent keys (e.g. j, k, or l) with the index, middle, or ring finger, respectively. To discourage process-of-elimination strategies, in set sizes over 1 the correct actions were not always evenly distributed among the stimuli (e.g., in some set size three blocks, each action could be correct for exactly one stimulus, while in other blocks, one action could be correct for two of the stimuli, one could be correct for the third stimulus, and the third action could be correct for no stimuli). Twelve such mappings were used overall and are each depicted in <ref type="figure">Figure 5</ref>.</p><p>Before each block, all of the images to be learned about in that block were centrally displayed in a tiled layout on the screen (e.g., all 3 images if set size = 3) for subjects to familiarize themselves with the stimuli before the block began. On each trial, one image was displayed at a time in the center of the computer screen over a black background (visual angle of stimulus, ~8˚). Subjects had a maximum of 1400 ms to respond to the image. For data sets 1 (N = 40; A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref> and 2 (N = 79; A. G. E. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>, the correct stimulus-response contingencies were consistent throughout the block. That is, the correct action for a given stimulus would always yield a reward, and both of the two incorrect actions for that stimulus would never yield a reward. Within a block, each stimulus was presented a minimum of 9 times and a maximum of 15 times (data sets 1 and 2); the block ended either after n_S × 15 trials, or when subjects reached a performance criterion whereby they had selected the correct action for three of the four last iterations. The specific sequence of stimuli within a block was pseudorandomized. Stimuli within a given block were drawn from a single category (e.g. scenes, fruits, animals), and stimuli never repeated across blocks. In data set 1, 22 blocks were completed (set sizes 1-6); in data set 2 ( <ref type="figure" target="#fig_3">Figure S4</ref>), 18 blocks were completed (set sizes 2-6).</p><p>The probabilistic experiment (N = 34; data set 3) had a similar design to the deterministic experiments, but with two key differences: First, only two set sizes were used (set sizes 3 and 6) with only one S-R mapping per set size ([0 1 2] and [2 2 2]). Second, two different feedback "reliability" conditions were introduced: In the High-prob condition, the "correct" response to a stimulus was rewarded 92% of the time, and an incorrect response (either of the other two actions) was rewarded 8% of the time. In the Low-Prob conditions, the correct response was rewarded 77% of the time, and an incorrect response was rewarded 23% of the time. That is, subjects still had to learn which action was the most rewarded for each stimulus, but the solution was not deterministic. The specific trials in which unreliable reward feedback was given were predetermined. Exactly twelve iterations of each stimulus were presented per block, and 14 blocks were completed in total. &gt; A. The s_v parameter was fixed at 0.1; fixing this parameter has been shown to significantly improve LBA model identifiability <ref type="bibr" target="#b23">(Heathcote et al., 2019)</ref>. (We note that our model comparison results were similar when allowing s_v to freely vary, but identifiability was strongly weakened for the other LBA parameters.) Inverse temperature was fixed at 50 for all fits and simulations, consistent with previous studies (A. G. E. <ref type="bibr" target="#b12">Collins &amp; Frank, 2018)</ref>. The non-decision time parameter t0 was subtracted from RT data before fitting and was fixed at 150 ms (data set 1 and the probabilistic experiment; <ref type="table">Table S1</ref>) or 225 ms (data set 2; <ref type="figure" target="#fig_3">Figure S4</ref>). All Q and W values were initialized at 1/3 for all fitting iterations and simulations.</p><p>Model comparisons were conducted using two methods: First, we fit models on subjects' full data sets and compared them using the Bayesian Information Criterion <ref type="bibr" target="#b56">(Schwarz, 1978)</ref>, plotting mean BIC differences with standard errors <ref type="figure" target="#fig_1">(Figure 2</ref>), reporting the mean BIC differences of the best model versus its competitors, and computing the protected exceedance probability <ref type="bibr" target="#b63">(Stephan et al., 2009</ref>) of the winning model (using the MATLAB spm_BMS function from the SPM toolbox;</p><p>https://www.fil.ion.ucl.ac.uk/spm). Second, we also computed a cross-validated likelihood measure: This value was determined by a leave-p-out cross-validation procedure, where models were fit to each subject on a reduced data set that left out the last block of each set size, after which a log-likelihood was computed on the six left-out blocks using the parameters gleaned from the fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model simulation</head><p>For simulations, the mean accumulation rate (v) is computed for each action i (Equations 11-14), which is shaped by the modeled learning of stimulus-response associations (Equations 1-8). The where the chosen action a corresponds to the accumulator that generates the minimum RT across the three accumulators (i.e., the winning accumulator). Simulated accumulators with negative drift rates, which are possible given that drift rates are normally distributed, were disqualified from reflecting the winning action, and simulated trials that produced an RT that exceeded the experimentally enforced maximum RT (1400 ms) were re-run until that constraint was satisfied.</p><p>Model simulations were performed 100 times per simulated subject then averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter recovery and model separability</head><p>We performed a model separability analysis by computing a model confusion matrix as follows:</p><p>Choices and RTs were simulated 40 times for each of the four models using the best-fit parameters gleaned from fitting each model to each of the 40 individual subjects. Each model was then fit to each of the four sets of simulations (using 40 starting points of randomized parameter initializations per fit and selecting the best result), in an attempt to recover the underlying model that produced the simulated data. In each case, the optimal outcome is for the winning model in the fitting procedure to match the model that was originally used to simulate the underlying synthetic data <ref type="bibr" target="#b70">(Wilson &amp; Collins, 2019)</ref>. After fitting, we plotted a confusion matrix using the proportion of simulations best fit by each model ( <ref type="figure">Figure S1</ref>). We performed this analysis using both the AIC <ref type="bibr" target="#b0">(Akaike, 1974)</ref> and BIC metrics and found that the BIC-based confusion matrix resulted in better model separability. Therefore, we used the BIC for our model comparisons, though all main findings were similar with the AIC.</p><p>We also performed a parameter recovery experiment to measure model identifiability ( <ref type="figure" target="#fig_1">Figure S2</ref>).</p><p>In this analysis, after fitting each model, we simulated data using either the A model (which simulates choices and RTs) or the basic RLWM model of (choices only; A. G. <ref type="bibr" target="#b14">Collins &amp; Frank, 2012)</ref>. We then attempted to recover the free parameters by fitting the resulting choice/RT data (again using 40 starting points of randomized parameter initializations and selecting the best fit).</p><p>Resulting Spearman correlations were computed and compared across models using the Fisher r-to-z transformation. Parameters were depicted in scatter plots to visualize recovery success ( <ref type="figure" target="#fig_1">Figures S2 and S3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information Legends</head><p>Supporting Information for this manuscript contains five figures <ref type="figure">(Figures S1-S5</ref>) and one table <ref type="table">(Table S1</ref>). <ref type="figure">Figure S1</ref>. Model Confusion Matrix. Each of the four tested models were used to simulate a synthetic data set (RTs and choices) and were then fit to each set of simulations in an attempt to recover the underlying model (see Methods for details). The heat map depicts the proportion of simulations best fit by each model (according to the BIC). <ref type="figure" target="#fig_1">Figure S2</ref>. Parameter recovery. (A) RT and choice data from the A model were simulated to generate 40 synthetic data sets, using the best fit parameters from the 40 subjects in data set 1 <ref type="bibr" target="#b12">(Collins &amp; Frank, 2018)</ref>. These data were then fit with the A model in an attempt to recover the ground-truth parameter values. Parameters include the reinforcement learning rate , the working memory decay rate , the initial working memory weight , negative prediction error learning modulation , working memory capacity C, evidence accumulation rate scaling factor , the upper evidence Bound, and the upper initial bias boundary A. Correlation coefficients (Spearman) are specified in the title of each subplot. (B) Same as (A), but for recovery of the RLWM parameters.</p><p>Here, the RLWM choice model <ref type="bibr" target="#b14">(Collins &amp; Frank, 2012)</ref> was fit to subjects' choices, then choices were simulated using those parameters, and, lastly, recovery was attempted. We note that the RLWM model does not model RTs. Correlations between simulated and fit parameters were compared across models with the Fisher r-to-z transform. Recovery of the A model was significantly better than the RLWM model for the RL learning rate (p = 0.006), working memory decay (p = 0.02), working memory weight (p &lt; 0.001), and working memory capacity C (p &lt; 0.001). No significant difference in recovery was found between the models for negative prediction error learning rate scaling factor (p = 0.56). Allowing for a freely varying t0 parameter improved model fit. (B) RT and choice data from the A model were simulated and fit as described above ( <ref type="figure" target="#fig_1">Figure S2</ref>), but with the non-decision time parameter t0 set as a freely varying parameter rather than fixed at 150 ms. Model recovery was generally successful with a freely varying t0, however, recovery was attenuated for 7/8 parameters relative to the original A model. <ref type="figure" target="#fig_3">Figure S4</ref>. Performance of the model on out-of-sample data. (A) The A model was fit on data from data set 1 <ref type="bibr" target="#b12">(Collins &amp; Frank, 2018)</ref>. Then, average fit parameter values were used to simulate data from data set 2 <ref type="bibr" target="#b14">(Collins &amp; Frank, 2012)</ref>. Here we depict model and data RT (A) and choice (B) learning curves, Hick's Law (C), and RT distribution quantiles (D). Error shading and error bars = 95% CIs. <ref type="figure">Figure S5</ref>. Differential weighting of the working memory (WM) module between deterministic (data set 1) and probabilistic learning contexts. The A model determines the weighting of the WM learning module using the and C parameters. After fitting the model, the weighting of WM can be directly computed using the equation depicted on the x-axes above (Equation 7 in main text).</p><p>Histograms of WM weightings are shown for each experiment, at set size 3 (left) and set size 6 (right). Between-subject comparisons are restricted to set sizes 3 and 6 as these were the set size conditions used in the probabilistic experiment. WM weighting was significantly higher in the deterministic experiment (data set 1) in both set size 3 (t(72) = 6.55, p &lt; 0.001) and set size 6 blocks (t(72) = 4.13, p &lt; 0.001). Moreover, the decrease in WM weighting from set size 3 to set size 6 was larger in the deterministic versus probabilistic context (t(72) = 6.88, p &lt; 0.001). <ref type="table">Table S1</ref>. Parameter values. Mean fit parameter values, +/-standard deviation of the mean. We note that the s_v and t0 parameters were fixed for all models. The fifth row ( A PROB) refers to the fit parameters of the A model for the probabilistic feedback experiment <ref type="figure" target="#fig_6">(Figure 8</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2E, F): Models were fit to individual subject's RT data, leaving out the last block from each set size as a test set. The π A model outperformed the alternatives in this analysis as well (paired t-tests on cross-validated loglikelihoods, all p's &lt; 0.001). We additionally performed a simulation and fitting procedure, using the best-fit parameters, to test how well differentiated the four models were from one another (see Methods;<ref type="bibr" target="#b70">Wilson &amp; Collins, 2019)</ref>. As suggested by the confusion matrix inFigure S1, the four models were reliably separable.The specific implications of our model comparisons (Figure 2C-F) are as follows: First, the A -RL model did not perform as well as any of the other models. This echoes previous work showing that modeling parallel WM and RL systems better describes behavior in this task versus modeling a single RL system alone (A. G. E.<ref type="bibr" target="#b9">Collins et al., , 2017</ref><ref type="bibr" target="#b12">A. G. E. Collins &amp; Frank, 2018;</ref> A.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Model overview and comparisons. (A) Schematic diagram of the RLWM model of choice. A working memory (WM) module deterministically learns stimulus-response associations, with trial-based forgetting. A reinforcement learning (RL) module learns stimulus-action associations with standard reward prediction error based RL. WM and RL are differentially weighted to produce an action policy. (B) Schematic diagram of the Linear Ballistic Accumulator<ref type="bibr" target="#b4">(Brown &amp; Heathcote, 2008)</ref>, where responses compete to produce a choice and a reaction time. (C-F) Model comparisons, showing (C) mean BIC differences relative to the winning model, (D) BIC differences between the best and second-best model for each individual, and (E, F) a leave-1-block out validation comparison procedure. Sorting of individuals in (F) matches the ordering in (D). LL = cross-validated log-likelihood. Error bars = 95% CIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>RT and choice learning curves with model simulations. Mean RT time courses (A) and choice performance (B) for each set size, showing subject data (solid lines) and model simulated data (dashed lines). Error shading = 95% CIs. Data from<ref type="bibr" target="#b12">Collins &amp; Frank, 2018.</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Set size effects. (A) Average RTs across set sizes form subject data (filled triangles), the 1 model (unfilled triangles), and the model (unfilled circles). (B) Simulated policy of chosen action i divided by the prior uncertainty, as specified in Equation 13, across set sizes. Simulations are averaged across simulated subjects. Error bars = 95% CIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>RT distributions. Model and data RT distributions for correct (A) and incorrect (B) trials, collapsed over set sizes. (C) RT quantile data and model simulations over five cumulative probability bins. Error bars = 95% CIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Example individual model fits. Five subjects were randomly selected for display after sorting subjects into five quantiles based on average choice performance, with one subject selected from each bin (top to bottom ordering reflects increasing choice performance). Left column: full RT distributions. Center column: RT time courses. Right column: Choice learning time courses. Solid lines: data. Dashed lines: model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Delay, repetition, and learning effects. (A) Subject delay effects on RT (purple), and model simulated delay effects (black). (B) Subject repetition effects, and model repetition effects. (C) The effect of practice on the Hick's Law function over time. (D) Effects of trial delay on human and model choice performance, separated by early and late learning phases. Error bars = 95% CIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Probabilistic task. (A) In this task, subjects learn stimulus-response associations under varying degrees of reward reliability given the correct action, and under two set sizes (nS = 3 and nS = 6). Subject data and fitted model simulations, showing RT (B) and choice (C) learning curves, as well as average RT (D) and choice (E) performance across the set size and probability conditions. Error bars and shading = 95% CIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Choice and reaction time are tightly intertwined aspects of decision making. Here, using a novel evidence accumulation reinforcement learning (RL) model, we show that leveraging both choice and reaction time (RT) data can help shed light on a variety of behavioral phenomena, including effects of repetition, delay, and set size on RT, and the interaction of working memory and reinforcement during instrumental learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Model fitting and model comparison Models were fit to reaction time data using maximum likelihood estimation, specifically by minimizing the negative log likelihood using the MATLAB function fmincon. All RTs were specified in milliseconds. Initial parameter values were randomized across fitting iterations, and 40 iterations were used per fitting run to avoid local minima. Parameter constraints were defined as follows: = [0,1]; = [0,1]; = [0,1]; = [0,1]; C = [2,5]; = [0,3]; A = [0,500]; b = [0,600]; and b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>ith accumulator's starting point k is drawn from a uniform distribution on the interval [0, A] and the drift rate d is drawn from a normal distribution, N(v, s_v). The time to threshold and the simulated choice can then be directly computed, 15) &amp; = 34&lt; (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S3 .</head><label>S3</label><figDesc>Model comparison and parameter recovery for A model with a free t0 parameter. (A)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank William Ryan for help with data collection, the CCN lab at UC Berkeley for helpful comments on analyses and interpretations, and the reviewers for their thorough and helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The data and code for all experiments and models will be made freely available upon publication. None of the experiments described here were preregistered.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.1974.1100705</idno>
		<ptr target="https://doi.org/10.1109/TAC.1974.1100705" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint modeling of reaction times and choice improves parameter identifiability in reinforcement learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcclure</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2019.01.006</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2019.01.006" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Serial Choice Reaction-time as a Function of Response versus Signal-and-Response Repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bertelson</surname></persName>
		</author>
		<idno type="DOI">10.1038/206217a0</idno>
		<ptr target="https://doi.org/10.1038/206217a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">4980</biblScope>
			<biblScope unit="page" from="217" to="218" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The variable nature of cognitive control: A dual mechanisms framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Braver</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2011.12.010</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2011.12.010" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="113" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The simplest complete model of choice response time: Linear ballistic accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cogpsych.2007.12.002</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2007.12.002" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cognitive and Neural Bases of Multi-Attribute, Multi-Alternative, Value-based Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.12.003</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2018.12.003" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Repetition Effects With Categorizable Stimulus and Response Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Proctor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1345" to="1362" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Working memory contributions to reinforcement learning impairments in schizophrenia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Waltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="13747" to="13756" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Working memory load strengthens reward prediction errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Badre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4332" to="4342" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Tortoise and the Hare: Interactions between Reinforcement Learning and Working Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01238</idno>
		<ptr target="https://doi.org/10.1162/jocn_a_01238" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1422" to="1432" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opponent actor learning (OpAL): Modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0037015</idno>
		<ptr target="https://doi.org/10.1037/a0037015" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="366" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Within-and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2502" to="2507" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1720963115</idno>
		<ptr target="https://doi.org/10.1073/pnas.1720963115" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1024" to="1035" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imitative responses and the rate of gain of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/17470216108416477</idno>
		<ptr target="https://doi.org/10.1080/17470216108416477" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The overconstraint of response time models: Rethinking the scaling problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1135" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/PBR.16.6.1129</idno>
		<ptr target="https://doi.org/10.3758/PBR.16.6.1129" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A reinforcement learning diffusion decision model for value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fontanesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Spektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-018-1554-2</idno>
		<ptr target="https://doi.org/10.3758/s13423-018-1554-2" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1099" to="1121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FMRI and EEG Predictors of Dynamic Decision Parameters during Human Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyhus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Wiecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Badre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="485" to="494" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.2036-14.2015</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2036-14.2015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Repetition and probability effects in a serial choice reaction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hale</surname></persName>
		</author>
		<idno type="DOI">10.1016/0001-6918(69)90011-0</idno>
		<ptr target="https://doi.org/10.1016/0001-6918(69)90011-0" />
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="163" to="171" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic models of choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Strickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="961" to="985" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/s13428-018-1067-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-018-1067-y" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the Rate of Gain of Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hick</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470215208416600</idno>
		<ptr target="https://doi.org/10.1080/17470215208416600" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computational psychiatry as a bridge from neuroscience to clinical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J M</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="413" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/nn.4238</idno>
		<ptr target="https://doi.org/10.1038/nn.4238" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stimulus information as a determinant of reaction time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hyman</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0056940</idno>
		<ptr target="https://doi.org/10.1037/h0056940" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="188" to="196" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Actor-critic models of the basal ganglia: New anatomical and computational perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S0893-6080(02</idno>
		<ptr target="https://doi.org/10.1016/S0893-6080(02" />
		<imprint>
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Latimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L R</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaa4056</idno>
		<ptr target="https://doi.org/10.1126/science.aaa4056" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6244</biblScope>
			<biblScope unit="page" from="184" to="187" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dissociating the contributions of reward-prediction errors to trial-level adaptation and long-term learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Lohse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Valerius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2019.107775</idno>
		<ptr target="https://doi.org/10.1016/j.biopsycho.2019.107775" />
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dissociable cognitive strategies for sensorimotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mcdougle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-07941-0</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-07941-0" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mutual benefits: Combining reinforcement learning with sequential sampling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miletić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2019.107261</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2019.107261" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">107261</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deterministic response strategies in a trial-and-error learning task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zwosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Markovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Wolfensteller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ruge</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1006621</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1006621" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the Reduction of Choice Reaction Times with Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Mowbray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rhoades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="23" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/17470215908416282</idno>
		<ptr target="https://doi.org/10.1080/17470215908416282" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An exemplar-based random walk model of speeded classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Palmeri</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.104.2.266</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.104.2.266" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="300" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dissociable Roles of Ventral and Dorsal Striatum in Instrumental Conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1094285</idno>
		<ptr target="https://doi.org/10.1126/science.1094285" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5669</biblScope>
			<biblScope unit="page" from="452" to="454" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Working memory retrieval as a decision process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raškevičius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Bays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pertzov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Husain</surname></persName>
		</author>
		<idno type="DOI">10.1167/14.2.2</idno>
		<ptr target="https://doi.org/10.1167/14.2.2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The drift diffusion model as the choice rule in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1251" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/s13423-016-1199-y</idno>
		<ptr target="https://doi.org/10.3758/s13423-016-1199-y" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decay of Visual Information from a Single Letter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Keele</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.158.3797.137</idno>
		<ptr target="https://doi.org/10.1126/science.158.3797.137" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">3797</biblScope>
			<biblScope unit="page" from="137" to="139" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hick&apos;s law for choice reaction time: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1281" to="1299" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/17470218.2017.1322622</idno>
		<ptr target="https://doi.org/10.1080/17470218.2017.1322622" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Repetition effects and signal classification strategies in serial choiceresponse tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M A</forename><surname>Rabbitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/14640746808400157</idno>
		<ptr target="https://doi.org/10.1080/14640746808400157" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<idno type="DOI">10.1162/neco.2008.12-06-420</idno>
		<ptr target="https://doi.org/10.1162/neco.2008.12-06-420" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling Response Times for Two-Choice Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9280.00067</idno>
		<ptr target="https://doi.org/10.1111/1467-9280.00067" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Analysis of sequential effects on choice reaction times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Remington</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0028122</idno>
		<ptr target="http://dx.doi.org/10.1037/h0028122" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="250" to="257" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Theory of Pavlovian Conditioning: Variations in the Effectiveness of Reinforcement and Nonreinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classical conditioning II: current research and theory</title>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="64" to="99" />
		</imprint>
	</monogr>
	<note type="report_type">Appleton-Century-Crofts</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian approach to assess learning and guessing strategies in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jepma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Huizenga</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2019.102276</idno>
		<ptr target="https://doi.org/10.1016/j.jmp.2019.102276" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Memory-Based Model of Hick&apos;s Law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2010.11.001</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2010.11.001" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating the Dimension of a Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
		<ptr target="https://doi.org/10.1214/aos/1176344136" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Combining error-driven models of associative learning with evidence accumulation models of decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Jach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Van Heer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<idno type="DOI">10.3758/s13423-019-01570-4</idno>
		<ptr target="https://doi.org/10.3758/s13423-019-01570-4" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Motion perception: Seeing and deciding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Newsome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="628" to="633" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.93.2.628</idno>
		<ptr target="https://doi.org/10.1073/pnas.93.2.628" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving the reliability of model-based decision-making estimates in the twostage decision task with reaction-times and drift-diffusion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moutoussis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keramati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Consortium</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1006803</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1006803" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
		<ptr target="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x" />
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bayesian model selection for group studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1004" to="1017" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.neuroimage.2009.03.025</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2009.03.025" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement learning: An introduction</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimal policy for multi-alternative decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-019-0453-9</idno>
		<ptr target="https://doi.org/10.1038/s41593-019-0453-9" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1503" to="1511" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The time course of perceptual choice: The leaky, competing accumulator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="592" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/0033-295X.108.3.550</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.108.3.550" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The roles of stimulus and response uncertainty in forced-choice performance: An amendment to Hick/Hyman Law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wifall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazeltine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mordkoff</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-015-0675-8</idno>
		<ptr target="https://doi.org/10.1007/s00426-015-0675-8" />
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="565" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Ten simple rules for the computational modeling of behavioral data. ELife, 8, e49547</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.49547</idno>
		<ptr target="https://doi.org/10.7554/eLife.49547" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Humans use directed and random exploration to solve the explore-exploit dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0038199</idno>
		<ptr target="https://doi.org/10.1037/a0038199" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2074" to="2081" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Causal contribution and dynamical encoding in the striatum during evidence accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Yartsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Brody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELife</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
