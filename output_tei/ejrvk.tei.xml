<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Multimodal can Predict Human Judgment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Multimodal can Predict Human Judgment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language models</term>
					<term>computer vision</term>
					<term>creativity</term>
					<term>innovation</term>
					<term>and pattern recognition</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This article delves into the field of multimodal, a relatively new yet profoundly skilled technological area, highlighting its exceptional ability to predicting marketing result with notable precision. The initial focus is on the impact of human judgment and decision-making in contrasting textual and visual data. Central to this discussion is the role of computer vision in forecasting marketing result. We examine how these systems, trained with extensive image datasets that capture human&apos;s perception of the image of advertisement and technological progress, are adept at identifying patterns, trends, and emerging characteristics that often escape human detection. Additionally, the article presents examples where computer vision could revolutionize various sectors, including design, engineering, and art. By processing and analyzing visual information on a scale never seen before, these systems provide unparalleled insights into the direction of human ingenuity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction:</head><p>The generative AI has grow rapidly, past research shows that large language model has passed Theory of mind test <ref type="bibr" target="#b12">(Trott, S., Jones, C., Chang, T., Michaelov, J., &amp; Bergen, B. 2023)</ref>, overcoming intuition <ref type="bibr" target="#b10">(Hagendorff, T., Fabi, S., &amp; Kosinski, M. 2023)</ref> and judging innovation funding <ref type="bibr">(Eapen, T. T., Finkenstadt, D. J., Folk, J., &amp; Venkataswamy, L., Du, M. 2023)</ref>. Recently, some large language models added a computer vision function to them, make it multimodal. This essay delves into the remarkable capabilities of multimodal systems in interpreting and analyzing images with an accuracy that often surpasses human visual judgment. Human perception, while sophisticated, is inherently subject to biases and inconsistencies, particularly in high-stakes environments such as criminal investigations <ref type="bibr" target="#b3">(Dror, I. E., &amp; Cole, S. A. 2010)</ref>, medical imaging <ref type="bibr" target="#b5">(Mullainathan, S., &amp; Obermeyer, Z. 2019)</ref>, and assessments of facial attractiveness <ref type="bibr">(Lee, Y., &amp; Jeong, S. K. 2023)</ref>. In contrast, computer vision offers a consistent, objective, and precise approach, leveraging vast datasets and advanced algorithms to identify patterns and insights that might elude the human eye. It can even identify political orientation, Political orientation was correctly classified in 72% of liberal-conservative face pairs, remarkably better than chance (50%) and human accuracy (55%) <ref type="bibr" target="#b20">(Kosinski, M. 2021)</ref>.</p><p>Personality can be the other reason people make noisy decision-making <ref type="bibr" target="#b4">(Kahneman, D., Sibony, O., &amp; Sunstein, C. R. 2021)</ref> or change their preferences <ref type="bibr">(Aleem, H., &amp; Grzywacz, N. M. 2023</ref><ref type="bibr" target="#b21">, Palumbo, L., Harrison, N. R., Trawiński, T., Kass, J., Metelmann, A. C., Bari, R. S. G., &amp; Donnelly, N. 2023</ref>, A handful of studies that have measured aesthetic preferences at multiple moments show that preferences may change in as little as two weeks. Personality such as openness to experience and need for cognitive closure, can be a predictor of aesthetic preferences.</p><p>Pattern recognition is not only a term used in computer vision, psychologists <ref type="bibr" target="#b8">(Tversky, A. &amp; Kahneman, D. 1983)</ref> found that people find patterns in words, such as the classic "Linda Problem", and GPT-4 can solve this now <ref type="bibr" target="#b2">(Du, M. 2023)</ref>.</p><p>"Linda Problem" is a classic probability question, language models acquire the ability to determine the likelihood of word sequences by analyzing the statistical patterns in which words are distributed within language.</p><p>Recently computer scientists come up with an idea of how to recognize patterns in language <ref type="bibr">(Vaswani, A., Shazeer, N., et.al.,2017)</ref>, by using "attention". Unlike human attention, computer seeks the pattern underlying language to better understand it, whereas humans use language to communicate, social Interact or convey the essence of art, and persuasion.</p><p>As we explore this topic, we will consider the inherent limitations of human visual judgment, characterized by its susceptibility to noise <ref type="bibr" target="#b4">(Kahneman, D., Sibony, O., &amp; Sunstein, C. R. 2021)</ref> and error. In criminal investigations, for instance, eyewitness accounts and suspect identifications can be notoriously unreliable, leading to wrongful convictions or overlooked leads.</p><p>Take radiologists, a visually oriented profession for another example, some common biases that impact clinical decisions in radiology include framing bias, satisfaction of search, and the influence of a colleague's opinion. Framing bias occurs when a radiologist considers a case only within the context of a specific clinical specialty, potentially leading to a narrow differential diagnosis. Satisfaction of search occurs when a radiologist stops searching for additional findings after identifying an initial abnormality. Finally, the influence of a colleague's opinion can lead to undue influence on a radiologist's medical decision-making. (Busby, L. P., Courtier, J. L., &amp; Glastonbury, C. M. 2018).</p><p>Relying solely on visual information can lead to biases and limitations in judgments of competence and performance. For example, people might use vision as a heuristic in judgments about competence in a way that is inconsistent with what they would deliberately choose and even contradicted by actual performance outcomes. Additionally, relying on demographic characteristics such as gender and ethnicity as visual cues can lead to stereotypes and discrimination. Therefore, it is important to consider multiple sources of information when making judgments about competence and performance. (Tolsá-Caballero, N., &amp; Tsay, C. J. 2022).</p><p>Humans are the only species that can use language <ref type="bibr" target="#b23">(Pinker, 2003)</ref>, while vision is an evolutionary process that existed on Earth hundreds of millions of years ago <ref type="bibr" target="#b24">(Li, 2023)</ref>, computer vision, as well as human vision, has developed amazing abilities to reason some unspoken things.</p><p>Pattern recognition is a highly important cognitive ability to make a judgment and decision making, even in the business domain <ref type="bibr" target="#b15">(Baron, R. A. 2006)</ref>, entrepreneurs may use their cognitive frameworks, such as prototypes and exemplars, to identify new business opportunities. These frameworks are developed through their previous life experience and serve as "templates" that enable specific individuals to perceive connections between seemingly unrelated changes or events. By discussing opportunities they have recognized with family, friends, and others, entrepreneurs may form more accurate and useful prototypes for identifying opportunities. These cognitive frameworks whether ideas for new products or services are practical and potentially valuable rather than merely interesting or novel. understanding the pattern recognition perspective on opportunity identification can help aspiring entrepreneurs improve their ability to identify and pursue new ventures in several ways. Some psychologists discuss that the medical expert using pattern recognition to diagnose disease <ref type="bibr" target="#b0">(Ericsson, A., &amp; Pool, R. 2016)</ref>, or a chess expert remembers where the chess was placed using their knowledge representation <ref type="bibr" target="#b7">(Simon, H., &amp; Chase, W. 1988)</ref>, some expert cannot even express how they remember the play using language, they use more abstract terms to describe it. By looking at the sports court, the expert can better predict which team is gonna win.</p><p>This article is trying to explore how multimodal can evaluate creativity achievement. When it comes to innovation judgment, participants may suffer from imagining the right product by simply reading the text, now this research, giving participants images, can help them better understand what the real product is, while previous studies only focus on language <ref type="bibr" target="#b2">(Du, M. 2023)</ref>, now multimodal gain more information, and it can judge the product better as well. Moreover, we investigate how multimodal can predict marketing research outcomes by using computer vision reasoning <ref type="bibr" target="#b25">(Karpathy, A., &amp; Fei-Fei, L. 2015</ref><ref type="bibr">, Krishna, R., et.al 2017</ref>. VQA (Visual Question Answering: a new dataset containing open-ended questions about images.) provides a fertile training and testing ground for models aimed at tasks for accurate object detection, segmentation, and summary-level image captioning, Advertisements come with images of the product that is being sold, social media profiles contain both descriptions and images of the user while multimedia websites that play audio and video have associated descriptions and opinions of the content <ref type="bibr" target="#b28">(Kiros et al. 2014)</ref> .The following study contains marketing research predicting by multimodal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1:</head><p>To evaluate the effectiveness of computer vision systems in identifying patterns, trends, and emergent properties in technological and creative domains, and compare this effectiveness with human judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis:</head><p>H1: Computer vision can distinguish a more successful innovation ( the one which got more funding) from another simply by using image recognition, comparing two innovation product image, a multimodal can tell which one is more successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>Randomly select 8 crowdfunding project and 3 of its image, input it to multimodals and ask: which creative product is more creative (novel and useful), We ask the real participants the same question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedures</head><p>We want to see if there are similar ideas, Machine learning involves an extensive amount of data, beyond human capacity to memorize.</p><p>Artificial Intelligence is capable of retrieving similar product information from the internet. In addressing complex issues like assessing creativity, the vastness of data can hinder certain heuristics <ref type="bibr" target="#b6">(Simon and Newell 1971)</ref>. Computer scientists can now use similar concept to identify image with one shot, and like most of the large language model, they use bayesian approach. <ref type="bibr" target="#b26">(Fei-Fei, L., Fergus, R., &amp; Perona, P. 2006)</ref> Results: The accuracy for GPT-4V and Google Gemini and other multimodal to distinguish the more successful product from a less one is 80%, which is much more accurate than human participants. <ref type="figure">Figure 1</ref>. Conceal the commercial logo and modify to the generated image derived from some Kickstarter project's creative illustration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2:</head><p>To see if multimodal can predict what recent marketing literature's result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2: Computer vision can distinguish what participants wants</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>We collect some literature of marketing research to see if multimodal can predict the result of the judgment of participants. Take presentation formats for example, some researchers investigate how the presentation format of scores (incremental, cumulative, or combined) influences decision makers' evaluations of entities, such as consumer ratings, employee performance, or product scores. The study demonstrates that the way in which scores are presented can significantly impact overall judgments and subsequent behaviors. The research provides theoretical insights, empirical evidence from multiple experiments, and practical implications for various domains, including consumer behavior, managerial decision-making, and marketing strategies. <ref type="bibr" target="#b22">(Lembregts, C., Schepers, J., &amp; Keyser, A. D. 2023)</ref>, based on psychological theory such as loss aversion <ref type="bibr" target="#b30">(Tversky and Kahneman, 1984)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedures</head><p>We ask GPT-4V, Google Gemini to guess which picture will the origin participants prefer from the original study. The preference for the incremental versus cumulative condition would likely be influenced by individual differences in how participants perceive and are motivated by feedback. Generally, the cumulative system might be more favored due to its ability to buffer the impact of any single poor performance.</p><p>For all the experiments, please check the original script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The advancements in computer vision and its capability to predict marketing result through image analysis surpass human accuracy, signaling a transformative shift in the realm of A/B testing and beyond. This technology, powered by sophisticated algorithms and deep learning, offers a unique perspective, analyzing visual data with unprecedented precision and speed. As computer vision systems continue to evolve, they are not only automating the process of image analysis but also unlocking new levels of understanding and insights that were previously unattainable by human capabilities. This leap in technology heralds a new era in marketing prediction, where data-driven decisions become more accurate, efficient, and effective. The implications of this shift are profound, promising to reshape industries, enhance creative processes and innovation forecasting. The future beckons with the promise of a more informed and innovative world, guided by the extraordinary capabilities of computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future suggestion:</head><p>We still don't know the difference when human progress image and language. Current research, including our study, has yet to unravel the full extent and nature of these differences. The intricacies of cognitive processing, influenced by a myriad of factors like cultural background, education, neurological diversity, and individual experiences, make it challenging to delineate clear boundaries and distinctions in how images and language are processed by humans, since personality and individual difference can be a major source of noise. <ref type="bibr" target="#b4">(Kahneman, D., Sibony, O., &amp; Sunstein, C. R. 2021)</ref>, Moreover, the tools and methodologies available for studying brain activity, while advanced, still have limitations in terms of resolution and specificity. This restricts our ability to capture the full complexity of neural processes involved in image and language perception. Additionally, the interaction between linguistic and visual processing is a dynamic and context-dependent phenomenon, which adds another layer of complexity to our understanding. Future research can use FMRI or other method to investigate the physiology behind these phenomena.</p><p>As such, while this study contributes valuable insights to the field, it represents a stepping stone rather than a conclusive exploration of the subject. Future research, employing more sophisticated technologies and interdisciplinary approaches, is essential to deepen our understanding of this intricate aspect of human cognition.</p><p>Other than predicting innovation, researchers can try to apply this technique to study criminal investigation, or medical decision making, and human resource development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>From the article: "Is It as Bad as It Looks? Judgments of Quantitative Scores Depend on Their Presentation Format." Result Both GPT4V and Google Gemini can predict all the experiment accurately.Here's an example of GPT-4V's answer for theFigure 2:</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Peak: Secrets from the new science of expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Random House</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Creativity in context: Update to the social psychology of creativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Amabile</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Westview</publisher>
			<pubPlace>Boulder, CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine vs. human, who makes a better judgment on innovation? Take GPT-4 for example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The vision in &quot;blind&quot; justice: Expert perception, judgment, and visual cognition in forensic pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Noise: a flaw in human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sibony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A machine learning approach to low-value health care: wasted tests, missed heart attacks and mis-predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human problem solving: The state of the theory in 1970</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skill in chess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer chess compendium</title>
		<meeting><address><addrLine>New York, NY; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="175" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. review</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">293</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How To Create A Good Crowdfunding Project? From A Natural Language Processing Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Marketing Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hagendorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Computational Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Overlap in Meaning Is a Stronger Predictor of Semantic Activation in GPT-3 Than in Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Digutsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note>Scientific Reports</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Do Large Language Models know what humans know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michaelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13309</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social psychology of creativity: A consensual assessment technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Amabile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">997</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<ptr target="https://openai.com/research/gpt-4)" />
	</analytic>
	<monogr>
		<title level="j">GPT-4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Opportunity recognition as pattern recognition: How entrepreneurs &quot;connect the dots&quot; to identify new business opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Baron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Academy of management perspectives</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="104" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Biases in predicting preferences for the whole visual patterns from product fragments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="304" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bias in radiology: the how and why of misses and misinterpretations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Busby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Courtier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Glastonbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="236" to="247" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Blinded by our sight: Understanding the prominence of visual information in judgments of competence and performance. Current opinion in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolsá-Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tsay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="219" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How Generative AI Can Augment Human Creativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Eapen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Finkenstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Folk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Venkataswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<biblScope unit="page" from="3" to="38" />
			<date type="published" when="2023-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial recognition technology can expose political orientation from naturalistic facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visual exploration mediates the influence of personal traits on responses to artworks in an art gallery setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trawiński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Metelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S G</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Donnelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note>Psychology of Aesthetics, Creativity, and the Arts</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Is It as Bad as It Looks? Judgments of Quantitative Scores Depend on Their Presentation Format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lembregts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Keyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="page">00222437231193343</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The language instinct: How the mind creates language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Penguin uK</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
			<publisher>Flatiron Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st international conference on machine learning (ICML-14)</title>
		<meeting>the 31st international conference on machine learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note>&amp; Polosukhin, I</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Choices, Values, and Frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="50" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Maybe submitted to Nature Computational Science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
