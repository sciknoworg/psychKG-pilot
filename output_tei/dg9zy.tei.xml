<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VALIDATION AND COMPARISON OF NON-STATIONARY COGNITIVE MODELS: A DIFFUSION MODEL APPLICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schumacher</surname></persName>
							<email>schuma.luk@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schnuerch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Voss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="institution">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="institution">University of Mannheim</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Psychology Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Cognitive Science Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VALIDATION AND COMPARISON OF NON-STATIONARY COGNITIVE MODELS: A DIFFUSION MODEL APPLICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Cognitive processes undergo various fluctuations and transient states across different temporal scales. Superstatistics are emerging as a flexible framework for incorporating such non-stationary dynamics into existing cognitive model classes. In this work, we provide the first experimental validation of superstatistics and formal comparison of four non-stationary diffusion decision models in a specifically designed perceptual decision-making task. Task difficulty and speed-accuracy trade-off were systematically manipulated to induce expected changes in model parameters. To validate our models, we assess whether the inferred parameter trajectories align with the patterns and sequences of the experimental manipulations. To address computational challenges, we present novel deep learning techniques for amortized Bayesian estimation and comparison of models with time-varying parameters. Our findings indicate that transition models incorporating both gradual and abrupt parameter shifts provide the best fit to the empirical data. Moreover, we find that the inferred parameter trajectories closely mirror the sequence of experimental manipulations. Posterior re-simulations further underscore the ability of the models to faithfully reproduce critical data patterns. Accordingly, our results suggest that the inferred non-stationary dynamics may reflect actual changes in the targeted psychological constructs. We argue that our initial experimental validation paves the way for the widespread application of superstatistics in cognitive modeling and beyond.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The human brain operates in a perpetual state of activity, whether it is focused on a particular task or wandering in the inner world of thoughts. This activity reflects the non-stationary nature of neuronal dynamics, which are characterized by a complex interplay between transient, evoked states, and ongoing spontaneous fluctuations <ref type="bibr" target="#b18">(Galadí et al., 2021;</ref><ref type="bibr" target="#b37">Melanson et al., 2017)</ref>. The complex cognitive processes that emerge from this neuronal activity also tend to exhibit non-stationary dynamics <ref type="bibr" target="#b8">(Craigmile et al., 2010;</ref><ref type="bibr" target="#b54">Sebastian Castro-Alvarez &amp; Tendeiro, 2023;</ref><ref type="bibr" target="#b57">Van Orden et al., 2003;</ref><ref type="bibr" target="#b62">Wagenmakers et al., 2004)</ref>. In other words, proverbial cognitive processes, such as attention, memory, and decisionmaking, are not constant over time, but instead undergo fluctuations, shifts, and alterations in their functions.</p><p>Lapses of attention are a canonical cause of such non-stationary dynamics. Even when actively engaged in a task, our focus can drift or momentarily falter <ref type="bibr" target="#b65">(Weissman et al., 2006)</ref>. Moreover, our capacity to sustain attention and concentrate may vary, influenced by factors such as fatigue, motivation, and external distractions <ref type="bibr" target="#b12">(Esterman &amp; Rothlein, 2019;</ref><ref type="bibr" target="#b48">Ratcliff &amp; Van Dongen, 2011;</ref><ref type="bibr" target="#b64">Walsh et al., 2017)</ref>. These fluctuations can have a significant impact on our cognitive functioning, but they are often overlooked or simplified in traditional models of cognition. And while these often assume cognitive processes to be stable and time-invariant, there has been a growing recognition that traditional models do not fully capture the complexity and variability of real-world cognition <ref type="bibr" target="#b4">(Beer, 2023;</ref><ref type="bibr" target="#b7">Cochrane et al., 2023;</ref><ref type="bibr" target="#b13">Evans &amp; Brown, 2017;</ref><ref type="bibr" target="#b24">Gunawan et al., 2022;</ref><ref type="bibr" target="#b30">Kucharský et al., 2021;</ref><ref type="bibr" target="#b33">Li et al., 2023;</ref>. Common approaches to address variability in the components of cognitive models can be broadly classified into four categories: stationary variability, trial binning, regression approach, and frontend-backend models.</p><p>The first approach assumes random fluctuations around a stable mean, referred to as stationary variability (see <ref type="bibr">Figure 1A)</ref>. A prominent example of this approach is the "full" diffusion decision model (DDM), which allows for inter- igure 1: A conceptual illustration of the five main approaches to model temporal variation in the parameters θ of a cognitive model G. A Stationary variability, also known as inter-trial variability, assumes that parameter values fluctuate around a stable mean. B Trial binning involves organizing the data into distinct bins and fitting a cognitive model G to each bin individually. C Regression approach employs time (and sometimes additional contextual variables) as predictors for the parameters θ. D Frontend-backend models employ a mechanistic model, referred to as the frontend, to elucidate the dynamics of the parameter of the cognitive model (i.e., the backend). E Superstatistics involve a superposition of multiple stochastic processes operating on different temporal scales. They comprise a low-level observation model G and a high-level transition model T that specifies how the parameters θ t evolve stochastically.</p><p>trial variability of its core parameters <ref type="bibr" target="#b46">(Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b47">Ratcliff &amp; Tuerlinckx, 2002)</ref>. However, stationary inter-trial variability mainly improves in-sample model fit and cannot identify systematic changes or sudden shifts in core model parameters. Moreover, the resulting model family still treats behavioral data as independent and identically distributed (IID) responses, making it unsuitable for investigating systematic changes in cognitive constructs.</p><p>Another approach for detecting systematic changes in cognitive model components is trial binning <ref type="bibr" target="#b13">(Evans &amp; Brown, 2017;</ref><ref type="bibr" target="#b15">Evans &amp; Hawkins, 2019;</ref><ref type="bibr" target="#b28">Kahana et al., 2018)</ref>. This method involves organizing data into discrete bins and then applying a stationary model to each of these data subsets separately (see <ref type="figure">Figure 1B</ref>). One can then examine variations in parameter estimates across these bins. The challenge in employing this approach is the selection of the number of time steps within each bin, which introduces an unwelcome trade-off between temporal resolution and estimation quality. For instance, if only a few time steps are chosen, the analysis can yield relatively fine-grained, but very uncertain estimates due to the low number of data points. A further shortcoming of trial binning is that estimates within a specific bin are not informed by data from neighboring bins. However, the appeal of dynamic modeling lies in the distinctive capability to utilize both past and future data to constrain the estimated parameter trajectories.</p><p>The third approach involves a generalized linear model (GLM) with time (and possibly other contextual factors) as a predictor of model parameters <ref type="bibr" target="#b7">(Cochrane et al., 2023;</ref><ref type="bibr" target="#b14">Evans et al., 2018)</ref>. The GLM approach is more appealing than trial binning, as it can detect linear or non-linear changes in model parameters without loss of resolution (see <ref type="figure">Figure 1C</ref>). However, the underlying regression function makes strong assumptions about the nature of the relationship between model parameters and time. Thus, even though a modeler will typically fit and compare a few plausible specifications (e.g., linear vs. exponential), it is often difficult to determine all plausible specifications a priori, and the overall flexibility of the GLM model as a process characterization remains severely limited <ref type="bibr" target="#b24">(Gunawan et al., 2022)</ref>.</p><p>Differently, the frontend-backend approach aims to account for changes in model parameters, while providing a mechanistic explanation for the dynamic nature of the target system (see <ref type="figure">Figure 1D)</ref>. Here, the backend model pertains to the cognitive model which formalizes how the behavioral data is generated (e.g., a DDM). The frontend constitutes a mechanistic model, elucidating how the parameters of the backend model adapt over time, in different contexts and in response to additional factors <ref type="bibr" target="#b16">Fontanesi et al., 2019;</ref><ref type="bibr" target="#b39">Osth et al., 2018;</ref>. This approach has several advantages, as it not only accommodates the dynamic nature of the parameters, but also provides a mechanistic description for their temporal variation through a set of static parameters and deterministic functions. For instance, there has been a recent trend to use reinforcement learning models as a frontend model to inform changes in DDM parameters due to reward-based learning <ref type="bibr" target="#b16">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b36">McDougle &amp; Collins, 2021;</ref><ref type="bibr" target="#b38">Miletić et al., 2021)</ref>. Nevertheless, detailed frontend models are often challenging to develop, estimate, and compare.</p><p>Recently, we proposed an alternative approach that infers non-stationary parameter trajectories directly from the data, while imposing minimal constraints on how parameters change over time . Our approach leverages a framework known as superstatistics <ref type="bibr" target="#b2">(Beck &amp; Cohen, 2003;</ref><ref type="bibr" target="#b3">Beck, 2004;</ref><ref type="bibr" target="#b35">Mark et al., 2018)</ref>, which involves a superposition of multiple stochastic processes operating on distinct time scales (see <ref type="figure">Figure 1E</ref>). At its core, this model comprises a low-level observation model and a high-level transition model. The former describes how data at a specific time point is generated, akin to the backend model. Like the frontend model, the transition model characterizes how the parameters change over time. However, from a superstatistics perspective, the transition model is inherently a stochastic process, exemplified, for instance, by a Gaussian random walk or a regime switching process.</p><p>The superstatistics approach effectively addresses the limitations of prior methodologies. Unlike stationary models, superstatistical models can readily generate non-stationary variations in the parameters of the low-level model, facilitating gradual or sudden transitions between different states. Furthermore, parameter estimates are contingent on past data points, thereby treating the data no longer as IID. In contrast to the trial-binning approach, models within the superstatistics framework leverage the entirety of available data, mitigating concerns about insufficient data points for parameter estimation. Different from GLM approaches, our superstatistics method imposes minimal assumptions on potential parameter trajectories, making it significantly less restrictive.</p><p>In contrast to frontend-backend models, superstatistics do not offer mechanistic explanations for parameter dynamics but provide greater flexibility in their estimation. Although mechanistic explanations are central to psychological research, there are cases where suitable explanations are lacking or are applicable only to specific parameters. Therefore, we consider these two approaches as complementary. The superstatistical framework takes a bottom-up, exploratory approach, functioning as a tool for generating hypotheses. In subsequent stages, one could potentially formulate plausible frontend models based on insights from parameter trajectories inferred with a superstatistical model. Additionally, superstatistical models can serve as benchmarks for testing and validating competing frontend-backend models by comparing resulting parameter trajectories from both methods.</p><p>Having laid out the potential benefits of the superstatistics framework and its applicability in the realm of cognitive process models , a pivotal question arises: Do the inferred parameter trajectories genuinely reflect shifts in the cognitive constructs they aim to represent, or are they merely a modeling artefact? To address this inquiry, we embark on an experimental validation study. In this study, we manipulate the experimental context in a manner that allows us to confidently anticipate how individuals and, consequently, their inferred cognitive constructs, will respond. In other words, if the inferred parameter time series mirror the alterations in the experimental context, we garner substantial evidence that these trajectories indeed reflect changes in the psychological constructs.</p><p>Throughout, we employ the well-established 4-parameter DDM <ref type="bibr" target="#b44">(Ratcliff, 1978)</ref> as a low-level observation model. The DDM is a mathematical model that simultaneously accounts for response time (RT) and choice data obtained from two-alternative decision tasks. Fundamentally, it posits that, in forced-choice binary decision task, individuals accumulate evidence for the decision alternatives until a certain threshold is met, triggering a decision. Each of the DDM's four core parameters corresponds to a specific psychological construct: (i) the drift rate v signifies the average speed of information uptake; (ii) the threshold a serves as a proxy for decision caution; (iii) the relative starting point β represents a priori decision preferences; and (iv) the additional constant τ accounts for the duration of all processes taking place prior and following a decision, such as stimulus encoding or motor action (but see <ref type="bibr" target="#b59">Verdonck et al., 2021)</ref>.</p><p>A primary reason for our choice of the DDM as the observation model lies in its rigorous prior validation <ref type="bibr" target="#b1">(Arnold et al., 2015;</ref><ref type="bibr" target="#b32">Lerche &amp; Voss, 2019;</ref><ref type="bibr" target="#b61">Voss et al., 2004)</ref>. These prior studies have convincingly demonstrated that the DDM's parameters are valid reflections of the intended psychological constructs. Moreover, the manipulation of experimental conditions leading to systematic alterations in specific DDM parameters is well-documented and comprehensively understood . For example, varying the difficulty of an experimental task alters the drift rate parameter, whereas providing verbal instructions to prioritize either speed or accuracy during task-solving leads to observable shifts in the threshold parameter and sometimes also in the non-decision time <ref type="bibr" target="#b31">(Lerche &amp; Voss, 2018)</ref>.</p><p>In this study, we focus on the aforementioned experimental manipulations targeting the drift rate and the threshold parameters. We employed a color discrimination task, which was also utilized in the validation study by <ref type="bibr" target="#b61">Voss et al. (2004)</ref>. During this task, individuals must decide whether there are more blue or more orange pixels in a patch of pixels. The difficulty of the task can be easily manipulated by adjusting the ratio of blue and orange pixels. The farther the ratio is from 1:1, the easier the task becomes. Additionally, we manipulated the emphasis on speed or accuracy by verbally instructing participants to prioritize one over the other.</p><p>Systematic changes in cognitive model parameter can appear in different ways, ranging from changing slowly and gradually to more rapid and large shifts. In our experiment, we focus on two different types. Firstly, task difficulty changes frequently to the next easier or harder level, imitating gradual changes. Secondly, the speed-accuracy emphasis changes less regularly after each trial block, resembling sudden shifts. The primary aim of our experiment is to investigate whether the parameter trajectories inferred with a non-stationary DDM (NSDDM) match these changing patterns of the experimental conditions. Specifically, we expect the drift rate parameter to mirror the gradual changes of the task difficulty. Additionally, the threshold parameter should show sudden shifts when the priority switches between speed and accuracy. It is crucial to understand that in this application, the NSDDM does not have information about the experimental context and has to infer the parameter trajectory solely on the behavioral data.</p><p>When dealing with different types of fluctuations, another crucial question arises: What kind of transition model is most suitable for capturing the expected dynamics? To address this question, we implement different NSDDMs that solely differ in their transition model for the drift rate and the threshold parameter. Specifically, we compare four different transition models: (i) a Gaussian random walk; (ii) a mixture between a Gaussian random walk and uniformly distributed regime changes; (iii) a Lévy flight; and (iv) a regime switching function, where parameters either remain the same as in the previous time step or shift uniformly. These four transition models differ in their complexity (i.e., number of high-level parameters) and their ability to account for different types of temporal shifts.</p><p>Performing Bayesian model comparison and parameter estimation with superstatistical models can be computationally challenging . Therefore, we employ simulation-based inference <ref type="bibr">(SBI, Cranmer et al., 2020)</ref> as implemented in the BayesFlow framework <ref type="bibr" target="#b43">(Radev et al., 2023)</ref>. BayesFlow enables us to carry out a principled Bayesian workflow utilizing simulation-based calibration <ref type="bibr">(SBC, Säilynoja et al., 2022;</ref><ref type="bibr" target="#b55">Talts et al., 2020)</ref> and other validation methods <ref type="bibr" target="#b19">(Gelman et al., 2020;</ref><ref type="bibr" target="#b51">Schad et al., 2021</ref>) that would otherwise be excessively time-consuming. The contributions of the present study can be summarized as follows:</p><p>1. We perform an experimental validation of different non-stationary instantiations of the diffusion decision model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We propose an amortized method for Bayesian model comparison of non-stationary models via deep ensembles.</p><p>3. We showcase the potential of amortized Bayesian inference for increasing the aspirations of cognitive modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A total of 14 participants (9 female, 5 male) were recruited for the experiment. The participants had an average age of 23.14 years (SD = 1.29, Range = <ref type="bibr">[22,</ref><ref type="bibr">26]</ref>). Every individual provided informed consent to participate in the study, and the research protocol received approval from the local ethics committee. The entire study was conducted in accordance with the ethical principles outlined in the Helsinki Declaration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>The participants completed a total of 800 trials in a color discrimination task, including 32 practice trials. In each trial, individuals were presented with a rectangular patch containing blue and orange pixels and had to determine whether there were more blue or orange pixels. Prior to the patch presentation, a fixation cross was displayed for 300 ms. All stimuli were presented on a gray background.</p><p>Task difficulty was manipulated by varying the proportion of blue/orange pixels in the patch. The following ratios were utilized: 50.5:49.5; 52.25:47.75; 53.5:46.5; and 55:45. Half of the trials featured orange as the dominant color, while the other half featured blue. The difficulty level remained constant for either 8 or 16 trials before transitioning to the next level of difficulty. In addition to manipulating task difficulty, participants received two types of instructions which changed every 48 trials. In the "accuracy" condition, individuals were instructed to prioritize accuracy in their responses. Conversely, in the "speed" condition, participants were directed to emphasize speed while maintaining a reasonable level of accuracy. Feedback was provided after each trial to make participants aware of their performance: a green cross for correct responses, a red minus for incorrect responses, and a red clock for responses slower than 700 ms in the speed condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superstatistics Framework</head><p>To represent non-stationary changes in DDM parameters, we adopt a superstatistics framework <ref type="bibr" target="#b2">(Beck &amp; Cohen, 2003;</ref><ref type="bibr" target="#b35">Mark et al., 2018)</ref>. Within this framework, each generative model comprises (at least) a low-level observation model G characterized by time-dependent local parameters θ t ∈ R K that vary according to a high-level transition model T with static high-level parameters η ∈ R D . These models simulate parameters and observable data x t ∈ X according to the following general recurrent system</p><formula xml:id="formula_0">θ t = T (θ 0:t−1 , η, ξ t ) with ξ t ∼ p(ξ | η), θ 0 ∼ p(θ) x t = G(x 1:t−1 , θ t , z t ) with z t ∼ p(z | θ t ),<label>(1)</label></formula><p>where T represents an arbitrary high-level transition function parameterized by η, and G is a (non-linear) transformation that encapsulates the functional assumptions of the low-level model. The random variates ξ t and z t govern the stochastic nature of the two model components through noise outsourcing. The initial parameter configuration θ 0 adheres to a prior distribution θ 0 ∼ p(θ) encoding the available information about feasible starting parameter values.</p><p>The above formulation is very abstract and general, highlighting the flexibility of the superstatistics framework. Moreover, it does not assume that the corresponding transition or likelihood densities, given by</p><formula xml:id="formula_1">T(θ t | η, θ 0:t−1 ) = p(θ t , ξ | η, θ 0:t−1 ) dξ (implied transition density) (2) p(x t | θ t , x 1:t−1 ) = p(x t , z | θ t , x 1:t−1 ) dz (implied likelihood density),<label>(3)</label></formula><p>are tractable or available in closed-form, situating our approach in the context of simulation-based inference <ref type="bibr">(SBI, Cranmer et al., 2020)</ref>. Here, we build on SBI with neural networks <ref type="bibr" target="#b0">(Ardizzone et al., 2018;</ref><ref type="bibr" target="#b22">Greenberg et al., 2019;</ref> as a principled approach to perform fully Bayesian inference by using only samples from the generative system defined by Equation 1. Importantly, our estimation methods overcome key limitations of previous approaches related to the curse of dimensionality <ref type="bibr" target="#b35">(Mark et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Level Model</head><p>In this work, we use the same standard DDM implementation as a low-level observation model G for all NSDDMs. The low-level dynamics of the evidence accumulation process are described by the following stochastic ordinary differential equation:</p><formula xml:id="formula_2">dx n = vdt s + z dt s with z ∼ N (0, 1)<label>(4)</label></formula><p>Accordingly, the evidence x n on a given trial n follows a random walk with drift v and Gaussian noise z, where t s represents time on a continuous time scale. The core assumption of the DDM is that evidence is accumulated with a fixed rate v until one of two thresholds, a or 0, is reached, and the corresponding decision D n is made:</p><formula xml:id="formula_3">D n = 1, if x n ≥ a 0, if x n ≤ 0 .<label>(5)</label></formula><p>Furthermore, the DDM incorporates an additive constant τ , which represents the time allocated to all non-decisional processes (i.e., stimulus encoding and motor action). Consequently, the DDM encompasses three distinct free parameters, namely θ = (v, a, τ ). We fixed the starting point of the evidence accumulation process at a/2, since, in our case, the two boundaries of the accumulation process correspond to correct and incorrect responses, respectively. Thus, it is unwarranted to estimate any potential a priori bias towards either of these boundaries <ref type="bibr" target="#b60">(Voss et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Level Models</head><p>We formulate and compare four different high-level transition models, denoted as T 1 , ..., T 4 , which govern the trialby-trial changes in local DDM parameters θ 1:T . These transition models vary in terms of their flexibility in allowing changes to the low-level parameters and their underlying complexity, including the number of high-level parameters</p><formula xml:id="formula_4">Trial (t) Parameter</formula><p>Random walk</p><formula xml:id="formula_5">Trial (t)</formula><p>Mixture random walk</p><formula xml:id="formula_6">Trial (t)</formula><p>Levy flight</p><formula xml:id="formula_7">Trial (t)</formula><p>Regime switching <ref type="figure">Figure 2</ref>: An example illustration of the four high-level (transition) models considered in our study, governing the temporal variation of a hypothetical low-level model parameter.</p><p>involved (see <ref type="figure">Figure 2</ref> for an exemplar trajectory generated by each transition model). To ensure that the low-level parameters remain within plausible ranges, we impose both lower and upper bounds on their trajectories. Specifically, we set the upper bounds for the parameters v, a, and τ to 8, 6, and 4, respectively. Additionally, since negative parameter values are not meaningful for our DDM specification, we set the lower bounds for all parameters to 0. For all transition models, we assume independence between the trajectories of the local DDM parameters.</p><p>Random Walk The first transition model (T 1 ) convolves the low-level model's parameters with a Gaussian distribution, resulting in a gradual change that follows a random walk:</p><formula xml:id="formula_8">T 1 (θ k,t | θ k,t−1 , σ k ) = N (θ k,t | θ k,t−1 , σ k )<label>(6)</label></formula><p>According to this transition model, the current value of each parameter θ k,t is only influenced by its previous value θ k,t−1 , generating more or less auto-correlated and gradual changes.</p><p>Mixture Random Walk The second transition model (T 2 ) corresponds to a mixture distribution between a random walk (cf. Equation 6) and uniformly distributed shifts:</p><formula xml:id="formula_9">T 2 (θ k,t | θ k,t−1 , ρ k , σ k , a k , b k ) = ρ k N (θ k,t | θ k,t−1 , σ k ) + (1 − ρ k ) U(a k , b k )<label>(7)</label></formula><p>where ρ indicates the probability of the type of change (gradual change or shift) as a mixing coefficient for the two states. The upper and lower bounds of the uniform distribution, denoted as a and b, are set to cover plausible parameter ranges and are not treated as free parameters themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lévy-Flight</head><p>The Lévy flight transition model (T 3 ) is similar to the Gaussian random walk. However, instead of assuming normally distributed noise, it assumes an alpha-stable transition for each component of θ:</p><formula xml:id="formula_10">T 3 (θ k,t | θ k,t−1 , σ k , α k ) = Alpha-Stable(θ k,t | θ k,t−1 , σ k , β = 0, α k )<label>(8)</label></formula><p>where 0 &lt; α ≤ 2 governs the heaviness of the noise distribution's tails. If α k = 2 then the distribution is equivalent to a Gaussian distribution. Notably, as the value of α decreases, the distribution's tails get heavier, allowing for larger shifts in the parameter values. When simulating from the Lévy flight transition model, we use a scale of σ k / √ 2, such that the corresponding Gaussian distribution for α k = 2 has a standard deviation of σ k .</p><p>Regime Switching Finally, the regime switching transition model (T 4 ) is a simpler version of the mixture random walk. The parameter's trajectory adheres to one of two possibilities: it either maintains its previous value or undergoes a uniform shift:</p><formula xml:id="formula_11">T 4 (θ k,t | θ k,t−1 , ρ k , a k , b k ) = ρ k δ(θ k,t − θ k,t−1 ) + (1 − ρ k ) U(a k , b k ),<label>(9)</label></formula><p>where δ(•) is the Dirac delta distribution indicating that the parameter either does not change at all with probability ρ or undergoes a sudden change with probability 1 − ρ.</p><p>Strictly speaking, some of the above transition models can effectively be transformed into others by employing specific high-level parameter configurations. For instance, the mixture random walk with σ = 0 reduces to the regime  <ref type="figure">Figure 3</ref>: A conceptual illustration of our amortized Bayesian inference training setup. A Parameter estimation A recurrent summary network processes the synthetic time series x 1:T and learns maximally informative temporal summary statisticsx 1:T . An inference network (i.e., normalizing flow) learns to approximate the joint posterior distribution of time-varying low-level parameters θ 1:T and static high-level parameters η given the learned summaries. B Model comparison A transformer summary network consumes time series simulated from competing models and learns maximally informative summary vectorsx. An inference network (i.e., a probabilistic classifier) learns to approximate posterior model probabilities (PMPs) given the summary vectors. Once trained, the networks can be efficiently validated using principled Bayesian methods and applied to the observed data. switching transition function. Conversely, when ρ = 0 it reduces to a simple Gaussian random walk. Also, the Lévy flight transition model with α = 2 turns into a random walk transition function. The mixture random walk and the Lévy flight transition function both have two high-level parameters and can thus be regarded as more complex and more flexible than the other two transition models, which only have a single high-level parameter. Notably, the random walk transition model is the only one that cannot generate relatively large sudden shifts in parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison Setup</head><p>One of the major aims of this study is to compare four NSDDMs sharing the same low-level diffusion model but differing in their assumptions about the type of stochastic variation of the drift rate (v) and threshold (a) parameters. All four NSDDMs employ the same Gaussian random walk model T 1 for the non-decision time parameter (τ ). We base this decision on previous research  and the rationale of our experimental manipulations, which should not imply sudden large shifts in the τ parameter. For M 1 , the drift rate and threshold parameter also follow a Gaussian random walk, resulting in three high-level parameters, η = (σ v , σ a , σ τ ). In M 2 , both v and a follow a mixture between a Gaussian random walk and uniform shifts (T 2 ), which results in a total of five highlevel parameters, η = (σ v , σ a , σ τ , ρ v , ρ a ). In contrast, M 3 introduces a trajectory for the drift rate and threshold parameters characterized by a Lévy flight (T 3 ), which has five free high-level parameters,</p><formula xml:id="formula_12">η = (σ v , σ a , σ τ , α v , α a ).</formula><p>Lastly, for M 4 , the two parameters v and a either remain the same as in the previous time point or shift uniformly (T 4 ). This model has a total of three high-level parameters, η = (σ τ , ρ v , ρ a ). A listing of the weakly informative prior distributions assigned to the model parameters can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amortized Bayesian Inference</head><p>Amortized Bayesian inference (ABI) is a flexible framework for estimating, comparing, and validating complex models through simulation-based training of specialized neural networks <ref type="bibr" target="#b43">(Radev et al., 2023)</ref>. ABI consists of (i) a training phase where the networks learn a surrogate distribution, and (ii) an inference phase where the networks infer the target quantities (e.g., model parameters or model posterior probabilities) in real-time for any new data set supported by the model(s). The neural networks are trained purely on simulations from the generative model and do not require an explicit likelihood or numerical integration. Thus, ABI re-casts expensive Bayesian inference into a neural network prediction task, such that sampling from the target posterior and model refits happen almost instantaneously.</p><p>Amortized Parameter Estimation Our deep learning approach for jointly estimating time-varying and static parameters follows , who extend ideas from ABI with static parameters <ref type="bibr" target="#b21">(Gonçalves et al., 2020;</ref> to non-stationary Bayesian models. Accordingly, our goal is not only to infer the tra-jectories of all three model parameters {θ t } T t=1 , but also to estimate the posterior distribution for the static high-level parameters η of the transition model. Thus, we are interested in recovering the full joint posterior p(θ 1:T , η | x 1:T ) from the observed time series {x t } T t=1 :</p><formula xml:id="formula_13">p(θ 1:T , η | x 1:T ) ∝ p(η, θ 0 ) p(x 1 | θ 1 ) T t=2 p(x t | θ t , x 1:t−1 ) T t=1 T(θ t | η, θ 0:t−1 )<label>(10)</label></formula><p>where p(η, θ 0 ) is the joint prior over high-level parameters and initial low-level parameter values. The joint prior typically factorizes as p(η, θ 0 ) = p(η)p(θ 0 ), assuming that η and θ 0 are independent in the absence of any information. Even though our SBI method is applicable to any model of the general form in Eq. 10, our low-level (Low-Level Model) and high-level (High-Level Models) specifications lead to a simplified formulation</p><formula xml:id="formula_14">p(θ 1:T , η | x 1:T ) ∝ p(η, θ 0 ) T t=1 p(x t | θ t ) T t=1 T(θ t | η, θ t−1 ).<label>(11)</label></formula><p>The simplified formulation follows from the fact that our transition models share the Markov property and the DDM likelihood depends on time only through the current parameter θ t in the latent trajectory θ 1:T .</p><p>Following the typical ABI offline training setting (see <ref type="figure">Figure 3A</ref> for a conceptual illustration), we generate a data set of simulated data sets,</p><formula xml:id="formula_15">D = {η (b) , θ (b) 1:T , x (b) 1:T } B b=1</formula><p>, and use the simulated data to train a specialized neural network, F ψ (θ 1:T , η; x 1:T ), which approximates the full joint posterior (i.e., a normalizing flow, see <ref type="bibr" target="#b40">Papamakarios et al., 2021)</ref>. In particular, we minimize the following loss in expectation over the full non-stationary generative model (i.e., the right hand-side of Eq. 10)</p><formula xml:id="formula_16">L(ψ) = E (η,θ 1:T ,x 1:T )∼D [− log q ψ (θ 1:T , η | x 1:T )] ,<label>(12)</label></formula><p>where we approximate the expectation over p(θ 0 ) p(η, θ 1:T , x 1:T ) via our training set D and regularize against overfitting with standard techniques, such as dropout and weight decay. It is also possible to run the simulator(s) indefinitely and perform online training using on-the-fly simulation . In fact, this approach should be preferred for fast simulators, as it makes overfitting hardly possible. Thus, online learning is the approach we pursue for estimating the parameters of our NSDDMs.</p><p>In the context of dynamic Bayesian models, we have many choices on how to factorize the joint posterior <ref type="bibr" target="#b50">(Särkkä, 2013)</ref>. The two most common choices are to approximate the filtering distribution or the smoothing distribution <ref type="bibr" target="#b35">(Mark et al., 2018)</ref>. The filtering distribution corresponds to an online analysis, where the low-level parameters θ t at time step t are only informed by past data points. Differently, the smoothing distribution conditions the posterior of θ t on all past and future data points, and provides potentially sharper estimates. Thus, in this study, we exclusively target the approximate smoothing distribution due to its superior parameter recoverability in an offline analysis. 2 In practice, we employ unidirectional or bidirectional long-short term memory (LSTM) networks <ref type="bibr" target="#b20">(Gers et al., 2000)</ref> with many-to-many input-output relationships as a backbone for approximating the filtering or the smoothing distribution, respectively. We then train four separate neural approximators, such that each network becomes an "expert" in inferring the smoothing distribution of the corresponding NSDDM. The Appendix contains more details on the neural network settings and training hyperparameters.</p><p>Amortized Model Comparison To conduct a comparative analysis of the four NSDDMs, we focus on Bayes factors (BFs) and posterior model probabilities (PMPs), which can be classified as prior predictive methods embodying Occam's razor <ref type="bibr" target="#b29">(Kass &amp; Raftery, 1995;</ref><ref type="bibr" target="#b34">MacKay, 2003)</ref>. The efficacy of these measures has been demonstrated in a wide range of psychological modeling studies <ref type="bibr" target="#b27">(Heck et al., 2023)</ref>. Nevertheless, an ongoing debate surrounds the preference between the two <ref type="bibr" target="#b56">(Tendeiro &amp; Kiers, 2019;</ref><ref type="bibr" target="#b58">van Ravenzwaaij &amp; Wagenmakers, 2022)</ref>. Since BFs and posterior odds (i.e., ratios between PMPs) are equivalent when all models are assumed to be equally likely a priori, we estimate and analyse both quantities in our study.</p><p>Following the common Bayesian terminology (MacKay, 2003), we can refer to the four competing models through an index set</p><formula xml:id="formula_17">M = {M 1 , M 2 , M 3 , M 4 }.</formula><p>The aim of prior predictive Bayesian model comparison is to find the simplest most plausible model within M. To this end, we can compute PMPs for each of the competing models</p><formula xml:id="formula_18">p(M j | x 1:T ) = p(x 1:T | M j ) p(M j ) E p(M) [p(x 1:T | M)] ,<label>(13)</label></formula><p>where p(M) refers to the prior distribution over the discrete model space. The marginal likelihood p(x 1:T | M j ) plays a crucial role in Equation 13, and can be expressed by integrating out all parameters of the joint model,</p><formula xml:id="formula_19">p(x 1:T | M j ) = p(η, θ 0 ) T t=1 p(x t | θ t , M j ) T t=1 T j (θ t | η, θ t−1 ) dη dθ 0 , . . . , dθ T .<label>(14)</label></formula><p>Importantly, since the marginal likelihood averages the likelihood over the joint prior, it automatically incorporates a probabilistic Occam's razor, favoring models with constrained prior predictive flexibility. When comparing a pair of competing models, M j and M i , we can compute the ratio between their respective marginal likelihood,</p><formula xml:id="formula_20">BF ji = p(x 1:T | M j ) p(x 1:T | M i ) .<label>(15)</label></formula><p>This ratio is referred to as the Bayes factor (BF). Consequently, a BF ji &gt; 1 signifies a relative preference for model j over model i based on the given data x 1:T <ref type="bibr" target="#b29">(Kass &amp; Raftery, 1995)</ref>.</p><p>Unfortunately, the marginal likelihood is notoriously hard to approximate <ref type="bibr" target="#b23">(Gronau et al., 2017)</ref> and even doubly intractable for mechanistic models with unknown or unnormalized likelihoods. To circumvent this intractability, we follow the neural method of <ref type="bibr" target="#b11">Elsemüller, Schnuerch, et al. (2023)</ref> and <ref type="bibr" target="#b41">Radev, D'Alessandro, et al. (2020)</ref> which enables amortized Bayesian model comparison for arbitrary computational models (see <ref type="figure">Figure 3B</ref> for a graphical illustration). This method involves the simultaneous training of two neural networks with different roles: a summary network and an inference network. The summary network learns maximally informative summary statistics from the raw data (e.g., behavioral time series). The inference network approximates the PMPs for the candidate models, q ϕ (M | x 1:T ) given the outputs of the summary network. Here, we subsume all trainable network parameters under ϕ and refer to the composition of the two networks as an evidential network.</p><p>The training data for the evidential network consists of all simulations from the candidate models together with the corresponding model index,</p><formula xml:id="formula_21">D(M) = {x (b) 1:T , M (b) j } B ′ b=1</formula><p>, where B ′ denotes the total number of simulations from all models. Together, the two networks minimize the standard cross-entropy loss,</p><formula xml:id="formula_22">L(ϕ) = E (Mj ,x 1:T )∼D(M) − J j=1 I Mj log q ϕ (M j | x 1:T ) ,<label>(16)</label></formula><p>and we approximate the expectation over p(η, θ 1:T , x 1:T ) by our training set D(M), and I Mj denotes an indicator function (i.e., one-hot encoding) for the true model index. In principle, we could use online learning for amortized model comparison as well, but we found offline training to yield sufficiently accurate results.</p><p>More recently, <ref type="bibr" target="#b10">Elsemüller, Olischläger, et al. (2023)</ref> demonstrated the importance of gauging the sensitivity of amortized neural approximators, especially in the context of model comparison. The authors suggest to train an ensemble of multiple evidential networks, instead of relying on a single network. Accordingly, we can measure the (lack of) agreement between ensemble members and obtain a hint at the robustness of the approximate PMPs. Here, we trained an ensemble of ten evidential networks and computed the mean and standard deviation of the estimated PMPs across all ten networks. For more details regarding the neural network architecture and training settings, we refer the reader to the Appendix.</p><p>Code Availability Complete code for reproducing the results reported in this manuscript is available in the project's GitHub repository https://github.com/bayesflow-org/Non-Stationary-DDM-Validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head><p>As a first step, we assess the closed-world (i.e., in silico) performance of our model comparison method in terms of computational faithfulness and accuracy of model recovery. To assess the former, we perform simulation-based B A <ref type="figure">Figure 4</ref>: In silico model comparison and sensitivity results. A Calibration curves of all four NSDDMs aggregated across the neural approximator ensemble. Additionally, the expected calibration error ( ECE) is annotated within each subfigure. The gray histograms depict the relative frequencies of the predicted model probabilities. B Confusion matrix between true data generating model and predicted model. The proportion values were averaged across the ten neural approximator within the ensemble. calibration (SBC; <ref type="bibr" target="#b49">Säilynoja et al., 2022;</ref><ref type="bibr" target="#b55">Talts et al., 2020</ref>) based on 10 000 synthetic data sets per model. <ref type="figure">Figure 4A</ref> shows the calibration curves for each NSDDM averaged across the ten evidential networks in our deep ensemble. We observe excellent calibration with very minimal expected calibration errors ( ECE) across all models. Thus, we conclude that the approximate posterior probabilities are well-calibrated in the closed-world setting.</p><p>Next, we assess the accuracy of our model comparison networks in terms of their ability to correctly identify the ground-truth data-generating model. To this end, we apply the deep ensemble to the 40 000 synthetic data sets we have already simulated for assessing calibration. In <ref type="figure">Figure 4B</ref>, we present the resulting confusion matrix, which illustrates the agreement between true and predicted models averaged across the ten approximators. Among the four models, the random walk DDM is the only one that rarely gets confused with the other models. A possible explanation is that it is the only transition model not capable of generating sudden shifts in parameter values. The remaining models are susceptible to more frequent misclassifications. For example, the mixture random walk DDM is correctly identified only 54% of the time, and it is often confused with the regime switching model, occurring 43% of the time. Notably, the Lévy flight DDM is prone to mimicry with the random walk DDM (on average 30% of the time).</p><p>It is essential to emphasize that these results do not imply a deficiency in your model comparison method, but rather underscore the fact that certain pairs of models, such as the mixture random walk and the regime switching DDM, can generate remarkably similar data patterns. For instance, a significant portion of the prior distribution's mass for the α parameter of the Lévy flight transition model centers around 2. If α ≈ 2, then the Lévy alpha-stable distribution closely resembles a Gaussian distribution, with equality in the case of α = 2. Consequently, simulating the Lévy flight DDM would often yield data patterns that could have just as plausibly originated from the simpler random walk DDM.</p><p>Similarly, a substantial portion of the prior mass for the σ priors of the mixture random walk transition model clusters around 0, which subsequently transforms it into a regime switching transition model, resulting in large overlap in synthetic data sets. Interestingly, the mixture random walk and the Lévy flight DDM are seldom confused, even though both models can produce subtle local changes and large sudden shifts. This implies that these two transition models generate qualitatively similar but quantitatively easy to distinguish parameter trajectories. In summary, the observation of occasional model confusion is not a limitation of our method; rather, it underscores our method's effectiveness in discerning when two models generate highly similar data, making them less straightforward to differentiate from each B A other. Moreover, the amortization property of our method enables us to easily conduct such simulation studies prior to analyzing real data -estimating 40 000 posterior model probabilities would have been infeasible for any other method.</p><p>After successfully validating our model comparison method, we apply the deep ensemble to the empirical data of the 14 participants. Each approximator in the ensemble was used to infer posterior model probabilities (PMP) for each model, considering each individual's data separately. Subsequently, we calculated the mean (points), median (stars), and 75% credibility interval (CI) for the PMPs for all approximators the 14 individuals ( <ref type="figure" target="#fig_1">Figure 5A</ref>). The analysis reveals that the Lévy flight DDM is the most plausible model with an average PMP of approximately 60%. It was the most plausible model for 9 out of the 14 participants. In contrast, the mixture random walk model collects an average PMP of less than 30%. Nevertheless, it was estimated to be the most plausible model for 5 participants. The random walk DDM and regime switching DDM were consistently less plausible than the other models and did not emerge as superior for any of the participants.</p><p>In addition to PMPs, we computed log 10 Bayes factors (BF). <ref type="figure" target="#fig_1">Figure 5B</ref> depicts a heatmap of BFs for all one-toone comparisons between our four NSDDMs, averaged across the participants and the evidential networks of the ensemble. Following <ref type="bibr" target="#b29">Kass and Raftery (1995)</ref>, an absolute value of log 10 (BF) &gt; 2 indicates decisive evidence, absolute values between 1 to 2 signify strong, and between 0.5 to 1 substantial evidence. An absolute value of log 10 (BF) &lt; 0.5 is labeled as not worth more than a bare mention. The BF patterns in <ref type="figure" target="#fig_1">Figure 5B</ref> align with the PMP findings, implying strong evidence for the Lévy flight DDM over the random walk DDM and substantial evidence over the other NSDDMs. Also, both the mixture random walk and the regime switching DDM have substantial evidence over the random walk model. Interestingly, there is little evidence favoring the mixture random walk DDM over the regime-switching model, suggesting comparable performance.</p><p>These findings offer two substantive insights. First, the ability of transition models to generate sudden shifts in parameters seems essential, as seen in the random walk DDM's lower plausibility. Moreover, the regime switching DDM, allowing for occasional shifts, but neglecting small gradual changes, performed less effectively than the more complex models. This result underscores the importance of accommodating both gradual as well as sharp changes in model parameters for achieving optimal fit. Consequently, the more complex NSDDMs, particularly the mixture random walk DDM and Lévy flight DDM, emerged as more plausible than their simpler counterparts, despite the implicit penalty for prior complexity imposed by Bayesian model comparison. Posterior Re-simulation</p><p>Subsequently, we fit all four variants of the NSDDM to each of the 14 data sets, evaluating the absolute goodnessof-fit of each model. To achieve this, we conducted 500 re-simulations with randomly sampled posterior parameter trajectories for each individual data set. In <ref type="figure" target="#fig_2">Figure 6A</ref>, we present the median and median absolute deviation (MAD) of response times (RT) across all individuals and re-simulations. We provide these aggregates for each NSDDM, categorized by task difficulty level and the two experimental conditions. Notably, an initial observation reveals that the experimental manipulations were effective on average: empirical median RTs increased with task difficulty, and individuals tended to respond faster in the speed condition compared to the accuracy condition. Remarkably, all four variants of the NSDDM demonstrated an outstanding fit to these empirical data patterns. Solely, RTs in the accuracy condition with the highest task difficult level consistently are underestimated by all NSDDM variants.</p><p>The empirical and re-simulated proportion of correct choices (accuracy) are aggregated and presented in the same way as the RTs (see <ref type="figure" target="#fig_2">Figure 6B</ref>). Again, the empirical data mirror the anticipated patterns resulting from our experimental manipulations. As expected, accuracy diminishes with increasing task difficulty. Individuals are generally less accurate in the speed condition compared to the accuracy condition. Although NSDDMs successfully reproduce the general patterns in the choice data, we observe notably worse re-simulation compared to that of the RTs data. In both accuracy and speed conditions, re-simulated accuracies exhibit a less pronounced decline as a function of difficulty than observed in the empirical data. Further, the difference in accuracy between the two experimental conditions is less pronounced in the re-simulated data compared to the behavioral data. Notably, the random walk DDM underperforms relative to the other three NSDDMs in this analysis.</p><p>It is important to highlight that, unlike conventional approaches, the models did not receive any information regarding the specific experimental context an individual faced at any given moment. From these analyses, we conclude that all NSDDM implementations successfully capture the general patterns in the empirical RT data. Individual participant analyses, detailed in the Appendix, affirm the same conclusions.  <ref type="figure">Figure 7</ref>: Model fit to response time (RT) time series. The empirical RT time series of two exemplar individuals are shown in black. From trial 1 to 700, the posterior re-simulations (aka retrodictive checks) using the best fitting non-stationary diffusion decision model (NSDDM) for the specific individual are shown in blue and red, respectively. In this instance, the left column showcases results from a Lévy flight DDM, while the right column displays parameter trajectories from a mixture random walk DDM. For the remaining trials, one-step-ahead posterior predictions from the NSDDMs are depicted in cyan and orange, respectively. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.</p><p>In addition to the analysis of the absolute model fit on the aggregate level, we evaluated the fit across the RT time series. For every participant, we generated 250 posterior re-simulations for the first 700 trials with the corresponding best fitting NSDDM. The remaining 68 data points were left out for predictive analysis. Employing a one-step-ahead prediction approach, we iteratively forecasted the subsequent data point, followed by a re-fitting of the model in each step. <ref type="figure">Figure 7</ref> illustrates the empirical and re-simulated RT time series for two exemplary participants. Results for the remaining 12 participants can be seen in the Appendix. The colored lines depict the median and the shaded bands represent to 90% credibility intervals (CI) across the 250 re-simulations. Both the empirical data (solid black lines) and the re-simulated/predicted RTs were smoothed using a simple moving average (SMA) with a period of 5. Yellow shaded regions highlight trials where speed was emphasised over accuracy, whereas blank white areas denote instances where the opposite emphasis was applied. Overall, RTs were slower and more variable in the accuracy condition. Notably, the NSDDM not only closely replicated the empirical time series but also effectively predicted future data points. This suggests that the model does not overfit the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Estimates</head><p>At the heart of the current validation study are the inferred parameters, prompting a crucial question: Do these parameter dynamics align with the sequence of experimental manipulations? We address this question by examining both the time-averaged and time-varying estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate Analysis</head><p>We initially examine the parameter estimates averaged across individuals for each difficulty level and condition separately. This provides a comprehensive overview of average effects on model parameters in different experimental contexts, at first, without delving into the temporal aspect. The bottom panel of <ref type="figure" target="#fig_2">Figure 6</ref> illustrates the posterior medians and MADs collapsed onto the different experimental contexts for the drift rate ( <ref type="figure" target="#fig_2">Figure 6C</ref>) and threshold parameter ( <ref type="figure" target="#fig_2">Figure 6D</ref>).</p><p>Analyzing the aggregated drift rate estimates reveal an anticipated pattern. On average, the drift rate decreases as task difficulty increases, observed in both the accuracy and speed conditions. Additionally, slightly higher overall values are estimated in the speed condition compared to the accuracy condition. While all four NSDDMs yield fairly similar Participant 11 Lévy flight DDM Participant 6 Mixture random walk DDM <ref type="figure">Figure 8</ref>: Estimated parameter trajectories of two exemplar individuals corresponding to the respective best-fitting non-stationary diffusion decision model (NSDDM). In this instance, the left column showcases results from a Lévy flight DDM, while the right column displays parameter trajectories from a mixture random walk DDM. Each low-level parameter (drift rate, threshold, and non-decision time) is displayed on a separate row. The solid lines are color-coded (blue for the Lévy flight DDM and red for the mixture random walk DDM) to represent the posterior medians, while the shaded regions mark the median absolute deviation (MAD). The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. The sequences of task difficulty levels are depicted with black lines and overlaid with the drift rate in the top panels.</p><p>parameter values, the distinctions in average parameter values between difficulty levels are less pronounced when estimated with the random walk DDM.</p><p>With the second experimental manipulation -namely, the instruction to emphasize speed or accuracy -we aimed to manipulate the participants' decision caution, which is assumed to be captured by the threshold parameter. Examining the aggregated estimates of the threshold parameter in <ref type="figure" target="#fig_2">Figure 6D</ref>, we observe generally increased values in the accuracy condition compared to the speed condition. Interestingly, in the accuracy condition, the threshold parameter also slightly increases with growing task difficulty -a pattern not observed in the speed condition. A comparison between the estimates of the four NSDDMs reveals that the mixture random walk DDM and the Lévy flight DDM yield higher threshold estimates in the accuracy condition compared to the other two NSDDMs. Conversely, all four NSDDMs seem to converge in their threshold parameter estimates in the speed condition.</p><p>Parameter Trajectories For a more fine-grained analysis, particularly considering temporal aspects, we present the complete inferred parameter trajectories of the three low-level parameters of a NSDDM for two exemplary individuals in <ref type="figure">Figure 8</ref>. The Appendix contains the inferred parameter trajectories of the remaining 12 participants. Each partici-pant's trajectory is depicted with the posterior median (solid lines) and the median absolute deviation (MAD, shaded bands) across all 768 experimental trials, estimated with the model with the highest posterior model probability for that specific individual. The trajectory of participant 11 corresponds to a Lévy flight DDM, whereas the trajectory of participant 6 comes from a mixture random walk DDM. Shaded blocks along the timeline denote the experimental condition at a given trial, with yellow indicating an emphasis on speed.</p><p>The top panel illustrates the estimated trajectories of the drift rate parameter alongside the sequences of task difficulty levels (depicted by black line). Here, 0 corresponds to the most difficult level, while 6 represents the easiest. It is important to note that the absolute values of the difficulty conditions hold no intrinsic meaning. As observed, the drift rates of both participants align with the overarching trend of the difficulty condition sequence. They decrease when the difficulty is high and increase as the task becomes easier.</p><p>Regarding the trajectory of the threshold parameter (middle panel), we anticipated that a shift from an accuracy instruction to a speed instruction would lead to a decrease in the threshold parameter, and vice versa. This hypothesized pattern is clearly evident when examining the estimated threshold parameter trajectories of the two participants in the middle panel of <ref type="figure">Figure 8</ref>. For instance, the threshold parameter estimated for participant 11 oscillates around an approximate value of 1 in the speed condition. Moreover, it consistently rises whenever a switch in the accuracy condition takes place. Intriguingly, the parameter's value during accuracy emphasis is not as uniform compared to the speed condition. In some blocks, it fluctuates around 2, while in others it hovers around 1.5 or even lower. Similarly, participant 6 displays pronounced shifts in the threshold parameter when a change in the condition occurs, with these shifts being more pronounced in the first half of the experiment and diminishing in the second half.</p><p>Finally, the bottom panel of <ref type="figure">Figure 8</ref> illustrates the trajectory of the non-decision time parameter. Although our experimental manipulations did not systematically target the dynamics of this parameter, it is sometimes assumed that the manipulation of speed and accuracy instructions may also influence it <ref type="bibr" target="#b1">(Arnold et al., 2015;</ref><ref type="bibr" target="#b61">Voss et al., 2004)</ref>. While both individuals exhibit some fluctuations in τ , no systematic differences between the two conditions are apparent.</p><p>Upon reviewing the parameter trajectories of the remaining participants in the Appendix, similar patterns emerge. In summary, both the inferred means and trajectories of the drift rate and threshold parameters align with the sequence of experimental manipulations, as predicted by our design. Moreover, our NSDDMs were able to estimate these trajectories directly from the behavioral data, getting no explicit information whatsoever about the experimental context. Thus, our validation study suggests that NSDDMs are able to detect genuine changes in cognitive constructs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Psychology and cognitive science are witnessing a growing interest in incorporating dynamic aspects into mechanistic models that seek to formalize and explain cognitive processes. In a previous study, we explored a method to estimate plausible trajectories of cognitive process model parameters directly from behavioral data . Nevertheless, an empirical validation of this modeling approach was lacking. Thus, the current study sought to bridge this gap by experimentally examining the validity of the inferred diffusion decision model (DDM) parameter dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental validation</head><p>The present study posed to the following core question: Can non-stationary DDMs (NSDDM) effectively detect experimentally induced changes in cognitive constructs from behavioral data alone? If this holds true, our findings can provide the first substantial evidence for the validity of the superstatistics framework as applied to cognitive models. Notably, our results demonstrated that the NSDDMs indeed reliably identified the sequence of two experimental manipulations, despite the absence of any contextual information. Moreover, posterior re-simulation revealed an outstanding fit to the response data, both on an aggregate level as well as on the level of the raw time series. This performance stands as compelling evidence supporting the validity of NSDDMs.</p><p>The trajectory of the drift rate parameter for all individuals closely mirrored the sequence of the task difficulty manipulation. Specifically, the drift rate parameter decreased when task difficulty increased, and conversely, increased as task difficulty decreased. This not only confirms the anticipated impact of the manipulation, but also highlights the NSDDMs' ability to discern these variations directly from the behavioral data, agnostic to additional contextual information.</p><p>Interestingly, drift rates increased throughout the experiment, although this was not the case for trials with the highest task difficulty. This observation suggests a practice effect among participants, where task performance generally improved with experience, except under the most challenging condition. Practice effects are a widely recognized phenomenon in various decision-making and memory paradigms <ref type="bibr" target="#b17">(Forstmann et al., 2008;</ref><ref type="bibr" target="#b25">Healey &amp; Kahana, 2014</ref><ref type="bibr" target="#b26">, 2016</ref><ref type="bibr" target="#b63">Wagenmakers et al., 2008;</ref><ref type="bibr" target="#b67">Wynton &amp; Anglim, 2017)</ref>. In fact, practice effects have been studied with various dynamic cognitive modeling approaches <ref type="bibr" target="#b14">(Evans et al., 2018;</ref><ref type="bibr" target="#b15">Evans &amp; Hawkins, 2019;</ref><ref type="bibr" target="#b24">Gunawan et al., 2022;</ref><ref type="bibr" target="#b28">Kahana et al., 2018)</ref>. A notable contribution to this field comes from <ref type="bibr" target="#b24">Gunawan et al. (2022)</ref>, who conducted a comprehensive re-analysis of three datasets derived from widely cited articles. Their study compared three dynamic models: (i) a smooth polynomial trend, (ii) a non-smooth autoregressive process, and (iii) a regime switching model instantiated by a hidden Markov model (HMM) with two different states.</p><p>In their study, <ref type="bibr" target="#b24">Gunawan et al. (2022)</ref> employed a low-level model similar to the DDM, namely the linear ballistic accumulator model (LBA; . However, their transition models, specifically the polynomial trend and the autoregressive process, differed in that they allowed LBA parameters to change only from block-toblock, neglecting trial-to-trial parameter fluctuations (except for the HMM). Their findings indicated that the HMM outperformed the other two dynamic model instantiations. This superiority can possibly be attributed to the model's capacity to flexibly change parameters from trial-to-trial, in contrast to changes occurring only from block-to-block. Even though the trial-by-trial specification of the HMM captures the microstructure of the decision-making process, it is still less flexible than the models we examined in the current study. HMMs assume a pre-defined number of possible states, whereas this is not the case with the implementation of our regime switching model. The advantage of not fixing the number of distinct states beforehand is particularly evident when the exact latent quantity is unknown prior to investigation. Moreover, results from our model comparison clearly favored transition models that account for both, gradual changes as well as sudden shifts. This suggests that regime-switching models may fall short in certain fields of application. Nevertheless, both models have their merits, and the choice between them should be guided by the specific research question at hand and formal model comparison.</p><p>As our study focused on experimentally validating parameter trajectories estimated with NSDDMs, we deliberately refrained from further analysing practice effects. However, we suggest that our flexible framework could be a promising alternative for investigating practice effects. Unlike pure regime switching models, it has the capacity to reveal a mixture of practice-related changes, ranging from abrupt shifts to gradual changes. When exploring substantive research questions, such as practice effects, with superstatistical models, it is imperative to depart from the approach taken in the current study. That is, one should always incorporate contextual information from the experimental setting when estimating parameter trajectories. Here the question arises, how to incorporate this information? In a previous study, we simply assumed separate low-level parameters for each experimental condition . This approach is particularly appropriate when conditions randomly change from trial to trial. However, future research could explore alternative ways of including experimental context information with the goal to further inform the parameters.</p><p>Concerning the second experimental manipulation, that is, the emphasis on speed or accuracy, their effect on the threshold parameter is more diverse across individuals. While a majority of participants demonstrated shifts in the threshold parameter in response to instructional changes, the consistency and magnitude of these changes varied significantly among individuals. Some participants exhibited only a few adjustments in the threshold parameters, seemingly overlooking the change in instruction on certain occasions. In contrast, others consistently heightened their threshold parameter during accuracy-focused tasks, followed by a subsequent decrease when transitioning to speed-oriented conditions. Meanwhile, some participants displayed rather unsystematic changes in decision caution, suggesting that the participants of reacted differently to the speed-accuracy manipulation. <ref type="bibr" target="#b30">Kucharský et al. (2021)</ref> introduced a dynamic LBA incorporating a hidden Markov transition model with two states, akin to the model proposed by <ref type="bibr" target="#b24">Gunawan et al. (2022)</ref>. Their focus centered on scrutinizing the speed-accuracy tradeoff, exploring the hypothesis that individuals dynamically switch between different operating states under varying instruction conditions. By fitting their model to previously collected data, they provided evidence that individuals tend to oscillate between two stable states: a deliberative, stimulus-driven mode emphasizing accuracy and sacrificing speed, and a guessing mode characterized by random and relatively faster choices.</p><p>However, our approach for estimating parameter trajectories reveals a more intricate scenario, challenging the assumed binary operational shift. Contrary to expectations, individuals manifest more than two discernible states. At times, they exhibit an extreme adaptation to a change in condition, while at other times, they display little or no reaction to the altered condition. This complexity underscores the necessity for more flexible transition models, as employed in our study. Failing to utilize such adaptive models could potentially obscure the complex unfolding of individuals' cognition and behavior over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparison</head><p>When implementing non-stationary models, a modeler encounters a myriad of options, ranging from various transition models to decisions about which parameter follows which transition model. In this study, we limited our choices to a small subset of the possibility space. Based on our experimental manipulations we anticipated that the DDM parameters, particularly the threshold parameter, would not only undergo gradual changes, but also manifest more abrupt shifts in response to changing conditions. Consequently, we tested different implementations accommodating such shifts (mixture random walk, Lévy flight, regime switching) against a transition model that does not, namely, the simple Gaussian random walk. The inferred posterior model probabilities (PMPs) and Bayes factors (BFs) consistently favored the Lévy flight and occasionally the mixture random walk transition models. However, in terms of the absolute goodness-of-fit, as assessed through posterior re-simulations, the performance of all four NSDDMs showed remarkable similarity. This leads to two notable conclusions. First, even the models with lower PMPs demonstrated a good fit to the data, likely owing to the inherent flexibility of the superstatistical framework. Second, our Bayesian model comparison method could reliably detect the most favorable model even when the absolute differences were marginal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Psychological research is usually interested in some group or overall estimate of parameters. Thus, it would have been informative to compute and inspect "average" parameter trajectories. Unfortunately, our experiment was designed in a way that the difficulty and the speed-accuracy instruction manipulation was randomized across participants. This made it impossible to average the individual trajectories directly. Instead, we collapsed the estimates by the different experimental conditions and provided an aggregate view across individuals. Although this is certainly a limitation of this study, we argue that the current analysis is sufficient to address our specific research question.</p><p>Moreover, despite using many default settings from the BayesFlow software <ref type="bibr" target="#b43">(Radev et al., 2023)</ref>, the configuration and training of neural approximators for both parameter inference and model comparison for non-stationary models can still be a challenge. A basic understanding of deep learning principles and simulation-based inference is an essential prerequisite. These requirements may pose obstacles to the adoption of our method, highlighting the necessity for improved software and tutorials addressing these intricacies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outlook</head><p>Going forward, we see the relevance of our superstatistics framework as twofold. First, superstatistics could become a powerful tool in the methodological toolkit of the researcher interested in temporal changes in cognitive constructs. It is a general framework and provides large flexibility. Thus far, we only used the DDM as a low-level observation model. However, there are many other cognitive process models that could benefit from such an framework. For instance, reinforcement learning model parameters, such as the learning rate or the softmax temperature parameter,likely change over time <ref type="bibr" target="#b33">(Li et al., 2023)</ref>. Second, even when the temporal evolution of cognitive parameters is not a central research question, the adoption of non-stationary models may bring advantages over their stationary counterparts . Our analysis of estimated trajectories vividly illustrates discernible changes in parameters. Assuming stationarity would have led to misleading substantive conclusions.</p><p>With great flexibility comes a great plethora of choices. In this study, we compared different transition models guided by the contrast between gradual and sudden changes. However, there are more degrees of freedom when implementing superstatistical models, or Bayesian models in general <ref type="bibr" target="#b19">(Gelman et al., 2020)</ref>. <ref type="bibr" target="#b10">Elsemüller, Olischläger, et al. (2023)</ref> advocates for the crucial role of sensitivity analysis, illustrating a potent methodology to facilitate informed decisions regarding factors such as the type and shape of prior distributions, neural network architectures, and other pivotal elements. We believe that using such an approach in the context of superstatistics could provide better guidelines for their implementation.</p><p>Up to this point, we focused on the estimates of the low-level parameter trajectories. Yet, it is crucial to note that we also obtain posterior distributions for the static high-level parameters. These estimates can also yield valuable insights into individuals' behavior and cognition. Depending on the chosen transition model, these estimates can offer indications of the frequency with which individuals transition between distinct operational states or the variability inherent in their cognitive constructs. Thus, analyzing these high-level parameters could constitute a compelling avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In conclusion, the experimental validation of non-stationary diffusion decision models presented in this study represents a significant step forward in the field of cognitive modeling. Our results provide compelling evidence that the estimated parameter trajectories genuinely reflect tangible changes in the targeted psychological constructs. We hope that our validation opens the door to widespread applications of non-stationary models in future modeling endeavors, offering a more nuanced understanding of cognitive processes across varying time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Appendix. Neural network architectures and training setups</head><p>In the following, we outline our implementation of the neural approximators and the training setup used for model comparison and parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model comparison</head><p>For model comparison we trained an ensemble of ten neural approximators. Each approximator consists of a summary network and an inference network. The summary network is a many-to-one transformer architecture for time series encoding <ref type="bibr" target="#b66">(Wen et al., 2023)</ref>. The time series transformer has 128 template and 64 summary dimensions. For inference, we use a network that approximates posterior model probabilities (PMPs) as employed in <ref type="bibr" target="#b11">Elsemüller, Schnuerch, et al. (2023)</ref>.</p><p>We performed offline training for each of the ten neural approximators separately. The training data consisted of 25 000 simulations per model. Training was performed with 25 epochs and a batch size of 16 starting with an initial learning rate of 0.0005. The learning rate was adjusted with a cosine decay from its initial value to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter estimation</head><p>For parameter estimation we trained one neural approximator for each of the four NSDDM implementations. Each approximator consists of a hierarchical summary network as employed in <ref type="bibr" target="#b11">Elsemüller, Schnuerch, et al. (2023)</ref> and two inference networks. Three bidirectional long-short term memory (LSTM) networks were used for the hierarchical summary network. The number of hidden units were 512, 256, and 128 respectively.</p><p>For inference, we use a composition of two invertible neural networks , one for the low-level and one for the high-level parameters. The network for the low-level parameters has 8 coupling layers with an interleaved affine and spline internal coupling design. The network for the high-level parameters only differs from the former in its number of coupling layers which is 6.</p><p>Since our simulators can be run fast, the training of the four neural approximators was performed online, with 75 epochs, 1 000 iterations per epoch, and a batch size of 16. Thus, each approximator was trained on N = 1 200 000 simulated data sets. The initial learning rate was set to 0.0005 and was reduced with a cosine decay function to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VALIDATION AND COMPARISON OF NON-STATIONARY DIFFUSION DECISION MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Appendix. Individual analyses</head><p>The following section shows the individual specific posterior re-simulations and parameter estimates for each difficulty level and both conditions separately. The visualizations are constructed in the vain of <ref type="figure" target="#fig_2">Figure 6</ref> in the main text.      <ref type="figure">Figure 13</ref>: Aggregate results from all models fitted to the data from participant 5. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.    <ref type="figure">Figure 17</ref>: Aggregate results from all models fitted to the data from participant 9. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.       From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Participant 3 Mixture random walk DDM <ref type="figure" target="#fig_1">Figure 25</ref>: Model fit to response time (RT) time series. The empirical RT time series of participant 3 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied <ref type="table">.   1  96  192  288  384  480  576  672  768   Trial   1   2   3   4</ref> Response time (s) Participant 7 Mixture random walk DDM <ref type="figure">Figure 28</ref>: Model fit to response time (RT) time series. The empirical RT time series of participant 7 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Response time (s) Participant 10 Lévy Flight DDM <ref type="figure">Figure 31</ref>: Model fit to response time (RT) time series. The empirical RT time series of participant 10 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. Participant 12 Mixture random walk DDM <ref type="figure">Figure 32</ref>: Model fit to response time (RT) time series. The empirical RT time series of participant 7 is shown in black. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in red. In this instance, the results stem from a mixture random walk DDM. For the remaining trials, one-step-ahead predictions are depicted in orange. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied. From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.  From trial 1 to 700, the posterior re-simulation (aka retrodictive check) using the best fitting non-stationary diffusion decision model (NSDDM) for this specific individual are shown in blue. In this instance, the results stem from a Lévy flight DDM. For the remaining trials, one-step-ahead predictions are depicted in cyan. Solid lines correspond to the median and shaded bands to 90% credibility intervals (CI). The empirical, re-simulated, and predicted RT time series were smoothed via a simple moving average (SMA) with a period of 5. The yellow shaded regions indicate trials where speed was emphasised over accuracy, while blank white areas denote instances where the opposite emphasis was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Appendix. Parameter trajectories</head><p>In the following, we present the inferred parameter trajectories for the remaining participants. For each visualisation the model with the highest posterior model probability for that specific individual was used.  <ref type="figure" target="#fig_1">Figure 35</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 1 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure" target="#fig_2">Figure 36</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 2 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 37</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 3 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 38</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 4 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 39</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 5 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 40</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 7 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 41</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 8 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 42</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 9 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 43</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 10 (a mixture random walk DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure">Figure 44</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 12 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure" target="#fig_1">Figure 45</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 13 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.  <ref type="figure" target="#fig_2">Figure 46</ref>: Posterior parameter trajectory inferred with the best fitting NSDDM of participant 14 (a Lévy flight DDM in this case) for all three DDM parameters (drift rate, threshold, and non-decision time) separately. The yellow shaded areas indicate trials where speed was emphasised over accuracy and blank white area indicated where the opposite was asked for. In the top panel, the task difficulty levels sequence is depicted in black lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Empirical model comparison results. A Aggregate posterior model probabilities (PMP) across the ensemble and the 14 individual participants. Points depict the mean, stars the median, and the error bars indicate the 75% credibility interval (CI). B Heatmap of average log 10 Bayes factors (BF). Both metrics agree on favoring the Lévy flight DDM over the other models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Aggregated results from all models fitted to the empirical data. The top row illustrates posterior resimulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated RTs for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individuals and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Aggregate results from all models fitted to the data from participant 1. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 2. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 3. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Aggregate results from all models fitted to the data from participant 4. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 :</head><label>14</label><figDesc>Aggregate results from all models fitted to the data from participant 6. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Aggregate results from all models fitted to the data from participant 7. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 8. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 18 :</head><label>18</label><figDesc>Aggregate results from all models fitted to the data from participant 10. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Aggregate results from all models fitted to the data from participant 11. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 12. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 21 :Figure 22 :</head><label>2122</label><figDesc>Aggregate results from all models fitted to the data from participant 13. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations. Aggregate results from all models fitted to the data from participant 14. The top row illustrates posterior re-simulations as a measure of the model's generative performance and absolute goodness-of-fit to the data. The bottom row depicts parameter estimates of the drift rate and the threshold parameter from the non-stationary diffusion decision models (NSDDM). A Empirical and re-simulated response times for each difficulty level and both conditions. B Empirical and re-simulated proportions of correct choices (accuracy) for each difficulty level and both conditions separately. C Posterior estimates of the drift rate parameter for each difficulty level and both conditions separately. D Posterior estimates of the threshold parameter for each difficulty level and both conditions separately. Points indicate medians and the error bars represent the median absolute deviations (MAD) across individual data and re-simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>S4Figure 23 :</head><label>23</label><figDesc>Appendix. Response time time seriesIn the following, we present the model fit to the whole response time time series for the remaining 12 participants. Model fit to response time (RT) time series. The empirical RT time series of participant 1 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 24 :</head><label>24</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 2 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 26 :</head><label>26</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 4 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 27 :</head><label>27</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 5 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 29 :</head><label>29</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 8 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 30 :</head><label>30</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 9 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 33 :</head><label>33</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 13 is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 34 :</head><label>34</label><figDesc>Model fit to response time (RT) time series. The empirical RT time series of participant 14 is shown in black.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> Note, that Schumacher et al. (2023)  focused exclusively on the filtering distribution in their benchmarking experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Steffen Ernst for his efforts in programming the experiment and collecting the data. L.S., M.S., and A.V. were supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; grant number GRK 2277 "Statistical Modeling in Psychology").</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>L.S. and M.S conceived the initial idea and the experimental design. L.S. and S.T.R. created and applied the models, and wrote the initial draft of the manuscript. A.V. supervised the project. All authors reviewed and refined the initial draft of the manuscript and agreed to its current version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Code Availability</head><p>All models, data, and scripts for reproducing the results of this work are publicly available in the project's GitHub repository https://github.com/bayesflow-org/Non-Stationary-DDM-Validation. The neural superstatistics method is implemented in the BayesFlow Python library for amortized Bayesian workflows <ref type="bibr" target="#b43">(Radev et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix S1 Appendix. Prior distributions</head><p>In the following we list the prior distributions we used for all four NSDDM's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDM Starting Values</head><p>For the starting values of the parameter trajectories we used half-normal distributions with a mean µ and a standard deviation σ denoted as HN (µ, σ):</p><p>Half-normal distributions were used for the standard deviations of the Gaussian random walk transition model:</p><p>We decided to use a relatively narrower prior on σ τ because the non-decision time parameter is not expected to fluctuate as heavily as the other two parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture Random Walk Transition Model</head><p>The mixture random walk transition model used the same prior for the Gaussian random walk as described above. Additionally, Uniform distributions denoted as U were used for the mixture proportion parameter ρ:</p><p>The Levy flight transition model uses an alpha stable distribution instead of a Gaussian distribution for the transition. We used the same priors for the standard deviations as in the random walk and the mixture random walk. The alpha stable distribution has an additional parameter α, which determines the fatness of the tails. This parameter is bound between 1 and 2. Therefore, we used a Beta distribution denoted as B and added 1 to the sampled values:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regime Switching Transition Model</head><p>The same prior distributions as for the mixture random walk were used for the mixture probabilities of the regime switching transition model.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wirkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Klessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04730</idno>
		<title level="m">Analyzing inverse problems with invertible neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical validation of the diffusion model for recognition memory and a comparison of parameter-estimation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">J</forename><surname>Bayen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="882" to="898" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G D</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Superstatistics. Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
		<title level="m">Superstatistics: Theory and applications. Continuum mechanics and thermodynamics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="293" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the proper treatment of dynamics in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Beer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The simplest complete model of choice response time: Linear ballistic accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An integrated model of choices and response times in absolute identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A J</forename><surname>Marley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="396" to="425" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple timescales of learning indicated by changes in evidence-accumulation processes during perceptual decision-making. npj Science of Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cochrane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Bejjanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bavelier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes Models for Response Time Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Craigmile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peruggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="632" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The frontier of simulation-based inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="30055" to="30062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elsemüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olischläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<title level="m">Sensitivity-Aware Amortized Bayesian Inference</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A deep learning method for comparing bayesian hierarchical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elsemüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schnuerch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Models of sustained attention. Current opinion in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Esterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothlein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="174" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">People adopt optimal policies in simple decision-making, after practice and guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Refining the law of practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Mewhort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="592" to="605" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">When humans behave like monkeys: Feedback delays and extensive practice increase the efficiency of speeded decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A reinforcement learning diffusion decision model for value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fontanesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Spektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1099" to="1121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Striatum and pre-SMA facilitate decision-making under time pressure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dutilh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Von Cramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Ridderinkhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="17538" to="17542" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Capturing the non-stationarity of whole-brain dynamics underlying human brain states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Galadí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silva Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sanz Perl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kringelbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gayte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laufs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tagliazucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Langa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="page">118551</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Margossian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modrák</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01808</idno>
		<title level="m">Bayesian workflow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training deep neural density estimators to identify mechanistic models of neural dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lueckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deistler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nonnenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Öcal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bassetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chintaluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Podlaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Vogels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Elife</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">56261</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic posterior transformation for likelihood-free inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nonnenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2404" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A tutorial on bridge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">F</forename><surname>Gronau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarafoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steingroever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical psychology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="80" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time-evolving psychological processes over repeated decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">438</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Is memory search governed by universal principles or idiosyncratic strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="575" to="596" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A four-component model of age-related memory change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="69" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of applications of the Bayes factor in psychological research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Böing-Messing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dienes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A L</forename><surname>Kiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Klugkist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Leplaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meijerink-Bosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moerbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Hoijtink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="579" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The variability puzzle in human memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1857" to="1863" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayes Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hidden Markov Models of Evidence Accumulation in Speeded Decision Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Š</forename><surname>Kucharský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raijmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Visser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="416" to="441" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speed-accuracy manipulations and diffusion modeling: Lack of discriminant validity of the manipulation or of the parameter estimates?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lerche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2568" to="2585" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Experimental validation of the diffusion model based on a slow response time paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lerche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1194" to="1209" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dynamic noise estimation: A generalized method for modeling noise in sequential decision-making behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note>bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian model selection for complex dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Metzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lautscham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Strissel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1803</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mcdougle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="39" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mejias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Longtin</surname></persName>
		</author>
		<title level="m">Nonstationary Stochastic Dynamics Underlie Spontaneous Transitions between Active and Inactive Behavioral States</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A new model of decision processing in instrumental learning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miletić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Trutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
		<editor>V. Wyart, J. I. Gold, &amp; J. W. de Gee</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">63055</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling the dynamics of recognition memory testing with an integrated model of retrieval and decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Osth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="106" to="142" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2617" to="2680" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Amortized Bayesian Model Comparison With Evidential Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>D'alessandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">K</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4903" to="4917" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesflow: Learning complex stochastic models with invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">K</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1452" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesflow: Amortized bayesian workflows with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elsemüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schälte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">89</biblScope>
			<biblScope unit="page">5702</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling Response Times for Two-Choice Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="438" to="481" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diffusion model for one-choice reaction-time tasks and the cognitive effects of sleep deprivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P A</forename><surname>Van Dongen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="11285" to="11290" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Säilynoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bayesian Filtering and Smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Toward a principled bayesian workflow in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural superstatistics for Bayesian estimation of dynamic cognitive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13778</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Duration discrimination: A diffusion decision modeling approach. Attention, Perception, &amp; Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="560" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A time-varying dynamic partial credit model to analyze polytomous and multivariate time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Castro-Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R M</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tendeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Validating Bayesian Inference Algorithms with Simulation-Based Calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Talts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A review of issues about null hypothesis Bayesian testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tendeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A L</forename><surname>Kiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="774" to="795" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-organization of cognitive performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Van Orden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Turvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="350" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Advantages masquerading as &quot;issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Ravenzwaaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian hypothesis testing: A commentary on Tendeiro and Kiers</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="451" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Leaky Integrating Threshold and its impact on evidence accumulation models of choice response time (RT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdonck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Loossens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Philiastides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="221" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Diffusion models in experimental psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lerche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental psychology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Interpreting the parameters of the diffusion model: An empirical validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rothermund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1206" to="1220" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Estimation and interpretation of 1/f α noise in human cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="615" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A diffusion model account of criterion shifts in the lexical decision task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="159" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Computational cognitive modeling of the temporal dynamics of fatigue from sleep loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gunzelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P A</forename><surname>Van Dongen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1785" to="1807" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The neural bases of momentary lapses in attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Visscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Woldorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="978" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Transformers in Time Series: A Survey</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Abrupt strategy change underlies gradual performance change: Bayesian hierarchical models of component and aggregate strategy use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K A</forename><surname>Wynton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anglim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1630" to="1642" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
