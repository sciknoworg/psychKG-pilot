<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Newly learned novel cues to location are combined with familiar cues but not always with each other</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacey</forename><surname>Aston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrik</forename><surname>Beierholm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Nardini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Newly learned novel cues to location are combined with familiar cues but not always with each other</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cue combination</term>
					<term>sensory integration</term>
					<term>sensory augmentation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Mature perceptual systems can learn new arbitrary sensory signals (novel cues) to properties of the environment, but little is known about the extent to which novel cues are integrated into normal perception. In normal perception, multiple uncertain familiar cues are combined, often nearoptimally (reliability-weighted averaging), to increase perceptual precision. We trained observers to use abstract novel cues to estimate horizontal locations of hidden objects on a monitor. In Experiment 1, four groups of observers each learned to use a different novel cue. All groups benefitted from a suboptimal but significant gain in precision using novel and familiar cues together after short-term training (3 x ~1.5 hour sessions), extending previous reports of novel-familiar cue combination. In Experiment 2, we tested whether two novel cues may also be combined with each other. One pair of novel cues could be combined to improve precision but the other could not, at least not after three sessions of repeated training. Overall, our results provide extensive evidence that novel cues can be learned and combined with familiar cues to enhance perception, but mixed evidence for whether perceptual and decision-making systems can extend this ability to the combination of multiple novel cues with only short-term training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>A mature perceptual system can learn new mappings between arbitrary sensory signals and properties of the environment (novel cues), such as an artificial correlation between the brightness and stiffness of an object <ref type="bibr" target="#b11">(Ernst, 2007)</ref> or an auditory cue to depth <ref type="bibr" target="#b40">(Negen, Wen, Thaler, &amp; Nardini, 2018)</ref>, among others (Di Luca, <ref type="bibr" target="#b9">Ernst, &amp; Backus, 2010;</ref><ref type="bibr" target="#b20">Haijiang, Saunders, Stone, &amp; Backus, 2006;</ref><ref type="bibr" target="#b22">Harrison &amp; Backus, 2012;</ref><ref type="bibr" target="#b35">Michel &amp; Jacobs, 2008)</ref>. However, little is known about the extent to which novel cues are integrated into the normal perceptual experience. In normal perception, there are often multiple uncertain familiar sensory cues (natural mappings between sensations and physical properties of the surrounding environment) providing similar information about the state of the surrounding world, such as disparity and texture cues to the slant of a surface <ref type="bibr" target="#b29">(Knill &amp; Saunders, 2003</ref>). An important feature of familiar cue use is that when multiple cues are available, rather than throwing one piece of information away and using only the most reliable cue, a mature perceptual system tends to combine the cues in line with reliability-weighted averaging -the Bayes-optimal solution to cue combination that maximises precision <ref type="bibr" target="#b1">(Alais &amp; Burr, 2004;</ref><ref type="bibr" target="#b12">Ernst &amp; Banks, 2002;</ref><ref type="bibr" target="#b24">Hillis, Watt, Landy, &amp; Banks, 2004;</ref><ref type="bibr" target="#b29">Knill &amp; Saunders, 2003)</ref>.</p><p>A limited number of studies suggest newly learned novel cues are also combined with familiar cues <ref type="bibr" target="#b11">(Ernst, 2007;</ref><ref type="bibr" target="#b16">Gibo, Mugge, &amp; Abbink, 2017;</ref><ref type="bibr" target="#b35">Michel &amp; Jacobs, 2008;</ref><ref type="bibr" target="#b40">Negen et al., 2018)</ref>. Importantly, although combination of novel and familiar cues is often suboptimal, with the gain in precision from combining the two cues less than that predicted by reliability-weighted averaging <ref type="bibr" target="#b11">(Ernst, 2007;</ref><ref type="bibr" target="#b16">Gibo et al., 2017;</ref><ref type="bibr" target="#b40">Negen et al., 2018)</ref>, it is "Bayes-like" in the sense that it shows some signatures of Bayes-optimal combination, such as weighting by reliability <ref type="bibr" target="#b40">(Negen et al., 2018)</ref>.</p><p>The ability to learn novel cues and combine them with familiar cues has vast applications for sensory substitution and augmentation. In the case of sensory substitution, it means that perceptual systems receiving disrupted familiar cues (for example, in partial vision loss) could not only learn to replace the disrupted input with a novel cue <ref type="bibr" target="#b3">Auvray, Hanneton, &amp; O'Regan, 2007;</ref><ref type="bibr" target="#b4">Bach-y-Rita, Collins, Saunders, White, &amp; Scadden, 1969;</ref><ref type="bibr" target="#b33">Maidenbaum et al., 2014)</ref>, but could combine the novel cue with disrupted familiar cues to make more precise judgements than using either cue alone would allow. Similarly, in the case of a healthy perceptual system, novel cues can be introduced to enhance the normal perceptual experience. New technologies offer a variety of options for providing perceptual systems with new sensory signals. To make the best use of these technologies, the design of new sensory signals should be grounded in research that explores which novel cues are most efficiently learned and combined with familiar or 4 other novel cues, as well as the training conditions that best promote integration of new sensory signals into the normal perceptual experience.</p><p>Here, we asked whether observers combine novel and familiar cues to increase precision above what is possible using the most reliable single cue alone, and how any such gains in precision differ from the optimal or maximum gain predicted by reliability-weighted averaging. In Experiment 1, we trained observers to use abstract novel cues to estimate the horizontal location of hidden objects on a computer screen. The novel cues were the colour of a pair of lines (colour cue), the angle between two lines (the angle cue), the axis ratio of an oval (the shape cue), and the height of a bar (the height cue). We refer to our novel cues as abstract as they do not have a natural relationship to location.</p><p>This contrasts with previous studies where observers learned to use an echolocation cue to make depth judgements <ref type="bibr" target="#b40">(Negen et al., 2018)</ref> or made movements with the assistance of a force cue that guided movements in a particular direction <ref type="bibr" target="#b16">(Gibo et al., 2017</ref>).</p><p>Observers completed a task that began with a short training period to teach (or reinforce) the mapping between the novel cue and location. After training, observers completed a series of trials where they were required to use either the novel cue, a familiar cue (e.g., a noisy dot-cloud), or the novel and familiar cues together to estimate the location of a hidden object. Forty observers were divided into equal groups so that each observer learned only one novel cue with each observer completing the same task on three different days (three sessions). This aspect of the design provided the observers with repeated training, allowing them not only to learn the mappings to location over time, but also to learn to discriminate finer differences in the novel cues (i.e. perceptual learning -an improvement in discrimination ability for a stimulus (cue) that was not previously well discriminated; <ref type="bibr" target="#b13">Fahle &amp; Poggio, 2002)</ref>. We considered that it was important to allow for perceptual learning as single cue reliabilities may be changing as discrimination ability improves, and changing cue reliabilities could be a barrier to reliability-weighted averaging and Bayes-like combination <ref type="bibr" target="#b1">(Alais &amp; Burr, 2004;</ref><ref type="bibr" target="#b12">Ernst &amp; Banks, 2002;</ref><ref type="bibr" target="#b24">Hillis et al., 2004;</ref><ref type="bibr" target="#b29">Knill &amp; Saunders, 2003)</ref>.</p><p>Each group of observers in Experiment 1 benefitted from a gain in precision using the novel and familiar cues together by the third session. The gain in precision was suboptimal but significant; location estimates were significantly less variable when both the novel and familiar cues were available than when observers used their best single cue alone. Our results show that observers can learn abstract novel cues to location and combine them with a familiar cue.</p><p>In Experiment 2, we tested if two novel cues may also be combined with each other. We tested this by teaching two different groups, each of ten observers, a different pairing of the abstract novel cues to location from Experiment 1 (the colour and angle cues or the colour and shape cues). In this experiment, observers received separate training with each novel cue. After training they completed a series of trials where they used either one of the novel cues, both novel cues, the familiar cue, or one of the novel cues and the familiar cue to estimate the location of the hidden object. As in Experiment 1, each observer completed the task three times on three different days. We found that one pair of novel cues could be combined to improve precision but the other could not, even after three sessions of repeated training.</p><p>Overall, our results provide extensive evidence that novel cues can be learned and combined with familiar cues to enhance perception, but mixed evidence for whether perceptual and decisionmaking systems can extend this ability to the combination of multiple novel cues with only shortterm training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Forty observers completed the same task three times on three different days (three sessions). The task required the observers to use a novel cue, a familiar cue, or the novel and familiar cues simultaneously to estimate the location of a hidden target by using a computer mouse to adjust the horizontal position of a bar on a computer screen. The task began with a block of training trials that taught observers the mapping between a novel cue and horizontal location on the screen. The forty observers were split into four groups of ten with each group learning a different novel cue to location <ref type="figure">(Figure 1</ref>). The colour group learned to use the average colour of eight pairs of lines as a cue to location (the colour cue), the angle group learned to use the average size of the angle between eight pairs of lines as a cue to location (the angle cue), the shape group learned to use the average axis ratio of eight ovals as a cue to location (the shape cue), and the height group learned to use the average height of eight vertical bars as a cue to location (the height cue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>Figure 1: The task in Experiment 1. (A-B) The task began with a block of training trials where observers were taught a mapping between a novel cue (colour, angle size, the axis ratio of an oval, or the height of a bar) and horizontal location on a computer screen. In the first set of training trials (A), observers could see the novel mapping on the screen and had to select the location along the mapping that corresponded to the average novel cue value of eight stimuli shown at the bottom of the screen. The direction of the mapping was randomly chosen for each observer. In the second set of training trials (B), the mapping was not shown but observers could continue to learn the mapping through feedback. (C) In test trials, observers used either the newly learned novel cue, a familiar spread cue (e.g., a dot cloud), or both the novel and familiar cue together to estimate the position of a hidden object (an octopus hiding in the sea). (D) After issuing a response by positioning a vertical bar horizontally across the screen, observers received feedback and, if they "caught" the octopus, saw an animation of the octopus moving into their bucket.</p><p>In the training block, observers first completed a set of trials where the mapping between the novel cue and location was shown on the screen ( <ref type="figure">Figure 1A</ref>). In these "with mapping" trials, the novel cue was presented at the bottom of the screen and observers were required to estimate the average colour, angle size, axis ratio, or height of the cue, indicating their response by moving a vertical bar to the correct location along the mapping. Observers then completed a set of "without mapping" trials ( <ref type="figure">Figure 1B</ref>) that encouraged them to learn the relationship between the cues and location as the mapping was no longer shown. Learning of the mapping was reinforced through feedback in these trials, with observers shown the correct average colour, angle size, axis ratio, or height in the correct location as feedback. The direction of the mapping (left-to-right or right-to-left) on the screen was randomly determined for each observer.</p><p>After observers completed the training block, the test trials began ( <ref type="figure">Figure 1C</ref>). At the start of the test block, observers were instructed that they would now begin to use the newly learnt novel cue, along with a familiar cue (e.g., a dot-cloud, or the spread cue) to estimate the location of a hidden objectan octopus hiding in the sea. On each trial, observers were presented with either the novel cue (colour-only, angle-only, shape-only, or height-only trials), the familiar cue (spread-only trials), or the novel and familiar cue together (colour-spread, angle-spread, shape-spread, or height-spread trials).</p><p>In colour-only and angle-only trials, observers were presented with eight pairs of lines (in fixed positions) at the bottom of the screen. The average colour of the pair of lines or angle between them provided a novel estimate of location according to a trained mapping. In shape-only trials observers were presented with eight ovals (in fixed positions) at the bottom of the screen. The average vertical to horizontal axis ratio of the ovals provided a novel estimate of location according to a trained mapping. In height-only trials observers were presented with eight vertical bars (in fixed positions) at the bottom of the screen. The average height of the vertical bars provided a novel estimate of location according to a trained mapping. In spread-only trials, eight pairs of parallel and grey lines (colour and shape groups), grey squares (shape group), or grey circles (height group) were spread out across the screen. The position of each pair of lines, square, or circle was drawn from a Gaussian distribution, centred on the hidden location, such that the mean or centroid of the locations was the best estimate. In colour-spread or angle-spread trials, the eight pairs of lines were spread across the screen and had the property of the novel cue (either the relevant colours or angles between the lines). In shape-spread trials the eight ovals were spread across the screen and had the property of the novel cue (the relevant axis ratios). In height-spread trials the eight bars were spread across the screen and had the property of the novel cue (the relevant bar heights).</p><p>Trials of all types were interleaved for each group (e.g., colour-only, spread-only, and colour-spread for the colour group). After the cue(s) appeared on each trial, observers adjusted the horizontal position of a vertical line, using a mouse, to their best guess of the hidden location ( <ref type="figure">Figure 1D</ref>).</p><p>Feedback was given indicating if the observers had "caught" the octopus along with an indicator of the true hidden location that displayed the corresponding novel cue values (the correct average colour, angle size, axis ratio, or height). If the octopus was caught, an animation showed the octopus move across the screen from its hidden location to the bucket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observers</head><p>Forty observers were recruited using Durham Psychology Department's Participant Pool programme or through word of mouth. Each observer was assigned to either the colour group, angle group, shape group, or height group such that there were ten observers in each group (colour group: 7 female, age range 19-29 years; angle group: 8 female, age range 19-27 years; shape group: 9 female, age range 18-42 years; height group: 8 female, age range 18-21 years). All observers had normal or corrected to normal visual acuity (self-report) and no colour vision deficiencies (assessed using Ishihara Colour Plates). Each observer was given either £8 per hour or participant pool credits for their time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>Stimuli were shown on a 10-bit ASUS Proart LCD screen (ASUS, Fremont, CA) with observers seated so that their eyes were approximately 60 cm from the screen. The monitor was controlled using a 64-bit Windows machine, equipped with an NVIDIA Quadro K600 10-bit graphics card (NVIDIA, Santa Clara, CA), running MATLAB scripts that used Psychtoolbox routines <ref type="bibr" target="#b7">(Brainard, 1997;</ref><ref type="bibr" target="#b28">Kleiner, Brainard, &amp; Pelli, 2007;</ref><ref type="bibr" target="#b42">Pelli, 1997)</ref>. The stimuli were colourimetrically calibrated using a linearized calibration table based on measurements of the monitor primaries made with a Konica Minolta CS2000 spectroradiometer (Konica Minolta, Nieuwegein, Netherlands). Conversions to CIELUV used the measured white point of the monitor: ( , , ) = (205.24, .31, .34) in CIE 1931 colour space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>In colour-only trials, the novel colour cue appeared in a fixed location at the bottom of the screen. object's location with a standard deviation of 3 pixels. The colours of the dots were then taken to be the colours that corresponded to each of the sampled locations according to the mapping. In the training trials, the mapping was shown on the screen as a colour gradient.</p><p>In angle-only trials, the novel angle size cue appeared in a fixed location at the bottom of the screen.</p><p>This cue was eight pairs of lines (length 24, width 5 pixels) where each pair formed an angle. Angles were always formed in either only the 1 st or across both the 1 st and 2 nd quadrants such that one of the lines forming the angle was always the abscissa in the 1 st quadrant. The size of the angle formed by each pair of lines was dictated by a pre-defined mapping of angle size to screen position. Angle sizes of 67.95° and 162.45° corresponded to 15% and 85% of the way across the screen, respectively, or vice versa (flipped at random for each observer). To set the angle sizes on each trial, eight horizontal positions were drawn from a Gaussian distribution centred on the hidden object's location with a standard deviation of 0.7 pixels. The angle sizes were then taken to be those that corresponded to each of the sampled locations according to the mapping. In the training trials, the angles corresponding to locations 17% to 85% of the way across the screen in steps of 4% were shown across the screen at their correct locations. On angle-only trials, the angles were always grey, as were the angles shown as part of the mapping. On colour-angle trials, each angle was also assigned a colour by the same method as the colour-only cue.</p><p>In shape-only trials, the novel shape cue appeared in a fixed location at the bottom of the screen.</p><p>The novel shape cue was a set of eight ovals. The ratio of the vertical ( ) to horizontal ( ) axis varied for each oval, while maintaining the total area, and was defined based on a mapping of axis ratio to location across the screen. A location 15% of the way across the screen, from left to right, corresponded to a ratio of ⁄ = 12.191 22.979 ⁄ , while 85% of the way across the screen corresponded to ⁄ = 22.979 12.191 ⁄ pixels, or vice versa (flipped randomly for each observer).</p><p>To set the ratio for each oval, eight horizontal positions were drawn from a Gaussian distribution centred on the hidden object's location with a standard deviation of 0.7 pixels. The ratios were then taken to be those that corresponded to each of the sampled locations according to the mapping. In the training trials, only the shapes corresponding to locations 17% to 85% of the way across the screen in steps of 4% were shown. When the novel shape cue was paired with the familiar spread cue, the eight symbols representing the shape cue were spread across the screen.</p><p>In height-only trials, the novel bar height cue appeared in a fixed location at the bottom of the screen. The novel bar height cue was a set of eight vertical bars (width 5 pixels) whose heights varied. The heights were decided according to a mapping of bar height to screen position. A height of 8.69 pixels corresponded to 15% of the way across the screen, from left to right, and a length of 30.82 pixels to 85%, or vice versa (flipped randomly for each observer). To set the height of each bar, eight horizontal positions were drawn from a Gaussian distribution centred on the hidden object's location with a standard deviation of 0.2 pixels. The heights of the bars were then taken to be those that corresponded to each of the sampled locations according to the mapping. In the training trials, the mapping was shown on the screen as a truncated 2D cone with the height of the cone at each location corresponding to the bar height that mapped there. When the novel bar height cue was paired with the familiar spread cue, the eight symbols representing the bar height cue were spread across the screen.</p><p>In spread-only trials the familiar cue appeared on the screen. The familiar cue was effectively a "dot" cloud generated by drawing the position of each "dot" from a Gaussian distribution centred on the hidden object's location with a standard deviation of 237 pixels and were scaled so that the standard deviation of the eight sampled locations matched the population standard deviation. However, we only displayed a dot at each location for the height group. In height-spread trials, the height group saw eight bars of varying heights spread across the locations. For the colour group and angle group, in spread-only trials, we displayed a pair of parallel vertical lines at each location. In spread-only trials for the colour and angle groups, the pairs of lines were all grey. In colour-spread and anglespread trials the pairs of lines spread across the screen were each assigned a colour by the same method as the colour-only cue or an angle size by the same method as the angle-only cue, respectively. In spread-only trials for the shape group, we displayed a grey square at each location. In shape-spread trials, eight ovals with varying axis ratios were shown at the different locations.</p><p>We used location estimation, with the spread of the stimuli being the familiar cue, as a framework to test for novel-familiar combination as this framework has been used multiple times to test the perceptual system's ability to learn novel stimulus distributions, or location priors <ref type="bibr" target="#b5">(Bejjanki, Knill, &amp; Aslin, 2016;</ref><ref type="bibr" target="#b8">Chambers, Sokhey, Gaebler-Spira, &amp; Kording, 2018;</ref><ref type="bibr" target="#b27">Kiryakova, Aston, Beierholm, &amp; Nardini, 2020;</ref><ref type="bibr" target="#b32">Körding &amp; Wolpert, 2004;</ref><ref type="bibr" target="#b45">Tassinari, Hudson, &amp; Landy, 2006;</ref><ref type="bibr" target="#b47">Vilares, Howard, Fernandes, Gottfried, &amp; Kording, 2012)</ref>. Those studies suggest that the spread of stimuli is an intuitive familiar cue to location that observers readily understand and can flexibly weight in relation to the mean of a novel location prior. We expect this to extend to combination with a novel cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Parameters</head><p>In the training block there were two repeats of each of 36 possible hidden locations (15% to 85% of the way across the screen from left to right, sampled every 2%) for both the "with mapping" and "without mapping" trials (72 trials of each type). In the test block, the same 36 unique hidden locations were used, with each repeated five times for each trial type (e.g., colour-only, spread-only, and colour-spread for the colour group; 180 trials each). Trials of all types were interleaved and presented in a random order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>Any response that was issued less than 500 ms after presentation of the cue(s) was considered a lapse and excluded from analysis. To check that observers could use the cue(s), we calculated the correlation coefficient between the responses and the hidden location for each trial type (e.g., colour-only, spread-only, and colour-spread for the colour group) and for each observer within each session. Our a priori learning criteria were as follows. If ≥ 0.7 (Pearson's correlation) for all trial types within a session for a given observer, we conclude that the observer learned to use the cue(s) and they are included in all analyses including data from that session. However, if &lt; 0.7 for any trial type in a session, we conclude that the observer did not learn to use the cue(s) well enough, and they are excluded from analyses involving that session.</p><p>Our main research questions were: (1) do observers combine the novel and familiar cues to increase precision above what is possible using the most reliable single cue alone, and (2) if so, does the gain in precision using both cues compared to the best single cue differ from the optimal or maximum gain predicted by reliability-weighted averaging? Thus, our main measure of interest is precision or, equivalently, variability. We calculate measures of variability according to a method we recently described elsewhere <ref type="bibr" target="#b2">(Aston, Negen, Nardini, &amp; Beierholm, 2021)</ref>. The method is designed to account for central biases in continuous responses that may reduce statistical power for detecting a gain in precision using multiple cues. To calculate measures of variability according to the method, we regress responses for each trial type on the true hidden object locations and calculate the standard deviation of the residuals. If the slope of the fitted regression line is significantly less than one, the standard deviation of the residuals is divided by the fitted slope of the regression line to correct for a central bias. Importantly, if there is no evidence of a central bias (the slope is not significantly less than one), no correction is performed. The mean strengths of the central bias for each trial type in the third session of each task (averaged across sessions and observers) were: colour-only = 0.04, angle-only = 0.06, shape-only = 0.05, height-only = 0.1, spread-only (colour group) = 0.07, spread-only (angle group) = 0.07, spread-only (shape group) = 0.08, spread-only (height group) = 0.08, colour-spread = 0.04, angle-spread = 0.02, shape-spread = 0.03, and height-spread = 0.04.</p><p>We will refer to our measures of variability as variable error. Our second main research question requires the comparison of variable error using both cues to the optimal prediction under the assumption of reliability-weighted averaging. Given variable errors for two single cues, 1 and 2 , we can predict the optimal variable error using both cues, , using the equation below <ref type="bibr" target="#b12">(Ernst &amp; Banks, 2002</ref>).</p><formula xml:id="formula_0">2 = 1 2 2 2</formula><p>( 1 2 + 2 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pilot Experiment and Power Analysis</head><p>Five observers (4 female, age range 18-24 years) completed a pilot experiment using the novel colour cue to location. By the third session of the experiment, all five observers issued less variable (more precise) responses in the novel-familiar cue trials compared to trials where they used their most reliable cue alone. The mean reduction in variable error in the third session (in terms of screen proportion) was 0.013 with standard deviation 0.013. Based on this pilot data, we used G*Power <ref type="bibr" target="#b14">(Faul, Erdfelder, Lang, &amp; Buchner, 2007)</ref> to calculate the statistical power that different sample sizes would allow for our most important research question: do observers issue less variable (more precise) responses using the novel and familiar cues together compared to the most reliable, or best, single cue. We planned to address this question by comparing variable error using the best single cue to variable error using the novel and familiar cues together using a one-tailed Wilcoxon signedrank test. Based on the pilot data, we required 9 participants for 80% power. We chose to recruit ten observers for each novel cue type in the main experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices Statement</head><p>This experiment was not pre-registered. The raw data files and analysis script are available online at https://osf.io/gj92a/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Results</head><p>Each row of plots in <ref type="figure">Figure 2</ref> shows the data that pertains to a single group of observers. The top row shows data from the colour group, the second row is the angle group, the third is the shape group, and the bottom row is the height group. The left panel of plots shows variable error using the familiar and novel cues alone across sessions <ref type="figure">(Figure 2A-D)</ref>. These plots show that variable error using the familiar cue is stable across sessions for all groups of observers but that some groups get better using the novel cue with increased training and exposure to the task. The right panel of plots shows variable error in each session using the worst of the two single cues (highest variable error), the best of the two single cues (lowest variable error), both cues together, and the optimal variable error using both cues together that would be achieved by taking a reliability-weighted average of estimates from the two single cues <ref type="figure">(Figure 2E-H)</ref>. A visual inspection of <ref type="figure">Figure 2E</ref>-H shows lower median variable error using both cues together than the best single cue in all groups by the third session of the experiment, suggesting all groups of observers combined the newly learned novel cue with the familiar cue. However, the median variable errors using both cues are all higher than the optimal variable error from reliability-weighted averaging, suggesting that combination of novel and familiar cues was still suboptimal. Observers quickly learned to use the novel cues, and variability using the cues decreased with repeated training and exposure to the task Thirty-eight of thirty-nine observers passed the a priori learning criteria in the first session of the experiment and each following session. To pass the learning criteria, an observer was required to show a correlation coefficient greater than 0.7 between their responses and the hidden target locations for each trial type. One observer's data from the first session (in the shape group) was lost as the computer crashed while the data was saving. That observer passed the learning criteria in both subsequent sessions. The remaining observer (in the angle group) also passed the learning criteria in the second and third sessions. Thus, observers quickly learned the mappings between the novel cues and location and could use the novel cues to complete the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>We were interested in whether the observers' performance changed over the sessions as they gained more practice with the novel cues. To address this question, we performed a Friedman's Test to compare variable errors over time (IV: session) for each group separately. We used a Friedman's</p><p>Test as variable errors were not normally distributed and, as the test relies on ranking the data rather than absolute values, does not depend on the measure of variable error that we use (we chose to use standard deviation, but could have used variance instead, leading to increased absolute differences between conditions). Both the angle group and height group significantly reduced their variable error over time using the novel cues (angle group: 2 (2) = 10.4, = .006, <ref type="figure">Figure 2B</ref>;</p><p>height group: 2 (2) = 8.6, = .014, <ref type="figure">Figure 2D</ref>). Variable error using the angle size cue significantly decreased from sessions one to three ( = 54, = .004) and two to three ( = 53, = .006) in the angle group. Variable error using the bar height cue significantly decreased from sessions one to two ( = 51 = .014) for the height group. There was no change in variable error using the novel cue over time for the colour or shape groups (colour group: 2 (2) = 1.4, = .497, <ref type="figure">Figure 2A;</ref> shape group: 2 (2) = 2.89, = .236, <ref type="figure">Figure 2C</ref>); although we note that the median variable error reduces from 0.084 in session one to 0.064 in session three for the shape group with the lack of significance likely caused by the outlier values in sessions two and three ( <ref type="figure">Figure 2C</ref>).</p><p>Variable error using the familiar spread cue did not change over time for any group of participants (colour group: 2 (2) = 1.4, = .497, <ref type="figure">Figure 2A</ref>; angle group: 2 (2) = 1.4, = .497, <ref type="figure">Figure 2B;</ref> shape group: 2 (2) = 4.67, = .0.97, <ref type="figure">Figure 2C</ref>; height group: 2 (2) = 2.4, = .301, <ref type="figure">Figure 2D</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel cues were combined with the familiar cue by, at most, the third session, but combination was often suboptimal</head><p>Recall that our main research questions were: (1) do observers combine the novel and familiar cues to increase precision above what is possible using the most reliable single cue alone, and (2) if so, does the gain in precision using both cues compared to the best single cue differ from the optimal or maximum gain predicted by reliability-weighted averaging? To answer (1), we performed a onetailed Wilcoxon Signed-Rank test comparing variable error with the best of the novel and familiar cues to performance with both cues together for each group in each session of the experiment. If variable error using both cues was significantly less than variable error using the best single cue, we conclude that the observers in that group and session showed evidence of combination (green dagger and lines in <ref type="figure">Figure 2</ref>). To answer (2), we performed a two-tailed Wilcoxon Signed-Rank test comparing variable error using both cues to the optimal prediction (calculated from measured variable error using each single cue alone). If variable error using both cues differed significantly from the optimal prediction, we concluded that the observers in that group and session were, on the hole, sub-optimal (red double dagger and lines in <ref type="figure">Figure 2</ref>). If not, we conclude that they optimally combined the novel and familiar cues.</p><p>In the first session, only the colour group showed evidence of combination and all groups were suboptimal (rows 1-4 of <ref type="table" target="#tab_1">Table 1</ref>; third column of plots in <ref type="figure">Figure 2</ref>). In the second session, all except the height group showed evidence of combination, but all groups remained suboptimal (rows 5-8 of <ref type="table" target="#tab_1">Table 1</ref>; forth column of plots in <ref type="figure">Figure 2</ref>). In the third session, all groups showed evidence of combination, with only the angle and shape groups remaining suboptimal (rows 9-12 of <ref type="table" target="#tab_1">Table 1</ref>; fifth column of plots in <ref type="figure">Figure 2</ref>). In Experiment 1, we showed that observers can combine newly learned novel cues (colour, angle size, shape, and the height of a bar) to horizontal location with a familiar cue (a dot cloud) to improve location estimate precision. Variable error using the novel cues alone decreased across sessions, likely due to extra training and increased exposure to the task. Importantly, by the third session of the experiment, all four groups of observers had significantly lower variable error using the novel and familiar cues together compared to their best single cue (35/40 observers were better with both cues than their best single cue in total across the groups in the third session), a feature of familiar cue combination. For two groups of observers, those who learned the colour and height cues, variable error using the novel and familiar cues together in the third session was not significantly different to the optimal variable error of an ideal observer who takes a reliabilityweighted average of estimates from the two single cues.</p><p>These findings complement the limited number of previous studies showing that the human perceptual system can combine newly learned novel cues with familiar cues to improve precision.</p><p>They extend the previous results to instances where observers must learn to use abstract novel cues to aid estimates of horizontal position on a computer screen.</p><p>In Experiment 2, we tested whether observers would also combine two newly learned novel cues (colour and angle size or colour and shape) to location with each other, as well as with a familiar cue (dot cloud).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Two separate groups, each of ten observers, completed a task three times in three separate sessions. The task required the observers to use one of two novel cues, a familiar cue, or two of the cues simultaneously to estimate the location of a hidden target by using a computer mouse to adjust the horizontal position of a bar on a computer screen. The task began with two blocks of training trials that taught observers the mappings between the novel cues (average colour or angle size of a set of stimuli for one group, and average colour or shape of a set of stimuli for the other) and</p><p>horizontal location on the screen. Observers completed the two novel cue training blocks in a random order. They were identical to the training blocks in Experiment 1 <ref type="figure">(Figure 1</ref>).</p><p>In each training block, observers first completed a set of trials where the mapping was shown on the screen. In these "with mapping" trials, the novel cue was presented at the bottom of the screen and observers were required to estimate the average colour, angle size of the cue, or shape of the cue, indicating their response by moving a vertical bar to the correct location along the mapping.</p><p>Observers then completed a set of "without mapping" trials that encouraged them to learn the relationship between the cues and location as the mapping was no longer shown. Learning of the mapping was reinforced through feedback in these trials, with observers shown the correct average colour, angle size, or shape in the correct location as feedback. The direction of the mapping (left-toright or right-to-left) on the screen was randomly determined for each novel cue for each observer.</p><p>After observers completed both novel cue training blocks, the test trials began <ref type="figure" target="#fig_1">(Figure 3)</ref>. At the start of the test block, observers were instructed that they would now begin to use the newly learnt novel cues, along with a familiar cue (a dot-cloud, or the spread cue) to estimate the location of a hidden object -an octopus hiding in the sea. The two different groups of ten observers (the colourangle-spread group and the colour-shape-spread group) saw different combinations of trials. were spread across the screen and had the property of the novel cue (either the relevant colours or angles between the lines). In colour-angle trials, the eight pairs of lines appeared in their fixed positions at the bottom of the screen and had the property of both novel cues (both the relevant colours and angles between the lines).</p><p>The colour-shape-spread group of observers also experienced the colour-only, spread-only, and colour-spread trials, with the small difference that cues were no longer presented as pairs of lines but as grey or coloured squares. This group of observers also experienced shape-only, shape-spread, or colour-shape trials. In shape-only and colour-shape trials, observers were presented with eight ovals (in fixed positions) at the bottom of the screen. Either the average axis ratio of the ovals alone (shape-only trials) or both the average axis ratio and colour of the ovals (colour-shape trials)</p><p>provided a novel estimate of location according to the trained mappings. In shape-spread trials, the eight ovals were spread across the screen and had the property of the novel cue (the relevant axis ratios).</p><p>For both groups of observers, trials of all types were interleaved. After the cue(s) appeared on each trial, observers adjusted the horizontal position of a vertical line, using a mouse, to their best guess of the hidden location. Feedback was given indicating if the observers had "caught" the octopus along with an indicator of the true hidden location that displayed the corresponding novel cue values (the colour or angle size, or the colour and shape). If the octopus was caught, an animation showed the octopus move across the screen from its hidden location to the bucket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observers</head><p>Ten observers were recruited for the colour-angle-spread group (6 female, age range 22-28 years) and ten for the colour-shape-spread group (9 female, age range 19-36 years) using Durham Psychology Department's Participant Pool programme or through word of mouth. All observers had normal or corrected to normal visual acuity (self-report) and no colour vision deficiencies (assessed using Ishihara Colour Plates). Each observer was given either £8 per hour or participant pool credits for their time. All observers gave written, informed consent prior to taking part in the study. Ethical approval was received from the Durham University Psychology Department Ethics Board (reference number: 17/07).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus and Stimuli</head><p>The apparatus and stimuli were the same we have already described for Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Parameters</head><p>In the colour, angle, and shape cue training blocks there were two repeats of each of possible hidden locations (15% to 85% of the way across the screen from left to right, sampled every 2%) for both the "with mapping" and "without mapping" trials (72 trials of each type). In the test block, the same 36 unique hidden locations were used, with each repeated three times for each trial type <ref type="bibr">(colour-angle-spread group: colour-only, angle-only, spread-only, colour-spread, angle-spread, colour-angle; colour-shape-spread group: colour-only, shape -only, spread-only, colour-spread, shape-spread, colour-shape; 108 trials each)</ref>. Trials of all types were interleaved and presented in a random order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>The analysis procedure was identical to Experiment 1. The mean strengths of the central bias for each trial type in the third session for the colour-angle-spread group (averaged across sessions and observers), where zero would indicate no bias and larger numbers indicate increasing bias, were:</p><p>colour-only = 0.1, angle-only = 0.05, spread-only = 0.09, colour-spread = 0.06, anglespread = 0.02, and colour-angle = 0.01. The mean strengths of the central bias for each trial type in the third session for the colour-shape-spread group were: colour-only = 0.13, shape-only = 0.11, spread-only = 0.1, colour-spread = 0.05, shape-spread = 0.05, and colour-shape = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices Statement</head><p>This experiment was not pre-registered. The raw data files and analysis script are available online at https://osf.io/gj92a/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Results</head><p>Each row of plots in <ref type="figure" target="#fig_2">Figure 4</ref> shows the data that pertains to each possible cue pairing for the colourangle-spread group. In the top row, we plot data from the colour-only, spread-only, and colourspread trials. In the second row, we plot data from the angle-only, spread-only, and angle-spread trials. In the third row, we plot data from the colour-only, angle-only, and colour-angle trials. The left panel of plots shows variable error using the familiar and novel cues alone across sessions ( <ref type="figure" target="#fig_2">Figure    4A</ref>-C). These plots show that variable error using the familiar spread cue and novel colour cue is stable across sessions but that observers get better using the novel angle cue with increased training and exposure to the task. The right panel of plots shows variable error in each session using the worst of the two single cues (highest variable error), the best of the two single cues (lowest variable error), both cues together, and the optimal variable error using both cues together that would be achieved by taking a reliability-weighted average of estimates from the two single cues ( <ref type="figure" target="#fig_2">Figure 4D-F)</ref>.</p><p>A visual inspection of <ref type="figure" target="#fig_2">Figure 4D</ref>-F suggests that the median variable error using both cues together may be lower than the best single cue in the third session of the experiment when using the angle and spread cues together but not the other pairs of cues. We also see that the median variable errors using both cues are all higher than the optimal variable error from reliability-weighted</p><p>averaging, suggesting that even if some pairing of cues resulted in combination, the combination was suboptimal.   <ref type="figure" target="#fig_3">Figure 5</ref> shows the data in the same way for the colour-shape-spread group. These plots show that variable error using all cues was stable across sessions for this group of observers ( <ref type="figure" target="#fig_3">Figure 5A-C)</ref>. A visual inspection of <ref type="figure" target="#fig_3">Figure 5D</ref>-F suggests that the median variable error using both cues together may be lower than the best single cue in the second and third session for all cue pairs and that median variable errors using both cues seem to approach the optimal variable error from reliabilityweighted averaging, suggesting combination may be optimal for this group of observers.  Observers quickly learned to use the novel cues and variability using some of the cues decreased with repeated training and exposure to the task in the colour-angle-spread group</p><p>Nine of the ten colour-angle-spread observers passed the learning criterion in all three sessions of the experiment. The remaining observer passed the learning criterion in the second and third sessions. Six of the ten colour-shape-spread observers passed the learning criterion in all three sessions. Of the remaining four, three of them passed the criterion in the second and third sessions, but one only passed the learning criterion in the second but not third session. Thus, overall, observers quickly learned the mappings between the novel cues and location and could use the novel cues to complete the task.</p><p>The colour-angle-spread observers reduced their variable error over time using the colour cue</p><p>( 2 (2) = 6.89, = .032, <ref type="figure" target="#fig_2">Figure 4A</ref>) and angle cue ( 2 (2) = 14.6, = .001, <ref type="figure" target="#fig_2">Figure 4B</ref>), but not the spread cue ( 2 (2) = 2.89, = .236, <ref type="figure" target="#fig_2">Figure 4A</ref>). Using the angle cue, variable errors reduced significantly from session one to three ( = 55, = .002) and two to three ( = 54, = .004).</p><p>None of the pairwise comparisons were significant for the colour cue, but the median variable error showed the same trend of reducing across sessions.</p><p>The colour-shape-spread observers did not reduce variable error over time for any of the cues (spread cue: 2 (2) = 1.8, = .407, <ref type="figure" target="#fig_3">Figure 5A</ref>; colour cue: 2 (2) = 0.25, = .882, <ref type="figure" target="#fig_3">Figure 5B</ref>;</p><p>shape cue: 2 (2) = 1, = .607, <ref type="figure" target="#fig_3">Figure 5C</ref>) Novel and familiar cues were consistently combined in the colour-shape-spread group but not the colour-angle-spread group, and novel colour and shape cues were combined while novel colour and angle cues were not <ref type="table" target="#tab_4">Table 2</ref> summarises the results for the colour-angle-spread group. In the first session, this group did not show evidence of combination for any cue pairing but were only suboptimal in colour-spread and colour-angle trials (rows 1-3 in Table 2; <ref type="figure" target="#fig_3">Figure 5</ref>). In the second session, they showed evidence of combination in colour-spread and angle-spread trials but not colour-angle and did not differ from optimal for any trial type (rows 4-6 in Table 2; <ref type="figure" target="#fig_3">Figure 5</ref>). In the third session, the colour-angle-spread group only showed evidence of combination in angle-spread trials and were suboptimal in all trial types (rows 7-9 in Table 2; <ref type="figure" target="#fig_3">Figure 5</ref>).  <ref type="table" target="#tab_5">Table 3</ref> summarises the results for the colour-shape-spread group. In the first session, this group also did not show evidence of combination for any cue pairing but were only suboptimal in colour-spread and shape-spread trials (rows 1-3 in <ref type="table" target="#tab_5">Table 3</ref>; <ref type="figure">Figure 6</ref>). In the second session, they showed evidence of combination and did not differ from optimal for any trial type (rows 4-6 in <ref type="table" target="#tab_5">Table 3</ref>; <ref type="figure">Figure 6</ref>). This was also true in the third session (rows 7-9 in <ref type="table" target="#tab_5">Table 3</ref>; <ref type="figure">Figure 6</ref>). We found that observers quickly learned to use the novel cues to location. Although use of some novel cues improved over time (location estimate variability reduced), observers were able to use the cues in the first session of the experiment, implying that they had leaned the association after only a small number of training trials. Observers were able to combine the newly learned novel cues with a familiar cue to improve precision (reduce variability) regardless of the pair of cues that they learned, but combination of novel and familiar cues was inconsistent for the colour-angle-spread group and often suboptimal. While the colour-shape group combined the two novel cues with each other to improve precision, the colour-angle-spread group did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>It is clear that a mature perceptual system can learn new mappings between novel cues and properties of the environment <ref type="bibr" target="#b9">(Di Luca et al., 2010;</ref><ref type="bibr" target="#b11">Ernst, 2007;</ref><ref type="bibr" target="#b20">Haijiang et al., 2006;</ref><ref type="bibr" target="#b22">Harrison &amp; Backus, 2012;</ref><ref type="bibr" target="#b35">Michel &amp; Jacobs, 2008;</ref><ref type="bibr" target="#b40">Negen et al., 2018)</ref>, with a limited number of studies suggesting that novel cues can be integrated into the normal perceptual experience by combining them with familiar cues in a "Bayes-like" way to increase perceptual precision <ref type="bibr" target="#b11">(Ernst, 2007;</ref><ref type="bibr" target="#b16">Gibo et al., 2017;</ref><ref type="bibr" target="#b35">Michel &amp; Jacobs, 2008;</ref><ref type="bibr" target="#b40">Negen et al., 2018)</ref>. Here, we trained observers to use abstract novel cues to estimate the horizontal location of hidden objects on a computer screen. In Experiment 1, observers benefitted from a suboptimal but significant gain in precision using novel and familiar cues together, extending previous reports of novel-familiar cue combination. We found evidence of a reduction in variable error from combining novel and familiar cues in the third session of the experiment for all four of the abstract novel cues we tested. In Experiment 2, we tested for the first time whether two novel cues may also be combined with each other. We found that one pair of novel cues could be combined to improve precision but the other could not, even after three sessions of repeated training. Taken together, our results add to the current literature on the integration of novel cues into the normal perceptual experience by showing that abstract novel cues to location are quickly learned and combined with familiar cues to increase perceptual precision, but that whether two novel cues to location are combined may depend on the choice of cues.</p><p>Why might some pairs of novel cues be easier to combine than others?</p><p>Whether or not two cues are combined can depend on the strength of the belief that the two cues are coupled <ref type="bibr" target="#b10">(Ernst, 2006)</ref> or that they come from the same source <ref type="bibr" target="#b31">(Körding et al., 2007)</ref>. It is possible that, in Experiment 2, the colour-shape group were able to combine the two novel cues, but the colour-angle group were not because our observers were more likely to expect a coupling or correspondence between colour and shape than they were between colour and angle size. There are many natural associations between different shapes and colours, but it is harder to think of similar associations between different angle sizes and colours. Indeed, in the colour perception literature there several reports of object shape modulating colour perception, such as when a grey banana appears slightly yellow <ref type="bibr" target="#b21">(Hansen, Olkkonen, Walter, &amp; Gegenfurtner, 2006;</ref><ref type="bibr" target="#b41">Olkkonen, Hansen, &amp; Gegenfurtner, 2008;</ref><ref type="bibr" target="#b48">Witzel &amp; Hansen, 2015;</ref><ref type="bibr" target="#b51">Witzel, Valkova, Hansen, &amp; Gegenfurtner, 2011)</ref>, an effect that can also be conceptualised within a reliability-weighted averaging framework where shape is an extra cue to colour <ref type="bibr" target="#b50">(Witzel, Olkkonen, &amp; Gegenfurtner, 2018)</ref>. This could explain why observers combined colour and shape cues but not colour and angle size cues in Experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why is combination of novel and familiar cues often suboptimal?</head><p>To take a reliability-weighted average of novel and familiar cues, observers must learn the novel cue's reliability. Obtaining an accurate estimate of the novel cue's reliability may require more time (feedback) than is offered in our experiments. In contrast, this is not an issue in experiments where an observer is presented with two familiar cues, where we can expect that, through a lifetime of repeated exposure, they have good internal estimates of the cue reliabilities. Such an explanation is in line with the inability of children to combine cues before the age of 10 <ref type="bibr" target="#b18">(Gori, Del Viva, Sandini, &amp; Burr, 2008;</ref><ref type="bibr" target="#b37">Nardini, Bedford, &amp; Mareschal, 2010)</ref> unless they receive explicit training <ref type="bibr" target="#b39">(Negen et al., 2019)</ref>. In our task, variable error using some of the novel cues decreases over time, so not only might repeated exposure be needed to develop good internal estimates of the cue reliabilities, but the learning the correct reliabilities is made harder by the fact that they are still to stabilise.</p><p>Another possibility is that optimal combination in not possible for the type of information provided to observers in our task. In classic cue combination experiments, low-level sensory cues are combined to increase perceptual precision and enhance discrimination <ref type="bibr" target="#b1">(Alais &amp; Burr, 2004;</ref><ref type="bibr" target="#b12">Ernst &amp; Banks, 2002;</ref><ref type="bibr" target="#b29">Knill &amp; Saunders, 2003)</ref>. In contrast, our task requires cues to be combined to improve precision of visually guided behaviours. It may be that the distinction between learning of new visually guided behaviours in response to new cues and learning of new cues that change perception is an important one if reliability-weighted averaging is a computation that is only performed by lowlevel sensory mechanisms, with the brain unable to perform the same calculation across more complex, higher-level information <ref type="bibr" target="#b25">(Jarvstad, Hahn, Warren, &amp; Rushton, 2014;</ref><ref type="bibr" target="#b44">Summerfield &amp; Tsetsos, 2012;</ref><ref type="bibr" target="#b52">Wu, Delgado, &amp; Maloney, 2009)</ref>. However, we must also note that even low-level sensory cue combination is not always optimal <ref type="bibr" target="#b43">(Rahnev &amp; Denison, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Overall, our results provide extensive evidence that novel cues can be learned and combined with familiar cues to enhance perception, but mixed evidence for whether perceptual and decisionmaking systems can extend this ability to the combination of multiple novel cues with only shortterm training. Whether the ability can be extended to the case of two novel cues may depend on the choice of cues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The novel colour cue was a set of eight pairs of parallel lines (length 24, width 5 pixels) where each pair of lines varied slightly in colour. The colour of the dots or pairs of lines was governed by a colour gradient from pink to green that mapped from 15% to 85% of the way across the screen from left toright or right to left (randomly flipped for each observer). The gradient was defined as a chord of a hue circle (chroma = 85) in CIELUV chromaticity space. The start and end values of the chord had CIE 1931 chromaticities of ( , ) = (. 3386, .2821) and ( , ) = (.3476, .3960) and a luminance of = 15 cd/m 2 . The colour gradient was defined in this way to ensure perceptual uniformity and defined a mapping from colour to location across the screen. The colours of the eight pairs of lines were defined by drawing eight horizontal positions from a Gaussian distribution centred on the hidden</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The test trials in Experiment 2.(A-B)  In test trials, observers used either one of the newly learned novel cues, a familiar spread cue, both the novel cues together, or one of the novel cues and the familiar cue together to estimate the position of a hidden object (an octopus hiding in the sea).On each trial, the colour-angle-spread group of observers were presented with either the colour cue, angle cue, or spread cue alone (colour-only, angle-only, or spread-only trials), or with a pairing of two cues (colour-spread, angle-spread, or colour-angle trials). In colour-only and angle-only trials, observers were presented with eight pairs of lines (in fixed positions) at the bottom of the screen.The average colour of the pair of lines or angle between them provided a novel estimate of location according to the trained mappings. In spread-only trials, eight pairs of parallel and grey lines (no novel cue information) were spread out across the screen. The position of each pair of lines was drawn from a Gaussian distribution, centred on the hidden location, such that the mean or centroid of the locations was the best estimate. In colour-spread or angle-spread trials, the eight pairs of lines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results of the colour-angle-spread group in Experiment 2. (A-C) Variable errors using the familiar and novel cues alone for each group of observers across sessions. (D-F) Variable errors for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results of the colour-shape-spread group in Experiment 2. (A-C) Variable errors using the familiar and novel cues alone for each group of observers across sessions. (D-F) Variable errors for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="10">: Statistical tests for evidence of combination and a difference from optimal for each group in</cell></row><row><cell cols="11">each session of Experiment 1. A one-tailed Wilcoxon Signed-Rank test was used to test for evidence</cell></row><row><cell cols="11">of combination and a two-tailed test was used to test for a difference from optimal. The columns</cell></row><row><cell cols="11">"Best &gt; Both" and "Both &gt; Optimal" show the number of participants whose individual data satisfy</cell></row><row><cell cols="11">the inequality out of the total number of participants included in the analysis of that session for that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>group.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Row</cell><cell>Group</cell><cell cols="2">Session Best &gt;</cell><cell></cell><cell></cell><cell>Combine?</cell><cell>Both &gt;</cell><cell></cell><cell></cell><cell>Subopti</cell></row><row><cell>No.</cell><cell></cell><cell></cell><cell>Both</cell><cell></cell><cell></cell><cell></cell><cell>Optimal</cell><cell></cell><cell></cell><cell>mal?</cell></row><row><cell>1</cell><cell>Colour</cell><cell>1</cell><cell>8/10</cell><cell>51</cell><cell>.007</cell><cell>Yes</cell><cell>9/10</cell><cell>53</cell><cell>.006</cell><cell>Yes</cell></row><row><cell>2</cell><cell>Angle</cell><cell>1</cell><cell>4/10</cell><cell>20</cell><cell>.784</cell><cell>No</cell><cell>10/10</cell><cell>55</cell><cell>.002</cell><cell>Yes</cell></row><row><cell>3</cell><cell>Shape</cell><cell>1</cell><cell>7/9</cell><cell>36</cell><cell>.064</cell><cell>No</cell><cell>9/9</cell><cell>45</cell><cell>.004</cell><cell>Yes</cell></row><row><cell>4</cell><cell>Height</cell><cell>1</cell><cell>5/10</cell><cell>31</cell><cell>.385</cell><cell>No</cell><cell>10/10</cell><cell>55</cell><cell>.002</cell><cell>Yes</cell></row><row><cell>5</cell><cell>Colour</cell><cell>2</cell><cell>10/10</cell><cell>55</cell><cell>.001</cell><cell>Yes</cell><cell>10/10</cell><cell>55</cell><cell>.002</cell><cell>Yes</cell></row><row><cell>6</cell><cell>Angle</cell><cell>2</cell><cell>8/10</cell><cell>49</cell><cell>.014</cell><cell>Yes</cell><cell>9/10</cell><cell>54</cell><cell>.004</cell><cell>Yes</cell></row><row><cell>7</cell><cell>Shape</cell><cell>2</cell><cell>10/10</cell><cell>55</cell><cell>.001</cell><cell>Yes</cell><cell>8/10</cell><cell>50</cell><cell>.02</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistical tests for evidence of combination and a difference from optimal for the colourangle-spread group in Experiment 2. A one-tailed Wilcoxon Signed-Rank test was used to test forevidence of combination and a two-tailed test was used to test for a difference from optimal. The columns "Best &gt; Both" and "Both &gt; Optimal" show the number of participants whose individual data satisfy the inequality out of the total number of participants included in the analysis of that session.</figDesc><table><row><cell>Row</cell><cell>Cue</cell><cell cols="2">Session Best &gt;</cell><cell></cell><cell></cell><cell>Combine?</cell><cell>Both &gt;</cell><cell></cell><cell></cell><cell>Subopti</cell></row><row><cell>No.</cell><cell>Pairing</cell><cell></cell><cell>Both</cell><cell></cell><cell></cell><cell></cell><cell>Optimal</cell><cell></cell><cell></cell><cell>mal?</cell></row><row><cell>1</cell><cell>Colour-</cell><cell>1</cell><cell>7/9</cell><cell>34</cell><cell>.102</cell><cell>No</cell><cell>9/9</cell><cell>45</cell><cell>.004</cell><cell>Yes</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Angle-</cell><cell>1</cell><cell>7/9</cell><cell>30</cell><cell>.213</cell><cell>No</cell><cell>8/9</cell><cell>38</cell><cell>.074</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>Colour-</cell><cell>1</cell><cell>6/9</cell><cell>27</cell><cell>.326</cell><cell>No</cell><cell>8/9</cell><cell>40</cell><cell>.039</cell><cell>Yes</cell></row><row><cell></cell><cell>angle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>Colour-</cell><cell>2</cell><cell>7/10</cell><cell>48</cell><cell>.019</cell><cell>Yes</cell><cell>8/10</cell><cell>43</cell><cell>.131</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>Angle-</cell><cell>2</cell><cell>9/10</cell><cell>45</cell><cell>.042</cell><cell>Yes</cell><cell>7/10</cell><cell>43</cell><cell>.131</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>Colour-</cell><cell>2</cell><cell>7/10</cell><cell>36</cell><cell>.216</cell><cell>No</cell><cell>8/10</cell><cell>41</cell><cell>.193</cell><cell>No</cell></row><row><cell></cell><cell>angle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>Colour-</cell><cell>3</cell><cell>6/10</cell><cell>42</cell><cell>.08</cell><cell>No</cell><cell>8/10</cell><cell>47</cell><cell>.049</cell><cell>Yes</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>Angle-</cell><cell>3</cell><cell>9/10</cell><cell>45</cell><cell>.042</cell><cell>Yes</cell><cell>9/10</cell><cell>53</cell><cell>.006</cell><cell>Yes</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>Colour-</cell><cell>3</cell><cell>3/10</cell><cell>22</cell><cell>.722</cell><cell>No</cell><cell>9/10</cell><cell>54</cell><cell>.004</cell><cell>Yes</cell></row><row><cell></cell><cell>angle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistical tests for evidence of combination and a difference from optimal for the colourshape-spread group in Experiment 2. A one-tailed Wilcoxon Signed-Rank test was used to test for 26 evidence of combination and a two-tailed test was used to test for a difference from optimal. The columns "Best &gt; Both" and "Both &gt; Optimal" show the number of participants whose individual data satisfy the inequality out of the total number of participants included in the analysis of that session.</figDesc><table><row><cell>Row</cell><cell>Cue</cell><cell cols="2">Session Best &gt;</cell><cell></cell><cell></cell><cell>Combine?</cell><cell>Both &gt;</cell><cell></cell><cell></cell><cell>Subopti</cell></row><row><cell>No.</cell><cell>Pairing</cell><cell></cell><cell>Both</cell><cell></cell><cell></cell><cell></cell><cell>Optimal</cell><cell></cell><cell></cell><cell>mal?</cell></row><row><cell>1</cell><cell>Colour-</cell><cell>1</cell><cell>5/8</cell><cell>28</cell><cell>.098</cell><cell>No</cell><cell>8/8</cell><cell>36</cell><cell>.008</cell><cell>Yes</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Shape-</cell><cell>1</cell><cell>5/8</cell><cell>23</cell><cell>.273</cell><cell>No</cell><cell>8/8</cell><cell>36</cell><cell>.008</cell><cell>Yes</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>Colour-</cell><cell>1</cell><cell>4/6</cell><cell>13</cell><cell>.344</cell><cell>No</cell><cell>5/6</cell><cell>18</cell><cell>.156</cell><cell>No</cell></row><row><cell></cell><cell>shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>Colour-</cell><cell>2</cell><cell>8/10</cell><cell>51</cell><cell>.007</cell><cell>Yes</cell><cell>5/10</cell><cell>32</cell><cell>.695</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>Shape-</cell><cell>2</cell><cell>9/10</cell><cell>53</cell><cell>.003</cell><cell>Yes</cell><cell>9/10</cell><cell>46</cell><cell>.064</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>Colour-</cell><cell>2</cell><cell>8/10</cell><cell>51</cell><cell>.007</cell><cell>Yes</cell><cell>8/10</cell><cell>46</cell><cell>.064</cell><cell>No</cell></row><row><cell></cell><cell>shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>Colour-</cell><cell>3</cell><cell>9/10</cell><cell>51</cell><cell>.007</cell><cell>Yes</cell><cell>6/10</cell><cell>42</cell><cell>.16</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>Shape-</cell><cell>3</cell><cell>8/9</cell><cell>37</cell><cell>.049</cell><cell>Yes</cell><cell>6/9</cell><cell>39</cell><cell>.055</cell><cell>No</cell></row><row><cell></cell><cell>spread</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-F)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>Colour-</cell><cell>3</cell><cell>9/9</cell><cell>45</cell><cell>.002</cell><cell>Yes</cell><cell>6/9</cell><cell>25</cell><cell>.82</cell><cell>No</cell></row><row><cell></cell><cell>shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(N-N)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Experiment 2: Summary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29</head><p>We would like to thank Sophie Barnes, Abbey Fletcher, and Josefin Rosman for their help with data collection. We would also like to thank Anya Hurlbert for use of the Konica Minolta CS2000 and James Negen for many useful conversations. This project has received funding from the European </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EyeMusic : Introducing a &quot; visual &quot; colorful experience for the blind using auditory sensory substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levy-Tzedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maidenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amedi</surname></persName>
		</author>
		<idno type="DOI">10.3233/RNN-130338</idno>
		<ptr target="https://doi.org/10.3233/RNN-130338" />
	</analytic>
	<monogr>
		<title level="j">Restorative Neurology and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="247" to="257" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ventriloquist Effect Results from Near-Optimal Bimodal Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burr</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0960-9822(04)00043-0</idno>
		<ptr target="https://doi.org/10.1016/S0960-9822(04)00043-0" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="262" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Central tendency biases must be accounted for to consistently capture Bayesian cue combination in continuous response data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Negen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Beierholm</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01633-2</idno>
		<ptr target="https://doi.org/10.3758/s13428-021-01633-2" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with &quot;The vOICe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auvray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanneton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Regan</surname></persName>
		</author>
		<idno type="DOI">10.1068/p5631</idno>
		<ptr target="https://doi.org/10.1068/p5631" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision Substitution by Tactile Image Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bach-Y-Rita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Scadden</surname></persName>
		</author>
		<idno type="DOI">10.1038/221963a0</idno>
		<ptr target="https://doi.org/10.1038/221963a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page">963</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning and inference using complex generative models in a spatial localization task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Bejjanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1167/16.5.9.doi</idno>
		<ptr target="https://doi.org/10.1167/16.5.9.doi" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Psychophysics Toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/9176952" />
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The development of Bayesian integration in sensorimotor estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sokhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gaebler-Spira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<idno type="DOI">10.1167/18.12.8</idno>
		<ptr target="https://doi.org/10.1167/18.12.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Use an Invisible Visual Signal for Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Di Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Backus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2010.09.047</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2010.09.047" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1860" to="1863" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Bayesian view on multimodal cue integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2011.11.039</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2011.11.039" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="105" to="131" />
		</imprint>
	</monogr>
	<note>Human Body Perception from the inside Out</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to integrate arbitrary signals from vision and touch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<idno type="DOI">10.1167/7.5.7</idno>
		<ptr target="https://doi.org/10.1167/7.5.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Humans integrate visual and haptic information in a statistically optimal fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<idno type="DOI">10.1038/415429a</idno>
		<ptr target="https://doi.org/10.1038/415429a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">6870</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Perceptual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fahle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<editor>M. Fahle &amp; T. Poggio</editor>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-G</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G*</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03193146</idno>
		<ptr target="https://doi.org/10.3758/BF03193146" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trust in haptic assistance: weighting visual and haptic cues based on error history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Gibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mugge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Abbink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2533" to="2546" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s00221-017-4986-4</idno>
		<ptr target="https://doi.org/10.1007/s00221-017-4986-4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Young Children Do Not Integrate Visual and Haptic Form Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Del Viva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Burr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="694" to="698" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cub.2008.04.036</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2008.04.036" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Demonstration of cue recruitment: Change in visual appearance by means of Pavlovian conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Haijiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Backus</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0506728103</idno>
		<ptr target="https://doi.org/10.1073/pnas.0506728103" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="483" to="488" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory modulates color appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1794</idno>
		<ptr target="https://doi.org/10.1038/nn1794" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1367" to="1368" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associative learning of shape as a cue to appearance : A new demonstration of cue recruitment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Backus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1167/12.3.15.Introduction</idno>
		<ptr target="https://doi.org/10.1167/12.3.15.Introduction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slant from texture and disparity cues: Optimal cue combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<idno type="DOI">10.1167/4.12.1</idno>
		<ptr target="https://doi.org/10.1167/4.12.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are perceptuo-motor decisions really more optimal than cognitive decisions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarvstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Rushton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="416" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cognition.2013.09.009</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2013.09.009" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian transfer in a complex spatial localization task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Kiryakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nardini</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.20.6.17</idno>
		<ptr target="https://doi.org/10.1167/jov.20.6.17" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">What&apos;s new in Psychtoolbox-3? Perception 36 ECVP Abstract Supplement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Do humans optimally integrate stereo and texture information for judgments of surface slant?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="2539" to="2558" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/S0042-6989</idno>
		<ptr target="https://doi.org/10.1016/S0042-6989" />
		<imprint>
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal Inference in Multisensory Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0000943</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0000943" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian integration in sensorimotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature02169</idno>
		<ptr target="https://doi.org/10.1038/nature02169" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">6971</biblScope>
			<biblScope unit="page" from="244" to="247" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The &quot;EyeCane&quot;, a new electronic travel aid for the blind: Technology, behavior &amp; swift learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maidenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Buchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Chebat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levy-Tzedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Restorative Neurology and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="813" to="824" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.3233/RNN-130351</idno>
		<ptr target="https://doi.org/10.3233/RNN-130351" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning optimal integration of arbitrary features in a perceptual discrimination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1167/8.2.3.Introduction</idno>
		<ptr target="https://doi.org/10.1167/8.2.3.Introduction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fusion of visual cues is not mandatory in children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mareschal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="17041" to="17046" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1001699107</idno>
		<ptr target="https://doi.org/10.1073/pnas.1001699107" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sensory cue combination in children under 10 years of age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Negen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Roome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Keenaghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nardini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2019.104014</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.cognition.2019.104014" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayes-Like Integration of a New Sensory Skill with Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Negen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nardini</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-35046-7</idno>
		<ptr target="https://doi.org/10.1038/s41598-018-35046-7" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16880</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Color appearance of familiar objects: Effects of object shape, texture, and illumination changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<idno type="DOI">10.1167/8.5.13</idno>
		<ptr target="https://doi.org/10.1167/8.5.13" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The VideoToolbox software for visual psychophysics: Transforming numbers into movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Pelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="437" to="442" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Suboptimality in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Denison</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X18000936</idno>
		<ptr target="https://doi.org/DOI:10.1017/S0140525X18000936" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Building bridges between perceptual and economic decisionmaking: Neural and computational mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2012.00070</idno>
		<ptr target="https://doi.org/10.3389/fnins.2012.00070" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Combining Priors and Noisy Visual Cues in a Rapid Pointing Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tassinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="10154" to="10163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.2779-06.2006</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2779-06.2006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Differential representations of prior and likelihood uncertainty in the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gottfried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2012.07.010</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2012.07.010" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Memory effects on color perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<editor>A. J. Elliot, A. Franklin, &amp; M</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<idno type="DOI">10.1017/CBO9781107337930.032</idno>
		<ptr target="https://doi.org/" />
		<title level="m">Handbook of Color Psychology</title>
		<editor>D. Fairchild</editor>
		<imprint>
			<biblScope unit="page" from="641" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A Bayesian Model of the Memory Colour Effect. I-Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<idno type="DOI">10.1177/2041669518771715</idno>
		<ptr target="https://doi.org/10.1177/2041669518771715" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Object knowledge modulates colour appearance. I-Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="13" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Economic decision-making compared with an equivalent motor task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0900102106</idno>
		<ptr target="https://doi.org/10.1073/pnas.0900102106" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="6088" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
